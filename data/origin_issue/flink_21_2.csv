Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocked),Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Cloners),Inward issue link (Completes),Inward issue link (Completes),Outward issue link (Completes),Inward issue link (Container),Outward issue link (Container),Inward issue link (Dependent),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Incorporates),Inward issue link (Issue split),Outward issue link (Issue split),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Required),Outward issue link (Supercedes),Inward issue link (Testing),Inward issue link (Testing),Outward issue link (Testing),Outward issue link (Testing),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Using a pipe symbol as pair delimiter in STR_TO_MAP in combination with concatenation results in broken output ,FLINK-25488,13420041,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,biyuhao,martijnvisser,martijnvisser,30/Dec/21 13:19,11/Jan/22 12:09,13/Jul/23 08:12,11/Jan/22 12:09,1.14.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Reproducible using Flink Faker:

{code:sql}
-- Create source table
CREATE TABLE `customers` (
  `identifier` STRING,
  `fullname` STRING,
  `postal_address` STRING,
  `residential_address` STRING
) WITH (
  'connector' = 'faker',
  'fields.identifier.expression' = '#{Internet.uuid}',
  'fields.fullname.expression' = '#{Name.firstName} #{Name.lastName}',
  'fields.postal_address.expression' = '#{Address.fullAddress}',
  'fields.residential_address.expression' = '#{Address.fullAddress}',
  'rows-per-second' = '1'
);
{code}

{code:sql}
-- Doesn't generate expected output
SELECT 
  `identifier`,
  `fullname`,
  STR_TO_MAP('postal_address:' || postal_address || '|residential_address:' || residential_address,'|',':') AS `addresses`
FROM `customers`;
{code}

Output will look like:
{code:sql}
{=, A=null, C=null, D=null, L=null, O=null, P=null, S=null, T=null, _=null,  =null, a=null, b=null, c=null, d=null, e=null, g=null, h=null, i=null, ,=null, l=null, -=null, m=null, .=null, n=null, o=null, p=null, q=null, 2=null, r=null, 3=null, s=null, 4=null, t=null, 5=null, u=null, 6=null, v=null, 7=null, w=null, 8=null, 9=null, |=null}
{code}

When using:
{code:sql}
-- Output looks like expected when using a different separator 
SELECT 
  `identifier`,
  `fullname`,
  STR_TO_MAP('postal_address:' || postal_address || ';residential_address:' || residential_address,';',':') AS `addresses`
FROM `customers`;
{code}

The output looks as expected:
{code:sql}
{postal_address=6654 Chong Meadows, East Lupita, CT 51702-8560, residential_address=Apt. 098 51845 Shields Fork, North Erikland, NV 10386}
{code}",,biyuhao,martijnvisser,matriv,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 11 12:09:15 UTC 2022,,,,,,,,,,"0|z0y4xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Dec/21 17:43;biyuhao; 

try this 
{code:java}
System.out.println(SqlFunctionUtils.strToMap(""a:1|b:2|c:3"", ""|"", "":""));
System.out.println(SqlFunctionUtils.strToMap(""a:1,b:2,c:3"", "","", "":"")); {code}
outputs
{code:java}
{=, a=null, 1=null, b=null, 2=null, c=null, 3=null, |=null}
{a=1, b=2, c=3} {code}
In SqlFunctionUtils#strToMap() the delimiters are used as regex expression.

[~MartijnVisser]  , could you please give some suggestions to fix this?

And maybe I can give it a try.

 ;;;","31/Dec/21 10:40;matriv;We should definitely document that it's a regex pattern and not simple string, so that the users need to properly escape such chars.;;;","31/Dec/21 10:43;matriv;The following works correctly:
{noformat}
System.out.println(SqlFunctionUtils.strToMap(""a:1|b:2|c:3"", ""\\|"", "":"")){noformat}
 ;;;","01/Jan/22 12:23;biyuhao;I've raised a PR.;;;","11/Jan/22 12:09;twalthr;Fixed in master: 7601bd36d8d7224efd1d0e4107262e2cec80e721;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Perjob can not recover from checkpoint when zookeeper leader changes,FLINK-25486,13420014,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Jiangang,Jiangang,Jiangang,30/Dec/21 11:20,30/Jan/22 16:23,13/Jul/23 08:12,29/Jan/22 15:45,1.13.5,1.14.2,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.4,1.15.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"When the config high-availability.zookeeper.client.tolerate-suspended-connections is default false, the appMaster will failover once zk leader changes. In this case, the old appMaster will clean up all the zk info and the new appMaster will not recover from the latest checkpoint.

The process is as following:
 # Start a perJob application.
 # kill zk's leade node which cause the perJob to suspend.
 # In MiniDispatcher's function jobReachedTerminalState, shutDownFuture is set to UNKNOWN .
 # The future is transferred to ClusterEntrypoint, the method is called with cleanupHaData true.
 # Clean up zk data and exit.
 # The new appMaster will not find any checkpoints to start and the state is lost.

Since the job can recover automatically when the zk leader changes, it is reasonable to keep zk info for the coming recovery.

 ",,dmvk,Jiangang,Ming Li,pnowojski,trohrmann,Zhanghao Chen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jan 29 15:45:53 UTC 2022,,,,,,,,,,"0|z0y4rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/22 10:53;Jiangang;Hello, [~trohrmann]. Can you verify the problem? If this is a problem, I would like to fix it. Thank you.;;;","05/Jan/22 11:34;trohrmann;Hi [~Jiangang], thanks for reporting this issue. I think this is indeed a bug and should be fixed. The problem seems as you described that the {{MiniDispatcher}} completes the {{shutDownFuture}} not only on globally terminal states.

Do you want to work on it?

cc [~dmvk].;;;","05/Jan/22 13:40;Jiangang;[~trohrmann] Thanks for the reply. It's also a problem in our production. I would like to fix it.;;;","05/Jan/22 14:06;dmvk;+1, this indeed sounds like a bug;;;","14/Jan/22 10:57;Jiangang;[~dmvk] Could you please review the code? Thanks.;;;","14/Jan/22 11:14;dmvk;Yes, I was off yesterday and I'm swamped with reviews, this one is on the list. Please be patient ;);;;","29/Jan/22 15:45;trohrmann;Fixed via

1.15.0: 8ba13f37afb9164f3bb17de78c4b0d85b1633638
1.14.4: 0b519c24222f61306f9faba2389c0958daa9cc0a
1.13.6: fe5a1718368e62eb7ac47c00aabbd94173dae668;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changlog materialization with incremental checkpoint cannot work well in local tests,FLINK-25479,13419872,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,yunta,yunta,29/Dec/21 13:07,02/Feb/22 12:26,13/Jul/23 08:12,28/Jan/22 21:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / Checkpointing,Runtime / State Backends,Tests,,,0,pull-request-available,,,,"Currently, changelog materialization would call RocksDB state backend's snapshot method to generate {{IncrementalRemoteKeyedStateHandle}} as ChangelogStateBackendHandleImpl's materialized artifacts. And before next materialization, it will always report the same {{IncrementalRemoteKeyedStateHandle}} as before.

For local tests, TM would report the {{IncrementalRemoteKeyedStateHandle}} to JM via local {{LocalRpcInvocation}}. However, as {{LocalRpcInvocation}} would not de/serialize message, which leads once we register the {{IncrementalRemoteKeyedStateHandle}} on JM side, it will also add a {{sharedStateRegistry}} to the one located on TM side. For the 2nd checkpoint, TM would reported same {{IncrementalRemoteKeyedStateHandle}} with  {{sharedStateRegistry}} to JM. And it will then throw exception as it already contains a {{sharedStateRegistry}}:

IncrementalRemoteKeyedStateHandle
{code:java}
public void registerSharedStates(SharedStateRegistry stateRegistry, long checkpointID) {
       Preconditions.checkState(
                sharedStateRegistry != stateRegistry,
                ""The state handle has already registered its shared states to the given registry."");

}
{code}

This bug would go in distribution environment as {{IncrementalRemoteKeyedStateHandle}} would be serialized and {{sharedStateRegistry}} is tagged as {{transient}}.",,roman,wind_ljy,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23559,FLINK-25143,,,,,,,,,,,,,,,,FLINK-25914,,,FLINK-25144,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 28 21:41:35 UTC 2022,,,,,,,,,,"0|z0y3w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jan/22 21:41;roman;Merged as 53c3525d0f4d4b3117edf1f8fcce0ff2ecfdda19.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The directory structure of the State Backends document is not standardized,FLINK-25477,13419837,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,masteryhx,masteryhx,masteryhx,29/Dec/21 09:00,04/Jan/22 03:15,13/Jul/23 08:12,31/Dec/21 08:15,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Documentation,Runtime / State Backends,,,,0,pull-request-available,,,,"The State Backends document uses multiple first-level headings. 
It may cause the directory structure displayed incorrectly.

Just as the picture shows, the two titles are not in the table of contents on the right.

 

!image-2021-12-29-16-56-24-657.png|width=522,height=230!",,masteryhx,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Dec/21 08:56;masteryhx;image-2021-12-29-16-56-24-657.png;https://issues.apache.org/jira/secure/attachment/13038020/image-2021-12-29-16-56-24-657.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 04 03:15:35 UTC 2022,,,,,,,,,,"0|z0y3o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/22 03:15;yunta;Merged in master: 332eee0a0d6edb9c9aa455d182f79977496af756

 

release-1.14: 7263d977150c7d7fb880a637e2d65996cca7ea3a

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Local recovery fails if local state storage and RocksDB working directory are not on the same volume,FLINK-25468,13419729,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,28/Dec/21 14:26,31/Dec/21 06:21,13/Jul/23 08:12,29/Dec/21 08:30,1.13.5,1.14.2,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"Local recovery with RocksDB fails if the state storage directory is not on the same volume as RocksDB's working directory. The reason is that the {{RocksDBHandle}} only tries to hard link the RocksDB files when calling {{restoreInstanceDirectoryFromPath}}. If hard linking is not supported, then the operation fails.

In order to harden this behaviour, I suggest to fall back to copying the files over if hard linking fails.",,trohrmann,ym,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-10954,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 29 08:30:07 UTC 2021,,,,,,,,,,"0|z0y308:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/21 08:30;trohrmann;Fixed via

1.15.0: 6bab695f85f8c9b1fec926880480c4ffb414a44b
1.14.3: f851f583e04ce671eb729b639c56618b4d5743a0
1.13.6: eace77b224e8981f12361b1eae3ec647f0fffdf2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TTL configuration could parse in StateTtlConfig#DISABLED,FLINK-25466,13419669,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,28/Dec/21 07:45,14/Feb/22 02:36,13/Jul/23 08:12,14/Feb/22 02:18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.7,1.14.4,1.15.0,,,Runtime / State Backends,,,,,0,pull-request-available,,,,Current API \{{StateDescriptor#enableTimeToLive(StateTtlConfig)}} cannot handle StateTtlConfig#DISABLED due to its current implementation.,,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 14 02:18:35 UTC 2022,,,,,,,,,,"0|z0y2mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/22 02:18;yunta;merged 
master: c1c966568b50c03257428e018f3cacd88b7dd116
release-1.14: 30994a1788085034ed1b467a5df6253ee44b1da6
release-1.13: ab86ffa78a2952126d90a4d4fa3ce0b16b957991;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the logic of materizating wrapped changelog state,FLINK-25465,13419665,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,28/Dec/21 07:16,29/Dec/21 02:45,13/Jul/23 08:12,29/Dec/21 02:45,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"{{ChangelogKeyedStateBackend#keyValueStatesByName}} would store wrapped state, such as TTL state or latency tracking state. It will throw exception on initMaterialization if wrapped:


{code:java}
for (InternalKvState<K, ?, ?> changelogState : keyValueStatesByName.values()) {
                checkState(changelogState instanceof ChangelogState);
                ((ChangelogState) changelogState).resetWritingMetaFlag();
}
{code}
",,roman,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25144,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 29 02:45:32 UTC 2021,,,,,,,,,,"0|z0y2m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/21 02:45;yunta;merged in master: 57391962d5119816beebec56e78a034130e0bfdd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python tests hangs on install dependencies,FLINK-25464,13419660,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,gaoyunhaii,gaoyunhaii,28/Dec/21 06:21,06/Jan/22 02:23,13/Jul/23 08:12,06/Jan/22 02:23,1.13.5,1.14.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,API / Python,Build System / Azure Pipelines,,,,0,pull-request-available,test-stability,,,"{code:java}
Dec 25 04:35:54 py38-cython create: /__w/1/s/flink-python/.tox/py38-cython
Dec 25 04:35:58 py38-cython installdeps: -rdev/dev-requirements.txt, pytest, apache-flink-libraries
Dec 25 04:51:00 ==============================================================================
Dec 25 04:51:00 Process produced no output for 900 seconds.
Dec 25 04:51:00 ==============================================================================
Dec 25 04:51:00 ==============================================================================
Dec 25 04:51:00 The following Java processes are running (JPS)
Dec 25 04:51:00 ==============================================================================
Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
Dec 25 04:51:00 137834 Jps
Dec 25 04:51:00 ==============================================================================
Dec 25 04:51:00 Printing stack trace of Java process 137834
Dec 25 04:51:00 ==============================================================================
Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
137834: No such process
Dec 25 04:51:00 Killing process with pid=725 and all descendants
./flink-python/dev/lint-python.sh: line 580:  2770 Terminated              $TOX_PATH -c $FLINK_PYTHON_DIR/tox.ini --recreate 2>&1
      2771                       | tee -a $LOG_FILE
/__w/1/s/tools/ci/watchdog.sh: line 113:   725 Terminated              $cmd
Dec 25 04:51:00 Process exited with EXIT CODE: 143.
Dec 25 04:51:00 Trying to KILL watchdog (720).
Dec 25 04:51:00 Searching for .dump, .dumpstream and related files in '/__w/1/s'
The STDIO streams did not close within 10 seconds of the exit event from process '/bin/bash'. This may indicate a child process inherited the STDIO streams and has not yet exited.
##[error]Bash exited with code '143'.
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28588&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23407",,gaoyunhaii,hxbks2ks,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 06 02:23:09 UTC 2022,,,,,,,,,,"0|z0y2kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Dec/21 06:23;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28592&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23402]

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28593&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=22752;;;","28/Dec/21 06:24;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28626&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23405;;;","28/Dec/21 06:25;gaoyunhaii;cc [~hxbks2ks] ~;;;","30/Dec/21 08:31;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28724&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=22774;;;","31/Dec/21 05:56;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28778&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23403;;;","02/Jan/22 03:00;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28819&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23401;;;","03/Jan/22 08:48;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28843&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=22753;;;","04/Jan/22 09:30;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28882&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23402;;;","04/Jan/22 09:30;trohrmann;Any updates on this issue [~hxbks2ks]?;;;","05/Jan/22 02:23;hxbks2ks;[~trohrmann] Because all hanging installation occurs in Python3.8 and other versions of Python do not have this problem, I guess it is caused by that an package is installed from source package(it will cause much more time) rather than installing from wheel package in Python 3.8. Unfortunately, the downloaded information is hidden that we can't see which package is being downloaded and installed. So I'm trying to adding some infos and run tests in my private Azure to find which package caused this unstable download failure.;;;","05/Jan/22 08:32;trohrmann;Thanks for the updates [~hxbks2ks]. Hopefully, you find the problem soon.;;;","05/Jan/22 08:32;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28946&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23404;;;","05/Jan/22 11:52;hxbks2ks;For a long time, all the log information about downloaded and installed by python has not been printed out if there is no failure during this progress. In the Azure laboratory network environment, this progress of installation takes a longer time. Once the total time exceeds 15 minutes, it will be killed although it doesn't hang up. My repair plan is to turn on log information (the downloaded infos is about 300 lines). 
The reason why python3.8 only hangs is that as I analyzed earlier, there is a python package of grpcio-tools under python3.8 that is installed from the source code rather than installation from whl package. This process takes about 5min, which lead to the total installation in the python3.8 environment is easier to exceed 15 minutes. I will create another JIRA and PR to upgrade grpcio-tools in the master branch.;;;","06/Jan/22 02:23;hxbks2ks;Merged into master via 99b8fc76ae5e61f62ef33726fb2c6744067dce27
Merged into release-1.14 via fee5532d34c32c683f32c118d9b350cc7b2ca755
Merged into release-1.13 via 02c433ca11a93099ceba7ade8c6e3b686676b7b9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read timeouts when downloading from alicloud-mvn-mirror ,FLINK-25457,13419518,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,martijnvisser,trohrmann,trohrmann,27/Dec/21 09:12,10/Jan/22 11:13,13/Jul/23 08:12,27/Dec/21 14:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Build System / Azure Pipelines,,,,,0,test-stability,,,,"Some of our builds fail with

{code}
[ERROR] The build could not read 1 project -> [Help 1]
[ERROR]   
[ERROR]   The project org.apache.flink:flink-parent:1.15-SNAPSHOT (/__w/3/s/pom.xml) has 7 errors
[ERROR]     Unresolveable build extension: Plugin org.apache.felix:maven-bundle-plugin:3.0.1 or one of its dependencies could not be resolved: Failed to read artifact descriptor for org.apache.felix:maven-bundle-plugin:jar:3.0.1: Could not transfer artifact org.apache.felix:maven-bundle-plugin:pom:3.0.1 from/to alicloud-mvn-mirror (http://172.17.0.1:8888/repository/maven-central/): transfer failed for http://172.17.0.1:8888/repository/maven-central/org/apache/felix/maven-bundle-plugin/3.0.1/maven-bundle-plugin-3.0.1.pom: Read timed out -> [Help 2]
[ERROR]     Non-resolvable import POM: Could not transfer artifact com.fasterxml.jackson:jackson-bom:pom:2.13.0 from/to alicloud-mvn-mirror (http://172.17.0.1:***@ line 545, column 16: Read timed out -> [Help 3]
[ERROR]     Non-resolvable import POM: Could not transfer artifact org.junit:junit-bom:pom:5.8.1 from/to alicloud-mvn-mirror (http://172.17.0.1:***@ line 583, column 16: Read timed out -> [Help 3]
[ERROR]     Non-resolvable import POM: Could not transfer artifact org.testcontainers:testcontainers-bom:pom:1.16.2 from/to alicloud-mvn-mirror (http://172.17.0.1:***@ line 801, column 16: Read timed out -> [Help 3]
[ERROR]     'dependencies.dependency.version' for org.junit.jupiter:junit-jupiter:jar is missing. @ line 193, column 15
[ERROR]     'dependencies.dependency.version' for org.junit.vintage:junit-vintage-engine:jar is missing. @ line 199, column 15
[ERROR]     'dependencies.dependency.version' for org.testcontainers:junit-jupiter:jar is missing. @ line 249, column 15
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException
[ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/PluginResolutionException
[ERROR] [Help 3] http://cwiki.apache.org/confluence/display/MAVEN/UnresolvableModelException
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28603&view=logs&j=3e60b793-4158-5027-ac6d-4cdc51dffe1e&t=d5ed4970-7667-5f7e-2ece-62e410f74748&l=32",,martijnvisser,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 27 14:51:01 UTC 2021,,,,,,,,,,"0|z0y1pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/21 09:13;trohrmann;cc [~chesnay], [~MartijnVisser];;;","27/Dec/21 14:51;martijnvisser;I think this error has been resolved. All Nexus instances have now been upgraded to the latest version and are working at the moment. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Negative time in throughput calculator,FLINK-25454,13419508,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,akalashnikov,akalashnikov,27/Dec/21 08:52,02/Nov/22 07:32,13/Jul/23 08:12,31/Dec/21 11:29,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.6,1.15.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,"Found during the random test:

{noformat}
2021-12-23 11:52:01,645 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - KeyedProcess -> Sink: Unnamed (3/3)#0 (1321490f33c6370f2d68c413a8a0b0c1) switched from RUNNING to FAILED with failure cause: java.lang.IllegalArgumentException: Time should be non negative
        at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)
        at org.apache.flink.runtime.throughput.ThroughputCalculator.calculateThroughput(ThroughputCalculator.java:90)
        at org.apache.flink.runtime.throughput.ThroughputCalculator.calculateThroughput(ThroughputCalculator.java:81)
        at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.triggerDebloating(SingleInputGate.java:414)
        at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.triggerDebloating(InputGateWithMetrics.java:90)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.debloat(StreamTask.java:786)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$scheduleBufferDebloater$3(StreamTask.java:777)
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
        at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:338)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:801)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:750)
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
        at java.base/java.lang.Thread.run(Unknown Source)
{noformat}",,akalashnikov,aliazov,pnowojski,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29845,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 23 09:33:04 UTC 2022,,,,,,,,,,"0|z0y1n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Dec/21 10:32;pnowojski;Merged to master as 7ccd525a15c^..7ccd525a15c;;;","14/Jun/22 15:03;aliazov; Is it possible to merge this fix also for  version 1.14. We are not yet ready to upgrade to 1.15. Thanks;;;","16/Jun/22 13:12;pnowojski;I've looked into backporting this fix, but it's not that simple. The fix would have to be re-implemented, as the code has changed quite a lot between 1.14.x and 1.15.x. If someone ([~aliazov]? [~akalashnikov]?) would like to pick up this work, and re-implement [~akalashnikov]'s fix for 1.14 I would be happy to review it.;;;","16/Jun/22 14:06;akalashnikov;I will take a look;;;","23/Jun/22 09:33;pnowojski;merged commit 10e6341 into apache:release-1.14

[~aliazov]: This fix will be released as part of 1.14.6 version (whenever this happens);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid sanity check on read bytes on DataInputStream#read(byte[]),FLINK-25446,13419475,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yunta,yunta,yunta,27/Dec/21 03:41,28/Dec/21 14:05,13/Jul/23 08:12,28/Dec/21 14:05,1.14.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Runtime / Checkpointing,Runtime / State Backends,,,,0,pull-request-available,,,,"Current changelog related code would check the number of read bytes whether equal to target bytes:

{code:java}
checkState(size == input.read(bytes));
{code}

However, this is not correct as the java doc said: {{""An attempt is made to read as many as len bytes, but a smaller number may be read, possibly zero.""}}
",,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25144,,,,,FLINK-25260,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 28 14:05:39 UTC 2021,,,,,,,,,,"0|z0y1fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Dec/21 14:05;yunta;Merged

master: c0f46ef324c35b3ed7813c74931ab9cb589896f7

release-1.14: 24016b83eec53ba7124d651201daa580323b80fc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ProducerFailedException will cause task status switch from RUNNING to CANCELED, which will cause the job to hang.",FLINK-25441,13419324,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wanglijie,wanglijie,wanglijie,24/Dec/21 08:27,13/Jan/22 14:08,13/Jul/23 08:12,13/Jan/22 14:08,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / Network,,,,,0,pull-request-available,,,,"The {{ProducerFailedException}} extends {{{}CancelTaskException{}}}, which will cause the task status switched from RUNNING to CANCELED. As described in FLINK-17726, if a task is directly CANCELED by TaskManager due to its own runtime issue, the task will not be recovered by JM and thus the job would hang.

Note that it will not cause problems before FLINK-24182 (it unifies the failureCause handling, changes the check of CancelTaskException from ""{{instanceof CancelTaskException}}"" to ""{{ExceptionUtils.findThrowable}}""), because the {{ProducerFailedException}} is always wrapped by {{{}RemoteTransportException{}}}.

The example log is as follows:
{code:java}
2021-12-23 21:20:14,965 DEBUG org.apache.flink.runtime.taskmanager.Task                    [] - MultipleInput[945] [Source: HiveSource-tpcds_bin_orc_10000.catalog_sales, Source: HiveSource-tpcds_bin_orc_10000.store_sales, Source: HiveSource-tpcds_bin_orc_10000.catalog_sales, Source: HiveSource-tpcds_bin_orc_10000.store_sales, Source: HiveSource-tpcds_bin_orc_10000.store_sales, Source: HiveSource-tpcds_bin_orc_10000.item, Source: HiveSource-tpcds_bin_orc_10000.web_sales, Source: HiveSource-tpcds_bin_orc_10000.web_sales] -&gt; Calc[885] (143/1024)#0 (8a883116ab601dd5b9ad5d2717d18918) switched from RUNNING to CANCELED due to CancelTaskException: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Error at remote task manager 'k28b09250.eu95sqa.tbsite.net/100.69.96.154:47459'.
  at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.decodeMsg(CreditBasedPartitionRequestClientHandler.java:301)
  at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.channelRead(CreditBasedPartitionRequestClientHandler.java:190)
  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
  at org.apache.flink.runtime.io.network.netty.NettyMessageClientDecoderDelegate.channelRead(NettyMessageClientDecoderDelegate.java:112)
  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
  at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
  at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
  at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:795)
  at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:480)
  at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378)
  at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
  at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
  at java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.flink.runtime.io.network.partition.ProducerFailedException: java.util.concurrent.TimeoutException: Buffer request timeout, this means there is a fierce contention of the batch shuffle read memory, please increase 'taskmanager.memory.framework.off-heap.batch-shuffle.size'.
  at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.writeAndFlushNextMessageIfPossible(PartitionRequestQueue.java:285)
  at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.enqueueAvailableReader(PartitionRequestQueue.java:123)
  at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.userEventTriggered(PartitionRequestQueue.java:234)
  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:346)
  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:332)
  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireUserEventTriggered(AbstractChannelHandlerContext.java:324)
  at org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.userEventTriggered(ChannelInboundHandlerAdapter.java:117)
  at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.userEventTriggered(ByteToMessageDecoder.java:365)
  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:346)
  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:332)
  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireUserEventTriggered(AbstractChannelHandlerContext.java:324)
  at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.userEventTriggered(DefaultChannelPipeline.java:1428)
  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:346)
  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:332)
  at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireUserEventTriggered(DefaultChannelPipeline.java:913)
  at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.lambda$notifyReaderNonEmpty$0(PartitionRequestQueue.java:91)
  at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
  at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
  at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384)
  at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
  at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
  at java.lang.Thread.run(Thread.java:756)
Caused by: java.util.concurrent.TimeoutException: Buffer request timeout, this means there is a fierce contention of the batch shuffle read memory, please increase 'taskmanager.memory.framework.off-heap.batch-shuffle.size'.
  at org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadScheduler.allocateBuffers(SortMergeResultPartitionReadScheduler.java:168)
  at org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadScheduler.run(SortMergeResultPartitionReadScheduler.java:139)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  ... 1 more
{code}",,kevin.cyj,pnowojski,reswqa,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 13 14:08:10 UTC 2022,,,,,,,,,,"0|z0y0ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Dec/21 08:29;wanglijie;cc [~kevin.cyj] [~pnowojski] ;;;","29/Dec/21 14:02;pnowojski;[~wanglijie95], do you mean that [exactly this change|https://github.com/apache/flink/pull/17253/commits/43f4db7ade41eb0d5e052dcc81748d71740d540f#diff-b84174e55cb1999d99ad60cdeded7be20ff4978472bfc785c5a77b6270f47b56R799-R801] is causing the problems?

It sounds to me like the logic there is correct. `ProducerFailedException` should be handled as `CancelTaskException`, as the downstream task is not the primary reason behind this failure. In principle I think

> ProducerFailedException will cause task status switch from RUNNING to CANCELED

is the correct thing to do. A better question would be why this error was not propagated up on the upstream task? It should be the upstream task's switch from `RUNNING` to `FAILED` trigger the job failover by JM. In other words, I think the bug here is that the ""java.util.concurrent.TimeoutException: Buffer request timeout, this means there is a fierce contention of the batch shuffle read memory, please increase 'taskmanager.memory.framework.off-heap.batch-shuffle.size'."" was not propagated on the upstream task. WDYT [~kevin.cyj]?

;;;","30/Dec/21 02:27;wanglijie;Hi [~pnowojski], thanks for your detailed explanation.

> do you mean that [exactly this change|https://github.com/apache/flink/pull/17253/commits/43f4db7ade41eb0d5e052dcc81748d71740d540f#diff-b84174e55cb1999d99ad60cdeded7be20ff4978472bfc785c5a77b6270f47b56R799-R801] is causing the problems?

Yes, I think that change exposes this problem, although it may not the root cause.

> `ProducerFailedException` should be handled as `CancelTaskException`

If so, I think the ""java.util.concurrent.TimeoutException: Buffer request timeout, this means there is a fierce contention of the batch shuffle read memory, please increase 'taskmanager.memory.framework.off-heap.batch-shuffle.size'"" should probably be wrapped as  {{PartitionException}} instead of {{{}ProducerFailedException{}}}, because the producer (upstream task) was FINISHED. (This problem appeared in batch jobs using sort shuffle). WDYT [~kevin.cyj] ?;;;","30/Dec/21 08:31;pnowojski;> because the producer (upstream task) was FINISHED. (This problem appeared in batch jobs using sort shuffle). 

If so, there shouldn't be any exception in the first place. In one way or another, the downstream task should reach `EndOfData`/``EndOfPartitionEvent` states cleanly.

[~wanglijie95] what is the scenario that's happening here? Upstream task finished before the downstream managed to start up? Was there any data produced, or was the partition empty?;;;","30/Dec/21 13:23;wanglijie;[~pnowojski] 

> what is the scenario that's happening here? Upstream task finished before the downstream managed to start up?

For batch jobs, I think it's always the upstream tasks finished first, and then the downstream tasks are scheduled and started? And the partition gereratlly should not be empty because the upstream task is finished normally.;;;","04/Jan/22 06:59;kevin.cyj;I guess for batch jobs, we should not throw ProducerFailedException. Maybe throwing the original root exception is better? An easy fix maybe moving the wrapping of ProducerFailedException from to PartitionRequestQueue to PipelinedSubpartitionView?;;;","11/Jan/22 13:32;pnowojski;I think your suggestion [~kevin.cyj] is the right thing to do. Indeed in this case the subpartition/subpartition view should be able to decide whether this is a primary or secondary failure. (secondary - i.e., upstream task is responsible for reporting the root cause). ;;;","12/Jan/22 05:45;kevin.cyj;[~wanglijie95] Would you prepare a fix for it?;;;","12/Jan/22 07:13;wanglijie;[~kevin.cyj] I would, I will prepare a fix according your suggestion.;;;","13/Jan/22 14:08;pnowojski;merged commit f957e3f into apache:master now;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apache Pulsar Connector Document description error about 'Starting Position'.,FLINK-25440,13419307,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,affe,xiechenling,xiechenling,24/Dec/21 03:59,01/Apr/22 06:43,13/Jul/23 08:12,01/Apr/22 06:43,1.14.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.5,1.15.0,,,,Documentation,,,,,0,pull-request-available,,,,"Starting Position description error.

Start from the specified message time by Message<byte[]>.getEventTime().

StartCursor.fromMessageTime(long)

it should be 'Start from the specified message time by publishTime.'",,affe,martijnvisser,xiechenling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 01 06:42:59 UTC 2022,,,,,,,,,,"0|z0y0eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/22 14:30;affe;I can take up this ticket, would someone assign this to me ? Thanks !;;;","30/Mar/22 13:47;martijnvisser;Fixed in master: b411e34a9e996f697f9bc1f3cc571032efea38bf;;;","31/Mar/22 11:40;martijnvisser;Fixed in release-1.15: f6a49a5627acb10c5c32b7d7e95b79c3a2a1cdd8;;;","01/Apr/22 06:42;martijnvisser;Fixed in release-1.14: e1de0e0181d2ac3937e048fd991fcefb24ceb098;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build wheels failed,FLINK-25437,13419297,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,24/Dec/21 02:38,24/Jan/22 13:50,13/Jul/23 08:12,24/Dec/21 11:03,1.12.8,1.13.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.6,,,,API / Python,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28552&view=logs&j=33dd8067-7758-552f-a1cf-a8b8ff0e44cd&t=bf344275-d244-5694-d05a-7ad127794669
",,dianfu,gaoyunhaii,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 24 11:03:16 UTC 2021,,,,,,,,,,"0|z0y0cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Dec/21 11:03;hxbks2ks;Merged into release-1.13 via 5afe88a648cfde306b9cc14e681c6226bf629a3f
Merged into release-1.12 bd2b763408e5011acf0935dd4cb2d6b3deff23d1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid to close output streams twice during uploading changelogs,FLINK-25429,13419195,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,23/Dec/21 11:27,31/Dec/21 08:28,13/Jul/23 08:12,24/Dec/21 06:58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"Current uploader implementation would close {{stream}} and {{fsStream}} one by one, which lead to {{fsStream}} closed twice.

{code:java}
        try (FSDataOutputStream fsStream = fileSystem.create(path, NO_OVERWRITE)) {
            fsStream.write(compression ? 1 : 0);
            try (OutputStreamWithPos stream = wrap(fsStream); ) {
                final Map<UploadTask, Map<StateChangeSet, Long>> tasksOffsets = new HashMap<>();
                for (UploadTask task : tasks) {
                    tasksOffsets.put(task, format.write(stream, task.changeSets));
                }
                FileStateHandle handle = new FileStateHandle(path, stream.getPos());
                // WARN: streams have to be closed before returning the results
                // otherwise JM may receive invalid handles
                return new LocalResult(tasksOffsets, handle);
            }
        }
{code}

Not all file system supports to close same stream twice.",,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25144,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 24 06:49:00 UTC 2021,,,,,,,,,,"0|z0xzq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Dec/21 06:49;yunta;merged in master:
804eb8dda556a2bea35c69a2662f13d1dafb9255;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SavepointITCase.testTriggerSavepointAndResumeWithNoClaim fails on AZP,FLINK-25427,13419164,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dmvk,trohrmann,trohrmann,23/Dec/21 08:58,21/Jan/22 11:07,13/Jul/23 08:12,20/Jan/22 13:07,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,"The test {{SavepointITCase.testTriggerSavepointAndResumeWithNoClaim}} fails on AZP with

{code}
2021-12-23T03:10:26.4240179Z Dec 23 03:10:26 [ERROR] org.apache.flink.test.checkpointing.SavepointITCase.testTriggerSavepointAndResumeWithNoClaim  Time elapsed: 62.289 s  <<< ERROR!
2021-12-23T03:10:26.4240998Z Dec 23 03:10:26 java.util.concurrent.TimeoutException: Condition was not met in given timeout.
2021-12-23T03:10:26.4241716Z Dec 23 03:10:26 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:166)
2021-12-23T03:10:26.4242643Z Dec 23 03:10:26 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:144)
2021-12-23T03:10:26.4243295Z Dec 23 03:10:26 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:136)
2021-12-23T03:10:26.4244433Z Dec 23 03:10:26 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForAllTaskRunning(CommonTestUtils.java:210)
2021-12-23T03:10:26.4245166Z Dec 23 03:10:26 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForAllTaskRunning(CommonTestUtils.java:184)
2021-12-23T03:10:26.4245830Z Dec 23 03:10:26 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForAllTaskRunning(CommonTestUtils.java:172)
2021-12-23T03:10:26.4246870Z Dec 23 03:10:26 	at org.apache.flink.test.checkpointing.SavepointITCase.testTriggerSavepointAndResumeWithNoClaim(SavepointITCase.java:446)
2021-12-23T03:10:26.4247813Z Dec 23 03:10:26 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-12-23T03:10:26.4248808Z Dec 23 03:10:26 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-12-23T03:10:26.4249426Z Dec 23 03:10:26 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-12-23T03:10:26.4250192Z Dec 23 03:10:26 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-12-23T03:10:26.4251196Z Dec 23 03:10:26 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2021-12-23T03:10:26.4252160Z Dec 23 03:10:26 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-12-23T03:10:26.4252888Z Dec 23 03:10:26 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2021-12-23T03:10:26.4253547Z Dec 23 03:10:26 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-12-23T03:10:26.4254142Z Dec 23 03:10:26 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-12-23T03:10:26.4254932Z Dec 23 03:10:26 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2021-12-23T03:10:26.4255513Z Dec 23 03:10:26 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2021-12-23T03:10:26.4256091Z Dec 23 03:10:26 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-12-23T03:10:26.4256636Z Dec 23 03:10:26 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2021-12-23T03:10:26.4257165Z Dec 23 03:10:26 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2021-12-23T03:10:26.4257744Z Dec 23 03:10:26 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2021-12-23T03:10:26.4258312Z Dec 23 03:10:26 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2021-12-23T03:10:26.4258884Z Dec 23 03:10:26 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2021-12-23T03:10:26.4259488Z Dec 23 03:10:26 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2021-12-23T03:10:26.4260049Z Dec 23 03:10:26 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2021-12-23T03:10:26.4260579Z Dec 23 03:10:26 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2021-12-23T03:10:26.4261108Z Dec 23 03:10:26 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2021-12-23T03:10:26.4261648Z Dec 23 03:10:26 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2021-12-23T03:10:26.4262183Z Dec 23 03:10:26 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2021-12-23T03:10:26.4262794Z Dec 23 03:10:26 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2021-12-23T03:10:26.4263312Z Dec 23 03:10:26 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2021-12-23T03:10:26.4263813Z Dec 23 03:10:26 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2021-12-23T03:10:26.4264377Z Dec 23 03:10:26 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2021-12-23T03:10:26.4264909Z Dec 23 03:10:26 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2021-12-23T03:10:26.4265529Z Dec 23 03:10:26 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2021-12-23T03:10:26.4266171Z Dec 23 03:10:26 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2021-12-23T03:10:26.4266815Z Dec 23 03:10:26 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2021-12-23T03:10:26.4267505Z Dec 23 03:10:26 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2021-12-23T03:10:26.4268212Z Dec 23 03:10:26 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2021-12-23T03:10:26.4268957Z Dec 23 03:10:26 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2021-12-23T03:10:26.4269809Z Dec 23 03:10:26 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2021-12-23T03:10:26.4270462Z Dec 23 03:10:26 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2021-12-23T03:10:26.4271133Z Dec 23 03:10:26 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2021-12-23T03:10:26.4271799Z Dec 23 03:10:26 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2021-12-23T03:10:26.4272578Z Dec 23 03:10:26 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2021-12-23T03:10:26.4273257Z Dec 23 03:10:26 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2021-12-23T03:10:26.4273953Z Dec 23 03:10:26 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2021-12-23T03:10:26.4274712Z Dec 23 03:10:26 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2021-12-23T03:10:26.4275360Z Dec 23 03:10:26 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2021-12-23T03:10:26.4275973Z Dec 23 03:10:26 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2021-12-23T03:10:26.4276552Z Dec 23 03:10:26 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2021-12-23T03:10:26.4277106Z Dec 23 03:10:26 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28502&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=9810",,akalashnikov,dmvk,dwysakowicz,gaoyunhaii,pnowojski,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25750,,,FLINK-25426,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 20 13:07:10 UTC 2022,,,,,,,,,,"0|z0xzj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Dec/21 08:58;trohrmann;cc [~pnowojski], [~dwysakowicz];;;","25/Dec/21 03:35;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28553&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=9806;;;","27/Dec/21 06:37;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28590&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=10205;;;","27/Dec/21 07:02;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28594&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=9806]

 

 ;;;","27/Dec/21 09:19;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28603&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca;;;","28/Dec/21 06:28;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28627&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=9806;;;","28/Dec/21 08:55;trohrmann;Disabled this test in 1.15.0 via fe3dfbca7ac8bada810726f1568e69fea855b039. Bumping the priority of this ticker to blocker to fix and re-enable this test before the 1.15 release.;;;","31/Dec/21 16:50;akalashnikov;I actually think that it is the same problem as https://issues.apache.org/jira/browse/FLINK-25426. It started to fail from one commit. It always fails when `UnalignedCheckpointRescaleITCase` fails. When I try to reproduce it locally I see  the same exceptions as for `UnalignedCheckpointRescaleITCase`.
Unfortunatelly, I can not be fully sure about it since the original problem  `Condition was not met in given timeout.`, doesn't contain anything in logs. This test just stucks during allocation slot(`Allocated slot for ..`) which on one hand points to the same problem as in `UnalignedCheckpointRescaleITCase`(the problem with resources managment) but on another hand I don't see explicit OOM exception there(we have problem with logging somewhere?).
Since the fix for `UnalignedCheckpointRescaleITCase` is almost merged. If nobody mind I can enable `SavepointITCase` there and we will see it helps or not.;;;","03/Jan/22 09:34;trohrmann;[~akalashnikov] if you think that the test failure is related to this change. Then let's reenable the test after FLINK-25426 has been merged and let's monitor whether this test has been stabilized by the fix.;;;","04/Jan/22 10:42;trohrmann;[~twalthr] re-enabled the test via d6f12989eda1ceb501ddd831a9c1b3098f6a1031. Unfortunately, the test is still unstable: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28883&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=10289;;;","04/Jan/22 11:22;akalashnikov;The commit(https://issues.apache.org/jira/browse/FLINK-25085) which was suspected to be the reason for this failure was reverted. But I see that this test continues to fail(https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28883&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=10289). 
Now I see that this test started to fail a little earlier than FLINK-25085 was committed so the problem is deeper than I thought. I suppose this is the first fail (https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28362&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=9806)

Sorry for disturbing but can anybody disable this test again - https://github.com/apache/flink/pull/18263;;;","04/Jan/22 12:16;trohrmann;Test has been disabled again via 7b43fd7cd0b36f203efda2a6f006096a59ec689c.;;;","10/Jan/22 21:11;trohrmann;[~akalashnikov] do you have an update for this ticket?;;;","11/Jan/22 07:37;akalashnikov;[~trohrmann], yes, I have a little progress. I successfully isolated `testTriggerSavepointAndResumeWithNoClaim` and right now, I at least have the explicit exception:
{noformat}
06:54:23,926 [flink-akka.actor.default-dispatcher-17] WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 0c6696a94a9c5efb3fa74a58f23cbb08. Free slots: 0
06:54:24,627 [flink-akka.actor.default-dispatcher-18] INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Failed to go from CreatingExecutionGraph to Executing because the ExecutionGraph cre
ation failed.
org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Not enough resources available for scheduling.
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$determineParallelism$21(AdaptiveScheduler.java:743) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.util.Optional.orElseThrow(Optional.java:290) ~[?:1.8.0_292]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.determineParallelism(AdaptiveScheduler.java:741) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.createExecutionGraphWithAvailableResourcesAsync(AdaptiveScheduler.java:915) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.goToCreatingExecutionGraph(AdaptiveScheduler.java:902) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.adaptive.WaitingForResources.createExecutionGraphWithAvailableResources(WaitingForResources.java:178) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.adaptive.WaitingForResources.resourceTimeout(WaitingForResources.java:174) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.runIfState(AdaptiveScheduler.java:1106) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$runIfState$26(AdaptiveScheduler.java:1121) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_292]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_292]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:455) ~[flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:455) ~[flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:213) ~[flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) ~[flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_0433dc93-b31b-44d6-8efc-777bd9bd7aa2.jar:1.15-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
{noformat}

It happens when we try to submit the second job to the same minicluster after the canceling the first one. As you can see `Free slots: 0`. So right now, I am trying to figure out why it happens.;;;","11/Jan/22 09:12;trohrmann;What are the TMs doing that should provide the slots?;;;","11/Jan/22 09:14;gaoyunhaii;Hi [~dmvk] could you or someone could also have a look~? Very thanks!;;;","11/Jan/22 09:23;pnowojski;{quote}
What are the TMs doing that should provide the slots?
{quote}
[~akalashnikov] has added some extra code to collect thread dumps and AFAIK we are waiting for this issue to happen again (on a test branch?) to try to answer this.;;;","11/Jan/22 10:28;dmvk;[~gaoyunhaii] I'll try to take a look at this later today, at least to provide Anton with some more context;;;","11/Jan/22 11:04;akalashnikov;Sorry, It was stupid of me but I thought for some reason that this test uses AdaptiveScheduler by default which is not true. So, unfortunately, I have tried to debug the wrong configuration locally. But in fact, if you run this test with AdaptiveScheduler, the test will fail even locally. I think it should help with debugging.
I am not an expert with StreamGraph so I indeed need to help here. But right now I see the following. The test looks like that:
{noformat}
streamEnvironment = ...
JobGraph jobGraph = env.getStreamGraph().getJobGraph();
...
jobGraph.setJobID(jobID1);
clusterClient.submitJob(jobGraph).get();
....
clusterClient.cancel(jobGraph).get();
...
jobGraph.setJobID(jobID2);
clusterClient.submitJob(jobGraph).get();
// submitting job will fail because zero slots available
{noformat}
I honestly don't know how it is legal to use one jobGraph with a different jobID(I don't see any restrictions about that). But if we change our logic in such a way it will fix the test:
{noformat}
streamEnvironment = ...
StreamGraph streamGraph = env.getStreamGraph(); // This is important
JobGraph jobGraph = streamGraph.getJobGraph();
...
jobGraph.setJobID(jobID1);
clusterClient.submitJob(jobGraph).get();
....
clusterClient.cancel(jobGraph).get();
...
jobGraph = streamGraph.getJobGraph(); // get new job graph again
jobGraph.setJobID(jobID2);
clusterClient.submitJob(jobGraph).get();
// now it will work
{noformat}
So I suppose the problem with the mutability of JobGraph. I can continue with an investigation of the certain reason or perhaps, somebody already understands what is wrong going on here.;;;","20/Jan/22 13:07;dwysakowicz;Fixed in 00856d068931a5988347b9bdbdf11bc700258fe1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnalignedCheckpointRescaleITCase.shouldRescaleUnalignedCheckpoint fails on AZP because it cannot allocate enough network buffers,FLINK-25426,13419161,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,trohrmann,trohrmann,23/Dec/21 08:53,21/Feb/22 11:45,13/Jul/23 08:12,21/Feb/22 11:45,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"The test {{UnalignedCheckpointRescaleITCase.shouldRescaleUnalignedCheckpoint}} fails with

{code}
2021-12-23T02:54:46.2862342Z Dec 23 02:54:46 [ERROR] UnalignedCheckpointRescaleITCase.shouldRescaleUnalignedCheckpoint  Time elapsed: 2.992 s  <<< ERROR!
2021-12-23T02:54:46.2865774Z Dec 23 02:54:46 java.lang.OutOfMemoryError: Could not allocate enough memory segments for NetworkBufferPool (required (Mb): 64, allocated (Mb): 14, missing (Mb): 50). Cause: Direct buffer memory. The direct out-of-memory error has occurred. This can mean two things: either job(s) require(s) a larger size of JVM direct memory or there is a direct memory leak. The direct memory can be allocated by user code or some of its dependencies. In this case 'taskmanager.memory.task.off-heap.size' configuration option should be increased. Flink framework and its dependencies also consume the direct memory, mostly for network communication. The most of network memory is managed by Flink and should not result in out-of-memory error. In certain special cases, in particular for jobs with high parallelism, the framework may require more direct memory which is not managed by Flink. In this case 'taskmanager.memory.framework.off-heap.size' configuration option should be increased. If the error persists then there is probably a direct memory leak in user code or some of its dependencies which has to be investigated and fixed. The task executor has to be shutdown...
2021-12-23T02:54:46.2868239Z Dec 23 02:54:46 	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.<init>(NetworkBufferPool.java:138)
2021-12-23T02:54:46.2868975Z Dec 23 02:54:46 	at org.apache.flink.runtime.io.network.NettyShuffleServiceFactory.createNettyShuffleEnvironment(NettyShuffleServiceFactory.java:140)
2021-12-23T02:54:46.2869771Z Dec 23 02:54:46 	at org.apache.flink.runtime.io.network.NettyShuffleServiceFactory.createNettyShuffleEnvironment(NettyShuffleServiceFactory.java:94)
2021-12-23T02:54:46.2870550Z Dec 23 02:54:46 	at org.apache.flink.runtime.io.network.NettyShuffleServiceFactory.createShuffleEnvironment(NettyShuffleServiceFactory.java:79)
2021-12-23T02:54:46.2871312Z Dec 23 02:54:46 	at org.apache.flink.runtime.io.network.NettyShuffleServiceFactory.createShuffleEnvironment(NettyShuffleServiceFactory.java:58)
2021-12-23T02:54:46.2872062Z Dec 23 02:54:46 	at org.apache.flink.runtime.taskexecutor.TaskManagerServices.createShuffleEnvironment(TaskManagerServices.java:414)
2021-12-23T02:54:46.2872767Z Dec 23 02:54:46 	at org.apache.flink.runtime.taskexecutor.TaskManagerServices.fromConfiguration(TaskManagerServices.java:282)
2021-12-23T02:54:46.2873436Z Dec 23 02:54:46 	at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.startTaskManager(TaskManagerRunner.java:523)
2021-12-23T02:54:46.2877615Z Dec 23 02:54:46 	at org.apache.flink.runtime.minicluster.MiniCluster.startTaskManager(MiniCluster.java:645)
2021-12-23T02:54:46.2878247Z Dec 23 02:54:46 	at org.apache.flink.runtime.minicluster.MiniCluster.startTaskManagers(MiniCluster.java:626)
2021-12-23T02:54:46.2878856Z Dec 23 02:54:46 	at org.apache.flink.runtime.minicluster.MiniCluster.start(MiniCluster.java:379)
2021-12-23T02:54:46.2879487Z Dec 23 02:54:46 	at org.apache.flink.runtime.testutils.MiniClusterResource.startMiniCluster(MiniClusterResource.java:209)
2021-12-23T02:54:46.2880152Z Dec 23 02:54:46 	at org.apache.flink.runtime.testutils.MiniClusterResource.before(MiniClusterResource.java:95)
2021-12-23T02:54:46.2880821Z Dec 23 02:54:46 	at org.apache.flink.test.util.MiniClusterWithClientResource.before(MiniClusterWithClientResource.java:64)
2021-12-23T02:54:46.2881519Z Dec 23 02:54:46 	at org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.execute(UnalignedCheckpointTestBase.java:151)
2021-12-23T02:54:46.2882310Z Dec 23 02:54:46 	at org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase.shouldRescaleUnalignedCheckpoint(UnalignedCheckpointRescaleITCase.java:534)
2021-12-23T02:54:46.2882978Z Dec 23 02:54:46 	at jdk.internal.reflect.GeneratedMethodAccessor123.invoke(Unknown Source)
2021-12-23T02:54:46.2883574Z Dec 23 02:54:46 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-12-23T02:54:46.2884171Z Dec 23 02:54:46 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2021-12-23T02:54:46.2884732Z Dec 23 02:54:46 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2021-12-23T02:54:46.2885527Z Dec 23 02:54:46 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-12-23T02:54:46.2886135Z Dec 23 02:54:46 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2021-12-23T02:54:46.2886755Z Dec 23 02:54:46 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-12-23T02:54:46.2887387Z Dec 23 02:54:46 	at org.junit.rules.Verifier$1.evaluate(Verifier.java:35)
2021-12-23T02:54:46.2887892Z Dec 23 02:54:46 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2021-12-23T02:54:46.2888435Z Dec 23 02:54:46 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2021-12-23T02:54:46.2889007Z Dec 23 02:54:46 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-12-23T02:54:46.2889568Z Dec 23 02:54:46 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2021-12-23T02:54:46.2890104Z Dec 23 02:54:46 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2021-12-23T02:54:46.2890686Z Dec 23 02:54:46 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2021-12-23T02:54:46.2891259Z Dec 23 02:54:46 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2021-12-23T02:54:46.2891819Z Dec 23 02:54:46 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2021-12-23T02:54:46.2892421Z Dec 23 02:54:46 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2021-12-23T02:54:46.2892978Z Dec 23 02:54:46 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2021-12-23T02:54:46.2893508Z Dec 23 02:54:46 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2021-12-23T02:54:46.2894049Z Dec 23 02:54:46 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2021-12-23T02:54:46.2894588Z Dec 23 02:54:46 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2021-12-23T02:54:46.2895203Z Dec 23 02:54:46 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2021-12-23T02:54:46.2895721Z Dec 23 02:54:46 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2021-12-23T02:54:46.2896304Z Dec 23 02:54:46 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-12-23T02:54:46.2896781Z Dec 23 02:54:46 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-12-23T02:54:46.2897359Z Dec 23 02:54:46 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2021-12-23T02:54:46.2897892Z Dec 23 02:54:46 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2021-12-23T02:54:46.2898429Z Dec 23 02:54:46 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2021-12-23T02:54:46.2898968Z Dec 23 02:54:46 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2021-12-23T02:54:46.2899487Z Dec 23 02:54:46 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2021-12-23T02:54:46.2900025Z Dec 23 02:54:46 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2021-12-23T02:54:46.2900542Z Dec 23 02:54:46 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2021-12-23T02:54:46.2901044Z Dec 23 02:54:46 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2021-12-23T02:54:46.2901540Z Dec 23 02:54:46 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2021-12-23T02:54:46.2902086Z Dec 23 02:54:46 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2021-12-23T02:54:46.2902702Z Dec 23 02:54:46 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2021-12-23T02:54:46.2903297Z Dec 23 02:54:46 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2021-12-23T02:54:46.2903944Z Dec 23 02:54:46 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2021-12-23T02:54:46.2904712Z Dec 23 02:54:46 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2021-12-23T02:54:46.2905493Z Dec 23 02:54:46 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2021-12-23T02:54:46.2906245Z Dec 23 02:54:46 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2021-12-23T02:54:46.2906968Z Dec 23 02:54:46 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2021-12-23T02:54:46.2907692Z Dec 23 02:54:46 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2021-12-23T02:54:46.2908303Z Dec 23 02:54:46 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2021-12-23T02:54:46.2908971Z Dec 23 02:54:46 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2021-12-23T02:54:46.2909664Z Dec 23 02:54:46 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2021-12-23T02:54:46.2910347Z Dec 23 02:54:46 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2021-12-23T02:54:46.2911042Z Dec 23 02:54:46 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2021-12-23T02:54:46.2911743Z Dec 23 02:54:46 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2021-12-23T02:54:46.2912399Z Dec 23 02:54:46 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2021-12-23T02:54:46.2913009Z Dec 23 02:54:46 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2021-12-23T02:54:46.2913589Z Dec 23 02:54:46 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2021-12-23T02:54:46.2914162Z Dec 23 02:54:46 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28502&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=14634

Maybe the test instability is caused by exceeding our available memory on the CI machines by running too many tests concurrently.",,akalashnikov,dwysakowicz,gaoyunhaii,guoyangze,jingzhang,pnowojski,roman,trohrmann,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18356,,,,FLINK-24180,,,,,,,FLINK-25085,,,,,,,,FLINK-25427,FLINK-25026,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 21 11:44:45 UTC 2022,,,,,,,,,,"0|z0xzig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Dec/21 08:53;trohrmann;cc [~pnowojski], [~roman];;;","23/Dec/21 08:54;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28502&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=21021;;;","23/Dec/21 09:02;trohrmann;Another instance https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28511&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=16419 with slightly different stack trace:

{code}
2021-12-23T08:25:08.3509162Z Dec 23 08:25:08 [ERROR] UnalignedCheckpointRescaleITCase.shouldRescaleUnalignedCheckpoint  Time elapsed: 33.061 s  <<< ERROR!
2021-12-23T08:25:08.3510439Z Dec 23 08:25:08 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-12-23T08:25:08.3511167Z Dec 23 08:25:08 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2021-12-23T08:25:08.3511851Z Dec 23 08:25:08 	at org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.execute(UnalignedCheckpointTestBase.java:168)
2021-12-23T08:25:08.3512817Z Dec 23 08:25:08 	at org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase.shouldRescaleUnalignedCheckpoint(UnalignedCheckpointRescaleITCase.java:543)
2021-12-23T08:25:08.3513741Z Dec 23 08:25:08 	at sun.reflect.GeneratedMethodAccessor140.invoke(Unknown Source)
2021-12-23T08:25:08.3514329Z Dec 23 08:25:08 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-12-23T08:25:08.3514897Z Dec 23 08:25:08 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-12-23T08:25:08.3515467Z Dec 23 08:25:08 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2021-12-23T08:25:08.3516102Z Dec 23 08:25:08 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-12-23T08:25:08.3516713Z Dec 23 08:25:08 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2021-12-23T08:25:08.3517325Z Dec 23 08:25:08 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-12-23T08:25:08.3521295Z Dec 23 08:25:08 	at org.junit.rules.Verifier$1.evaluate(Verifier.java:35)
2021-12-23T08:25:08.3522047Z Dec 23 08:25:08 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2021-12-23T08:25:08.3522980Z Dec 23 08:25:08 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2021-12-23T08:25:08.3523842Z Dec 23 08:25:08 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-12-23T08:25:08.3524462Z Dec 23 08:25:08 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2021-12-23T08:25:08.3525010Z Dec 23 08:25:08 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2021-12-23T08:25:08.3525605Z Dec 23 08:25:08 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2021-12-23T08:25:08.3526193Z Dec 23 08:25:08 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2021-12-23T08:25:08.3526757Z Dec 23 08:25:08 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2021-12-23T08:25:08.3527372Z Dec 23 08:25:08 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2021-12-23T08:25:08.3527940Z Dec 23 08:25:08 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2021-12-23T08:25:08.3528483Z Dec 23 08:25:08 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2021-12-23T08:25:08.3529052Z Dec 23 08:25:08 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2021-12-23T08:25:08.3529973Z Dec 23 08:25:08 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2021-12-23T08:25:08.3530814Z Dec 23 08:25:08 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2021-12-23T08:25:08.3531653Z Dec 23 08:25:08 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2021-12-23T08:25:08.3532607Z Dec 23 08:25:08 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-12-23T08:25:08.3533414Z Dec 23 08:25:08 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-12-23T08:25:08.3534229Z Dec 23 08:25:08 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2021-12-23T08:25:08.3535108Z Dec 23 08:25:08 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2021-12-23T08:25:08.3535957Z Dec 23 08:25:08 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2021-12-23T08:25:08.3536836Z Dec 23 08:25:08 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2021-12-23T08:25:08.3537387Z Dec 23 08:25:08 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2021-12-23T08:25:08.3538174Z Dec 23 08:25:08 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2021-12-23T08:25:08.3538725Z Dec 23 08:25:08 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2021-12-23T08:25:08.3539237Z Dec 23 08:25:08 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2021-12-23T08:25:08.3539851Z Dec 23 08:25:08 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2021-12-23T08:25:08.3540413Z Dec 23 08:25:08 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2021-12-23T08:25:08.3541054Z Dec 23 08:25:08 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2021-12-23T08:25:08.3541681Z Dec 23 08:25:08 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2021-12-23T08:25:08.3542428Z Dec 23 08:25:08 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2021-12-23T08:25:08.3543468Z Dec 23 08:25:08 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2021-12-23T08:25:08.3544586Z Dec 23 08:25:08 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2021-12-23T08:25:08.3545748Z Dec 23 08:25:08 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2021-12-23T08:25:08.3546948Z Dec 23 08:25:08 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2021-12-23T08:25:08.3547724Z Dec 23 08:25:08 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2021-12-23T08:25:08.3548487Z Dec 23 08:25:08 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2021-12-23T08:25:08.3549513Z Dec 23 08:25:08 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2021-12-23T08:25:08.3550631Z Dec 23 08:25:08 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2021-12-23T08:25:08.3551769Z Dec 23 08:25:08 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2021-12-23T08:25:08.3553111Z Dec 23 08:25:08 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2021-12-23T08:25:08.3554282Z Dec 23 08:25:08 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2021-12-23T08:25:08.3555307Z Dec 23 08:25:08 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2021-12-23T08:25:08.3555968Z Dec 23 08:25:08 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2021-12-23T08:25:08.3556561Z Dec 23 08:25:08 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2021-12-23T08:25:08.3557149Z Dec 23 08:25:08 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2021-12-23T08:25:08.3557963Z Dec 23 08:25:08 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=100)
2021-12-23T08:25:08.3558778Z Dec 23 08:25:08 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2021-12-23T08:25:08.3559593Z Dec 23 08:25:08 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2021-12-23T08:25:08.3560459Z Dec 23 08:25:08 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
2021-12-23T08:25:08.3561460Z Dec 23 08:25:08 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
2021-12-23T08:25:08.3562374Z Dec 23 08:25:08 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
2021-12-23T08:25:08.3563202Z Dec 23 08:25:08 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:696)
2021-12-23T08:25:08.3563862Z Dec 23 08:25:08 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2021-12-23T08:25:08.3564505Z Dec 23 08:25:08 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:445)
2021-12-23T08:25:08.3565068Z Dec 23 08:25:08 	at sun.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
2021-12-23T08:25:08.3565623Z Dec 23 08:25:08 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-12-23T08:25:08.3566195Z Dec 23 08:25:08 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-12-23T08:25:08.3566767Z Dec 23 08:25:08 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
2021-12-23T08:25:08.3567475Z Dec 23 08:25:08 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2021-12-23T08:25:08.3568165Z Dec 23 08:25:08 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
2021-12-23T08:25:08.3568807Z Dec 23 08:25:08 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2021-12-23T08:25:08.3569466Z Dec 23 08:25:08 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2021-12-23T08:25:08.3570193Z Dec 23 08:25:08 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2021-12-23T08:25:08.3570778Z Dec 23 08:25:08 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2021-12-23T08:25:08.3571314Z Dec 23 08:25:08 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2021-12-23T08:25:08.3571850Z Dec 23 08:25:08 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2021-12-23T08:25:08.3572448Z Dec 23 08:25:08 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2021-12-23T08:25:08.3572997Z Dec 23 08:25:08 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2021-12-23T08:25:08.3573568Z Dec 23 08:25:08 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-12-23T08:25:08.3574127Z Dec 23 08:25:08 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2021-12-23T08:25:08.3574685Z Dec 23 08:25:08 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2021-12-23T08:25:08.3575293Z Dec 23 08:25:08 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2021-12-23T08:25:08.3575888Z Dec 23 08:25:08 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2021-12-23T08:25:08.3576393Z Dec 23 08:25:08 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2021-12-23T08:25:08.3576932Z Dec 23 08:25:08 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2021-12-23T08:25:08.3577453Z Dec 23 08:25:08 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2021-12-23T08:25:08.3577960Z Dec 23 08:25:08 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2021-12-23T08:25:08.3578457Z Dec 23 08:25:08 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2021-12-23T08:25:08.3578936Z Dec 23 08:25:08 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2021-12-23T08:25:08.3579425Z Dec 23 08:25:08 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2021-12-23T08:25:08.3580063Z Dec 23 08:25:08 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2021-12-23T08:25:08.3580811Z Dec 23 08:25:08 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2021-12-23T08:25:08.3581510Z Dec 23 08:25:08 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2021-12-23T08:25:08.3583326Z Dec 23 08:25:08 Caused by: org.apache.flink.runtime.io.network.netty.exception.LocalTransportException: failed to allocate 4194304 byte(s) of direct memory (used: 2147483648, max: 2147483648) (connection to 'localhost/127.0.0.1:35896')
2021-12-23T08:25:08.3584444Z Dec 23 08:25:08 	at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.exceptionCaught(CreditBasedPartitionRequestClientHandler.java:177)
2021-12-23T08:25:08.3585388Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:302)
2021-12-23T08:25:08.3586520Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:281)
2021-12-23T08:25:08.3587360Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireExceptionCaught(AbstractChannelHandlerContext.java:273)
2021-12-23T08:25:08.3588181Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.exceptionCaught(ChannelInboundHandlerAdapter.java:143)
2021-12-23T08:25:08.3589147Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:302)
2021-12-23T08:25:08.3590041Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:232)
2021-12-23T08:25:08.3590840Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:216)
2021-12-23T08:25:08.3591813Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:209)
2021-12-23T08:25:08.3592737Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelActive(DefaultChannelPipeline.java:1398)
2021-12-23T08:25:08.3593553Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:230)
2021-12-23T08:25:08.3594383Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:216)
2021-12-23T08:25:08.3595165Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelActive(DefaultChannelPipeline.java:895)
2021-12-23T08:25:08.3595972Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.fulfillConnectPromise(AbstractEpollChannel.java:658)
2021-12-23T08:25:08.3596827Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.finishConnect(AbstractEpollChannel.java:691)
2021-12-23T08:25:08.3597665Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.epollOutReady(AbstractEpollChannel.java:567)
2021-12-23T08:25:08.3598604Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:470)
2021-12-23T08:25:08.3599305Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378)
2021-12-23T08:25:08.3600073Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
2021-12-23T08:25:08.3600806Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
2021-12-23T08:25:08.3601385Z Dec 23 08:25:08 	at java.lang.Thread.run(Thread.java:748)
2021-12-23T08:25:08.3602351Z Dec 23 08:25:08 Caused by: org.apache.flink.shaded.netty4.io.netty.util.internal.OutOfDirectMemoryError: failed to allocate 4194304 byte(s) of direct memory (used: 2147483648, max: 2147483648)
2021-12-23T08:25:08.3603612Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent.incrementMemoryCounter(PlatformDependent.java:802)
2021-12-23T08:25:08.3604893Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent.allocateDirectNoCleaner(PlatformDependent.java:731)
2021-12-23T08:25:08.3605788Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena$DirectArena.allocateDirect(PoolArena.java:632)
2021-12-23T08:25:08.3606577Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena$DirectArena.newChunk(PoolArena.java:607)
2021-12-23T08:25:08.3607244Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena.allocateNormal(PoolArena.java:202)
2021-12-23T08:25:08.3607924Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena.tcacheAllocateSmall(PoolArena.java:172)
2021-12-23T08:25:08.3608582Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena.allocate(PoolArena.java:134)
2021-12-23T08:25:08.3609210Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena.allocate(PoolArena.java:126)
2021-12-23T08:25:08.3610000Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:395)
2021-12-23T08:25:08.3610780Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:187)
2021-12-23T08:25:08.3611537Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:178)
2021-12-23T08:25:08.3612384Z Dec 23 08:25:08 	at org.apache.flink.runtime.io.network.netty.BufferResponseDecoder.onChannelActive(BufferResponseDecoder.java:54)
2021-12-23T08:25:08.3613156Z Dec 23 08:25:08 	at org.apache.flink.runtime.io.network.netty.NettyMessageClientDecoderDelegate.channelActive(NettyMessageClientDecoderDelegate.java:74)
2021-12-23T08:25:08.3613974Z Dec 23 08:25:08 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:230)
2021-12-23T08:25:08.3614556Z Dec 23 08:25:08 	... 14 more
{code};;;","23/Dec/21 10:47;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28518&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=10531;;;","23/Dec/21 10:48;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28518&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","23/Dec/21 11:26;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28520&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=10531;;;","23/Dec/21 11:26;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28520&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","23/Dec/21 11:42;trohrmann;Given the frequency of this test failure I will raise the priority to blocker.;;;","24/Dec/21 10:05;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28567&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=10574;;;","24/Dec/21 10:05;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28567&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=16979;;;","25/Dec/21 03:33;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28553&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=14634
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28553&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=21020;;;","25/Dec/21 03:37;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28566&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=16536;;;","27/Dec/21 06:37;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28590&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=14656]

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28590&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=21419;;;","27/Dec/21 07:02;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28594&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=17118]

 ;;;","27/Dec/21 09:18;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28603&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a;;;","28/Dec/21 06:16;yunta;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28607&view=logs&j=38d6b56a-d502-56fb-7b73-c09f8fe7becd&t=6e6509fa-8a5d-5a6c-e17e-64f5ecc17842;;;","28/Dec/21 06:27;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28627&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=10132]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28627&view=logs&j=119bbba7-f5e3-5e08-e72d-09f1529665de&t=7166e71c-cad6-5ec9-ae14-15891ce68128&l=10121]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28627&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=14656]

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28627&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=21436;;;","28/Dec/21 08:18;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28629&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=10531;;;","28/Dec/21 08:18;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28629&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=16537;;;","28/Dec/21 08:52;trohrmann;Disabled test on 1.15.0 via 276570e223a9b4ddb3939101b0fe3ff995ac46c5. Bumping this ticket to blocker because we need to fix and re-enable this test before the release.;;;","28/Dec/21 09:58;yunta;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28634&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","29/Dec/21 06:45;jingzhang;Another instance: 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28641&view=logs&j=38d6b56a-d502-56fb-7b73-c09f8fe7becd&t=6e6509fa-8a5d-5a6c-e17e-64f5ecc17842;;;","29/Dec/21 20:50;akalashnikov;What exactly happens:
* Inside `org.apache.flink.shaded.netty4.io.netty.util.internal.PlatformDependent#incrementMemoryCounter` we increment `DIRECT_MEMORY_COUNTER` on each allocation of the direct buffer.  `DIRECT_MEMORY_COUNTER` is a static field.
* We decrement `DIRECT_MEMORY_COUNTER` in `org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena#finalize`. This` means decrementing depends on GC(because of `finalize`). So if GC collects all PoolArena objects correctly between the parametrized tests we don't have any problem but if it doesn't collect we have problems.

The reason why it started to fail recently is the ticket - https://issues.apache.org/jira/browse/FLINK-25085.
Unfortunately, this ticket has a bug with closing the thread pool. So because threads are alive the GC doesn't collect PoolArena objects and we only increment static `DIRECT_MEMORY_COUNTER` until it reaches its maximum and then we fail with OOM.
I have fixed the bug but I don't fully agree with my fix so let's discuss it in PR(https://github.com/apache/flink/pull/18239). [~zjureel], [~guoyangze], can you please take a look at my PR and help with the right solution?;;;","30/Dec/21 02:19;guoyangze;[~akalashnikov] I'll take a look.;;;","04/Jan/22 11:25;akalashnikov;I close this ticket since the commit which introduced this problem was reverted(see https://issues.apache.org/jira/browse/FLINK-18356) and I don't see new failures after it. ;;;","02/Feb/22 11:52;trohrmann;Unfortunately, the test failed again :-( https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30598&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=6580;;;","07/Feb/22 07:49;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30817&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=6602;;;","21/Feb/22 11:44;dwysakowicz;Closing as the reason for frequent failures has been fixed. The occasional failures should be tracked in FLINK-25026.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Azure pipelines are failing due to Python tests unable to install dependencies,FLINK-25422,13419079,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hxbks2ks,martijnvisser,martijnvisser,22/Dec/21 19:38,24/Jan/22 13:50,13/Jul/23 08:12,23/Dec/21 06:38,1.12.8,1.13.6,1.14.3,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.6,1.14.3,1.15.0,,API / Python,,,,,0,pull-request-available,test-stability,,,"{code:java}
Dec 22 16:10:02 Command ""/__w/1/s/flink-python/.tox/py38/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-install-zwy7_7or/numpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-record-emfnsngu/install-record.txt --single-version-externally-managed --compile --install-headers /__w/1/s/flink-python/.tox/py38/include/site/python3.8/numpy"" failed with error code 1 in /tmp/pip-install-zwy7_7or/numpy/
Dec 22 16:10:02 You are using pip version 10.0.1, however version 21.3.1 is available.
Dec 22 16:10:02 You should consider upgrading via the 'pip install --upgrade pip' command.
Dec 22 16:10:02 
Dec 22 16:10:02 =================================== log end ====================================
Dec 22 16:10:02 ERROR: could not install deps [pytest, apache-beam==2.27.0, cython==0.29.16, grpcio>=1.17.0,<=1.26.0, grpcio-tools>=1.3.5,<=1.14.2, apache-flink-libraries]; v = InvocationError(""/__w/1/s/flink-python/dev/install_command.sh pytest apache-beam==2.27.0 cython==0.29.16 'grpcio>=1.17.0,<=1.26.0' 'grpcio-tools>=1.3.5,<=1.14.2' apache-flink-libraries"", 1)
Dec 22 16:10:02 ___________________________________ summary ____________________________________
Dec 22 16:10:02 ERROR:   py38: could not install deps [pytest, apache-beam==2.27.0, cython==0.29.16, grpcio>=1.17.0,<=1.26.0, grpcio-tools>=1.3.5,<=1.14.2, apache-flink-libraries]; v = InvocationError(""/__w/1/s/flink-python/dev/install_command.sh pytest apache-beam==2.27.0 cython==0.29.16 'grpcio>=1.17.0,<=1.26.0' 'grpcio-tools>=1.3.5,<=1.14.2' apache-flink-libraries"", 1)
Dec 22 16:10:02 ============tox checks... [FAILED]============
Dec 22 16:10:02 Process exited with EXIT CODE: 1.
Dec 22 16:10:02 Trying to KILL watchdog (1195).
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28443&view=logs&j=161eb7af-7e37-5bda-031e-1dd139988f4b&t=1dd6a048-0e04-5036-8cea-768313805a09

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28444&view=logs&j=161eb7af-7e37-5bda-031e-1dd139988f4b&t=e489b367-f966-5d50-f73c-2caaa8549a1f

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28454&view=logs&j=dd7e7115-b4b1-5414-20ec-97b9411e0cfc&t=c759a57f-2774-59e9-f882-8e4d5d3fbb9f",,dianfu,hxbks2ks,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 23 06:38:34 UTC 2021,,,,,,,,,,"0|z0xz0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/21 19:40;martijnvisser;[~dianfu] [~hxbks2ks] can you help out on this? ;;;","23/Dec/21 06:38;hxbks2ks;Merged into master via 8a3d033bdf12b9894c81aa3073f84c238d8a8f87
Merged into release-1.14 via ae4856a3b4fad75cf58dfdb070add040f3e5eeb5
Merged into release-1.13 via 365715ddcf9214e71b5c6f52c1b73793c6baa443
Merged into release-1.12 via 97513b247f98f559a5028d2b22bd43f7ca25f853;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The dir_cache is specified in the flink task. When there is no network, you will still download the python third-party library",FLINK-25418,13418996,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ana4,yang-y,yang-y,22/Dec/21 11:37,31/Dec/21 06:09,13/Jul/23 08:12,31/Dec/21 06:09,1.13.0,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,API / Python,,,,,1,pull-request-available,,,,"Specified in Python code set_python_requirements(requirements_cache_dir=dir_cache)

During task execution, priority will be given to downloading Python third-party packages from the network，Can I directly use the python package in the cache I specify when I specify the cache value and don't want the task task to download the python package from the network","python3.7

flink1.13.1",ana4,dianfu,martijnvisser,yang-y,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 31 06:09:30 UTC 2021,,,,,,,,,,"0|z0xyi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/21 11:48;martijnvisser;[~yang-y] Please change the contents of this ticket to English;;;","22/Dec/21 12:03;ana4;cc [~dianfu] I want to fix this.  I will add the '--no-index' option in PythonEnvironmentManagerUtils#pipInstallRequirements method, when the requirementsCacheDir is not null.

When we set '--no-index', it will only install with offline, never download from the internet.

When we do not set '--no-index', it will first download from the internet, and then install with offline.;;;","23/Dec/21 03:41;dianfu;[~ana4] Thanks for taking care of this issue. Have assigned the issue to you. The solution makes sense to me and looking forward to the PR~;;;","31/Dec/21 06:09;dianfu;Merged to
- master via bf83614dfe16edc9a43f83cdf60416a60b44faa5
- release-1.14 via 19c365509c858010f320edd66aa09a6b4815485c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Network stack deadlock when cancellation happens during initialisation,FLINK-25407,13418857,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,kevin.cyj,pnowojski,pnowojski,21/Dec/21 15:45,13/Jan/22 14:12,13/Jul/23 08:12,13/Jan/22 14:12,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.4,1.15.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,"This issue was extracted from and initially reported in FLINK-25185. It is most likely caused by FLINK-24035.

{noformat}
Java stack information for the threads listed above:
===================================================
""Canceler for Source: Custom Source -> Filter (7/12)#14176 (0fbb8a89616ca7a40e473adad51f236f)."":
   at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.destroyBufferPool(NetworkBufferPool.java:420)
   - waiting to lock <0x0000000082937f28> (a java.lang.Object)
   at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.lazyDestroy(LocalBufferPool.java:567)
   at org.apache.flink.runtime.io.network.partition.ResultPartition.closeBufferPool(ResultPartition.java:264)
   at org.apache.flink.runtime.io.network.partition.ResultPartition.fail(ResultPartition.java:276)
   at org.apache.flink.runtime.taskmanager.Task.failAllResultPartitions(Task.java:999)
   at org.apache.flink.runtime.taskmanager.Task.access$100(Task.java:138)
   at org.apache.flink.runtime.taskmanager.Task$TaskCanceler.run(Task.java:1669)
   at java.lang.Thread.run(Thread.java:748)
""Canceler for Map -> Map (6/12)#14176 (6195862d199aa4d52c12f25b39904725)."":
   at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.setNumBuffers(LocalBufferPool.java:585)
   - waiting to lock <0x0000000097108898> (a java.util.ArrayDeque)
   at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.redistributeBuffers(NetworkBufferPool.java:544)
   at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.destroyBufferPool(NetworkBufferPool.java:424)
   - locked <0x0000000082937f28> (a java.lang.Object)
   at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.lazyDestroy(LocalBufferPool.java:567)
   at org.apache.flink.runtime.io.network.partition.ResultPartition.closeBufferPool(ResultPartition.java:264)
   at org.apache.flink.runtime.io.network.partition.ResultPartition.fail(ResultPartition.java:276)
   at org.apache.flink.runtime.taskmanager.Task.failAllResultPartitions(Task.java:999)
   at org.apache.flink.runtime.taskmanager.Task.access$100(Task.java:138)
   at org.apache.flink.runtime.taskmanager.Task$TaskCanceler.run(Task.java:1669)
   at java.lang.Thread.run(Thread.java:748)
""Map -> Sink: Unnamed (7/12)#14176"":
   at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.recycleMemorySegments(NetworkBufferPool.java:256)
   - waiting to lock <0x0000000082937f28> (a java.lang.Object)
   at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.internalRequestMemorySegments(NetworkBufferPool.ja
   at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.requestMemorySegmentsBlocking(NetworkBufferPool.ja
   at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.reserveSegments(LocalBufferPool.java:247)
   - locked <0x0000000097108898> (a java.util.ArrayDeque)
   at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.setupChannels(SingleInputGate.java:497)
   at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.setup(SingleInputGate.java:276)
   at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.setup(InputGateWithMetrics.java:105)
   at org.apache.flink.runtime.taskmanager.Task.setupPartitionsAndGates(Task.java:965)
   at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:652)
   at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
   at java.lang.Thread.run(Thread.java:748)

Found 1 deadlock.
{noformat}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28297&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=19003
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28306&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=19832

CC [~kevin.cyj]",,kevin.cyj,pnowojski,Thesharing,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24035,,,,FLINK-25185,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 13 14:12:33 UTC 2022,,,,,,,,,,"0|z0xxn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/22 14:12;pnowojski;Thanks [~kevin.cyj] for the fix!
Merged to master as 1ea2a7a5c90^ and 1ea2a7a5c90
Merged to release-1.14 as 7784ec7284b^ and 7784ec7284b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileNotFoundException during recovery caused by Incremental shared state being discarded by TM,FLINK-25395,13418691,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,roman,roman,20/Dec/21 20:16,19/Jan/22 09:08,13/Jul/23 08:12,17/Jan/22 10:50,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / Checkpointing,Runtime / State Backends,,,,0,pull-request-available,,,,"Extracting from FLINK-25185 discussion

On checkpoint abortion or any failure in AsyncCheckpointRunnable,
it discards the state, in particular shared (incremental) state.

Since FLINK-24611, this creates a problem because shared state can be re-used for future checkpoints.

 

A similar case is in PeriodicMaterializationManager (uploaded SST files will be deleted on failure without notifying the wrapped RocksDB state backend).

 

Symptom of this failure is a following exception during recovery:
{noformat}
Caused by: java.io.FileNotFoundException: /tmp/junit3146957979516280339/junit1602669867129285236/d6a6dbdd-3fd7-4786-9dc1-9ccc161740da (No such file or directory)
        at java.io.FileInputStream.open0(Native Method) ~[?:1.8.0_292]
        at java.io.FileInputStream.open(FileInputStream.java:195) ~[?:1.8.0_292]
        at java.io.FileInputStream.<init>(FileInputStream.java:138) ~[?:1.8.0_292]
        at org.apache.flink.core.fs.local.LocalDataInputStream.<init>(LocalDataInputStream.java:50) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.core.fs.local.LocalFileSystem.open(LocalFileSystem.java:134) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.open(SafetyNetWrapperFileSystem.java:87) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.state.filesystem.FileStateHandle.openInputStream(FileStateHandle.java:68) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.changelog.fs.StateChangeFormat.read(StateChangeFormat.java:92) ~[flink-dstl-dfs-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:85) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
{noformat}",,Feifan Wang,pnowojski,roman,ym,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25399,,,,,,,FLINK-24611,FLINK-24163,,,FLINK-25185,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 17 10:50:22 UTC 2022,,,,,,,,,,"0|z0xwmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jan/22 11:35;roman;In the linked PR, I've added a test to reproduce the issue: [https://github.com/apache/flink/pull/18297]

cc: [~pnowojski] ;;;","17/Jan/22 10:50;roman;Merged into master as:

4691b66545010ed812624a259869c7a522663720 (rocksDB - revert to re-upload again)

e28f4e2c5d4d54f5f727b9557024c537becfd054 (don't discard state);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Log4j to 2.17.0,FLINK-25375,13418451,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,knaufk,Sergey Nuyanzin,Sergey Nuyanzin,19/Dec/21 15:35,26/Dec/21 07:45,13/Jul/23 08:12,26/Dec/21 07:44,1.11.6,1.12.7,1.13.5,1.14.2,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.6,1.14.3,1.15.0,,,,,,,2,pull-request-available,,,,"Log4j 2.17.0 has been released [1] 

This release contains the changes noted below:

    Address CVE-2021-45105.
    Require components that use JNDI to be enabled individually via system properties.
    Remove LDAP and LDAPS as supported protocols from JNDI.

[1] https://github.com/apache/logging-log4j2/blob/6b1581901ba7a107cdc4a2208ecec03655722b44/RELEASE-NOTES.md#apache-log4j-2170-release-notes",,ana4,apaC1212,knaufk,koalalam,leonard,liyu,martijnvisser,PedroMrChaves,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Dec 26 07:44:10 UTC 2021,,,,,,,,,,"0|z0xv54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/21 15:54;Sergey Nuyanzin;After creation of the ticket I've noticed a PR https://github.com/apache/flink/pull/18149;;;","20/Dec/21 07:49;martijnvisser;Thanks for noticing that [~Sergey Nuyanzin];;;","21/Dec/21 16:32;knaufk;master: fef7f46c476875bd2974e5303f7ae0fb9bf2faf4

Waiting for a CI for the backports to 1.14/1.13/1.12. The community currently supports Flink 1.14 and Flink 1.13 with bug fixes. I included Flink 1.12 as well, because these Log4J vulnerabilities have caused such high waves, but not Flink 1.11 because it is just a HIGH vulnerability and we have to stop somewhere. 
;;;","23/Dec/21 10:06;apaC1212;Hi [~knaufk] 

Are you planning to release update for version 1.11?;;;","23/Dec/21 10:19;martijnvisser;[~apaC1212] That's not planned at the moment. The Flink community normally supports the last 3 versions, so that's 1.14, 1.13 and 1.12 right now. Because of the severity of previous CVEs, we also released additional hotfixes for older versions. ;;;","26/Dec/21 07:44;knaufk;release-1.14: 49971b8a527b81b5e00169d5b187e21a274184e2
release-1.13: d903739d6cf95bcac4be46eedafe016d6dd02446
release-1.12: 1561f0f908a96b8efa83e79d44cfe579cb7d29ef;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CatalogManager provide wrong tables or views in given catalog database,FLINK-25369,13418341,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liyubin117,liyubin117,liyubin117,18/Dec/21 05:22,10/Jan/22 14:48,13/Jul/23 08:12,10/Jan/22 14:47,1.14.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Table SQL / API,,,,,0,pull-request-available,,,,"table list like this:
{code:java}
//catalog c1, database d1
t1
//current catalog c2, current database d2
t2{code}
CatalogManager.listTables(c1, d1) should return t1, in fact return t2

the result combine the tables in current database and the temporay tables in given database, it's probably meaningless and uncorrect",,airblader,liyubin117,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 10 14:47:35 UTC 2022,,,,,,,,,,"0|z0xugo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/22 03:27;liyubin117;Hi, [~jark] , [~lzljs3620320]  , [~yunta] , Looking forward to hearing your opinions :D;;;","10/Jan/22 14:47;airblader;Fixed in master

commit 0b0a76a460c8194b705efbfbbaced97b29a7814a
[table] Provide tables of specified catalog/database;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enforce BINARY/VARBINARY precision when outputing to a Sink,FLINK-25366,13418250,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,matriv,matriv,matriv,17/Dec/21 13:51,20/Dec/21 16:24,13/Jul/23 08:12,20/Dec/21 16:24,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Table SQL / Planner,Table SQL / Runtime,,,,0,pull-request-available,,,,"When a column is declared with *BINARY/VARBINARY* with a specific length, i.e. *BINARY(10)* a sink that would require following this precision strictly (like a relational DB) would throw errors if the records received exceed this limit.

 ",,matriv,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25187,FLINK-24419,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 20 16:24:40 UTC 2021,,,,,,,,,,"0|z0xtwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/21 16:24;twalthr;Fixed in master: 19bc18100802e8e5a56c5ce08e985d589db81838;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect dependencies in Table Confluent/Avro docs,FLINK-25362,13418200,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nkruber,nkruber,nkruber,17/Dec/21 09:15,11/Jan/22 18:58,13/Jul/23 08:12,11/Jan/22 08:13,1.12.7,1.13.5,1.14.2,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.4,1.15.0,,,Documentation,,,,,0,pull-request-available,,,,"""Confluent Avro Format"" is missing an explanation to also
 * add the dependency to flink-avro
 * have the confluent repository defined

""Avro Format"" should not show the maven dependency to {{flink-sql-avro}} but instead {{flink-avro}}",,nkruber,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 11 08:13:09 UTC 2022,,,,,,,,,,"0|z0xtlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/22 08:13;nkruber;Fixed in
 * master: e9dba5e81f35..7bbf368bdfbf
 * release-1.14: d8b6f896c424..7b24c90c125d
 * release-1.13: 327113d26e80..e3d06b8807fd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup temporary directories from JM container before tearing down FlinkContainers,FLINK-25340,13417900,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,renqs,renqs,16/Dec/21 07:59,04/Jan/22 15:17,13/Jul/23 08:12,04/Jan/22 15:17,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Test Infrastructure,,,,,0,pull-request-available,,,,"Currently FlinkContainers will try to delete temporary directories from JVM after tearing down JM and TM containers, but this approach has file permission issues on AZP. Instead we remove these temporary directories directly inside the container of JobManager before we stop it.",,fpaul,martijnvisser,renqs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25263,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 04 15:17:28 UTC 2022,,,,,,,,,,"0|z0xrqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/21 08:11;martijnvisser;Thanks for this [~renqs] !

I've raised the priority to a Blocker since it's currently blocking all tests that rely on testcontainers. ;;;","04/Jan/22 15:17;fpaul;Merged in master: fc44a6dee8338c4e4af1e3cecf4512c8bf11ae67;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Harden UID management when constructing statefun universe,FLINK-25333,13417774,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sjwiesman,sjwiesman,sjwiesman,15/Dec/21 16:45,15/Dec/21 21:13,13/Jul/23 08:12,15/Dec/21 21:13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,statefun-3.2.0,,,,,Stateful Functions,,,,,1,pull-request-available,,,,"As described in this stack overflow, we do not set UIDs on routers. Because of how the stream graph is generated, the uids may be non-deterministic. We should manually set all UIDs and enforce this via configuration. 

 

https://stackoverflow.com/questions/70316498/flink-statefun-high-availability-exception-java-lang-illegalstateexception-th",,keremulutas,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25267,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 15 21:12:58 UTC 2021,,,,,,,,,,"0|z0xqyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/21 17:37;keremulutas;For tracking - FLINK-25267;;;","15/Dec/21 20:33;keremulutas;[~sjwiesman] I tested changes in your pull request and my application could recover successfully.

I've tried killing the jobmanager pod, killing the elected jobmanager pod when there is 1 leader and 1 standby jobmanager, deleting the whole application from minikube and re-deploying - the application could always recover from last checkpoint.

One thing to note is, the ""Generated hash ..."" log lines mentioned in FLINK-25267 are missing now - I believe this is the direct result of your changes.;;;","15/Dec/21 21:11;sjwiesman;[~keremulutas] yes, that log line was actually the clue to me of what was happening. You should never see that with a statefun application. 

 

 ;;;","15/Dec/21 21:12;sjwiesman;resolved in flink-statefun-docker: 7f8a9b29aee841a252308436e2fc2524702687bb

flink-statefun: 99892751a60d31991161a097923fbd5ee08cb2eb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaUtil.createKafkaContainer log levels are not set correctly,FLINK-25326,13417683,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,alexanderpreuss,alexanderpreuss,alexanderpreuss,15/Dec/21 10:07,16/Dec/21 14:04,13/Jul/23 08:12,16/Dec/21 14:04,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Connectors / Kafka,Test Infrastructure,,,,0,pull-request-available,pull-requests-available,,,The internal kafka log levels set in KafkaUtils.createKafkaContainer method are wrong due to the order of the log hierarchy. If the test logger is set to e.g. 'DEBUG' it means that `logger.isErrorEnabled()` already evaluated to true and therefore the log level gets set to ERROR instead.,,alexanderpreuss,fpaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 16 14:03:54 UTC 2021,,,,,,,,,,"0|z0xqeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/21 14:32;fpaul;Merged in master: 28eb197c5b7a07b9615eef1ce58459b8b5d5b9b2;;;","16/Dec/21 14:03;fpaul;Merged in release-1.14: 311fa53631bcc14c1f279ad123664e4b10c88fc7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Resuming Savepoint (hashmap, async, no parallelism change) end-to-end test timeout on azure",FLINK-25307,13417476,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gaoyunhaii,gaoyunhaii,gaoyunhaii,14/Dec/21 15:20,05/Aug/22 03:43,13/Jul/23 08:12,01/Feb/22 05:42,1.13.3,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Build System / Azure Pipelines,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,"
{code:java}
/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/common.sh: line 860: kill: (93166) - No such process
Dec 14 10:30:13 Stopping job timeout watchdog (with pid=93166)
Dec 14 10:30:13 [FAIL] Test script contains errors.
Dec 14 10:30:13 Checking for errors...
Dec 14 10:30:14 No errors in log files.
Dec 14 10:30:14 Checking for exceptions...
Dec 14 10:30:14 No exceptions in log files.
Dec 14 10:30:14 Checking for non-empty .out files...
Dec 14 10:30:14 No non-empty .out files.
Dec 14 10:30:14 
Dec 14 10:30:14 [FAIL] 'Resuming Savepoint (hashmap, async, no parallelism change) end-to-end test' failed after 15 minutes and 0 seconds! Test exited with exit code 1
Dec 14 10:30:14 
10:30:14 ##[group]Environment Information
Dec 14 10:30:15 Searching for .dump, .dumpstream and related files in '/home/vsts/work/1/s'
dmesg: read kernel buffer failed: Operation not permitted
Dec 14 10:30:16 Stopping taskexecutor daemon (pid: 93751) on host fv-az43-70.
Dec 14 10:30:17 Stopping standalonesession daemon (pid: 93500) on host fv-az43-70.
The STDIO streams did not close within 10 seconds of the exit event from process '/usr/bin/bash'. This may indicate a child process inherited the STDIO streams and has not yet exited.
##[error]Bash exited with code '1'.
Finishing: Run e2e tests

{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28088&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=79112",,dmvk,dwysakowicz,gaoyunhaii,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25901,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 01 05:41:49 UTC 2022,,,,,,,,,,"0|z0xp4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/21 15:20;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28096&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=79240;;;","15/Dec/21 07:12;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28112&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=79192;;;","15/Dec/21 07:53;dwysakowicz;Looking at logs it seems that we do not even reach a point when the Dispatcher REST endpoint is up, from the point of view of the client. The logs show:
{code}
Dec 14 15:38:55 Waiting for Dispatcher REST endpoint to come up...
Dec 14 15:41:06 Waiting for Dispatcher REST endpoint to come up...
Dec 14 15:43:17 Waiting for Dispatcher REST endpoint to come up...
Dec 14 15:45:28 Waiting for Dispatcher REST endpoint to come up...
Dec 14 15:47:39 Waiting for Dispatcher REST endpoint to come up...
Dec 14 15:49:50 Waiting for Dispatcher REST endpoint to come up...
Dec 14 15:51:41 Test (pid: 93533) did not finish after 900 seconds.
Dec 14 15:51:41 Printing Flink logs and killing it:
{code}

What is strange is the frequency with which we're querying the Dispatcher. We query it every ~2minutes, whereas as far as I can tell looking at the code we should query it every ~30seconds. Could it be some caused by a load on our machines?;;;","15/Dec/21 08:38;gaoyunhaii;Hi [~dwysakowicz] perhaps the client is blocked since the rest server does not response ? Since the master seems started normally, but the client does not fetch the expected result, thus perhaps the rest server does not response normally, but I also have no idea about more detailed causes.;;;","30/Dec/21 10:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","30/Dec/21 15:55;dmvk;I don't see any hints in the logs either. In general it seems that we had a higher failure rate on e2e tests in this period. If this happens again, we can probably make the curl call more verbose in case of failure.;;;","07/Jan/22 08:24;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29068&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=7f606211-1454-543c-70ab-c7a028a1ce8c&l=79233;;;","07/Jan/22 08:26;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29067&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=79195;;;","07/Jan/22 08:26;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29067&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=2a8cc459-df7a-5e6f-12bf-96efcc369aa9&l=79191;;;","07/Jan/22 08:26;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29067&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=cc5499f8-bdde-5157-0d76-b6528ecd808e&l=79150;;;","07/Jan/22 08:28;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29069&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=79191;;;","07/Jan/22 08:28;trohrmann;It looks to me that we broke something wrt savepoints/recovery. Making this a blocker.;;;","07/Jan/22 08:28;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29069&view=logs&j=af885ea8-6b05-5dc2-4a37-eab9c0d1ab09&t=f779a55a-0ffe-5bbc-8824-3a79333d4559&l=79103;;;","07/Jan/22 08:32;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29075&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=79150;;;","07/Jan/22 08:46;gaoyunhaii;It seems the cases are also blocked on querying whether a cluster is started. I'll first open a PR to add verbose output for the curl operation used for check. ;;;","07/Jan/22 12:59;dmvk;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29094&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14;;;","08/Jan/22 12:26;gaoyunhaii;The PR is added on

master: 8b62a6267569bb417db21b75d5e6482341fea665
release-1.14: ae8a4b78814fbc09bb1630d616a8737d909d738e
release-1.13:  86a6e2b1136b508db7250b46272fe2f9c27f1b3c;;;","12/Jan/22 14:15;gaoyunhaii;Compared the successful log with the failed log:
Successful one: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29235&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=133]

Failed one: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29231&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=687]

 
 # In the successful one the curl returns error code 7 before JM started, which means the host is not reachable and is as expected, while in the failed case, the curl failed after ~ 2mins with error code 28, which means operation timed out. In fact the JM has started at that time.
 # More important for the successful case the curl are always trying to connect to the same ip address, while in the failed case, the ip address is always changing.

Thus it looks to me perhaps the problem is with DNS resolve? Perhaps we could try to use `localhost` or `127.0.0.1` instead of hostname in this case. I'll have a try. ;;;","12/Jan/22 14:18;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29230&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=978]

 ;;;","12/Jan/22 14:30;dmvk;I think using loopback only for these tests is a good idea in general and it should make the e2e tests more robust / maybe slightly faster.

That the host IP keeps changing indeed sounds weird and might signal some infrastructure issues we might want to look into as well.

(we're already planning on not binding to all interfaces by default in https://issues.apache.org/jira/browse/FLINK-24474);;;","13/Jan/22 09:43;gaoyunhaii;Very thanks [~dmvk] for the information! I'm a bit wondering if there are machines with the same name in the environment, I'll have a look if it is possible to add some more debug logs~;;;","24/Jan/22 08:52;trohrmann;Any progress on this issue [~gaoyunhaii]?;;;","24/Jan/22 09:28;gaoyunhaii;Very sorry for the long delay... I'll open the PR today~;;;","31/Jan/22 13:09;trohrmann;Any updates here [~gaoyunhaii]?;;;","31/Jan/22 13:17;gaoyunhaii;Hi [~trohrmann]~ the pr to change the host to localhost is ready and all tests are passed. But I'm not fully got the reason why the same hostname are resolved to different ips, I'm debugging the ci pipeline with another PR which adds some more debugging info.;;;","31/Jan/22 13:42;trohrmann;Thanks a lot for the update. I hope that you'll find the root cause soon.;;;","01/Feb/22 05:41;gaoyunhaii;Hi all~ The fix of changing hostname to localhost is merged in dea2b10502a493e9d4137e7d94d2dac85d9fa666. Since before the fix the issue has not reproduced for a long time, it might requires a long time to find the root cause, thus I'll first close this issue and created a new issue https://issues.apache.org/jira/browse/FLINK-25901 to continue investigating the hostname resoluation problem on the azure pipeline. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect cloudpickle import,FLINK-25294,13417304,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,siavash119,siavash119,siavash119,14/Dec/21 07:27,31/Dec/21 06:25,13/Jul/23 08:12,31/Dec/21 06:25,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,API / Python,,,,,0,pull-request-available,,,,"In flink-python/pyflink/fn_execution/coder_impl_fast.pyx line 30
{code:python}
from cloudpickle import cloudpickle
{code}
should simply be
{code:python}
import cloudpickle{code}
or else I get AttributeError: module 'cloudpickle.cloudpickle' has no attribute 'dumps' when using keyed states

I assume this is left over from when cloudpickle was incorrectly packaged, then fixed in [FLINK-14556|https://issues.apache.org/jira/browse/FLINK-14556], so this might have been a problem since 1.10.0",,dianfu,siavash119,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 31 06:25:00 UTC 2021,,,,,,,,,,"0|z0xo2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Dec/21 06:25;dianfu;Merged to
- master via 36d341691fdfdae0c8df7acfd2cc9269cb5f2b59
- release-1.14 via 6525971b4c09f6ac6e737f1fe2c646cd05810f06;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Azure failed due to unable to fetch some archives,FLINK-25292,13417290,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,gaoyunhaii,gaoyunhaii,14/Dec/21 06:35,28/Apr/23 14:13,13/Jul/23 08:12,28/Apr/23 14:13,1.13.3,1.14.0,1.15.0,1.16.2,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Build System / Azure Pipelines,,,,,0,test-stability,,,,"{code:java}
/bin/bash --noprofile --norc /__w/_temp/ba0f8961-8595-4ace-b13f-d60e17df8803.sh
Reading package lists...
Building dependency tree...
Reading state information...
The following additional packages will be installed:
  libio-pty-perl libipc-run-perl
Suggested packages:
  libtime-duration-perl libtimedate-perl
The following NEW packages will be installed:
  libio-pty-perl libipc-run-perl moreutils
0 upgraded, 3 newly installed, 0 to remove and 0 not upgraded.
Need to get 177 kB of archives.
After this operation, 573 kB of additional disk space will be used.
Err:1 http://archive.ubuntu.com/ubuntu xenial/main amd64 libio-pty-perl amd64 1:1.08-1.1build1
  Could not connect to archive.ubuntu.com:80 (91.189.88.152), connection timed out [IP: 91.189.88.152 80]
Err:2 http://archive.ubuntu.com/ubuntu xenial/main amd64 libipc-run-perl all 0.94-1
  Unable to connect to archive.ubuntu.com:http: [IP: 91.189.88.152 80]
Err:3 http://archive.ubuntu.com/ubuntu xenial/universe amd64 moreutils amd64 0.57-1
  Unable to connect to archive.ubuntu.com:http: [IP: 91.189.88.152 80]
E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/libi/libio-pty-perl/libio-pty-perl_1.08-1.1build1_amd64.deb  Could not connect to archive.ubuntu.com:80 (91.189.88.152), connection timed out [IP: 91.189.88.152 80]

E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/libi/libipc-run-perl/libipc-run-perl_0.94-1_all.deb  Unable to connect to archive.ubuntu.com:http: [IP: 91.189.88.152 80]

E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/universe/m/moreutils/moreutils_0.57-1_amd64.deb  Unable to connect to archive.ubuntu.com:http: [IP: 91.189.88.152 80]

E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?
Running command './tools/ci/test_controller.sh kafka/gelly' with a timeout of 234 minutes.
./tools/azure-pipelines/uploading_watchdog.sh: line 76: ts: command not found
The STDIO streams did not close within 10 seconds of the exit event from process '/bin/bash'. This may indicate a child process inherited the STDIO streams and has not yet exited.
##[error]Bash exited with code '141'.
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28064&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=e424005a-b16e-540f-196d-da062cc19bdf&l=13",,gaoyunhaii,hackergin,martijnvisser,Sergey Nuyanzin,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30921,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 24 12:46:32 UTC 2023,,,,,,,,,,"0|z0xnzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/21 06:36;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28065&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354&l=13;;;","14/Dec/21 08:25;trohrmann;cc [~chesnay] another infrastructure problem :-(;;;","14/Dec/21 08:44;chesnay;Let's wait a bit, it sounds like an external issue with {{archive.ubuntu.com}}.;;;","14/Dec/21 09:32;gaoyunhaii;May I have a double confirmation if we want to include it in the 1.13.4 and 1.14.1? Since the two versions are going to be released as the fast fix version soon~;;;","24/Dec/21 10:04;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28567&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=27;;;","25/Dec/21 03:09;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28551&view=logs&j=56781494-ebb0-5eae-f732-b9c397ec6ede&t=f34192cb-f912-5aba-c822-2283f32eeb24&l=31
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28551&view=logs&j=119bbba7-f5e3-5e08-e72d-09f1529665de&t=7166e71c-cad6-5ec9-ae14-15891ce68128&l=31
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28551&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=576aba0a-d787-51b6-6a92-cf233f360582&l=31
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28551&view=logs&j=ff2e2ea5-07e3-5521-7b04-a4fc3ad765e9&t=1ec6382b-bafe-5817-63ae-eda7d4be718e&l=31
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28551&view=logs&j=ed934a8e-982d-5d3f-03cf-c751f5bd1b22&t=972d3f6c-09f6-5149-9cf8-2eaaf718eb08&l=29
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28551&view=logs&j=51fed01c-4eb0-5511-d479-ed5e8b9a7820&t=948a1472-716f-5b18-3d4a-33ca0a14a784&l=31
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28551&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=31
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28551&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=31
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28551&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=31
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28551&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=31;;;","25/Dec/21 03:27;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28552&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3&l=27
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28552&view=logs&j=b2f046ab-ae17-5406-acdc-240be7e870e4&t=93e5ae06-d194-513d-ba8d-150ef6da1d7c&l=27
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28552&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=30
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28552&view=logs&j=6bfdaf55-0c08-5e3f-a2d2-2a0285fd41cf&t=fd9796c3-9ce8-5619-781c-42f873e126a6&l=30
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28552&view=logs&j=f450c1a5-64b1-5955-e215-49cb1ad5ec88&t=ea63c80c-957f-50d1-8f67-3671c14686b9&l=31
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28552&view=logs&j=d8d26c26-7ec2-5ed2-772e-7a1a1eb8317c&t=be5fb08e-1ad7-563c-4f1a-a97ad4ce4865&l=31
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28552&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=19336553-69ec-5b03-471a-791a483cced6&l=29
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28552&view=logs&j=af0c3dd6-ccea-53d1-d352-344c568905e4&t=f898bece-d8f3-5fab-10f5-eacbefdb2d1b&l=31
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28552&view=logs&j=02c4e775-43bf-5625-d1cc-542b5209e072&t=e5961b24-88d9-5c77-efd3-955422674c25&l=31
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28552&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=81f2da51-a161-54c7-5b84-6001fed26530&l=31
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28552&view=logs&j=3b6ec2fd-a816-5e75-c775-06fb87cb6670&t=2aff8966-346f-518f-e6ce-de64002a5034&l=31
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28552&view=logs&j=f66801b3-5d8b-58b4-03aa-cc67e0663d23&t=1abe556e-1530-599d-b2c7-b8c00d549e53&l=31
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28552&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b&l=31
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28552&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=2c7d57b9-7341-5a87-c9af-2cf7cc1a37dc&l=30
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28552&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=53f6305f-55e6-561c-8f1e-3a1dde2c77df&l=29
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28552&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=9c1ddabe-d186-5a2c-5fcc-f3cafb3ec699&l=29

It seems nearly all the ci machines are affected. 
;;;","25/Dec/21 03:35;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28553&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=29
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28553&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=f53023d8-92c3-5d78-ec7e-70c2bf37be20&l=31
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28553&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=e424005a-b16e-540f-196d-da062cc19bdf&l=31
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28553&view=logs&j=119bbba7-f5e3-5e08-e72d-09f1529665de&t=7166e71c-cad6-5ec9-ae14-15891ce68128&l=27
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28553&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=576aba0a-d787-51b6-6a92-cf233f360582&l=28
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28553&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=29
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28553&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=29
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28553&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=31
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28553&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=31;;;","25/Dec/21 03:37;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28566&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=29
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28566&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=31
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28566&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=31
;;;","25/Dec/21 03:38;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28583&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3&l=31
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28583&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=31;;;","27/Dec/21 06:17;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28588&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=26
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28588&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=26
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28588&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=29
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28588&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=e424005a-b16e-540f-196d-da062cc19bdf&l=27
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28588&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=28
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28588&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=29
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28588&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=576aba0a-d787-51b6-6a92-cf233f360582&l=27
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28588&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=27
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28588&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=26
;;;","27/Dec/21 06:27;gaoyunhaii;A lot of failures in https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28589&view=results
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28590&view=results;;;","27/Dec/21 08:52;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28601&view=logs&j=b2f046ab-ae17-5406-acdc-240be7e870e4&t=93e5ae06-d194-513d-ba8d-150ef6da1d7c
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28601&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28601&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28601&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28601&view=logs&j=bdd9ea51-4de2-506a-d4d9-f3930e4d2355&t=98717c4f-b888-5636-bb1e-db7aca25755e
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28601&view=logs&j=904e5037-64c0-5f69-f6d5-e21b89cf6484&t=39857031-7f0c-5fd5-d730-a19c5794f839
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28601&view=logs&j=d8d26c26-7ec2-5ed2-772e-7a1a1eb8317c&t=be5fb08e-1ad7-563c-4f1a-a97ad4ce4865
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28601&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28601&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28601&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=81f2da51-a161-54c7-5b84-6001fed26530
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28601&view=logs&j=3b6ec2fd-a816-5e75-c775-06fb87cb6670&t=2aff8966-346f-518f-e6ce-de64002a5034
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28601&view=logs&j=219e462f-e75e-506c-3671-5017d866ccf6&t=4c5dc768-5c82-5ab0-660d-086cb90b76a0
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28601&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=43529380-51b4-5e90-5af4-2dccec0ef402
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28601&view=logs&j=eddb60be-c754-50c0-36d1-45167e55652d&t=6c0c4594-2e5a-5230-32ae-c163a76621ab

;;;","27/Dec/21 08:53;trohrmann;More failures: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28602&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","27/Dec/21 09:07;trohrmann;More failures: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28603&view=results;;;","03/Jan/22 12:46;martijnvisser;[~trohrmann] [~gaoyunhaii] I believe this has hasn't happened since Dec 27th. Shall we either lower the priority or close this ticket? ;;;","03/Jan/22 13:44;trohrmann;Yes, let's close it.;;;","09/Mar/22 07:20;gaoyunhaii;1.14: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32696&view=logs&j=4dd4dbdd-1802-5eb7-a518-6acd9d24d0fc&t=7c4a8fb8-eeee-5a77-f518-4176bfae300b&l=31 Let's observe if it would re-occur.;;;","20/Apr/23 06:59;Sergey Nuyanzin;It appears again 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48241&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=f8a6d3eb-38cf-5cca-9a99-d0badeb5fe62&l=42;;;","20/Apr/23 06:59;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48241&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=39a035c3-c65e-573c-fb66-104c66c28912&l=42;;;","20/Apr/23 07:02;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48242&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=39a035c3-c65e-573c-fb66-104c66c28912&l=44;;;","20/Apr/23 07:03;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48242&view=logs&j=b31992a1-93b0-59f3-2c17-4a9deb43d11c&t=3a444376-fbe9-578d-2efd-f711b2558a25&l=44;;;","20/Apr/23 07:03;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48242&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=5846934b-7a4f-545b-e5b0-eb4d8bda32e1&l=44;;;","20/Apr/23 07:15;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48248&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=44;;;","20/Apr/23 07:16;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48251&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=44;;;","20/Apr/23 07:16;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48251&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=44;;;","20/Apr/23 07:17;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48252&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=44;;;","24/Apr/23 12:46;martijnvisser;This will be mitigated via FLINK-30921;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support nulls in DataGen,FLINK-25284,13417217,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,13/Dec/21 22:17,04/Jun/23 21:19,13/Jul/23 08:12,04/Jun/23 21:19,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,Table SQL / API,,,,,0,pull-request-available,stale-assigned,,,"Currently it is impossible to specify that some values should be null sometimes.
It would be nice to have some property something like {{null-rate}} telling how often there should be {{null}} value generated
something like that
{code:sql}
CREATE TABLE Orders (
    order_number STRING,
    price        DECIMAL(32,2),
    buyer        ROW<id INT, last_name STRING>,
    order_time   TIMESTAMP(3),
    my_map       MAP<INT,STRING>,
    my_arrray    ARRAY<STRING>
) WITH (
   'connector' = 'datagen',
   'fields.order_number.null-rate' = '0.7',
   'fields.price.null-rate' = '1.0',
   'fields.order_time.null-rate' = '0.5',
   'fields.buyer.id.null-rate' = '0.5',
   'fields.buyer.null-rate' = '0.5',
   'fields.my_map.key.null-rate' = '0.5',
   'fields.my_map.null-rate' = '0.5',
   'fields.my_array.element.null-rate' = '0.1',
   'fields.my_array.null-rate' = '0.5'
);
{code}",,Sergey Nuyanzin,trushev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jun 04 21:19:27 UTC 2023,,,,,,,,,,"0|z0xnj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","04/Jun/23 21:19;Sergey Nuyanzin;Merged as [d04405c0f6ef0c661909935a1a3fb0b1c79d2540|https://github.com/apache/flink/commit/d04405c0f6ef0c661909935a1a3fb0b1c79d2540];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaPartitionSplitReaderTest failed on azure due to Offsets out of range with no configured reset policy for partitions,FLINK-25280,13417060,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,gaoyunhaii,gaoyunhaii,13/Dec/21 15:15,12/Jan/22 14:32,13/Jul/23 08:12,12/Jan/22 14:32,1.13.5,1.14.3,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.4,1.15.0,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"{code:java}
2021-12-13T03:30:12.8392593Z Dec 13 03:30:12 [ERROR] Tests run: 6, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 85.344 s <<< FAILURE! - in org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest
2021-12-13T03:30:12.8394604Z Dec 13 03:30:12 [ERROR] testNumBytesInCounter  Time elapsed: 0.24 s  <<< ERROR!
2021-12-13T03:30:12.8396218Z Dec 13 03:30:12 org.apache.kafka.clients.consumer.OffsetOutOfRangeException: Offsets out of range with no configured reset policy for partitions: {topic1-0=0}
2021-12-13T03:30:12.8397052Z Dec 13 03:30:12 	at org.apache.kafka.clients.consumer.internals.Fetcher.initializeCompletedFetch(Fetcher.java:1260)
2021-12-13T03:30:12.8397697Z Dec 13 03:30:12 	at org.apache.kafka.clients.consumer.internals.Fetcher.fetchedRecords(Fetcher.java:607)
2021-12-13T03:30:12.8398394Z Dec 13 03:30:12 	at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1313)
2021-12-13T03:30:12.8399306Z Dec 13 03:30:12 	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1240)
2021-12-13T03:30:12.8399924Z Dec 13 03:30:12 	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1211)
2021-12-13T03:30:12.8400610Z Dec 13 03:30:12 	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.fetch(KafkaPartitionSplitReader.java:113)
2021-12-13T03:30:12.8401385Z Dec 13 03:30:12 	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest.testNumBytesInCounter(KafkaPartitionSplitReaderTest.java:153)
2021-12-13T03:30:12.8402174Z Dec 13 03:30:12 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-12-13T03:30:12.8402911Z Dec 13 03:30:12 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-12-13T03:30:12.8403818Z Dec 13 03:30:12 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-12-13T03:30:12.8404452Z Dec 13 03:30:12 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-12-13T03:30:12.8405028Z Dec 13 03:30:12 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)
2021-12-13T03:30:12.8405740Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2021-12-13T03:30:12.8406749Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2021-12-13T03:30:12.8407886Z Dec 13 03:30:12 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2021-12-13T03:30:12.8408845Z Dec 13 03:30:12 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2021-12-13T03:30:12.8409507Z Dec 13 03:30:12 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2021-12-13T03:30:12.8410219Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2021-12-13T03:30:12.8411081Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2021-12-13T03:30:12.8411785Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2021-12-13T03:30:12.8412740Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2021-12-13T03:30:12.8413553Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2021-12-13T03:30:12.8414293Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2021-12-13T03:30:12.8415078Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2021-12-13T03:30:12.8415977Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2021-12-13T03:30:12.8417383Z Dec 13 03:30:12 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)
2021-12-13T03:30:12.8418339Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8419209Z Dec 13 03:30:12 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)
2021-12-13T03:30:12.8419942Z Dec 13 03:30:12 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)
2021-12-13T03:30:12.8420716Z Dec 13 03:30:12 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)
2021-12-13T03:30:12.8421423Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
2021-12-13T03:30:12.8422192Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8423061Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2021-12-13T03:30:12.8423740Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-12-13T03:30:12.8424514Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2021-12-13T03:30:12.8425224Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8425912Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2021-12-13T03:30:12.8426672Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2021-12-13T03:30:12.8427251Z Dec 13 03:30:12 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2021-12-13T03:30:12.8427947Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
2021-12-13T03:30:12.8428848Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
2021-12-13T03:30:12.8429554Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8430394Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2021-12-13T03:30:12.8431046Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-12-13T03:30:12.8431704Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2021-12-13T03:30:12.8432455Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8433310Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2021-12-13T03:30:12.8433983Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2021-12-13T03:30:12.8434685Z Dec 13 03:30:12 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2021-12-13T03:30:12.8435379Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
2021-12-13T03:30:12.8436164Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
2021-12-13T03:30:12.8436853Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8437658Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2021-12-13T03:30:12.8438321Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-12-13T03:30:12.8439127Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2021-12-13T03:30:12.8439833Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8440559Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2021-12-13T03:30:12.8441222Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2021-12-13T03:30:12.8442108Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
2021-12-13T03:30:12.8443108Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
2021-12-13T03:30:12.8443845Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)
2021-12-13T03:30:12.8444481Z Dec 13 03:30:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
2021-12-13T03:30:12.8445164Z Dec 13 03:30:12 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
2021-12-13T03:30:12.8445835Z Dec 13 03:30:12 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
2021-12-13T03:30:12.8446527Z Dec 13 03:30:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
2021-12-13T03:30:12.8447150Z Dec 13 03:30:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
2021-12-13T03:30:12.8447920Z Dec 13 03:30:12 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
2021-12-13T03:30:12.8448754Z Dec 13 03:30:12 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2021-12-13T03:30:12.8449532Z Dec 13 03:30:12 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-12-13T03:30:12.8450305Z Dec 13 03:30:12 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-12-13T03:30:12.8450976Z Dec 13 03:30:12 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-12-13T03:30:12.8451567Z Dec 13 03:30:12 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-12-13T03:30:12.8451989Z Dec 13 03:30:12 
2021-12-13T03:30:12.8452504Z Dec 13 03:30:12 [ERROR] testHandleSplitChangesAndFetch  Time elapsed: 0.037 s  <<< ERROR!
2021-12-13T03:30:12.8454253Z Dec 13 03:30:12 org.apache.kafka.clients.consumer.OffsetOutOfRangeException: Offsets out of range with no configured reset policy for partitions: {topic1-2=2}
2021-12-13T03:30:12.8455008Z Dec 13 03:30:12 	at org.apache.kafka.clients.consumer.internals.Fetcher.initializeCompletedFetch(Fetcher.java:1260)
2021-12-13T03:30:12.8455818Z Dec 13 03:30:12 	at org.apache.kafka.clients.consumer.internals.Fetcher.fetchedRecords(Fetcher.java:607)
2021-12-13T03:30:12.8456428Z Dec 13 03:30:12 	at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1282)
2021-12-13T03:30:12.8457043Z Dec 13 03:30:12 	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1240)
2021-12-13T03:30:12.8457626Z Dec 13 03:30:12 	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1211)
2021-12-13T03:30:12.8458300Z Dec 13 03:30:12 	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.fetch(KafkaPartitionSplitReader.java:113)
2021-12-13T03:30:12.8459238Z Dec 13 03:30:12 	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest.assignSplitsAndFetchUntilFinish(KafkaPartitionSplitReaderTest.java:261)
2021-12-13T03:30:12.8460115Z Dec 13 03:30:12 	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest.testHandleSplitChangesAndFetch(KafkaPartitionSplitReaderTest.java:105)
2021-12-13T03:30:12.8460760Z Dec 13 03:30:12 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-12-13T03:30:12.8461310Z Dec 13 03:30:12 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-12-13T03:30:12.8461937Z Dec 13 03:30:12 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-12-13T03:30:12.8462742Z Dec 13 03:30:12 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-12-13T03:30:12.8463452Z Dec 13 03:30:12 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)
2021-12-13T03:30:12.8464077Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2021-12-13T03:30:12.8464794Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2021-12-13T03:30:12.8465636Z Dec 13 03:30:12 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2021-12-13T03:30:12.8466298Z Dec 13 03:30:12 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2021-12-13T03:30:12.8466990Z Dec 13 03:30:12 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2021-12-13T03:30:12.8467722Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2021-12-13T03:30:12.8468454Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2021-12-13T03:30:12.8469192Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2021-12-13T03:30:12.8469941Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2021-12-13T03:30:12.8470649Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2021-12-13T03:30:12.8471493Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2021-12-13T03:30:12.8472238Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2021-12-13T03:30:12.8473107Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2021-12-13T03:30:12.8473827Z Dec 13 03:30:12 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)
2021-12-13T03:30:12.8474551Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8475262Z Dec 13 03:30:12 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)
2021-12-13T03:30:12.8476109Z Dec 13 03:30:12 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)
2021-12-13T03:30:12.8477234Z Dec 13 03:30:12 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)
2021-12-13T03:30:12.8478320Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
2021-12-13T03:30:12.8479437Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8480706Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2021-12-13T03:30:12.8481725Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-12-13T03:30:12.8482890Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2021-12-13T03:30:12.8483628Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8484318Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2021-12-13T03:30:12.8484989Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2021-12-13T03:30:12.8485557Z Dec 13 03:30:12 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2021-12-13T03:30:12.8486228Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
2021-12-13T03:30:12.8487030Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
2021-12-13T03:30:12.8487731Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8488431Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2021-12-13T03:30:12.8489079Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-12-13T03:30:12.8489731Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2021-12-13T03:30:12.8490576Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8491262Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2021-12-13T03:30:12.8491917Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2021-12-13T03:30:12.8492550Z Dec 13 03:30:12 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2021-12-13T03:30:12.8493404Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
2021-12-13T03:30:12.8494341Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
2021-12-13T03:30:12.8495043Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8495729Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2021-12-13T03:30:12.8496390Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-12-13T03:30:12.8497036Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2021-12-13T03:30:12.8497730Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8498411Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2021-12-13T03:30:12.8499075Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2021-12-13T03:30:12.8499820Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
2021-12-13T03:30:12.8500699Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
2021-12-13T03:30:12.8501427Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)
2021-12-13T03:30:12.8502155Z Dec 13 03:30:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
2021-12-13T03:30:12.8502853Z Dec 13 03:30:12 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
2021-12-13T03:30:12.8503533Z Dec 13 03:30:12 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
2021-12-13T03:30:12.8504175Z Dec 13 03:30:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
2021-12-13T03:30:12.8504790Z Dec 13 03:30:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
2021-12-13T03:30:12.8505463Z Dec 13 03:30:12 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
2021-12-13T03:30:12.8506155Z Dec 13 03:30:12 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2021-12-13T03:30:12.8506837Z Dec 13 03:30:12 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-12-13T03:30:12.8507495Z Dec 13 03:30:12 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-12-13T03:30:12.8508098Z Dec 13 03:30:12 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-12-13T03:30:12.8508684Z Dec 13 03:30:12 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-12-13T03:30:12.8509121Z Dec 13 03:30:12 
2021-12-13T03:30:12.8509555Z Dec 13 03:30:12 [ERROR] testPendingRecordsGauge{String}[1]  Time elapsed: 0.037 s  <<< ERROR!
2021-12-13T03:30:12.8510837Z Dec 13 03:30:12 org.apache.kafka.clients.consumer.OffsetOutOfRangeException: Offsets out of range with no configured reset policy for partitions: {topic1-0=0}
2021-12-13T03:30:12.8511603Z Dec 13 03:30:12 	at org.apache.kafka.clients.consumer.internals.Fetcher.initializeCompletedFetch(Fetcher.java:1260)
2021-12-13T03:30:12.8512314Z Dec 13 03:30:12 	at org.apache.kafka.clients.consumer.internals.Fetcher.fetchedRecords(Fetcher.java:607)
2021-12-13T03:30:12.8513023Z Dec 13 03:30:12 	at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1313)
2021-12-13T03:30:12.8513726Z Dec 13 03:30:12 	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1240)
2021-12-13T03:30:12.8514320Z Dec 13 03:30:12 	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1211)
2021-12-13T03:30:12.8514990Z Dec 13 03:30:12 	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.fetch(KafkaPartitionSplitReader.java:113)
2021-12-13T03:30:12.8515785Z Dec 13 03:30:12 	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReaderTest.testPendingRecordsGauge(KafkaPartitionSplitReaderTest.java:195)
2021-12-13T03:30:12.8516439Z Dec 13 03:30:12 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-12-13T03:30:12.8516982Z Dec 13 03:30:12 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-12-13T03:30:12.8517599Z Dec 13 03:30:12 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-12-13T03:30:12.8518168Z Dec 13 03:30:12 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-12-13T03:30:12.8518689Z Dec 13 03:30:12 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)
2021-12-13T03:30:12.8519286Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2021-12-13T03:30:12.8519965Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2021-12-13T03:30:12.8520724Z Dec 13 03:30:12 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2021-12-13T03:30:12.8521346Z Dec 13 03:30:12 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2021-12-13T03:30:12.8522067Z Dec 13 03:30:12 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)
2021-12-13T03:30:12.8522856Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2021-12-13T03:30:12.8523569Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2021-12-13T03:30:12.8524281Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2021-12-13T03:30:12.8525034Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2021-12-13T03:30:12.8525759Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2021-12-13T03:30:12.8526477Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2021-12-13T03:30:12.8527146Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2021-12-13T03:30:12.8527788Z Dec 13 03:30:12 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2021-12-13T03:30:12.8528479Z Dec 13 03:30:12 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)
2021-12-13T03:30:12.8529197Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8529907Z Dec 13 03:30:12 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)
2021-12-13T03:30:12.8530615Z Dec 13 03:30:12 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)
2021-12-13T03:30:12.8531290Z Dec 13 03:30:12 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)
2021-12-13T03:30:12.8532087Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
2021-12-13T03:30:12.8532981Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8533690Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2021-12-13T03:30:12.8534344Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-12-13T03:30:12.8535079Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2021-12-13T03:30:12.8535920Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8536611Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2021-12-13T03:30:12.8537256Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2021-12-13T03:30:12.8538004Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
2021-12-13T03:30:12.8538773Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:212)
2021-12-13T03:30:12.8539606Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:192)
2021-12-13T03:30:12.8540337Z Dec 13 03:30:12 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)
2021-12-13T03:30:12.8541043Z Dec 13 03:30:12 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)
2021-12-13T03:30:12.8541698Z Dec 13 03:30:12 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2021-12-13T03:30:12.8542374Z Dec 13 03:30:12 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2021-12-13T03:30:12.8543038Z Dec 13 03:30:12 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
2021-12-13T03:30:12.8543597Z Dec 13 03:30:12 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2021-12-13T03:30:12.8544175Z Dec 13 03:30:12 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2021-12-13T03:30:12.8544752Z Dec 13 03:30:12 	at java.util.stream.ReferencePipeline$11$1.accept(ReferencePipeline.java:440)
2021-12-13T03:30:12.8545336Z Dec 13 03:30:12 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2021-12-13T03:30:12.8545909Z Dec 13 03:30:12 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2021-12-13T03:30:12.8546483Z Dec 13 03:30:12 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2021-12-13T03:30:12.8547063Z Dec 13 03:30:12 	at java.util.stream.Streams$StreamBuilderImpl.forEachRemaining(Streams.java:419)
2021-12-13T03:30:12.8547657Z Dec 13 03:30:12 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647)
2021-12-13T03:30:12.8548221Z Dec 13 03:30:12 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2021-12-13T03:30:12.8548796Z Dec 13 03:30:12 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2021-12-13T03:30:12.8549382Z Dec 13 03:30:12 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2021-12-13T03:30:12.8549941Z Dec 13 03:30:12 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2021-12-13T03:30:12.8550510Z Dec 13 03:30:12 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2021-12-13T03:30:12.8551089Z Dec 13 03:30:12 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2021-12-13T03:30:12.8551742Z Dec 13 03:30:12 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2021-12-13T03:30:12.8552355Z Dec 13 03:30:12 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2021-12-13T03:30:12.8553007Z Dec 13 03:30:12 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2021-12-13T03:30:12.8553600Z Dec 13 03:30:12 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2021-12-13T03:30:12.8554173Z Dec 13 03:30:12 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2021-12-13T03:30:12.8554746Z Dec 13 03:30:12 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2021-12-13T03:30:12.8555332Z Dec 13 03:30:12 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2021-12-13T03:30:12.8555910Z Dec 13 03:30:12 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2021-12-13T03:30:12.8556492Z Dec 13 03:30:12 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2021-12-13T03:30:12.8557062Z Dec 13 03:30:12 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2021-12-13T03:30:12.8557661Z Dec 13 03:30:12 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2021-12-13T03:30:12.8558246Z Dec 13 03:30:12 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2021-12-13T03:30:12.8558915Z Dec 13 03:30:12 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2021-12-13T03:30:12.8559554Z Dec 13 03:30:12 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)
2021-12-13T03:30:12.8560251Z Dec 13 03:30:12 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)
2021-12-13T03:30:12.8560959Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
2021-12-13T03:30:12.8561662Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8562420Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2021-12-13T03:30:12.8563140Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-12-13T03:30:12.8563795Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2021-12-13T03:30:12.8564500Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8565183Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2021-12-13T03:30:12.8565852Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2021-12-13T03:30:12.8566632Z Dec 13 03:30:12 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2021-12-13T03:30:12.8567724Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
2021-12-13T03:30:12.8568691Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
2021-12-13T03:30:12.8569384Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8570088Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2021-12-13T03:30:12.8570756Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-12-13T03:30:12.8571405Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2021-12-13T03:30:12.8572436Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8573210Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2021-12-13T03:30:12.8573873Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2021-12-13T03:30:12.8574445Z Dec 13 03:30:12 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2021-12-13T03:30:12.8575106Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
2021-12-13T03:30:12.8575902Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
2021-12-13T03:30:12.8576603Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8577308Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2021-12-13T03:30:12.8577953Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-12-13T03:30:12.8578602Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2021-12-13T03:30:12.8579383Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-13T03:30:12.8580063Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2021-12-13T03:30:12.8580724Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2021-12-13T03:30:12.8581456Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
2021-12-13T03:30:12.8582313Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
2021-12-13T03:30:12.8583105Z Dec 13 03:30:12 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)
2021-12-13T03:30:12.8583737Z Dec 13 03:30:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
2021-12-13T03:30:12.8584367Z Dec 13 03:30:12 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
2021-12-13T03:30:12.8585019Z Dec 13 03:30:12 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
2021-12-13T03:30:12.8585660Z Dec 13 03:30:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
2021-12-13T03:30:12.8586287Z Dec 13 03:30:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
2021-12-13T03:30:12.8586947Z Dec 13 03:30:12 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
2021-12-13T03:30:12.8587623Z Dec 13 03:30:12 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2021-12-13T03:30:12.8588313Z Dec 13 03:30:12 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-12-13T03:30:12.8588963Z Dec 13 03:30:12 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-12-13T03:30:12.8589570Z Dec 13 03:30:12 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-12-13T03:30:12.8590154Z Dec 13 03:30:12 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-12-13T03:30:12.8590666Z Dec 13 03:30:12 
{code}

testHandleSplitChangesAndFetch
testNumBytesInCounter
testPendingRecordsGauge

from KafkaPartitionSplitReaderTest failed with this issue",,fpaul,gaoyunhaii,renqs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 12 14:31:20 UTC 2022,,,,,,,,,,"0|z0xmk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/22 06:17;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29136&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7270;;;","10/Jan/22 06:17;gaoyunhaii;perhaps cc [~fpaul];;;","10/Jan/22 07:49;fpaul;[~renqs] can you take a look at this? It is possible that the error is caused again by the fact that we do not guarantee to commit back the offsets but the test assumes that they are committed.;;;","10/Jan/22 09:17;renqs;I think this is caused by the same reason of FLINK-22198 that records produced to Kafka had a quite early timestamp (e.g 1000, which is in year 1970) and were deleted on broker. Similar to the solution in FLINK-22198, we can disable the log deletion by setting log.retention.ms to -1 in KafkaTestBase.;;;","11/Jan/22 09:53;fpaul;Fixed in master: 23c477fd97d02bab00ae672e876f9b0890338d1d;;;","12/Jan/22 14:31;fpaul;Merged in release-1.14: e93fb032b09cf3f0605848faa619e51e1fbf8fed, release-1.13: 98275f96e92f30d3717b16a02b269d3bdc3e45ef

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use ResolvedSchema in DataGen instead of TableSchema,FLINK-25274,13417000,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,13/Dec/21 11:00,16/Dec/21 10:47,13/Jul/23 08:12,16/Dec/21 10:47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Table SQL / API,,,,,0,pull-request-available,,,,"{{TableSchema}} is deprecated 
It is recommended to use {{ResolvedSchema}} and {{Schema}} in {{TableSchema}} javadoc",,airblader,RocMarshal,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 16 10:47:15 UTC 2021,,,,,,,,,,"0|z0xm6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/21 11:10;airblader;It'd actually be even better if the runtime source only had access to the physical data type, it doesn't need the schema at all. We also recently introduced several utility methods to make working with this easier, see DynamicTableFactory#Context#getPhysicalRowDataType to pass it to the source and then DataType#getFieldNames, DataType#getFieldCount, DataType#getFieldDataTypes, …;;;","13/Dec/21 11:34;Sergey Nuyanzin;Thanks for your fast response [~airblader]. I will have a look at utility methods you've mentioned;;;","13/Dec/21 14:46;Sergey Nuyanzin;[~airblader], I corrected the PR based on your suggestion.
Could you please have a look once you have time?;;;","16/Dec/21 10:47;airblader;Fixed in master

commit 586f2b69f3db2257bd526963c4b1ab99aae47447
[table-api-java-bridge] Migrate datagen connector from deprecated TableSchema;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ApplicationDispatcherBootstrapITCase. testDispatcherRecoversAfterLosingAndRegainingLeadership failed on azure,FLINK-25271,13416955,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,gaoyunhaii,gaoyunhaii,13/Dec/21 07:12,12/Jan/22 13:26,13/Jul/23 08:12,22/Dec/21 10:38,1.14.0,1.14.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,,,,,Client / Job Submission,,,,,0,pull-request-available,test-stability,,,"{code:java}
Dec 12 04:22:42 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
Dec 12 04:22:42 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
Dec 12 04:22:42 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
Dec 12 04:22:42 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Dec 12 04:22:42 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Dec 12 04:22:42 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Dec 12 04:22:42 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Dec 12 04:22:42 
Dec 12 04:22:42 [INFO] 
Dec 12 04:22:42 [INFO] Results:
Dec 12 04:22:42 [INFO] 
Dec 12 04:22:42 [ERROR] Errors: 
Dec 12 04:22:42 [ERROR]   ApplicationDispatcherBootstrapITCase.testDispatcherRecoversAfterLosingAndRegainingLeadership:124->awaitJobStatus:135->lambda$awaitJobStatus$2:138 » IllegalState
Dec 12 04:22:42 [INFO] 
Dec 12 04:22:42 [ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0
Dec 12 04:22:42 [INFO] 
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27993&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=9350",,gaoyunhaii,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25317,,,,,,,,,,,FLINK-23946,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 22 10:38:58 UTC 2021,,,,,,,,,,"0|z0xlww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/21 10:38;mapohl;1.14: 68148d99b528bfb5f8d641ee34ad9e083653803d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Azure failed due to stopped hearing from agent AlibabaCI006-agent01,FLINK-25270,13416954,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,gaoyunhaii,gaoyunhaii,13/Dec/21 07:05,11/Jan/22 14:12,13/Jul/23 08:12,11/Jan/22 14:12,1.13.3,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Build System / Azure Pipelines,,,,,0,test-stability,,,,"{code:java}
##[error]We stopped hearing from agent AlibabaCI006-agent01. Verify the agent machine is running and has a healthy network connection. Anything that terminates an agent process, starves it for CPU, or blocks its network access can cause this error. For more information, see: https://go.microsoft.com/fwlink/?linkid=846610
Started: 周六 at 下午1:37
Duration: <1s
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27982&view=logs&j=f2b08047-82c3-520f-51ee-a30fd6254285",,gaoyunhaii,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 13 07:07:43 UTC 2021,,,,,,,,,,"0|z0xlwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/21 07:07;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27983&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483;;;","13/Dec/21 07:07;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27984&view=logs&j=b1e44b80-6687-5cc5-6529-292f7212c609;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changelog not truncated on materialization,FLINK-25261,13416846,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,11/Dec/21 22:06,04/Jan/22 13:52,13/Jul/23 08:12,04/Jan/22 13:52,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,[https://github.com/apache/flink/blob/dcc4d43e413b20f70036e73c61d52e2e1c5afee7/flink-state-backends/flink-statebackend-changelog/src/main/java/org/apache/flink/state/changelog/ChangelogKeyedStateBackend.java#L640],,roman,ym,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25144,FLINK-21352,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 04 13:52:34 UTC 2022,,,,,,,,,,"0|z0xl8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/21 06:35;ym;Shouldn't truncation happen when a checkpoint is subsumed?  

I do not think it is a safe/right place to do truncation when materialization completes.;;;","22/Dec/21 08:40;roman;On checkpoint subsumption, actual state is deleted on DFS.

 

On materialization, in-memory data SHOULD be cleaned up in writer. This is what is currently missing. It is safe to do by definition of materialization: backend will no longer use these changes once materialized.;;;","22/Dec/21 09:05;ym;1. What does ""truncation"" mean? It means from seq # x, data is safe to delete.
2. Materialization up to seq # y means *new* checkpoints will not depend on log changes before seq # y. But it does not necessarily mean all data before y can be deleted (*old* checkpoints may also depend on some parts before y).
3. what exactly are the `in-memory` data here? Those are not flushed to DFS? Then what about those already flushed to DFS, but not within any checkpoint yet (not within any state)?
4. Also, since materialization is independent of checkpointing; an ongoing checkpoint may depend on in-memory data not flushed to DFS yet.

         
New CP
--|---------|----------> (In Mem Log)
            Materialization upto (but not complete yet)
     ;;;","22/Dec/21 13:37;roman;# Correct (as I wrote above in context of this ticket I mean in-memory data only)
 # As I wrote above in context of this ticket I mean in-memory data only
 # In-memory means any in-memory object not needed for checkpointing anymore, depending on DSTL implementation (reference to file or an in-memory byte array to-be -flushed); To discard already flushed but not included into any checkpoint changes, we have three options: a) shared/private state ownership and TM-side registry (FLINK-23139); b) TM-side registry only for changelog; c) rely on FLINK-24852 (which will likely be needed by FLINK-25395). I propose to postpone this decision until we decide on FLINK-25395 and state ownership. Note that this only happens if pre-emptive upload is enabled (otherwise, state changes are always associated with some checkpoint)
 # Materialization is independent, but handling its result is ""synchronized"" with checkpointing by using Task mailbox; so it's only the writer.truncate() method that should take ongoing checkpoints into account; and this is the with the current FS writer.;;;","23/Dec/21 04:51;ym;{quote}1. Correct (as I wrote above in the context of this ticket I mean in-memory data only)
2. As I wrote above in the context of this ticket I mean in-memory data only
{quote}
If truncating only means truncating the in-memory part, would this API be general enough for other implementations as well? (Like in-memory implementation, Kafka based implementation)
{quote}3. In-memory means any in-memory object not needed for checkpointing anymore, depending on DSTL implementation (reference to file or an in-memory byte array to-be -flushed); To discard already flushed but not included into any checkpoint changes, we have three options: a) shared/private state ownership and TM-side registry (FLINK-23139); b) TM-side registry only for changelog; c) rely on FLINK-24852 (which will likely be needed by FLINK-25395). I propose to postpone this decision until we decide on FLINK-25395 and state ownership. Note that this only happens if pre-emptive upload is enabled (otherwise, state changes are always associated with some checkpoint)
{quote}
I am fine to postpone this decision.
{quote}4. Materialization is independent, but handling its result is ""synchronized"" with checkpointing by using Task mailbox; so it's only the writer.truncate() method that should take ongoing checkpoints into account; and this is the with the current FS writer.
{quote}
Let's consider this:

New CP
{-}{{-}}|{{-}}{-}--{-}{{-}}{{-}}|{{-}}{{-}}{-}-------> (In Mem Log)
Materialization up to (but not complete yet)

1). Materialization triggered

2). CP triggered (the uploading part is not in the task thread)

3). Materialization finished and put truncating action into the mailbox

4). task thread truncate the log

5). checkpoint complete

=====================

Also, if I am understanding correctly, now we put clean-up into three different places:

1). State Cleanup upon checkpoint subsumption

2). In-memory part clean-up upon materialization completes

3). DFS files cleanup (not included in any state) TBD.

Again, would this abstraction general enough to support using other implmentation? It is fragile to me.;;;","04/Jan/22 07:53;ym;Upon offline discussion, there are two more questions/problems related. I will extract them and open a separate ticket, and discuss whether/how to handle them before 1.15.

For truncation upon materialization, it is wrapped in the mailbox, so it should work fine regarding the race condition mentioned above. It does not block checkpoint snapshots as well, because what to upload/not upload is decided in the task thread fairly fast; while the real uploading is done asycnously.;;;","04/Jan/22 13:52;roman;Merged as 70a3f661c467e670b0769abd1d0a03d77a45b910 into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Savepoints do not work with ExternallyInducedSources,FLINK-25256,13416531,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,arvid,dwysakowicz,dwysakowicz,10/Dec/21 10:52,30/Mar/23 10:54,13/Jul/23 08:12,15/Jul/22 12:30,1.12.7,1.13.3,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,1.16.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,stale-assigned,,,"It is not possible to take a proper savepoint with {{ExternallyInducedSource}} or {{ExternallyInducedSourceReader}} (both legacy and FLIP-27 versions). The problem is that we're hardcoding {{CheckpointOptions}} in the {{triggerHook}}.

The outcome of current state is that operators would try to take checkpoints in the checkpoint location whereas the {{CheckpointCoordinator}} will write metadata for those states in the savepoint location.

Moreover the situation gets even weirder (I have not checked it entirely), if we have a mixture of {{ExternallyInducedSource(s)}} and regular sources. In such a case the location and format at which the state of a particular task is persisted depends on the order of barriers arrival. If a barrier from a regular source arrives last the task takes a savepoint, on the other hand if last barrier is from an externally induced source it will take a checkpoint.",,becket_qin,Brian Zhou,dwysakowicz,leonard,martijnvisser,Ming Li,pnowojski,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26392,FLINK-25192,FLINK-31560,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 14 22:38:22 UTC 2022,,,,,,,,,,"0|z0xjs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/21 10:53;dwysakowicz;cc [~pnowojski];;;","10/Dec/21 11:12;pnowojski;This is related to supporting/handling ""force full snapshot"" flag in the -no-claim recovery mode.;;;","01/Mar/22 10:29;dwysakowicz;Externally induced sources are quite limited atm. How important is the feature for Pravega connector? See also https://issues.apache.org/jira/browse/FLINK-26392 As far as I know it is the only one that has ever used it. cc [~Brian Zhou] [~jqin];;;","02/Mar/22 03:48;Brian Zhou;Yes, this interface is now used in the both the legacy reader and our coming FLIP-27 reader(still in review now).

Let me explain the reason first.

In general, Pravega handles checkpoint in a different way with other common messaging system such as Kafka. It is a self-contained approach that does not require the extra external management for offsets. Pravega itself has an internal synchronizer to synchronize the read progress for each reader and returns a Checkpoint object on the reader output to have a consistent view of the read progress.

This is why we need such ExternallyInducedSource interface to integrate with Flink, because we need to control the time we trigger and pass the barrier to achieve the exactly once semantics on the source side.

We need to fix this issue as we are just starting integrating Flink 1.14.;;;","03/Mar/22 03:03;becket_qin;[~dwysakowicz] Pravega essentially uses an in-band checkpoint style. The checkpoint process is roughly the following:
 # Flink CheckpointCoordinator initiates the checkpoint, and invokes the MasterHooks.triggerHook(). The Pravega hook then tell the Pravega server that the Flink job has triggered a checkpoint.
 # The Pravega server inserts the checkpoint control messages in the data stream to each of the Prevaga readers of the Flink job.
 # When the Prevaga readers see the checkpoint control messages, they trigger the Flink task checkpoint via the {{ExternallyInducedSource.CheckpointTrigger}}

After FLIP-27, the SplitEnumerator can completely replace the MasterHook in JM. But Prevaga connector still relies on the {{ExternallyInducedSource.CheckpointTrigger}} to perform checkpoint in the subtasks.

Ultimately, the requirement is to manipulate the task based on some user space records. A similar requirement is stopping the subtask when it sees a given message in the stream. What we need to think of would be how much control plane actions do we want to expose to the users. So far the two actions we see are taking checkpoint on the tasks and stopping the tasks, and by now such manipulation requirements are only in the Source tasks.

We can probably just make such records driven task actions a more explicit primitive for the users. For example, we can have an interface like {{{}TaskActionTrigger{}}}, which is passed to each user logic. And that allows user logic to ask the task to take some action based on the records it processed. That said, I do think such control plane exposure should be minimal.;;;","03/Mar/22 07:45;dwysakowicz;I don't have an opinion on a more explicit control plane at the moment. I'll abstain from that discussion for now, hope that's fine.

The current situation is that we do have interfaces for that use case, for some time already. Unfortunately, they do not work too well in many situations. Does any of you have spare cycles to work on improvements there to get it to a usable state? We would be happy to guide such a contribution through in the runtime team.;;;","09/Mar/22 02:02;Brian Zhou;Hi [~dwysakowicz] , there are still something we want to know more details.
 * What Flink version is affected. From this ticket, I see the affect version is 1.14.0 and 1.13.3, does that mean the earlier versions are not affected?
 * Do you know the exact commit or feature that causes this regression issue? If you can explain more, we are willing to look deeper and work together with [~becket_qin] to fix;;;","09/Mar/22 03:29;becket_qin;[~Brian Zhou] I think [~renqs] and [~Leonard] seems the best person to follow up with this issue.;;;","09/Mar/22 07:47;dwysakowicz;All Flink versions are affected. It has never worked properly. It is not a regression.;;;","11/Mar/22 08:29;arvid;I'll have a look. I'd focus on FLIP-27 interfaces.;;;","12/Mar/22 07:51;Brian Zhou;Hi [~dwysakowicz] , I want to ask what the influence is of this issue, for example, what kind of application or state backend settings is not able to recover correctly.

We have internally tried to reproduce this issue for some basic applications. We've found that when savepointing, all of the states are stored in the metadata, which makes the job to recover correctly. We have tried both stateful and stateless apps, both filesystem and rocksdb backend, and always seeing the metadata file in the savepoint directory.

I understand from the code, it does look incorrect in checkpointing when treating the externally induced source, but we need to get the influence to help us make plans in our product releases.;;;","29/Mar/22 08:15;arvid;Merged into 1.15 as 5a5490a49866d02ab5c4761f59ddf94c06f42b41..42e978548f44e98fafbf5f62175e48acba8642e0.;;;","29/Mar/22 19:43;arvid;Merged into master as 7aefdf2c6aefe8c24af30a4f28a59f2780503d21..a4d194e4a0981dceb003508a3178c6a4e0ce0e82. 1.14 backport is pending.;;;","14/Jul/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fabric8FlinkKubeClientITCase.testCheckAndUpdateConfigMapConcurrently,FLINK-25243,13416487,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wangyang0918,trohrmann,trohrmann,10/Dec/21 08:35,07/Mar/22 08:38,13/Jul/23 08:12,07/Mar/22 08:38,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Deployment / Kubernetes,,,,,0,pull-request-available,test-stability,,,"The test {{Fabric8FlinkKubeClientITCase.testCheckAndUpdateConfigMapConcurrently}} fails on AZP with

{code}
2021-12-10T01:11:11.4908955Z Dec 10 01:11:11 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.186 s <<< FAILURE! - in org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClientITCase
2021-12-10T01:11:11.4920172Z Dec 10 01:11:11 [ERROR] testCheckAndUpdateConfigMapConcurrently(org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClientITCase)  Time elapsed: 0.637 s  <<< ERROR!
2021-12-10T01:11:11.4921592Z Dec 10 01:11:11 java.util.concurrent.ExecutionException: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Could not complete the operation. Number of retries has been exhausted.
2021-12-10T01:11:11.4925669Z Dec 10 01:11:11 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2021-12-10T01:11:11.4926769Z Dec 10 01:11:11 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2021-12-10T01:11:11.4927867Z Dec 10 01:11:11 	at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClientITCase.testCheckAndUpdateConfigMapConcurrently(Fabric8FlinkKubeClientITCase.java:144)
2021-12-10T01:11:11.4928904Z Dec 10 01:11:11 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-12-10T01:11:11.4929769Z Dec 10 01:11:11 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-12-10T01:11:11.4930729Z Dec 10 01:11:11 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-12-10T01:11:11.4931607Z Dec 10 01:11:11 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-12-10T01:11:11.4932485Z Dec 10 01:11:11 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-12-10T01:11:11.4933449Z Dec 10 01:11:11 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-12-10T01:11:11.4934395Z Dec 10 01:11:11 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-12-10T01:11:11.4935330Z Dec 10 01:11:11 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-12-10T01:11:11.4937971Z Dec 10 01:11:11 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-12-10T01:11:11.4939315Z Dec 10 01:11:11 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-12-10T01:11:11.4940050Z Dec 10 01:11:11 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-12-10T01:11:11.4940711Z Dec 10 01:11:11 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-12-10T01:11:11.4941320Z Dec 10 01:11:11 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-12-10T01:11:11.4941928Z Dec 10 01:11:11 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-12-10T01:11:11.4942797Z Dec 10 01:11:11 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-12-10T01:11:11.4943519Z Dec 10 01:11:11 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-12-10T01:11:11.4944180Z Dec 10 01:11:11 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-12-10T01:11:11.4944815Z Dec 10 01:11:11 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-12-10T01:11:11.4945461Z Dec 10 01:11:11 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-12-10T01:11:11.4946107Z Dec 10 01:11:11 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-12-10T01:11:11.4946749Z Dec 10 01:11:11 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-12-10T01:11:11.4947402Z Dec 10 01:11:11 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-12-10T01:11:11.4948030Z Dec 10 01:11:11 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-12-10T01:11:11.4948625Z Dec 10 01:11:11 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-12-10T01:11:11.4949291Z Dec 10 01:11:11 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-12-10T01:11:11.4951248Z Dec 10 01:11:11 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-12-10T01:11:11.4951968Z Dec 10 01:11:11 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-12-10T01:11:11.4952643Z Dec 10 01:11:11 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-12-10T01:11:11.4953346Z Dec 10 01:11:11 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-12-10T01:11:11.4954066Z Dec 10 01:11:11 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-12-10T01:11:11.4954749Z Dec 10 01:11:11 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-12-10T01:11:11.4955371Z Dec 10 01:11:11 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-12-10T01:11:11.4956126Z Dec 10 01:11:11 Caused by: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Could not complete the operation. Number of retries has been exhausted.
2021-12-10T01:11:11.4956924Z Dec 10 01:11:11 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperation$1(FutureUtils.java:183)
2021-12-10T01:11:11.4958988Z Dec 10 01:11:11 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2021-12-10T01:11:11.4960177Z Dec 10 01:11:11 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2021-12-10T01:11:11.4961252Z Dec 10 01:11:11 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2021-12-10T01:11:11.4961973Z Dec 10 01:11:11 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2021-12-10T01:11:11.4962692Z Dec 10 01:11:11 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2021-12-10T01:11:11.4963293Z Dec 10 01:11:11 	at java.lang.Thread.run(Thread.java:748)
2021-12-10T01:11:11.4968062Z Dec 10 01:11:11 Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.persistence.PossibleInconsistentStateException: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: PUT at: https://10.1.0.82:8443/api/v1/namespaces/default/configmaps/test-config-map. Message: Operation cannot be fulfilled on configmaps ""test-config-map"": the object has been modified; please apply your changes to the latest version and try again. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=null, kind=configmaps, name=test-config-map, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=Operation cannot be fulfilled on configmaps ""test-config-map"": the object has been modified; please apply your changes to the latest version and try again, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=Conflict, status=Failure, additionalProperties={}).
2021-12-10T01:11:11.4971865Z Dec 10 01:11:11 	at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$null$7(Fabric8FlinkKubeClient.java:316)
2021-12-10T01:11:11.4972946Z Dec 10 01:11:11 	at java.util.Optional.map(Optional.java:215)
2021-12-10T01:11:11.4973764Z Dec 10 01:11:11 	at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$null$8(Fabric8FlinkKubeClient.java:290)
2021-12-10T01:11:11.4974416Z Dec 10 01:11:11 	at java.util.Optional.map(Optional.java:215)
2021-12-10T01:11:11.4975062Z Dec 10 01:11:11 	at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$null$10(Fabric8FlinkKubeClient.java:287)
2021-12-10T01:11:11.4975797Z Dec 10 01:11:11 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
2021-12-10T01:11:11.4976309Z Dec 10 01:11:11 	... 3 more
2021-12-10T01:11:11.4980247Z Dec 10 01:11:11 Caused by: org.apache.flink.runtime.persistence.PossibleInconsistentStateException: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: PUT at: https://10.1.0.82:8443/api/v1/namespaces/default/configmaps/test-config-map. Message: Operation cannot be fulfilled on configmaps ""test-config-map"": the object has been modified; please apply your changes to the latest version and try again. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=null, kind=configmaps, name=test-config-map, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=Operation cannot be fulfilled on configmaps ""test-config-map"": the object has been modified; please apply your changes to the latest version and try again, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=Conflict, status=Failure, additionalProperties={}).
2021-12-10T01:11:11.4983161Z Dec 10 01:11:11 	... 9 more
2021-12-10T01:11:11.4986826Z Dec 10 01:11:11 Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: PUT at: https://10.1.0.82:8443/api/v1/namespaces/default/configmaps/test-config-map. Message: Operation cannot be fulfilled on configmaps ""test-config-map"": the object has been modified; please apply your changes to the latest version and try again. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=null, kind=configmaps, name=test-config-map, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=Operation cannot be fulfilled on configmaps ""test-config-map"": the object has been modified; please apply your changes to the latest version and try again, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=Conflict, status=Failure, additionalProperties={}).
2021-12-10T01:11:11.4989960Z Dec 10 01:11:11 	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:568)
2021-12-10T01:11:11.4990726Z Dec 10 01:11:11 	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:507)
2021-12-10T01:11:11.4991573Z Dec 10 01:11:11 	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:471)
2021-12-10T01:11:11.4992315Z Dec 10 01:11:11 	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:430)
2021-12-10T01:11:11.4993041Z Dec 10 01:11:11 	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleReplace(OperationSupport.java:289)
2021-12-10T01:11:11.4993777Z Dec 10 01:11:11 	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleReplace(OperationSupport.java:269)
2021-12-10T01:11:11.4994494Z Dec 10 01:11:11 	at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleReplace(BaseOperation.java:820)
2021-12-10T01:11:11.4995314Z Dec 10 01:11:11 	at io.fabric8.kubernetes.client.dsl.base.HasMetadataOperation.lambda$replace$1(HasMetadataOperation.java:86)
2021-12-10T01:11:11.4996033Z Dec 10 01:11:11 	at io.fabric8.kubernetes.api.model.DoneableConfigMap.done(DoneableConfigMap.java:26)
2021-12-10T01:11:11.5028417Z Dec 10 01:11:11 	at io.fabric8.kubernetes.api.model.DoneableConfigMap.done(DoneableConfigMap.java:5)
2021-12-10T01:11:11.5029177Z Dec 10 01:11:11 	at io.fabric8.kubernetes.client.dsl.base.HasMetadataOperation.replace(HasMetadataOperation.java:92)
2021-12-10T01:11:11.5030107Z Dec 10 01:11:11 	at io.fabric8.kubernetes.client.dsl.base.HasMetadataOperation.replace(HasMetadataOperation.java:36)
2021-12-10T01:11:11.5030856Z Dec 10 01:11:11 	at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$null$7(Fabric8FlinkKubeClient.java:301)
2021-12-10T01:11:11.5031427Z Dec 10 01:11:11 	... 8 more
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27917&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=3425d8ba-5f03-540a-c64b-51b8481bf7d6&l=3029",,gaoyunhaii,trohrmann,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 07 08:38:07 UTC 2022,,,,,,,,,,"0|z0xji8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/22 07:25;wangyang0918;I will take a look on this ticket.;;;","22/Feb/22 10:08;wangyang0918;We do not have bugs in the current implementation. The test failed because of ConfigMap update conflicts. I think we could increase the retry attempt for the testing.

BTW, this is not a major issue in production since we are not updating the HA ConfigMaps concurrently so frequently.;;;","02/Mar/22 06:42;gaoyunhaii;Merged on master via 152ad4fc14920372076c0004793c179141ae10c7. If the issue do not occur after some period I'll close this issue~;;;","07/Mar/22 08:38;gaoyunhaii;I'll first close this issue since it does not reproduced for some days. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink iceberg source reading array types fail with Cast Exception,FLINK-25238,13416427,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,sr.praneeth@gmail.com,sr.praneeth@gmail.com,10/Dec/21 03:06,27/Jul/22 12:12,13/Jul/23 08:12,06/Apr/22 07:21,1.13.2,1.14.4,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.16.0,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"I have a stream with iceberg table as a source. I have few columns of array types in the table. 

I try to read using iceberg connector. 

Flink Version : 1.13.2

Iceberg Flink Version: 0.12.1

 

I see the error as below.

java.lang.ClassCastException: class org.apache.iceberg.flink.data.FlinkParquetReaders$ReusableArrayData cannot be cast to class org.apache.flink.table.data.ColumnarArrayData (org.apache.iceberg.flink.data.FlinkParquetReaders$ReusableArrayData and org.apache.flink.table.data.ColumnarArrayData are in unnamed module of loader 'app')
    at org.apache.flink.table.runtime.typeutils.ArrayDataSerializer.copy(ArrayDataSerializer.java:90)
    at org.apache.flink.table.runtime.typeutils.ArrayDataSerializer.copy(ArrayDataSerializer.java:47)
    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copyRowData(RowDataSerializer.java:170)
    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:131)
    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:48)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:69)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:50)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:28)
    at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:317)
    at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:411)
    at org.apache.iceberg.flink.source.StreamingReaderOperator.processSplits(StreamingReaderOperator.java:155)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
    at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:359)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:323)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:202)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:681)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:636)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:647)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:620)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
    at java.base/java.lang.Thread.run(Thread.java:834)

 

Could be same issue as https://issues.apache.org/jira/browse/FLINK-21247 except it happening for another type.

I see that Iceberg use custom types other than the types from 

org.apache.flink.table.data like

org.apache.iceberg.flink.data.FlinkParquetReaders.ReusableArrayData and these types are not handled in org.apache.flink.table.runtime.typeutils.ArrayDataSerializer

!Screen Shot 2021-12-09 at 6.58.56 PM.png!

 Just to try I changed the above code to handle the iceberg type as a binary Array and built it locally and used in my application and that worked. 

 

!Screen Shot 2021-12-09 at 7.04.10 PM.png!

Not sure if this is already handled in some newer versions. ",,chengbing.liu,gyfora,martijnvisser,sjwiesman,sr.praneeth@gmail.com,yittg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28214,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/21 03:02;sr.praneeth@gmail.com;Screen Shot 2021-12-09 at 6.58.56 PM.png;https://issues.apache.org/jira/secure/attachment/13037223/Screen+Shot+2021-12-09+at+6.58.56+PM.png","10/Dec/21 03:04;sr.praneeth@gmail.com;Screen Shot 2021-12-09 at 7.04.10 PM.png;https://issues.apache.org/jira/secure/attachment/13037222/Screen+Shot+2021-12-09+at+7.04.10+PM.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 27 12:12:20 UTC 2022,,,,,,,,,,"0|z0xj4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/21 16:03;sjwiesman;The Iceberg Sink is maintained by the Apache Iceberg community. I am going to close this ticket and recommend you reach out to them for assistance. ;;;","01/Apr/22 03:05;yittg;[~sjwiesman] Shall we reopen this issue. The similar issue for MapData is fixed in FLINK-21247, and the detail is like that been explained in https://issues.apache.org/jira/browse/FLINK-21247?focusedCommentId=17278483&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17278483

 

cc [~openinx] , [~lzljs3620320] ;;;","05/Apr/22 10:47;gyfora;Reopening this. [~sjwiesman] this is a bug in Flink, not in iceberg;;;","06/Apr/22 07:00;yittg;[~gyfora] Thanks, would you mind having a look at the [fix #19316|https://github.com/apache/flink/pull/19316];;;","06/Apr/22 07:20;gyfora;Perfect, thank you;;;","06/Apr/22 07:21;gyfora;merged to master: 99c0438253efaf0a9626a807ce2b882402a367d1;;;","06/Apr/22 07:22;gyfora;[~yittg] can you also backport this to the release-1.15 branch and open a PR there too?;;;","06/Apr/22 07:33;yittg;[~gyfora], glad to do it. I think we should backport it to both release-1.14 and release-1.15?;;;","06/Apr/22 07:40;gyfora;sounds good!;;;","07/Apr/22 01:47;yittg;Hi [~gyfora], thanks for helping merging these fixes.

Since release-1.15 is not released officially, I'm not sure whether it is proper to add the fix to 1.13 due to the upgrade policy. What's your opinion? Will there be another fixing release for version 1.13?;;;","07/Apr/22 06:32;gyfora;[~yittg] I dont really know for sure :) If you have time we can always backport it, no harm in that;;;","07/Apr/22 09:24;martijnvisser;[~yittg] I don't think there will be another 1.13 release. ;;;","11/Apr/22 03:28;yittg;Thanks [~martijnvisser], i'll close the PR for 1.13.;;;","27/Jul/22 12:12;chengbing.liu;[~gyfora] This was already fixed in 1.15.0, the following versions could be corrected to prevent confusion, thanks.
 * *Affects Version/s:* 1.13.2, 1.14.4, 1.15.0

 * *Fix Version/s:* 1.16.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Comparing the equality of the same (boxed) numeric values returns false,FLINK-25227,13416200,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,matriv,TsReaper,TsReaper,09/Dec/21 06:26,30/Mar/22 13:44,13/Jul/23 08:12,30/Mar/22 13:44,1.12.5,1.13.3,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.7,1.14.5,1.15.0,1.16.0,,Table SQL / Runtime,,,,,0,pull-request-available,stale-assigned,,,"Add the following test case to {{TableEnvironmentITCase}} to reproduce this bug.

{code:scala}
@Test
def myTest(): Unit = {
  val data = Seq(
    Row.of(
      java.lang.Integer.valueOf(1000),
      java.lang.Integer.valueOf(2000),
      java.lang.Integer.valueOf(1000),
      java.lang.Integer.valueOf(2000))
  )

  tEnv.executeSql(
    s""""""
       |create table T (
       |  a int,
       |  b int,
       |  c int,
       |  d int
       |) with (
       |  'connector' = 'values',
       |  'bounded' = 'true',
       |  'data-id' = '${TestValuesTableFactory.registerData(data)}'
       |)
       |"""""".stripMargin)

  tEnv.executeSql(""select greatest(a, b) = greatest(c, d) from T"").print()
}
{code}

The result is false, which is obviously incorrect.

This is caused by the generated java code:
{code:java}
public class StreamExecCalc$8 extends org.apache.flink.table.runtime.operators.TableStreamOperator
        implements org.apache.flink.streaming.api.operators.OneInputStreamOperator {

    private final Object[] references;
    org.apache.flink.table.data.BoxedWrapperRowData out =
            new org.apache.flink.table.data.BoxedWrapperRowData(1);
    private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement =
            new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);

    public StreamExecCalc$8(
            Object[] references,
            org.apache.flink.streaming.runtime.tasks.StreamTask task,
            org.apache.flink.streaming.api.graph.StreamConfig config,
            org.apache.flink.streaming.api.operators.Output output,
            org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService)
            throws Exception {
        this.references = references;

        this.setup(task, config, output);
        if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {
            ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)
                    .setProcessingTimeService(processingTimeService);
        }
    }

    @Override
    public void open() throws Exception {
        super.open();
    }

    @Override
    public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element)
            throws Exception {
        org.apache.flink.table.data.RowData in1 =
                (org.apache.flink.table.data.RowData) element.getValue();

        int field$0;
        boolean isNull$0;
        int field$1;
        boolean isNull$1;
        int field$3;
        boolean isNull$3;
        int field$4;
        boolean isNull$4;
        boolean isNull$6;
        boolean result$7;

        isNull$3 = in1.isNullAt(2);
        field$3 = -1;
        if (!isNull$3) {
            field$3 = in1.getInt(2);
        }
        isNull$0 = in1.isNullAt(0);
        field$0 = -1;
        if (!isNull$0) {
            field$0 = in1.getInt(0);
        }
        isNull$1 = in1.isNullAt(1);
        field$1 = -1;
        if (!isNull$1) {
            field$1 = in1.getInt(1);
        }
        isNull$4 = in1.isNullAt(3);
        field$4 = -1;
        if (!isNull$4) {
            field$4 = in1.getInt(3);
        }

        out.setRowKind(in1.getRowKind());

        java.lang.Integer result$2 = field$0;
        boolean nullTerm$2 = false;

        if (!nullTerm$2) {
            java.lang.Integer cur$2 = field$0;
            if (isNull$0) {
                nullTerm$2 = true;
            } else {
                int compareResult = result$2.compareTo(cur$2);
                if ((true && compareResult < 0) || (compareResult > 0 && !true)) {
                    result$2 = cur$2;
                }
            }
        }

        if (!nullTerm$2) {
            java.lang.Integer cur$2 = field$1;
            if (isNull$1) {
                nullTerm$2 = true;
            } else {
                int compareResult = result$2.compareTo(cur$2);
                if ((true && compareResult < 0) || (compareResult > 0 && !true)) {
                    result$2 = cur$2;
                }
            }
        }

        if (nullTerm$2) {
            result$2 = null;
        }

        java.lang.Integer result$5 = field$3;
        boolean nullTerm$5 = false;

        if (!nullTerm$5) {
            java.lang.Integer cur$5 = field$3;
            if (isNull$3) {
                nullTerm$5 = true;
            } else {
                int compareResult = result$5.compareTo(cur$5);
                if ((true && compareResult < 0) || (compareResult > 0 && !true)) {
                    result$5 = cur$5;
                }
            }
        }

        if (!nullTerm$5) {
            java.lang.Integer cur$5 = field$4;
            if (isNull$4) {
                nullTerm$5 = true;
            } else {
                int compareResult = result$5.compareTo(cur$5);
                if ((true && compareResult < 0) || (compareResult > 0 && !true)) {
                    result$5 = cur$5;
                }
            }
        }

        if (nullTerm$5) {
            result$5 = null;
        }

        isNull$6 = nullTerm$2 || nullTerm$5;
        result$7 = false;
        if (!isNull$6) {

            result$7 = result$2 == result$5;
        }

        if (isNull$6) {
            out.setNullAt(0);
        } else {
            out.setBoolean(0, result$7);
        }

        output.collect(outElement.replace(out));
    }

    @Override
    public void close() throws Exception {
        super.close();
    }
}
{code}

You can see that line 137 compares two boxed Integer types with {{==}} instead of {{.equals}}, which causes this problem.

In older Flink versions where the return types of {{cast}} functions are also boxed types, casting strings to numeric values are also affected by this bug.

Currently for a quick fix we can rewrite the generated code. But for a long term solution we shouldn't use boxed types as internal data structures.",,libenchao,lzljs3620320,martijnvisser,matriv,neighborhood,RocMarshal,slinkydeveloper,trohrmann,TsReaper,tunyu,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 30 13:43:38 UTC 2022,,,,,,,,,,"0|z0xhqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/21 15:25;slinkydeveloper;Hi [~TsReaper], I don't understand why we need this fix. As you state here:

> But for a long term solution we shouldn't use boxed types as internal data structures.

Which is already the case, so why we need this ""quick fix""? Nowhere in the codegen codebase there should be any logic generating a boxed type as result of an operation, so why should we support this specifically for equals in this specific case?;;;","15/Dec/21 01:27;TsReaper;Hi [~slinkydeveloper].

We're still generating boxed types. For an example see {{ScalarOperatorGens#generateGreatestLeast}}, where {{resultTypeTerm}} is a boxed type. You can also try out my PR by removing the fix and running the added tests to see why this is necessary. Besides, we also need to fix older Flink versions.;;;","15/Dec/21 07:22;slinkydeveloper;[~TsReaper] but then I suggest we focus on hardening the code generator fixing these functions that generate boxed types, more than introducing new functions that generate boxed types that we'll need to fix later. Can you come up with a list of such functions?

For previous versions I agree to just push the fix you proposed in https://github.com/apache/flink/pull/18076.;;;","15/Dec/21 15:04;matriv;I also agree to fix the issue with the boxed types for *master* (future releases) and only apply this fix to versions <= 1.14;;;","30/Dec/21 07:07;TsReaper;Hi [~matriv] and [~slinkydeveloper] ! I also agree it is reasonable to fix these in Flink <= 1.14. I'll recreate the PRs.;;;","14/Feb/22 02:48;lzljs3620320;release-1.13: b0d0a0026fbe58ca87a422d87999cb1d99ce5127

release-1.14: 4b5e23236a4f46fe93bece00d37a9ed1d5a886ac;;;","27/Mar/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","30/Mar/22 09:23;martijnvisser;Fixed in master: 89cdc6e01f291e9ce5c1dfeb4bd883809e1eeaf5;;;","30/Mar/22 13:43;martijnvisser;Fixed in release-1.15: e0c8df17f3b0469affeb4c07be5b7c03d3a94333;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ElasticsearchWriterITCase fails on AZP,FLINK-25223,13416105,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,alexanderpreuss,trohrmann,trohrmann,08/Dec/21 15:58,11/Jan/22 10:55,13/Jul/23 08:12,17/Dec/21 07:50,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Connectors / ElasticSearch,,,,,0,pull-request-available,test-stability,,,"The {{ElasticsearchWriterITCase}} fails on AZP because

{code}
2021-12-08T13:56:59.5449851Z Dec 08 13:56:59 [ERROR] org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriterITCase  Time elapsed: 171.046 s  <<< ERROR!
2021-12-08T13:56:59.5450680Z Dec 08 13:56:59 org.testcontainers.containers.ContainerLaunchException: Container startup failed
2021-12-08T13:56:59.5451652Z Dec 08 13:56:59 	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:336)
2021-12-08T13:56:59.5452677Z Dec 08 13:56:59 	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:317)
2021-12-08T13:56:59.5453637Z Dec 08 13:56:59 	at org.testcontainers.junit.jupiter.TestcontainersExtension$StoreAdapter.start(TestcontainersExtension.java:242)
2021-12-08T13:56:59.5454757Z Dec 08 13:56:59 	at org.testcontainers.junit.jupiter.TestcontainersExtension$StoreAdapter.access$200(TestcontainersExtension.java:229)
2021-12-08T13:56:59.5455946Z Dec 08 13:56:59 	at org.testcontainers.junit.jupiter.TestcontainersExtension.lambda$null$1(TestcontainersExtension.java:59)
2021-12-08T13:56:59.5457322Z Dec 08 13:56:59 	at org.junit.jupiter.engine.execution.ExtensionValuesStore.lambda$getOrComputeIfAbsent$4(ExtensionValuesStore.java:86)
2021-12-08T13:56:59.5458571Z Dec 08 13:56:59 	at org.junit.jupiter.engine.execution.ExtensionValuesStore$MemoizingSupplier.computeValue(ExtensionValuesStore.java:223)
2021-12-08T13:56:59.5459771Z Dec 08 13:56:59 	at org.junit.jupiter.engine.execution.ExtensionValuesStore$MemoizingSupplier.get(ExtensionValuesStore.java:211)
2021-12-08T13:56:59.5460693Z Dec 08 13:56:59 	at org.junit.jupiter.engine.execution.ExtensionValuesStore$StoredValue.evaluate(ExtensionValuesStore.java:191)
2021-12-08T13:56:59.5461437Z Dec 08 13:56:59 	at org.junit.jupiter.engine.execution.ExtensionValuesStore$StoredValue.access$100(ExtensionValuesStore.java:171)
2021-12-08T13:56:59.5462198Z Dec 08 13:56:59 	at org.junit.jupiter.engine.execution.ExtensionValuesStore.getOrComputeIfAbsent(ExtensionValuesStore.java:89)
2021-12-08T13:56:59.5467999Z Dec 08 13:56:59 	at org.junit.jupiter.engine.execution.NamespaceAwareStore.getOrComputeIfAbsent(NamespaceAwareStore.java:53)
2021-12-08T13:56:59.5468791Z Dec 08 13:56:59 	at org.testcontainers.junit.jupiter.TestcontainersExtension.lambda$beforeAll$2(TestcontainersExtension.java:59)
2021-12-08T13:56:59.5469436Z Dec 08 13:56:59 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2021-12-08T13:56:59.5470058Z Dec 08 13:56:59 	at org.testcontainers.junit.jupiter.TestcontainersExtension.beforeAll(TestcontainersExtension.java:59)
2021-12-08T13:56:59.5470846Z Dec 08 13:56:59 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeBeforeAllCallbacks$10(ClassBasedTestDescriptor.java:381)
2021-12-08T13:56:59.5471641Z Dec 08 13:56:59 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-08T13:56:59.5472403Z Dec 08 13:56:59 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeBeforeAllCallbacks(ClassBasedTestDescriptor.java:381)
2021-12-08T13:56:59.5473190Z Dec 08 13:56:59 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:205)
2021-12-08T13:56:59.5474001Z Dec 08 13:56:59 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:80)
2021-12-08T13:56:59.5474759Z Dec 08 13:56:59 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:148)
2021-12-08T13:56:59.5475833Z Dec 08 13:56:59 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-08T13:56:59.5476739Z Dec 08 13:56:59 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2021-12-08T13:56:59.5477520Z Dec 08 13:56:59 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-12-08T13:56:59.5478227Z Dec 08 13:56:59 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2021-12-08T13:56:59.5479190Z Dec 08 13:56:59 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-08T13:56:59.5479936Z Dec 08 13:56:59 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2021-12-08T13:56:59.5480635Z Dec 08 13:56:59 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2021-12-08T13:56:59.5481244Z Dec 08 13:56:59 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2021-12-08T13:56:59.5481970Z Dec 08 13:56:59 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
2021-12-08T13:56:59.5482835Z Dec 08 13:56:59 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2021-12-08T13:56:59.5483652Z Dec 08 13:56:59 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-08T13:56:59.5484536Z Dec 08 13:56:59 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2021-12-08T13:56:59.5485251Z Dec 08 13:56:59 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-12-08T13:56:59.5485951Z Dec 08 13:56:59 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2021-12-08T13:56:59.5486685Z Dec 08 13:56:59 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-08T13:56:59.5487503Z Dec 08 13:56:59 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2021-12-08T13:56:59.5488224Z Dec 08 13:56:59 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2021-12-08T13:56:59.5489176Z Dec 08 13:56:59 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
2021-12-08T13:56:59.5490487Z Dec 08 13:56:59 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
2021-12-08T13:56:59.5491261Z Dec 08 13:56:59 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
2021-12-08T13:56:59.5492016Z Dec 08 13:56:59 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2021-12-08T13:56:59.5492767Z Dec 08 13:56:59 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2021-12-08T13:56:59.5493583Z Dec 08 13:56:59 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2021-12-08T13:56:59.5494579Z Dec 08 13:56:59 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2021-12-08T13:56:59.5495363Z Dec 08 13:56:59 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2021-12-08T13:56:59.5496073Z Dec 08 13:56:59 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2021-12-08T13:56:59.5496732Z Dec 08 13:56:59 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2021-12-08T13:56:59.5497536Z Dec 08 13:56:59 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2021-12-08T13:56:59.5498300Z Dec 08 13:56:59 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2021-12-08T13:56:59.5499030Z Dec 08 13:56:59 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2021-12-08T13:56:59.5499775Z Dec 08 13:56:59 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2021-12-08T13:56:59.5500530Z Dec 08 13:56:59 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2021-12-08T13:56:59.5501234Z Dec 08 13:56:59 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2021-12-08T13:56:59.5501891Z Dec 08 13:56:59 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2021-12-08T13:56:59.5502518Z Dec 08 13:56:59 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2021-12-08T13:56:59.5503134Z Dec 08 13:56:59 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2021-12-08T13:56:59.5503829Z Dec 08 13:56:59 Caused by: org.rnorth.ducttape.RetryCountExceededException: Retry limit hit with exception
2021-12-08T13:56:59.5504493Z Dec 08 13:56:59 	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:88)
2021-12-08T13:56:59.5505161Z Dec 08 13:56:59 	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:329)
2021-12-08T13:56:59.5505676Z Dec 08 13:56:59 	... 56 more
2021-12-08T13:56:59.5506178Z Dec 08 13:56:59 Caused by: org.testcontainers.containers.ContainerLaunchException: Could not create/start container
2021-12-08T13:56:59.5506853Z Dec 08 13:56:59 	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:525)
2021-12-08T13:56:59.5507605Z Dec 08 13:56:59 	at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:331)
2021-12-08T13:56:59.5508283Z Dec 08 13:56:59 	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)
2021-12-08T13:56:59.5508787Z Dec 08 13:56:59 	... 57 more
2021-12-08T13:56:59.5509245Z Dec 08 13:56:59 Caused by: java.lang.IllegalStateException: Container exited with code 137
2021-12-08T13:56:59.5509971Z Dec 08 13:56:59 	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:497)
2021-12-08T13:56:59.5510562Z Dec 08 13:56:59 	... 59 more
2021-12-08T13:56:59.5510874Z Dec 08 13:56:59 
2021-12-08T13:56:59.9256152Z Dec 08 13:56:59 [INFO] 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27817&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11865",,alexanderpreuss,gaoyunhaii,martijnvisser,trohrmann,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18356,FLINK-24341,FLINK-24794,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 11 10:54:08 UTC 2022,,,,,,,,,,"0|z0xh5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/21 16:04;martijnvisser;CC [~alexanderpreuss];;;","08/Dec/21 16:04;chesnay;How much memory does a test container actually consume? Does it depend on the specific container?;;;","09/Dec/21 00:41;wanglijie;One more instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27818&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d;;;","09/Dec/21 09:13;alexanderpreuss;[~chesnay] digging around I found that there is no memory limit applied by default to containers started by Testcontainers.
I also checked the logs and saw this:
{code:java}
13:54:59,435 [                main] INFO  org.testcontainers.containers.wait.strategy.HttpWaitStrategy [] - /sad_ellis: Waiting for 120 seconds for URL: http://172.17.0.1:45862/ (where port 45862 maps to container port 9200)
13:56:59,444 [                main] ERROR 🐳 [docker.elastic.co/elasticsearch/elasticsearch:7.15.2]    [] - Could not start container {code}
Could it also be that the ports are not freed up?;;;","09/Dec/21 09:20;chesnay;I would expect testcontainers/docker to be smart enough that they always pick a free port without any potential for race conditions.

Based on the exit code the contained failed with an out-of-memory error.;;;","09/Dec/21 09:58;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27824&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27826&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11865
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27830&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11468
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27840&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11865;;;","09/Dec/21 09:58;trohrmann;[~alexanderpreuss] can we disable this test until we have figured out what is causing the problem? It causes our builds to fail quite consistently.;;;","09/Dec/21 10:00;alexanderpreuss;[~trohrmann] yeah, I saw that too, will submit a PR for disabling it for the time being;;;","09/Dec/21 10:03;trohrmann;It seems also to fail for {{Elasticsearch7DynamicSinkITCase}} and {{ElasticsearchSinkITCase}}.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27840&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=12412;;;","09/Dec/21 10:03;trohrmann;Great, thank you [~alexanderpreuss]!;;;","09/Dec/21 10:28;alexanderpreuss;[~trohrmann]  PR available;;;","09/Dec/21 16:48;trohrmann;Tests have been disabled via 39e815fb278626281d03bfc9c57ead8331d8499f.;;;","13/Dec/21 15:52;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28011&view=results] 

Hi, since it seems the OOM error also failed in connector module in release-1.13, if there are no concerns perhaps I could also pick the fix to 1.14 and 1.13?;;;","13/Dec/21 18:05;chesnay;[~gaoyunhaii] It would be great if you could prepare the backports for 1.13/1.14. ;;;","17/Dec/21 07:50;arvid;Reactivated tests and provided memory limit in master as 98a74baeaf45350fb665558cdbed5fb72fb310dd.;;;","17/Dec/21 07:52;arvid;[~alexanderpreuss] could you please double-check if we need a backport? The related docker image was only used in master afaik but there seems to be similar issues on old release branches. So maybe we also need to apply a similar fix to that legacy code.;;;","17/Dec/21 09:27;chesnay;??The related docker image was only used in master afaik??

1.13/1.14 were already using ES docker images.;;;","17/Dec/21 09:38;alexanderpreuss;[~chesnay] I believe it only started happening on master because we increased the docker image version there, so it might not be an issue on older releases. Can anyone provide failing builds from older releases? The failing one mentioned by [~gaoyunhaii]  looks to me like it is not related to Elasticsearch but rather docker in general;;;","11/Jan/22 10:54;chesnay;I can confirm that the ES version we previously used (7.5.1) only used 1g by default. This does make me wonder though whether we shouldn't lower the memory limit to 1g from the current 2g.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove NetworkFailureProxy used for Kafka connector tests,FLINK-25222,13416053,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fpaul,fpaul,fpaul,08/Dec/21 13:51,16/Dec/21 00:58,13/Jul/23 08:12,15/Dec/21 13:07,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Connectors / Kafka,Test Infrastructure,,,,0,pull-request-available,,,,"Recently the number of Kafka connector tests either hitting a timeout due to blocked networking or corrupted network responses increased significantly. 

We think it is caused by our custom network failure implementation since all the tests are for the legacy FlinkKafkaProducer or FlinkKafkaConsumer we can safely remove them because we will not add more features to this connector, to increase the overall stability.",,fpaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 15 13:07:01 UTC 2021,,,,,,,,,,"0|z0xgu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/21 08:57;fpaul;Merged to master: 5f5cab2f9a1175fa396833299c313c2de6d1dae3;;;","15/Dec/21 13:07;fpaul;Merged to release-1.14: 3de8b3d1e023a49220e33fdbcef6061bc048bb5b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ISODOW for timestamps with timezones fails with Invalid start unit,FLINK-25215,13415815,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,07/Dec/21 12:32,20/Dec/21 16:28,13/Jul/23 08:12,20/Dec/21 16:28,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Table SQL / API,,,,,0,pull-request-available,,,,"{{DECADE}} returns wrong result for timestamps with timezones and
{{ISODOW}}, {{ISOYEAR}} fail for timestamps with timezones like 
{noformat}
[ERROR] Could not execute SQL statement. Reason:
java.lang.IllegalArgumentException: Invalid start unit.
	at org.apache.flink.table.utils.DateTimeUtils.getFactor(DateTimeUtils.java:1213)
	at org.apache.flink.table.utils.DateTimeUtils.convertExtract(DateTimeUtils.java:1164)
	at org.apache.flink.table.utils.DateTimeUtils.extractFromTimestamp(DateTimeUtils.java:993)
	at StreamExecCalc$13.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:330)

{noformat}",,Sergey Nuyanzin,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 20 16:28:22 UTC 2021,,,,,,,,,,"0|z0xfd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/21 16:28;twalthr;Fixed in master: 382261232649625146608cc526fce7fc25af0d9a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"StreamEdges are not unique in self-union, which blocks propagation of watermarks",FLINK-25199,13415639,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pnowojski,twalthr,twalthr,06/Dec/21 17:07,07/Feb/22 10:30,13/Jul/23 08:12,21/Jan/22 15:12,1.12.7,1.13.5,1.14.3,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.4,1.15.0,,,Runtime / Task,,,,,0,pull-request-available,,,,"It seems {{fromValues}} that generates multiple rows does not emit any watermarks:

{code}
        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);

        Table inputTable =
                tEnv.fromValues(
                        DataTypes.ROW(
                                DataTypes.FIELD(""weight"", DataTypes.DOUBLE()),
                                DataTypes.FIELD(""f0"", DataTypes.STRING()),
                                DataTypes.FIELD(""f1"", DataTypes.DOUBLE()),
                                DataTypes.FIELD(""f2"", DataTypes.DOUBLE()),
                                DataTypes.FIELD(""f3"", DataTypes.DOUBLE()),
                                DataTypes.FIELD(""f4"", DataTypes.INT()),
                                DataTypes.FIELD(""label"", DataTypes.STRING())),
                        Row.of(1., ""a"", 1., 1., 1., 2, ""l1""),
                        Row.of(1., ""a"", 1., 1., 1., 2, ""l1""));

        DataStream<Row> input = tEnv.toDataStream(inputTable);
{code}

{{fromValues(1, 2, 3)}} or {{fromValues}} with only 1 row works correctly.",,matriv,pnowojski,RocMarshal,tunyu,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 07 10:30:39 UTC 2022,,,,,,,,,,"0|z0xea8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/21 10:26;matriv;This issue is not Table/SQL related but originates from self {*}union{*}ing a datastream, so one can reproduce it with:

 
{noformat}
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
DataStream<Integer> dataStream1 = env.fromElements(1, 2, 3).setParallelism(1);

// add a printout statement to print the emitted watermark
dataStream1.union(dataStream1).print();
// or  use .addSink(new DiscardingSink<>()); and debug to check that the writeWatermark() on the DiscardingSink is not called
env.execute();{noformat}
If instead of the ""self"" union we union 2 separate datastreams like:

 
{noformat}
DataStream<Integer> dataStream1 = env.fromElements(1, 2, 3).setParallelism(1);
DataStream<Integer> dataStream2 = dataStream1.map(i -> i + 1).setParallelism(1);

dataStream1.union(dataStream1).print(); //addSink(new DiscardingSink<>());
env.execute();{noformat}
Then the watermark is emitted correctly.

 

 

From some debugging in *StatusWatermarkValve#findAndOutputNewMinWatermarkAcrossAlignedChannels*

The following is false:
{noformat}
if (hasAlignedChannels && newMinWatermark > lastOutputWatermark) {
    lastOutputWatermark = newMinWatermark;
    output.emitWatermark(new Watermark(lastOutputWatermark));
}{noformat}
 

 

Because there are 2 channels involved and in the for loop above that code ^^:


{noformat}
for (InputChannelStatus channelStatus : channelStatuses) {
    if (channelStatus.isWatermarkAligned) {
        hasAlignedChannels = true;
        newMinWatermark = Math.min(channelStatus.watermark, newMinWatermark);
    }
}{noformat}
One of the channels is updated with the watermark set to *Long.MAX_VALUE* but the other channel still has the initial value of *Long.MIN_VALUE* so the total *Math.min* gives out *Long.MIN_VALUE.*

 

That's because the  {*}StatusWatermarkValve#{*}{*}inputWatermark{*} which updates the channel's watermark in line *95* is always called from *AbstractStreamTaskNetworkInput#processElement* with

the same *lastChannel* which is just one of the 2 channels used in this union case.

 ;;;","20/Jan/22 10:31;pnowojski;The problem was quite strange. If there was a node that was self-unioned with itself, it was creating a situation with two identical StreamEdges. Both with the same partitioning, from the same source node to the same target node. 

This was causing issues when constructing output collectors and picking the correct RecordWriters, as StreamTask was not able to uniquely identify given StreamEdge and was assigning the same RecordWriter to both of the edges. As a result all stream elements
were sent twice through the same RecordWriter. It was actually pretty harmless apart of calculating the combined watermark downstream, since all watermarks were always comming just from one single edge/inputgate, and the unused edges were always stuck with min watermark.

As a solution we are making sure that StreamEdges are unique by introducing a uniqueId field, incremented for every pair of StreamEdges connecting the same node.;;;","21/Jan/22 15:12;pnowojski;merged commit 5192fd7 into apache:master
444641c9d3 to release-1.14
9c1bf6cdb3 to release-1.13;;;","07/Feb/22 10:30;twalthr;Tests on the Table API end in master: b795dd55d449edba5670dc0c1010666f9d97bf02;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Using Statefun RequestReplyFunctionBuilder fails with Java 8 date/time type `java.time.Duration` not supported by default: add Module ""org.apache.flink.shaded.jackson2.com.fasterxml.jackson.datatype:jackson-datatype-jsr310"" to enable handling ",FLINK-25197,13415613,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,galenwarren,galenwarren,galenwarren,06/Dec/21 14:51,27/May/22 12:28,13/Jul/23 08:12,27/Dec/21 15:40,statefun-3.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,statefun-3.1.2,statefun-3.2.0,,,,Stateful Functions,,,,,0,pull-request-available,,,,"When using RequestReplyFunctionBuilder to build a stateful functions job, the job fails at runtime with:

Java 8 date/time type `java.time.Duration` not supported by default: add Module ""org.apache.flink.shaded.jackson2.com.fasterxml.jackson.datatype:jackson-datatype-jsr310"" to enable handling 

It appears this is because, in [RequestReplyFunctionBuilder::transportClientPropertiesAsObjectNode|https://github.com/apache/flink-statefun/blob/b4ba9547b8f0105a28544fd28a5e0433666e9023/statefun-flink/statefun-flink-datastream/src/main/java/org/apache/flink/statefun/flink/datastream/RequestReplyFunctionBuilder.java#L127], a default instance of ObjectMapper is used to serialize the client properties, which now include a java.time.Duration. There is a [StateFunObjectMapper|https://github.com/apache/flink-statefun/blob/master/statefun-flink/statefun-flink-common/src/main/java/org/apache/flink/statefun/flink/common/json/StateFunObjectMapper.java] class in the project that has customized serde support, but it is not used here.

The fix seems to be to:
 * Use an instance of StateFunObjectMapper to serialize the client properties in RequestReplyFunctionBuilder
 * Modify StateFunObjectMapper to both serialize and deserialize instances of java.time.Duration (currently, only deserialization is supported)

I've made these changes locally and it seems to fix the problem. Would you be interested in a PR? Thanks.

 ",,galenwarren,Kazimirov,sjwiesman,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 27 12:28:13 UTC 2022,,,,,,,,,,"0|z0xe4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Dec/21 16:51;galenwarren;PR created: https://github.com/apache/flink-statefun/pull/282;;;","26/Dec/21 16:54;galenwarren;PR created: https://github.com/apache/flink-statefun/pull/282

On Sun, Dec 26, 2021 at 11:43 AM ASF GitHub Bot (Jira) <jira@apache.org>

;;;","27/Dec/21 15:40;trohrmann;Fixed via

statefun-3.2.0: 51d3130a172a40ed1dfda9269292b08054534c64
statefun-3.1.2: 56786b1d16ae3300ae8e0da917531093adeb10cf;;;","27/May/22 12:28;Kazimirov;Hi, [~galenwarren] could your fix affect this issue - https://issues.apache.org/jira/browse/FLINK-27813 ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 KafkaDynamicTableFactoryTest and UpsertKafkaDynamicTableFactoryTest fails on AZP,FLINK-25186,13415533,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,slinkydeveloper,trohrmann,trohrmann,06/Dec/21 08:46,06/Dec/21 14:48,13/Jul/23 08:12,06/Dec/21 14:48,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Connectors / Kafka,Table SQL / Ecosystem,,,,0,pull-request-available,test-stability,,,"A lot of {{KafkaDynamicTableFactoryTest}} and {{UpsertKafkaDynamicTableFactoryTest}} tests fail on AZP.

{code}
Dec 06 03:00:28 [ERROR]   UpsertKafkaDynamicTableFactoryTest.testInvalidSinkBufferFlush 
Dec 06 03:00:28 Expected: (an instance of org.apache.flink.table.api.ValidationException and Expected failure cause is <org.apache.flink.table.api.ValidationException: 'sink.buffer-flush.max-rows' and 'sink.buffer-flush.interval' must be set to be greater than zero together to enable sink buffer flushing.>)
Dec 06 03:00:28      but: Expected failure cause is <org.apache.flink.table.api.ValidationException: 'sink.buffer-flush.max-rows' and 'sink.buffer-flush.interval' must be set to be greater than zero together to enable sink buffer flushing.> The throwable <org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default.default.t1'.
Dec 06 03:00:28 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27569&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=918e890f-5ed9-5212-a25e-962628fb4bc5&l=10186",,trohrmann,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 06 14:48:08 UTC 2021,,,,,,,,,,"0|z0xdmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/21 08:47;trohrmann;cc [~twalthr].;;;","06/Dec/21 10:10;twalthr;CC [~slinkydeveloper];;;","06/Dec/21 14:48;twalthr;Fixed in master: dd446c9d56be5f33c683611102ec7026cf95e395;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamFaultToleranceTestBase hangs on AZP,FLINK-25185,13415532,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,trohrmann,trohrmann,06/Dec/21 08:42,27/Feb/23 10:33,13/Jul/23 08:12,18/Jan/22 09:33,1.13.3,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / State Backends,,,,,0,test-stability,,,,"The {{StreamFaultToleranceTestBase}} hangs on AZP.

{code}
2021-12-06T04:24:48.1676089Z ==========================================================================================
2021-12-06T04:24:48.1678883Z === WARNING: This task took already 95% of the available time budget of 237 minutes ===
2021-12-06T04:24:48.1679596Z ==========================================================================================
2021-12-06T04:24:48.1680326Z ==============================================================================
2021-12-06T04:24:48.1680877Z The following Java processes are running (JPS)
2021-12-06T04:24:48.1681467Z ==============================================================================
2021-12-06T04:24:48.6514536Z 13701 surefirebooter17740627448580534543.jar
2021-12-06T04:24:48.6515353Z 1622 Jps
2021-12-06T04:24:48.6515795Z 780 Launcher
2021-12-06T04:24:48.6825889Z ==============================================================================
2021-12-06T04:24:48.6826565Z Printing stack trace of Java process 13701
2021-12-06T04:24:48.6827012Z ==============================================================================
2021-12-06T04:24:49.1876086Z 2021-12-06 04:24:49
2021-12-06T04:24:49.1877098Z Full thread dump OpenJDK 64-Bit Server VM (11.0.10+9 mixed mode):
2021-12-06T04:24:49.1877362Z 
2021-12-06T04:24:49.1877672Z Threads class SMR info:
2021-12-06T04:24:49.1878049Z _java_thread_list=0x00007f254c007630, length=365, elements={
2021-12-06T04:24:49.1878504Z 0x00007f2598028000, 0x00007f2598280800, 0x00007f2598284800, 0x00007f2598299000,
2021-12-06T04:24:49.1878973Z 0x00007f259829b000, 0x00007f259829d800, 0x00007f259829f800, 0x00007f25982a1800,
2021-12-06T04:24:49.1879680Z 0x00007f2598337800, 0x00007f25983e3000, 0x00007f2598431000, 0x00007f2528016000,
2021-12-06T04:24:49.1896613Z 0x00007f2599003000, 0x00007f259972e000, 0x00007f2599833800, 0x00007f259984c000,
2021-12-06T04:24:49.1897558Z 0x00007f259984f000, 0x00007f2599851000, 0x00007f2599892000, 0x00007f2599894800,
2021-12-06T04:24:49.1898075Z 0x00007f2499a16000, 0x00007f2485acd800, 0x00007f2485ace000, 0x00007f24876bb800,
2021-12-06T04:24:49.1898562Z 0x00007f2461e59000, 0x00007f2499a0e800, 0x00007f2461e5e800, 0x00007f2461e81000,
2021-12-06T04:24:49.1899037Z 0x00007f24dc015000, 0x00007f2461e86800, 0x00007f2448002000, 0x00007f24dc01c000,
2021-12-06T04:24:49.1899522Z 0x00007f2438001000, 0x00007f2438003000, 0x00007f2438005000, 0x00007f2438006800,
2021-12-06T04:24:49.1899982Z 0x00007f2438008800, 0x00007f2434017800, 0x00007f243401a800, 0x00007f2414008800,
2021-12-06T04:24:49.1900495Z 0x00007f24e8089800, 0x00007f24e8090000, 0x00007f23e4005800, 0x00007f24e8092800,
2021-12-06T04:24:49.1901163Z 0x00007f24e8099000, 0x00007f2414015800, 0x00007f24dc04c000, 0x00007f2414018800,
2021-12-06T04:24:49.1901680Z 0x00007f2414020000, 0x00007f24dc058000, 0x00007f24dc05b000, 0x00007f2414022000,
2021-12-06T04:24:49.1902283Z 0x00007f24d400f000, 0x00007f241402e800, 0x00007f2414031800, 0x00007f2414033800,
2021-12-06T04:24:49.1902880Z 0x00007f2414035000, 0x00007f2414037000, 0x00007f2414038800, 0x00007f241403a800,
2021-12-06T04:24:49.1903354Z 0x00007f241403c000, 0x00007f241403e000, 0x00007f241403f800, 0x00007f2414041800,
2021-12-06T04:24:49.1903812Z 0x00007f2414043000, 0x00007f2414045000, 0x00007f24dc064800, 0x00007f2414047000,
2021-12-06T04:24:49.1904284Z 0x00007f2414048800, 0x00007f241404a800, 0x00007f241404c800, 0x00007f241404e000,
2021-12-06T04:24:49.1904800Z 0x00007f2414050000, 0x00007f2414051800, 0x00007f2414053800, 0x00007f2414055000,
2021-12-06T04:24:49.1905455Z 0x00007f2414057000, 0x00007f2414059000, 0x00007f241405a800, 0x00007f241405c800,
2021-12-06T04:24:49.1906098Z 0x00007f241405e000, 0x00007f2414060000, 0x00007f2414062000, 0x00007f2414063800,
2021-12-06T04:24:49.1906728Z 0x00007f22e400c800, 0x00007f2328008000, 0x00007f2284007000, 0x00007f22cc019800,
2021-12-06T04:24:49.1907396Z 0x00007f21f8004000, 0x00007f2304012800, 0x00007f230001b000, 0x00007f223c011000,
2021-12-06T04:24:49.1908080Z 0x00007f24e40c1800, 0x00007f2454001000, 0x00007f24e40c3000, 0x00007f2454003000,
2021-12-06T04:24:49.1908794Z 0x00007f24e40c5000, 0x00007f2454004800, 0x00007f2444002000, 0x00007f2444002800,
2021-12-06T04:24:49.1909522Z 0x00007f245808b800, 0x00007f24b8032800, 0x00007f24ac021000, 0x00007f24b8034800,
2021-12-06T04:24:49.1910280Z 0x00007f24b8036800, 0x00007f24ac032800, 0x00007f24b8052000, 0x00007f24ac033800,
2021-12-06T04:24:49.1911023Z 0x00007f24ac035000, 0x00007f24b8067000, 0x00007f24ac036000, 0x00007f241407d000,
2021-12-06T04:24:49.1911714Z 0x00007f24ac0a9800, 0x00007f24b4018800, 0x00007f254008a800, 0x00007f24ac06f800,
2021-12-06T04:24:49.1912565Z 0x00007f2540247800, 0x00007f21f400d000, 0x00007f24b4058000, 0x00007f24b4052000,
2021-12-06T04:24:49.1913476Z 0x00007f24ac0a3000, 0x00007f24b4052800, 0x00007f24b401f800, 0x00007f24b4020800,
2021-12-06T04:24:49.1914496Z 0x00007f24b4022000, 0x00007f24b4025000, 0x00007f24b4026000, 0x00007f24b4027000,
2021-12-06T04:24:49.1915097Z 0x00007f24ac022000, 0x00007f24b4028000, 0x00007f24b4029800, 0x00007f24b402b000,
2021-12-06T04:24:49.1915580Z 0x00007f24b402c000, 0x00007f24ac024000, 0x00007f2540248800, 0x00007f2540013000,
2021-12-06T04:24:49.1916060Z 0x00007f2540014000, 0x00007f24b402d000, 0x00007f24b402e000, 0x00007f24b4030000,
2021-12-06T04:24:49.1916716Z 0x00007f24b4031000, 0x00007f24ac025000, 0x00007f24ac027000, 0x00007f24b4032000,
2021-12-06T04:24:49.1917403Z 0x00007f24b4033800, 0x00007f2540014800, 0x00007f24b4035800, 0x00007f2540015800,
2021-12-06T04:24:49.1918098Z 0x00007f2540017800, 0x00007f24ac028000, 0x00007f24ac096000, 0x00007f241406b000,
2021-12-06T04:24:49.1918810Z 0x00007f24140a4000, 0x00007f24ac097000, 0x00007f24140a5000, 0x00007f24ac098000,
2021-12-06T04:24:49.1919516Z 0x00007f24140a6000, 0x00007f24ac099000, 0x00007f24ac09b000, 0x00007f24ac09d000,
2021-12-06T04:24:49.1920238Z 0x00007f24ac02a800, 0x00007f24140a7000, 0x00007f2414091000, 0x00007f2414092800,
2021-12-06T04:24:49.1924443Z 0x00007f2414094000, 0x00007f2414095800, 0x00007f2414096800, 0x00007f2414098000,
2021-12-06T04:24:49.1924945Z 0x00007f24b4037000, 0x00007f24140aa000, 0x00007f24b4038800, 0x00007f24b403a000,
2021-12-06T04:24:49.1925435Z 0x00007f24b403b800, 0x00007f24b403d000, 0x00007f24140ab000, 0x00007f24b403e000,
2021-12-06T04:24:49.1926138Z 0x00007f24140ac800, 0x00007f24b403f000, 0x00007f2540018800, 0x00007f254001a000,
2021-12-06T04:24:49.1926888Z 0x00007f24140ad800, 0x00007f24b4040800, 0x00007f24b4042000, 0x00007f24b4043800,
2021-12-06T04:24:49.1927871Z 0x00007f24b4045000, 0x00007f24140af000, 0x00007f24140b1000, 0x00007f24b4046000,
2021-12-06T04:24:49.1928615Z 0x00007f24b4047800, 0x00007f24b4049000, 0x00007f24b404a800, 0x00007f2414088000,
2021-12-06T04:24:49.1929344Z 0x00007f2414089800, 0x00007f24b404b000, 0x00007f24b404c800, 0x00007f24b405b000,
2021-12-06T04:24:49.1930089Z 0x00007f24b405c800, 0x00007f24b405d800, 0x00007f24b405f000, 0x00007f24b4060000,
2021-12-06T04:24:49.1930874Z 0x00007f24b4061800, 0x00007f24b4062800, 0x00007f24b4064000, 0x00007f24b4065800,
2021-12-06T04:24:49.1931655Z 0x00007f24b4066800, 0x00007f24b4068000, 0x00007f241408a800, 0x00007f241408c000,
2021-12-06T04:24:49.1932594Z 0x00007f241408e000, 0x00007f241407f800, 0x00007f24b4069000, 0x00007f24b406a800,
2021-12-06T04:24:49.1933513Z 0x00007f2414080800, 0x00007f2414082000, 0x00007f24ac02b800, 0x00007f24b406c000,
2021-12-06T04:24:49.1934301Z 0x00007f24ac02c000, 0x00007f24b406d000, 0x00007f24b406e800, 0x00007f24b4070800,
2021-12-06T04:24:49.1935060Z 0x00007f24b4071800, 0x00007f24b4073000, 0x00007f24b4074800, 0x00007f2540306000,
2021-12-06T04:24:49.1935827Z 0x00007f2540307800, 0x00007f24b4075800, 0x00007f24b4076800, 0x00007f2540309000,
2021-12-06T04:24:49.1936660Z 0x00007f24ac02d800, 0x00007f254030a000, 0x00007f24b4078800, 0x00007f24ac02e800,
2021-12-06T04:24:49.1937398Z 0x00007f24b4079800, 0x00007f24b407a800, 0x00007f24ac030000, 0x00007f24ac070800,
2021-12-06T04:24:49.1938146Z 0x00007f24b407b800, 0x00007f24ac071800, 0x00007f24b407d000, 0x00007f24ac072800,
2021-12-06T04:24:49.1938914Z 0x00007f24b407e800, 0x00007f24ac074000, 0x00007f24b407f800, 0x00007f24b4081800,
2021-12-06T04:24:49.1939771Z 0x00007f254030b000, 0x00007f2540068800, 0x00007f254006a000, 0x00007f24ac075000,
2021-12-06T04:24:49.1940516Z 0x00007f24ac077000, 0x00007f24ac079000, 0x00007f24b4082800, 0x00007f24b4084000,
2021-12-06T04:24:49.1941260Z 0x00007f254006b000, 0x00007f254006c000, 0x00007f24b4085000, 0x00007f24b4086800,
2021-12-06T04:24:49.1942145Z 0x00007f24b4088800, 0x00007f24ac07a000, 0x00007f24ac07b800, 0x00007f254006d800,
2021-12-06T04:24:49.1943222Z 0x00007f24ac07c800, 0x00007f24ac07e000, 0x00007f24ac080000, 0x00007f24ac081800,
2021-12-06T04:24:49.1944028Z 0x00007f24ac083000, 0x00007f24140db000, 0x00007f2414084000, 0x00007f2414086000,
2021-12-06T04:24:49.1944838Z 0x00007f24140c2800, 0x00007f254006e800, 0x00007f2540082000, 0x00007f2540084000,
2021-12-06T04:24:49.1945905Z 0x00007f24140c3000, 0x00007f24ac084000, 0x00007f24ac085800, 0x00007f2540085000,
2021-12-06T04:24:49.1946697Z 0x00007f2540086800, 0x00007f24ac087000, 0x00007f2540088000, 0x00007f2540209800,
2021-12-06T04:24:49.1947452Z 0x00007f24140c4000, 0x00007f24140c6000, 0x00007f24140c8000, 0x00007f254020b000,
2021-12-06T04:24:49.1948246Z 0x00007f254020c800, 0x00007f24140c8800, 0x00007f24140ca000, 0x00007f24140cc000,
2021-12-06T04:24:49.1949033Z 0x00007f254020d800, 0x00007f24ac088000, 0x00007f254020e800, 0x00007f24ac001000,
2021-12-06T04:24:49.1949834Z 0x00007f2540210800, 0x00007f254001c800, 0x00007f24140cd000, 0x00007f24140cf000,
2021-12-06T04:24:49.1950625Z 0x00007f254001e000, 0x00007f24140d0800, 0x00007f254001f800, 0x00007f24140e0000,
2021-12-06T04:24:49.1951396Z 0x00007f2540021000, 0x00007f24140e1800, 0x00007f2540023000, 0x00007f24140e3800,
2021-12-06T04:24:49.1952354Z 0x00007f2540025000, 0x00007f2540027800, 0x00007f24ac002000, 0x00007f24ac005000,
2021-12-06T04:24:49.1953367Z 0x00007f24b40f9800, 0x00007f24b40fb800, 0x00007f24ac006800, 0x00007f24b40fd000,
2021-12-06T04:24:49.1953862Z 0x00007f2540029800, 0x00007f25402f0000, 0x00007f25402f2000, 0x00007f24ac008000,
2021-12-06T04:24:49.1954520Z 0x00007f25402f3800, 0x00007f24ac00a000, 0x00007f24ac00c800, 0x00007f25402f5000,
2021-12-06T04:24:49.1955005Z 0x00007f25402f7800, 0x00007f24ac00e800, 0x00007f24ac010000, 0x00007f24ac012800,
2021-12-06T04:24:49.1955688Z 0x00007f25402f9800, 0x00007f24ac014000, 0x00007f24ac016000, 0x00007f24ac018000,
2021-12-06T04:24:49.1956176Z 0x00007f24ac01a000, 0x00007f25402fb000, 0x00007f25402fd000, 0x00007f2540255800,
2021-12-06T04:24:49.1956653Z 0x00007f2540257800, 0x00007f24ac01b000, 0x00007f24ac01d000, 0x00007f24ac01f800,
2021-12-06T04:24:49.1957118Z 0x00007f24b40fe800, 0x00007f24ac037800, 0x00007f24ac039000, 0x00007f24ac03b800,
2021-12-06T04:24:49.1957598Z 0x00007f24ac03d800, 0x00007f24b4100800, 0x00007f24ac03e800, 0x00007f24ac040000,
2021-12-06T04:24:49.1958086Z 0x00007f24b4102800, 0x00007f24b4104800, 0x00007f24b4107000, 0x00007f2540258800,
2021-12-06T04:24:49.1958567Z 0x00007f24b4108000, 0x00007f254025a000, 0x00007f24ac042000, 0x00007f254025b800,
2021-12-06T04:24:49.1959126Z 0x00007f254025e000, 0x00007f24b4109000, 0x00007f24b410b800, 0x00007f2540260000,
2021-12-06T04:24:49.1960049Z 0x00007f2540261800, 0x00007f24b410d800, 0x00007f2540263800, 0x00007f2540002800,
2021-12-06T04:24:49.1960764Z 0x00007f2540004800, 0x00007f24b401b800, 0x00007f24b401e800, 0x00007f24b4114000,
2021-12-06T04:24:49.1961638Z 0x00007f254c006800
2021-12-06T04:24:49.1962301Z }
2021-12-06T04:24:49.1962496Z 
2021-12-06T04:24:49.1963236Z ""main"" #1 prio=5 os_prio=0 cpu=3941.93ms elapsed=12601.44s tid=0x00007f2598028000 nid=0x3586 waiting on condition  [0x00007f25a129c000]
2021-12-06T04:24:49.1963791Z    java.lang.Thread.State: WAITING (parking)
2021-12-06T04:24:49.1964243Z 	at jdk.internal.misc.Unsafe.park(java.base@11.0.10/Native Method)
2021-12-06T04:24:49.1965467Z 	- parking to wait for  <0x00000000816de610> (a java.util.concurrent.CompletableFuture$Signaller)
2021-12-06T04:24:49.1966054Z 	at java.util.concurrent.locks.LockSupport.park(java.base@11.0.10/LockSupport.java:194)
2021-12-06T04:24:49.1966626Z 	at java.util.concurrent.CompletableFuture$Signaller.block(java.base@11.0.10/CompletableFuture.java:1796)
2021-12-06T04:24:49.1967433Z 	at java.util.concurrent.ForkJoinPool.managedBlock(java.base@11.0.10/ForkJoinPool.java:3128)
2021-12-06T04:24:49.1967965Z 	at java.util.concurrent.CompletableFuture.waitingGet(java.base@11.0.10/CompletableFuture.java:1823)
2021-12-06T04:24:49.1968522Z 	at java.util.concurrent.CompletableFuture.get(java.base@11.0.10/CompletableFuture.java:1998)
2021-12-06T04:24:49.1969094Z 	at org.apache.flink.test.util.TestUtils.submitJobAndWaitForResult(TestUtils.java:74)
2021-12-06T04:24:49.1970124Z 	at org.apache.flink.test.checkpointing.StreamFaultToleranceTestBase.runCheckpointedProgram(StreamFaultToleranceTestBase.java:124)
2021-12-06T04:24:49.1970886Z 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(java.base@11.0.10/Native Method)
2021-12-06T04:24:49.1971968Z 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(java.base@11.0.10/NativeMethodAccessorImpl.java:62)
2021-12-06T04:24:49.1972745Z 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(java.base@11.0.10/DelegatingMethodAccessorImpl.java:43)
2021-12-06T04:24:49.1973551Z 	at java.lang.reflect.Method.invoke(java.base@11.0.10/Method.java:566)
2021-12-06T04:24:49.1974066Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-12-06T04:24:49.1974656Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-12-06T04:24:49.1975236Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-12-06T04:24:49.1975894Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-12-06T04:24:49.1976436Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-12-06T04:24:49.1976965Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-12-06T04:24:49.1977525Z 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-12-06T04:24:49.1978151Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-12-06T04:24:49.1978620Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-12-06T04:24:49.1979088Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-12-06T04:24:49.1979990Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-12-06T04:24:49.1980661Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-12-06T04:24:49.1981232Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-12-06T04:24:49.1981722Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-12-06T04:24:49.1982285Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-12-06T04:24:49.1982963Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-12-06T04:24:49.1983501Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-12-06T04:24:49.1984216Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-12-06T04:24:49.1984710Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-12-06T04:24:49.1985150Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-12-06T04:24:49.1985592Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-12-06T04:24:49.1986084Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-12-06T04:24:49.1986783Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-12-06T04:24:49.1987435Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-12-06T04:24:49.1988142Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-12-06T04:24:49.1988755Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-12-06T04:24:49.1989222Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-12-06T04:24:49.1989664Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-12-06T04:24:49.1990207Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-12-06T04:24:49.1990907Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-12-06T04:24:49.1991408Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-12-06T04:24:49.1992088Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-12-06T04:24:49.1992821Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-12-06T04:24:49.1993304Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-12-06T04:24:49.1993900Z 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2021-12-06T04:24:49.1994488Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2021-12-06T04:24:49.1995112Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
2021-12-06T04:24:49.1999560Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
2021-12-06T04:24:49.2000208Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2021-12-06T04:24:49.2000807Z 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2021-12-06T04:24:49.2001423Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-12-06T04:24:49.2002128Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-12-06T04:24:49.2002952Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-12-06T04:24:49.2003562Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27568&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=2c7d57b9-7341-5a87-c9af-2cf7cc1a37dc",,dmvk,dwysakowicz,gaoyunhaii,kevin.cyj,pnowojski,roman,trohrmann,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24035,,,,,,,,FLINK-25395,FLINK-25407,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 18 09:33:35 UTC 2022,,,,,,,,,,"0|z0xdmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Dec/21 07:10;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28297&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=19003;;;","17/Dec/21 14:21;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28306&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=19832;;;","20/Dec/21 09:16;trohrmann;cc [~pnowojski];;;","20/Dec/21 11:25;pnowojski;It looks like those tests were stuck in an endless loop being unable to allocate enough slots to run the job:

{noformat}
org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
06:42:22,189 [flink-akka.actor.default-dispatcher-7] WARN  org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager [] - Could not fulfill resource requirements of job 5a5ac441318e8085606c78b40c3a2f25.
06:42:22,189 [flink-akka.actor.default-dispatcher-7] WARN  org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge [] - Could not acquire the minimum required resources, failing slot requests. Acquired: [ResourceRequirement{resourceProfile=ResourceProfile{taskHeapMemory=256.000gb (274877906944 bytes), taskOffHeapMemory=256.000gb (274877906944 bytes), managedMemory=20.000mb (20971520 bytes), networkMemory=16.000mb (16777216 bytes)}, numberOfRequiredSlots=8}]. Current slot pool status: Registered TMs: 2, registered slots: 8 free slots: 0
org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
06:42:22,259 [flink-akka.actor.default-dispatcher-9] WARN  org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager [] - Could not fulfill resource requirements of job 5a5ac441318e8085606c78b40c3a2f25.
06:42:22,259 [flink-akka.actor.default-dispatcher-9] WARN  org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge [] - Could not acquire the minimum required resources, failing slot requests. Acquired: [ResourceRequirement{resourceProfile=ResourceProfile{taskHeapMemory=256.000gb (274877906944 bytes), taskOffHeapMemory=256.000gb (274877906944 bytes), managedMemory=20.000mb (20971520 bytes), networkMemory=16.000mb (16777216 bytes)}, numberOfRequiredSlots=8}]. Current slot pool status: Registered TMs: 2, registered slots: 8 free slots: 0
org.apache.flink.runtime.j
{noformat}

It's very hard to say, but it looks like (one of?) the first failure was this one:

{noformat}
04:06:26,659 [Map -> Sink: Unnamed (9/12)#1] WARN  org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Exception while restoring keyed state backend for StreamMap_dc2290bb6f8f5cd2bd425368843494fe_(9/12) from alternative (1/1), will retry while mor
e alternatives are available.
java.lang.RuntimeException: java.io.FileNotFoundException: /tmp/junit3146957979516280339/junit1602669867129285236/d6a6dbdd-3fd7-4786-9dc1-9ccc161740da (No such file or directory)
        at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:319) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:87) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.hasNext(StateChangelogHandleStreamHandleReader.java:69) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.readBackendHandle(ChangelogBackendRestoreOperation.java:92) ~[flink-statebackend-changelog-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.restore(ChangelogBackendRestoreOperation.java:74) ~[flink-statebackend-changelog-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.state.changelog.ChangelogStateBackend.restore(ChangelogStateBackend.java:221) ~[flink-statebackend-changelog-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.state.changelog.ChangelogStateBackend.createKeyedStateBackend(ChangelogStateBackend.java:145) ~[flink-statebackend-changelog-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:329) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:346) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:164) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:110) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:696) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:672) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:639) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948) [flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917) [flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741) [flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) [flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
Caused by: java.io.FileNotFoundException: /tmp/junit3146957979516280339/junit1602669867129285236/d6a6dbdd-3fd7-4786-9dc1-9ccc161740da (No such file or directory)
        at java.io.FileInputStream.open0(Native Method) ~[?:1.8.0_292]
        at java.io.FileInputStream.open(FileInputStream.java:195) ~[?:1.8.0_292]
        at java.io.FileInputStream.<init>(FileInputStream.java:138) ~[?:1.8.0_292]
        at org.apache.flink.core.fs.local.LocalDataInputStream.<init>(LocalDataInputStream.java:50) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.core.fs.local.LocalFileSystem.open(LocalFileSystem.java:134) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.open(SafetyNetWrapperFileSystem.java:87) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.state.filesystem.FileStateHandle.openInputStream(FileStateHandle.java:68) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.changelog.fs.StateChangeFormat.read(StateChangeFormat.java:92) ~[flink-dstl-dfs-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:85) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
{noformat}

cc [~ym] [~roman];;;","20/Dec/21 14:25;roman;I think it might be related to checkpoint abortion and incremental state:
When a checkpoitnt is aborted, TM will try to discard in progress uploads. This state can't be re-used for future checkpoints.
 
Prior to FLINK-24611, this worked for RocksDB, because RocksDB backend would wait for JM confirmation before trying to reuse the state.
After FLINK-24611, this breaks any incremental backend (rocksdb and changelog, though the latter one is more likely fail).
WDYT [~pnowojski]?

I'll try to validate this locally. If the cause is different, I'll open a new bug for the above FLINK-24611 related issue.;;;","20/Dec/21 14:58;pnowojski;{quote}
When a checkpoitnt is aborted, TM will try to discard in progress uploads.
{quote}
[~roman], do you mean {{SubtaskCheckpointCoordinatorImpl#cancelAsyncCheckpointRunnable}} being invoked and the uploads being cancelled? 

Doesn't it point to a larger problem? That future checkpoints in general can be deemed as completed, even if previous async phases are still uploading some of the files that those future checkpoints are referencing? 
{quote}
This state can't be re-used for future checkpoints.
{quote}
Probably it's not only about ""future"" as not yet triggered checkpoints, but any subsequent checkpoints, of which some of them might have been already in progress.

It seems like neither of those problem will be easy to fix?;;;","20/Dec/21 16:29;roman;> Roman Khachatryan, do you mean SubtaskCheckpointCoordinatorImpl#cancelAsyncCheckpointRunnable being invoked and the uploads being cancelled?
Yes, or any other case when AsyncCheckpointRunnable.cleanup() is invoked; for example, reporting to JM failed.

> Doesn't it point to a larger problem? That future checkpoints in general can be deemed as completed, even if previous async phases are still uploading some of the files that those future checkpoints are referencing?

I don't think so: the decision whether to re-use some state or not is made by the State backend, not runtime (not AsyncCheckpointRunnable/SubtaskCheckpointCoordinatorImpl).
Both RocksDB and Changelog consider state as re-usable only once the upload finishes (RocksIncrementalSnapshotStrategy.lastUploadedSstFiles or FsStateChangelogWriter.uploaded is updated).
For a concurrent checkpoint, RocksDB will be re-uploaded the state; and changelog will wait for upload completion in UploadCompletionListener.
Does this make sense?

> It seems like neither of those problem will be easy to fix?
The only easy fix I see is to never discard shared state in IncrementalRemoteKeyedStateHandle.discardState and rely on checkpoint subsumption or job termination for the cleanup. In any case when the state didn't reach JM it will be left orphaned (e.g. checkpoint aborted and backend materialized, not reporting this state again).
Orphaned files problem should be mitigated by FLINK-24852.

A proper fix I think would be private/shared state separation and TM-side registry (FLINK-23139 and related tickets).
As the latter is much more invasive, I'd choose the former for the upcoming release (still need to confirm this is the root cause).;;;","20/Dec/21 20:19;roman;I've just noticed that the issue is also reported for 1.13 which doesn't use changelog.
So I created a separate ticket FLINK-25395 for the issue discussed above (removing incremental state in AsyncCheckpointRunnable).

As for the StreamCheckpointingITCase, I wasn't able to reproduce it locally so far (1000 iterations, master).;;;","20/Dec/21 21:00;roman;
In later (1.15) logs, I see a deadlock in the network stack:
{code}
Java stack information for the threads listed above:
===================================================
""Canceler for Source: Custom Source -> Filter (7/12)#14176 (0fbb8a89616ca7a40e473adad51f236f)."":
   at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.destroyBufferPool(NetworkBufferPool.java:420)
   - waiting to lock <0x0000000082937f28> (a java.lang.Object)
   at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.lazyDestroy(LocalBufferPool.java:567)
   at org.apache.flink.runtime.io.network.partition.ResultPartition.closeBufferPool(ResultPartition.java:264)
   at org.apache.flink.runtime.io.network.partition.ResultPartition.fail(ResultPartition.java:276)
   at org.apache.flink.runtime.taskmanager.Task.failAllResultPartitions(Task.java:999)
   at org.apache.flink.runtime.taskmanager.Task.access$100(Task.java:138)
   at org.apache.flink.runtime.taskmanager.Task$TaskCanceler.run(Task.java:1669)
   at java.lang.Thread.run(Thread.java:748)
""Canceler for Map -> Map (6/12)#14176 (6195862d199aa4d52c12f25b39904725)."":
   at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.setNumBuffers(LocalBufferPool.java:585)
   - waiting to lock <0x0000000097108898> (a java.util.ArrayDeque)
   at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.redistributeBuffers(NetworkBufferPool.java:544)
   at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.destroyBufferPool(NetworkBufferPool.java:424)
   - locked <0x0000000082937f28> (a java.lang.Object)
   at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.lazyDestroy(LocalBufferPool.java:567)
   at org.apache.flink.runtime.io.network.partition.ResultPartition.closeBufferPool(ResultPartition.java:264)
   at org.apache.flink.runtime.io.network.partition.ResultPartition.fail(ResultPartition.java:276)
   at org.apache.flink.runtime.taskmanager.Task.failAllResultPartitions(Task.java:999)
   at org.apache.flink.runtime.taskmanager.Task.access$100(Task.java:138)
   at org.apache.flink.runtime.taskmanager.Task$TaskCanceler.run(Task.java:1669)
   at java.lang.Thread.run(Thread.java:748)
""Map -> Sink: Unnamed (7/12)#14176"":
   at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.recycleMemorySegments(NetworkBufferPool.java:256)
   - waiting to lock <0x0000000082937f28> (a java.lang.Object)
   at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.internalRequestMemorySegments(NetworkBufferPool.ja
   at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.requestMemorySegmentsBlocking(NetworkBufferPool.ja
   at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.reserveSegments(LocalBufferPool.java:247)
   - locked <0x0000000097108898> (a java.util.ArrayDeque)
   at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.setupChannels(SingleInputGate.java:497)
   at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.setup(SingleInputGate.java:276)
   at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.setup(InputGateWithMetrics.java:105)
   at org.apache.flink.runtime.taskmanager.Task.setupPartitionsAndGates(Task.java:965)
   at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:652)
   at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
   at java.lang.Thread.run(Thread.java:748)

Found 1 deadlock.
{code}
;;;","21/Dec/21 08:09;trohrmann;Deadlock instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28393&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","21/Dec/21 08:10;trohrmann;Deadlock: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28393&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","21/Dec/21 09:46;pnowojski;{quote}
I don't think so: the decision whether to re-use some state or not is made by the State backend, not runtime (not AsyncCheckpointRunnable/SubtaskCheckpointCoordinatorImpl).
(...){quote}
Ok. I've thought that the {{lastUploadedSstFiles.putAll(sstFiles);}} in {{uploadSstFiles()}} happens in the sync part of checkpoint process. Now I see it's in the async phase and it actually happens only once files are actually uploaded.

Let's chat offline about what is exactly happening here and what's your proposal to fix it.

Regarding the deadlock that you posted, is it the primary issue causing those test failures? It looks like the deadlock might have been introduced in FLINK-24035. CC [~kevin.cyj];;;","21/Dec/21 12:43;kevin.cyj;After looking into the code and deadlock stacks, I can confirm that FLINK-24035 caused the deadlock. NetworkBufferPool#internalRequestMemorySegments may also need to acquire the 'factoryLock' in the ```catch``` block, I did not realize that previously. Apart from this, I am still trying to understand why we reached the ```catch``` block. Maybe there is another issue. Anyway, I will try first to fix the issue caused by FLINK-24035 and update if I have some any new findings.;;;","21/Dec/21 15:55;pnowojski;After an offline discussion with [~roman] and some further analysis this is what we think is happening for 1.15 branch.

# Test is hitting {{FileNotFoundException}}, probably caused by FLINK-25395
# Test ends up in an infinite restart loop, where each restart attempt hits {{FileNotFoundException}}
# After tens of thousands of restart attempts and cancellations (for example in attempt #14176 as [commented in Roman's post|  https://issues.apache.org/jira/browse/FLINK-25185?focusedCommentId=17462834&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17462834]), this endless cycle of restarts and cancellations is causing FLINK-25407 deadlock to surface. 
# From now on, StreamFaultToleranceTestBase will end up in yet another infinite restart loop, but this time scheduling will be failing with ""Could not acquire the minimum required resources."" This is because one TaskManager is stuck in this deadlock and hence we are missing resources to restart the job.

We have extracted those two issues FLINK-25395 (affects only 1.15, after merging FLINK-24611 a couple of days ago. It's a release blocker) and FLINK-25407 (affects 1.14.x and 1.15.x, but not as severe issue) to independent tickets. For the time being we will disable changelog state backend randomisation until FLINK-25395 is fixed to reduce the number of test failure.

However the first report was from 1.13 branch, and I can not see the same deadlock there. I can not verify the logs from that failure, because logs upload has failed. So most likely there is still another issue present in the code base (At least in 1.13.x branch), that we have no way of analysing at the moment and we will have to wait for another failure with successful logs upload this time.;;;","22/Dec/21 03:00;kevin.cyj;I think I now understand that it should be the interrupt exception which leaded to the the reach of _catch_ block. I will try to prepare a fix for FLINK-25407 soon.;;;","22/Dec/21 08:25;dmvk;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28447&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","18/Jan/22 09:33;pnowojski;The underlying issues where extracted to separate tickets FLINK-25407 and FLINK-25395 and fixed. Closing this one.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoClassDefFoundError of PulsarAdminImpl by using flink-connector-pulsar:1.14 on k8s flink cluster,FLINK-25182,13415527,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,forsake0120,forsake0120,06/Dec/21 08:10,10/Dec/21 11:08,13/Jul/23 08:12,10/Dec/21 11:08,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Pulsar,,,,,0,,,,,"NoClassDefFoundError of PulsarAdminImpl by using flink-connector-pulsar:1.14 on k8s flink cluster

 

Flink: Session mode in HA cluster on k8s

Version: Flink 1.14.0

Connector:  flink-connector-pulsar_2.11:1.14.0

 

The connector is worked by using IntelliJ IDEA, but meets exception on dev k8s clusters, the exception please check screenshot

!image-2021-12-06-16-09-12-816.png!

!image-2021-12-06-16-09-52-042.png!

!image-2021-12-06-16-10-13-697.png!","* Flink: HA cluster on k8s
 * Flink Mode: session
 * Version: Flink 1.14.0
 * Connector:  flink-connector-pulsar_2.11:1.14.0
 * Pulsar cluster: ( by StreamNative' s helm charts)
 *    broker version: 2.8.0.8
 *    bookie version: 2.7.2.8
 *    pulsar proxy: 2.8.0.8",affe,forsake0120,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/21 08:09;forsake0120;image-2021-12-06-16-09-12-816.png;https://issues.apache.org/jira/secure/attachment/13037028/image-2021-12-06-16-09-12-816.png","06/Dec/21 08:09;forsake0120;image-2021-12-06-16-09-52-042.png;https://issues.apache.org/jira/secure/attachment/13037027/image-2021-12-06-16-09-52-042.png","06/Dec/21 08:10;forsake0120;image-2021-12-06-16-10-13-697.png;https://issues.apache.org/jira/secure/attachment/13037026/image-2021-12-06-16-10-13-697.png",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Fri Dec 10 11:08:16 UTC 2021,,,,,,,,,,"0|z0xdlc:",9223372036854775807,"after add <scope>provided</scope> on flink-streaming-java_2.11 in pom.xml, it works well
",,,,,,,,,,,,,,,,,,,"10/Dec/21 10:50;affe;[~forsake0120]  Hi, Looks like the issue has been resolved, would you mind comment on the root cause and close this issue ? Thank you so much~ ;;;","10/Dec/21 11:08;forsake0120;after add <scope>provided</scope> on flink-streaming-java_2.11 in pom.xml, it works well;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jepsen test fails while setting up libzip4,FLINK-25180,13415524,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,trohrmann,trohrmann,06/Dec/21 07:56,08/Dec/21 10:42,13/Jul/23 08:12,08/Dec/21 10:42,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Test Infrastructure,,,,,0,pull-request-available,test-stability,,,"The Jepsen tests fail from time to time while trying to set up libzip4.

{code}
    java.util.concurrent.ExecutionException: clojure.lang.ExceptionInfo: throw+: {:type :jepsen.control/nonzero-exit, :cmd ""sudo -S -u root bash -c \""cd /; env DEBIAN_FRONTEND=noninteractive apt-get install -y --force-yes libzip4\"""", :exit -1, :out ""Reading package lists...
Building dependency tree...
Reading state information...
The following NEW packages will be installed:
  libzip4
0 upgraded, 1 newly installed, 0 to remove and 120 not upgraded.
Need to get 40.6 kB of archives.
After this operation, 103 kB of additional disk space will be used.
Get:1 http://cdn-aws.deb.debian.org/debian stretch/main amd64 libzip4 amd64 1.1.2-1.1+b1 [40.6 kB]
Fetched 40.6 kB in 0s (0 B/s)
Selecting previously unselected package libzip4:amd64.
	
(Reading database ... 
	(Reading database ... 5%
	(Reading database ... 10%
	(Reading database ... 15%
	(Reading database ... 20%
	(Reading database ... 25%
	(Reading database ... 30%
	(Reading database ... 35%
	(Reading database ... 40%
	(Reading database ... 45%
	(Reading database ... 50%
	(Reading database ... 55%
	(Reading database ... 60%
	(Reading database ... 65%
	(Reading database ... 70%
	(Reading database ... 75%
	(Reading database ... 80%
	(Reading database ... 85%
	(Reading database ... 90%
	(Reading database ... 95%
	(Reading database ... 100%
	(Reading database ... 49065 files and directories currently installed.)
	
Preparing to unpack .../libzip4_1.1.2-1.1+b1_amd64.deb ...
	
Unpacking libzip4:amd64 (1.1.2-1.1+b1) ...
	
Setting up libzip4:amd64 (1.1.2-1.1+b1) ...
	
"", :err """", :host ""172.31.4.8"", :action {:cmd ""sudo -S -u root bash -c \""cd /; env DEBIAN_FRONTEND=noninteractive apt-get install -y --force-yes libzip4\"""", :in ""root
""}} {:type :jepsen.control/nonzero-exit, :cmd ""sudo -S -u root bash -c \""cd /; env DEBIAN_FRONTEND=noninteractive apt-get install -y --force-yes libzip4\"""", :exit -1, :out ""Reading package lists...
Building dependency tree...
Reading state information...
The following NEW packages will be installed:
  libzip4
0 upgraded, 1 newly installed, 0 to remove and 120 not upgraded.
Need to get 40.6 kB of archives.
After this operation, 103 kB of additional disk space will be used.
Get:1 http://cdn-aws.deb.debian.org/debian stretch/main amd64 libzip4 amd64 1.1.2-1.1+b1 [40.6 kB]
Fetched 40.6 kB in 0s (0 B/s)
Selecting previously unselected package libzip4:amd64.
	
(Reading database ... 
	(Reading database ... 5%
	(Reading database ... 10%
	(Reading database ... 15%
	(Reading database ... 20%
	(Reading database ... 25%
	(Reading database ... 30%
	(Reading database ... 35%
	(Reading database ... 40%
	(Reading database ... 45%
	(Reading database ... 50%
	(Reading database ... 55%
	(Reading database ... 60%
	(Reading database ... 65%
	(Reading database ... 70%
	(Reading database ... 75%
	(Reading database ... 80%
	(Reading database ... 85%
	(Reading database ... 90%
	(Reading database ... 95%
	(Reading database ... 100%
	(Reading database ... 49065 files and directories currently installed.)
	
Preparing to unpack .../libzip4_1.1.2-1.1+b1_amd64.deb ...
	
Unpacking libzip4:amd64 (1.1.2-1.1+b1) ...
	
Setting up libzip4:amd64 (1.1.2-1.1+b1) ...
	
"", :err """", :host ""172.31.4.8"", :action {:cmd ""sudo -S -u root bash -c \""cd /; env DEBIAN_FRONTEND=noninteractive apt-get install -y --force-yes libzip4\"""", :in ""root""}}
{code}

https://app.travis-ci.com/github/dataArtisans/flink-jepsen-ci/jobs/550915650#L1300",,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 08 10:42:31 UTC 2021,,,,,,,,,,"0|z0xdko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/21 10:42;chesnay;Upgraded Jepsen to 0.1.19, which should fix the issue.

master: d2da7b085ff0c4c16ef4188c30a9f68fdfeaad44;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When the DDL statement was executed, the column names of the Derived Columns were not validated",FLINK-25171,13415491,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,m_jelly,m_jelly,m_jelly,06/Dec/21 03:53,25/Jan/22 17:10,13/Jul/23 08:12,25/Jan/22 04:35,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.4,1.15.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"When I execute the DDL statement, I mistakenly use the duplicate field name in THE SQL, but the execution result of the program does not throw any exception or prompt. In MergeTableLikeUtilTest. Java# mergePhysicalColumns add repeated the TableColumn (), also do not throw any exceptions, review the code logic found only on the source table schema fields, It is not a duplicate and derived tables and fields of the source table is verified, and no field of derived tables if repeated verification, adding physicalFieldNamesToTypes, there will be a repeating field coverage,The following are the execution statements and the results

DDL sql：

CREATE TABLE test1 (
  `log_version` string COMMENT '日志版本',
  `log_version` INTEGER COMMENT '日志版本',
  `pv_time` string COMMENT '日志时间'
) with(
        'connector' = 'kafka',
        'topic' = 'xxx',
        'properties.bootstrap.servers' = 'xxx:9110',
        'scan.startup.mode'='latest-offset',
        'format' = 'json',
)
{code:java}
StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);
TableResult result = tEnv.executeSql(
CREATE TABLE test1 (
  `log_version` string COMMENT '日志版本',
  `log_version` INTEGER COMMENT '日志版本',
  `pv_time` string COMMENT '日志时间'
) with(
        'connector' = 'kafka',
        'topic' = 'xxx',
        'properties.bootstrap.servers' = 'xxx:9110',
        'scan.startup.mode'='latest-offset',
        'format' = 'json',
)
) 

{code}",,godfreyhe,libenchao,lsy,m_jelly,wenlong.lwl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/21 07:28;m_jelly;5261638775663_.pic.jpg;https://issues.apache.org/jira/secure/attachment/13037024/5261638775663_.pic.jpg",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Tue Jan 25 04:35:40 UTC 2022,,,,,,,,,,"0|z0xddc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/21 06:14;godfreyhe;can you given an example ?;;;","06/Dec/21 08:25;m_jelly;[~godfreyhe]  Hello, I have submitted an example, thanks for your attention;;;","07/Dec/21 07:37;wenlong.lwl;I think it is good to not allow duplicated column name in both tables and views, the result could be weird and hard to debug, when there are tens of column in a table. ;;;","12/Jan/22 05:49;m_jelly;[~godfreyhe] Hi，the solution to this problem has been provided, please spare some time to do the final check;;;","25/Jan/22 04:35;godfreyhe;Fixed in 
master: 34de3989a613cf7124f9e301cb8284080f4df4ac
1.14.4: 70acbc591c8c66e82e5fb140f3d49afb0cc107bc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.flink.connector.file.src.reader.TextLineInputFormat.createReader failed due to violate ApiAnnotationRules,FLINK-25150,13415191,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gaoyunhaii,gaoyunhaii,gaoyunhaii,03/Dec/21 07:22,14/Dec/21 07:36,13/Jul/23 08:12,14/Dec/21 07:36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,API / DataStream,,,,,0,pull-request-available,test-stability,,,"{code:java}
021-12-02T21:44:11.4722598Z Dec 02 21:44:11 [ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 38.799 s <<< FAILURE! - in org.apache.flink.architecture.rules.ApiAnnotationRules
2021-12-02T21:44:11.4724167Z Dec 02 21:44:11 [ERROR] ApiAnnotationRules.PUBLIC_EVOLVING_API_METHODS_USE_ONLY_PUBLIC_EVOLVING_API_TYPES  Time elapsed: 0.231 s  <<< FAILURE!
2021-12-02T21:44:11.4725222Z Dec 02 21:44:11 java.lang.AssertionError: 
2021-12-02T21:44:11.4726849Z Dec 02 21:44:11 Architecture Violation [Priority: MEDIUM] - Rule 'Return and argument types of methods annotated with @PublicEvolving must be annotated with @Public(Evolving).' was violated (2 times):
2021-12-02T21:44:11.4730100Z Dec 02 21:44:11 org.apache.flink.connector.file.src.reader.TextLineInputFormat.createReader(org.apache.flink.configuration.Configuration, org.apache.flink.core.fs.FSDataInputStream): Returned leaf type org.apache.flink.connector.file.src.reader.StreamFormat$Reader does not satisfy: reside outside of package 'org.apache.flink..' or annotated with @Public or annotated with @PublicEvolving or annotated with @Deprecated
2021-12-02T21:44:11.4734006Z Dec 02 21:44:11 org.apache.flink.connector.file.src.reader.TextLineInputFormat.createReader(org.apache.flink.configuration.Configuration, org.apache.flink.core.fs.FSDataInputStream): Returned leaf type org.apache.flink.connector.file.src.reader.TextLineInputFormat$Reader does not satisfy: reside outside of package 'org.apache.flink..' or annotated with @Public or annotated with @PublicEvolving or annotated with @Deprecated
2021-12-02T21:44:11.4736377Z Dec 02 21:44:11 	at com.tngtech.archunit.lang.ArchRule$Assertions.assertNoViolation(ArchRule.java:94)
2021-12-02T21:44:11.4737400Z Dec 02 21:44:11 	at com.tngtech.archunit.lang.ArchRule$Assertions.check(ArchRule.java:82)
2021-12-02T21:44:11.4738529Z Dec 02 21:44:11 	at com.tngtech.archunit.library.freeze.FreezingArchRule.check(FreezingArchRule.java:96)
2021-12-02T21:44:11.4739712Z Dec 02 21:44:11 	at com.tngtech.archunit.junit.ArchUnitTestDescriptor$ArchUnitRuleDescriptor.execute(ArchUnitTestDescriptor.java:159)
2021-12-02T21:44:11.4740983Z Dec 02 21:44:11 	at com.tngtech.archunit.junit.ArchUnitTestDescriptor$ArchUnitRuleDescriptor.execute(ArchUnitTestDescriptor.java:142)
2021-12-02T21:44:11.4742309Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2021-12-02T21:44:11.4743532Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-02T21:44:11.4744736Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2021-12-02T21:44:11.4745846Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-12-02T21:44:11.4747137Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2021-12-02T21:44:11.4748480Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-02T21:44:11.4749659Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2021-12-02T21:44:11.4750777Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2021-12-02T21:44:11.4751822Z Dec 02 21:44:11 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2021-12-02T21:44:11.4752983Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
2021-12-02T21:44:11.4754345Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2021-12-02T21:44:11.4755550Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-02T21:44:11.4756753Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2021-12-02T21:44:11.4757869Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-12-02T21:44:11.4759122Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2021-12-02T21:44:11.4760323Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-02T21:44:11.4761553Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2021-12-02T21:44:11.4762666Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2021-12-02T21:44:11.4763654Z Dec 02 21:44:11 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2021-12-02T21:44:11.4764819Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
2021-12-02T21:44:11.4766132Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2021-12-02T21:44:11.4767336Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-02T21:44:11.4768776Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2021-12-02T21:44:11.4769897Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-12-02T21:44:11.4771005Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2021-12-02T21:44:11.4772295Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-02T21:44:11.4773468Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2021-12-02T21:44:11.4774587Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2021-12-02T21:44:11.4775723Z Dec 02 21:44:11 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2021-12-02T21:44:11.4776915Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
2021-12-02T21:44:11.4778368Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2021-12-02T21:44:11.4779579Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-02T21:44:11.4780914Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2021-12-02T21:44:11.4782092Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-12-02T21:44:11.4783212Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2021-12-02T21:44:11.4784423Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-12-02T21:44:11.4785576Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2021-12-02T21:44:11.4786709Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2021-12-02T21:44:11.4787992Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
2021-12-02T21:44:11.4789458Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
2021-12-02T21:44:11.4790723Z Dec 02 21:44:11 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
2021-12-02T21:44:11.4792006Z Dec 02 21:44:11 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2021-12-02T21:44:11.4793228Z Dec 02 21:44:11 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2021-12-02T21:44:11.4794454Z Dec 02 21:44:11 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2021-12-02T21:44:11.4795736Z Dec 02 21:44:11 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2021-12-02T21:44:11.4796973Z Dec 02 21:44:11 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2021-12-02T21:44:11.4798172Z Dec 02 21:44:11 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2021-12-02T21:44:11.4799229Z Dec 02 21:44:11 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2021-12-02T21:44:11.4800377Z Dec 02 21:44:11 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2021-12-02T21:44:11.4801782Z Dec 02 21:44:11 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2021-12-02T21:44:11.4802991Z Dec 02 21:44:11 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
2021-12-02T21:44:11.4804190Z Dec 02 21:44:11 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2021-12-02T21:44:11.4805225Z Dec 02 21:44:11 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
2021-12-02T21:44:11.4806424Z Dec 02 21:44:11 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2021-12-02T21:44:11.4807600Z Dec 02 21:44:11 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2021-12-02T21:44:11.4808837Z Dec 02 21:44:11 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2021-12-02T21:44:11.4809891Z Dec 02 21:44:11 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2021-12-02T21:44:11.4810888Z Dec 02 21:44:11 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2021-12-02T21:44:11.4811954Z Dec 02 21:44:11 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2021-12-02T21:44:11.4812823Z Dec 02 21:44:11 
2021-12-02T21:44:11.4813481Z Dec 02 21:44:11 [INFO] Running org.apache.flink.architecture.rules.TableApiRules
2021-12-02T21:44:11.5590569Z Dec 02 21:44:11 [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.082 s - in org.apache.flink.architecture.rules.TableApiRules
2021-12-02T21:44:11.5634655Z Dec 02 21:44:11 [INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 38.904 s - in org.apache.flink.architecture.ArchitectureTest
2021-12-02T21:44:12.2063747Z Dec 02 21:44:12 [INFO] 
2021-12-02T21:44:12.2064224Z Dec 02 21:44:12 [INFO] Results:
2021-12-02T21:44:12.2064637Z Dec 02 21:44:12 [INFO] 
2021-12-02T21:44:12.2064987Z Dec 02 21:44:12 [ERROR] Failures: 
2021-12-02T21:44:12.2066745Z Dec 02 21:44:12 [ERROR]   Architecture Violation [Priority: MEDIUM] - Rule 'Return and argument types of methods annotated with @PublicEvolving must be annotated with @Public(Evolving).' was violated (2 times): {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27474&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=10301",,airblader,gaoyunhaii,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 14 07:36:45 UTC 2021,,,,,,,,,,"0|z0xbj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Dec/21 07:28;gaoyunhaii;Hi [~airblader] , it seems the method does violate the rules, but may I double confirm that it seems the tests has enabled for some days and the related code seems also have no change, so why the issue starts to pop up from today? ;;;","03/Dec/21 07:45;gaoyunhaii;I'll first fix the violation.;;;","03/Dec/21 08:10;airblader;It's possible this was caused by changes to dependencies and a not-rebased PR. Thanks for fixing the violations!;;;","14/Dec/21 07:33;trohrmann;[~gaoyunhaii] can this ticket be closed?;;;","14/Dec/21 07:36;gaoyunhaii;Yes, it is fixed, I'll close it~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming File Sink s3 end-to-end test failed due to job has not started within a timeout of 10 sec,FLINK-25140,13414934,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,gaoyunhaii,gaoyunhaii,02/Dec/21 03:25,02/Dec/21 09:06,13/Jul/23 08:12,02/Dec/21 07:43,1.13.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / FileSystem,,,,,0,test-stability,,,,"{code:java}
Dec 01 19:06:38 Starting taskexecutor daemon on host fv-az26-327.
Dec 01 19:06:38 Submitting job.
Dec 01 19:06:54 Job (62f9a00856309492574699642574071c) is not yet running.
Dec 01 19:06:57 Job (62f9a00856309492574699642574071c) is not yet running.
Dec 01 19:07:00 Job (62f9a00856309492574699642574071c) is not yet running.
Dec 01 19:07:03 Job (62f9a00856309492574699642574071c) is not yet running.
Dec 01 19:07:06 Job (62f9a00856309492574699642574071c) is not yet running.
Dec 01 19:07:09 Job (62f9a00856309492574699642574071c) is not yet running.
Dec 01 19:07:12 Job (62f9a00856309492574699642574071c) is not yet running.
Dec 01 19:07:15 Job (62f9a00856309492574699642574071c) is not yet running.
Dec 01 19:07:18 Job (62f9a00856309492574699642574071c) is not yet running.
Dec 01 19:07:21 Job (62f9a00856309492574699642574071c) is not yet running.
Dec 01 19:07:22 Job (62f9a00856309492574699642574071c) has not started within a timeout of 10 sec
Dec 01 19:07:22 Stopping job timeout watchdog (with pid=401626)
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27382&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=12560",,gaoyunhaii,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25139,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 02 09:06:10 UTC 2021,,,,,,,,,,"0|z0x9y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/21 07:43;martijnvisser;This test failed due to the related ticket;;;","02/Dec/21 08:25;gaoyunhaii;Very thanks [~MartijnVisser] for investigating the issue!;;;","02/Dec/21 09:06;martijnvisser;My pleasure [~gaoyunhaii];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S3 tests failing due to rotating credentials,FLINK-25139,13414931,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,gaoyunhaii,gaoyunhaii,02/Dec/21 03:09,02/Dec/21 07:46,13/Jul/23 08:12,02/Dec/21 07:20,1.13.3,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FileSystems,,,,,0,test-stability,,,,"{code:java}
Dec 01 19:33:39 	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1232) ~[?:?]
Dec 01 19:33:39 	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2169) ~[?:?]
Dec 01 19:33:39 	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2149) ~[?:?]
Dec 01 19:33:39 	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2088) ~[?:?]
Dec 01 19:33:39 	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1734) ~[?:?]
Dec 01 19:33:39 	at org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:2970) ~[?:?]
Dec 01 19:33:39 	at org.apache.flink.fs.s3hadoop.common.HadoopFileSystem.exists(HadoopFileSystem.java:165) ~[?:?]
Dec 01 19:33:39 	at org.apache.flink.core.fs.PluginFileSystemFactory$ClassLoaderFixingFileSystem.exists(PluginFileSystemFactory.java:148) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Dec 01 19:33:39 	at org.apache.flink.core.fs.FileSystem.initOutPathDistFS(FileSystem.java:984) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Dec 01 19:33:39 	at org.apache.flink.api.common.io.FileOutputFormat.initializeGlobal(FileOutputFormat.java:299) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Dec 01 19:33:39 	at org.apache.flink.runtime.jobgraph.InputOutputFormatVertex.initializeOnMaster(InputOutputFormatVertex.java:110) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Dec 01 19:33:39 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraphBuilder.buildGraph(DefaultExecutionGraphBuilder.java:174) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Dec 01 19:33:39 	at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:107) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Dec 01 19:33:39 	at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:342) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Dec 01 19:33:39 	at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:190) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Dec 01 19:33:39 	at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:122) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Dec 01 19:33:39 	at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:132) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Dec 01 19:33:39 	at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:110) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Dec 01 19:33:39 	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:340) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Dec 01 19:33:39 	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:317) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Dec 01 19:33:39 	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:107) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Dec 01 19:33:39 	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:95) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Dec 01 19:33:39 	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Dec 01 19:33:39 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604) ~[?:1.8.0_292]
Dec 01 19:33:39 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_292]
Dec 01 19:33:39 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_292]
Dec 01 19:33:39 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_292]
Dec 01 19:33:39 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) ~[?:1.8.0_292]
Dec 01 19:33:39 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_292]
Dec 01 19:33:39 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_292]
Dec 01 19:33:39 	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292]
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27379&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=23278",,gaoyunhaii,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25140,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 02 03:14:25 UTC 2021,,,,,,,,,,"0|z0x9xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/21 03:10;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27381&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=13299];;;","02/Dec/21 03:11;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27381&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=15547];;;","02/Dec/21 03:13;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27382&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=12605;;;","02/Dec/21 03:13;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27384&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=15525];;;","02/Dec/21 03:14;gaoyunhaii;The tests should be failed due to rotating credentials;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBase downloads in e2e tests fail often,FLINK-25135,13414779,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,01/Dec/21 12:13,03/Jan/22 12:46,13/Jul/23 08:12,03/Jan/22 12:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / HBase,Test Infrastructure,Tests,,,0,,,,,This is an umbrella ticket for all HBase download related failures during e2e testing,,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 03 12:46:38 UTC 2022,,,,,,,,,,"0|z0x8zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/21 19:30;martijnvisser;I'm keeping this ticket open for now, even though I expect that it should be resolved. I do suspect that there's still an issue specifically with downloading HBase and a proper fix is to change the test to a different setup;;;","03/Jan/22 12:46;martijnvisser;This issue hasn't recurred since beginning of December so I'm closing this ticket, situation has been resolved;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unused RetryRule in KafkaConsumerTestBase swallows retries,FLINK-25134,13414774,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fpaul,fpaul,fpaul,01/Dec/21 11:47,15/Dec/21 01:44,13/Jul/23 08:12,02/Dec/21 12:01,1.13.3,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.3,1.15.0,,,Tests,,,,,0,pull-request-available,,,,After merging https://issues.apache.org/jira/browse/FLINK-15493 a few tests are still not retried because the KafkaConsumerTestBase overwrites the RetryRule unnecessarily.,,fpaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 02 12:01:30 UTC 2021,,,,,,,,,,"0|z0x8yg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/21 07:55;fpaul;Merged

master: e4c7d7863962b32b1ea1f799278289e0fe07bf70
release-1.14: 0d9aed95ad0618ef190fd219251be0e068320a44;;;","02/Dec/21 12:01;fpaul;Merged

release-1.13: 9843fa90e281e24fc9bf00018cab0ea2e1e776af;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSource cannot work with object-reusing DeserializationSchema,FLINK-25132,13414743,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,renqs,renqs,renqs,01/Dec/21 09:57,09/Jun/23 16:47,13/Jul/23 08:12,27/Dec/21 02:15,1.14.0,1.14.1,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"Currently Kafka source deserializes ConsumerRecords in split reader and puts them into the elementQueue, then task's main thread polls these records from the queue asynchronously. This mechanism cannot cooperate with DeserializationSchemas with object reuse: all records staying in the element queue points to the same object.

A solution would be moving deserialization to RecordEmitter, which works in the task's main thread. 

Notes that this issue actually effects all sources which do deserialization in split reader. ",,liliwei,martijnvisser,mason6345,renqs,syhily,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32303,,,,,,,FLINK-28083,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 27 02:15:20 UTC 2021,,,,,,,,,,"0|z0x8s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/21 22:43;mason6345;[~renqs] does this affect 1.13.3?;;;","08/Dec/21 06:25;renqs;[~mason6345] I think so. We'll make a back-port on 1.13 after fix this on master;;;","14/Dec/21 07:33;trohrmann;[~renqs] what is missing for the PR to be merged?;;;","14/Dec/21 07:41;renqs;[~trohrmann] It's waiting for another round of review from Becket. [~jqin] Could you PTAL? Thanks!;;;","27/Dec/21 02:15;renqs;Merged to master: 2b1a9dea74a334adb1fe890f024f4153ad11a985
release-1.14: ebbf772ea287ee987f5eb628ad2e395895b312aa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkKafkaInternalProducer state is not reset if transaction finalization fails,FLINK-25126,13414698,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fpaul,tonyboo9527,tonyboo9527,01/Dec/21 08:11,15/Dec/21 01:44,13/Jul/23 08:12,10/Dec/21 15:05,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"flinksql task submitted by sql client will failed,

this is the sql :

SET 'execution.runtime-mode' = 'batch';

 CREATE TABLE ka15 (
     name String,
     cnt bigint
 ) WITH (
   'connector' = 'kafka',
   'topic' = 'shifang8888',
  'properties.bootstrap.servers' = 'flinkx1:9092',
  'properties.transaction.timeout.ms' = '800000',
  'properties.max.block.ms' = '300000',
   'value.format' = 'json',
   'sink.parallelism' = '2',
   'sink.delivery-guarantee' = 'exactly-once',
   'sink.transactional-id-prefix' = 'dtstack9999');

 

insert into ka15 SELECT
  name,
  cnt
FROM
  (VALUES ('Bob',100), ('Alice',100), ('Greg',100), ('Bob',100)) AS NameTable(name,cnt);

 

this is the error:

Caused by: java.lang.IllegalStateException
at org.apache.flink.util.Preconditions.checkState(Preconditions.java:177)
at org.apache.flink.connector.kafka.sink.FlinkKafkaInternalProducer.setTransactionId(FlinkKafkaInternalProducer.java:164)
at org.apache.flink.connector.kafka.sink.KafkaCommitter.getRecoveryProducer(KafkaCommitter.java:144)
at org.apache.flink.connector.kafka.sink.KafkaCommitter.lambda$commit$0(KafkaCommitter.java:76)
at java.util.Optional.orElseGet(Optional.java:267)
at org.apache.flink.connector.kafka.sink.KafkaCommitter.commit(KafkaCommitter.java:76)
... 14 more
 
 
i found the reason why  kafka commit failed, when downstream operator CommitterOperator was commiting transaction, the upstream  operator SinkOperator has closed , it will abort the transaction which  is committing by CommitterOperator, this is occurs when execution.runtime-mode is batch","SET 'execution.runtime-mode' = 'batch';

 CREATE TABLE ka15 (
     name String,
     cnt bigint
 ) WITH (
   'connector' = 'kafka',
   'topic' = 'shifang8888',
  'properties.bootstrap.servers' = 'flinkx1:9092',
  'properties.transaction.timeout.ms' = '800000',
  'properties.max.block.ms' = '300000',
   'value.format' = 'json',
   'sink.parallelism' = '2',
   'sink.delivery-guarantee' = 'exactly-once',
   'sink.transactional-id-prefix' = 'dtstack9999');

 

insert into ka15 SELECT
  name,
  cnt
FROM
  (VALUES ('Bob',100), ('Alice',100), ('Greg',100), ('Bob',100)) AS NameTable(name,cnt);",fpaul,lzljs3620320,tonyboo9527,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 10 15:04:50 UTC 2021,,,,,,,,,,"0|z0x8i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/21 08:20;lzljs3620320;+1;;;","01/Dec/21 08:24;tonyboo9527;[~rmetzger] ;;;","01/Dec/21 08:25;lzljs3620320;CC:  [~fpaul] ;;;","01/Dec/21 08:29;fpaul;I'll take that we fix it before releasing 1.14.1;;;","01/Dec/21 09:42;tonyboo9527;[~fpaul]  hi,I wonder to know your ideas to solve this problem, 
I fix the problem by modify the source code of KafkaWriter, when KafkaWriter executing the method close,before aborting transcation ,I let the thread sleep(transaction.timeout.ms);;;","03/Dec/21 12:09;fpaul;[~tonyboo9527] thanks for the investigation. I had a look and think the problem is a slightly different one. I suspect that somewhere in your Kafka logs the committing failed and was retried. In this case, the old KafkaProducer is reused and a new transactional id is set [1].

Unfortunately, if the committing fails [2] we do not reset the `inTransaction` variable so during the recycling it seems that another transaction is still open and it fails. I am preparing a fix to always reset the `inTransaction` variable.

[1] https://github.com/apache/flink/blob/bbfe6521c2dfe9c48b810e7266e8dc3e5a501f21/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducer.java#L162

 [2] [https://github.com/apache/flink/blob/bbfe6521c2dfe9c48b810e7266e8dc3e5a501f21/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducer.java#L97] ;;;","10/Dec/21 09:02;fpaul;Merged release-1.14: dccd7f08cde2b13ba4549c94ebbc04ff2c0c5152;;;","10/Dec/21 15:04;fpaul;Merged master: 577648379c2abb429259ac1a46ca6a04550f3dbd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GlueSchemaRegistryAvroKinesisITCase and GlueSchemaRegistryJsonKinesisITCase are skipped on AzureCI but fail when enabled,FLINK-25107,13414479,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dannycranmer,mapohl,mapohl,30/Nov/21 08:27,11/Jan/22 17:36,13/Jul/23 08:12,11/Jan/22 17:36,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,test-stability,,,"[GlueSchemaRegistryAvroKinesisITCase|https://github.com/apache/flink/blob/master/flink-end-to-end-tests/flink-glue-schema-registry-avro-test/src/test/java/org/apache/flink/glue/schema/registry/test/GlueSchemaRegistryAvroKinesisITCase.java] and [GlueSchemaRegistryJsonKinesisITCase|https://github.com/apache/flink/blob/master/flink-end-to-end-tests/flink-glue-schema-registry-json-test/src/test/java/org/apache/flink/glue/schema/registry/test/json/GlueSchemaRegistryJsonKinesisITCase.java], are not executed on Azure CI runs, because the access key and secret key env variables are not present, see e.g. [this run|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26986&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=15852].

Then, during recent testing on Github Actions, we noticed that the tests don't work even if the env variables are present because AWS expects different variable names (it expects AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY). See [this run|https://github.com/ververica/flink/runs/4301833493?check_suite_focus=true#step:13:17885].
{code:java}
Nov 23 18:40:46 Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), WebIdentityTokenCredentialsProvider(), ProfileCredentialsProvider(), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., WebIdentityTokenCredentialsProvider(): Either the environment variable AWS_WEB_IDENTITY_TOKEN_FILE or the javaproperty aws.webIdentityTokenFile must be set., ProfileCredentialsProvider(): Profile file contained no credentials for profile 'default': ProfileFile(profiles=[]), ContainerCredentialsProvider(): Cannot fetch credentials from container - neither AWS_CONTAINER_CREDENTIALS_FULL_URI or AWS_CONTAINER_CREDENTIALS_RELATIVE_URI environment variables are set., InstanceProfileCredentialsProvider(): The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/]
{code}
Finally, even with correct env variable naming, the test still fails because of an assertion error. It looks like only the first record ever makes it to the results. See [this run|https://github.com/ververica/flink/runs/4315084463?check_suite_focus=true#step:13:5317] for the error, also posted here for convenience:
{code:java}
Nov 24 18:00:55 java.lang.AssertionError: Results received from 'gsr_json_output_stream': [JsonDataWithSchema(schema={""$id"":""https://example.com/address.schema.json"",""$schema"":""http://json-schema.org/draft-07/schema#"",""type"":""object"",""properties"":{""f1"":{""type"":""string""},""f2"":{""type"":""integer"",""maximum"":10000}}}, payload={""f1"":""olympic"",""f2"":2020})] expected:<8> but was:<1>
{code}
FYI: [~Nicolaus Weidner]  investigated this issue and provided these details",,dannycranmer,mapohl,martijnvisser,Nicolaus Weidner,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23389,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 11 17:36:02 UTC 2022,,,,,,,,,,"0|z0x75c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/21 09:48;dannycranmer;Taking a look;;;","30/Nov/21 11:11;dannycranmer;This was a miss on the migration from {{.sh}} to {{java}} e2e tests. Before, we were setting the {{AWS_ACCESS_KEY_ID/AWS_SECRET_ACCESS_KEY}} from {{IT_CASE_GLUE_SCHEMA_ACCESS_KEY/IT_CASE_GLUE_SCHEMA_SECRET_KEY}} keys as system variables. I am not inclined to do this for the java suite, since it is more difficult to isolate properties across tests. Currently the GSR format does not have a mechanism to pass AWS keys in the properties map, which is how I would like to resolve this issue. To avoid reinventing the wheel, we are refactoring the way AWS keys are defined in config in FLINK-24227. We will provide a common mechanism to define AWS keys which will be reused for Kinesis Data Streams, DynamoDB, Kinesis Firehose and Glue Schema Registry. We are working towards a PR for this now, and I will pickup this issue once that is merged.;;;","30/Nov/21 16:37;Nicolaus Weidner;Thanks for looking into it, [~dannycranmer], that sounds good! I also stumbled across the issues that you described (system variables in java tests being a pain and no other obvious way to set the key) and having a common mechanism sounds preferrable.;;;","14/Dec/21 07:34;trohrmann;Hi [~dannycranmer] can you give a quick update on the status of this ticket?;;;","14/Dec/21 08:54;dannycranmer;I have opened a PR with tests refactored and passing. I have 2 follow ups to do before I merge:
- Update test code to junit 5 + AssertJ
- Update documentation

I am planning on getting back to it this week, but have been rather busy over the last week!;;;","16/Dec/21 10:41;trohrmann;Great, thanks for the update and the work [~dannycranmer]!;;;","10/Jan/22 21:09;trohrmann;[~dannycranmer] do you think that you can finish the open PR or shall somebody else jump in?;;;","10/Jan/22 21:17;dannycranmer;[~trohrmann] sorry for the delay, I have been oncall for the last week. I finished my rotation today so can get back to this now. Will rebase the PR for now.;;;","11/Jan/22 17:36;dannycranmer;This is merged. I have verified both tests pass locally when the AWS keys are set, using: {{mvn clean install -Prun-end-to-end-tests}} ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 ParquetFileSystemITCase.testPartialDynamicPartition failed on azure,FLINK-25102,13414454,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,gaoyunhaii,gaoyunhaii,30/Nov/21 06:06,06/Dec/21 08:47,13/Jul/23 08:12,06/Dec/21 08:47,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Connectors / FileSystem,,,,,0,test-stability,,,,"{code:java}
Nov 29 23:00:01 	at org.apache.hadoop.conf.Configuration.getResource(Configuration.java:2780)
Nov 29 23:00:01 	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3036)
Nov 29 23:00:01 	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2995)
Nov 29 23:00:01 	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2968)
Nov 29 23:00:01 	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2848)
Nov 29 23:00:01 	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1200)
Nov 29 23:00:01 	at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1254)
Nov 29 23:00:01 	at org.apache.hadoop.conf.Configuration.getInt(Configuration.java:1479)
Nov 29 23:00:01 	at org.apache.parquet.hadoop.codec.SnappyCodec.createInputStream(SnappyCodec.java:75)
Nov 29 23:00:01 	at org.apache.parquet.hadoop.CodecFactory$HeapBytesDecompressor.decompress(CodecFactory.java:109)
Nov 29 23:00:01 	at org.apache.parquet.hadoop.ColumnChunkPageReadStore$ColumnChunkPageReader$1.visit(ColumnChunkPageReadStore.java:103)
Nov 29 23:00:01 	at org.apache.parquet.hadoop.ColumnChunkPageReadStore$ColumnChunkPageReader$1.visit(ColumnChunkPageReadStore.java:99)
Nov 29 23:00:01 	at org.apache.parquet.column.page.DataPageV1.accept(DataPageV1.java:120)
Nov 29 23:00:01 	at org.apache.parquet.hadoop.ColumnChunkPageReadStore$ColumnChunkPageReader.readPage(ColumnChunkPageReadStore.java:99)
Nov 29 23:00:01 	at org.apache.flink.formats.parquet.vector.reader.AbstractColumnReader.readToVector(AbstractColumnReader.java:154)
Nov 29 23:00:01 	at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat$ParquetReader.nextBatch(ParquetVectorizedInputFormat.java:390)
Nov 29 23:00:01 	at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat$ParquetReader.readBatch(ParquetVectorizedInputFormat.java:358)
Nov 29 23:00:01 	at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:67)
Nov 29 23:00:01 	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
Nov 29 23:00:01 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)
Nov 29 23:00:01 	... 6 more
Nov 29 23:00:01 
Nov 29 23:00:02 [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.341 s - in org.apache.flink.formats.parquet.ParquetFileCompactionITCase
Nov 29 23:00:03 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
Nov 29 23:00:03 [INFO] Running org.apache.flink.formats.parquet.ParquetFsStreamingSinkITCase
Nov 29 23:00:20 [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 17.033 s - in org.apache.flink.formats.parquet.ParquetFsStreamingSinkITCase
Nov 29 23:00:21 [INFO] 
Nov 29 23:00:21 [INFO] Results:
Nov 29 23:00:21 [INFO] 
Nov 29 23:00:21 [ERROR] Errors: 
Nov 29 23:00:21 [ERROR] ParquetFileSystemITCase.testPartialDynamicPartition
Nov 29 23:00:21 [ERROR]   Run 1: Failed to fetch next result
Nov 29 23:00:21 [INFO]   Run 2: PASS
Nov 29 23:00:21 [INFO] 
Nov 29 23:00:21 [INFO] 
Nov 29 23:00:21 [ERROR] Tests run: 42, Failures: 0, Errors: 1, Skipped: 1
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27235&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=11545",,gaoyunhaii,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24763,FLINK-25071,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 06 08:47:57 UTC 2021,,,,,,,,,,"0|z0x6zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/21 06:07;gaoyunhaii;Hi [~lzljs3620320] this issue seems to have the same cause with https://issues.apache.org/jira/browse/FLINK-24763, could you have a double look on this issue or if we need to also ignore this case~?;;;","30/Nov/21 06:20;lzljs3620320;Thanks [~gaoyunhaii] for reporting, I will take a look~;;;","01/Dec/21 06:21;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27319&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=11546];;;","02/Dec/21 03:26;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27396&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=11549];;;","02/Dec/21 09:58;lzljs3620320;FLINK-25071  merged, let's see if the problem still recurs.;;;","06/Dec/21 08:47;lzljs3620320;Closed, feel free to re-open this if failure re-produced;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in inner join when the filter condition is boolean type,FLINK-25097,13414315,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,xuechu,xuechu,29/Nov/21 14:28,05/Jul/22 14:09,13/Jul/23 08:12,05/Jul/22 14:09,1.12.2,1.13.0,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.2,1.16.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"When I test the inner join, the column type of the filter condition is Boolean, and there is an error in the SQL conversion process。
The SQL as follow：
{code:java}
source-1：
""CREATE TABLE IF NOT EXISTS data_source (\n"" +
"" id INT,\n"" +
"" name STRING,\n"" +
"" sex boolean\n"" +
"") WITH (\n"" +
"" 'connector' = 'datagen',\n"" +
"" 'rows-per-second'='1',\n"" +
"" 'fields.id.kind'='sequence',\n"" +
"" 'fields.id.start'='1',\n"" +
"" 'fields.id.end'='10',\n"" +
"" 'fields.name.kind'='random',\n"" +
"" 'fields.name.length'='10'\n"" +
"")"";   
source-2：
""CREATE TABLE IF NOT EXISTS info (\n"" +
"" id INT,\n"" +
"" name STRING,\n"" +
"" sex boolean\n"" +
"") WITH (\n"" +
"" 'connector' = 'datagen',\n"" +
"" 'rows-per-second'='1',\n"" +
"" 'fields.id.kind'='sequence',\n"" +
"" 'fields.id.start'='1',\n"" +
"" 'fields.id.end'='10',\n"" +
"" 'fields.name.kind'='random',\n"" +
"" 'fields.name.length'='10'\n"" +
"")"";   
sink：
""CREATE TABLE IF NOT EXISTS print_sink ( \n"" +
"" id INT,\n"" +
"" name STRING,\n"" +
"" left_sex boolean,\n"" +
"" right_sex boolean\n"" +
"") WITH (\n"" +
"" 'connector' = 'print'\n"" +
"")"";    

SQL-1:
""insert into print_sink"" +
"" select l.id, l.name, l.sex, r.sex from data_source l "" +
""inner join info r on l.sex = r.sex where l.sex is true"";{code}
The SQL fails with：
{code:java}
The program finished with the following exception:
org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Error while applying rule FlinkLogicalCalcConverter(in:NONE,out:LOGICAL), args [rel#135:LogicalCalc.NONE.any.None: 0.[NONE].[NONE](input=RelSubset#115,expr#0..5={inputs},proj#0..2={exprs},3=$t5)]
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:366)
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:219)
        at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)
        at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:812)
        at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:246)
        at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1054)
        at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1132)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
        at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1132)
Caused by: java.lang.RuntimeException: Error while applying rule FlinkLogicalCalcConverter(in:NONE,out:LOGICAL), args [rel#135:LogicalCalc.NONE.any.None: 0.[NONE].[NONE](input=RelSubset#115,expr#0..5={inputs},proj#0..2={exprs},3=$t5)]
        at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:256)
        at org.apache.calcite.plan.volcano.IterativeRuleDriver.drive(IterativeRuleDriver.java:58)
        at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:510)
        at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:312)
        at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:64)
        at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
        at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
        at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
        at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
        at scala.collection.Iterator$class.foreach(Iterator.scala:891)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
        at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
        at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
        at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
        at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:163)
        at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:79)
        at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
        at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:287)
        at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:100)
        at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:42)
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainSql(TableEnvironmentImpl.java:625)
        at com.xue.testSql.main(testSql.java:60)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:349)
        ... 11 more
Caused by: java.lang.RuntimeException: Error occurred while applying rule FlinkLogicalCalcConverter(in:NONE,out:LOGICAL)
        at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:161)
        at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:268)
        at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:283)
        at org.apache.calcite.rel.convert.ConverterRule.onMatch(ConverterRule.java:169)
        at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:229)
        ... 39 more
Caused by: java.lang.ClassCastException: org.apache.calcite.rex.RexInputRef cannot be cast to org.apache.calcite.rex.RexCall
        at org.apache.flink.table.planner.plan.utils.ColumnIntervalUtil$.org$apache$flink$table$planner$plan$utils$ColumnIntervalUtil$$columnIntervalOfSinglePredicate(ColumnIntervalUtil.scala:236)
        at org.apache.flink.table.planner.plan.utils.ColumnIntervalUtil$$anonfun$5$$anonfun$6.apply(ColumnIntervalUtil.scala:223)
        at org.apache.flink.table.planner.plan.utils.ColumnIntervalUtil$$anonfun$5$$anonfun$6.apply(ColumnIntervalUtil.scala:223)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.Iterator$class.foreach(Iterator.scala:891)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
        at scala.collection.AbstractTraversable.map(Traversable.scala:104)
        at org.apache.flink.table.planner.plan.utils.ColumnIntervalUtil$$anonfun$5.apply(ColumnIntervalUtil.scala:223)
        at org.apache.flink.table.planner.plan.utils.ColumnIntervalUtil$$anonfun$5.apply(ColumnIntervalUtil.scala:221)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.Iterator$class.foreach(Iterator.scala:891)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
        at scala.collection.AbstractTraversable.map(Traversable.scala:104)
        at org.apache.flink.table.planner.plan.utils.ColumnIntervalUtil$.getColumnIntervalWithFilter(ColumnIntervalUtil.scala:221)
        at org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.getColumnIntervalOfCalc(FlinkRelMdColumnInterval.scala:227)
        at org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.getColumnInterval(FlinkRelMdColumnInterval.scala:203)
        at GeneratedMetadataHandler_ColumnInterval.getColumnInterval_$(Unknown Source)
        at GeneratedMetadataHandler_ColumnInterval.getColumnInterval(Unknown Source)
        at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getColumnInterval(FlinkRelMetadataQuery.java:112)
        at org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.getColumnInterval(FlinkRelMdColumnInterval.scala:801)
        at GeneratedMetadataHandler_ColumnInterval.getColumnInterval_$(Unknown Source)
        at GeneratedMetadataHandler_ColumnInterval.getColumnInterval(Unknown Source)
        at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getColumnInterval(FlinkRelMetadataQuery.java:112)
        at org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount$$anonfun$1.apply(FlinkRelMdRowCount.scala:308)
        at org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount$$anonfun$1.apply(FlinkRelMdRowCount.scala:306)
        at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38)
        at scala.collection.IndexedSeqOptimized$class.exists(IndexedSeqOptimized.scala:46)
        at scala.collection.mutable.ArrayBuffer.exists(ArrayBuffer.scala:48)
        at org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount.getEquiInnerJoinRowCount(FlinkRelMdRowCount.scala:306)
        at org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount.getRowCount(FlinkRelMdRowCount.scala:268)
        at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source)
        at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source)
        at org.apache.calcite.rel.metadata.RelMetadataQuery.getRowCount(RelMetadataQuery.java:212)
        at org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount.getRowCount(FlinkRelMdRowCount.scala:410)
        at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source)
        at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source)
        at org.apache.calcite.rel.metadata.RelMetadataQuery.getRowCount(RelMetadataQuery.java:212)
        at org.apache.calcite.rel.metadata.RelMdUtil.estimateFilteredRows(RelMdUtil.java:766)
        at org.apache.calcite.rel.metadata.RelMdUtil.estimateFilteredRows(RelMdUtil.java:761)
        at org.apache.flink.table.planner.plan.metadata.FlinkRelMdRowCount.getRowCount(FlinkRelMdRowCount.scala:62)
        at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source)
        at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source)
        at org.apache.calcite.rel.metadata.RelMetadataQuery.getRowCount(RelMetadataQuery.java:212)
        at org.apache.flink.table.planner.plan.nodes.common.CommonCalc.computeSelfCost(CommonCalc.scala:59)
        at org.apache.flink.table.planner.plan.metadata.FlinkRelMdNonCumulativeCost.getNonCumulativeCost(FlinkRelMdNonCumulativeCost.scala:41)
        at GeneratedMetadataHandler_NonCumulativeCost.getNonCumulativeCost_$(Unknown Source)
        at GeneratedMetadataHandler_NonCumulativeCost.getNonCumulativeCost(Unknown Source)
        at org.apache.calcite.rel.metadata.RelMetadataQuery.getNonCumulativeCost(RelMetadataQuery.java:288)
        at org.apache.calcite.plan.volcano.VolcanoPlanner.getCost(VolcanoPlanner.java:705)
        at org.apache.calcite.plan.volcano.RelSubset.propagateCostImprovements0(RelSubset.java:415)
        at org.apache.calcite.plan.volcano.RelSubset.propagateCostImprovements(RelSubset.java:398)
        at org.apache.calcite.plan.volcano.VolcanoPlanner.addRelToSet(VolcanoPlanner.java:1268)
        at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1227)
        at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:589)
        at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:604)
        at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:148)
        ... 43 more
{code}
I change the flink version，such as  1.12.2 ，1.13.3 and 1.14.0，this error occur in all versions during executeSql。

There is a little different in 1.12.2  between other version.The above errors will be reported directly when explain SQL-1  in 1.12.2,but other version explain SQL-1 successfully. 

Then,I modify the SQL-1 .Change l.sex from true to false
{code:java}
SQL-2:
insert into print_sink select l.id, l.name, l.sex, r.sex from data_source l inner join info r on l.sex = r.sex where l.sex is false{code}
The SQL-2 can run normally. 

 

I attempt to modify org.apache.flink.table.planner.plan.utils.ColumnIntervalUtil#columnIntervalOfSinglePredicate,like this
{code:java}
private def columnIntervalOfSinglePredicate(condition: RexNode): ValueInterval = {
//Add a judgment
if ( !condition.isInstanceOf[RexCall] ){
    return null
}
val convertedCondition = condition.asInstanceOf[RexCall]
...
}{code}
 Both SQL-1 and SQL-2 run normally.Result are [#ConditionTrueResult.txt] and [#ConditionFalseResult.txt].

 

 ",,337361684@qq.com,godfreyhe,jingzhang,Jocean,libenchao,pedrosbs,xuechu,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/21 14:16;xuechu;ColumnIntervalUtil.scala;https://issues.apache.org/jira/secure/attachment/13036762/ColumnIntervalUtil.scala","29/Nov/21 14:26;xuechu;ConditionFalseResult.txt;https://issues.apache.org/jira/secure/attachment/13036761/ConditionFalseResult.txt","29/Nov/21 14:26;xuechu;ConditionTrueResult.txt;https://issues.apache.org/jira/secure/attachment/13036760/ConditionTrueResult.txt","29/Nov/21 13:54;xuechu;errorLog.txt;https://issues.apache.org/jira/secure/attachment/13036763/errorLog.txt",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 27 12:15:15 UTC 2022,,,,,,,,,,"0|z0x64w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/21 03:39;jingzhang;[~xuechu]Thanks for reporting this problem, would you like to fix this bug?
BTW, if input condition of columnIntervalOfSinglePredicate is RexInputRef, ValueInterval still could be inferred which lower equals to upper, right?;;;","03/Dec/21 10:53;xuechu;[~jingzhang] I am sorry to reply you late for the reason was that I have something to do these day.First, I don't think RexInputRef should be Passed to columnIntervalOfSinglePredicate in theory.Then, a previous optimization rule may subtract the judgment condition(is true), resulting in a bug in the ValueInterval of joinkey when predicate push down.Next, considering that the evaluation result of the expression(is false) is null, return null directly when rexinputref is passed.If there are other conditions will be subtracted by some optimization rules, causing another RexInputRef is passed to columnIntervalOfSinglePredicate, there may be a problem.However, the boundary of the predicate is maximized after null is returned, and the SQL results are consistent.Finally, we should solve the bug of optimization rules. Above modifications is equivalent to a protective measure.;;;","14/Apr/22 08:30;337361684@qq.com;Hi, [~xuechu] , I think your are right. Do you have time to fix this bug？ If not, I will fix it.;;;","20/Apr/22 02:35;337361684@qq.com;Hi, [~godfreyhe]  could you assign this to me, thanks.;;;","26/Apr/22 03:56;godfreyhe;[~337361684@qq.com] assign to you;;;","27/Jun/22 12:15;godfreyhe;Fixed in
master: 686634aa05aadc33cc7b655d25ad162fe59e41fd
1.15: 1e47430137f6db24e2e24cafc30e1227787c6720;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make errors happened during JobMaster initialization accessible through the exception history,FLINK-25096,13414299,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mahi.29agarwal,mahi.29agarwal,29/Nov/21 13:06,26/Apr/22 11:40,13/Jul/23 08:12,10/Dec/21 07:40,1.13.3,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"Currently we are using flink version 1.13.2 and as per the flink documentation we should get all exceptions through exceptions api in exceptionHistory tag. While running few scenarios we observed that the below two exceptions are not coming in exceptionHistory tag but are coming in root-exception tag.

Exception 1 - caused by: java.util.concurrent.CompletionException: java.lang.RuntimeException: java.io.FileNotFoundException: Cannot find checkpoint or savepoint file/directory 'C:\Users\abc\Documents\checkpoints\a737088e21206281db87f6492bcba074' on file system 'file'.

Exception 2 - Caused by: java.lang.IllegalStateException: Failed to rollback to checkpoint/savepoint file:/mnt/c/Users/abc/Documents/checkpoints/a737088e21206281db87f6492bcba074/chk-144.

Please find the attachment for the logs of above exceptions.",,mahi.29agarwal,mapohl,Yuval.Itzchakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/21 13:06;mahi.29agarwal;Failed to rollback to checkpoint.txt;https://issues.apache.org/jira/secure/attachment/13036754/Failed+to+rollback+to+checkpoint.txt","29/Nov/21 13:06;mahi.29agarwal;FileNotFoundException.txt;https://issues.apache.org/jira/secure/attachment/13036755/FileNotFoundException.txt",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 10 07:40:43 UTC 2021,,,,,,,,,,"0|z0x61c:",9223372036854775807,Fixes issue where the failover cause it not listed in the exception history but as a root cause. That could have happened if the failure occurred during JobMaster initialization.,,,,,,,,,,,,,,,,,,,"10/Dec/21 07:40;mapohl;master: f6b220004855d269d4287079a22acf51676b4fb9
1.14: 7c380ba7b92b7526e236f4d72f030a858c74bae5
1.13: 8c3b96e96f1d6cdba11a240e447314badf0f1881;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The verify code in LatencyTrackingMapStateTest#verifyIterator is not actually executed,FLINK-25094,13414259,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,lijinzhong,lijinzhong,lijinzhong,29/Nov/21 09:50,24/Dec/21 06:57,13/Jul/23 08:12,24/Dec/21 06:57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / State Backends,Tests,,,,0,pull-request-available,,,,"In LatencyTrackingMapStateTest, iterator()/entries().iterator()/keys().iterator()/values().iterator() will be invoke before verifyIterator method is invoked, this is, 
iterator()/... will be invode before putting the test data into latencyTrackingMapState. So the verify code is not actually executed since ""iterator.hasNext()"" is always false.
{code:java}
private <E> void verifyIterator(
        LatencyTrackingMapState<Integer, VoidNamespace, Long, Double> latencyTrackingState,
        LatencyTrackingMapState.MapStateLatencyMetrics latencyTrackingStateMetric,
        Iterator<E> iterator,
        boolean removeIterator)
        throws Exception {
    ThreadLocalRandom random = ThreadLocalRandom.current();
    for (int index = 1; index <= SAMPLE_INTERVAL; index++) {
        latencyTrackingState.put((long) index, random.nextDouble());
    }
    int count = 1;
    while (iterator.hasNext()) {
        int expectedResult = count == SAMPLE_INTERVAL ? 0 : count;
        assertEquals(expectedResult, latencyTrackingStateMetric.getIteratorHasNextCount());

        iterator.next();
        assertEquals(expectedResult, latencyTrackingStateMetric.getIteratorNextCount());

        if (removeIterator) {
            iterator.remove();
            assertEquals(expectedResult, latencyTrackingStateMetric.getIteratorRemoveCount());
        }
        count += 1;
    }
    // as we call #hasNext on more time than #next, to avoid complex check, just reset hasNext
    // counter in the end.
    latencyTrackingStateMetric.resetIteratorHasNextCount();
    latencyTrackingState.clear();
} {code}",,lijinzhong,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 24 06:57:46 UTC 2021,,,,,,,,,,"0|z0x5sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Dec/21 06:57;yunta;Merged in master: d08a1d0f4035485283f10611113d4d0fc0a8aaca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Official website document FileSink orc compression attribute reference error,FLINK-25091,13414241,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,apche-Px,apche-Px,apche-Px,29/Nov/21 08:40,15/Dec/21 01:44,13/Jul/23 08:12,09/Dec/21 12:00,1.12.0,1.13.0,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.6,1.14.3,1.15.0,,Documentation,,,,,0,pull-request-available,,,,"!image-2021-11-29-16-34-48-511.png!
 * I see the following version is like this [1.12、1.13、1.14 。。。]
 * What should be quoted here is {{writerProperties}} Shouldn't be is {{writerProps}}
 * [docUrl|https://nightlies.apache.org/flink/flink-docs-release-1.12/dev/connectors/file_sink.html]",,apche-Px,martijnvisser,,,,,,,,,,,,,,,,,,604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/21 08:34;apche-Px;image-2021-11-29-16-34-48-511.png;https://issues.apache.org/jira/secure/attachment/13036729/image-2021-11-29-16-34-48-511.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 09 12:00:07 UTC 2021,,,,,,,,,,"0|z0x5og:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/21 08:43;martijnvisser;[~apche-Px] Thanks for noticing and creating the ticket. Do you want to open a PR to fix this issue?;;;","29/Nov/21 09:00;apche-Px;Ok, let me solve this problem;;;","02/Dec/21 20:51;martijnvisser;Resolved in master:
a7f0e5a78df2cc404bc161fbbc917c602626b2ac
e8633bd19b6636319a85835ad9949754d93a9dbd

[~apche-Px] Can you also create backports to release-1.14 and release-1.13? ;;;","09/Dec/21 12:00;martijnvisser;Fixed in:

release-1.14: fc060b078563e5a75248a0becfdebdcbedb49ad0
release-1.13: 2a13b7f05c833a4d97d6ae313281551e7df628e5
release-1.12: 7296c50010da3bd3ee975969e6c6f2ff36761837;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Field names must be unique. Found duplicates,FLINK-25084,13414141,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,ibuda,ibuda,28/Nov/21 11:11,15/Dec/21 01:44,13/Jul/23 08:12,29/Nov/21 02:33,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,API / DataStream,,,,,0,,,,,"I am getting a ""Field names must be unique. Found duplicates"" error when trying to aggregate a column used as a descriptor in HOP windowing.

Imagine this example, with *events_table* reading from kinesis stream, the definition given below, I am getting the ""Field names must be unique. Found duplicates: [ts]"" when trying to run the following SQL in Kinesis Data Analytics Application in Zeppelin:

{code:sql}
%flink.ssql(type=update)

-- insert into learn_actions_deduped 
SELECT window_start, window_end, uuid, event_type, max(ts) as max_event_ts
FROM TABLE(HOP(TABLE events_table, DESCRIPTOR(ts), INTERVAL '5' SECONDS, INTERVAL '15' MINUTES))
GROUP BY window_start, window_end, uuid, event_type;
{code}

The question is how can I use the descriptor column in aggregation without having to duplicate it?

The error details:
java.io.IOException: Fail to run stream sql job
	at org.apache.zeppelin.flink.sql.AbstractStreamSqlJob.run(AbstractStreamSqlJob.java:172)
	at org.apache.zeppelin.flink.sql.AbstractStreamSqlJob.run(AbstractStreamSqlJob.java:105)
	at org.apache.zeppelin.flink.FlinkStreamSqlInterpreter.callInnerSelect(FlinkStreamSqlInterpreter.java:89)
	at org.apache.zeppelin.flink.FlinkSqlInterrpeter.callSelect(FlinkSqlInterrpeter.java:503)
	at org.apache.zeppelin.flink.FlinkSqlInterrpeter.callCommand(FlinkSqlInterrpeter.java:266)
	at org.apache.zeppelin.flink.FlinkSqlInterrpeter.runSqlList(FlinkSqlInterrpeter.java:160)
	at org.apache.zeppelin.flink.FlinkSqlInterrpeter.internalInterpret(FlinkSqlInterrpeter.java:112)
	at org.apache.zeppelin.interpreter.AbstractInterpreter.interpret(AbstractInterpreter.java:47)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:110)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:852)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:744)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:172)
	at org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)
	at org.apache.zeppelin.scheduler.ParallelScheduler.lambda$runJobInScheduler$0(ParallelScheduler.java:46)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: Error while applying rule PullUpWindowTableFunctionIntoWindowAggregateRule, args [rel#1172:StreamPhysicalWindowAggregate.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=RelSubset#1170,groupBy=uuid, event_type,window=HOP(win_start=[window_start], win_end=[window_end], size=[15 min], slide=[5 s]),select=uuid, event_type, MAX(ts) AS max_event_ts, start('w$) AS window_start, end('w$) AS window_end), rel#1179:StreamPhysicalExchange.STREAM_PHYSICAL.hash[2, 3]true.None: 0.[NONE].[NONE](input=RelSubset#1169,distribution=hash[uuid, event_type]), rel#1168:StreamPhysicalCalc.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=RelSubset#1167,select=window_start, window_end, uuid, event_type, CAST(ts) AS ts), rel#1166:StreamPhysicalWindowTableFunction.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=RelSubset#1165,window=HOP(time_col=[ts], size=[15 min], slide=[5 s]))]
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:256)
	at org.apache.calcite.plan.volcano.IterativeRuleDriver.drive(IterativeRuleDriver.java:58)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:510)
	at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:312)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:62)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:58)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:163)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:83)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:279)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1518)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translateAndClearBuffer(TableEnvironmentImpl.java:1510)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1460)
	at org.apache.zeppelin.flink.sql.AbstractStreamSqlJob.run(AbstractStreamSqlJob.java:161)
	... 16 more
Caused by: java.lang.RuntimeException: Error occurred while applying rule PullUpWindowTableFunctionIntoWindowAggregateRule
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:161)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:268)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:283)
	at org.apache.flink.table.planner.plan.rules.physical.stream.PullUpWindowTableFunctionIntoWindowAggregateRule.onMatch(PullUpWindowTableFunctionIntoWindowAggregateRule.scala:143)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:229)
	... 42 more
Caused by: org.apache.flink.table.api.ValidationException: Field names must be unique. Found duplicates: [ts]
	at org.apache.flink.table.types.logical.RowType.validateFields(RowType.java:272)
	at org.apache.flink.table.types.logical.RowType.(RowType.java:157)
	at org.apache.flink.table.types.logical.RowType.of(RowType.java:297)
	at org.apache.flink.table.types.logical.RowType.of(RowType.java:289)
	at org.apache.flink.table.planner.calcite.FlinkTypeFactory$.toLogicalRowType(FlinkTypeFactory.scala:657)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWindowAggregate.aggInfoList$lzycompute(StreamPhysicalWindowAggregate.scala:60)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWindowAggregate.aggInfoList(StreamPhysicalWindowAggregate.scala:59)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWindowAggregate.explainTerms(StreamPhysicalWindowAggregate.scala:86)
	at org.apache.calcite.rel.AbstractRelNode.getDigestItems(AbstractRelNode.java:409)
	at org.apache.calcite.rel.AbstractRelNode.deepHashCode(AbstractRelNode.java:391)
	at org.apache.calcite.rel.AbstractRelNode$InnerRelDigest.hashCode(AbstractRelNode.java:443)
	at java.base/java.util.HashMap.hash(HashMap.java:339)
	at java.base/java.util.HashMap.get(HashMap.java:552)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1150)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:589)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:604)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:148)
	... 46 more

{code:sql}
CREATE TABLE events_table (
    uuid varchar(36),
    event_type VARCHAR(20),
    ts TIMESTAMP(3),
    WATERMARK FOR ts AS ts - INTERVAL '5' SECOND
)
PARTITIONED BY (event_type)
WITH (
    'connector' = 'kinesis',
    'stream' = 'kinesis-event-stream',
    'aws.region' = 'us-west-2',
    'scan.stream.initpos' = 'TRIM_HORIZON',
    'format' = 'json',
    'scan.stream.recordpublisher' = 'EFO',
    'scan.stream.efo.consumername' = 'learn-actions-efo',
    'scan.stream.efo.registration' = 'LAZY', -- EAGER
    'json.timestamp-format.standard' = 'ISO-8601'
);
{code}
","AWS Kinesis Application in Zeppelin
Apache Flink 1.13, Apache Zeppelin 0.9
 ",ibuda,jingzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23919,,,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/21 11:11;ibuda;Screenshot 2021-11-28 at 13.10.57.png;https://issues.apache.org/jira/secure/attachment/13036702/Screenshot+2021-11-28+at+13.10.57.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,sql,Mon Nov 29 02:32:40 UTC 2021,,,,,,,,,,"0|z0x528:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/21 02:32;jingzhang;[~ibuda] Thanks for reporting this BUG. It seems to be a duplicate with [FLINK-23919|https://issues.apache.org/jira/browse/FLINK-23919] which would been solved in 1.13.4.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Globally introduce assertj assertions for tests,FLINK-25079,13414003,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,slinkydeveloper,slinkydeveloper,slinkydeveloper,26/Nov/21 10:46,01/Dec/21 12:59,13/Jul/23 08:12,01/Dec/21 12:59,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Test Infrastructure,,,,,0,pull-request-available,,,,"As discussed on the dev@ mailing list, we advertise assertj instead of Hamcrest in the future.

https://lists.apache.org/thread/33t7hz8w873p1bc5msppk65792z08rgt",,slinkydeveloper,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 01 12:59:58 UTC 2021,,,,,,,,,,"0|z0x47k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/21 12:59;twalthr;Fixed in master:

commit 39ffdcc191ab0f686be587179bbdd77d4b9f0a9f
[table-common] Refactored some tests to use the new assertions

commit 023100d086b1db0e3fd4a3df101d819b5b9d2f74
[table-common] Add some initial assertj assertions for table data and types apis

commit 0452506b78db6c19a2e234581f2709d69ce516c6
[test-utils-junit] Add new assertj style assertions in FlinkAssertions to replace FlinkMatchers;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable ParquetFileSystemITCase.testLimitableBulkFormat,FLINK-25071,13413977,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,26/Nov/21 08:33,02/Dec/21 09:57,13/Jul/23 08:12,02/Dec/21 09:57,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,"The test {{ParquetFileSystemITCase.testLimitableBulkFormat}} is unstable. See more in FLINK-24763

We have ignored this test. We should find the unstable reason and re-open it.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25102,,,,FLINK-24763,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 02 09:57:11 UTC 2021,,,,,,,,,,"0|z0x41s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/21 08:40;lzljs3620320;Maybe we need to disable globalNumberRead in `LimitableBulkFormat`.;;;","02/Dec/21 05:43;lzljs3620320;SerializableConfiguration load resources from ClassLoader lead to many unstable tests:
https://issues.apache.org/jira/browse/FLINK-24763
https://issues.apache.org/jira/browse/FLINK-25102

But this is not necessary, because from serialization, the resources to be read are already serialized into properties.;;;","02/Dec/21 09:57;lzljs3620320;master: ea7a356b861dd464d37b57ca10f5aff797afb905;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the description of RocksDB's background threads,FLINK-25067,13413938,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,yunta,yunta,yunta,26/Nov/21 04:06,15/Dec/21 01:44,13/Jul/23 08:12,26/Nov/21 09:14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Documentation,Runtime / State Backends,,,,0,pull-request-available,,,,"RocksDB actually has changed the maximum number of concurrent background flush and compaction jobs to 2 for long time, we should fix the related documentation.",,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 26 09:14:18 UTC 2021,,,,,,,,,,"0|z0x3t4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/21 09:14;yunta;Merged in

master: 832d412e8c34a28849b346ac1088c704622a08e1

release-1.14: 315f3bc8a44cfa2edcfbeacb1f52089406a5471b

release-1.13: 52c3931c9fda2ac8483fae312e8d265b2226a54c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace DataType.projectFields with Projection type,FLINK-25060,13413816,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,slinkydeveloper,slinkydeveloper,slinkydeveloper,25/Nov/21 11:12,01/Dec/21 08:20,13/Jul/23 08:12,01/Dec/21 08:20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Table SQL / API,,,,,0,pull-request-available,,,,"FLINK-24399 introduced new methods to perform data types projections in DataType. Note: no release included such changes.

FLINK-24776 introduced a new, more powerful, type to perform operations on projections, that is project types, but also difference and complement.

In spite of avoiding to provide different entrypoints for the same functionality, we should cleanup the new methods introduced by FLINK-24399 and replace them with the new Projection type. We should also deprecate the functions in DataTypeUtils.",,slinkydeveloper,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 01 08:20:27 UTC 2021,,,,,,,,,,"0|z0x320:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/21 08:20;twalthr;Fixed in master: 30644a025acd2239c551adb68c0483d80358e07a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document how to use the usrlib to load code in the user code class loader,FLINK-25053,13413790,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wanglijie,trohrmann,trohrmann,25/Nov/21 08:55,23/Feb/22 02:40,13/Jul/23 08:12,23/Feb/22 02:40,1.12.5,1.13.3,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Documentation,Runtime / Coordination,,,,0,pull-request-available,,,,With FLINK-13993 we introduced the {{usrlib}} directory that can be used to load code in the user code class loader. This functionality has not been properly documented so that it is very hard to use. I would suggest to change this so that our users can benefit from this cool feature.,,liliwei,maguowei,Thesharing,trohrmann,wanglijie,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13993,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 23 02:40:59 UTC 2022,,,,,,,,,,"0|z0x2w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/22 09:02;trohrmann;[~wanglijie95] are you still working on this issue?;;;","12/Jan/22 10:36;wanglijie;[~trohrmann] I'm focusing on other things currently, but I'll finish it before the release of 1.15 (It's probably the end of this month).;;;","12/Jan/22 14:08;trohrmann;Great, thanks a lot [~wanglijie95]!;;;","23/Feb/22 02:40;wangyang0918;Fixed via:
master(release-1.15): f503fef9e923453a0eee42f0a955897c06462df9 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnalignedCheckpointRescaleITCase.shouldRescaleUnalignedCheckpoint fails on AZP,FLINK-25026,13413449,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,trohrmann,trohrmann,23/Nov/21 18:23,01/Mar/22 10:30,13/Jul/23 08:12,01/Mar/22 10:30,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.5,1.15.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,"{{UnalignedCheckpointRescaleITCase.shouldRescaleUnalignedCheckpoint}} fails on AZP with

{code}
2021-11-23T00:58:03.8286352Z Nov 23 00:58:03 [ERROR] Tests run: 72, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 716.362 s <<< FAILURE! - in org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase
2021-11-23T00:58:03.8288790Z Nov 23 00:58:03 [ERROR] shouldRescaleUnalignedCheckpoint[downscale union from 3 to 2, buffersPerChannel = 1]  Time elapsed: 4.051 s  <<< ERROR!
2021-11-23T00:58:03.8289953Z Nov 23 00:58:03 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-11-23T00:58:03.8291473Z Nov 23 00:58:03 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2021-11-23T00:58:03.8292776Z Nov 23 00:58:03 	at org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase.execute(UnalignedCheckpointTestBase.java:168)
2021-11-23T00:58:03.8294520Z Nov 23 00:58:03 	at org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase.shouldRescaleUnalignedCheckpoint(UnalignedCheckpointRescaleITCase.java:534)
2021-11-23T00:58:03.8295909Z Nov 23 00:58:03 	at jdk.internal.reflect.GeneratedMethodAccessor123.invoke(Unknown Source)
2021-11-23T00:58:03.8297310Z Nov 23 00:58:03 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-11-23T00:58:03.8298922Z Nov 23 00:58:03 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2021-11-23T00:58:03.8300298Z Nov 23 00:58:03 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2021-11-23T00:58:03.8301741Z Nov 23 00:58:03 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-11-23T00:58:03.8303233Z Nov 23 00:58:03 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2021-11-23T00:58:03.8304514Z Nov 23 00:58:03 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-11-23T00:58:03.8305736Z Nov 23 00:58:03 	at org.junit.rules.Verifier$1.evaluate(Verifier.java:35)
2021-11-23T00:58:03.8306856Z Nov 23 00:58:03 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2021-11-23T00:58:03.8308218Z Nov 23 00:58:03 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2021-11-23T00:58:03.8309532Z Nov 23 00:58:03 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-11-23T00:58:03.8310780Z Nov 23 00:58:03 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2021-11-23T00:58:03.8312026Z Nov 23 00:58:03 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2021-11-23T00:58:03.8313515Z Nov 23 00:58:03 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2021-11-23T00:58:03.8314842Z Nov 23 00:58:03 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2021-11-23T00:58:03.8316116Z Nov 23 00:58:03 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2021-11-23T00:58:03.8317538Z Nov 23 00:58:03 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2021-11-23T00:58:03.8320044Z Nov 23 00:58:03 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2021-11-23T00:58:03.8321044Z Nov 23 00:58:03 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2021-11-23T00:58:03.8321978Z Nov 23 00:58:03 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2021-11-23T00:58:03.8322915Z Nov 23 00:58:03 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2021-11-23T00:58:03.8323848Z Nov 23 00:58:03 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2021-11-23T00:58:03.8325330Z Nov 23 00:58:03 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2021-11-23T00:58:03.8337747Z Nov 23 00:58:03 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-11-23T00:58:03.8339178Z Nov 23 00:58:03 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-11-23T00:58:03.8340038Z Nov 23 00:58:03 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2021-11-23T00:58:03.8340967Z Nov 23 00:58:03 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2021-11-23T00:58:03.8341889Z Nov 23 00:58:03 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2021-11-23T00:58:03.8342923Z Nov 23 00:58:03 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2021-11-23T00:58:03.8343842Z Nov 23 00:58:03 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2021-11-23T00:58:03.8344979Z Nov 23 00:58:03 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2021-11-23T00:58:03.8345869Z Nov 23 00:58:03 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2021-11-23T00:58:03.8346717Z Nov 23 00:58:03 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2021-11-23T00:58:03.8347642Z Nov 23 00:58:03 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2021-11-23T00:58:03.8348570Z Nov 23 00:58:03 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
2021-11-23T00:58:03.8349582Z Nov 23 00:58:03 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2021-11-23T00:58:03.8350583Z Nov 23 00:58:03 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)
2021-11-23T00:58:03.8351521Z Nov 23 00:58:03 	at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133)
2021-11-23T00:58:03.8352509Z Nov 23 00:58:03 	at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
2021-11-23T00:58:03.8353653Z Nov 23 00:58:03 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
2021-11-23T00:58:03.8354667Z Nov 23 00:58:03 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
2021-11-23T00:58:03.8355701Z Nov 23 00:58:03 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2021-11-23T00:58:03.8356744Z Nov 23 00:58:03 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2021-11-23T00:58:03.8357943Z Nov 23 00:58:03 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2021-11-23T00:58:03.8358932Z Nov 23 00:58:03 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
2021-11-23T00:58:03.8359943Z Nov 23 00:58:03 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
2021-11-23T00:58:03.8360971Z Nov 23 00:58:03 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
2021-11-23T00:58:03.8361986Z Nov 23 00:58:03 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
2021-11-23T00:58:03.8363143Z Nov 23 00:58:03 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
2021-11-23T00:58:03.8364237Z Nov 23 00:58:03 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
2021-11-23T00:58:03.8365279Z Nov 23 00:58:03 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
2021-11-23T00:58:03.8366302Z Nov 23 00:58:03 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
2021-11-23T00:58:03.8367523Z Nov 23 00:58:03 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
2021-11-23T00:58:03.8368587Z Nov 23 00:58:03 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2021-11-23T00:58:03.8369888Z Nov 23 00:58:03 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-11-23T00:58:03.8370965Z Nov 23 00:58:03 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-11-23T00:58:03.8371971Z Nov 23 00:58:03 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-11-23T00:58:03.8373028Z Nov 23 00:58:03 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-11-23T00:58:03.8374193Z Nov 23 00:58:03 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=0, backoffTimeMS=100)
2021-11-23T00:58:03.8375495Z Nov 23 00:58:03 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2021-11-23T00:58:03.8376816Z Nov 23 00:58:03 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2021-11-23T00:58:03.8378373Z Nov 23 00:58:03 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:228)
2021-11-23T00:58:03.8379481Z Nov 23 00:58:03 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:218)
2021-11-23T00:58:03.8380644Z Nov 23 00:58:03 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:209)
2021-11-23T00:58:03.8381808Z Nov 23 00:58:03 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:682)
2021-11-23T00:58:03.8382989Z Nov 23 00:58:03 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2021-11-23T00:58:03.8384070Z Nov 23 00:58:03 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
2021-11-23T00:58:03.8385037Z Nov 23 00:58:03 	at jdk.internal.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
2021-11-23T00:58:03.8386030Z Nov 23 00:58:03 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-11-23T00:58:03.8387020Z Nov 23 00:58:03 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2021-11-23T00:58:03.8388110Z Nov 23 00:58:03 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
2021-11-23T00:58:03.8389282Z Nov 23 00:58:03 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2021-11-23T00:58:03.8390376Z Nov 23 00:58:03 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
2021-11-23T00:58:03.8391456Z Nov 23 00:58:03 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2021-11-23T00:58:03.8392539Z Nov 23 00:58:03 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2021-11-23T00:58:03.8393742Z Nov 23 00:58:03 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2021-11-23T00:58:03.8394719Z Nov 23 00:58:03 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2021-11-23T00:58:03.8395626Z Nov 23 00:58:03 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2021-11-23T00:58:03.8396513Z Nov 23 00:58:03 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2021-11-23T00:58:03.8397497Z Nov 23 00:58:03 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2021-11-23T00:58:03.8398435Z Nov 23 00:58:03 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2021-11-23T00:58:03.8399349Z Nov 23 00:58:03 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-11-23T00:58:03.8400292Z Nov 23 00:58:03 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2021-11-23T00:58:03.8401223Z Nov 23 00:58:03 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2021-11-23T00:58:03.8402260Z Nov 23 00:58:03 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2021-11-23T00:58:03.8403157Z Nov 23 00:58:03 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2021-11-23T00:58:03.8404030Z Nov 23 00:58:03 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2021-11-23T00:58:03.8404934Z Nov 23 00:58:03 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2021-11-23T00:58:03.8405775Z Nov 23 00:58:03 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2021-11-23T00:58:03.8406616Z Nov 23 00:58:03 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2021-11-23T00:58:03.8407512Z Nov 23 00:58:03 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2021-11-23T00:58:03.8408312Z Nov 23 00:58:03 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2021-11-23T00:58:03.8409185Z Nov 23 00:58:03 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
2021-11-23T00:58:03.8410322Z Nov 23 00:58:03 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
2021-11-23T00:58:03.8411305Z Nov 23 00:58:03 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
2021-11-23T00:58:03.8412209Z Nov 23 00:58:03 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
2021-11-23T00:58:03.8413192Z Nov 23 00:58:03 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2021-11-23T00:58:03.8415356Z Nov 23 00:58:03 Caused by: org.apache.flink.runtime.io.network.netty.exception.LocalTransportException: Direct buffer memory (connection to 'localhost/127.0.0.1:35630')
2021-11-23T00:58:03.8416790Z Nov 23 00:58:03 	at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.exceptionCaught(CreditBasedPartitionRequestClientHandler.java:177)
2021-11-23T00:58:03.8418297Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:302)
2021-11-23T00:58:03.8419680Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:281)
2021-11-23T00:58:03.8421026Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireExceptionCaught(AbstractChannelHandlerContext.java:273)
2021-11-23T00:58:03.8422366Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.exceptionCaught(ChannelInboundHandlerAdapter.java:143)
2021-11-23T00:58:03.8423806Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:302)
2021-11-23T00:58:03.8425153Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:232)
2021-11-23T00:58:03.8426512Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:216)
2021-11-23T00:58:03.8427954Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:209)
2021-11-23T00:58:03.8429271Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelActive(DefaultChannelPipeline.java:1398)
2021-11-23T00:58:03.8430603Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:230)
2021-11-23T00:58:03.8431934Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:216)
2021-11-23T00:58:03.8433297Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelActive(DefaultChannelPipeline.java:895)
2021-11-23T00:58:03.8434775Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.fulfillConnectPromise(AbstractEpollChannel.java:658)
2021-11-23T00:58:03.8436107Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.finishConnect(AbstractEpollChannel.java:691)
2021-11-23T00:58:03.8437442Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.epollOutReady(AbstractEpollChannel.java:567)
2021-11-23T00:58:03.8438654Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:470)
2021-11-23T00:58:03.8439768Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378)
2021-11-23T00:58:03.8440960Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
2021-11-23T00:58:03.8442282Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
2021-11-23T00:58:03.8443317Z Nov 23 00:58:03 	at java.base/java.lang.Thread.run(Thread.java:829)
2021-11-23T00:58:03.8448100Z Nov 23 00:58:03 Caused by: java.lang.OutOfMemoryError: Direct buffer memory. The direct out-of-memory error has occurred. This can mean two things: either job(s) require(s) a larger size of JVM direct memory or there is a direct memory leak. The direct memory can be allocated by user code or some of its dependencies. In this case 'taskmanager.memory.task.off-heap.size' configuration option should be increased. Flink framework and its dependencies also consume the direct memory, mostly for network communication. The most of network memory is managed by Flink and should not result in out-of-memory error. In certain special cases, in particular for jobs with high parallelism, the framework may require more direct memory which is not managed by Flink. In this case 'taskmanager.memory.framework.off-heap.size' configuration option should be increased. If the error persists then there is probably a direct memory leak in user code or some of its dependencies which has to be investigated and fixed. The task executor has to be shutdown...
2021-11-23T00:58:03.8451299Z Nov 23 00:58:03 	at java.base/java.nio.Bits.reserveMemory(Bits.java:175)
2021-11-23T00:58:03.8452187Z Nov 23 00:58:03 	at java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:118)
2021-11-23T00:58:03.8453176Z Nov 23 00:58:03 	at java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:317)
2021-11-23T00:58:03.8454212Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena$DirectArena.allocateDirect(PoolArena.java:632)
2021-11-23T00:58:03.8455342Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena$DirectArena.newChunk(PoolArena.java:607)
2021-11-23T00:58:03.8456445Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena.allocateNormal(PoolArena.java:202)
2021-11-23T00:58:03.8457611Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena.tcacheAllocateSmall(PoolArena.java:172)
2021-11-23T00:58:03.8458688Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena.allocate(PoolArena.java:134)
2021-11-23T00:58:03.8459728Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena.allocate(PoolArena.java:126)
2021-11-23T00:58:03.8460782Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:395)
2021-11-23T00:58:03.8462013Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:187)
2021-11-23T00:58:03.8463326Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:178)
2021-11-23T00:58:03.8464695Z Nov 23 00:58:03 	at org.apache.flink.runtime.io.network.netty.BufferResponseDecoder.onChannelActive(BufferResponseDecoder.java:54)
2021-11-23T00:58:03.8465948Z Nov 23 00:58:03 	at org.apache.flink.runtime.io.network.netty.NettyMessageClientDecoderDelegate.channelActive(NettyMessageClientDecoderDelegate.java:74)
2021-11-23T00:58:03.8467368Z Nov 23 00:58:03 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:230)
2021-11-23T00:58:03.8468324Z Nov 23 00:58:03 	... 14 more
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26873&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=10771",,akalashnikov,dwysakowicz,hxbks2ks,pnowojski,sjwiesman,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25426,,,,FLINK-25988,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 28 15:32:20 UTC 2022,,,,,,,,,,"0|z0x0s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Dec/21 06:09;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28505&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba
;;;","23/Dec/21 21:09;sjwiesman;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28549&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba

EDIT ([~dwysakowicz]) This failure is unrelated.;;;","30/Dec/21 09:04;trohrmann;[~pnowojski] could you elaborate why you have changed the component assignment? This information might be useful for further debugging.;;;","30/Dec/21 12:39;pnowojski;[~trohrmann] our best guess is/was that this, or at the very least the 2 most recent reports, a duplicate of FLINK-25426. If so, that would make it Runtime / Coordination production code issue and that's why I've updated the modules.

[~akalashnikov] will/is taking a deeper look at this issue. If it proves to be something else, we will re-adjust the modules.;;;","30/Dec/21 13:48;akalashnikov;as I can see last two reports relate to the master rather than release-1.14 which means that they should be reported inside https://issues.apache.org/jira/browse/FLINK-25426. So right now we have only one fail in release-1.14 one month ago.;;;","30/Dec/21 14:06;pnowojski;Ok. In that case let's disregard those latter two reports in the context of this ticket. I've restored the assigned component, and let's see if this issue will re-appear in the future.;;;","21/Feb/22 11:44;dwysakowicz;Moved over from FLINK-25426:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30598&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=6580
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30817&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=6602;;;","28/Feb/22 15:32;dwysakowicz;Introduced a solution to make the test more stable:
* master
** e0dd372337fb43e89988cd53129542bff0e86b14
* 1.14.5
** ba92ddf5073496aa4c79051f36a03043bd5eba25;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassLoader leak with ThreadLocals on the JM when submitting a job through the REST API,FLINK-25022,13413425,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,nkruber,nkruber,23/Nov/21 16:55,07/Apr/22 18:21,13/Jul/23 08:12,07/Dec/21 15:13,1.12.5,1.13.3,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.6,1.14.3,1.15.0,,Runtime / REST,,,,,0,pull-request-available,,,,"If a job is submitted using the REST API's {{/jars/:jarid/run}} endpoint, user code has to be executed on the JobManager and it is doing this in a couple of (pooled) dispatcher threads like {{{}Flink-DispatcherRestEndpoint-thread-*{}}}.

If the user code is using thread locals (and not cleaning them up), they may remain in the thread with references to the {{ChildFirstClassloader}} of the job and thus leaking that.

We saw this for the {{jsoniter}} scala library at the JM which [creates ThreadLocal instances|https://github.com/plokhotnyuk/jsoniter-scala/blob/95c7053cfaa558877911f3448382f10d53c4fcbf/jsoniter-scala-core/jvm/src/main/scala/com/github/plokhotnyuk/jsoniter_scala/core/package.scala] but doesn't remove them, but it can actually happen with any user code or (worse) library used in user code.

 

There are a few *workarounds* a user can use, e.g. putting the library in Flink's lib/ folder or submitting via the Flink CLI, but these may actually not be possible to use, depending on the circumstances.

 

A *proper fix* should happen in Flink by guarding against any of these things in the dispatcher threads. We could, for example, spawn a separate thread for executing the user's {{main()}} method and once the job is submitted exit that thread and destroy all thread locals along with it.",,kevin.cyj,nkruber,qinjunjerry,Thesharing,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24888,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Dec 04 13:24:42 UTC 2021,,,,,,,,,,"0|z0x0mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Dec/21 13:24;chesnay;master: 1f212f2ef04e36e0248098a26e7db43a6d65796a
1.14: 5207fe560dc6a054beb0eb0a25af009215ca9f23
1.13: 59d19caf3687dfd3dcaadc14cc11c6bbdf33198e
1.12: fdd52a787260e2d4dd97473a74e7e45222dbd099;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkSecurityManager can swallow exception,FLINK-25020,13413408,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,23/Nov/21 14:20,14/Dec/21 09:07,13/Jul/23 08:12,14/Dec/21 08:09,1.13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,API / Core,,,,,0,pull-request-available,,,,"If the security manager cannot be set then FlinkSecurityManager#setFromConfiguration swallows the exception, because it is passed to String.format instead of the IllegalConfigurationException constructor.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 14 08:09:01 UTC 2021,,,,,,,,,,"0|z0x0j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/21 08:09;chesnay;master: 1d4bd80ac1aaa78b79544ab3f17232dc7df946d9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
validatePKConstraints in KafkaDynamicTableFactory throws wrong exception message,FLINK-24977,13413111,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,22/Nov/21 09:39,08/Dec/21 12:00,13/Jul/23 08:12,08/Dec/21 12:00,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"options.get(VALUE_FORMAT) will return null.

It should be configuration.get(VALUE_FORMAT)",,fpaul,jark,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 08 12:00:43 UTC 2021,,,,,,,,,,"0|z0wypc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/21 09:42;lzljs3620320;This is caused by FLINK-24397

CC: [~fpaul] ;;;","22/Nov/21 09:44;fpaul;[~lzljs3620320] good catch I'll have a look;;;","08/Dec/21 12:00;fpaul;Merged master: 551dfe4842bbf559d22953f32c567a8a35f131c9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming File Sink s3 end-to-end test stalled on azure,FLINK-24971,13413009,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,gaoyunhaii,gaoyunhaii,21/Nov/21 11:24,15/Dec/21 01:44,13/Jul/23 08:12,26/Nov/21 15:31,1.12.5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.6,1.14.3,1.15.0,,Connectors / FileSystem,,,,,0,pull-request-available,test-stability,,,"{code:java}
Nov 21 00:04:36 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:04:41 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:04:46 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:04:51 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:04:56 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:01 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:06 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:11 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:16 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:21 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:26 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:31 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:36 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:41 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:46 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:52 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:05:57 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:02 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:07 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:12 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:17 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:22 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:27 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:32 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:37 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:42 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:47 Still waiting for restarts. Expected: 1 Current: 0
Nov 21 00:06:51 Test (pid: 414853) did not finish after 900 seconds.
Nov 21 00:06:51 Printing Flink logs and killing it: {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26784&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=3425d8ba-5f03-540a-c64b-51b8481bf7d6&l=12438",,gaoyunhaii,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 26 15:31:55 UTC 2021,,,,,,,,,,"0|z0wy2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/21 11:26;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26784&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=0b23652f-b18b-5b6e-6eb6-a11070364610&l=4329]

This is another case for new file sink test, but since the two tests shares a lot of codes, I think they would be related.;;;","22/Nov/21 11:13;martijnvisser;I'll have a look [~gaoyunhaii];;;","23/Nov/21 03:09;gaoyunhaii;Very thanks [~MartijnVisser] ~;;;","26/Nov/21 08:45;martijnvisser;[~gaoyunhaii] We've merged a fix into master and there are currently backports ready for 1.12, 1.13 and 1.14. When the build is green, they'll be merged and then I'll close the ticket. ;;;","26/Nov/21 15:31;martijnvisser;Fixed via:

1.12: c34074c710111f42d20d07e624bd3a8c4ed847df
1.13: 5f7f4bfd6273f3678f9c24c49e89105311ebf156
1.14: 15723e6b40e1caa55fc7edbcbbfe7c042de12808
master: 8df3b668e66cb56d9c8450446624d504d04d8d1f ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARNSessionCapacitySchedulerITCase.testVCoresAreSetCorrectlyAndJobManagerHostnameAreShownInWebInterfaceAndDynamicPropertiesAndYarnApplicationNameAndTaskManagerSlots hangs on azure,FLINK-24960,13412709,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,gaoyunhaii,gaoyunhaii,19/Nov/21 06:49,29/Aug/22 11:05,13/Jul/23 08:12,29/Aug/22 11:05,1.14.3,1.15.0,1.16.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.16.0,,,,,Deployment / YARN,,,,,0,pull-request-available,test-stability,,,"{code:java}
Nov 18 22:37:08 ================================================================================
Nov 18 22:37:08 Test testVCoresAreSetCorrectlyAndJobManagerHostnameAreShownInWebInterfaceAndDynamicPropertiesAndYarnApplicationNameAndTaskManagerSlots(org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase) is running.
Nov 18 22:37:08 --------------------------------------------------------------------------------
Nov 18 22:37:25 22:37:25,470 [                main] INFO  org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase     [] - Extracted hostname:port: 5718b812c7ab:38622
Nov 18 22:52:36 ==============================================================================
Nov 18 22:52:36 Process produced no output for 900 seconds.
Nov 18 22:52:36 ==============================================================================
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26722&view=logs&j=f450c1a5-64b1-5955-e215-49cb1ad5ec88&t=cc452273-9efa-565d-9db8-ef62a38a0c10&l=36395",,bgeng777,gaoyunhaii,mapohl,martijnvisser,nsemmler,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26710,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 29 11:05:36 UTC 2022,,,,,,,,,,"0|z0ww80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/21 06:50;gaoyunhaii;Hi [~chesnay]  could you help to have a look at this issue~?;;;","04/Jan/22 16:06;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28931&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=35283;;;","24/Jan/22 13:00;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30001&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=34898;;;","07/Mar/22 16:28;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32611&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=37651;;;","08/Mar/22 06:51;gaoyunhaii;1.14: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32582&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=34954]

 ;;;","11/Mar/22 16:46;nsemmler;An early analysis.
 # We start the JobManager via YARN - Works
 # We identify the address of the REST server - We get the right address
 # We start a RestClient to submit a job via YARN - Works
 # RestClusterClient tries to submit job

In the last step, a default address ""localhost:8081"" is used instead of the correct external address port combination. This leads to a connection refused error. I am not sure why this happens though. From how I understand {{RestClusterClient#sendRetriableRequest}}, it tries to get the address by using web monitor leader retrieval. It doesn't make sense to me, why this would return localhost:8081. I also don't see any place where this could fallback to the localhost:8081 pair.;;;","14/Mar/22 15:17;nsemmler;Anyhow, the failure alone doesn't explain why the test gets stuck.

We have three threads here
# The outer thread running the test.
# A Flink cluster running the JobManager and TaskManager. 
# A rest client thread that submits the job.

The rest client thread fails:
# Contacting the Rest Server fails (it retries for about 1 minute)
# The job submission fails
# The Session interface returns a nonzero exit code
# An assertion failure is created
# The assertion failure is caught and stored on the Runner class
# The inner thread exits

The outer thread should in principle either re-throw the exception or fail due to the timeout (also 1 minute). There should be no way out of this thread that doesn't lead to an exception. Yet, we see no indicator that the outer thread acknowledges the failure of the inner thread. Instead, the thread hangs in the finally part of the try-finally of the main thread. It is unable to close the Flink Cluster.

In contrast, from the JobManager logs we see that it doesn't receive any stop signal. So, somehow the connection seems to be severed?;;;","15/Mar/22 06:42;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33025&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=38866;;;","15/Mar/22 08:26;nsemmler;[~mapohl] and I debugged this some more.

It looks like the external address of the rest server is set by the [YarnClusterDescriptor|https://github.com/apache/flink/blob/c16e4b4ce20704a0ad4387591894f13105d5e530/flink-yarn/src/main/java/org/apache/flink/yarn/YarnClusterDescriptor.java#L1801]. In short, the MiniYarnCluster propages the YarnClusterDescriptor into the execution process of the RestClusterClient. The address is then set via leader retrieval, but there is no actual leader retrieval taking place. Instead, the StandaloneHaServices returns the preconfigured rest server address.

In principle, this should never return a ""localhost"" address. To better debug future scenarios of this bug, we add a PR that ensures that the log line in the code above is always printed. If this returns ""localhost"", then there is something going wrong with the address the YARN application report includes. If instead, it returns an external address but the RestClusterClient can still not connect, then we missed another place where this property is set. Finally, if the log line does not appear at all, then we need to figure out if there is yet another code path.;;;","17/Mar/22 14:08;nsemmler;Turns out that the [relevant log line|https://github.com/apache/flink/blob/c16e4b4ce20704a0ad4387591894f13105d5e530/flink-yarn/src/main/java/org/apache/flink/yarn/YarnClusterDescriptor.java#L1799] is hidden by a [test rule|https://github.com/apache/flink/blob/c16e4b4ce20704a0ad4387591894f13105d5e530/flink-yarn-tests/src/test/java/org/apache/flink/yarn/YARNSessionCapacitySchedulerITCase.java#L120]. We recorded this as a bug in FLINK-26710. ;;;","28/Mar/22 06:44;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=33780&view=logs&j=8fd975ef-f478-511d-4997-6f15fe8a1fd3&t=494f6362-8ffa-5ff8-9158-c7f00e541279&l=36852;;;","18/Apr/22 07:30;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34743&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=d4c90338-c843-57b0-3232-10ae74f00347&l=36026;;;","19/Apr/22 10:10;mapohl;[~ferenc-csaky] [~bgeng777] may one of you have a closer look at it? [~nsemmler] worked on a fix around the missing logs in FLINK-26710. Maybe, that's of help to fix this test instability.;;;","20/Apr/22 10:05;bgeng777;Hi [~mapohl], I tried to do some research but in latest [failure|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34743&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=d4c90338-c843-57b0-3232-10ae74f00347&l=36026] posted by Yun, I did not find the {{Extracted hostname:port: }} was not shown. Though in the description of this ticket, it shows  {{Extracted hostname:port: 5718b812c7ab:38622}} in the old CI test, which seems to be correct. 
I plan to verify {{yarnSessionClusterRunner.sendStop();}}  works fine(i.e. the session cluster will be stopped normally) first but I have not found a way to run the cron test's ""test_cron_jdk11 misc"" test only on the my own [azure pipeline|https://dev.azure.com/samuelgeng7/Flink/_build/results?buildId=109&view=results], which made the verification pretty slow and hard. Is there any guidelines about debugging the azure pipeline with some specific tests?;;;","27/Apr/22 12:22;mapohl;HI [~bgeng777], unfortunately, running individual tests within CI is a bit tedious right now. The only way, I know so far, is to alter {{tools/ci/test_controller.sh}} to run the test you want to execute;;;","30/May/22 21:36;nsemmler;I took another look at this instability and tried to retrace my earlier steps. There are two aspects about this instability:

1. Why is the {{localhost:8081}} address used in this Yarn context? (Instead of the Yarn container & port)
2. Why does the exception not halt the program? I.e., why is the program continuing to run after the exception is thrown?

Previously I thought that {{localhost}} was set via the setup of the YarnClusterDescriptor. In the newer logs, the following [log line|https://github.com/apache/flink/blob/c16e4b4ce20704a0ad4387591894f13105d5e530/flink-yarn/src/main/java/org/apache/flink/yarn/YarnClusterDescriptor.java#L1799] shows that the address is still correctly set in the YarnClusterDescriptor: 

{{7:09:15,731 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO org.apache.flink.yarn.YarnClusterDescriptor [] - Found Web Interface 6dad280b2159:40493 of application}}

Apparently the {{localhost}} address is added somewhere else. But where? Maybe it is done when the job is submitted? It may make sense to change the log level of this [log line|https://github.com/apache/flink/blob/f9438dd54fa6896563b152d50b7a4b3c47ad9ebf/flink-clients/src/main/java/org/apache/flink/client/cli/CliFrontend.java#L244] by changing it [here|https://github.com/apache/flink/blob/7d85b273ccdbd5a2242e05e5d645ea82280f5eea/flink-yarn-tests/src/test/java/org/apache/flink/yarn/YARNSessionCapacitySchedulerITCase.java#L116].

This would however just explain point 1 not point 2. My intuition is that there is some timing problem at play that we are so far missing. Let's compare a successful to an erroneous execution:

*Successful execution*
{code}
05:26:42,745 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - YARN application has been deployed successfully.
05:26:42,745 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Found Web Interface 6d4ce71bbaec:45932 of application 'application_1650173077369_0008'.    05:26:42,931 [                main] INFO  org.apache.flink.yarn.YarnTestBase                           [] - Found expected output in redirected streams
05:26:42,933 [                main] INFO  org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase     [] - Extracted hostname:port: 6d4ce71bbaec:45932
05:26:42,934 [                main] INFO  org.apache.flink.yarn.YarnTestBase                           [] - Running with args [run, --detached, /__w/3/s/flink-yarn-tests/target/programs/WindowJoin.jar]
05:26:42,940 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                [] - Found Yarn properties file under /tmp/.yarn-properties-agent03_azpcontainer.
05:26:42,940 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] WARN  org.apache.flink.core.plugin.PluginConfig                    [] - The plugins directory [plugins] does not exist.
05:26:42,941 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.client.cli.CliFrontend                      [] - Running 'run' command.
05:26:42,942 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.client.cli.CliFrontend                      [] - Building program from JAR file
05:26:42,945 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.client.ClientUtils                          [] - Starting program (detached: true)
05:26:42,975 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.streaming.api.datastream.CoGroupedStreams$TaggedUnion does not      contain a setter for field one                                                                                                                                                                                                                05:26:42,975 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - Class class org.apache.flink.streaming.api.datastream.CoGroupedStreams$TaggedUnion cannot  be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on ""Data Types & Serialization"" for details of the effect on performance.                      05:26:42,978 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.streaming.api.datastream.CoGroupedStreams$TaggedUnion does not      contain a setter for field one                                                                                                                                                                                                                05:26:42,978 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - Class class org.apache.flink.streaming.api.datastream.CoGroupedStreams$TaggedUnion cannot  be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on ""Data Types & Serialization"" for details of the effect on performance.                      05:26:43,033 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] WARN  org.apache.flink.yarn.configuration.YarnLogConfigUtil        [] - The configuration directory ('/tmp/junit8048407617893879030/conf') already contains a      LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.                                                                                                                                        05:26:43,033 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] WARN  org.apache.flink.runtime.util.HadoopUtils                    [] - Could not find Hadoop configuration via any of the supported methods (Flink configuration, environment variables).                                                                                                                                                                                                                       05:26:43,082 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.hadoop.yarn.client.RMProxy                        [] - Connecting to ResourceManager at 6d4ce71bbaec/192.168.192.2:42104
05:26:43,083 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.       YarnClusterDescriptor to locate the jar
05:26:43,086 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Found Web Interface 6d4ce71bbaec:45932 of application 'application_1650173077369_0008'.
05:26:43,101 [Flink-RestClusterClient-IO-thread-1] INFO  org.apache.flink.client.program.rest.RestClusterClient       [] - Submitting job 'Windowed Join Example' (8e2a9cb1e08a4b6642ac342bd4d96dcb).                                         05:26:43,620 [Flink-RestClusterClient-IO-thread-4] INFO  org.apache.flink.client.program.rest.RestClusterClient       [] - Successfully submitted job 'Windowed Join Example' (8e2a9cb1e08a4b6642ac342bd4d96dcb) to 'http://6d4ce71bbaec:     45932'.
{code}

*Erroneous execution*
{code}
07:09:15,731 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - YARN application has been deployed successfully.
07:09:15,731 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Found Web Interface 6dad280b2159:40493 of application 'application_1650179225006_0008'.    07:09:15,750 [                main] INFO  org.apache.flink.yarn.YarnTestBase                           [] - Found expected output in redirected streams
07:09:15,752 [                main] INFO  org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase     [] - Extracted hostname:port: 6dad280b2159:40493
07:09:15,752 [                main] INFO  org.apache.flink.yarn.YarnTestBase                           [] - Running with args [run, --detached, /__w/2/s/flink-yarn-tests/target/programs/WindowJoin.jar]
07:09:15,757 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] WARN  org.apache.flink.core.plugin.PluginConfig                    [] - The plugins directory [plugins] does not exist.
07:09:15,758 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.client.cli.CliFrontend                      [] - Running 'run' command.
07:09:15,759 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.client.cli.CliFrontend                      [] - Building program from JAR file
07:09:15,762 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.client.ClientUtils                          [] - Starting program (detached: true)
07:09:15,867 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.streaming.api.datastream.CoGroupedStreams$TaggedUnion does not      contain a setter for field one                                                                                                                                                                                                                07:09:15,867 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - Class class org.apache.flink.streaming.api.datastream.CoGroupedStreams$TaggedUnion cannot  be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on ""Data Types & Serialization"" for details of the effect on performance.                      07:09:15,871 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - class org.apache.flink.streaming.api.datastream.CoGroupedStreams$TaggedUnion does not      contain a setter for field one                                                                                                                                                                                                                07:09:15,871 [Frontend (CLI/YARN Client) runner thread (startWithArgs()).] INFO  org.apache.flink.api.java.typeutils.TypeExtractor            [] - Class class org.apache.flink.streaming.api.datastream.CoGroupedStreams$TaggedUnion cannot  be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on ""Data Types & Serialization"" for details of the effect on performance.                      07:09:16,194 [Flink-RestClusterClient-IO-thread-1] INFO  org.apache.flink.client.program.rest.RestClusterClient       [] - Submitting job 'Windowed Join Example' (2788aebd84c123f092c0992e5a7321ca).
07:09:16,236 [flink-rest-client-netty-thread-1] WARN  org.apache.flink.client.program.rest.RestClusterClient       [] - Attempt to submit job 'Windowed Join Example' (2788aebd84c123f092c0992e5a7321ca) to 'http://localhost:8081' has       failed.                                                                                                                                                                                                                                       java.util.concurrent.CompletionException: org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:8081
{code}

It is noteworthy that the line {{The configuration directory ('/tmp/junit8048407617893879030/conf') already contains a      LOG4J config file}} only appears in the successful execution. This may indicate a timing issue.;;;","31/May/22 08:13;nsemmler;Note, the code on master has now replaced {{TestLoggerResource}} by {{LoggerAuditingExtension}}. In my understanding {{LoggerAuditingExtension}} has the same problem as the previous implementation of {{TestLoggerResource}}. See https://issues.apache.org/jira/browse/FLINK-26710;;;","31/May/22 09:14;nsemmler;Ok, so I think what happens here is roughly the following.

The test submits a packaged program. This program is invoked [here|https://github.com/apache/flink/blob/c6997c97c575d334679915c328792b8a3067cfb5/flink-clients/src/main/java/org/apache/flink/client/ClientUtils.java#L114]. There is a thread local context setup that determines how the program is executed. If set up correctly, the program will be run using a Yarn specific code path (using the {{YarnSessionClusterExecutorFactory}} and {{YarnClusterDescriptor}}). In this case, the rest address of the job manager will be retrieved via {{YarnClusterDescriptor}}. Everything is well.

If things don't work out correctly, I _think_ the Yarn specific code path is not set up and during the program execution the rest address is extracted from the default config file. This leads to the use of the wrong {{localhost}} address.

Now the big question is: Why is the Yarn specific code path not set up correctly for the erroneous executions? Again, I _think_ this has something to do with the timing of the different threads. But how exactly is still unclear to me.;;;","31/May/22 10:10;nsemmler;I just noticed, that the line {{Found Yarn properties file under /tmp/.yarn-properties-agent03_azpcontainer}} is missing for the errorneous execution. The file is loaded here [here|https://github.com/apache/flink/blob/6086e327cd4168e09eac4f6b0b86fb29ebe3860c/flink-yarn/src/main/java/org/apache/flink/yarn/cli/FlinkYarnSessionCli.java#L288] and the content ends up in the configuration [here|https://github.com/apache/flink/blob/f9438dd54fa6896563b152d50b7a4b3c47ad9ebf/flink-clients/src/main/java/org/apache/flink/client/cli/CliFrontend.java#L241]. If the file contains the Yarn specific settings and is not available for the erroneous execution, then this explains why the Yarn code path is not used.;;;","31/May/22 11:22;nsemmler;This file is written [here|https://github.com/apache/flink/blob/6086e327cd4168e09eac4f6b0b86fb29ebe3860c/flink-yarn/src/main/java/org/apache/flink/yarn/cli/FlinkYarnSessionCli.java#L617]. 

In the test we wait for the thread, but only for the print output that comes before writing the Yarn properties file. (see [here|https://github.com/apache/flink/blob/b98c66cfe44d1b4002fe56dbf323d8ea8ce0409c/flink-yarn-tests/src/test/java/org/apache/flink/yarn/YARNSessionCapacitySchedulerITCase.java#L309]).

We should be able to fix the issue by exchanging the order of the print and the write file command.;;;","01/Jun/22 10:10;nsemmler;I finally found an explanation for the aspect 2: Why the yarn session thread sometimes continues to run even when an exception is thrown.

There is a race condition happening between the [interactive CLI of the YARN session|https://github.com/apache/flink/blob/6086e327cd4168e09eac4f6b0b86fb29ebe3860c/flink-yarn/src/main/java/org/apache/flink/yarn/cli/FlinkYarnSessionCli.java#L874] and the [job submission|https://github.com/apache/flink/blob/98a6a5432b642aa647f6edcd60dae49ef9093786/flink-yarn-tests/src/test/java/org/apache/flink/yarn/YarnTestBase.java#L890].

To explain the problem:
 # The _main thread_ executing the test starts two threads: the _jobmanager thread_ executing the job manager as part of a yarn session and the _submission thread_ submitting the Flink job.
 # The _jobmanager thread_ is created before and ends after the _submission thread_.
 # Communication between _main thread_ and the other threads happens via rerouted stdin, stdout, stderr channels. The rerouting takes place when the _jobmanager thread_ and _submission thread_ are created respectively.
 # The _main thread_ waits for the _jobmanager thread_ and _submission thread_ to print a specific output message to the rerouted stdout before continuing. 
 # The _jobmanager thread_ needs to be explicitly shutdown via ""stop"" string communicated via the rerouted stdin

The problem appears, if _submission thread_ reroutes the stdin before _jobmanager thread_ opens a BufferedReader on the old stdin. In this case, the stop message from the _main thread_ to the _jobmanager thread_ is lost and the {*}_jobmanager thread_ continues running indefinitely{*}. In this cases even an exception will not fail the test.

We can improve on this by changing the output the _main thread_ matches on. Instead of the [current|https://github.com/apache/flink/blob/b98c66cfe44d1b4002fe56dbf323d8ea8ce0409c/flink-yarn-tests/src/test/java/org/apache/flink/yarn/YARNSessionCapacitySchedulerITCase.java#L309], we could add a new output after the creation of the [BufferedReader|https://github.com/apache/flink/blob/6086e327cd4168e09eac4f6b0b86fb29ebe3860c/flink-yarn/src/main/java/org/apache/flink/yarn/cli/FlinkYarnSessionCli.java#L874].

The question here is whether adding an output line here will have some unintended consequences. The code is directly used by the FLINK Yarn session CLI, so if anybody is already parsing the output this may have adverse effects. Also, if we change it here, we may also want to touch other tests that make use of the same code.

Alternative approaches that only touch the tests are:
 * move the current println statement further down in the code (see [PR19852|https://github.com/apache/flink/pull/19852])
 * add a (one second?) delay before the job submission

However, both of these solutions only reduce the chance that the test instability will appear.;;;","01/Jun/22 10:35;nsemmler;[PR19852|https://github.com/apache/flink/pull/19852] implements the partial solution.

I have tested the partial solution in this [azure pipeline|https://dev.azure.com/NiklasFlink/NiklasFlink/_build/results?buildId=121&view=logs&j=f3dc9b18-b77a-55c1-591e-264c46fe44d1&t=2d3cd81e-1c37-5c31-0ee4-f5d5cdb9324d]. After four hours of running the same test, we still don't see any instabiliity.

In contrast, when we don't use the PR we see a failure in [this azure pipeline|https://dev.azure.com/NiklasFlink/NiklasFlink/_build/results?buildId=122&view=logs&j=f3dc9b18-b77a-55c1-591e-264c46fe44d1&t=2d3cd81e-1c37-5c31-0ee4-f5d5cdb9324d] after 45 minutes.

The alternative but more heavy weight change is [PR19863|https://github.com/apache/flink/pull/19863].;;;","01/Jun/22 16:01;nsemmler;After a discussion with [~mapohl], a different solution for the problem around stdin is to replace the use of stdin on the test path with a different stream.

Together with the reordering of the print statement and write yarn properties file command described above, this would remove the synchronization problem;;;","14/Jun/22 11:42;martijnvisser;[~mapohl] potentially [~ferenc-csaky] can also help out if needed, he will also look into the other YARN test stability problem FLINK-27667 this week ;;;","14/Jun/22 14:32;mapohl;Thanks for looking into it [~nsemmler] and your thorough review. Of the two proposed PRs, I'd go with [PR #19852|https://github.com/apache/flink/pull/19852/files], because it doesn't touch the production code. Essentially, we're moving the {{*.properties}} creation into the waiting period before proceeding with the job submission to avoid it to take too long before the interactive cli logic (including the BufferedReader initialization around {{{}System.in{}}}) is started.

[PR #19863|https://github.com/apache/flink/pull/19863] touches production code which I am hesitant to do. The test runs in AzureCI indicate already that it might help reducing the flakiness. We might want to tackle the race condition you described above, still.;;;","14/Jun/22 14:38;mapohl;I merged PR #19863 and related backports:
master - 6fe543bc8568015bc64eed558817d680a7fce85f
1.15 - 96c400cf909a4af941a54fa663758d73bb0f9679
1.14 - 5275206fcb891d67153e108f2e81b3e2734b4784

I'm going to keep this ticket open to cover the race condition and to observe the frequency of still happening build failures. [~ferenc-csaky] feel free to pick this issue up.;;;","05/Jul/22 07:22;martijnvisser;[~mapohl] What's the current status of this ticket? ;;;","05/Jul/22 07:34;mapohl;[~nsemmler] provided a workaround where we hope that it reduces the frequency of the error. Fixing it properly requires a bigger change (making the {{PrintStream}} instances configurable instead of using {{System.out}} and {{System.err}} in a hard-coded fashion) which is not implemented, yet. That's why I left the ticket open...;;;","05/Jul/22 07:41;martijnvisser;Are we OK with downgrading this to a Major instead of Critical?;;;","08/Jul/22 08:19;mapohl;Yes, I guess, that makes sense. Thanks for doing so...;;;","29/Aug/22 11:05;chesnay;Considering it as fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Use Hive Dialect execute Hive DDL, But throw a NullPointerException ",FLINK-24950,13412456,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,slashchenxiaojun,slashchenxiaojun,18/Nov/21 07:45,22/Apr/22 02:51,13/Jul/23 08:12,19/Nov/21 08:53,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,,,,,Connectors / Hive,,,,,0,flink-connector-hive,flinksql,,,"Dear all friends:

I try to execute a hive ddl sql with stream table api on flink-1.13.2, the code like:

```java

String hiveDDL = ResourceUtil.readClassPathSource(""hive-ddl.sql"");
EnvironmentSettings settings = EnvironmentSettings.newInstance()
.useBlinkPlanner()
.inStreamingMode().build();
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings);

String name = ""hive"";
String defaultDatabase = ""stream"";
String hiveConfDir = ""conf"";

HiveCatalog hive = new HiveCatalog(name, defaultDatabase, hiveConfDir);
tableEnv.registerCatalog(""hive"", hive);
tableEnv.useCatalog(""hive"");
tableEnv.useDatabase(""stream"");

tableEnv.executeSql(""DROP TABLE IF EXISTS dimension_table"");
// 设置HIVE方言
tableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);
tableEnv.executeSql(hiveDDL);

```

the hive server in cdh5.14.2, and the ddl sql like:

```sql

CREATE TABLE dimension_table (
product_id STRING,
product_name STRING,
unit_price DECIMAL(10, 4),
pv_count BIGINT,
like_count BIGINT,
comment_count BIGINT,
update_time TIMESTAMP(3),
update_user STRING
)
PARTITIONED BY (
pt_year STRING,
pt_month STRING,
pt_day STRING
)
TBLPROPERTIES (
– using default partition-name order to load the latest partition every 12h (the most recommended and convenient way)
'streaming-source.enable' = 'true',
'streaming-source.partition.include' = 'latest',
'streaming-source.monitor-interval' = '12 h',
'streaming-source.partition-order' = 'partition-name', – option with default value, can be ignored.

– using partition file create-time order to load the latest partition every 12h
'streaming-source.enable' = 'true',
'streaming-source.partition.include' = 'latest',
'streaming-source.partition-order' = 'create-time',
'streaming-source.monitor-interval' = '12 h',

– using partition-time order to load the latest partition every 12h
'streaming-source.enable' = 'true',
'streaming-source.partition.include' = 'latest',
'streaming-source.monitor-interval' = '12 h',
'streaming-source.partition-order' = 'partition-time',
'partition.time-extractor.kind' = 'default',
'partition.time-extractor.timestamp-pattern' = '$pt_year-$pt_month-$pt_day 00:00:00'
)

```

then run it, but throw NullPointerException, like:

```

2021-11-18 15:33:00,387 INFO [org.apache.flink.table.catalog.hive.HiveCatalog] - Setting hive conf dir as conf
2021-11-18 15:33:00,481 WARN [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2021-11-18 15:33:01,345 INFO [org.apache.flink.table.catalog.hive.HiveCatalog] - Created HiveCatalog 'hive'
2021-11-18 15:33:01,371 INFO [hive.metastore] - Trying to connect to metastore with URI thrift://cdh-dev-node-119:9083
2021-11-18 15:33:01,441 INFO [hive.metastore] - Opened a connection to metastore, current connections: 1
2021-11-18 15:33:01,521 INFO [hive.metastore] - Connected to metastore.
2021-11-18 15:33:01,856 INFO [org.apache.flink.table.catalog.hive.HiveCatalog] - Connected to Hive metastore
2021-11-18 15:33:01,899 INFO [org.apache.flink.table.catalog.CatalogManager] - Set the current default catalog as [hive] and the current default database as [stream].
2021-11-18 15:33:03,290 INFO [org.apache.hadoop.hive.ql.session.SessionState] - Created local directory: /var/folders/4m/n1wgh7rd2yqfv301kq00l4q40000gn/T/681dd0aa-ba35-4a0e-b069-3ad48f030774_resources
2021-11-18 15:33:03,298 INFO [org.apache.hadoop.hive.ql.session.SessionState] - Created HDFS directory: /tmp/hive/chenxiaojun/681dd0aa-ba35-4a0e-b069-3ad48f030774
2021-11-18 15:33:03,305 INFO [org.apache.hadoop.hive.ql.session.SessionState] - Created local directory: /var/folders/4m/n1wgh7rd2yqfv301kq00l4q40000gn/T/chenxiaojun/681dd0aa-ba35-4a0e-b069-3ad48f030774
2021-11-18 15:33:03,311 INFO [org.apache.hadoop.hive.ql.session.SessionState] - Created HDFS directory: /tmp/hive/chenxiaojun/681dd0aa-ba35-4a0e-b069-3ad48f030774/_tmp_space.db
2021-11-18 15:33:03,314 INFO [org.apache.hadoop.hive.ql.session.SessionState] - No Tez session required at this point. hive.execution.engine=mr.
Exception in thread ""main"" java.lang.NullPointerException
    at org.apache.flink.table.catalog.hive.client.HiveShimV100.registerTemporaryFunction(HiveShimV100.java:422)
    at org.apache.flink.table.planner.delegation.hive.HiveParser.parse(HiveParser.java:217)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:724)
    at com.hacker.flinksql.hive.HiveSqlTest.main(HiveSqlTest.java:48)

```

I found the error code in flink-1.13.2, 

org.apache.flink.table.catalog.hive.client.HiveShimV100.java - line:422

this method params is null, the code:

```

@Override
public void registerTemporaryFunction(String funcName, Class funcClass) {
try

{ registerTemporaryFunction.invoke(null, funcName, funcClass); }

catch (IllegalAccessException | InvocationTargetException e)

{ throw new FlinkHiveException(""Failed to register temp function"", e); }

}

```

my maven dependency

```

<properties>
<hadoop.version>2.6.0-cdh5.14.2</hadoop.version>
<hive.version>1.1.0-cdh5.14.2</hive.version>
</properties>

<!-- flink sql core -->
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-table-api-java-bridge_${scala.binary.version}</artifactId>
<version>${flink.version}</version>
<scope>provided</scope>
</dependency>

<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-table-planner-blink_${scala.binary.version}</artifactId>
<version>${flink.version}</version>
<scope>provided</scope>
</dependency>

<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-clients_${scala.binary.version}</artifactId>
<version>${flink.version}</version>
<scope>provided</scope>
</dependency>

<dependency>
<groupId>org.slf4j</groupId>
<artifactId>slf4j-log4j12</artifactId>
<version>1.7.5</version>
<scope>provided</scope>
</dependency>

<!-- hive catalog -->
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-connector-hive_${scala.binary.version}</artifactId>
<version>${flink.version}</version>
<scope>provided</scope>
</dependency>

<dependency>
<groupId>org.apache.hive</groupId>
<artifactId>hive-exec</artifactId>
<version>${hive.version}</version>
<scope>provided</scope>
</dependency>

<!-- catalog hadoop dependency -->
<dependency>
<groupId>org.apache.hadoop</groupId>
<artifactId>hadoop-client</artifactId>
<version>2.6.0-cdh5.15.2</version>
<scope>provided</scope>
</dependency>

<dependency>
<groupId>org.apache.hadoop</groupId>
<artifactId>hadoop-mapreduce-client-core</artifactId>
<version>2.6.0-cdh5.15.2</version>
<scope>provided</scope>
</dependency>

```","flink-1.13.2

cdh5.14.2

jdk8",Adrian Z,luoyuxia,slashchenxiaojun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 19 01:12:17 UTC 2021,,,,,,,,,,"0|z0wuns:",9223372036854775807,"hive-exec.version = 1.1.0 do not supper hive Dialect, try to use higher version like 1.2.0",,,,,,,,,,,,,,,,,,,"18/Nov/21 09:49;luoyuxia;[~slashchenxiaojun]  Thanks for reporting it,

It seems no such method 

`public static boolean registerTemporaryFunction(String functionName, Class<?> udfClass)`

in hive. 1.1.0-cdh5.14.2.

 

I check it in hive. 1.1.0, and it does have the method.;;;","18/Nov/21 12:09;slashchenxiaojun;[~luoyuxia] ，thanks, my friend, but that means hive-ddl can't execute on hive-1.1.0-cdh5.14.2 ?

what should i do ? the jar run in cdh5.14.2, can i change hive-exec version or some ?;;;","19/Nov/21 01:12;luoyuxia;I think you can try to change your hive-exec version, Maybe a higher version like 1.2.0 or others can work.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Could not find any factory for identifier 'hive' that implements 'org.apache.flink.table.factories.DynamicTableFactory' in the classpath,FLINK-24942,13412272,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,JasonLee,JasonLee,17/Nov/21 13:20,22/Nov/21 08:50,13/Jul/23 08:12,18/Nov/21 09:19,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Connectors / Hive,Table SQL / Client,,,,0,,,,,"[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.ValidationException: Could not find any factory for identifier 'hive' that implements 'org.apache.flink.table.factories.DynamicTableFactory' in the classpath.

Available factory identifiers are:

blackhole
datagen
filesystem
kafka
print
upsert-kafka

 

The above exception is thrown when I execute the following SQL, even though I have added flink-sql-connector-hive-2.3.6_2.11-1.14.0.jar in flink/lib
{code:java}
// code placeholder

insert into fs_table
select xxx, 
xxx, 
xxx, 
xxx, 
xxx, 
DATE_FORMAT(ts_ltz, 'yyyy-MM-dd'), DATE_FORMAT(ts_ltz, 'HH')
from kafka_table; {code}
 ",Flink-1.14.0,fsk119,JasonLee,martijnvisser,ruanhang1993,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 18 08:18:39 UTC 2021,,,,,,,,,,"0|z0wtiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/21 02:18;ruanhang1993;How do you create table `fs_table`? We should make use of `HiveCatalog` to write to a hive table.

https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/connectors/table/hive/hive_read_write/#writing;;;","18/Nov/21 03:58;JasonLee;[~ruanhang1993] 

SET table.sql-dialect=hive;
drop table if exists fs_table;
CREATE TABLE fs_table (
xxx STRING,
xxx INT,
xxx STRING,
xxx STRING,
xxx BIGINT
) PARTITIONED BY (dt STRING,`hour` STRING) STORED AS PARQUET TBLPROPERTIES (
  'sink.partition-commit.delay'='1s',
  'sink.partition-commit.policy.kind'='metastore,success-file',
  'sink.rolling-policy.check-interval'='1min',
  'sink.partition-commit.trigger'='partition-time',
  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',
  'sink.partition-commit.watermark-time-zone'='Asia/Shanghai'
);;;;","18/Nov/21 04:17;fsk119;It seems the sql-client doesn't find the hive jar. You can start the sql client with -j option to specify which jar should be loaded.;;;","18/Nov/21 06:01;JasonLee;[~fsk119]  After looking at the relevant code, I found that the class hivedynamictablefactory was not added to meta-inf / services And I tried adding jar packages with -j but it didn't work;;;","18/Nov/21 08:18;fsk119;In the doc, it suggests to use catalog to manage tables in hive. I think the issue is not sql client's problem. Do you mind to close it? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""kubernetes application HA test"" hangs on azure",FLINK-24937,13412213,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangyang0918,gaoyunhaii,gaoyunhaii,17/Nov/21 09:21,15/Dec/21 01:44,13/Jul/23 08:12,23/Nov/21 09:13,1.14.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"{code:java}
2021-11-17T02:15:23.4319085Z Nov 17 02:15:23 ##[endgroup]
2021-11-17T02:15:23.4319779Z Nov 17 02:15:23 Searching for .dump, .dumpstream and related files in '/home/vsts/work/1/s'
2021-11-17T02:15:24.7906640Z dmesg: read kernel buffer failed: Operation not permitted
2021-11-17T02:15:25.1156830Z Nov 17 02:15:25 No taskexecutor daemon to stop on host fv-az123-364.
2021-11-17T02:15:25.3229454Z Nov 17 02:15:25 No standalonesession daemon to stop on host fv-az123-364.
2021-11-17T06:09:57.2488148Z ==========================================================================================
2021-11-17T06:09:57.2489380Z === WARNING: This task took already 95% of the available time budget of 281 minutes ===
2021-11-17T06:09:57.2490646Z ========================================================================================== {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26628&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=2562",,gaoyunhaii,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 23 09:13:20 UTC 2021,,,,,,,,,,"0|z0wt5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/21 09:24;gaoyunhaii;Hi [~wangyang0918] ~ Are you convenient to have a look at this issue~?;;;","17/Nov/21 11:34;wangyang0918;I think the root cause is that building the image failed without retrying and had a zero return code. Currently, the {{build_image}} function in {{common_docker.sh}} does not handle the return code correctly.

 

https://github.com/apache/flink/blob/master/flink-end-to-end-tests/test-scripts/common_docker.sh#L36

 
{code:java}
2021-11-17T02:14:36.0714817Z Nov 17 02:14:36  ---> Running in e99d4d069cea
2021-11-17T02:14:36.1943135Z Nov 17 02:14:36 [91m+ dpkg --print-architecture
2021-11-17T02:14:36.1989940Z Nov 17 02:14:36 [0m[91m+ wget -nv -O /usr/local/bin/gosu https://github.com/tianon/gosu/releases/download/1.11/gosu-amd64
2021-11-17T02:14:36.8405227Z Nov 17 02:14:36 [0m[91mhttps://objects.githubusercontent.com/github-production-release-asset-2e65be/19708981/82e9dd00-d091-11e8-8734-a1caffcee352?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211117%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211117T021304Z&X-Amz-Expires=300&X-Amz-Signature=d60d553d399e9738cfd987bdc7877eae11a3a16e654c9462e31bc3c10b54e73d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=19708981&response-content-disposition=attachment%3B%20filename%3Dgosu-amd64&response-content-type=application%2Foctet-stream:
2021-11-17T02:14:36.8408478Z Nov 17 02:14:36 2021-11-17 02:14:36 ERROR 503: Egress is over the account limit..
2021-11-17T02:14:36.8798098Z The command '/bin/sh -c set -ex;   wget -nv -O /usr/local/bin/gosu ""https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture)"";   wget -nv -O /usr/local/bin/gosu.asc ""https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture).asc"";   export GNUPGHOME=""$(mktemp -d)"";   for server in ha.pool.sks-keyservers.net $(shuf -e                           hkp://p80.pool.sks-keyservers.net:80                           keyserver.ubuntu.com                           hkp://keyserver.ubuntu.com:80                           pgp.mit.edu) ; do       gpg --batch --keyserver ""$server"" --recv-keys B42F6819007F00F88E364FD4036A9C25BF357DD4 && break || : ;   done &&   gpg --batch --verify /usr/local/bin/gosu.asc /usr/local/bin/gosu;   gpgconf --kill all;   rm -rf ""$GNUPGHOME"" /usr/local/bin/gosu.asc;   chmod +x /usr/local/bin/gosu;   gosu nobody true' returned a non-zero code: 8 {code};;;","18/Nov/21 03:04;gaoyunhaii;Very thanks [~wangyang0918] for investigating and attending this issue!;;;","23/Nov/21 09:13;wangyang0918;Fixed via:

master: 54ed407318004a9ae2107f8cffc086011c127906

1.14: 6ca6c9c7e89e1e2e4e1dc9ac621d72d114b91348

1.13: 29429eaf63b982ce386614cb90caeb70697cefb2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink v1.13.2 restarts itself while Tenable Nessus Vulnerability scans the machines,FLINK-24923,13412017,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,ozan,ozan,16/Nov/21 11:22,18/Aug/22 08:20,13/Jul/23 08:12,18/Nov/21 11:25,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"Every day at the same time (1:00 AM), we are scanning all the machines with nessus. But only flink machines fails and restart itself.

We are using flink v.1.13.2, with java 8

I have also opened stackoverflow issue, but it was not solved.

In the log: 1.2.3.4 is the nessus scanner ip address:
{code:java}
2021-11-16 01:02:25,020 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [/1.2.3.4:52128] failed with org.apache.flink.shaded.akka.org.jboss.netty.handler.codec.frame.TooLongFrameException: Adjusted frame length exceeds 10485760: 1195725860 - discarded
2021-11-16 01:02:25,021 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [/1.2.3.4:59658] failed with org.apache.flink.shaded.akka.org.jboss.netty.handler.codec.frame.TooLongFrameException: Adjusted frame length exceeds 10485760: 1195725860 - discarded
2021-11-16 01:02:27,872 INFO  org.apache.kafka.clients.FetchSessionHandler                 [] - [Consumer clientId=consumer-kafkaGroup-15, groupId=kafkaGroup] Node 5 was unable to process the fetch request with (sessionId=715318048, epoch=18189): FETCH_SESSION_ID_NOT_FOUND.
2021-11-16 01:02:28,837 INFO  org.apache.kafka.clients.FetchSessionHandler                 [] - [Consumer clientId=consumer-kafkaGroup-14, groupId=kafkaGroup] Node 7 was unable to process the fetch request with (sessionId=1922249004, epoch=18126): FETCH_SESSION_ID_NOT_FOUND.
2021-11-16 01:02:29,415 INFO  org.apache.kafka.clients.FetchSessionHandler                 [] - [Consumer clientId=consumer-kafkaGroup-12, groupId=kafkaGroup] Node 5 was unable to process the fetch request with (sessionId=511071171, epoch=18261): FETCH_SESSION_ID_NOT_FOUND.
2021-11-16 01:02:33,006 ERROR org.apache.flink.runtime.io.network.netty.PartitionRequestQueue [] - Encountered error while consuming partitions
org.apache.flink.shaded.netty4.io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
2021-11-16 01:02:33,062 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [/1.2.3.4:56542] failed with org.apache.flink.shaded.akka.org.jboss.netty.handler.codec.frame.TooLongFrameException: Adjusted frame length exceeds 10485760: 1224736772 - discarded
2021-11-16 01:02:33,063 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [/1.2.3.4:35858] failed with org.apache.flink.shaded.akka.org.jboss.netty.handler.codec.frame.TooLongFrameException: Adjusted frame length exceeds 10485760: 1224736772 - discarded
2021-11-16 01:02:33,069 ERROR akka.actor.OneForOneStrategy                                 [] - Error while decoding incoming Akka PDU of length: 22
akka.remote.transport.AkkaProtocolException: Error while decoding incoming Akka PDU of length: 22
Caused by: akka.remote.transport.PduCodecException: Decoding PDU failed.
    at akka.remote.transport.AkkaPduProtobufCodec$.decodePdu(AkkaPduCodec.scala:174) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.akka$remote$transport$ProtocolStateActor$$decodePdu(AkkaProtocolTransport.scala:658) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:412) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:375) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.processEvent(FSM.scala:684) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.processEvent(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:678) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.aroundReceive(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.13.2.jar:1.13.2]
Caused by: akka.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero).
    at akka.protobuf.InvalidProtocolBufferException.invalidTag(InvalidProtocolBufferException.java:93) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.CodedInputStream.readTag(CodedInputStream.java:112) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.<init>(WireFormats.java:8964) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.<init>(WireFormats.java:8928) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage$1.parsePartialFrom(WireFormats.java:9024) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage$1.parsePartialFrom(WireFormats.java:9019) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:145) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:181) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:192) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:197) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:53) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.parseFrom(WireFormats.java:9142) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.AkkaPduProtobufCodec$.decodePdu(AkkaPduCodec.scala:175) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.akka$remote$transport$ProtocolStateActor$$decodePdu(AkkaProtocolTransport.scala:658) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:412) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:375) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.processEvent(FSM.scala:684) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.processEvent(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:678) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.aroundReceive(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
2021-11-16 01:02:33,089 ERROR akka.actor.OneForOneStrategy                                 [] - Error while decoding incoming Akka PDU of length: 22
akka.remote.transport.AkkaProtocolException: Error while decoding incoming Akka PDU of length: 22
Caused by: akka.remote.transport.PduCodecException: Decoding PDU failed.
    at akka.remote.transport.AkkaPduProtobufCodec$.decodePdu(AkkaPduCodec.scala:174) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.akka$remote$transport$ProtocolStateActor$$decodePdu(AkkaProtocolTransport.scala:658) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:412) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:375) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.processEvent(FSM.scala:684) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.processEvent(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:678) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.aroundReceive(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
Caused by: akka.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero).
    at akka.protobuf.InvalidProtocolBufferException.invalidTag(InvalidProtocolBufferException.java:93) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.CodedInputStream.readTag(CodedInputStream.java:112) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.<init>(WireFormats.java:8964) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.<init>(WireFormats.java:8928) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage$1.parsePartialFrom(WireFormats.java:9024) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage$1.parsePartialFrom(WireFormats.java:9019) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:145) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:181) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:192) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:197) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:53) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.parseFrom(WireFormats.java:9142) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.AkkaPduProtobufCodec$.decodePdu(AkkaPduCodec.scala:175) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.akka$remote$transport$ProtocolStateActor$$decodePdu(AkkaProtocolTransport.scala:658) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:412) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:375) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.processEvent(FSM.scala:684) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.processEvent(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:678) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.aroundReceive(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
2021-11-16 01:02:33,080 ERROR akka.actor.OneForOneStrategy                                 [] - Error while decoding incoming Akka PDU of length: 64
akka.remote.transport.AkkaProtocolException: Error while decoding incoming Akka PDU of length: 64
Caused by: akka.remote.transport.PduCodecException: Decoding PDU failed.
    at akka.remote.transport.AkkaPduProtobufCodec$.decodePdu(AkkaPduCodec.scala:174) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.akka$remote$transport$ProtocolStateActor$$decodePdu(AkkaProtocolTransport.scala:658) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:412) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:375) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.processEvent(FSM.scala:684) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.processEvent(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:678) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.aroundReceive(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.13.2.jar:1.13.2]
Caused by: akka.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero).
    at akka.protobuf.InvalidProtocolBufferException.invalidTag(InvalidProtocolBufferException.java:93) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.CodedInputStream.readTag(CodedInputStream.java:112) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.<init>(WireFormats.java:8964) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.<init>(WireFormats.java:8928) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage$1.parsePartialFrom(WireFormats.java:9024) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage$1.parsePartialFrom(WireFormats.java:9019) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:145) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:181) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:192) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:197) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:53) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.parseFrom(WireFormats.java:9142) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.AkkaPduProtobufCodec$.decodePdu(AkkaPduCodec.scala:175) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.akka$remote$transport$ProtocolStateActor$$decodePdu(AkkaProtocolTransport.scala:658) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:412) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:375) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.processEvent(FSM.scala:684) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.processEvent(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:678) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.aroundReceive(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
2021-11-16 01:02:33,098 ERROR akka.actor.OneForOneStrategy                                 [] - Error while decoding incoming Akka PDU of length: 64
akka.remote.transport.AkkaProtocolException: Error while decoding incoming Akka PDU of length: 64
Caused by: akka.remote.transport.PduCodecException: Decoding PDU failed.
    at akka.remote.transport.AkkaPduProtobufCodec$.decodePdu(AkkaPduCodec.scala:174) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.akka$remote$transport$ProtocolStateActor$$decodePdu(AkkaProtocolTransport.scala:658) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:412) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:375) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.processEvent(FSM.scala:684) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.processEvent(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:678) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.aroundReceive(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
Caused by: akka.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero).
    at akka.protobuf.InvalidProtocolBufferException.invalidTag(InvalidProtocolBufferException.java:93) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.CodedInputStream.readTag(CodedInputStream.java:112) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.<init>(WireFormats.java:8964) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.<init>(WireFormats.java:8928) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage$1.parsePartialFrom(WireFormats.java:9024) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage$1.parsePartialFrom(WireFormats.java:9019) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:145) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:181) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:192) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:197) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:53) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.parseFrom(WireFormats.java:9142) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.AkkaPduProtobufCodec$.decodePdu(AkkaPduCodec.scala:175) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.akka$remote$transport$ProtocolStateActor$$decodePdu(AkkaProtocolTransport.scala:658) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:412) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:375) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.processEvent(FSM.scala:684) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.processEvent(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:678) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.aroundReceive(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
2021-11-16 01:02:33,123 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [/1.2.3.4:35898] failed with java.io.IOException: Connection reset by peer
2021-11-16 01:02:33,123 ERROR org.apache.flink.runtime.io.network.netty.PartitionRequestQueue [] - Encountered error while consuming partitions
org.apache.flink.shaded.netty4.io.netty.handler.codec.DecoderException: java.lang.IllegalStateException: Network stream corrupted: received incorrect magic number.
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:471) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:792) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:475) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.IllegalStateException: Network stream corrupted: received incorrect magic number.
    at org.apache.flink.runtime.io.network.netty.NettyMessage$NettyMessageDecoder.decode(NettyMessage.java:210) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.LengthFieldBasedFrameDecoder.decode(LengthFieldBasedFrameDecoder.java:332) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    ... 14 more
2021-11-16 01:02:33,123 ERROR org.apache.flink.runtime.io.network.netty.PartitionRequestQueue [] - Encountered error while consuming partitions
org.apache.flink.shaded.netty4.io.netty.handler.codec.DecoderException: java.lang.IllegalStateException: Network stream corrupted: received incorrect magic number.
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:471) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:792) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:475) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.IllegalStateException: Network stream corrupted: received incorrect magic number.
    at org.apache.flink.runtime.io.network.netty.NettyMessage$NettyMessageDecoder.decode(NettyMessage.java:210) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.LengthFieldBasedFrameDecoder.decode(LengthFieldBasedFrameDecoder.java:332) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    ... 14 more
2021-11-16 01:02:33,127 ERROR org.apache.flink.runtime.io.network.netty.PartitionRequestQueue [] - Encountered error while consuming partitions
org.apache.flink.shaded.netty4.io.netty.handler.codec.CorruptedFrameException: Adjusted frame length (0) is less than lengthFieldEndOffset: 4
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.LengthFieldBasedFrameDecoder.failOnFrameLengthLessThanLengthFieldEndOffset(LengthFieldBasedFrameDecoder.java:358) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.LengthFieldBasedFrameDecoder.decode(LengthFieldBasedFrameDecoder.java:415) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.runtime.io.network.netty.NettyMessage$NettyMessageDecoder.decode(NettyMessage.java:201) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.LengthFieldBasedFrameDecoder.decode(LengthFieldBasedFrameDecoder.java:332) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:404) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:371) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:354) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:818) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at java.lang.Thread.run(Thread.java:829) [?:?]
2021-11-16 01:02:33,164 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [/1.2.3.4:56838] failed with org.apache.flink.shaded.akka.org.jboss.netty.handler.codec.frame.TooLongFrameException: Adjusted frame length exceeds 10485760: 1212501076 - discarded
2021-11-16 01:02:33,164 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [/1.2.3.4:36148] failed with org.apache.flink.shaded.akka.org.jboss.netty.handler.codec.frame.TooLongFrameException: Adjusted frame length exceeds 10485760: 1212501076 - discarded
2021-11-16 01:02:37,009 ERROR org.apache.flink.runtime.io.network.netty.PartitionRequestQueue [] - Encountered error while consuming partitions
org.apache.flink.shaded.netty4.io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer 

{code}
 

*------------------------------------------------------------------------------------*

 
 * *At the same time, here are the logs inside the standalonesession.log:*

 

 
{code:java}
2021-11-16 01:02:36,256 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [/1.2.3.5:45502] failed with org.apache.flink.shaded.akka.org.jboss.netty.handler.codec.frame.TooLongFrameException: Adjusted frame length exceeds 10485760: 1195725860 - discarded
2021-11-16 01:02:36,256 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [/1.2.3.5:38880] failed with org.apache.flink.shaded.akka.org.jboss.netty.handler.codec.frame.TooLongFrameException: Adjusted frame length exceeds 10485760: 1195725860 - discarded
2021-11-16 01:02:36,245 ERROR org.apache.flink.runtime.blob.BlobServerConnection           [] - Error while executing BLOB connection.
java.io.IOException: Unknown operation 71
    at org.apache.flink.runtime.blob.BlobServerConnection.run(BlobServerConnection.java:116) [flink-dist_2.11-1.13.2.jar:1.13.2]
2021-11-16 01:02:44,246 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [/1.2.3.5:55498] failed with org.apache.flink.shaded.akka.org.jboss.netty.handler.codec.frame.TooLongFrameException: Adjusted frame length exceeds 10485760: 1224736772 - discarded
2021-11-16 01:02:44,252 ERROR akka.actor.OneForOneStrategy                                 [] - Error while decoding incoming Akka PDU of length: 64
akka.remote.transport.AkkaProtocolException: Error while decoding incoming Akka PDU of length: 64
Caused by: akka.remote.transport.PduCodecException: Decoding PDU failed.
    at akka.remote.transport.AkkaPduProtobufCodec$.decodePdu(AkkaPduCodec.scala:174) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.akka$remote$transport$ProtocolStateActor$$decodePdu(AkkaProtocolTransport.scala:658) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:412) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:375) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.processEvent(FSM.scala:684) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.processEvent(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:678) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.aroundReceive(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.13.2.jar:1.13.2]
Caused by: akka.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero).
    at akka.protobuf.InvalidProtocolBufferException.invalidTag(InvalidProtocolBufferException.java:93) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.CodedInputStream.readTag(CodedInputStream.java:112) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.<init>(WireFormats.java:8964) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.<init>(WireFormats.java:8928) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage$1.parsePartialFrom(WireFormats.java:9024) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage$1.parsePartialFrom(WireFormats.java:9019) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:145) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:181) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:192) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:197) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:53) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.parseFrom(WireFormats.java:9142) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.AkkaPduProtobufCodec$.decodePdu(AkkaPduCodec.scala:175) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.akka$remote$transport$ProtocolStateActor$$decodePdu(AkkaProtocolTransport.scala:658) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:412) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:375) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.processEvent(FSM.scala:684) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.processEvent(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:678) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.aroundReceive(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
2021-11-16 01:02:44,256 ERROR akka.actor.OneForOneStrategy                                 [] - Error while decoding incoming Akka PDU of length: 22
akka.remote.transport.AkkaProtocolException: Error while decoding incoming Akka PDU of length: 22
Caused by: akka.remote.transport.PduCodecException: Decoding PDU failed.
    at akka.remote.transport.AkkaPduProtobufCodec$.decodePdu(AkkaPduCodec.scala:174) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.akka$remote$transport$ProtocolStateActor$$decodePdu(AkkaProtocolTransport.scala:658) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:412) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:375) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.processEvent(FSM.scala:684) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.processEvent(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:678) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.aroundReceive(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.13.2.jar:1.13.2]
Caused by: akka.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero).
    at akka.protobuf.InvalidProtocolBufferException.invalidTag(InvalidProtocolBufferException.java:93) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.CodedInputStream.readTag(CodedInputStream.java:112) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.<init>(WireFormats.java:8964) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.<init>(WireFormats.java:8928) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage$1.parsePartialFrom(WireFormats.java:9024) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage$1.parsePartialFrom(WireFormats.java:9019) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:145) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:181) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:192) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:197) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:53) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.parseFrom(WireFormats.java:9142) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.AkkaPduProtobufCodec$.decodePdu(AkkaPduCodec.scala:175) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.akka$remote$transport$ProtocolStateActor$$decodePdu(AkkaProtocolTransport.scala:658) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:412) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:375) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.processEvent(FSM.scala:684) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.processEvent(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:678) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.aroundReceive(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
2021-11-16 01:02:44,280 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [/1.2.3.5:48956] failed with org.apache.flink.shaded.akka.org.jboss.netty.handler.codec.frame.TooLongFrameException: Adjusted frame length exceeds 10485760: 1224736772 - discarded
2021-11-16 01:02:44,303 ERROR org.apache.flink.runtime.blob.BlobServerConnection           [] - Error while executing BLOB connection.
java.io.IOException: Unknown operation 73
    at org.apache.flink.runtime.blob.BlobServerConnection.run(BlobServerConnection.java:116) [flink-dist_2.11-1.13.2.jar:1.13.2]
2021-11-16 01:02:44,310 ERROR akka.actor.OneForOneStrategy                                 [] - Error while decoding incoming Akka PDU of length: 64
akka.remote.transport.AkkaProtocolException: Error while decoding incoming Akka PDU of length: 64
Caused by: akka.remote.transport.PduCodecException: Decoding PDU failed.
    at akka.remote.transport.AkkaPduProtobufCodec$.decodePdu(AkkaPduCodec.scala:174) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.akka$remote$transport$ProtocolStateActor$$decodePdu(AkkaProtocolTransport.scala:658) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:412) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:375) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.processEvent(FSM.scala:684) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.processEvent(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:678) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.aroundReceive(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
Caused by: akka.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero).
    at akka.protobuf.InvalidProtocolBufferException.invalidTag(InvalidProtocolBufferException.java:93) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.CodedInputStream.readTag(CodedInputStream.java:112) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.<init>(WireFormats.java:8964) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.<init>(WireFormats.java:8928) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage$1.parsePartialFrom(WireFormats.java:9024) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage$1.parsePartialFrom(WireFormats.java:9019) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:145) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:181) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:192) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:197) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:53) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.parseFrom(WireFormats.java:9142) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.AkkaPduProtobufCodec$.decodePdu(AkkaPduCodec.scala:175) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.akka$remote$transport$ProtocolStateActor$$decodePdu(AkkaProtocolTransport.scala:658) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:412) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:375) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.processEvent(FSM.scala:684) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.processEvent(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:678) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.aroundReceive(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
2021-11-16 01:02:44,314 ERROR akka.actor.OneForOneStrategy                                 [] - Error while decoding incoming Akka PDU of length: 22
akka.remote.transport.AkkaProtocolException: Error while decoding incoming Akka PDU of length: 22
Caused by: akka.remote.transport.PduCodecException: Decoding PDU failed.
    at akka.remote.transport.AkkaPduProtobufCodec$.decodePdu(AkkaPduCodec.scala:174) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.akka$remote$transport$ProtocolStateActor$$decodePdu(AkkaProtocolTransport.scala:658) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:412) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:375) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.processEvent(FSM.scala:684) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.processEvent(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:678) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.aroundReceive(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
Caused by: akka.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero).
    at akka.protobuf.InvalidProtocolBufferException.invalidTag(InvalidProtocolBufferException.java:93) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.CodedInputStream.readTag(CodedInputStream.java:112) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.<init>(WireFormats.java:8964) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.<init>(WireFormats.java:8928) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage$1.parsePartialFrom(WireFormats.java:9024) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage$1.parsePartialFrom(WireFormats.java:9019) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:145) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:181) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:192) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:197) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.protobuf.AbstractParser.parseFrom(AbstractParser.java:53) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.WireFormats$AkkaProtocolMessage.parseFrom(WireFormats.java:9142) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.AkkaPduProtobufCodec$.decodePdu(AkkaPduCodec.scala:175) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.akka$remote$transport$ProtocolStateActor$$decodePdu(AkkaProtocolTransport.scala:658) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:412) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor$$anonfun$3.applyOrElse(AkkaProtocolTransport.scala:375) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.processEvent(FSM.scala:684) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.processEvent(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:678) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.remote.transport.ProtocolStateActor.aroundReceive(AkkaProtocolTransport.scala:286) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
2021-11-16 01:02:44,311 ERROR org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler [] - Caught exception
java.io.IOException: Connection reset by peer
    at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[?:?]
    at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[?:?]
    at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:276) ~[?:?]
    at sun.nio.ch.IOUtil.read(IOUtil.java:233) ~[?:?]
    at sun.nio.ch.IOUtil.read(IOUtil.java:223) ~[?:?]
    at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:356) ~[?:?]
    at org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:253) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1133) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:148) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at java.lang.Thread.run(Thread.java:829) [?:?]
2021-11-16 01:02:44,342 ERROR org.apache.flink.runtime.blob.BlobServerConnection           [] - PUT operation failed
java.lang.IllegalArgumentException: Invalid BLOB addressing for permanent BLOBs
    at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.runtime.blob.BlobServerConnection.put(BlobServerConnection.java:334) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.runtime.blob.BlobServerConnection.run(BlobServerConnection.java:110) [flink-dist_2.11-1.13.2.jar:1.13.2]
2021-11-16 01:02:44,345 ERROR org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler [] - Caught exception
java.io.IOException: Connection reset by peer
    at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[?:?]
    at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[?:?]
    at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:276) ~[?:?]
    at sun.nio.ch.IOUtil.read(IOUtil.java:233) ~[?:?]
    at sun.nio.ch.IOUtil.read(IOUtil.java:223) ~[?:?]
    at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:356) ~[?:?]
    at org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:253) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1133) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:148) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at java.lang.Thread.run(Thread.java:829) [?:?]
2021-11-16 01:02:44,817 ERROR org.apache.flink.runtime.blob.BlobServerConnection           [] - PUT operation failed
java.lang.IllegalArgumentException: Invalid BLOB addressing for permanent BLOBs
    at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.runtime.blob.BlobServerConnection.put(BlobServerConnection.java:334) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.runtime.blob.BlobServerConnection.run(BlobServerConnection.java:110) [flink-dist_2.11-1.13.2.jar:1.13.2]
2021-11-16 01:02:44,875 ERROR org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler [] - Caught exception
java.io.IOException: Connection reset by peer
    at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[?:?]
    at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[?:?]
    at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:276) ~[?:?]
    at sun.nio.ch.IOUtil.read(IOUtil.java:233) ~[?:?]
    at sun.nio.ch.IOUtil.read(IOUtil.java:223) ~[?:?]
    at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:356) ~[?:?]
    at org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:253) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1133) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:148) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at java.lang.Thread.run(Thread.java:829) [?:?]
2021-11-16 01:02:44,903 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [/1.2.3.5:56582] failed with org.apache.flink.shaded.akka.org.jboss.netty.handler.codec.frame.TooLongFrameException: Adjusted frame length exceeds 10485760: 1212501076 - discarded
2021-11-16 01:02:44,916 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [/1.2.3.5:49988] failed with org.apache.flink.shaded.akka.org.jboss.netty.handler.codec.frame.TooLongFrameException: Adjusted frame length exceeds 10485760: 1212501076 - discarded
2021-11-16 01:02:44,925 ERROR org.apache.flink.runtime.blob.BlobServerConnection           [] - Error while executing BLOB connection.
java.io.IOException: Unknown operation 72
    at org.apache.flink.runtime.blob.BlobServerConnection.run(BlobServerConnection.java:116) [flink-dist_2.11-1.13.2.jar:1.13.2]
2021-11-16 01:02:46,149 ERROR org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler [] - Caught exception
java.io.IOException: Connection reset by peer
    at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[?:?]
    at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[?:?]
    at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:276) ~[?:?]
    at sun.nio.ch.IOUtil.read(IOUtil.java:233) ~[?:?]
    at sun.nio.ch.IOUtil.read(IOUtil.java:223) ~[?:?]
    at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:356) ~[?:?]
    at org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:253) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1133) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:148) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at java.lang.Thread.run(Thread.java:829) [?:?]
2021-11-16 01:02:49,630 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [/1.2.3.5:59724] failed with org.apache.flink.shaded.akka.org.jboss.netty.handler.codec.frame.TooLongFrameException: Adjusted frame length exceeds 10485760: 50331671 - discarded
2021-11-16 01:02:51,250 ERROR org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler [] - Caught exception
java.io.IOException: Connection reset by peer
    at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[?:?]
    at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[?:?]
    at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:276) ~[?:?]
    at sun.nio.ch.IOUtil.read(IOUtil.java:233) ~[?:?]
    at sun.nio.ch.IOUtil.read(IOUtil.java:223) ~[?:?]
    at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:356) ~[?:?]
    at org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:253) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1133) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:148) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist_2.11-1.13.2.jar:1.13.2]
    at java.lang.Thread.run(Thread.java:829) [?:?]
2021-11-16 01:02:51,285 WARN  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Unhandled exception
java.io.IOException: Connection reset by peer {code}
 

 

 

 

 

 

 ",,liufangliang,martijnvisser,ozan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 18 08:20:48 UTC 2022,,,,,,,,,,"0|z0wryo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/21 12:16;martijnvisser;If I remember correctly, Nessus actively scans ports, tries to determine which service is running there by communicating via those ports. Flink nodes also communicate with each other (for example, via RPC on port 6123 by default https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/deployment/config/). I can imagine that Nessus sends an unexpected message to those ports which interferes with Flink's communication, resulting in a restart. Especially since Flink by default doesn't use something like TLS/SSL for authentication and encryption on the network communication. Easiest is to not run Nessus on a Flink cluster :) or enable TLS/SSL for network communication. See https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/deployment/security/security-ssl/ for more info;;;","18/Nov/21 11:24;ozan;[~MartijnVisser] , after enabling ssl/tls for network communication,  we solved our problem, thank you..;;;","18/Nov/21 11:25;ozan;Enable SSL/TLS for internal communications;;;","18/Aug/22 08:20;liufangliang;[~ozan] , just need to set security.ssl.internal.enabled to true?

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix spelling errors in the word ""parallism""",FLINK-24922,13412005,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiangqiao,xiangqiao,xiangqiao,16/Nov/21 10:00,15/Dec/21 01:44,13/Jul/23 08:12,25/Nov/21 05:26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Documentation,Table SQL / Client,,,,0,pull-request-available,,,,"Fix the spelling error of ""parallism"" in the document of SQL client.",,jingzhang,xiangqiao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 25 05:25:46 UTC 2021,,,,,,,,,,"0|z0wrw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/21 05:25;jingzhang;Fixed in 1.15.0: 591c398f518ece86023b2f51589e07bbd7a8cdc4
Fixed in 1.14.1 d654297eb2e696320628c6092c7f467d71933ea2
Fixed in 1.13.4 ada27e3b9794b57ec4cc20b096f1bc93827b0953
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnalignedCheckpointITCase hangs on Azure,FLINK-24919,13411986,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,pnowojski,pnowojski,16/Nov/21 08:36,15/Dec/21 01:44,13/Jul/23 08:12,02/Dec/21 16:02,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"Extracted from FLINK-23466

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26304&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=13067

Nov 10 16:13:03 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointITCase#execute[pipeline with mixed channels, p = 20, timeout = 0, buffersPerChannel = 1].

From the log, we can see this case hangs. I guess this seems a new issue which is different from the one reported in this ticket. From the stack, it seems there is something wrong with the checkpoint coordinator, the following thread locked 0x0000000087db4fb8:
{code:java}
2021-11-10T17:14:21.0899474Z Nov 10 17:14:21 ""jobmanager-io-thread-2"" #12984 daemon prio=5 os_prio=0 tid=0x00007f12e000b800 nid=0x3fb6 runnable [0x00007f0fcd6d4000]
2021-11-10T17:14:21.0899924Z Nov 10 17:14:21    java.lang.Thread.State: RUNNABLE
2021-11-10T17:14:21.0900300Z Nov 10 17:14:21 	at java.util.HashMap$TreeNode.balanceDeletion(HashMap.java:2338)
2021-11-10T17:14:21.0900745Z Nov 10 17:14:21 	at java.util.HashMap$TreeNode.removeTreeNode(HashMap.java:2112)
2021-11-10T17:14:21.0901146Z Nov 10 17:14:21 	at java.util.HashMap.removeNode(HashMap.java:840)
2021-11-10T17:14:21.0901577Z Nov 10 17:14:21 	at java.util.LinkedHashMap.afterNodeInsertion(LinkedHashMap.java:301)
2021-11-10T17:14:21.0902002Z Nov 10 17:14:21 	at java.util.HashMap.putVal(HashMap.java:664)
2021-11-10T17:14:21.0902531Z Nov 10 17:14:21 	at java.util.HashMap.putMapEntries(HashMap.java:515)
2021-11-10T17:14:21.0902931Z Nov 10 17:14:21 	at java.util.HashMap.putAll(HashMap.java:785)
2021-11-10T17:14:21.0903429Z Nov 10 17:14:21 	at org.apache.flink.runtime.checkpoint.ExecutionAttemptMappingProvider.getVertex(ExecutionAttemptMappingProvider.java:60)
2021-11-10T17:14:21.0904060Z Nov 10 17:14:21 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.reportStats(CheckpointCoordinator.java:1867)
2021-11-10T17:14:21.0904686Z Nov 10 17:14:21 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:1152)
2021-11-10T17:14:21.0905372Z Nov 10 17:14:21 	- locked <0x0000000087db4fb8> (a java.lang.Object)
2021-11-10T17:14:21.0905895Z Nov 10 17:14:21 	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$acknowledgeCheckpoint$1(ExecutionGraphHandler.java:89)
2021-11-10T17:14:21.0906493Z Nov 10 17:14:21 	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler$$Lambda$1368/705813936.accept(Unknown Source)
2021-11-10T17:14:21.0907086Z Nov 10 17:14:21 	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119)
2021-11-10T17:14:21.0907698Z Nov 10 17:14:21 	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler$$Lambda$1369/1447418658.run(Unknown Source)
2021-11-10T17:14:21.0908210Z Nov 10 17:14:21 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2021-11-10T17:14:21.0908735Z Nov 10 17:14:21 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2021-11-10T17:14:21.0909333Z Nov 10 17:14:21 	at java.lang.Thread.run(Thread.java:748) {code}
But other thread is waiting for the lock. I am not familiar with these logics and not sure if this is in the right state. Could anyone who is familiar with these logics take a look?

 

BTW, concurrent access of HashMap may cause infinite loop，I see in the stack that there are multiple threads are accessing HashMap, though I am not sure if they are the same instance.",,dwysakowicz,kevin.cyj,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23466,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 02 08:33:53 UTC 2021,,,,,,,,,,"0|z0wrrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/21 08:33;dwysakowicz;Fixed in:
* master
** 5bf49f1dc9215e86758d002bc5a2ab82e738d3fa
* 1.14.1
** d26c0e511e9f37671b52c23df4c09e7aa3719d5a
* 1.13.4
** 704941c883727e9cf8ca3dd7ee6e6f23056a527e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveSchedulerTest.testJobStatusListenerNotifiedOfJobStatusChanges unstable,FLINK-24903,13411809,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,roman,roman,15/Nov/21 13:15,22/Nov/21 21:37,13/Jul/23 08:12,22/Nov/21 21:37,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / Coordination,,,,,0,test-stability,,,,"[https://dev.azure.com/khachatryanroman/flink/_build/results?buildId=1225&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=511d2595-ec54-5ab7-86ce-92f328796f20&l=7753]
 {code}
 2021-11-14T20:22:23.1142812Z Nov 14 20:22:23 [ERROR] Failures:
 2021-11-14T20:22:23.1149388Z Nov 14 20:22:23 [ERROR]   AdaptiveSchedulerTest.testJobStatusListenerNotifiedOfJobStatusChanges:684
 2021-11-14T20:22:23.1150058Z Nov 14 20:22:23 Expected: (a collection containing <RUNNING> and a collection containing <FINISHED>)
 2021-11-14T20:22:23.1150581Z Nov 14 20:22:23      but: a collection containing <FINISHED> was <RUNNING>
 2021-11-14T20:22:23.1152966Z Nov 14 20:22:23 [INFO]
 2021-11-14T20:22:23.1156414Z Nov 14 20:22:23 [ERROR] Tests run: 6048, Failures: 1, Errors: 0, Skipped: 97
{code}

Locally, it fails ~14 runs out of 100 (when running only testJobStatusListenerNotifiedOfJobStatusChanges in a loop).
Also on master.


It looks like job termination future is always completed before the jobStatusChangeListener is notified (AdaptiveScheduler.transitionToState, targetState.getState() completes the future).

Sleeping for 1ms before checking the assertion prevents the failure.

 

cc: [~trohrmann] ",,gaoyunhaii,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 22 21:37:34 UTC 2021,,,,,,,,,,"0|z0wqoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/21 08:37;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26624&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=7747];;;","22/Nov/21 21:37;chesnay;master: 60f2f3c045df34655e2b51d4b248039eea7ee883;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to execute goal com.github.siom79.japicmp:japicmp-maven-plugin:0.11.0:cmp (default) on project flink-metrics-core,FLINK-24896,13411746,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,gaoyunhaii,gaoyunhaii,15/Nov/21 08:58,17/Nov/21 15:21,13/Jul/23 08:12,15/Nov/21 09:31,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Build System / Azure Pipelines,,,,,0,test-stability,,,,"{code:java}
[ERROR] Failed to execute goal com.github.siom79.japicmp:japicmp-maven-plugin:0.11.0:cmp (default) on project flink-metrics-core: Execution default of goal com.github.siom79.japicmp:japicmp-maven-plugin:0.11.0:cmp failed: Marshalling of XML document failed: Implementation of JAXB-API has not been found on module path or classpath. com.sun.xml.internal.bind.v2.ContextFactory -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :flink-metrics-core
 {code}
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26496&view=logs&j=946871de-358d-5815-3994-8175615bc253&t=e0240c62-4570-5d1c-51af-dd63d2093da1&l=1086]",,fpaul,gaoyunhaii,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 15 09:31:17 UTC 2021,,,,,,,,,,"0|z0wqao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/21 08:59;gaoyunhaii;But I tried the plugin locally, it does not cause throws the exception. I'm a bit suspect that some jars are not downloaded in the azure pipeline.;;;","15/Nov/21 09:00;fpaul;[~MartijnVisser] maybe it is related to https://issues.apache.org/jira/browse/FLINK-24724 ?;;;","15/Nov/21 09:06;martijnvisser;[~fpaul] That is a suspicious one yes. [~gaoyunhaii] was this the first run since that PR was merged or were there more instances? ;;;","15/Nov/21 09:13;martijnvisser;Checked it myself, it's multiple. Let's revert https://issues.apache.org/jira/browse/FLINK-24724;;;","15/Nov/21 09:18;chesnay;[~MartijnVisser] It fails the Java 11 compile phase consistently.;;;","15/Nov/21 09:22;fpaul;I suspect it is caused by [https://github.com/eclipse-ee4j/jaxb-ri/issues/1235] ;;;","15/Nov/21 09:31;chesnay;I have reverted FLINK-24724. The updated dependencies were just not compatible with the plugin.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL Client should print corrently multisets,FLINK-24889,13411537,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,12/Nov/21 18:14,18/Nov/21 15:56,13/Jul/23 08:12,18/Nov/21 15:56,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Table SQL / API,,,,,0,pull-request-available,,,,"Probably the easiest way to reproduce is 
{code:sql}
CREATE TABLE flink_multiset_example (
     m multiset<BIGINT>
 ) WITH (
   'connector' = 'datagen'
 );
select * from flink_multiset_example;
{code}

I think it relates to https://issues.apache.org/jira/browse/FLINK-21456",,airblader,Sergey Nuyanzin,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 18 15:56:23 UTC 2021,,,,,,,,,,"0|z0wp08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Nov/21 19:03;airblader;[~slinkydeveloper];;;","18/Nov/21 15:56;twalthr;Fixed in master: ad16e2c50ce17a713bbabd8b6afe4637b2123a5e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Retrying savepoints may cause early cluster shutdown,FLINK-24887,13411464,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,12/Nov/21 11:11,13/Nov/21 10:22,13/Jul/23 08:12,13/Nov/21 10:22,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / REST,,,,,0,pull-request-available,,,,"If an operation is retried we potentially access the result of a previous attempt to see if it has already failed and eagerly fail the trigger request. If that attempt is already complete then this may lead to an unexpected shutdown of the cluster.

Beyond this issue, the eager checking of previous attempts makes error handling more complicated, because you have to cover all cases for both the trigger and status-retrieval operations.

",,Thesharing,trohrmann,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Nov 13 10:22:46 UTC 2021,,,,,,,,,,"0|z0wok0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/21 10:22;chesnay;master:
6b9c1ac9c6d4d89f961612672ece326e8b9cb02d
a66a876126b2f702fa224be534aca4c729dd6f8a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"flink flame args bug, may we need to modify TimeUtils to supports m",FLINK-24886,13411443,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,12/Nov/21 09:41,14/Jan/22 09:10,13/Jul/23 08:12,14/Jan/22 09:10,1.13.3,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / REST,,,,,1,pull-request-available,,,,"When I refer to the configuration documentation setting rest.flamegraph.cleanup-interval to ""10 m"", it will be bug

 
{code:java}
public static final ConfigOption<Duration> FLAMEGRAPH_CLEANUP_INTERVAL =
        key(""rest.flamegraph.cleanup-interval"")
                .durationType()
                .defaultValue(Duration.ofMinutes(10))
                .withDescription(
                        ""Time after which cached stats are cleaned up if not accessed. It can""
                                + "" be specified using notation: \""100 s\"", \""10 m\"".""); {code}
 

 

!image-2021-11-12-17-38-28-969.png!",,jackylau,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/21 09:38;jackylau;image-2021-11-12-17-38-28-969.png;https://issues.apache.org/jira/secure/attachment/13036022/image-2021-11-12-17-38-28-969.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 14 09:10:29 UTC 2022,,,,,,,,,,"0|z0wofc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Nov/21 09:45;jackylau;we can fix it 2 ways
 # modify the comments 10m -> 10min
{code:java}
                   ""Time after which cached stats are cleaned up if not accessed. It can""
                                + "" be specified using notation: \""100 s\"", \""10 m\""."");  {code}

 

      2. modify the TimeUtils to support it.

I think the 2 is better. Because it fits user habits. And i am wiling to fix it;;;","15/Nov/21 15:04;jackylau;hi [~jark]  [~trohrmann] , do you have time to review it ?;;;","17/Nov/21 17:52;trohrmann;Thanks for reporting this issue [~jackylau]. If we change {{TimeUtils}}, then it should follow some standard. At the moment, it follows Scala's {{Duration}} logic. [~afedulov] can you take care of it?;;;","14/Jan/22 09:10;trohrmann;Fixed via 4cd0e396514a6efd7a180a3aea106124d919ebf4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProcessElement Interface parameter Collector  : java.lang.NullPointerException,FLINK-24885,13411436,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,wangbaohua,wangbaohua,12/Nov/21 09:22,05/Apr/23 12:22,13/Jul/23 08:12,22/Dec/21 06:08,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"2021-11-15 11:11:55,032 INFO  com.asap.demo.function.dealMapFunction                       [] - size:160
2021-11-15 11:11:55,230 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Co-Process-Broadcast-Keyed -> Map -> DataSteamToTable(stream=default_catalog.default_database.Unregistered_DataStream_Source_8, type=*com.asap.demo.model.BeanField<`account` STRING, `accountId` STRING, `accountIn` STRING, `accountInName` STRING, `accountInOrgId` STRING, `accountInOrgName` STRING, `accountInType` STRING, `accountName` STRING, `accountOrgId` STRING, `accountOrgName` STRING, `accountOut` STRING, `accountOutName` STRING, `accountOutOrgId` STRING, `accountOutOrgName` STRING, `accountOutType` STRING, `accountStatus` STRING, `accountType` STRING, `action` STRING, `actionDesc` STRING, `alarmcontext` STRING, `alarmgrade` STRING, `alarmtype` STRING, `alertId` STRING, `alertInfo` STRING, `alertLevel` STRING, `alertSignatureIdL` STRING, `appId` STRING, `appName` STRING, `appProtocol` STRING, `appType` STRING, `areaId` STRING, `areaName` STRING, `areaType` STRING, `assetFrom` STRING, `assetId` STRING, `assetInfo` STRING, `assetIp` STRING, `assetLevel` STRING, `assetName` STRING, `assetPid` STRING, `assetType` STRING, `assetUse` STRING, `assetVendor` STRING, `attackStage` STRING, `attackStageCode` STRING, `attackType` STRING, `attackTypeName` STRING, `authSerNum` STRING, `authTime` STRING, `authType` STRING, `bankSeqNum` STRING, `batchNo` STRING, `blackDomain` STRING, `blackDomainDesc` STRING, `blackDomainTag` STRING, `blackDstIp` STRING, `blackFile` STRING, `blackFileDesc` STRING, `blackFileTag` STRING, `blackId` STRING, `blackIpTag` STRING, `blackSrcIp` STRING, `blackTag` STRING, `blackTagMatchCount` STRING, `blackTagMatchDesc` STRING, `blackUrl` STRING, `blackUrlDesc` STRING, `blackUrlTag` STRING, `blackVulnCve` STRING, `blackVulnDesc` STRING, `blackVulnName` STRING, `blackVulnTag` STRING, `branchId` STRING, `branchName` STRING, `businessSystemName` STRING, `businessType` STRING, `cardId` STRING, `cascadeSourceIp` STRING, `cascadeSourceName` STRING, `cebUid` STRING, `certNum` STRING, `certType` STRING, `chainId` STRING, `channel` STRING, `channelId` STRING, `character` STRING, `charge` STRING, `cifSeqNum` STRING, `clientInfo` STRING, `clientIp` STRING, `clientMac` STRING, `clientName` STRING, `clientPort` STRING, `collectTime` TIMESTAMP_LTZ(9), `collectTimeL` TIMESTAMP_LTZ(9), `command` STRING, `commandLine` STRING, `commandResult` STRING, `company` STRING, `companyCustomId` STRING, `companyId` STRING, `completenessTag` STRING, `confidence` STRING, `confidenceLevel` STRING, `consignedUser` STRING, `contractNo` STRING, `count` STRING, `couponAmount` STRING, `couponId` STRING, `createTime` TIMESTAMP_LTZ(3), `createTimeL` BIGINT, `createdBy` STRING, `curType` STRING, `currency` STRING, `currentBal` STRING, `customLabel1` STRING, `customLabel10` STRING, `customLabel2` STRING, `customLabel3` STRING, `customLabel4` STRING, `customLabel5` STRING, `customLabel6` STRING, `customLabel7` STRING, `customLabel8` STRING, `customLabel9` STRING, `customValue1` STRING, `customValue10` STRING, `customValue2` STRING, `customValue3` STRING, `customValue4` STRING, `customValue5` STRING, `customValue6` STRING, `customValue7` STRING, `customValue8` STRING, `customValue9` STRING, `dataQualityTag` STRING, `dataType` STRING, `dataTypeName` STRING, `dbInstance` STRING, `dbName` STRING, `dbTable` STRING, `dbVersion` STRING, `dealSuggest` STRING, `defVManagerId` STRING, `department` STRING, `deviceCategory` STRING, `deviceId` STRING, `deviceIp` STRING, `deviceMac` STRING, `deviceName` STRING, `deviceParentType` STRING, `deviceType` STRING, `deviceVersion` STRING, `direction` STRING, `directionDesc` STRING, `directionOfAttackTag` STRING, `domain` STRING, `dstAdminAccount` STRING, `dstAdminEmail` STRING, `dstAdminFOrgId` STRING, `dstAdminId` STRING, `dstAdminMobile` STRING, `dstAdminName` STRING, `dstAdminOrgId` STRING, `dstAdminOrgName` STRING, `dstAdminType` STRING, `dstAsset` STRING, `dstAssetId` STRING, `dstAssetInfo` STRING, `dstAssetKey` STRING, `dstAssetLevel` STRING, `dstAssetModel` STRING, `dstAssetName` STRING, `dstAssetPid` STRING, `dstAssetStatus` STRING, `dstAssetSubType` STRING, `dstAssetType` STRING, `dstAssetVendor` STRING, `dstBizId` STRING, `dstCity` STRING, `dstCompany` STRING, `dstCountry` STRING, `dstDbInstance` STRING, `dstDomainName` STRING, `dstFGroupId` STRING, `dstGroupId` STRING, `dstGroupName` STRING, `dstHostName` STRING, `dstIndustry` STRING, `dstIntelDesc` STRING, `dstIntelId` STRING, `dstIntelType` STRING, `dstInterface` STRING, `dstIp` STRING, `dstIpL` STRING, `dstLatitude` STRING, `dstLongitude` STRING, `dstMac` STRING, `dstManagerIp` STRING, `dstNatIp` STRING, `dstNatPort` STRING, `dstOperator` STRING, `dstOrgAdmin` STRING, `dstOrgId` STRING, `dstOrgName` STRING, `dstOsId` STRING, `dstPort` STRING, `dstPost` STRING, `dstProvince` STRING, `dstService` STRING, `dstSubDomainName` STRING, `dstUser` STRING, `dstZone` STRING, `duration` STRING, `empNum` STRING, `endTime` TIMESTAMP_LTZ(9), `engineName` STRING, `entryTime` TIMESTAMP_LTZ(9), `errorCode` STRING, `eventAppendix` STRING, `eventCount` STRING, `eventId` STRING, `eventIp` STRING, `eventName` STRING, `eventOneType` STRING, `eventOneTypeDesc` STRING, `eventOneTypeName` STRING, `eventParentType` STRING, `eventThreeType` STRING, `eventThreeTypeDesc` STRING, `eventThreeTypeName` STRING, `eventTwoType` STRING, `eventTwoTypeDesc` STRING, `eventTwoTypeName` STRING, `eventType` STRING, `fileHash` STRING, `fileName` STRING, `filePath` STRING, `fileSize` STRING, `fileType` STRING, `flag` STRING, `flow` STRING, `flowAvg` STRING, `flowDiscard` STRING, `flowDown` STRING, `flowMax` STRING, `flowNum` STRING, `flowUp` STRING, `groupId` STRING, `groupName` STRING, `id` STRING, `idCard` STRING, `indexTag` STRING, `infectionDstIp` STRING, `infectionDstName` STRING, `infectionFile` STRING, `infectionIp` STRING, `infectionSrcIp` STRING, `infectionSrcName` STRING, `installNum` STRING, `instance` STRING, `interestedIp` STRING, `intranetInternetTag` STRING, `ipType` STRING, `isBack` STRING, `jobTitle` STRING, `languageSign` STRING, `lastLoginTime` TIMESTAMP_LTZ(9), `lastUpdBy` STRING, `lastUpdTime` TIMESTAMP_LTZ(9), `latnId` STRING, `length` STRING, `location` STRING, `lockDesc` STRING, `lockTime` TIMESTAMP_LTZ(9), `logStatus` STRING, `logSubType` STRING, `logType` STRING, `loginTime` TIMESTAMP_LTZ(9), `loginType` STRING, `loginWay` STRING, `mailAdd` STRING, `mailIn` STRING, `mailOut` STRING, `mailRecipient` STRING, `mailSender` STRING, `mailSubject` STRING, `mailTotal` STRING, `mailType` STRING, `mainAccount` STRING, `mainAccountCreateTime` TIMESTAMP_LTZ(9), `mainAccountCreateUser` STRING, `mainAccountDesc` STRING, `mainAccountId` STRING, `mainAccountInvalidTime` TIMESTAMP_LTZ(9), `mainAccountLoginDateLast` STRING, `mainAccountLoginFailCount` STRING, `mainAccountModifyPwdTime` TIMESTAMP_LTZ(9), `mainAccountModifyTime` TIMESTAMP_LTZ(9), `mainAccountStatus` STRING, `mainAccountType` STRING, `mainAccountValidTime` TIMESTAMP_LTZ(9), `malwareName` STRING, `malwareSubType` STRING, `malwareType` STRING, `managerId` STRING, `managerIp` STRING, `managerTypeId` STRING, `menuDesc` STRING, `menuId` STRING, `menuName` STRING, `menuPid` STRING, `menuStatus` STRING, `menuType` STRING, `merchantId` STRING, `merchantName` STRING, `message` STRING, `method` STRING, `missingField` STRING, `model` STRING, `module` STRING, `moduleId` STRING, `name` STRING, `networkType` STRING, `newValue` STRING, `nextFlowNum` STRING, `object` STRING, `oldIdCard` STRING, `oldValue` STRING, `openid` STRING, `operType` STRING, `operTypeName` STRING, `ordSerNum` STRING, `order` STRING, `orderNo` STRING, `orderType` STRING, `orgCode` STRING, `orgId` STRING, `orgName` STRING, `orgNameLevel` STRING, `orgNamePath` STRING, `osId` STRING, `osName` STRING, `osVersion` STRING, `parentGroupId` STRING, `parentOrgId` STRING, `parentOrgName` STRING, `parentOrgNamePath` STRING, `passUpdateTime` STRING, `password` STRING, `payItemId` STRING, `payItemName` STRING, `payTime` TIMESTAMP_LTZ(9), `payUnitName` STRING, `payUnitType` STRING, `personId` STRING, `personName` STRING, `phone` STRING, `phoneImer` STRING, `phs` STRING, `policyId` STRING, `policyInfo` STRING, `policyName` STRING, `position` STRING, `priority` STRING, `profession` STRING, `professionName` STRING, `protocol` STRING, `provinceFromId` STRING, `provinceFromName` STRING, `rate` STRING, `rawMsg` STRING, `realFee` STRING, `reason` STRING, `receFee` STRING, `recvPacket` STRING, `recvSize` STRING, `refundOrderNo` STRING, `refundOrderTime` TIMESTAMP_LTZ(9), `registerNum` STRING, `registerRate` STRING, `rejCode` STRING, `relateAccount` STRING, `relateAccountId` STRING, `relateAccountName` STRING, `remark` STRING, `requestMessage` STRING, `requestNo` STRING, `requestTime` TIMESTAMP_LTZ(9), `responseCode` STRING, `responseIp` STRING, `responseMessage` STRING, `result` STRING, `resultCode` STRING, `resultDesc` STRING, `retain` STRING, `riskLevel` STRING, `riskLevelDesc` STRING, `roleId` STRING, `roleName` STRING, `ruleId` STRING, `ruleName` STRING, `ruleTjCount` STRING, `safetyMargin` STRING, `sceneId` STRING, `sceneOneType` STRING, `sceneThreeType` STRING, `sceneTwoType` STRING, `sendPacket` STRING, `sendSize` STRING, `serialNum` STRING, `serverIp` STRING, `serverName` STRING, `serverPort` STRING, `service` STRING, `serviceTime` TIMESTAMP_LTZ(9), `sessionCount` BIGINT, `sessionId` STRING, `settleMethod` STRING, `sex` STRING, `shareFlag` STRING, `sid` STRING, `signData` STRING, `snowId` STRING, `softwareInfo` STRING, `source` STRING, `srcAdminAccount` STRING, `srcAdminEmail` STRING, `srcAdminFOrgId` STRING, `srcAdminId` STRING, `srcAdminMobile` STRING, `srcAdminName` STRING, `srcAdminOrgId` STRING, `srcAdminOrgName` STRING, `srcAdminType` STRING, `srcAsset` STRING, `srcAssetId` STRING, `srcAssetInfo` STRING, `srcAssetKey` STRING, `srcAssetLevel` STRING, `srcAssetModel` STRING, `srcAssetName` STRING, `srcAssetPid` STRING, `srcAssetStatus` STRING, `srcAssetSubType` STRING, `srcAssetType` STRING, `srcAssetVendor` STRING, `srcBizId` STRING, `srcCity` STRING, `srcCompany` STRING, `srcContnent` STRING, `srcCountry` STRING, `srcDbInstance` STRING, `srcDomainName` STRING, `srcFGroupId` STRING, `srcGroupId` STRING, `srcGroupName` STRING, `srcHostName` STRING, `srcIndustry` STRING, `srcIntelDesc` STRING, `srcIntelId` STRING, `srcIntelType` STRING, `srcInterface` STRING, `srcIp` STRING, `srcIpL` STRING, `srcLatitude` STRING, `srcLongitude` STRING, `srcMac` STRING, `srcManagerIp` STRING, `srcNatIp` STRING, `srcNatPort` STRING, `srcOperator` STRING, `srcOrgAdmin` STRING, `srcOrgId` STRING, `srcOrgName` STRING, `srcOsId` STRING, `srcPort` STRING, `srcPost` STRING, `srcProvince` STRING, `srcService` STRING, `srcSubDomainName` STRING, `srcUser` STRING, `srcZone` STRING, `staffCode` STRING, `staffCrm` STRING, `staffName` STRING, `staffState` STRING, `startTime` TIMESTAMP_LTZ(9), `status` STRING, `subAccount` STRING, `subAccountCreateTime` TIMESTAMP_LTZ(9), `subAccountCreateUser` STRING, `subAccountDesc` STRING, `subAccountId` STRING, `subAccountInvalidTime` TIMESTAMP_LTZ(9), `subAccountLoginDateLast` STRING, `subAccountLoginFailCount` STRING, `subAccountModifyPwdTime` TIMESTAMP_LTZ(9), `subAccountModifyTime` TIMESTAMP_LTZ(9), `subAccountStatus` STRING, `subAccountType` STRING, `subAccountValidTime` TIMESTAMP_LTZ(9), `sumAreaId` STRING, `sumManagerId` STRING, `tag` STRING, `taskId` STRING, `taskName` STRING, `telephone` STRING, `telephoneType` STRING, `tenantId` STRING, `tenantName` STRING, `terminalNum` STRING, `threatName` STRING, `threatType` STRING, `threatTypeDesc` STRING, `transBal` STRING, `transChannel` STRING, `transCode` STRING, `transId` STRING, `transName` STRING, `transStatus` STRING, `transTime` TIMESTAMP_LTZ(9), `transType` STRING, `type` STRING, `unitName` STRING, `updateTime` TIMESTAMP_LTZ(9), `upmpQn` STRING, `upmpSerialNum` STRING, `url` STRING, `user` STRING, `userGroupId` STRING, `userGroupName` STRING, `userId` STRING, `userOrgId` STRING, `userOrgName` STRING, `userType` STRING, `uuId` STRING, `value` STRING, `version` STRING, `voidOrderNo` STRING, `vulnId` STRING, `vulnInfo` STRING, `vulnLevel` STRING, `vulnName` STRING, `vulnType` STRING, `weixinId` STRING, `weixinVersion` STRING, `wpTag` STRING, `writeOffTime` TIMESTAMP_LTZ(9)>*, rowtime=false, watermark=true) -> Calc(select=[eventTwoType, deviceParentType, type, eventName, directionDesc, srcIp, dstIp, createTime, snowId]) -> SinkConversionToTuple2 -> Sink: Print to Std. Out (1/1)#0 (851b2092ae4f274d5c7be1f2ea7acaba) switched from RUNNING to FAILED with failure cause: java.lang.NullPointerException
	at SinkConversion$22.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:71)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:50)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:28)
	at StreamExecCalc$18.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:71)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:50)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:28)
	at org.apache.flink.table.runtime.operators.source.InputConversionOperator.processElement(InputConversionOperator.java:128)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:71)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:50)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:28)
	at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:38)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:71)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:46)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:26)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:50)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:28)
	at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:50)
	at com.asap.demo.function.dealStreamProcessFunction.match(dealStreamProcessFunction.java:131)
	at com.asap.demo.function.dealStreamProcessFunction.processElement(dealStreamProcessFunction.java:115)
	at com.asap.demo.function.dealStreamProcessFunction.processElement(dealStreamProcessFunction.java:33)
	at org.apache.flink.streaming.api.operators.co.CoBroadcastWithKeyedOperator.processElement1(CoBroadcastWithKeyedOperator.java:125)
	at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory.processRecord1(StreamTwoInputProcessorFactory.java:213)
	at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory.lambda$create$0(StreamTwoInputProcessorFactory.java:178)
	at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory$StreamTaskNetworkOutput.emitRecord(StreamTwoInputProcessorFactory.java:291)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66)
	at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processInput(StreamTwoInputProcessor.java:96)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:423)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:204)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:681)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:636)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:647)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:620)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
	at java.lang.Thread.run(Thread.java:748)",,lincoln.86xy,TsReaper,wangbaohua,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Nov/21 03:19;wangbaohua;error.jpg;https://issues.apache.org/jira/secure/attachment/13036070/error.jpg",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Wed Dec 22 07:20:50 UTC 2021,,,,,,,,,,"0|z0wods:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/21 02:35;TsReaper;Hi [~wangbaohua], thanks for the feedback. This seems to be a SQL issue rather than a datastream issue. Could you share your SQL script and configuration files so that we can look deeper into this problem?;;;","15/Nov/21 03:09;wangbaohua;code:
https://gitee.com/wang_bh/flink/blob/master/flink-demo/src/main/java/com/asap/demo/rete/ReteDemo4.java;;;","15/Nov/21 06:34;TsReaper;[~wangbaohua] is it possible that {{kafkaData1}} produce a null {{BeanField}}? I mean the whole {{BeanField}} object is null, not some of its fields are null.;;;","15/Nov/21 07:19;TsReaper;I've reproduced this case. This is because SourceConversion does not check for null watermarks. I'm fixing this.

Reproduce test case:
{code:java}
public class MyTest {

    @Test
    public void myTest() throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        EnvironmentSettings blinkStreamSettings =
                EnvironmentSettings.newInstance().inStreamingMode().build();
        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, blinkStreamSettings);

        DataStream<MyObj> stream =
                env.fromElements(new MyObj(1, 100L), new MyObj(2, 200L), new MyObj(3, null))
                        .process(
                                new ProcessFunction<MyObj, MyObj>() {
                                    @Override
                                    public void processElement(
                                            MyObj value, Context ctx, Collector<MyObj> out)
                                            throws Exception {
                                        if (value.a % 2 == 0) {
                                            out.collect(null);
                                        } else {
                                            out.collect(value);
                                        }
                                    }
                                });
        Table table =
                tEnv.fromDataStream(
                        stream,
                        Schema.newBuilder()
                                .column(""a"", ""INT"")
                                .column(""b"", ""TIMESTAMP_LTZ(3)"")
                                .watermark(""b"", ""SOURCE_WATERMARK()"")
                                .build());
        tEnv.createTemporaryView(""T"", table);
        Table table2 = tEnv.sqlQuery(""SELECT * FROM T"");
        tEnv.toRetractStream(table2, Row.class).print(""query=="");
        env.execute();
    }

    public static class MyObj {
        public Integer a;
        public Instant b;

        public MyObj() {
            this.a = 0;
            this.b = Instant.ofEpochMilli(0);
        }

        public MyObj(Integer a, Long b) {
            this.a = a;
            this.b = b == null ? null : Instant.ofEpochMilli(b);
        }
    }
}
{code};;;","15/Nov/21 07:43;wangbaohua;Yes, the BeanField field is null, but why does the ProcessElement Interface parameter Collector throw an exception  ;;;","15/Nov/21 08:51;TsReaper;[~wangbaohua] This is because createTime might be null. Currently sink conversions does not handle the case when row time is a null value. This is however also invalid because row time must have a value. Currently you can make sure that {{kafkaData1}} does not  contain null row time to work around this problem.;;;","22/Dec/21 07:20;TsReaper;Hi [~wangbaohua] is this issue really fixed? Have you tested that? Which Flink version are you testing?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink flame graph webui bug,FLINK-24884,13411420,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,jackylau,jackylau,12/Nov/21 07:49,11/Jan/22 12:15,13/Jul/23 08:12,11/Jan/22 12:15,1.13.3,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,"i can not compile success when i port the flame graph feature to our low version of flink.

but it is success in the high version of flink 



 ",,airblader,jackylau,junhan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/21 07:48;jackylau;image-2021-11-12-15-48-08-140.png;https://issues.apache.org/jira/secure/attachment/13036021/image-2021-11-12-15-48-08-140.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 11 12:15:21 UTC 2022,,,,,,,,,,"0|z0woa8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Nov/21 08:04;airblader;Can you please elaborate on what exactly you are trying to do?;;;","12/Nov/21 09:07;jackylau;hi [~junhany] 

this is a bug, fix here

https://github.com/apache/flink/pull/17775/files;;;","12/Nov/21 09:13;jackylau;[~airblader] our flink version is base on flink1.5, and i cannot compile success when i port the flame graph feature to our version.;;;","12/Nov/21 09:53;junhan;[~jackylau] [~airblader] , I believe this `tip` type is indeed a bug that causes the compiling failure. The reason we did not reproduce it was probably due to different tsconfig settings.;;;","15/Nov/21 15:06;jackylau;hi [~airblader] [~junhany] , when will you merge it ?;;;","26/Nov/21 05:34;jackylau;hi [~trohrmann] , anyone will have time to review it. it has already some time past;;;","11/Jan/22 12:15;airblader;Fixed in master

commit c661cf9ec9427e4736ddc9158fefd297714b0983
[runtime-web] use correct type;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dropdown menu is not properly shown in UI,FLINK-24874,13411256,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mapohl,mapohl,mapohl,11/Nov/21 11:38,15/Dec/21 01:44,13/Jul/23 08:12,12/Nov/21 09:37,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,,,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,"FLINK-21867 introduced a new dropdown menu to browse through concurrently failed {{Executions}}. This feature is disabled due to ngzorro modules not being imported properly in {{release-1.14}}.

Additionally, the tooltip is not printed correctly.

These two issues are fixed on {{master}} already due to [903185d|https://github.com/apache/flink/commit/903185d72c97dd93c777eeb90cb81a7b1c7465e7]",,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21867,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 12 09:37:41 UTC 2021,,,,,,,,,,"0|z0wn9s:",9223372036854775807,1.14.0 introduced a new dropdown menu to browse through concurrently failed Executions in the exception history. This feature was accidentally disabled due to some ngzorro modules not being imported properly. This imports the missing modules to make the dropdown menu being rendered properly.,,,,,,,,,,,,,,,,,,,"12/Nov/21 09:37;chesnay;1.14: 8e8b2379006ee54527c8429b4ea948a1c79c1e9e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-core should be provided in flink-file-sink-common,FLINK-24869,13411158,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zzccctv,grossws,grossws,11/Nov/21 00:17,15/Nov/21 17:39,13/Jul/23 08:12,15/Nov/21 17:39,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Build System,,,,,0,pull-request-available,,,,As example {{flink-connector-files}} brings {{flink-core}} with {{compile}} scope via {{flink-file-sink-common}}.,,grossws,trohrmann,zzccctv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 15 17:39:15 UTC 2021,,,,,,,,,,"0|z0wmo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/21 09:09;trohrmann;I think you are right [~grossws]. Do you want to open a PR for this change?;;;","13/Nov/21 16:18;zzccctv;Hello [~trohrmann] I have open a PR this changge,Please check whether PR is correct;;;","15/Nov/21 17:39;trohrmann;Fixed via e40b4b7626556bf5026ccb2a318de653b2981e75;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AZP crashed in Post-job: Cache Maven local repo,FLINK-24866,13411101,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,trohrmann,trohrmann,10/Nov/21 15:48,29/Nov/21 11:44,13/Jul/23 08:12,29/Nov/21 11:44,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Build System / Azure Pipelines,,,,,0,,,,,"An AZP build failed while running the post-job: Cache Maven local repo step with exit code 2:

{code}
Resolved to: maven|Linux|kI+vc4kUoz33JEfRluJAo4vEVFz7aQdIKJJbq3fbuGw=
ApplicationInsightsTelemetrySender will correlate events with X-TFS-Session 2919f31f-021b-468b-851e-f92f99f5681f
Getting a pipeline cache artifact with one of the following fingerprints:
Fingerprint: `maven|Linux|kI+vc4kUoz33JEfRluJAo4vEVFz7aQdIKJJbq3fbuGw=`
There is a cache miss.
tar: f202add2a23c497f93e0ceff83df8823_archive.tar: Wrote only 6144 of 10240 bytes
tar: Error is not recoverable: exiting now
ApplicationInsightsTelemetrySender correlated 1 events with X-TFS-Session 2919f31f-021b-468b-851e-f92f99f5681f
##[error]Process returned non-zero exit code: 2
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26271&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=85df99a3-dd32-4a6c-8fa0-7c375f4cbc3a&l=212",,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24875,,,,,,FLINK-24801,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 29 11:44:19 UTC 2021,,,,,,,,,,"0|z0wmbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/21 16:39;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26286&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=5d217d74-fdda-432f-8795-2258ac7e95f0&l=220;;;","10/Nov/21 16:40;trohrmann;[~chesnay] could this have something to do with your latest changes?;;;","10/Nov/21 17:17;chesnay;No, this happened in the e2e profile which doesn't run on our CI machines.;;;","29/Nov/21 11:44;trohrmann;We assume that this issue has been fixed as side-effect of FLINK-24875.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The user-defined hive udaf/udtf cannot be used normally in hive dialect,FLINK-24862,13411061,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiangqiao,xiangqiao,xiangqiao,10/Nov/21 12:55,20/Jul/22 16:00,13/Jul/23 08:12,16/Dec/21 03:03,1.11.0,1.12.0,1.13.0,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.6,1.15.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"When hive udaf/udtf is used, a validate exception is thrown ，i added a unit test in HiveDialectITCase to reproduce this question:
{code:java}
@Test
public void testTemporaryFunctionUDAF() throws Exception {
    // create temp function
    tableEnv.executeSql(
            String.format(
                    ""create temporary function temp_count as '%s'"",
                    GenericUDAFCount.class.getName()));
    String[] functions = tableEnv.listUserDefinedFunctions();
    assertArrayEquals(new String[] {""temp_count""}, functions);
    // call the function
    tableEnv.executeSql(""create table src(x int)"");
    tableEnv.executeSql(""insert into src values (1),(-1)"").await();
    assertEquals(
            ""[+I[2]]"",
            queryResult(tableEnv.sqlQuery(""select temp_count(x) from src"")).toString());
    // switch DB and the temp function can still be used
    tableEnv.executeSql(""create database db1"");
    tableEnv.useDatabase(""db1"");
    assertEquals(
            ""[+I[2]]"",
            queryResult(tableEnv.sqlQuery(""select temp_count(x) from `default`.src""))
                    .toString());
    // drop the function
    tableEnv.executeSql(""drop temporary function temp_count"");
    functions = tableEnv.listUserDefinedFunctions();
    assertEquals(0, functions.length);
    tableEnv.executeSql(""drop temporary function if exists foo"");
} {code}
!image-2021-11-10-20-55-11-988.png|width=1363,height=282!",,hackergin,hehuiyuan,jark,jingzhang,libenchao,tartarus,xiangqiao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25142,,,,,,,,,,,,,,,,"10/Nov/21 12:55;xiangqiao;image-2021-11-10-20-55-11-988.png;https://issues.apache.org/jira/secure/attachment/13035916/image-2021-11-10-20-55-11-988.png","10/Nov/21 13:04;xiangqiao;image-2021-11-10-21-04-32-660.png;https://issues.apache.org/jira/secure/attachment/13035919/image-2021-11-10-21-04-32-660.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 20 16:00:15 UTC 2022,,,,,,,,,,"0|z0wm2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/21 13:37;jark;AFAIK, create temporary function for hive functions is not supported. You can try to remove the ""temporary"" keyword which should work. ;;;","11/Nov/21 02:55;xiangqiao;Thank you [~jark] ,remove the ""temporary"" keyword is to create a global function, which will be written to hive Metastore, which can not meet our needs.

I have solved this problem and can work normally now. Can you review it for me? I'm not sure if it will cause other problems. THX.

https://github.com/apache/flink/pull/17761;;;","16/Dec/21 02:56;jingzhang;Fixed in master: 95b8309213a5470b2e7e61771d6e90de75c71227
Fixed in release-1.14: b7e3eb948017e50b3d8f9f7ef3f94330bea71400
 ;;;","16/Mar/22 02:13;hehuiyuan;[~jingzhang]  [~jark] , are there any plans to merge 1.14 ? https://github.com/apache/flink/pull/19069;;;","20/Jul/22 16:00;jingzhang;[~hehuiyuan] Thanks for patch. Merged.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the wrong position mappings in the Python UDTF,FLINK-24860,13411043,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,10/Nov/21 11:33,15/Dec/21 01:41,13/Jul/23 08:12,11/Nov/21 02:07,1.12.5,1.13.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.6,,,,API / Python,,,,,0,pull-request-available,,,,"The failed example:
{code:python}
        @udtf(result_types=[DataTypes.STRING(), DataTypes.STRING()])
        def StoTraceMqSourcePlugUDTF(s: str):
            import json
            try:
                data = json.loads(s)
            except Exception as e:
                return None
            source_code = ""trace""
            try:
                shipment_no = data['shipMentNo']
            except Exception as e:
                return None
            yield source_code, shipment_no

        class StoTraceFindNameUDTF(TableFunction):
            def eval(self, shipment_no):
                yield shipment_no, shipment_no

        sto_trace_find_name = udtf(StoTraceFindNameUDTF(),
                                   result_types=[DataTypes.STRING(), DataTypes.STRING()])

        # self.env.set_parallelism(1)
        self.t_env.create_temporary_system_function(
            ""StoTraceMqSourcePlugUDTF"", StoTraceMqSourcePlugUDTF)
        self.t_env.create_temporary_system_function(
            ""sto_trace_find_name"", sto_trace_find_name
        )
        source_table = self.t_env.from_elements([(
            '{""shipMentNo"":""84210186879""}',)],
            ['biz_context'])
        # self.t_env.execute_sql(source_table)
        self.t_env.register_table(""source_table"", source_table)

        t = self.t_env.sql_query(
            ""SELECT biz_context, source_code, shipment_no FROM source_table LEFT JOIN LATERAL TABLE(StoTraceMqSourcePlugUDTF(biz_context)) as T(source_code, shipment_no)""
            "" ON TRUE"")
        self.t_env.register_table(""Table2"", t)
        t = self.t_env.sql_query(
            ""SELECT source_code, shipment_no, shipment_name, shipment_type FROM Table2 LEFT JOIN LATERAL TABLE(sto_trace_find_name(shipment_no)) as T(shipment_name, shipment_type)""
            "" ON TRUE""
        )
        print(t.to_pandas())
{code}
In the failed example, the input arguments of the second Python Table Function has the wrong positions mapping.
",,dianfu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 11 02:07:13 UTC 2021,,,,,,,,,,"0|z0wlyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/21 02:07;hxbks2ks;Merged into release-1.13 via 3ef637d1e3861d1de18e54317df91f56439f843c
Merged into release-1.12 via 7b2ae80c943fdb91c15d9f844ddab4a9a1aabe73;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TypeSerializer version mismatch during eagerly restore,FLINK-24858,13411030,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fpaul,fpaul,fpaul,10/Nov/21 10:11,17/Jan/22 15:29,13/Jul/23 08:12,24/Nov/21 14:14,1.13.3,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,API / Type Serialization System,,,,,0,pull-request-available,,,,"Currently, some of our TypeSerializer snapshots assume information about the binary layout of the actual data rather than only holding information about the TypeSerialzer.

Multiple users ran into this problem i.e.[https://lists.apache.org/thread/4q5q7wp0br96op6p7f695q2l8lz4wfzx|https://lists.apache.org/thread/4q5q7wp0br96op6p7f695q2l8lz4wfzx]
{quote}This manifest itself when state is restored egarly (for example an operator state) but, for example a user doesn't register the state on their intializeState/open,* and then a checkpoint happens.
The result is that we will have elements serialized according to an old binary layout, but our serializer snapshot declares a new version which indicates that the elements are written with a new binary layout.
The next restore will fail.
{quote}",,fpaul,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 24 14:12:21 UTC 2021,,,,,,,,,,"0|z0wlvk:",9223372036854775807,"This ticket resolves an issue that during state migration between Flink versions the wrong serializer might have been picked.

When upgrading from Flink 1.13.x please immediately choose 1.14.3 or higher and skip 1.14.0, 1.14.1, 1.14.2 because all are affected and it might prevent your job from starting.",,,,,,,,,,,,,,,,,,,"24/Nov/21 14:12;fpaul;Fixed:
 * master 601ef3b3bce040264daa3aedcb9d98ead8303485
 * release-1.14 564dee0752619ecb739b4bee1cacba856ea53bac;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AsyncWaitOperator fails during stop-with-savepoint,FLINK-24846,13410814,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,pnowojski,pnowojski,09/Nov/21 11:34,17/Dec/21 11:37,13/Jul/23 08:12,17/Dec/21 11:37,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Runtime / Checkpointing,Runtime / Task,,,,0,pull-request-available,,,,"{noformat}
Caused by: org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailbox$MailboxClosedException: Mailbox is in state QUIESCED, but is required to be in state OPEN for put operations.
        at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.checkPutStateConditions(TaskMailboxImpl.java:269) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.put(TaskMailboxImpl.java:197) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.execute(MailboxExecutorImpl.java:74) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.api.common.operators.MailboxExecutor.execute(MailboxExecutor.java:103) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.outputCompletedElement(AsyncWaitOperator.java:304) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.access$100(AsyncWaitOperator.java:78) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.processResults(AsyncWaitOperator.java:370) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.lambda$processInMailbox$0(AsyncWaitOperator.java:351) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.drain(MailboxProcessor.java:177) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:854) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:767) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at java.lang.Thread.run(Thread.java:829) ~[?:?]

{noformat}

As reported by a user on [the mailing list:|https://mail-archives.apache.org/mod_mbox/flink-user/202111.mbox/%3CCAO6dnLwtLNxkr9qXG202ysrnse18Wgvph4hqHZe3ar8cuXAfDw%40mail.gmail.com%3E]
{quote}
I failed to stop a job with savepoint with the following message:
Inconsistent execution state after stopping with savepoint. At least one execution is still in one of the following states: FAILED, CANCELED. A global fail-over is triggered to recover the job 452594f3ec5797f399e07f95c884a44b.

The job manager said
 A savepoint was created at hdfs://mobdata-flink-hdfs/driving-habits/svpts/savepoint-452594-f60305755d0e but the corresponding job 452594f3ec5797f399e07f95c884a44b didn't terminate successfully.
while complaining about
Mailbox is in state QUIESCED, but is required to be in state OPEN for put operations.

Is it okay to ignore this kind of error?

Please see the attached files for the detailed context.

FYI, 
- I used the latest 1.14.0
- I started the job with ""$FLINK_HOME""/bin/flink run --target yarn-per-job
- I couldn't reproduce the exception using the same jar so I might not able to provide DUBUG messages
{quote}",,csq,dwysakowicz,gaoyunhaii,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/21 11:32;pnowojski;log-jm.txt;https://issues.apache.org/jira/secure/attachment/13035874/log-jm.txt",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 15 12:59:00 UTC 2021,,,,,,,,,,"0|z0wkjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/21 16:02;gaoyunhaii;The direct reason for this problem is currently on stopping the StreamTask, the mailbox would be first preClose to forbid adding new mails, then it would drain all the pending mails. However, in this case for the AsyncWaitOperator to produce the last piece of the async task result, it seems to use a recursive way to output the pending completed records: namely for each mail it outputs one record, if there are new records, it would submit more mails, which conflict with the action of preClose before. 

Perhaps after we finish reverting the process of stop-with-savepoint to first endInput and emit all the completed elements, and then taking a savepoint and terminate, the issue would be solved? ;;;","15/Nov/21 08:58;dwysakowicz;[~gaoyunhaii] We won't call {{finish}} for {{stop-with-savepoint w/o drain}}. I believe it works fine for {{stop-with-savepoint w drain}} already.

I believe the rest of your investigation is correct.;;;","15/Nov/21 14:16;gaoyunhaii;Ah, indeed, sorry for the misleadings and very thanks [~dwysakowicz] for pointing out this. Then we would indeed need to fix the issue directly~;;;","15/Dec/21 12:59;pnowojski;merged commit 4065bfb + b54c413febc^ and b54c413febc into apache:master
merged as e7df5ec81fe and 8d5d7d46463 into release-1.14
merged commit 2bdc194 into apache:release-1.13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CsvFilesystemStreamSinkITCase.testPart times out on AZP,FLINK-24839,13410771,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,trohrmann,trohrmann,09/Nov/21 08:08,15/Dec/21 01:44,13/Jul/23 08:12,16/Nov/21 08:43,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,,,,,,0,pull-request-available,test-stability,,,"The test {{CsvFilesystemStreamSinkITCase.testPart}} times out on AZP.

{code}
2021-11-08T16:36:28.6542078Z Nov 08 16:36:28 org.junit.runners.model.TestTimedOutException: test timed out after 20 seconds
2021-11-08T16:36:28.6561998Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.putFields(ObjectOutputStream.java:463)
2021-11-08T16:36:28.6581789Z Nov 08 16:36:28 	at java.util.Locale.writeObject(Locale.java:2156)
2021-11-08T16:36:28.6601916Z Nov 08 16:36:28 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-11-08T16:36:28.6621871Z Nov 08 16:36:28 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-11-08T16:36:28.6632222Z Nov 08 16:36:28 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-11-08T16:36:28.6633082Z Nov 08 16:36:28 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-11-08T16:36:28.6633845Z Nov 08 16:36:28 	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1154)
2021-11-08T16:36:28.6634442Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
2021-11-08T16:36:28.6634968Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6637691Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6640766Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6641958Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6642763Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6643563Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6644365Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6645138Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6647747Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6648657Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6649439Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6650189Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6650958Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6651975Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6652632Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6653314Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6664918Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6665679Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6666409Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6667211Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6667907Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6668585Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6669301Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6669991Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6670706Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6671353Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6672227Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6672878Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6673381Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6673864Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6674366Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6674864Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6675348Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6675851Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6676340Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6676827Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6677321Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6677797Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6678290Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
2021-11-08T16:36:28.6678781Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
2021-11-08T16:36:28.6679262Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2021-11-08T16:36:28.6679764Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2021-11-08T16:36:28.6680236Z Nov 08 16:36:28 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
2021-11-08T16:36:28.6680733Z Nov 08 16:36:28 	at org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:632)
2021-11-08T16:36:28.6681669Z Nov 08 16:36:28 	at org.apache.flink.util.InstantiationUtil.writeObjectToConfig(InstantiationUtil.java:548)
2021-11-08T16:36:28.6682620Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamConfig.setStreamOperatorFactory(StreamConfig.java:308)
2021-11-08T16:36:28.6683633Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.setVertexConfig(StreamingJobGraphGenerator.java:713)
2021-11-08T16:36:28.6684729Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createChain(StreamingJobGraphGenerator.java:461)
2021-11-08T16:36:28.6685788Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createChain(StreamingJobGraphGenerator.java:411)
2021-11-08T16:36:28.6686964Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createChain(StreamingJobGraphGenerator.java:411)
2021-11-08T16:36:28.6688033Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.setChaining(StreamingJobGraphGenerator.java:377)
2021-11-08T16:36:28.6689024Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:178)
2021-11-08T16:36:28.6689973Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:116)
2021-11-08T16:36:28.6690944Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.graph.StreamGraph.getJobGraph(StreamGraph.java:960)
2021-11-08T16:36:28.6692027Z Nov 08 16:36:28 	at org.apache.flink.client.StreamGraphTranslator.translateToJobGraph(StreamGraphTranslator.java:50)
2021-11-08T16:36:28.6692998Z Nov 08 16:36:28 	at org.apache.flink.client.FlinkPipelineTranslationUtil.getJobGraph(FlinkPipelineTranslationUtil.java:39)
2021-11-08T16:36:28.6694130Z Nov 08 16:36:28 	at org.apache.flink.client.deployment.executors.PipelineExecutorUtils.getJobGraph(PipelineExecutorUtils.java:56)
2021-11-08T16:36:28.6695274Z Nov 08 16:36:28 	at org.apache.flink.test.util.MiniClusterPipelineExecutorServiceLoader$MiniClusterExecutor.execute(MiniClusterPipelineExecutorServiceLoader.java:137)
2021-11-08T16:36:28.6696466Z Nov 08 16:36:28 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2095)
2021-11-08T16:36:28.6697500Z Nov 08 16:36:28 	at org.apache.flink.table.planner.delegation.DefaultExecutor.executeAsync(DefaultExecutor.java:95)
2021-11-08T16:36:28.6698470Z Nov 08 16:36:28 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:772)
2021-11-08T16:36:28.6699494Z Nov 08 16:36:28 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:753)
2021-11-08T16:36:28.6700427Z Nov 08 16:36:28 	at org.apache.flink.table.api.internal.TableImpl.executeInsert(TableImpl.java:574)
2021-11-08T16:36:28.6701363Z Nov 08 16:36:28 	at org.apache.flink.table.api.internal.TableImpl.executeInsert(TableImpl.java:556)
2021-11-08T16:36:28.6702418Z Nov 08 16:36:28 	at org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.test(FsStreamingSinkITCaseBase.scala:118)
2021-11-08T16:36:28.6703498Z Nov 08 16:36:28 	at org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.testPart(FsStreamingSinkITCaseBase.scala:84)
2021-11-08T16:36:28.6704348Z Nov 08 16:36:28 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-11-08T16:36:28.6705109Z Nov 08 16:36:28 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-11-08T16:36:28.6705993Z Nov 08 16:36:28 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-11-08T16:36:28.6706775Z Nov 08 16:36:28 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-11-08T16:36:28.6707560Z Nov 08 16:36:28 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2021-11-08T16:36:28.6708439Z Nov 08 16:36:28 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-11-08T16:36:28.6709327Z Nov 08 16:36:28 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2021-11-08T16:36:28.6710196Z Nov 08 16:36:28 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-11-08T16:36:28.6711113Z Nov 08 16:36:28 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-11-08T16:36:28.6712067Z Nov 08 16:36:28 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-11-08T16:36:28.6712971Z Nov 08 16:36:28 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
2021-11-08T16:36:28.6714031Z Nov 08 16:36:28 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
2021-11-08T16:36:28.6714883Z Nov 08 16:36:28 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-11-08T16:36:28.6715538Z Nov 08 16:36:28 	at java.lang.Thread.run(Thread.java:748)
2021-11-08T16:36:28.6715983Z Nov 08 16:36:28 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26165&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=13306",,lzljs3620320,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24458,FLINK-24092,,,,,,,,FLINK-24458,FLINK-24092,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 16 08:43:08 UTC 2021,,,,,,,,,,"0|z0wka0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/21 08:09;trohrmann;cc [~arvid] there seems to be a class of filesystem tests that are susceptible to time outs on AZP. It might be worth looking into this in order to fix three instabilities in one go :-);;;","09/Nov/21 08:21;lzljs3620320;Thanks [~trohrmann] for reporting.
I take a look to these failures. There are various situations during timeout. These stacks should not be blocked. One possible reason is that the timeout time is too short.
For example: in https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23629&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=4cf71635-d33f-53ff-7185-c5abb11ae3a0&l=14505
There is only memory computation in {{org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.$anonfun$check$1(FsStreamingSinkITCaseBase.scala:137)}}, no blocker.;;;","16/Nov/21 08:43;lzljs3620320;Fixed via:

master: 44378fa5fde6c17e1712a62b834cb6251605f416

release-1.14: 7e5ada57afedc986cae9edda907694c93e8484b9

Feel free to re-open this if case still fails.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""group by"" in the interval join will throw a exception",FLINK-24835,13410741,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,jingzhang,xuyangzhong,xuyangzhong,09/Nov/21 03:22,15/Dec/21 01:44,13/Jul/23 08:12,19/Nov/21 03:03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Can reproduce this bug by the following code added into IntervalJoinTest.scala:
{code:java}
@Test
def testSemiIntervalJoinWithSimpleConditionAndGroup(): Unit = {
  val sql =
    """"""
      |SELECT t1.a FROM MyTable t1 WHERE t1.a IN (
      | SELECT t2.a FROM MyTable2 t2
      |   WHERE t1.b = t2.b AND t1.rowtime between t2.rowtime and t2.rowtime + INTERVAL '5' MINUTE
      |   GROUP BY t2.a
      |)
    """""".stripMargin
  util.verifyExecPlan(sql)
} {code}
The exception is :
{code:java}
java.lang.IllegalStateException
    at org.apache.flink.util.Preconditions.checkState(Preconditions.java:177)
    at org.apache.flink.table.planner.plan.rules.physical.stream.StreamPhysicalJoinRule.matches(StreamPhysicalJoinRule.scala:64)
    at org.apache.calcite.plan.volcano.VolcanoRuleCall.matchRecurse(VolcanoRuleCall.java:284)
    at org.apache.calcite.plan.volcano.VolcanoRuleCall.matchRecurse(VolcanoRuleCall.java:411)
    at org.apache.calcite.plan.volcano.VolcanoRuleCall.matchRecurse(VolcanoRuleCall.java:411)
    at org.apache.calcite.plan.volcano.VolcanoRuleCall.match(VolcanoRuleCall.java:268)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.fireRules(VolcanoPlanner.java:985)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1245)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:589)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:604)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:84)
    at org.apache.calcite.rel.AbstractRelNode.onRegister(AbstractRelNode.java:268)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1132)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:589)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:604)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:84)
    at org.apache.calcite.rel.AbstractRelNode.onRegister(AbstractRelNode.java:268)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1132)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:589)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:604)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:84)
    at org.apache.calcite.rel.AbstractRelNode.onRegister(AbstractRelNode.java:268)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1132)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:589)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:604)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:84)
    at org.apache.calcite.rel.AbstractRelNode.onRegister(AbstractRelNode.java:268)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1132)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:589)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:604)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.changeTraits(VolcanoPlanner.java:486)
    at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:309)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:69)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:64)
    at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
    at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
    at scala.collection.Iterator.foreach(Iterator.scala:937)
    at scala.collection.Iterator.foreach$(Iterator.scala:937)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
    at scala.collection.IterableLike.foreach(IterableLike.scala:70)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
    at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:60)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:165)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:83)
    at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
    at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:309)
    at org.apache.flink.table.planner.utils.TableTestUtilBase.assertPlanEquals(TableTestBase.scala:894)
    at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:790)
    at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyExecPlan(TableTestBase.scala:591)
    at org.apache.flink.table.planner.plan.stream.sql.join.IntervalJoinTest.testSemiIntervalJoinWithSimpleConditionAndGroup(IntervalJoinTest.scala:76) {code}
It is caused by that the agg casts the time indicator type to the common timestamp.",,godfreyhe,jingzhang,libenchao,xuyangzhong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 19 03:03:42 UTC 2021,,,,,,,,,,"0|z0wk3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/21 04:07;jingzhang;[~xuyangzhong] Thanks for reporting this problem.

The case is interesting, the sql looks like a interval join query at first glance, but it is a regular join because right side is a unbounded aggregate which has no time attribute field.

In theory, the query would translated into regular join instead of interval join because the time attribute of right side would be materialized because it is an unbounded aggregate.

For this case, both left side and right side of join  should be materialized. But there is a bug in `RelTimeIndicatorConverter#visitJoin`. The bug would mistaken regard the join as interval join, so it skip materialized time attribute of left side which leads an exception in `StreamPhysicalJoinRule`.

I would fix this bug soon, thanks again for reporting this bug.;;;","19/Nov/21 03:03;godfreyhe;Fixed in 
  1.15.0: 8aa74d5ad7026734bdd98eabbc9cbbb243bbe8b0
  1.14.1: d3df986a75e34e1ed475b2e1236b7770698e7bd1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveScheduler#getJobStatus never returns CREATED,FLINK-24824,13410615,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,08/Nov/21 13:10,17/Jan/22 10:12,13/Jul/23 08:12,17/Jan/22 10:12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"None of the AdaptiveScheduler states return {{JobStatus#CREATED}} when {{requestJobStatus()}} is called. This violates the job state machine, and makes it a bit difficult to setup job state timestamps in the {{AdaptiveScheduler}} (FLINK-24775).

We are in the {{INITIALIZING}} state from {{Created}} -> {{WaitingForResources}} -> {{CreatingExecutionGraph}}, and then switch straight to RUNNING in {{Executing}}.

It is tricky to retain the same semantics for the {{Default}} and {{AdaptiveScheduler}}, but I think it would be fine to return {{CREATED}} once we reached {{WaitingForResources}} because from the users-perspective it behaves similarly.",,dmvk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21513,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 17 10:12:58 UTC 2022,,,,,,,,,,"0|z0wjbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/21 13:10;chesnay;[~trohrmann] WDYT?;;;","06/Dec/21 09:56;dmvk;{code}
    /**
     * The job has been received by the Dispatcher, and is waiting for the job manager to receive
     * leadership and to be created.
     */
    INITIALIZING(TerminalState.NON_TERMINAL),

    /** Job is newly created, no task has started to run. */
    CREATED(TerminalState.NON_TERMINAL),
{code}

[~chesnay] Makes sense to me. We should stay as close to the original contract as possible. These javadocs imply, that we should switch to CREATED state right after we have an active JobMaster for the job.
;;;","17/Jan/22 10:12;chesnay;master: 57e40105ae4c7890016109d588d2682289ecea67;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yarn-session report metrics error,FLINK-24823,13410604,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,sgvmb0,sgvmb0,08/Nov/21 11:58,09/Nov/21 02:58,13/Jul/23 08:12,09/Nov/21 02:58,1.11.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Metrics,,,,,0,,,,,"I run 2 streaming jobs in yarn-session。

I report Metrics to graphite_exporter and something was wrong。
{code:java}
* collected metric ""flink_jobmanager_fullRestarts"" { label:<name:""host"" value:""xx-xx-xxx-xxx"" > label:<name:""job_name"" value:""flow-session"" > gauge:<value:0 > } was c
ollected before with the same name and label values{code}
I think some default metric was conflict.

I use this to know how many jobs is running. This error make graphite crash.

Is this a bug?

yarn-session start shell

 
{code:java}
exec yarn-session.sh -nm flow-session  -D metrics.reporter.grph.prefix=""flink.flow-session"" -D env.java.opts=""-Djob_name=flow-session"" -d {code}
flink-conf
{code:java}
metrics.reporter.grph.class: org.apache.flink.metrics.graphite.GraphiteReporter
metrics.reporter.grph.host: xx-xx-xxx-xxx
metrics.reporter.grph.port: 9109
metrics.reporter.grph.protocol: TCP{code}
 ","Flink 1.11

 ",sgvmb0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,2021-11-08 11:58:27.0,,,,,,,,,,"0|z0wj94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkKafkaProducerMigrationOperatorTest.testRestoreProducer fails on AZP,FLINK-24821,13410584,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,arvid,trohrmann,trohrmann,08/Nov/21 10:18,14/Feb/22 08:15,13/Jul/23 08:12,14/Feb/22 08:15,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Connectors / Kafka,,,,,0,test-stability,,,,"The test {{FlinkKafkaProducerMigrationOperatorTest.testRestoreProducer}} fails on AZP with

{code}
Nov 07 23:02:14 [ERROR] testRestoreProducer[Migration Savepoint: 1.8]  Time elapsed: 2.008 s  <<< ERROR!
Nov 07 23:02:14 java.net.BindException: Address already in use
Nov 07 23:02:14 	at sun.nio.ch.Net.bind0(Native Method)
Nov 07 23:02:14 	at sun.nio.ch.Net.bind(Net.java:461)
Nov 07 23:02:14 	at sun.nio.ch.Net.bind(Net.java:453)
Nov 07 23:02:14 	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:222)
Nov 07 23:02:14 	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:85)
Nov 07 23:02:14 	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:78)
Nov 07 23:02:14 	at org.apache.zookeeper.server.NIOServerCnxnFactory.configure(NIOServerCnxnFactory.java:90)
Nov 07 23:02:14 	at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:120)
Nov 07 23:02:14 	at org.apache.curator.test.TestingZooKeeperMain.runFromConfig(TestingZooKeeperMain.java:93)
Nov 07 23:02:14 	at org.apache.curator.test.TestingZooKeeperServer$1.run(TestingZooKeeperServer.java:148)
Nov 07 23:02:14 	at java.lang.Thread.run(Thread.java:748)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26096&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=e424005a-b16e-540f-196d-da062cc19bdf&l=7302

It looks that there is a race condition for avaiable ports.",,gaoyunhaii,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 14 08:14:51 UTC 2022,,,,,,,,,,"0|z0wj4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Nov/21 15:15;arvid;This seems to be caused by the use of [NetUtils#getAvailablePort|https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/util/NetUtils.java#L161-L161]. 

{noformat}
    public static int getAvailablePort() {
        for (int i = 0; i < 50; i++) {
            try (ServerSocket serverSocket = new ServerSocket(0)) {
                int port = serverSocket.getLocalPort();
                if (port != 0) {
                    return port;
                }
            } catch (IOException ignored) {
            }
        }

        throw new RuntimeException(""Could not find a free permitted port on the machine."");
    }
{noformat}

There are a couple of places that use it, in particular all legacy Kafka tests, and it seems to be a larger effort to replace it. I see 3 options:
- Leave as is and live with sporadic issues. This test has been in the codebase since ages and it didn't pop up so far(?).
- Repeat the test X times in case of failures. It would be nice if we could only repeat the test on BindException.
- Remove legacy Kafka tests (note that quite a few Netty and Task threads also use the same flawed way to acquire a port).;;;","06/Dec/21 17:44;arvid;https://github.com/apache/flink/pull/16108 is currently addressing the issue by introducing file locks.;;;","14/Feb/22 08:14;gaoyunhaii;I'll first close this issue since https://github.com/apache/flink/pull/16108 is merged and the test has not been reproduced for a long time. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Examples in documentation for value1 IS DISTINCT FROM value2 are wrong,FLINK-24820,13410580,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,08/Nov/21 09:48,06/May/22 11:37,13/Jul/23 08:12,06/May/22 11:37,1.13.3,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.16.0,,,,,Documentation,Table SQL / API,,,,0,auto-deprioritized-major,pull-request-available,,,"Currently it is stated in docs for {{value1 IS DISTINCT FROM value2}}
{quote}
E.g., 1 IS NOT DISTINCT FROM NULL returns TRUE; NULL IS NOT DISTINCT FROM NULL returns FALSE.
{quote}
In fact they return opposite values.",,martijnvisser,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 06 11:37:18 UTC 2022,,,,,,,,,,"0|z0wj3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/22 10:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","12/Feb/22 10:37;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","06/May/22 11:37;martijnvisser;Fixed in master: 3f157bbaa29fc6c71ffe85a31ed0ef19e5fe1e6a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
compile_cron_python_wheels should use Ubuntu 20,FLINK-24812,13410479,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,gaoyunhaii,gaoyunhaii,07/Nov/21 15:49,15/Dec/21 01:40,13/Jul/23 08:12,09/Nov/21 09:36,1.12.5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,,,,,Build System / Azure Pipelines,,,,,0,pull-request-available,test-stability,,,"{code:java}
##[warning]An image label with the label ubuntu-16.04 does not exist.
,##[error]The remote provider was unable to process the request.
Started: Today at 上午5:00
Duration: 18h 44m 12s {code}
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26084&view=logs&j=a29bcfe1-064d-50b9-354f-07802213a3c0]",,gaoyunhaii,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 09 09:36:26 UTC 2021,,,,,,,,,,"0|z0wihk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/21 08:35;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26189&view=logs&j=a29bcfe1-064d-50b9-354f-07802213a3c0;;;","09/Nov/21 09:36;chesnay;1.12: ccf6f1a256e7c5c4647d65bb744a73b1fc280faa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check & possible fix decimal precision and scale for all Aggregate functions,FLINK-24809,13410379,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,matriv,matriv,matriv,06/Nov/21 10:15,29/Apr/22 09:52,13/Jul/23 08:12,30/Dec/21 08:16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Similar to FLINK-24691, check the behaviour of the rest of Aggregate functions like *Sum0AggFunction, AvgAggFunction,* etc. regarding the precision/scale of the resulting decimal.",,icshuo,libenchao,matriv,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24691,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 30 08:16:03 UTC 2021,,,,,,,,,,"0|z0whzs:",9223372036854775807,This changes the result of a decimal SUM with retraction and AVG() between 1.14.0 and 1.14.1. It restores part of behavior back to 1.13 to be consistent with Hive/Spark.,,,,,,,,,,,,,,,,,,,"08/Nov/21 03:55;icshuo;`SumWithRetractAggFunction` as well.;;;","30/Dec/21 08:16;twalthr;Fixed in master: ec893d200f12197a03cc85fa7bf87581a29c6149;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RMQSourceITCase.testStopWithSavepoint and testAckFailure failed on azure due to connection error,FLINK-24806,13410354,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,cmick,gaoyunhaii,gaoyunhaii,06/Nov/21 03:55,15/Dec/21 01:44,13/Jul/23 08:12,08/Nov/21 10:47,1.13.3,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Connectors/ RabbitMQ,,,,,0,pull-request-available,test-stability,,,"{code:java}
Nov 05 22:12:57 [INFO] Running org.apache.flink.streaming.connectors.rabbitmq.RMQSourceITCase
Nov 05 22:13:18 [ERROR] Tests run: 3, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 20.921 s <<< FAILURE! - in org.apache.flink.streaming.connectors.rabbitmq.RMQSourceITCase
Nov 05 22:13:18 [ERROR] testStopWithSavepoint  Time elapsed: 3.39 s  <<< ERROR!
Nov 05 22:13:18 java.io.IOException
Nov 05 22:13:18 	at com.rabbitmq.client.impl.AMQChannel.wrap(AMQChannel.java:129)
Nov 05 22:13:18 	at com.rabbitmq.client.impl.AMQChannel.wrap(AMQChannel.java:125)
Nov 05 22:13:18 	at com.rabbitmq.client.impl.AMQConnection.start(AMQConnection.java:396)
Nov 05 22:13:18 	at com.rabbitmq.client.impl.recovery.RecoveryAwareAMQConnectionFactory.newConnection(RecoveryAwareAMQConnectionFactory.java:64)
Nov 05 22:13:18 	at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.init(AutorecoveringConnection.java:156)
Nov 05 22:13:18 	at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:1130)
Nov 05 22:13:18 	at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:1087)
Nov 05 22:13:18 	at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:1045)
Nov 05 22:13:18 	at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:1207)
Nov 05 22:13:18 	at org.apache.flink.streaming.connectors.rabbitmq.RMQSourceITCase.getRMQConnection(RMQSourceITCase.java:203)
Nov 05 22:13:18 	at org.apache.flink.streaming.connectors.rabbitmq.RMQSourceITCase.setUp(RMQSourceITCase.java:98)
Nov 05 22:13:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Nov 05 22:13:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Nov 05 22:13:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Nov 05 22:13:18 	at java.lang.reflect.Method.invoke(Method.java:498)
Nov 05 22:13:18 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Nov 05 22:13:18 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Nov 05 22:13:18 
Nov 05 22:13:18 [ERROR] testAckFailure  Time elapsed: 1.842 s  <<< ERROR!
Nov 05 22:13:18 java.io.IOException
Nov 05 22:13:18 	at com.rabbitmq.client.impl.AMQChannel.wrap(AMQChannel.java:129)
Nov 05 22:13:18 	at com.rabbitmq.client.impl.AMQChannel.wrap(AMQChannel.java:125)
Nov 05 22:13:18 	at com.rabbitmq.client.impl.AMQConnection.start(AMQConnection.java:396)
Nov 05 22:13:18 	at com.rabbitmq.client.impl.recovery.RecoveryAwareAMQConnectionFactory.newConnection(RecoveryAwareAMQConnectionFactory.java:64)
Nov 05 22:13:18 	at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.init(AutorecoveringConnection.java:156)
Nov 05 22:13:18 	at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:1130)
Nov 05 22:13:18 	at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:1087)
Nov 05 22:13:18 	at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:1045)
Nov 05 22:13:18 	at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:1207)
Nov 05 22:13:18 	at org.apache.flink.streaming.connectors.rabbitmq.RMQSourceITCase.getRMQConnection(RMQSourceITCase.java:203)
Nov 05 22:13:18 	at org.apache.flink.streaming.connectors.rabbitmq.RMQSourceITCase.setUp(RMQSourceITCase.java:98)
Nov 05 22:13:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Nov 05 22:13:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Nov 05 22:13:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)


{code}
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26065&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=13478]",,cmick,gaoyunhaii,roman,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 08 10:47:00 UTC 2021,,,,,,,,,,"0|z0whu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/21 04:01;gaoyunhaii;Also cc [~fpaul] [~cmick] for I saw you ever working on RabbitMQ related issues, is this problem similar to the previous timeout issues~?;;;","06/Nov/21 09:59;roman;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26072&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=13479;;;","07/Nov/21 15:39;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26083&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=f53023d8-92c3-5d78-ec7e-70c2bf37be20&l=13663];;;","07/Nov/21 15:40;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26083&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=13362];;;","07/Nov/21 15:40;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26083&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=41463ccd-0694-5d4d-220d-8f771e7d098b&l=13478;;;","08/Nov/21 01:48;cmick;Hi [~gaoyunhaii] , the issue reported here is caused by the RabbitMQ service not started before trying to connect to it (so I'm not sure if this can be related to timeout issues).

In the current code, we wait for the RMQ socket to be opened during the container startup (here: [https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-rabbitmq/src/test/java/org/apache/flink/streaming/connectors/rabbitmq/RMQSourceITCase.java#L94] ). It seems, it is possible that the port is opened, but the RabbitMQ service has not started yet.

To resolve it I would recommend to just remove this line.. If removed, it will use the default RabbitMQContainer wait strategy, which is to wait for a specific log message (""Server startup complete"" to be specific). 

What do you think? Can I prepare a PR for this?;;;","08/Nov/21 07:52;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26093&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=13479;;;","08/Nov/21 07:53;trohrmann;[~cmick] sure. A fix will be highly appreciated by the community. I'll assign the ticket to you.;;;","08/Nov/21 10:47;trohrmann;Fixed via

1.15.0: dd3cf70ac8d167ee79c302c0c5f9183d58b6b0b4
1.14.1: b8810cc68969c2dc8dd51158d222650e76a3b56b
1.13.4: 8707347fb7ab300580791bb58e4d188f20db607a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BufferTimeoutITCase.testDisablingBufferTimeout failed on Azure,FLINK-24800,13410262,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,akalashnikov,gaoyunhaii,gaoyunhaii,05/Nov/21 14:00,15/Dec/21 01:44,13/Jul/23 08:12,11/Nov/21 13:40,1.13.3,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Runtime / Network,,,,,0,pull-request-available,test-stability,,,"{code:java}
2021-11-05T12:18:50.5272055Z Nov 05 12:18:50 [INFO] Results:
2021-11-05T12:18:50.5273369Z Nov 05 12:18:50 [INFO] 
2021-11-05T12:18:50.5274011Z Nov 05 12:18:50 [ERROR] Failures: 
2021-11-05T12:18:50.5274518Z Nov 05 12:18:50 [ERROR]   BufferTimeoutITCase.testDisablingBufferTimeout:85 
2021-11-05T12:18:50.5274871Z Nov 05 12:18:50 Expected: <0>
2021-11-05T12:18:50.5275150Z Nov 05 12:18:50      but: was <1>
2021-11-05T12:18:50.5276136Z Nov 05 12:18:50 [INFO] 
2021-11-05T12:18:50.5276667Z Nov 05 12:18:50 [ERROR] Tests run: 1849, Failures: 1, Errors: 0, Skipped: 58
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26018&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10850",,akalashnikov,dmvk,dwysakowicz,fpaul,gaoyunhaii,pnowojski,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 18 06:56:17 UTC 2021,,,,,,,,,,"0|z0wh9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/21 10:07;chesnay;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26021&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","08/Nov/21 10:20;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26104&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5116;;;","08/Nov/21 10:20;trohrmann;cc [~pnowojski];;;","09/Nov/21 08:01;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26136&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=4736;;;","09/Nov/21 08:02;trohrmann;Can we disable this test and create a blocker for fixing it [~pnowojski], [~dwysakowicz]?;;;","09/Nov/21 08:05;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26158&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10759;;;","09/Nov/21 08:22;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26181&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5061;;;","09/Nov/21 10:16;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26142&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5119;;;","10/Nov/21 07:39;fpaul;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26241&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba
;;;","10/Nov/21 08:11;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26239&view=logs&j=d8d26c26-7ec2-5ed2-772e-7a1a1eb8317c&t=ec8797b0-5eee-5a0e-f936-8db65cff44cc&l=11094;;;","10/Nov/21 13:23;dwysakowicz;Temporarily disabled the test in 133943fc1fd6870c056515bf09d20f458fb1b447;;;","10/Nov/21 15:45;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26271&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5063;;;","10/Nov/21 17:31;akalashnikov;I have fixed the test but the problem is not the test but the race condition in `{*}PipelinedSubpartition#add{*}`:
 # Suppose we add the event to the empty queue(event is always the finished buffer)
 # `{*}notifyDataAvailable{*}` is set to true because we have only one buffer in the queue which is finished
 # we call the `{*}notifyDataAvailable{*}` method
 # we add one more unfinished buffer which will be the second in the queue
 # `notifyDataAvailable` is set to true because it is the second buffer(in fact, we should set this flag to false because we already notified about the first buffer but unfortunately we don't have such information so we want to notify again, just in case)
 # reader thread polls(`{*}PipelinedSubpartition#pollBuffer{*}`) the first buffer from the queue(finished one).
 # only now, we call the `{*}notifyDataAvailable{*}` method but in fact, we have only one unfinished buffer in the queue so this notification doesn't make sense.
 # reader thread polls the unfinished buffer from the queue.

So as you can see it is possible to notify about one buffer twice which can lead to the situation when the unfinished buffer will be read. This can happen because the calculation of `notifyDataAvailable` happens inside the `synchronized` block while the calling `notifyDataAvailable` happens outside of this block which leads to a race condition.

at least for now, I personally don't think that it is a big problem and I don't think that it makes sense to complicate our implementation of data availability notification in order to fix this problem. It is why I just have fixed the test. But I am still thinking about that and maybe I will change my mind.

[~pnowojski] , Anyway, I am not really confident in my conclusion so let's discuss it especially if you think that it is a more serious problem than I think.;;;","11/Nov/21 13:01;pnowojski;I agree with your assessment [~akalashnikov]. Let's keep the existing production code behaviour and let's just try to fix this in the tests.;;;","11/Nov/21 13:40;dwysakowicz;Fixed and re-enabled the test in 
* master
** 23e8c8ce62e50d478cb55bee2288a4a35af83d4f
* 1.14.1
** 39866ce3418974c9f9b746a3fb8b8e5c30a5f5db;;;","17/Nov/21 09:03;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26626&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=2c7d57b9-7341-5a87-c9af-2cf7cc1a37dc&l=4712]

 

Hi [~akalashnikov] , Might we pick the fix also to 1.13.x ~?;;;","17/Nov/21 09:32;akalashnikov;[~gaoyunhaii] , yes, sure, I have prepared the PR - https://github.com/apache/flink/pull/17818.;;;","18/Nov/21 06:56;gaoyunhaii;Very thanks [~akalashnikov] for backporting the fix! I'll try having a look~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaDynamicTableFactoryTest.testTableSourceWithKeyValueAndMetadata fails on Azure,FLINK-24799,13410260,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,twalthr,gaoyunhaii,gaoyunhaii,05/Nov/21 13:56,05/Nov/21 17:15,13/Jul/23 08:12,05/Nov/21 17:15,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,0,test-stability,,,,"{code:java}
2021-11-05T11:17:50.6139382Z Nov 05 11:17:50 [INFO] Results:
2021-11-05T11:17:50.6139960Z Nov 05 11:17:50 [INFO] 
2021-11-05T11:17:50.6140979Z Nov 05 11:17:50 [ERROR] Failures: 
2021-11-05T11:17:50.6143015Z Nov 05 11:17:50 [ERROR]   KafkaDynamicTableFactoryTest.testTableSourceWithKeyValueAndMetadata:355 expected:<org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSource@655d9585> but was:<org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSource@1d030551>
2021-11-05T11:17:50.6145725Z Nov 05 11:17:50 [INFO] 
2021-11-05T11:17:50.6146333Z Nov 05 11:17:50 [ERROR] Tests run: 329, Failures: 1, Errors: 0, Skipped: 42
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26018&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=6698",,gaoyunhaii,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 05 17:15:04 UTC 2021,,,,,,,,,,"0|z0wh9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/21 14:07;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26019&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7043];;;","05/Nov/21 17:15;twalthr;Fixed in master: 2a582f3392f7f38e7c318fb76017a21fe338be2a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefaultSchedulerLocalRecoveryITCase fails on AZP,FLINK-24793,13410225,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zhuzh,trohrmann,trohrmann,05/Nov/21 10:42,08/Nov/21 14:51,13/Jul/23 08:12,08/Nov/21 14:51,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"{{DefaultSchedulerLocalRecoveryITCase.testLocalRecoveryFull}} and {{DefaultSchedulerLocalRecoveryITCase.testLocalRecoveryRegion}} fails on AZP with:

{code}
Nov 04 23:01:32 java.lang.IllegalArgumentException: attempt does not exist
Nov 04 23:01:32 	at org.apache.flink.runtime.executiongraph.ArchivedExecutionVertex.getPriorExecutionAttempt(ArchivedExecutionVertex.java:109)
Nov 04 23:01:32 	at org.apache.flink.test.runtime.DefaultSchedulerLocalRecoveryITCase.assertNonLocalRecoveredTasksEquals(DefaultSchedulerLocalRecoveryITCase.java:92)
Nov 04 23:01:32 	at org.apache.flink.test.runtime.DefaultSchedulerLocalRecoveryITCase.testLocalRecoveryInternal(DefaultSchedulerLocalRecoveryITCase.java:80)
Nov 04 23:01:32 	at org.apache.flink.test.runtime.DefaultSchedulerLocalRecoveryITCase.testLocalRecoveryFull(DefaultSchedulerLocalRecoveryITCase.java:65)
Nov 04 23:01:32 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Nov 04 23:01:32 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Nov 04 23:01:32 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Nov 04 23:01:32 	at java.lang.reflect.Method.invoke(Method.java:498)
Nov 04 23:01:32 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Nov 04 23:01:32 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Nov 04 23:01:32 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Nov 04 23:01:32 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Nov 04 23:01:32 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Nov 04 23:01:32 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Nov 04 23:01:32 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Nov 04 23:01:32 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Nov 04 23:01:32 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Nov 04 23:01:32 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Nov 04 23:01:32 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Nov 04 23:01:32 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Nov 04 23:01:32 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Nov 04 23:01:32 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Nov 04 23:01:32 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Nov 04 23:01:32 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Nov 04 23:01:32 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Nov 04 23:01:32 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Nov 04 23:01:32 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Nov 04 23:01:32 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Nov 04 23:01:32 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
Nov 04 23:01:32 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Nov 04 23:01:32 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Nov 04 23:01:32 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Nov 04 23:01:32 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
Nov 04 23:01:32 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Nov 04 23:01:32 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Nov 04 23:01:32 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Nov 04 23:01:32 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Nov 04 23:01:32 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Nov 04 23:01:32 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
{code} 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25983&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=4451",,gaoyunhaii,trohrmann,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19142,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 08 14:51:19 UTC 2021,,,,,,,,,,"0|z0wh1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/21 10:53;trohrmann;cc [~zhuzh]. I think we introduced a test instability with FLINK-19142.;;;","06/Nov/21 11:09;zhuzh;Thanks for the notification! [~trohrmann]
I will look into it.;;;","07/Nov/21 15:42;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26083&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=4398];;;","08/Nov/21 07:18;zhuzh;This IT case fails when it is testing using AdaptiveScheduler. The cause of the exception is that execution history is missing in AdaptiveScheduler. When a restarting happens, new execution graph will be re-generated, the attempt number will be retained, while the prior executions are not inherited from the previous execution graph.
Even if the problem above is solved, the case will still fail because local recovery is not supported by AdaptiveScheduler yet (see FLINK-21450). 
So for now I will annotate the tests with {{FailsWithAdaptiveScheduler}} so that they can be skipped when testing AdaptiveScheduler.
;;;","08/Nov/21 07:58;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26093&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=4842;;;","08/Nov/21 14:51;zhuzh;Fixed via 130db3830a9dc9173b2afd8f22e577c9711812bc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IllegalStateException with CheckpointCleaner being closed already,FLINK-24789,13410203,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,mapohl,mapohl,05/Nov/21 08:53,15/Dec/21 01:44,13/Jul/23 08:12,26/Nov/21 17:10,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Runtime / Checkpointing,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,"* We experienced a failure of {{OperatorCoordinatorSchedulerTest}} in our VVP Fork of Flink. The {{finegrained_resource_management}} test run failed with an non-0 exit code:
{code}
Nov 01 17:19:12 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.2:test (default-test) on project flink-runtime: There are test failures.
Nov 01 17:19:12 [ERROR] 
Nov 01 17:19:12 [ERROR] Please refer to /__w/1/s/flink-runtime/target/surefire-reports for the individual test results.
Nov 01 17:19:12 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
Nov 01 17:19:12 [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Nov 01 17:19:12 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-runtime && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /__w/1/s/flink-runtime/target/surefire/surefirebooter6007815607334336440.jar /__w/1/s/flink-runtime/target/surefire 2021-11-01T16-51-51_363-jvmRun2 surefire6448660128033443499tmp surefire_4131168043975619749001tmp
Nov 01 17:19:12 [ERROR] Error occurred in starting fork, check output in log
Nov 01 17:19:12 [ERROR] Process Exit Code: 239
Nov 01 17:19:12 [ERROR] Crashed tests:
Nov 01 17:19:12 [ERROR] org.apache.flink.runtime.operators.coordination.OperatorCoordinatorSchedulerTest
Nov 01 17:19:12 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Nov 01 17:19:12 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-runtime && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /__w/1/s/flink-runtime/target/surefire/surefirebooter6007815607334336440.jar /__w/1/s/flink-runtime/target/surefire 2021-11-01T16-51-51_363-jvmRun2 surefire6448660128033443499tmp surefire_4131168043975619749001tmp
Nov 01 17:19:12 [ERROR] Error occurred in starting fork, check output in log
Nov 01 17:19:12 [ERROR] Process Exit Code: 239
Nov 01 17:19:12 [ERROR] Crashed tests:
Nov 01 17:19:12 [ERROR] org.apache.flink.runtime.operators.coordination.OperatorCoordinatorSchedulerTest
Nov 01 17:19:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:510)
Nov 01 17:19:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:457)
{code}

It looks like the {{testSnapshotAsyncFailureFailsCheckpoint}} caused it even though finishing successfully due to a fatal error when shutting down the cluster:
{code}
17:07:27,264 [    Checkpoint Timer] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'Checkpoint Timer' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: java.lang.IllegalStateException: CheckpointsCleaner has already been closed
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.lambda$startTriggeringCheckpoint$7(CheckpointCoordinator.java:626) ~[classes/:?]
        at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) [?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575) [?:1.8.0_292]
        at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:814) [?:1.8.0_292]
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) [?:1.8.0_292]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_292]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: CheckpointsCleaner has already been closed
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:838) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_292]
        ... 8 more
Caused by: java.lang.IllegalStateException: CheckpointsCleaner has already been closed
        at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193) ~[flink-core-1.14-stream-SNAPSHOT.jar:1.14-stream-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointsCleaner.incrementNumberOfCheckpointsToClean(CheckpointsCleaner.java:105) ~[classes/:?]
        at org.apache.flink.runtime.checkpoint.CheckpointsCleaner.cleanup(CheckpointsCleaner.java:87) ~[classes/:?]
        at org.apache.flink.runtime.checkpoint.CheckpointsCleaner.cleanCheckpoint(CheckpointsCleaner.java:62) ~[classes/:?]
        at org.apache.flink.runtime.checkpoint.PendingCheckpoint.dispose(PendingCheckpoint.java:573) ~[classes/:?]
        at org.apache.flink.runtime.checkpoint.PendingCheckpoint.abort(PendingCheckpoint.java:551) ~[classes/:?]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:1939) ~[classes/:?]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:1926) ~[classes/:?]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.onTriggerFailure(CheckpointCoordinator.java:910) ~[classes/:?]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.onTriggerFailure(CheckpointCoordinator.java:875) ~[classes/:?]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.lambda$startTriggeringCheckpoint$6(CheckpointCoordinator.java:614) ~[classes/:?]
        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_292]
        ... 8 more
{code}",,dmvk,gaoyunhaii,mapohl,pnowojski,trohrmann,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24792,FLINK-24938,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/21 08:54;mapohl;logs-ci_build-test_ci_build_finegrained_resource_management-1635785399.zip;https://issues.apache.org/jira/secure/attachment/13035724/logs-ci_build-test_ci_build_finegrained_resource_management-1635785399.zip",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 26 17:10:47 UTC 2021,,,,,,,,,,"0|z0wgwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/21 10:08;dmvk;The root cause is in the race condition between checkpoint future completion and checkpoint disposal. I was able to reproduce the issue locally with the following change + repeated test:


{code:java}
diff --git a/flink-core/src/main/java/org/apache/flink/util/FatalExitExceptionHandler.java b/flink-core/src/main/java/org/apache/flink/util/FatalExitExceptionHandler.java
index 8ff0e9b7b0a..2898df0e458 100644
--- a/flink-core/src/main/java/org/apache/flink/util/FatalExitExceptionHandler.java
+++ b/flink-core/src/main/java/org/apache/flink/util/FatalExitExceptionHandler.java
@@ -43,6 +43,7 @@ public final class FatalExitExceptionHandler implements Thread.UncaughtException
                     ""FATAL: Thread '{}' produced an uncaught exception. Stopping the process..."",
                     t.getName(),
                     e);
+            e.printStackTrace();
         } finally {
             FlinkSecurityManager.forceProcessExit(EXIT_CODE);
         }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/PendingCheckpoint.java b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/PendingCheckpoint.java
index af15c82a398..8791a0ab78f 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/PendingCheckpoint.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/PendingCheckpoint.java
@@ -547,6 +547,11 @@ public class PendingCheckpoint implements Checkpoint {
             reportFailedCheckpoint(failureCause, statsCallback);
             assertAbortSubsumedForced(reason);
         } finally {
+            try {
+                Thread.sleep(1000);
+            } catch (InterruptedException e) {
+                e.printStackTrace();
+            }
             dispose(true, checkpointsCleaner, postCleanup, executor);
         }
     } {code};;;","05/Nov/21 11:44;chesnay;Note that recently some changes were made to the CheckpointCleaner in {{-FLINK-24789-}} FLINK-23647.;;;","05/Nov/21 11:58;dmvk;[~chesnay] you mean in FLINK-23647 right?

The actual commit introducing the regression is [https://github.com/apache/flink/commit/daa8ac9426e26ae3c6fc4841ce2c5b217436ff80];;;","05/Nov/21 12:41;chesnay;yes, that's the ticket I meant. :/;;;","06/Nov/21 04:06;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26070&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9740];;;","07/Nov/21 15:30;gaoyunhaii;Another Instance on 1.14: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26070&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9740];;;","08/Nov/21 07:46;trohrmann;What's the progress on this ticket [~dmvk]?;;;","08/Nov/21 10:07;dmvk;[~trohrmann] it should be ready to merge;;;","09/Nov/21 08:42;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26191&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=8089;;;","10/Nov/21 08:09;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26236&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=7831;;;","11/Nov/21 08:19;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26295&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=8107;;;","11/Nov/21 08:24;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26295&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=8107;;;","11/Nov/21 08:28;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26326&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=7832;;;","12/Nov/21 10:51;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26389&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9746;;;","15/Nov/21 13:23;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26499&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=8105];;;","16/Nov/21 07:00;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26573&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=7820];;;","23/Nov/21 16:39;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26797&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","23/Nov/21 16:56;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26800&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9500

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26800&view=logs&j=6bfdaf55-0c08-5e3f-a2d2-2a0285fd41cf&t=cb073eeb-41fa-5f93-7035-c175e0e49392;;;","23/Nov/21 18:29;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26904&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9746;;;","23/Nov/21 18:33;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26936&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=7737;;;","26/Nov/21 17:10;trohrmann;Fixed via

1.15.0: e42890fc67f55702f8aa61e465a16b251d0bc87a
1.14.1: 0258419c66a696baf37b69ad29bb58ee5c62e912;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Processed (persisted) in-flight data description miss on Monitoring Checkpointing page,FLINK-24777,13410095,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,camilesing,camilesing,camilesing,04/Nov/21 16:13,15/Dec/21 01:44,13/Jul/23 08:12,06/Dec/21 12:26,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Documentation,,,,,0,pull-request-available,,,,"!image-2021-11-05-00-10-08-081.png!

Processed (persisted) in-flight data description is missed, we need to merge from Processed (persisted) in-flight data and Persisted) in-flight data ",,camilesing,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/21 16:10;camilesing;image-2021-11-05-00-10-08-081.png;https://issues.apache.org/jira/secure/attachment/13035708/image-2021-11-05-00-10-08-081.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 06 12:26:13 UTC 2021,,,,,,,,,,"0|z0wg8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/21 12:26;yunta;Merged
master: 5f1f480978811f3dcf2b0f9ea118a5d9068db5fb
release-1.14: ef0e17ad6319175ce0054fc3c4db14b78e690dd6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clarify semantics of DecodingFormat and its data type,FLINK-24776,13410091,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,slinkydeveloper,slinkydeveloper,slinkydeveloper,04/Nov/21 16:02,07/Apr/22 10:07,13/Jul/23 08:12,17/Nov/21 15:57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Today the {{org.apache.flink.table.connector.format.DecodingFormat}} interface has not clear requirements and it's confusing for implementers. In particular, it's unclear whether the format need to support projection push down or not, and whether the {{DataType}} provided to {{createRuntimeDecoder}} is projected and includes partition keys or not. An example of such misunderstanding is shown here: https://github.com/apache/flink/blob/991dd0466ff28995a22ded0727ef2a1706d9bddc/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java#L107

The PR https://github.com/apache/flink/pull/17544 partially addresses the issue, because it removes the need from BulkFormat implementations to take care of partition keys handling. Neverthless, it's still unclear whether formats support projections or not and if they support nested projections.

We should refactor {{DecodingFormat}} as follows:

- Clarify DecodingFormat and introduce ProjectableDecodingFormat.
- Introduce ProjectedRowData and Projection to simplify implementations of connectors that needs to deal with projections
- Apply the changes to most of the formats and connectors we have.",,slinkydeveloper,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 17 15:57:20 UTC 2021,,,,,,,,,,"0|z0wg7s:",9223372036854775807,The DecodingFormat interface was used for both projectable and non-projectable formats which led to inconsistent implementations. The FileSystemTableSource has been updated to distinguish between those two interfaces now. Users that implement custom formats for FileSystemTableSource might need to verify the implementation and make sure to implement ProjectableDecodingFormat if necessary.,,,,,,,,,,,,,,,,,,,"17/Nov/21 15:57;twalthr;Fixed in master: e5111c970877b5772f0326ffbc998e0f6a8d351f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaCommitter should fail on unknown Exception,FLINK-24773,13410067,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fpaul,fpaul,fpaul,04/Nov/21 14:10,15/Dec/21 01:44,13/Jul/23 08:12,12/Nov/21 14:28,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"Some of the exceptions during the committing phase are tolerable or even retriable but generally, if an unknown exception is raised we should fail the job.",,fpaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 12 14:28:42 UTC 2021,,,,,,,,,,"0|z0wg2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Nov/21 14:28;arvid;Merged into master as f28bb89f3520875cb79b2a5ac8fbaab06ca2a547, into 1.14 as 4248102997f4ad1095e5aa31bd52547d8073b863.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ceiling/flooring dates to day return wrong results,FLINK-24766,13410010,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,04/Nov/21 09:45,04/May/22 18:15,13/Jul/23 08:12,04/May/22 18:14,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.16.0,,,,,Table SQL / API,,,,,0,auto-deprioritized-major,pull-request-available,,,"Query to reproduce
{code:sql}
select ceil(date '2021-11-04' to day) as `ceil`,
       floor(date '2021-11-04' to day) as `floor`;
{code}
gives
{noformat}
ceil         floor
8525-03-02 1970-01-01
{noformat}

expected
{noformat}
ceil         floor
2021-11-05 2021-11-05
{noformat}",,libenchao,martijnvisser,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 04 18:14:55 UTC 2022,,,,,,,,,,"0|z0wfps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/22 10:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","04/Feb/22 10:37;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","04/May/22 18:14;martijnvisser;Fixed in master: a39fe96b9642214600760f1ddc6ad7e8f3bace0f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ParquetFileSystemITCase.testLimitableBulkFormat failed on Azure,FLINK-24763,13410002,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,trohrmann,trohrmann,04/Nov/21 09:19,30/Nov/21 06:06,13/Jul/23 08:12,29/Nov/21 01:51,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Connectors / FileSystem,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,test-stability,,,"The test {{ParquetFileSystemITCase.testLimitableBulkFormat}} fails with 

{code}
2021-11-03T22:10:11.5106075Z Nov 03 22:10:11 [ERROR] testLimitableBulkFormat[false]  Time elapsed: 9.177 s  <<< ERROR!
2021-11-03T22:10:11.5106643Z Nov 03 22:10:11 java.lang.RuntimeException: Failed to fetch next result
2021-11-03T22:10:11.5107213Z Nov 03 22:10:11 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
2021-11-03T22:10:11.5111034Z Nov 03 22:10:11 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2021-11-03T22:10:11.5112190Z Nov 03 22:10:11 	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:188)
2021-11-03T22:10:11.5112892Z Nov 03 22:10:11 	at java.util.Iterator.forEachRemaining(Iterator.java:115)
2021-11-03T22:10:11.5113393Z Nov 03 22:10:11 	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:109)
2021-11-03T22:10:11.5114157Z Nov 03 22:10:11 	at org.apache.flink.formats.parquet.ParquetFileSystemITCase.testLimitableBulkFormat(ParquetFileSystemITCase.java:128)
2021-11-03T22:10:11.5114951Z Nov 03 22:10:11 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-11-03T22:10:11.5115568Z Nov 03 22:10:11 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-11-03T22:10:11.5116115Z Nov 03 22:10:11 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-11-03T22:10:11.5116591Z Nov 03 22:10:11 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-11-03T22:10:11.5117088Z Nov 03 22:10:11 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2021-11-03T22:10:11.5117807Z Nov 03 22:10:11 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-11-03T22:10:11.5118821Z Nov 03 22:10:11 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2021-11-03T22:10:11.5119417Z Nov 03 22:10:11 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-11-03T22:10:11.5119944Z Nov 03 22:10:11 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-11-03T22:10:11.5120427Z Nov 03 22:10:11 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-11-03T22:10:11.5120919Z Nov 03 22:10:11 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2021-11-03T22:10:11.5121571Z Nov 03 22:10:11 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-11-03T22:10:11.5122526Z Nov 03 22:10:11 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2021-11-03T22:10:11.5123245Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2021-11-03T22:10:11.5123804Z Nov 03 22:10:11 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2021-11-03T22:10:11.5124314Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2021-11-03T22:10:11.5124806Z Nov 03 22:10:11 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2021-11-03T22:10:11.5125313Z Nov 03 22:10:11 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2021-11-03T22:10:11.5125810Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2021-11-03T22:10:11.5126281Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2021-11-03T22:10:11.5126739Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2021-11-03T22:10:11.5127349Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2021-11-03T22:10:11.5128092Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2021-11-03T22:10:11.5128984Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2021-11-03T22:10:11.5129685Z Nov 03 22:10:11 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-11-03T22:10:11.5130330Z Nov 03 22:10:11 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-11-03T22:10:11.5130771Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2021-11-03T22:10:11.5131222Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2021-11-03T22:10:11.5131663Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2021-11-03T22:10:11.5132139Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2021-11-03T22:10:11.5132776Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2021-11-03T22:10:11.5133441Z Nov 03 22:10:11 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2021-11-03T22:10:11.5134150Z Nov 03 22:10:11 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2021-11-03T22:10:11.5134816Z Nov 03 22:10:11 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-11-03T22:10:11.5135741Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2021-11-03T22:10:11.5136292Z Nov 03 22:10:11 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2021-11-03T22:10:11.5136717Z Nov 03 22:10:11 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2021-11-03T22:10:11.5137140Z Nov 03 22:10:11 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2021-11-03T22:10:11.5137603Z Nov 03 22:10:11 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
2021-11-03T22:10:11.5138134Z Nov 03 22:10:11 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2021-11-03T22:10:11.5138766Z Nov 03 22:10:11 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2021-11-03T22:10:11.5139235Z Nov 03 22:10:11 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2021-11-03T22:10:11.5139733Z Nov 03 22:10:11 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
2021-11-03T22:10:11.5140493Z Nov 03 22:10:11 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2021-11-03T22:10:11.5141265Z Nov 03 22:10:11 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2021-11-03T22:10:11.5141991Z Nov 03 22:10:11 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2021-11-03T22:10:11.5142892Z Nov 03 22:10:11 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2021-11-03T22:10:11.5143712Z Nov 03 22:10:11 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2021-11-03T22:10:11.5144655Z Nov 03 22:10:11 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2021-11-03T22:10:11.5145423Z Nov 03 22:10:11 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
2021-11-03T22:10:11.5146236Z Nov 03 22:10:11 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
2021-11-03T22:10:11.5147106Z Nov 03 22:10:11 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
2021-11-03T22:10:11.5148061Z Nov 03 22:10:11 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
2021-11-03T22:10:11.5149081Z Nov 03 22:10:11 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
2021-11-03T22:10:11.5149900Z Nov 03 22:10:11 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
2021-11-03T22:10:11.5150722Z Nov 03 22:10:11 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
2021-11-03T22:10:11.5151619Z Nov 03 22:10:11 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
2021-11-03T22:10:11.5152790Z Nov 03 22:10:11 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2021-11-03T22:10:11.5153810Z Nov 03 22:10:11 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-11-03T22:10:11.5154754Z Nov 03 22:10:11 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-11-03T22:10:11.5155649Z Nov 03 22:10:11 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-11-03T22:10:11.5156235Z Nov 03 22:10:11 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-11-03T22:10:11.5156702Z Nov 03 22:10:11 Caused by: java.io.IOException: Failed to fetch job execution result
2021-11-03T22:10:11.5157298Z Nov 03 22:10:11 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
2021-11-03T22:10:11.5157969Z Nov 03 22:10:11 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
2021-11-03T22:10:11.5158972Z Nov 03 22:10:11 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2021-11-03T22:10:11.5160158Z Nov 03 22:10:11 	... 67 more
2021-11-03T22:10:11.5160863Z Nov 03 22:10:11 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-11-03T22:10:11.5161734Z Nov 03 22:10:11 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2021-11-03T22:10:11.5162618Z Nov 03 22:10:11 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2021-11-03T22:10:11.5163501Z Nov 03 22:10:11 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
2021-11-03T22:10:11.5164204Z Nov 03 22:10:11 	... 69 more
2021-11-03T22:10:11.5164752Z Nov 03 22:10:11 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-11-03T22:10:11.5165533Z Nov 03 22:10:11 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2021-11-03T22:10:11.5166427Z Nov 03 22:10:11 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2021-11-03T22:10:11.5167353Z Nov 03 22:10:11 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2021-11-03T22:10:11.5168131Z Nov 03 22:10:11 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2021-11-03T22:10:11.5169012Z Nov 03 22:10:11 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2021-11-03T22:10:11.5169614Z Nov 03 22:10:11 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:134)
2021-11-03T22:10:11.5170488Z Nov 03 22:10:11 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:181)
2021-11-03T22:10:11.5170942Z Nov 03 22:10:11 	... 69 more
2021-11-03T22:10:11.5171362Z Nov 03 22:10:11 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2021-11-03T22:10:11.5171990Z Nov 03 22:10:11 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2021-11-03T22:10:11.5172937Z Nov 03 22:10:11 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2021-11-03T22:10:11.5173707Z Nov 03 22:10:11 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:228)
2021-11-03T22:10:11.5174364Z Nov 03 22:10:11 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:218)
2021-11-03T22:10:11.5174999Z Nov 03 22:10:11 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:209)
2021-11-03T22:10:11.5175621Z Nov 03 22:10:11 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:681)
2021-11-03T22:10:11.5176374Z Nov 03 22:10:11 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2021-11-03T22:10:11.5177185Z Nov 03 22:10:11 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:445)
2021-11-03T22:10:11.5177900Z Nov 03 22:10:11 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-11-03T22:10:11.5178571Z Nov 03 22:10:11 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-11-03T22:10:11.5179526Z Nov 03 22:10:11 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-11-03T22:10:11.5180271Z Nov 03 22:10:11 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-11-03T22:10:11.5181030Z Nov 03 22:10:11 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
2021-11-03T22:10:11.5182018Z Nov 03 22:10:11 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2021-11-03T22:10:11.5183204Z Nov 03 22:10:11 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
2021-11-03T22:10:11.5183790Z Nov 03 22:10:11 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2021-11-03T22:10:11.5184385Z Nov 03 22:10:11 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2021-11-03T22:10:11.5184939Z Nov 03 22:10:11 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2021-11-03T22:10:11.5185462Z Nov 03 22:10:11 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2021-11-03T22:10:11.5185923Z Nov 03 22:10:11 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2021-11-03T22:10:11.5186504Z Nov 03 22:10:11 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2021-11-03T22:10:11.5186966Z Nov 03 22:10:11 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2021-11-03T22:10:11.5187419Z Nov 03 22:10:11 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2021-11-03T22:10:11.5187885Z Nov 03 22:10:11 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-11-03T22:10:11.5188346Z Nov 03 22:10:11 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2021-11-03T22:10:11.5188890Z Nov 03 22:10:11 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2021-11-03T22:10:11.5189566Z Nov 03 22:10:11 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2021-11-03T22:10:11.5190267Z Nov 03 22:10:11 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2021-11-03T22:10:11.5190700Z Nov 03 22:10:11 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2021-11-03T22:10:11.5191170Z Nov 03 22:10:11 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2021-11-03T22:10:11.5191586Z Nov 03 22:10:11 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2021-11-03T22:10:11.5192019Z Nov 03 22:10:11 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2021-11-03T22:10:11.5192544Z Nov 03 22:10:11 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2021-11-03T22:10:11.5192937Z Nov 03 22:10:11 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2021-11-03T22:10:11.5193584Z Nov 03 22:10:11 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2021-11-03T22:10:11.5194359Z Nov 03 22:10:11 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2021-11-03T22:10:11.5195082Z Nov 03 22:10:11 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2021-11-03T22:10:11.5195807Z Nov 03 22:10:11 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2021-11-03T22:10:11.5196524Z Nov 03 22:10:11 Caused by: java.lang.RuntimeException: One or more fetchers have encountered exception
2021-11-03T22:10:11.5197410Z Nov 03 22:10:11 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)
2021-11-03T22:10:11.5198368Z Nov 03 22:10:11 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
2021-11-03T22:10:11.5199370Z Nov 03 22:10:11 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)
2021-11-03T22:10:11.5200292Z Nov 03 22:10:11 	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:350)
2021-11-03T22:10:11.5201255Z Nov 03 22:10:11 	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
2021-11-03T22:10:11.5202193Z Nov 03 22:10:11 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
2021-11-03T22:10:11.5203230Z Nov 03 22:10:11 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:474)
2021-11-03T22:10:11.5203819Z Nov 03 22:10:11 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
2021-11-03T22:10:11.5204599Z Nov 03 22:10:11 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:788)
2021-11-03T22:10:11.5205415Z Nov 03 22:10:11 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:737)
2021-11-03T22:10:11.5206119Z Nov 03 22:10:11 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:947)
2021-11-03T22:10:11.5206635Z Nov 03 22:10:11 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:926)
2021-11-03T22:10:11.5207306Z Nov 03 22:10:11 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:740)
2021-11-03T22:10:11.5207928Z Nov 03 22:10:11 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
2021-11-03T22:10:11.5208474Z Nov 03 22:10:11 	at java.lang.Thread.run(Thread.java:748)
2021-11-03T22:10:11.5209343Z Nov 03 22:10:11 Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
2021-11-03T22:10:11.5209976Z Nov 03 22:10:11 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150)
2021-11-03T22:10:11.5210570Z Nov 03 22:10:11 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)
2021-11-03T22:10:11.5211092Z Nov 03 22:10:11 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2021-11-03T22:10:11.5211564Z Nov 03 22:10:11 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-11-03T22:10:11.5212180Z Nov 03 22:10:11 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2021-11-03T22:10:11.5212969Z Nov 03 22:10:11 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2021-11-03T22:10:11.5213361Z Nov 03 22:10:11 	... 1 more
2021-11-03T22:10:11.5215786Z Nov 03 22:10:11 Caused by: java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2021-11-03T22:10:11.5217523Z Nov 03 22:10:11 	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:164)
2021-11-03T22:10:11.5218577Z Nov 03 22:10:11 	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.getResource(FlinkUserCodeClassLoaders.java:183)
2021-11-03T22:10:11.5219513Z Nov 03 22:10:11 	at org.apache.hadoop.conf.Configuration.getResource(Configuration.java:2780)
2021-11-03T22:10:11.5220068Z Nov 03 22:10:11 	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3036)
2021-11-03T22:10:11.5220721Z Nov 03 22:10:11 	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2995)
2021-11-03T22:10:11.5221505Z Nov 03 22:10:11 	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2968)
2021-11-03T22:10:11.5222138Z Nov 03 22:10:11 	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2848)
2021-11-03T22:10:11.5222733Z Nov 03 22:10:11 	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1200)
2021-11-03T22:10:11.5223230Z Nov 03 22:10:11 	at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1254)
2021-11-03T22:10:11.5223707Z Nov 03 22:10:11 	at org.apache.hadoop.conf.Configuration.getInt(Configuration.java:1479)
2021-11-03T22:10:11.5224231Z Nov 03 22:10:11 	at org.apache.parquet.hadoop.codec.SnappyCodec.createInputStream(SnappyCodec.java:75)
2021-11-03T22:10:11.5224772Z Nov 03 22:10:11 	at org.apache.parquet.hadoop.CodecFactory$HeapBytesDecompressor.decompress(CodecFactory.java:109)
2021-11-03T22:10:11.5225418Z Nov 03 22:10:11 	at org.apache.parquet.hadoop.ColumnChunkPageReadStore$ColumnChunkPageReader.readDictionaryPage(ColumnChunkPageReadStore.java:196)
2021-11-03T22:10:11.5226286Z Nov 03 22:10:11 	at org.apache.flink.formats.parquet.vector.reader.AbstractColumnReader.<init>(AbstractColumnReader.java:110)
2021-11-03T22:10:11.5226876Z Nov 03 22:10:11 	at org.apache.flink.formats.parquet.vector.reader.IntColumnReader.<init>(IntColumnReader.java:33)
2021-11-03T22:10:11.5227492Z Nov 03 22:10:11 	at org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.createColumnReader(ParquetSplitReaderUtil.java:280)
2021-11-03T22:10:11.5228185Z Nov 03 22:10:11 	at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat$ParquetReader.readNextRowGroup(ParquetVectorizedInputFormat.java:412)
2021-11-03T22:10:11.5228961Z Nov 03 22:10:11 	at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat$ParquetReader.nextBatch(ParquetVectorizedInputFormat.java:381)
2021-11-03T22:10:11.5229660Z Nov 03 22:10:11 	at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat$ParquetReader.readBatch(ParquetVectorizedInputFormat.java:358)
2021-11-03T22:10:11.5230333Z Nov 03 22:10:11 	at org.apache.flink.table.filesystem.LimitableBulkFormat$LimitableReader.readBatch(LimitableBulkFormat.java:108)
2021-11-03T22:10:11.5230939Z Nov 03 22:10:11 	at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:67)
2021-11-03T22:10:11.5231515Z Nov 03 22:10:11 	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
2021-11-03T22:10:11.5232095Z Nov 03 22:10:11 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)
2021-11-03T22:10:11.5232614Z Nov 03 22:10:11 	... 6 more
2021-11-03T22:10:11.5232979Z Nov 03 22:10:11 
2021-11-03T22:10:11.5234489Z Exception in thread ""Thread-11"" java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2021-11-03T22:10:11.5235610Z 	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:164)
2021-11-03T22:10:11.5236345Z 	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.getResource(FlinkUserCodeClassLoaders.java:183)
2021-11-03T22:10:11.5236944Z 	at org.apache.hadoop.conf.Configuration.getResource(Configuration.java:2780)
2021-11-03T22:10:11.5237383Z 	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3036)
2021-11-03T22:10:11.5237843Z 	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2995)
2021-11-03T22:10:11.5238296Z 	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2968)
2021-11-03T22:10:11.5238798Z 	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2848)
2021-11-03T22:10:11.5239225Z 	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1200)
2021-11-03T22:10:11.5239647Z 	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1812)
2021-11-03T22:10:11.5240102Z 	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1789)
2021-11-03T22:10:11.5240583Z 	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
2021-11-03T22:10:11.5241070Z 	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
2021-11-03T22:10:11.5241561Z 	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
2021-11-03T22:10:11.5242032Z 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
2021-11-03T22:10:12.8086663Z Nov 03 22:10:12 [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.576 s - in org.apache.flink.formats.parquet.ParquetFileCompactionITCase
2021-11-03T22:10:12.8135630Z Exception in thread ""Thread-10"" java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2021-11-03T22:10:12.8137964Z 	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:164)
2021-11-03T22:10:12.8139304Z 	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.getResource(FlinkUserCodeClassLoaders.java:183)
2021-11-03T22:10:12.8140308Z 	at org.apache.hadoop.conf.Configuration.getResource(Configuration.java:2780)
2021-11-03T22:10:12.8141050Z 	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3036)
2021-11-03T22:10:12.8141824Z 	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2995)
2021-11-03T22:10:12.8142673Z 	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2968)
2021-11-03T22:10:12.8143402Z 	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2848)
2021-11-03T22:10:12.8144120Z 	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1200)
2021-11-03T22:10:12.8144880Z 	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1812)
2021-11-03T22:10:12.8145632Z 	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1789)
2021-11-03T22:10:12.8146463Z 	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
2021-11-03T22:10:12.8147479Z 	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
2021-11-03T22:10:12.8148320Z 	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
2021-11-03T22:10:12.8149186Z 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
2021-11-03T22:10:13.1599258Z Nov 03 22:10:13 [INFO] Running org.apache.flink.formats.parquet.ParquetFsStreamingSinkITCase
2021-11-03T22:10:28.5117719Z Nov 03 22:10:28 [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.346 s - in org.apache.flink.formats.parquet.ParquetFsStreamingSinkITCase
2021-11-03T22:10:28.5155684Z Exception in thread ""Thread-11"" java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2021-11-03T22:10:28.5157699Z 	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:164)
2021-11-03T22:10:28.5158964Z 	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.getResource(FlinkUserCodeClassLoaders.java:183)
2021-11-03T22:10:28.5159902Z 	at org.apache.hadoop.conf.Configuration.getResource(Configuration.java:2780)
2021-11-03T22:10:28.5160644Z 	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3036)
2021-11-03T22:10:28.5161387Z 	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2995)
2021-11-03T22:10:28.5162100Z 	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2968)
2021-11-03T22:10:28.5162937Z 	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2848)
2021-11-03T22:10:28.5163616Z 	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1200)
2021-11-03T22:10:28.5164320Z 	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1812)
2021-11-03T22:10:28.5165054Z 	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1789)
2021-11-03T22:10:28.5165827Z 	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
2021-11-03T22:10:28.5166962Z 	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
2021-11-03T22:10:28.5167777Z 	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
2021-11-03T22:10:28.5168523Z 	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25896&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=13356",,gaoyunhaii,lzljs3620320,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25071,FLINK-25102,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 29 01:51:30 UTC 2021,,,,,,,,,,"0|z0wfo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/21 10:36;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25983&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=13949

cc [~arvid];;;","07/Nov/21 15:39;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26083&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=13693];;;","08/Nov/21 07:45;trohrmann;cc [~arvid];;;","08/Nov/21 07:57;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26093&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=13598;;;","09/Nov/21 08:28;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26188&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=13601;;;","09/Nov/21 08:29;trohrmann;[~arvid] can we disable this test and create a blocker for fixing it?;;;","10/Nov/21 08:10;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26236&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=13923;;;","11/Nov/21 08:30;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26326&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354;;;","12/Nov/21 10:48;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26386&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=14104;;;","12/Nov/21 16:38;arvid;Please stop pinging me. Blame shows that [~lzljs3620320] is maintaining parquet.

Apparently, Hadoop cleanup hooks try to access the configurations from resources inside the user jar that is already unloaded because the job finished (failed?).;;;","15/Nov/21 03:53;lzljs3620320;I will take a look.;;;","15/Nov/21 03:54;lzljs3620320;[~trohrmann]  I will fix this as soon as possible. If it goes well, we don't have to ignore it.;;;","15/Nov/21 04:16;lzljs3620320;It seems the reason is the early termination of iterator in the `LimitableIterator`.
 * A `LimitableIterator` is over, it has reached the limit, so it return null. Then the reader thread finds that there is no split next, so it thinks it has finished reading. 

 * But the fetcher thread dose not know, the current split has remaining records.  The fetcher still read records from hadoop files, but the class loader is closed.

Therefore, we can solve this: when the iterator thinks it is over, even if the reader encounters an exception, we should exit normally.;;;","15/Nov/21 04:18;lzljs3620320;CC: [~sewen] [~becket_qin] This is just my guess. Do you have any other ideas? We can see if the fix works.

If this is the reason. Do we have a better way to support the early termination of iterator? Maybe we can have an elegant way to turn off the fetcher? Wait for the fetcher thread to shut down before cleaning up resources?;;;","15/Nov/21 07:45;arvid;How does the fetcher thread determines if there are remaining records? From your description, it sounds like the fetcher thread checks for {{readerPosition < offset}} but I can't find a respective place.


;;;","15/Nov/21 08:53;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26496&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=13531;;;","15/Nov/21 09:46;lzljs3620320;> How does the fetcher thread determines if there are remaining records? From your description, it sounds like the fetcher thread checks for {{readerPosition < offset}} but I can't find a respective place.
The fetcher return null when:
- reader.readBatch() return null
- or reachLimit
The fetcher return a batch to reader, and the fetcher read next batch.  It doesn't know if it the returned batch that exceeded the limit.;;;","15/Nov/21 20:14;arvid;For me, it still not clear why the reader is closed at all. The reader should shutdown the fetcher and wait for termination and only after that should the classloader be closed.

If I look at the extracted log, I don't see any sign of shutdown before the failure

{noformat}
23:14:08,052 [flink-akka.actor.default-dispatcher-10] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: TableSourceScan(table=[[default_catalog, default_database, parquetLimitTable, limit=[5]]], fields=[x, y, a]) -> Limit(offset=[0], fetch=[5], global=[false]) (2/3) (4cdd9d2aec738c05342d7c2603a72cc8) switched from INITIALIZING to RUNNING.

23:14:08,068 [Source Data Fetcher for Source: TableSourceScan(table=[[default_catalog, default_database, parquetLimitTable, limit=[5]]], fields=[x, y, a]) -> Limit(offset=[0], fetch=[5], global=[false]) (2/3)#0] INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0

23:14:08,276 [Source Data Fetcher for Source: TableSourceScan(table=[[default_catalog, default_database, parquetLimitTable, limit=[5]]], fields=[x, y, a]) -> Limit(offset=[0], fetch=[5], global=[false]) (2/3)#0] WARN  org.apache.hadoop.util.NativeCodeLoader                      [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable

23:14:08,457 [Source Data Fetcher for Source: TableSourceScan(table=[[default_catalog, default_database, parquetLimitTable, limit=[5]]], fields=[x, y, a]) -> Limit(offset=[0], fetch=[5], global=[false]) (2/3)#0] INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.snappy]

23:14:08,463 [Source Data Fetcher for Source: TableSourceScan(table=[[default_catalog, default_database, parquetLimitTable, limit=[5]]], fields=[x, y, a]) -> Limit(offset=[0], fetch=[5], global=[false]) (2/3)#0] ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.

23:14:08,474 [Source Data Fetcher for Source: TableSourceScan(table=[[default_catalog, default_database, parquetLimitTable, limit=[5]]], fields=[x, y, a]) -> Limit(offset=[0], fetch=[5], global=[false]) (2/3)#0] INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.

23:14:08,474 [Source: TableSourceScan(table=[[default_catalog, default_database, parquetLimitTable, limit=[5]]], fields=[x, y, a]) -> Limit(offset=[0], fetch=[5], global=[false]) (2/3)#0] INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.

23:14:08,478 [Source: TableSourceScan(table=[[default_catalog, default_database, parquetLimitTable, limit=[5]]], fields=[x, y, a]) -> Limit(offset=[0], fetch=[5], global=[false]) (2/3)#0] WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: TableSourceScan(table=[[default_catalog, default_database, parquetLimitTable, limit=[5]]], fields=[x, y, a]) -> Limit(offset=[0], fetch=[5], global=[false]) (2/3)#0 (4cdd9d2aec738c05342d7c2603a72cc8) switched from RUNNING to FAILED with failure cause: java.lang.RuntimeException: One or more fetchers have encountered exception

23:14:08,479 [Source: TableSourceScan(table=[[default_catalog, default_database, parquetLimitTable, limit=[5]]], fields=[x, y, a]) -> Limit(offset=[0], fetch=[5], global=[false]) (2/3)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: TableSourceScan(table=[[default_catalog, default_database, parquetLimitTable, limit=[5]]], fields=[x, y, a]) -> Limit(offset=[0], fetch=[5], global=[false]) (2/3)#0 (4cdd9d2aec738c05342d7c2603a72cc8).

23:14:08,484 [flink-akka.actor.default-dispatcher-10] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: TableSourceScan(table=[[default_catalog, default_database, parquetLimitTable, limit=[5]]], fields=[x, y, a]) -> Limit(offset=[0], fetch=[5], global=[false]) (2/3)#0 4cdd9d2aec738c05342d7c2603a72cc8.

23:14:08,498 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: TableSourceScan(table=[[default_catalog, default_database, parquetLimitTable, limit=[5]]], fields=[x, y, a]) -> Limit(offset=[0], fetch=[5], global=[false]) (2/3) (4cdd9d2aec738c05342d7c2603a72cc8) switched from RUNNING to FAILED on 736f6206-2099-4579-b8cd-cd737f09e9c9 @ localhost (dataPort=-1).
{noformat}

Unfortunately, I currently cannot execute this test in my IDE. Could you check when {{SafetyNetWrapperClassLoader#close}} is called on the respective failing task?
;;;","16/Nov/21 02:31;lzljs3620320;You can add guava dependency to execute test in IDE.

I can not re-produce in my local test...;;;","17/Nov/21 08:39;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26624&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=13842;;;","18/Nov/21 02:34;lzljs3620320;[~arvid] Hi, Considering the instability of this case, can we merge this PR first? I think it is harmless.
If the failure is reproduced later, we can consider ignoring this case first and look deeply.;;;","18/Nov/21 07:09;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26676&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=13523];;;","19/Nov/21 06:54;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26722&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=13523];;;","21/Nov/21 10:56;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26774&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=13847;;;","21/Nov/21 11:11;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26783&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=13846];;;","23/Nov/21 18:14;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26870&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=13737;;;","23/Nov/21 18:15;trohrmann;[~lzljs3620320] any updates on this problem?;;;","24/Nov/21 03:34;lzljs3620320;Fixed in master: a5477938952863fb516a1a14d67808ecda047134

[~trohrmann] I merged the fix first, but there is no very certainty that the fix is complete, and we will keep watching.
If the problem still recurs, I will disable the test for the time being and create a follow-up blocker ticket to re-enable it.;;;","24/Nov/21 10:00;trohrmann;Not sure whether this build includes your fix or not but there is another test failure: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26967&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=14060;;;","24/Nov/21 10:05;lzljs3620320;[~trohrmann] Thanks for reporting. The build dose not include the fix, because the code line is not matched: LimitableBulkFormat.java:108. Let's keep watching.;;;","25/Nov/21 08:37;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27021&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=12067.

This failure will probably contain your fix [~lzljs3620320].;;;","26/Nov/21 08:06;trohrmann;This time the test failed with 

{code}
2021-11-25T23:46:14.2851171Z Nov 25 23:46:14 [ERROR] ParquetFileSystemITCase.testLimitableBulkFormat  Time elapsed: 9.327 s  <<< ERROR!
2021-11-25T23:46:14.2853037Z Nov 25 23:46:14 java.lang.RuntimeException: Failed to fetch next result
2021-11-25T23:46:14.2854102Z Nov 25 23:46:14 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
2021-11-25T23:46:14.2855189Z Nov 25 23:46:14 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2021-11-25T23:46:14.2856400Z Nov 25 23:46:14 	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:211)
2021-11-25T23:46:14.2857455Z Nov 25 23:46:14 	at java.util.Iterator.forEachRemaining(Iterator.java:115)
2021-11-25T23:46:14.2859821Z Nov 25 23:46:14 	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:109)
2021-11-25T23:46:14.2861716Z Nov 25 23:46:14 	at org.apache.flink.formats.parquet.ParquetFileSystemITCase.testLimitableBulkFormat(ParquetFileSystemITCase.java:128)
2021-11-25T23:46:14.2863306Z Nov 25 23:46:14 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-11-25T23:46:14.2864288Z Nov 25 23:46:14 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-11-25T23:46:14.2865305Z Nov 25 23:46:14 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-11-25T23:46:14.2866221Z Nov 25 23:46:14 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-11-25T23:46:14.2867134Z Nov 25 23:46:14 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2021-11-25T23:46:14.2868161Z Nov 25 23:46:14 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-11-25T23:46:14.2869156Z Nov 25 23:46:14 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2021-11-25T23:46:14.2870152Z Nov 25 23:46:14 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-11-25T23:46:14.2871327Z Nov 25 23:46:14 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-11-25T23:46:14.2872274Z Nov 25 23:46:14 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-11-25T23:46:14.2873216Z Nov 25 23:46:14 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2021-11-25T23:46:14.2874170Z Nov 25 23:46:14 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-11-25T23:46:14.2875086Z Nov 25 23:46:14 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2021-11-25T23:46:14.2927640Z Nov 25 23:46:14 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2021-11-25T23:46:14.2929169Z Nov 25 23:46:14 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2021-11-25T23:46:14.2930298Z Nov 25 23:46:14 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2021-11-25T23:46:14.2932033Z Nov 25 23:46:14 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2021-11-25T23:46:14.2933342Z Nov 25 23:46:14 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2021-11-25T23:46:14.2934429Z Nov 25 23:46:14 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2021-11-25T23:46:14.2935429Z Nov 25 23:46:14 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2021-11-25T23:46:14.2936466Z Nov 25 23:46:14 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2021-11-25T23:46:14.2937501Z Nov 25 23:46:14 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2021-11-25T23:46:14.2938525Z Nov 25 23:46:14 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2021-11-25T23:46:14.2939549Z Nov 25 23:46:14 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2021-11-25T23:46:14.2940635Z Nov 25 23:46:14 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-11-25T23:46:14.2941824Z Nov 25 23:46:14 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-11-25T23:46:14.2942805Z Nov 25 23:46:14 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2021-11-25T23:46:14.2943826Z Nov 25 23:46:14 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2021-11-25T23:46:14.2944863Z Nov 25 23:46:14 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2021-11-25T23:46:14.2945892Z Nov 25 23:46:14 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2021-11-25T23:46:14.2946915Z Nov 25 23:46:14 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2021-11-25T23:46:14.2947980Z Nov 25 23:46:14 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2021-11-25T23:46:14.2949054Z Nov 25 23:46:14 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2021-11-25T23:46:14.2950071Z Nov 25 23:46:14 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-11-25T23:46:14.2951499Z Nov 25 23:46:14 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2021-11-25T23:46:14.2952855Z Nov 25 23:46:14 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2021-11-25T23:46:14.2953750Z Nov 25 23:46:14 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2021-11-25T23:46:14.2954654Z Nov 25 23:46:14 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2021-11-25T23:46:14.2955580Z Nov 25 23:46:14 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2021-11-25T23:46:14.2956620Z Nov 25 23:46:14 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2021-11-25T23:46:14.2957652Z Nov 25 23:46:14 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2021-11-25T23:46:14.2958751Z Nov 25 23:46:14 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2021-11-25T23:46:14.2959893Z Nov 25 23:46:14 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2021-11-25T23:46:14.2961330Z Nov 25 23:46:14 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2021-11-25T23:46:14.2962576Z Nov 25 23:46:14 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2021-11-25T23:46:14.2963778Z Nov 25 23:46:14 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2021-11-25T23:46:14.2964864Z Nov 25 23:46:14 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2021-11-25T23:46:14.2965889Z Nov 25 23:46:14 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2021-11-25T23:46:14.2967002Z Nov 25 23:46:14 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2021-11-25T23:46:14.2968467Z Nov 25 23:46:14 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2021-11-25T23:46:14.2969919Z Nov 25 23:46:14 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2021-11-25T23:46:14.2971245Z Nov 25 23:46:14 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2021-11-25T23:46:14.2972362Z Nov 25 23:46:14 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2021-11-25T23:46:14.2973389Z Nov 25 23:46:14 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2021-11-25T23:46:14.2974389Z Nov 25 23:46:14 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2021-11-25T23:46:14.2975366Z Nov 25 23:46:14 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2021-11-25T23:46:14.2976310Z Nov 25 23:46:14 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2021-11-25T23:46:14.2977240Z Nov 25 23:46:14 Caused by: java.io.IOException: Failed to fetch job execution result
2021-11-25T23:46:14.2978309Z Nov 25 23:46:14 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
2021-11-25T23:46:14.2979512Z Nov 25 23:46:14 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
2021-11-25T23:46:14.2980799Z Nov 25 23:46:14 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2021-11-25T23:46:14.2981862Z Nov 25 23:46:14 	... 62 more
2021-11-25T23:46:14.2982778Z Nov 25 23:46:14 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-11-25T23:46:14.2983879Z Nov 25 23:46:14 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2021-11-25T23:46:14.2984843Z Nov 25 23:46:14 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2021-11-25T23:46:14.2985946Z Nov 25 23:46:14 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
2021-11-25T23:46:14.2986847Z Nov 25 23:46:14 	... 64 more
2021-11-25T23:46:14.2987626Z Nov 25 23:46:14 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-11-25T23:46:14.2988649Z Nov 25 23:46:14 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2021-11-25T23:46:14.2989764Z Nov 25 23:46:14 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2021-11-25T23:46:14.2990961Z Nov 25 23:46:14 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2021-11-25T23:46:14.2992019Z Nov 25 23:46:14 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2021-11-25T23:46:14.2992998Z Nov 25 23:46:14 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2021-11-25T23:46:14.2994071Z Nov 25 23:46:14 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:134)
2021-11-25T23:46:14.2995244Z Nov 25 23:46:14 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:181)
2021-11-25T23:46:14.2996139Z Nov 25 23:46:14 	... 64 more
2021-11-25T23:46:14.2996951Z Nov 25 23:46:14 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2021-11-25T23:46:14.2998103Z Nov 25 23:46:14 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2021-11-25T23:46:14.2999392Z Nov 25 23:46:14 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2021-11-25T23:46:14.3000827Z Nov 25 23:46:14 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:249)
2021-11-25T23:46:14.3002121Z Nov 25 23:46:14 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:239)
2021-11-25T23:46:14.3003278Z Nov 25 23:46:14 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:230)
2021-11-25T23:46:14.3004419Z Nov 25 23:46:14 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:693)
2021-11-25T23:46:14.3005461Z Nov 25 23:46:14 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2021-11-25T23:46:14.3006533Z Nov 25 23:46:14 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:445)
2021-11-25T23:46:14.3007453Z Nov 25 23:46:14 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-11-25T23:46:14.3008353Z Nov 25 23:46:14 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-11-25T23:46:14.3009392Z Nov 25 23:46:14 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-11-25T23:46:14.3010303Z Nov 25 23:46:14 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-11-25T23:46:14.3011416Z Nov 25 23:46:14 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
2021-11-25T23:46:14.3012578Z Nov 25 23:46:14 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2021-11-25T23:46:14.3013702Z Nov 25 23:46:14 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
2021-11-25T23:46:14.3014755Z Nov 25 23:46:14 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2021-11-25T23:46:14.3015807Z Nov 25 23:46:14 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2021-11-25T23:46:14.3016889Z Nov 25 23:46:14 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2021-11-25T23:46:14.3017834Z Nov 25 23:46:14 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2021-11-25T23:46:14.3018727Z Nov 25 23:46:14 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2021-11-25T23:46:14.3019613Z Nov 25 23:46:14 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2021-11-25T23:46:14.3020567Z Nov 25 23:46:14 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2021-11-25T23:46:14.3021568Z Nov 25 23:46:14 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2021-11-25T23:46:14.3022656Z Nov 25 23:46:14 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-11-25T23:46:14.3023575Z Nov 25 23:46:14 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2021-11-25T23:46:14.3024480Z Nov 25 23:46:14 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2021-11-25T23:46:14.3025367Z Nov 25 23:46:14 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2021-11-25T23:46:14.3026191Z Nov 25 23:46:14 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2021-11-25T23:46:14.3027057Z Nov 25 23:46:14 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2021-11-25T23:46:14.3027946Z Nov 25 23:46:14 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2021-11-25T23:46:14.3028801Z Nov 25 23:46:14 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2021-11-25T23:46:14.3029621Z Nov 25 23:46:14 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2021-11-25T23:46:14.3030313Z Nov 25 23:46:14 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2021-11-25T23:46:14.3030846Z Nov 25 23:46:14 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2021-11-25T23:46:14.3031620Z Nov 25 23:46:14 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2021-11-25T23:46:14.3032371Z Nov 25 23:46:14 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2021-11-25T23:46:14.3033045Z Nov 25 23:46:14 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2021-11-25T23:46:14.3033654Z Nov 25 23:46:14 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2021-11-25T23:46:14.3034261Z Nov 25 23:46:14 Caused by: java.lang.RuntimeException: One or more fetchers have encountered exception
2021-11-25T23:46:14.3034944Z Nov 25 23:46:14 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)
2021-11-25T23:46:14.3035668Z Nov 25 23:46:14 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
2021-11-25T23:46:14.3036368Z Nov 25 23:46:14 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)
2021-11-25T23:46:14.3037031Z Nov 25 23:46:14 	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:350)
2021-11-25T23:46:14.3037687Z Nov 25 23:46:14 	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
2021-11-25T23:46:14.3038530Z Nov 25 23:46:14 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
2021-11-25T23:46:14.3039215Z Nov 25 23:46:14 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:516)
2021-11-25T23:46:14.3039902Z Nov 25 23:46:14 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
2021-11-25T23:46:14.3040671Z Nov 25 23:46:14 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:837)
2021-11-25T23:46:14.3041388Z Nov 25 23:46:14 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:786)
2021-11-25T23:46:14.3042006Z Nov 25 23:46:14 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:947)
2021-11-25T23:46:14.3042626Z Nov 25 23:46:14 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:926)
2021-11-25T23:46:14.3043202Z Nov 25 23:46:14 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:740)
2021-11-25T23:46:14.3043750Z Nov 25 23:46:14 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
2021-11-25T23:46:14.3044242Z Nov 25 23:46:14 	at java.lang.Thread.run(Thread.java:748)
2021-11-25T23:46:14.3044787Z Nov 25 23:46:14 Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
2021-11-25T23:46:14.3045664Z Nov 25 23:46:14 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150)
2021-11-25T23:46:14.3046354Z Nov 25 23:46:14 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)
2021-11-25T23:46:14.3046977Z Nov 25 23:46:14 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2021-11-25T23:46:14.3047544Z Nov 25 23:46:14 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-11-25T23:46:14.3048115Z Nov 25 23:46:14 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2021-11-25T23:46:14.3048721Z Nov 25 23:46:14 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2021-11-25T23:46:14.3049196Z Nov 25 23:46:14 	... 1 more
2021-11-25T23:46:14.3051131Z Nov 25 23:46:14 Caused by: java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2021-11-25T23:46:14.3052282Z Nov 25 23:46:14 	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:164)
2021-11-25T23:46:14.3053585Z Nov 25 23:46:14 	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.getResource(FlinkUserCodeClassLoaders.java:183)
2021-11-25T23:46:14.3054438Z Nov 25 23:46:14 	at org.apache.hadoop.conf.Configuration.getResource(Configuration.java:2780)
2021-11-25T23:46:14.3055033Z Nov 25 23:46:14 	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3036)
2021-11-25T23:46:14.3055635Z Nov 25 23:46:14 	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2995)
2021-11-25T23:46:14.3056208Z Nov 25 23:46:14 	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2968)
2021-11-25T23:46:14.3056790Z Nov 25 23:46:14 	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2848)
2021-11-25T23:46:14.3057360Z Nov 25 23:46:14 	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1200)
2021-11-25T23:46:14.3057922Z Nov 25 23:46:14 	at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1254)
2021-11-25T23:46:14.3058500Z Nov 25 23:46:14 	at org.apache.hadoop.conf.Configuration.getInt(Configuration.java:1479)
2021-11-25T23:46:14.3059089Z Nov 25 23:46:14 	at org.apache.parquet.hadoop.codec.SnappyCodec.createInputStream(SnappyCodec.java:75)
2021-11-25T23:46:14.3059737Z Nov 25 23:46:14 	at org.apache.parquet.hadoop.CodecFactory$HeapBytesDecompressor.decompress(CodecFactory.java:109)
2021-11-25T23:46:14.3060452Z Nov 25 23:46:14 	at org.apache.parquet.hadoop.ColumnChunkPageReadStore$ColumnChunkPageReader.readDictionaryPage(ColumnChunkPageReadStore.java:196)
2021-11-25T23:46:14.3061487Z Nov 25 23:46:14 	at org.apache.flink.formats.parquet.vector.reader.AbstractColumnReader.<init>(AbstractColumnReader.java:110)
2021-11-25T23:46:14.3062225Z Nov 25 23:46:14 	at org.apache.flink.formats.parquet.vector.reader.IntColumnReader.<init>(IntColumnReader.java:33)
2021-11-25T23:46:14.3062928Z Nov 25 23:46:14 	at org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.createColumnReader(ParquetSplitReaderUtil.java:280)
2021-11-25T23:46:14.3063705Z Nov 25 23:46:14 	at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat$ParquetReader.readNextRowGroup(ParquetVectorizedInputFormat.java:412)
2021-11-25T23:46:14.3064496Z Nov 25 23:46:14 	at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat$ParquetReader.nextBatch(ParquetVectorizedInputFormat.java:381)
2021-11-25T23:46:14.3065272Z Nov 25 23:46:14 	at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat$ParquetReader.readBatch(ParquetVectorizedInputFormat.java:358)
2021-11-25T23:46:14.3066013Z Nov 25 23:46:14 	at org.apache.flink.table.filesystem.LimitableBulkFormat$LimitableReader.readBatch(LimitableBulkFormat.java:119)
2021-11-25T23:46:14.3066707Z Nov 25 23:46:14 	at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:67)
2021-11-25T23:46:14.3067363Z Nov 25 23:46:14 	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
2021-11-25T23:46:14.3068019Z Nov 25 23:46:14 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)
2021-11-25T23:46:14.3068530Z Nov 25 23:46:14 	... 6 more
2021-11-25T23:46:14.3068843Z Nov 25 23:46:14 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27093&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=11441;;;","26/Nov/21 08:07;trohrmann;It is actually the same problem that has been reported so far. Hence, I believe that this test has not been fixed.;;;","26/Nov/21 08:34;lzljs3620320;[~trohrmann] Thanks, I created FLINK-25071 and will ignore this test.;;;","26/Nov/21 15:53;trohrmann;Can you post the sha with which you ignored the test and then close this ticket [~lzljs3620320]?;;;","29/Nov/21 01:51;lzljs3620320;Ignore case in:

master(1.15): c1b119290bea3bcf8de85b357a357e6181edb040;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix PartitionPruner code gen compile fail,FLINK-24761,13409987,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tartarus,tartarus,tartarus,04/Nov/21 07:54,15/Dec/21 01:44,13/Jul/23 08:12,09/Nov/21 03:45,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Table SQL / Planner,Table SQL / Runtime,,,,0,pull-request-available,,,,"{color:#4c9aff}PartitionPruner{color} compile code use {color:#de350b}AppClassLoader{color} (Obtained by getClass.getClassLoader)
 but {color:#4c9aff}org.apache.flink.table.functions.hive.HiveGenericUDF {color:#172b4d}is in user's jar, so classloader is {color:#de350b}UserCodeClassLoader{color},  {color}{color}

So compile fail.

we need change 
{code:java}
val function = genFunction.newInstance(getClass.getClassLoader)
{code}
to
{code:java}
val function = genFunction.newInstance(Thread.currentThread().getContextClassLoader)
{code}

The following is the error message:
{code:java}
org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:76) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:98) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.utils.PartitionPruner$.prunePartitions(PartitionPruner.scala:112) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.utils.PartitionPruner.prunePartitions(PartitionPruner.scala) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.lambda$onMatch$3(PushPartitionIntoTableSourceScanRule.java:163) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.readPartitionFromCatalogWithoutFilterAndPrune(PushPartitionIntoTableSourceScanRule.java:373) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.readPartitionFromCatalogAndPrune(PushPartitionIntoTableSourceScanRule.java:351) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.readPartitionsAndPrune(PushPartitionIntoTableSourceScanRule.java:303) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.onMatch(PushPartitionIntoTableSourceScanRule.java:171) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:63) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:60) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.collection.Iterator$class.foreach(Iterator.scala:891) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:60) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:55) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.collection.immutable.Range.foreach(Range.scala:160) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:55) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.collection.Iterator$class.foreach(Iterator.scala:891) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:87) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at scala.collection.immutable.List.foreach(List.scala:392) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:46) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:279) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1518) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:740) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:856) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:730) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at batchProcess.BatchSqlJob.main(BatchSqlJob.java:74) ~[xxxxxx-batch-streaming-integration-1.0-SNAPSHOT_v_1.jar:1.0-SNAPSHOT]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_102]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_102]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_102]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_102]
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:357) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:223) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:812) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:246) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1054) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1132) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_102]
	at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_102]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796) [hadoop-common-2.6.0U41.3-cdh5.10.0-hotfix1.jar:?]
	at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) [flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1132) [flink-dist_2.11-1.13.1.jar:1.13.1]
Caused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	... 77 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:89) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	... 77 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 32, Column 30: Cannot determine simple type name ""org""
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6833) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.access$1300(UnitCompiler.java:215) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$25.getType(UnitCompiler.java:8271) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6873) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.access$14400(UnitCompiler.java:215) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$22$2$1.visitFieldAccess(UnitCompiler.java:6499) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$22$2$1.visitFieldAccess(UnitCompiler.java:6494) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.Java$FieldAccess.accept(Java.java:4310) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6494) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6490) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6490) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6469) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.Java$Rvalue.accept(Java.java:4116) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6855) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.access$14200(UnitCompiler.java:215) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$22$2$1.visitAmbiguousName(UnitCompiler.java:6497) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$22$2$1.visitAmbiguousName(UnitCompiler.java:6494) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4224) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6494) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6490) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6490) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6469) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.Java$Rvalue.accept(Java.java:4116) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9026) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5062) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3783) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.access$5900(UnitCompiler.java:215) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3762) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3734) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) ~[flink-table_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	... 77 more
2021-11-04 11:51:51,230 DEBUG org.apache.flink.table.runtime.generated.CompileUtils        [] - Compiling: PartitionPruner$23 

 Code:

      public class PartitionPruner$23
          extends org.apache.flink.api.common.functions.RichMapFunction {

        private transient org.apache.flink.table.runtime.typeutils.StringDataSerializer typeSerializer$7;
        
        private final org.apache.flink.table.data.binary.BinaryStringData str$9 = org.apache.flink.table.data.binary.BinaryStringData.fromString(""20210927"");
                   
        
        private final org.apache.flink.table.data.binary.BinaryStringData str$12 = org.apache.flink.table.data.binary.BinaryStringData.fromString(""product_series"");
                   
        private transient org.apache.flink.table.functions.hive.HiveGenericUDF function_org$apache$flink$table$functions$hive$HiveGenericUDF$00ff86955cfcc960a686ea8c68f6af97;
        private transient org.apache.flink.table.data.conversion.StringStringConverter converter$15;
        private transient org.apache.flink.table.data.conversion.StringStringConverter converter$16;
        
        private final org.apache.flink.table.data.binary.BinaryStringData str$19 = org.apache.flink.table.data.binary.BinaryStringData.fromString(""KS"");

        public PartitionPruner$23(Object[] references) throws Exception {
          typeSerializer$7 = (((org.apache.flink.table.runtime.typeutils.StringDataSerializer) references[0]));
        function_org$apache$flink$table$functions$hive$HiveGenericUDF$00ff86955cfcc960a686ea8c68f6af97 = (((org.apache.flink.table.functions.hive.HiveGenericUDF) references[1]));
          converter$15 = (((org.apache.flink.table.data.conversion.StringStringConverter) references[2]));
          converter$16 = (((org.apache.flink.table.data.conversion.StringStringConverter) references[3]));
        }

        @Override
        public void open(org.apache.flink.configuration.Configuration parameters) throws Exception {
function_org$apache$flink$table$functions$hive$HiveGenericUDF$00ff86955cfcc960a686ea8c68f6af97.open(new org.apache.flink.table.functions.ConstantFunctionContext(parameters));
          converter$15.open(this.getClass().getClassLoader());
          converter$16.open(this.getClass().getClassLoader());      
        }

        @Override
        public Object map(Object _in1) throws Exception {
          org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) _in1;
          
          org.apache.flink.table.data.binary.BinaryStringData field$6;
          boolean isNull$6;
          org.apache.flink.table.data.binary.BinaryStringData field$8;
          boolean isNull$10;
          boolean result$11;
          org.apache.flink.table.data.binary.BinaryStringData field$13;
          boolean isNull$13;
          org.apache.flink.table.data.binary.BinaryStringData field$14;
          java.lang.String externalResult$17;
          org.apache.flink.table.data.binary.BinaryStringData result$18;
          boolean isNull$18;
          boolean isNull$20;
          boolean result$21;
         
          isNull$6 = in1.isNullAt(0);
          field$6 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
          if (!isNull$6) {
            field$6 = ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(0));
          }
          field$8 = field$6;
          if (!isNull$6) {
            field$8 = (org.apache.flink.table.data.binary.BinaryStringData) (typeSerializer$7.copy(field$8));
          }
          isNull$13 = in1.isNullAt(1);
          field$13 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
          if (!isNull$13) {
            field$13 = ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(1));
          }
          field$14 = field$13;
          if (!isNull$13) {
            field$14 = (org.apache.flink.table.data.binary.BinaryStringData) (typeSerializer$7.copy(field$14));
          }
          isNull$10 = isNull$6 || false;
          result$11 = false;
          if (!isNull$10) {
            result$11 = field$8.equals(((org.apache.flink.table.data.binary.BinaryStringData) str$9));
          }
          boolean result$22 = false;
          boolean isNull$22 = false;
          if (!isNull$10 && !result$11) {
            // left expr is false, skip right expr
          } else {
          externalResult$17 = (java.lang.String) function_org$apache$flink$table$functions$hive$HiveGenericUDF$00ff86955cfcc960a686ea8c68f6af97
            .eval(false ? null : ((java.lang.String) converter$15.toExternal((org.apache.flink.table.data.binary.BinaryStringData) ((org.apache.flink.table.data.binary.BinaryStringData) str$12))), isNull$6 ? null : ((java.lang.String) converter$16.toExternal((org.apache.flink.table.data.binary.BinaryStringData) field$8)), isNull$13 ? null : ((java.lang.String) converter$16.toExternal((org.apache.flink.table.data.binary.BinaryStringData) field$14)));
          isNull$18 = externalResult$17 == null;
          result$18 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
          if (!isNull$18) {
            result$18 = (org.apache.flink.table.data.binary.BinaryStringData) converter$16.toInternalOrNull((java.lang.String) externalResult$17);
          }
          isNull$20 = isNull$18 || false;
          result$21 = false;
          if (!isNull$20) {
            result$21 = result$18.equals(((org.apache.flink.table.data.binary.BinaryStringData) str$19));
          }
            if (!isNull$10 && !isNull$20) {
              result$22 = result$11 && result$21;
              isNull$22 = false;
            }
            else if (!isNull$10 && result$11 && isNull$20) {
              result$22 = false;
              isNull$22 = true;
            }
            else if (!isNull$10 && !result$11 && isNull$20) {
              result$22 = false;
              isNull$22 = false;
            }
            else if (isNull$10 && !isNull$20 && result$21) {
              result$22 = false;
              isNull$22 = true;
            }
            else if (isNull$10 && !isNull$20 && !result$21) {
              result$22 = false;
              isNull$22 = false;
            }
            else {
              result$22 = false;
              isNull$22 = true;
            }
          }
          return result$22;
        }

        @Override
        public void close() throws Exception {
function_org$apache$flink$table$functions$hive$HiveGenericUDF$00ff86955cfcc960a686ea8c68f6af97.close();        
        }
      }
{code}

",,lzljs3620320,tartarus,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 09 03:45:54 UTC 2021,,,,,,,,,,"0|z0wfko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/21 07:57;tartarus;[~jark] Please assign this jira to me, thanks;;;","04/Nov/21 08:16;martijnvisser;[~agen-taruhan-bola] I've assigned it to you;;;","04/Nov/21 08:24;TsReaper;Thanks for raising this issue [~tartarus] and a nice catch! Please ping me (@tsreaper) in your github PR so that I can help for the review.;;;","09/Nov/21 03:45;lzljs3620320;master: 08a5969069b23ae831fbc8f759c9d7fe9473ffde
release-1.14: aa6ef87029ea5f24d831dea0b89c0fa47d9e13ba
release-1.13: 04ce83574e39a3f4401c0d5f7b55cd3f4af01cd7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL client add info about key strokes to docs,FLINK-24742,13409746,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,02/Nov/21 22:05,22/Nov/22 11:20,13/Jul/23 08:12,22/Nov/22 11:20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.17.0,,,,,Table SQL / Client,,,,,0,auto-deprioritized-major,pull-request-available,,,"SQL client supports key strokes from jline.
Unfortunately there is no docs about that in jline however there is source from which it could be found [1]
here it is a list of most useful key strokes which are already supported by all existing Flink SQL client

|| Key-Stroke || Description ||
| `alt-b` | Backward word |
| `alt-f` | Forward word |
| `alt-c` | Capitalize word |
| `alt-l` | Lowercase word |
| `alt-u` | Uppercase word |
| `alt-d` | Kill word |
| `alt-n` | History search forward |
| `alt-p` | History search backward |
| `alt-t` | Transpose words |
| `ctrl-a` | To the beginning of line |
| `ctrl-e` | To the end of line |
| `ctrl-b` | Backward char |
| `ctrl-f` | Forward char |
| `ctrl-d` | Delete char |
| `ctrl-h` | Backward delete char |
| `ctrl-t` | Transpose chars |
| `ctrl-i` | Invoke completion |
| `ctrl-j` | Submit a query |
| `ctrl-m` | Submit a query |
| `ctrl-k` | Kill the line to the right from the cursor |
| `ctrl-w` | Kill the line to the left from the cursor |
| `ctrl-u` | Kill the whole line |
| `ctrl-l` | Clear screen |
| `ctrl-n` | Down line from history |
| `ctrl-p` | Up line from history |
| `ctrl-r` | History incremental search backward |
| `ctrl-s` | History incremental search forward |


[1] https://github.com/jline/jline3/blob/997496e6a6338ca5d82c7dec26f32cf089dd2838/reader/src/main/java/org/jline/reader/impl/LineReaderImpl.java#L5907",,jark,mapohl,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 22 11:20:05 UTC 2022,,,,,,,,,,"0|z0we3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/22 10:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","04/Feb/22 10:37;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","22/Nov/22 11:20;mapohl;master: f21090b529d5781dd15d403df054937b21f376df;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail during announcing buffer size to released local channel,FLINK-24738,13409680,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,akalashnikov,akalashnikov,02/Nov/21 14:08,15/Dec/21 01:44,13/Jul/23 08:12,10/Nov/21 13:10,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,"Since we can release all resources not only when the mailbox would be finished but also from the mailbox:
{noformat}
org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.releaseAllResources(LocalInputChannel.java:331)
org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.transformEvent(SingleInputGate.java:808)
org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.transformToBufferOrEvent(SingleInputGate.java:757)
org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:687)
org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.pollNext(SingleInputGate.java:666)
org.apache.flink.runtime.taskmanager.InputGateWithMetrics.pollNext(InputGateWithMetrics.java:142)
org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:150)
org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:503)
org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:816)
org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:768)
org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:936)
org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:750)
org.apache.flink.runtime.taskmanager.Task.run(Task.java:571)
{noformat}
It is possible that after it the BufferDebloater announce the new buffer size which will fail because the channel is released already:
{noformat}
Caused by: java.lang.IllegalStateException: Channel released.
	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
	at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.announceBufferSize(LocalInputChannel.java:354)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.announceBufferSize(SingleInputGate.java:389)
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.announceBufferSize(InputGateWithMetrics.java:102)
	at org.apache.flink.streaming.runtime.tasks.bufferdebloat.BufferDebloater.recalculateBufferSize(BufferDebloater.java:101)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.debloat(StreamTask.java:801)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$null$4(StreamTask.java:791)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:338)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:816)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:768)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:936)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:750)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:571)
	at java.lang.Thread.run(Thread.java:748)
{noformat}
 So I think that we should replace `checkState` with `if` for LocalInputChannel#announceBufferSize since released channel is expected here.",,akalashnikov,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 08 07:57:32 UTC 2021,,,,,,,,,,"0|z0wdoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/21 07:57;dwysakowicz;Merged in:
* master
** 138e83e2bc6feb59ecd839b03491cd8f23ee1c66
* 1.14.1
** d1a3ad01a96d2d565c5311e4d786917d0c71e8af;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL client crashes with `Cannot add expression of different type to set`,FLINK-24735,13409654,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,martijnvisser,martijnvisser,02/Nov/21 11:46,23/May/22 11:31,13/Jul/23 08:12,23/May/22 11:31,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.1,1.16.0,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"Reproductions steps:

1. Download airports.csv from https://www.kaggle.com/usdot/flight-delays
2. Start Flink SQL client and create table

{code:sql}
CREATE TABLE `airports` (
  `IATA_CODE` CHAR(3),
  `AIRPORT` STRING,
  `CITY` STRING,
  `STATE` CHAR(2),
  `COUNTRY` CHAR(3),
  `LATITUDE` DOUBLE NULL,
  `LONGITUDE` DOUBLE NULL,
  PRIMARY KEY (`IATA_CODE`) NOT ENFORCED
) WITH (
  'connector' = 'filesystem',
  'path' = 'file:///flink-sql-cookbook/other-builtin-functions/04_override_table_options/airports.csv',
  'format' = 'csv'
);
{code}

3. Run the following SQL statement:

{code:sql}
SELECT * FROM `airports` /*+ OPTIONS('csv.ignore-parse-errors'='true') */ WHERE COALESCE(`IATA_CODE`, `AIRPORT`) IS NULL;
{code}

Stacktrace:

{code:bash}
Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.
	at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:201)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161)
Caused by: java.lang.AssertionError: Cannot add expression of different type to set:
set type is RecordType(CHAR(3) CHARACTER SET ""UTF-16LE"" NOT NULL IATA_CODE, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" AIRPORT, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" CITY, CHAR(2) CHARACTER SET ""UTF-16LE"" STATE, CHAR(3) CHARACTER SET ""UTF-16LE"" COUNTRY, DOUBLE LATITUDE, DOUBLE LONGITUDE) NOT NULL
expression type is RecordType(CHAR(3) CHARACTER SET ""UTF-16LE"" IATA_CODE, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" AIRPORT, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" CITY, CHAR(2) CHARACTER SET ""UTF-16LE"" STATE, CHAR(3) CHARACTER SET ""UTF-16LE"" COUNTRY, DOUBLE LATITUDE, DOUBLE LONGITUDE) NOT NULL
set is rel#426:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#425,inputs=0..6)
expression is LogicalProject(IATA_CODE=[null:CHAR(3) CHARACTER SET ""UTF-16LE""], AIRPORT=[$1], CITY=[$2], STATE=[$3], COUNTRY=[$4], LATITUDE=[$5], LONGITUDE=[$6])
  LogicalFilter(condition=[IS NULL(CAST($0):VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"")])
    LogicalTableScan(table=[[default_catalog, default_database, airports]], hints=[[[OPTIONS inheritPath:[] options:{csv.ignore-parse-errors=true}]]])

	at org.apache.calcite.plan.RelOptUtil.verifyTypeEquivalence(RelOptUtil.java:381)
	at org.apache.calcite.plan.hep.HepRuleCall.transformTo(HepRuleCall.java:58)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:268)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:283)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:310)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:62)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:58)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:163)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:81)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:300)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:183)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1665)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:805)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1274)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeOperation$3(LocalExecutor.java:209)
	at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:88)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:209)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeQuery(LocalExecutor.java:231)
	at org.apache.flink.table.client.cli.CliClient.callSelect(CliClient.java:532)
	at org.apache.flink.table.client.cli.CliClient.callOperation(CliClient.java:423)
	at org.apache.flink.table.client.cli.CliClient.lambda$executeStatement$1(CliClient.java:332)
	at java.util.Optional.ifPresent(Optional.java:159)
	at org.apache.flink.table.client.cli.CliClient.executeStatement(CliClient.java:325)
	at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:297)
	at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:221)
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:151)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:95)
	at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187)
	... 1 more
{code}

Any exception/error should be wrapped into a SqlExecutionException (in the executor) or SqlClientException in the highest layer, which is not happening in this case. ",,fsk119,godfreyhe,jark,leonard,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 23 07:59:16 UTC 2022,,,,,,,,,,"0|z0wdiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/22 07:42;fsk119;The main cause is the sql-client doesn't catch the error thrown by the planner, which causes the sql client crash. We can just catch the root of the Error and Exception, that is, we catch the Throwable in the Sql client.;;;","23/May/22 07:59;leonard;Fixed in:
       master : cdbc3f61583b339f508c54717a34bad16a00a681
       release-1.15: 9a4ca4c4bfd4740ec0ec9d008450845acf6c8bc7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testSecretOption fails due to missing required options,FLINK-24734,13409652,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,akalashnikov,akalashnikov,02/Nov/21 11:22,15/Dec/21 01:41,13/Jul/23 08:12,02/Nov/21 11:56,1.13.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,,,,,,,,,,0,,,,,"{noformat}
java.lang.AssertionError: Expected: (an instance of org.apache.flink.table.api.ValidationException and Expected failure cause is <org.apache.flink.table.api.ValidationException: Table options are:
'buffer-size'='1000''connector'='test-connector''key.format'='test-format''key.test-format.delimiter'=',''password'='******''property-version'='1''value.format'='test-format''value.test-format.delimiter'='|''value.test-format.fail-on-missing'='true'>)     but: Expected failure cause is <org.apache.flink.table.api.ValidationException: Table options are:
'buffer-size'='1000''connector'='test-connector''key.format'='test-format''key.test-format.delimiter'=',''password'='******''property-version'='1''value.format'='test-format''value.test-format.delimiter'='|''value.test-format.fail-on-missing'='true'> The throwable <org.apache.flink.table.api.ValidationException: Unable to create a source for reading table 'default.default.t1'.
Table options are:
'buffer-size'='1000''connector'='test-connector''key.format'='test-format''key.test-format.delimiter'=',''password'='******''property-version'='1''value.format'='test-format''value.test-format.delimiter'='|''value.test-format.fail-on-missing'='true'> does not contain the expected failure cause <org.apache.flink.table.api.ValidationException: Table options are:
'buffer-size'='1000''connector'='test-connector''key.format'='test-format''key.test-format.delimiter'=',''password'='******''property-version'='1''value.format'='test-format''value.test-format.delimiter'='|''value.test-format.fail-on-missing'='true'>Stacktrace was: org.apache.flink.table.api.ValidationException: Unable to create a source for reading table 'default.default.t1'.
Table options are:
'buffer-size'='1000''connector'='test-connector''key.format'='test-format''key.test-format.delimiter'=',''password'='******''property-version'='1''value.format'='test-format''value.test-format.delimiter'='|''value.test-format.fail-on-missing'='true' at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:140) at org.apache.flink.table.factories.utils.FactoryMocks.createTableSource(FactoryMocks.java:58) at org.apache.flink.table.factories.FactoryUtilTest.testError(FactoryUtilTest.java:374) at org.apache.flink.table.factories.FactoryUtilTest.testSecretOption(FactoryUtilTest.java:162) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)Caused by: org.apache.flink.table.api.ValidationException: One or more required options are missing.
Missing required options are:
target at org.apache.flink.table.factories.FactoryUtil.validateFactoryOptions(FactoryUtil.java:384) at org.apache.flink.table.factories.FactoryUtil.validateFactoryOptions(FactoryUtil.java:357) at org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.validate(FactoryUtil.java:718) at org.apache.flink.table.factories.TestDynamicTableFactory.createDynamicTableSource(TestDynamicTableFactory.java:80) at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:137) ... 27 more

at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.junit.Assert.assertThat(Assert.java:956) at org.junit.Assert.assertThat(Assert.java:923) at org.junit.rules.ExpectedException.handleException(ExpectedException.java:252) at org.junit.rules.ExpectedException.access$000(ExpectedException.java:106) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:241) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)

Process finished with exit code 255

{noformat}",,akalashnikov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24381,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 02 11:56:27 UTC 2021,,,,,,,,,,"0|z0wdig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/21 11:30;chesnay;[~akalashnikov] Did this happen on CI (if so, please link the build)?;;;","02/Nov/21 11:41;akalashnikov;[~chesnay], oh yes sorry, I forgot about the link:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25714&view=logs&j=b2f046ab-ae17-5406-acdc-240be7e870e4&t=93e5ae06-d194-513d-ba8d-150ef6da1d7c&l=6933;;;","02/Nov/21 11:56;chesnay;Reverted FLINK-24381.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data loss in pulsar source when using shared mode,FLINK-24733,13409651,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,shengzhi,shengzhi,02/Nov/21 11:16,15/Dec/21 01:44,13/Jul/23 08:12,11/Nov/21 10:14,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Connectors / Pulsar,,,,,0,pull-request-available,,,,"Description：
 Noticed that when using Flink-connector-pulsar with Shared mode (default mode), some messages are lost. 
Details：
 When a topic partition does not receive message for more than 10s (pulsar.source.maxFetchTime), the next coming message will be dropped by source.
In stream applications, this situation which there is no data for more than 10s is very common. So this bug will cause data loss.",,martijnvisser,shengzhi,syhily,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23944,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 11 10:14:05 UTC 2021,,,,,,,,,,"0|z0wdi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/21 11:50;martijnvisser;[~syhily] What are your thoughts on this one?;;;","02/Nov/21 12:12;shengzhi;[~syhily] [~MartijnVisser] I have fixed this bug, please check it out.;;;","02/Nov/21 14:47;syhily;[~shengzhi] Tks for your fix. I'm digging into the Pulsar client API for making sure this fix is OK.
I think you may need to add another fix for the ordered reader.;;;","02/Nov/21 16:27;syhily;[~shengzhi] The fix didn't touch the core of data loss. We need more time to investigate why we meet data loss in low data rates.;;;","03/Nov/21 06:57;shengzhi;[~syhily] I think that the wrong understand of 'CompletableFutureCancellationHandler' leads to the data lost. Refer to what I said in the PR link. https://github.com/apache/flink/pull/17643;;;","11/Nov/21 10:14;arvid;Merged into master as 384af4ac47bc8ef30bed78fdfcea677a7d257ac0, merged into 1.14 as 6a319e421b848f20ea67f54ee2eaa9e1c73d91c4.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch SQL file sink forgets to close the output stream,FLINK-24728,13409604,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,02/Nov/21 07:58,17/Dec/21 05:39,13/Jul/23 08:12,17/Dec/21 05:39,1.11.4,1.12.5,1.13.3,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"I tried to write a large avro file into HDFS and discover that the displayed file size in HDFS is extremely small, but copying that file to local yields the correct size. If we create another Flink job and read that avro file from HDFS, the job will finish without outputting any record because the file size Flink gets from HDFS is the very small file size.

This is because the output format created in {{FileSystemTableSink#createBulkWriterOutputFormat}} only finishes the {{BulkWriter}}. According to the java doc of {{BulkWriter#finish}} bulk writers should not close the output stream and should leave them to the framework.",,libenchao,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 17 05:39:35 UTC 2021,,,,,,,,,,"0|z0wd7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/21 07:23;lzljs3620320;[~TsReaper] Can you cherry-pick for 1.14 and 1.13? Two commits in one PR is OK.;;;","17/Dec/21 05:39;lzljs3620320;master:

6af0b9965293cb732a540b9364b6aae76a9b356a

2e9f9ad166f472edd693c8a47857e14e76928dc9

release-1.14:

9a0c5e00839983de23f662e337dfc626d0bdaad9

11d24708be32605243bc404679b17758c4e76e79

release-1.13:

3200e8ef43b3024b0b44f184dfa833d1aa7d7d75;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`ConvertToNotInOrInRule` has a bug which leads to wrong result,FLINK-24708,13409144,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,jingzhang,jingzhang,29/Oct/21 15:58,30/Aug/22 07:26,13/Jul/23 08:12,23/Nov/21 04:36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"A user report this bug in maillist, I paste the content here.

We are in the process of upgrading from Flink 1.9.3 to 1.13.3.  We have noticed that statements with either where UPPER(field) or LOWER(field) in combination with an IN do not always evaluate correctly. 

 

The following test case highlights this problem.

 

 
{code:java}
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.Schema;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;

public class TestCase {
    public static void main(String[] args) throws Exception {
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        TestData testData = new TestData();
        testData.setField1(""bcd"");
        DataStream<TestData> stream = env.fromElements(testData);
        stream.print();  // To prevent 'No operators' error

        final StreamTableEnvironment tableEnvironment = StreamTableEnvironment.create(env);
        tableEnvironment.createTemporaryView(""testTable"", stream, Schema.newBuilder().build());

        // Fails because abcd is larger than abc
        tableEnvironment.executeSql(""select *, '1' as run from testTable WHERE lower(field1) IN ('abcd', 'abc', 'bcd', 'cde')"").print();
        // Succeeds because lower was removed
        tableEnvironment.executeSql(""select *, '2' as run from testTable WHERE field1 IN ('abcd', 'abc', 'bcd', 'cde')"").print();
        // These 4 succeed because the smallest literal is before abcd
        tableEnvironment.executeSql(""select *, '3' as run from testTable WHERE lower(field1) IN ('abc', 'abcd', 'bcd', 'cde')"").print();
        tableEnvironment.executeSql(""select *, '4' as run from testTable WHERE lower(field1) IN ('abc', 'bcd', 'abhi', 'cde')"").print();
        tableEnvironment.executeSql(""select *, '5' as run from testTable WHERE lower(field1) IN ('cde', 'abcd', 'abc', 'bcd')"").print();
        tableEnvironment.executeSql(""select *, '6' as run from testTable WHERE lower(field1) IN ('cde', 'abc', 'abcd', 'bcd')"").print();
        // Fails because smallest is not first
        tableEnvironment.executeSql(""select *, '7' as run from testTable WHERE lower(field1) IN ('cdef', 'abce', 'abcd', 'ab', 'bcd')"").print();
        // Succeeds
        tableEnvironment.executeSql(""select *, '8' as run from testTable WHERE lower(field1) IN ('ab', 'cdef', 'abce', 'abcdefgh', 'bcd')"").print();

        env.execute(""TestCase"");
    }

    public static class TestData {
        private String field1;

        public String getField1() {
            return field1;
        }

        public void setField1(String field1) {
            this.field1 = field1;
        }
    }
}
{code}
 

The job produces the following output:

Empty set

+-----+-------------------------------++--------------------------------
|op|                         field1|                            run|

+-----+-------------------------------++--------------------------------
|+I|                            bcd|                              2|

+-----+-------------------------------++--------------------------------

1 row in set

+-----+-------------------------------++--------------------------------
|op|                         field1|                            run|

+-----+-------------------------------++--------------------------------
|+I|                            bcd|                              3|

+-----+-------------------------------++--------------------------------

1 row in set

+-----+-------------------------------++--------------------------------
|op|                         field1|                            run|

+-----+-------------------------------++--------------------------------
|+I|                            bcd|                              4|

+-----+-------------------------------++--------------------------------

1 row in set

+-----+-------------------------------++--------------------------------
|op|                         field1|                            run|

+-----+-------------------------------++--------------------------------
|+I|                            bcd|                              5|

+-----+-------------------------------++--------------------------------

1 row in set

+-----+-------------------------------++--------------------------------
|op|                         field1|                            run|

+-----+-------------------------------++--------------------------------
|+I|                            bcd|                              6|

+-----+-------------------------------++--------------------------------

1 row in set

Empty set

+-----+-------------------------------++--------------------------------
|op|                         field1|                            run|

+-----+-------------------------------++--------------------------------
|+I|                            bcd|                              8|

+-----+-------------------------------++--------------------------------

1 row in set

 

 ",,godfreyhe,hackergin,jark,jingzhang,lam167,libenchao,sharonxr55,tartarus,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CALCITE-4888,,,,,,,,,,,,"29/Oct/21 15:59;jingzhang;image-2021-10-29-23-59-48-074.png;https://issues.apache.org/jira/secure/attachment/13035482/image-2021-10-29-23-59-48-074.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 30 07:26:08 UTC 2022,,,,,,,,,,"0|z0wads:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Oct/21 16:00;jingzhang;[~TsReaper] cc [~godfrey] [~godfreyhe]  It seems to be a bug in `ConvertToNotInOrInRule`.
For query 1 and query 7, after apply `ConvertToNotInOrInRule`  in 'default_rewrite' phase, 'abc','bcd','cde' would be cast to CHAR(4) which leads to empty result, because 'bcd' != 'bcd '.
!image-2021-10-29-23-59-48-074.png!;;;","31/Oct/21 15:16;godfreyhe;Thanks for reporting this bug. [~qingru zhang] Currently the first element's type of Range is used as the element type for the Search node when calling RexBuilder#makeIn. We need to fix RexBuilder#makeIn. Search is introduced in Flink 1.12 when upgrading Calcite to 1.23;;;","15/Nov/21 02:16;jingzhang;[~godfreyhe] Thanks for response. BTW,  I also created a Jira in Calcite: https://issues.apache.org/jira/browse/CALCITE-4888

 ;;;","23/Nov/21 04:36;godfreyhe;Fixed in:
1.15.0: 90e850301e672fc0da293abc55eb446f7ec68ffa
1.14.1: 4315c1be1f94367058c85be82e89d1bd623c63a7
1.13.4: 39134d1c1524f6014e0ce676e471379d386bc659;;;","30/Aug/22 07:26;lam167;hello, [~jingzhang], [~godfreyhe] 

I had this problem recently too with flink 1.12.2, and fixed it locally. Then I saw the issue, and found maybe the test case you provided can not fully reproduce the error, there's a rule named `SimplifyFilterConditionRule` that would be applied optimization before the `ConvertToNotInOrInRule`, so the test case

 
{code:java}
SELECT * from MyTable where e in ('CTNBSmokeSensor','H388N','H389N     ','GHL-IRD','JY-BF-20YN','HC809','DH-9908N-AEP','DH-9908N') {code}
would alse produce the right result before this bug was fixed.
{code:java}
LogicalProject(a=[$0], b=[$1], c=[$2], d=[$3], e=[$4])
+- LogicalFilter(condition=[SEARCH($4, Sarg[_UTF-16LE'CTNBSmokeSensor':VARCHAR(15) CHARACTER SET ""UTF-16LE"", _UTF-16LE'DH-9908N':VARCHAR(15) CHARACTER SET ""UTF-16LE"", _UTF-16LE'DH-9908N-AEP':VARCHAR(15) CHARACTER SET ""UTF-16LE"", _UTF-16LE'GHL-IRD':VARCHAR(15) CHARACTER SET ""UTF-16LE"", _UTF-16LE'H388N':VARCHAR(15) CHARACTER SET ""UTF-16LE"", _UTF-16LE'H389N     ':VARCHAR(15) CHARACTER SET ""UTF-16LE"", _UTF-16LE'HC809':VARCHAR(15) CHARACTER SET ""UTF-16LE"", _UTF-16LE'JY-BF-20YN':VARCHAR(15) CHARACTER SET ""UTF-16LE""]:VARCHAR(15) CHARACTER SET ""UTF-16LE"")])
   +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c, d, e)]]]) {code}
Only when we tried to check if `f(e)` in the literals scope, which `f` is a used-defined function, `SimplifyFilterConditionRule` would be invalid, and then `ConvertToNotInOrInRule` would take an effect and reproduce the bug.

I'm not very sure if there's a need to add more tests for this problem and I just add some of my understanding of the problem. If you think this is not a problem, I'm ok with this.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AkkaInvocationHandler silently ignores deserialization errors,FLINK-24706,13409113,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,29/Oct/21 12:30,15/Dec/21 01:44,13/Jul/23 08:12,05/Nov/21 13:42,1.13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"In AkkaInvocationHandler#invokeRpc we create a new CompletableFuture to return to the caller, and setup a forwarding routine that runs when the response is received via akka.

{code}
final CompletableFuture<Object> completableFuture = new CompletableFuture<>();
resultFuture.whenComplete(
        (resultValue, failure) -> {
            if (failure != null) {
                completableFuture.completeExceptionally(
                        resolveTimeoutException(
                                failure, callStackCapture, address, rpcInvocation));
            } else {
                completableFuture.complete(
                        deserializeValueIfNeeded(resultValue, method));
            }
        });
{code}

If {{deserializeValueIfNeeded}} fails then {{completableFuture}} will never be completed, and we exception will not be logged anywhere.",,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24550,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 05 13:42:13 UTC 2021,,,,,,,,,,"0|z0wa6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/21 13:42;chesnay;master: e932aad8ff5ac953f1e73193dda22da9f242851e

1.14: 4651be9011ff6661d4de9347268a80c8317f11d9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception occurs when the input record loses monotonicity on the sort key field of UpdatableTopNFunction,FLINK-24704,13409108,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,29/Oct/21 12:01,15/Dec/21 01:44,13/Jul/23 08:12,10/Nov/21 02:08,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"An IllegalArgumentException occurred when the input retract record's sort key is lower than old sort key, this's because it breaks the monotonicity on sort key field which is guaranteed by the sql semantic. It's highly possible upstream stateful operator has shorter state ttl than the stream records is that cause the staled record cleared by state ttl. 

A reproduce case like below:
{code:java|title=RankHarnessTest.java|borderStyle=solid}
}}

val sql =
 """"""
 |SELECT word, cnt, rank_num
 |FROM (
 | SELECT word, cnt,
 | ROW_NUMBER() OVER (PARTITION BY type ORDER BY cnt DESC) as rank_num
 | FROM (
 | select word, type, sum(cnt) filter (where cnt > 0) cnt from T group by word, type
 | )
 | )
 |WHERE rank_num <= 6
 """""".stripMargin

{code}
when then aggregated result column `cnt` becomes lower for a key, then downstream retract rank operator will fail on such exception:

 
{quote}java.lang.IllegalArgumentExceptionjava.lang.IllegalArgumentException at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:122) at org.apache.flink.table.runtime.operators.rank.UpdatableTopNFunction.emitRecordsWithRowNumber(UpdatableTopNFunction.java:399) at org.apache.flink.table.runtime.operators.rank.UpdatableTopNFunction.processElementWithRowNumber(UpdatableTopNFunction.java:274) at org.apache.flink.table.runtime.operators.rank.UpdatableTopNFunction.processElement(UpdatableTopNFunction.java:167) at org.apache.flink.table.runtime.operators.rank.UpdatableTopNFunction.processElement(UpdatableTopNFunction.java:69) at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83) at org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness.processElement(OneInputStreamOperatorTestHarness.java:209)
{quote}
Here we should align with the RetractableTopNFunction, continue processing(but incorrectly result) by default or can be configured to failover after [Flink-24666|https://issues.apache.org/jira/browse/FLINK-24666] was addressed.

 ",,libenchao,lincoln.86xy,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 10 02:08:31 UTC 2021,,,,,,,,,,"0|z0wa5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/21 01:51;ykt836;Thanks for reporting this [~lincoln.86xy], will this leads to incorrect result or endless exception-failover-exception loop?;;;","01/Nov/21 03:30;lincoln.86xy;[~ykt836], yes, it will cause continuous failover and users cannot recover it only discard the current state and re-run the job.;;;","01/Nov/21 07:36;ykt836;I changed the priority to critical, and we'd better to also back port this fix to 1.14;;;","10/Nov/21 02:08;lzljs3620320;master: 33aaabebc63f7a44fb64930f5daf8006ce030752

release-1.14: d31cfb9b1b206270f5ccdb456c84d8c639e0524d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLINK SQL SUM() causes a precision error,FLINK-24691,13409048,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,matriv,Ashulin,Ashulin,29/Oct/21 06:41,15/Dec/21 01:44,13/Jul/23 08:12,09/Nov/21 08:05,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"code: 
{code:java}
tableEnv.executeSql(""select sum(cast( 1.03520274 as decimal(32, 8))) as num "").print();
{code}
expected:

1.03520274

actual:

1.03520270

 

This function is normal in version 1.13.x.

I found the code that caused the inconsistency：

org.apache.flink.table.types.logical.utils.LogicalTypeMerging#adjustPrecisionScale method was added in 1.14. Because sum() generates max precision(38) by default, it leads to special circumstances, which reduces the scale by 1.

  !image-2021-10-29-15-49-14-419.png!",,Ashulin,godfreyhe,libenchao,martijnvisser,matriv,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24809,,,,,,,,,,,,,,,,"29/Oct/21 07:49;Ashulin;image-2021-10-29-15-49-14-419.png;https://issues.apache.org/jira/secure/attachment/13035462/image-2021-10-29-15-49-14-419.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 09 08:05:52 UTC 2021,,,,,,,,,,"0|z0w9sg:",9223372036854775807,This changes the result of a decimal SUM() between 1.14.0 and 1.14.1. It restores the behavior of 1.13 to be consistent with Hive/Spark.,,,,,,,,,,,,,,,,,,,"29/Oct/21 07:02;martijnvisser;[~matriv] Do you have any thoughts on this one?;;;","29/Oct/21 07:45;Ashulin;[~matriv] org.apache.flink.table.types.logical.utils.LogicalTypeMerging#adjustPrecisionScale method was added in 1.14. Because sum() generates max precision(38) by default, it leads to special circumstances, which reduces the scale by 1.;;;","01/Nov/21 14:42;matriv;[~Ashulin] Thank you! I missed your comment, but I found it myself, working on fixing the issue and adding tests.;;;","04/Nov/21 12:20;matriv;Decided to keep the *adjustPrecisionScale* as is (follow Hive/Spark approach and not 100% SQL Server approach) and instead use an explicit *cast* to wrap the *plus* operator implementing sum to prevent overriding the precision/scale calculated specifically for *SUM*.;;;","09/Nov/21 08:05;twalthr;Fixed in master: 09b3061345e10ac043346dedf5e5ffe5cb45d091

Fixed in 1.14: bafb0b4c2377d6d502ed9dba8853631ebf16cfb7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-table uber jar should not include flink-connector-files dependency,FLINK-24687,13408993,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,slinkydeveloper,thw,thw,28/Oct/21 20:57,03/Dec/21 17:03,13/Jul/23 08:12,03/Dec/21 17:03,1.13.3,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Table SQL / API,,,,,0,pull-request-available,,,,"flink-table-common currently depends on flink-connector-files due to BulkReaderFormatFactory. Since the connector isn't relocated and the jar file is part of the distributions lib directory, it conflicts with application packaged connector dependencies, including flink-connector-base

https://lists.apache.org/thread.html/r4345a9ec53b1d5a1c3e4f6143ceb2fa0f950bd92d0266dc24b69a255%40%3Cdev.flink.apache.org%3E",,jark,lzljs3620320,thw,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 03 17:03:13 UTC 2021,,,,,,,,,,"0|z0w9g8:",9223372036854775807,"The table file system connector is not part of the table-uber JAR anymore but is a dedicated (but removable) flink-connector-files JAR in the /lib directory of a Flink distribution. The Scala suffixes of the following modules have been removed: flink-orc, flink-orc-nohive, flink-parquet.",,,,,,,,,,,,,,,,,,,"03/Nov/21 15:05;twalthr;[~lzljs3620320] you introduced this dependency in `flink-table-common`. Could we move the `BulkWriterFormatFactory` and `BulkReaderFormatFactory` to `flink-connector-files`? I don't see a reason why it need to be in `table-common`, those formats are very connector specific. Also, `flink-table-uber` should not package the file system connector. Instead, we should redirect users to a dedicated SQL JAR for reading files. We can also add the JAR to the lib directory by default. But we should give users like Thomas the possibility to opt out from this connector.

CC [~arvid];;;","04/Nov/21 02:36;lzljs3620320;I think it should be possible, because the previous file related implementations were placed in the flink-core. After the flink-connector-files was introduced, it is still a basic component and is dependent. Therefore, flink-table-common used to rely on flink-connector-files. It's OK to flip the dependency now.;;;","03/Dec/21 17:03;twalthr;commit 9bbadb9b105b233b7565af120020ebd8dce69a4f
[table][connectors] Move FileSystemTableSink, FileSystemTableSource to flink-connector-files and columnar support to flink-table-common

commit 6bb090751093e4f9f8a05c80857af11381f88599
[table-runtime] Refactored test csv format to be independent of planner (except ScanRuntimeProviderContext.INSTANCE::createDataStructureConverter) and to implement SerializationSchema more than BulkWriterFormatFactory. Moved to a specific package

commit 2ae04c24262810646675c490616fd0053d0d0107
[parquet] Copied DecimalDataUtils#is32BitDecimal and DecimalDataUtils#is32BitDecimal in ParquetSchemaConverter to remove the dependency on DecimalDataUtils (from planner)

commit d12fd3d729f772d5c545d58987049bb9ce9f1da8
[table-planner] Remove planner dependency on FileSystemConnectorOptions

commit 97237d0d86ef4610174ba8a2579341822ed8d21e
[table-common] Fix the Table Factory loading mechanism to tolerate NoClassDefFoundError. Added a test and converted FactoryUtil to use assertj.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configuration parsing exceptions may leak sensitive data,FLINK-24679,13408834,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,28/Oct/21 07:01,21/Nov/22 11:18,13/Jul/23 08:12,21/Nov/22 11:18,1.13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.17.0,,,,,Runtime / Configuration,,,,,0,pull-request-available,,,,"Various exceptions thrown by the Configuration when a parsing error occurs print the problematic value as-is.

This means a typo by the user may result in all or part of a sensitive value to be leaked.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29650,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 21 11:18:21 UTC 2022,,,,,,,,,,"0|z0w8gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/22 11:18;chesnay;master: 3d1c43cc1da1f227b409952daca70d5b663e1a5a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the metric name of map state contains latency,FLINK-24678,13408830,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,lijinzhong,yunta,yunta,28/Oct/21 06:28,15/Dec/21 01:44,13/Jul/23 08:12,29/Oct/21 03:04,1.13.3,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Runtime / Metrics,,,,,0,pull-request-available,,,,Current metric name of map state contains is {{mapStateContainsAllLatency}} which should be {{mapStateContainsLatency}}.,,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 29 03:04:46 UTC 2021,,,,,,,,,,"0|z0w8g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Oct/21 03:04;yunta;Merged
master: b1a409ef1c53a24d717c789eea8c09ddbc94f7f1
release-1.14: 314fd0a14dfc4bfaa122bc2d7c1bc4f253216f7a
release-1.13: 139bba8b93ae1870f11da0adff5bc4ab67bf73a9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schema does not match if explain insert statement with partial column,FLINK-24676,13408802,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,28/Oct/21 02:19,15/Dec/21 01:44,13/Jul/23 08:12,28/Oct/21 15:15,1.13.0,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"create table MyTable (a int, b int) with ('connector' = 'datagen');
create table MySink (c int, d int) with ('connector' = 'print');
explain plan for insert into MySink(d) select a from MyTable where a > 10;

If execute the above statement, we will get the following exception

org.apache.flink.table.api.ValidationException: Column types of query result and sink for registered table 'default_catalog.default_database.MySink' do not match.
Cause: Different number of columns.

Query schema: [a: BIGINT]
Sink schema:  [d: BIGINT, e: INT]
",,godfreyhe,libenchao,Paul Lin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 28 15:15:06 UTC 2021,,,,,,,,,,"0|z0w89s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/21 15:15;godfreyhe;Fixed in 1.15.0: 37f8b380e2a962a46d287371bf37b7a97f4573cf
Fixed in 1.14.1: 4ceaac13a0b5a8eceac78f3f22c247dea62a55f3
Fixed in 1.13.4: 05d19616b200da052f345761325726d32c6c1c11;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate old Row SerializationSchema/DeserializationSchema,FLINK-24673,13408760,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,slinkydeveloper,slinkydeveloper,slinkydeveloper,27/Oct/21 18:39,28/Oct/21 15:39,13/Jul/23 08:12,28/Oct/21 15:03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Table SQL / Ecosystem,,,,,0,pull-request-available,,,,"As discussed in the [mailing list|https://lists.apache.org/thread.html/red2f04ac782f2cd156a639a44c9962b7b92659b76e5ff683de664534%40%3Cdev.flink.apache.org%3E], we should deprecate in the next version:

   - org.apache.flink.formats.json.JsonRowSerializationSchema
   - org.apache.flink.formats.json.JsonRowDeserializationSchema
   - org.apache.flink.formats.avro.AvroRowSerializationSchema
   - org.apache.flink.formats.avro.AvroRowDeserializationSchema
   - org.apache.flink.formats.csv.CsvRowDeserializationSchema
   - org.apache.flink.formats.csv.CsvRowSerializationSchema",,caozhen1937,slinkydeveloper,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 28 15:39:47 UTC 2021,,,,,,,,,,"0|z0w80g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/21 15:03;twalthr;Fixed in master: ef13f23ba8a62d84d7a845c2bd050c199012ba8b;;;","28/Oct/21 15:39;caozhen1937;High efficiency.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible NPE in LocalInputChannel#getBuffersInUseCount before initialization of subpartitionView ,FLINK-24671,13408738,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,akalashnikov,akalashnikov,27/Oct/21 16:11,03/Nov/21 16:27,13/Jul/23 08:12,02/Nov/21 09:57,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Network,,,,,0,pull-request-available,,,,"If the buffer debloater tries to get the number of buffers in use before

subpartitionView will be initialized (or after released?), NPE will be possible in LocalInputChannel#getBuffersInUseCount",,akalashnikov,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23974,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 02 09:56:56 UTC 2021,,,,,,,,,,"0|z0w7vk:",9223372036854775807,"merged commit f91bd77 into apache:master
merged commit c629c5e into apache:release-1.14",,,,,,,,,,,,,,,,,,,"01/Nov/21 11:16;ym;merged commit f91bd77 into apache:master
;;;","02/Nov/21 09:56;ym;merged commit c629c5e into apache:release-1.14;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Channel state writer would fail the task directly if meeting exception previously,FLINK-24667,13408646,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,yunta,yunta,27/Oct/21 09:25,15/Dec/21 01:44,13/Jul/23 08:12,10/Nov/21 14:12,1.13.3,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Runtime / Checkpointing,Runtime / Task,,,,0,pull-request-available,,,,"Currently, if channel state writer come across exception when closing a file, such as meet exception during {{SubtaskCheckpointCoordinatorImpl#cancelAsyncCheckpointRunnable}}, it will exit the loop. However, in the following {{channelStateWriter#abort}} it will throw exception directly：


{code:java}
switched from RUNNING to FAILED with failure cause: java.io.IOException: java.lang.RuntimeException: unable to send request to worker
	at org.apache.flink.runtime.io.network.partition.consumer.InputChannel.checkError(InputChannel.java:228)
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.checkPartitionRequestQueueInitialized(RemoteInputChannel.java:735)
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.getNextBuffer(RemoteInputChannel.java:204)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.waitAndGetNextData(SingleInputGate.java:651)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:626)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.pollNext(SingleInputGate.java:612)
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.pollNext(InputGateWithMetrics.java:109)
	at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:149)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66)
	at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processInput(StreamTwoInputProcessor.java:98)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:424)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:204)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:685)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:640)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:651)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:624)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:798)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:585)
{code}

This is not expected as checkpoint failure should not lead to task failover each time.",,roman,wanglijie,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 10 14:12:20 UTC 2021,,,,,,,,,,"0|z0w7b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/21 14:12;roman;Merged as 

61b217f72369c4ff4ff3e3d1bb16232bb07bbc7f to 1.13,

2ba57e29b8c7cc7d48f5313f0aeb96960c0796f6 to 1.14,

d1997b827a0e21308c57450dd7a6df1e8efa5bce to master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"PyFlink sphinx check failed with ""node class 'meta' is already registered, its visitors will be overridden""",FLINK-24662,13408570,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,27/Oct/21 03:06,15/Dec/21 01:44,13/Jul/23 08:12,27/Oct/21 03:23,1.12.0,1.13.0,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.6,1.14.3,1.15.0,,API / Python,Tests,,,,0,,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25481&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3]

{code}
==========mypy checks... [SUCCESS]===========
Oct 26 22:08:34 rm -rf _build/*
Oct 26 22:08:34 /__w/1/s/flink-python/dev/.conda/bin/sphinx-build -b html -d _build/doctrees  -a -W . _build/html
Oct 26 22:08:34 Running Sphinx v2.4.4
Oct 26 22:08:34 
Oct 26 22:08:34 Warning, treated as error:
Oct 26 22:08:34 node class 'meta' is already registered, its visitors will be overridden
Oct 26 22:08:34 Makefile:76: recipe for target 'html' failed
{code}",,dianfu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 27 03:23:09 UTC 2021,,,,,,,,,,"0|z0w6u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/21 03:22;dianfu;It should be caused by the latest release docutils [https://pypi.org/project/docutils/0.18/#history]

To fix it, we could simply set the version to an old version.;;;","27/Oct/21 03:23;dianfu;Fixed in
- master via aebd31054366581dde0445b2aa73f63cd6687d24
- release-1.14 via 808b32b758e010b1d60ce62ae3bb43ac8845b6c1
- release-1.13 via c62f7634ca8836d207a024de0032b04fc8125d4c
- release-1.12 via 9233b9b6513d6e912a620b8f18a93fa73909622c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Two active miniCluster in RemoteBenchmarkBase,FLINK-24659,13408493,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,akalashnikov,akalashnikov,26/Oct/21 15:34,09/Nov/21 07:54,13/Jul/23 08:12,09/Nov/21 07:54,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Benchmarks,Runtime / Network,,,,0,pull-request-available,,,,"It seems that all children of RemoteBenchmarkBase work incorrectly since they configure the environment for miniCluster from FlinkEnvironmentContext but in reality, they use miniCluster from RemoteBenchmarkBase. So it definitely we should remove one of them.

I think we can get rid of RemoteBenchmarkBase#miniCluster and use FlinkEnvironmentContext#miniCluster everywhere.",,akalashnikov,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 09 07:54:35 UTC 2021,,,,,,,,,,"0|z0w6d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/21 07:54;pnowojski;merged commit 0f8d2e3 into flink-benchmarks:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE on RetractableTopNFunction when some records were cleared by state ttl ,FLINK-24654,13408424,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,26/Oct/21 11:28,15/Dec/21 01:44,13/Jul/23 08:12,10/Nov/21 02:09,1.13.3,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"NullPointerException will occurr when some records were cleared by the state ttl.

stack:
{quote}java.lang.NullPointerExceptionjava.lang.NullPointerException at org.apache.flink.table.runtime.operators.rank.RetractableTopNFunction.retractRecordWithRowNumber(RetractableTopNFunction.java:379) at org.apache.flink.table.runtime.operators.rank.RetractableTopNFunction.processElement(RetractableTopNFunction.java:175) at org.apache.flink.table.runtime.operators.rank.RetractableTopNFunction.processElement(RetractableTopNFunction.java:54) at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83) at org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness.processElement(OneInputStreamOperatorTestHarness.java:209) at org.apache.flink.table.planner.runtime.harness.RankHarnessTest.testRetractRankWithRowNumber(RankHarnessTest.scala:111)
{quote}",,jark,libenchao,lincoln.86xy,lzljs3620320,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 28 07:54:01 UTC 2021,,,,,,,,,,"0|z0w5xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/21 08:13;lzljs3620320;Thanks [~lincoln.86xy] for reporting. Can you provide the SQL?;;;","27/Oct/21 08:19;lincoln.86xy;[~lzljs3620320] The sql case was added in `RankHarnessTest` (the real sql  we met the exception was to complex), mainly part is to simulate a retract rank operator and its staled state record when processing input retract record.;;;","28/Oct/21 07:54;lzljs3620320;master: 2ae9c4db0a00ab08c85f0813e473c8f1fce141ea

release-1.14: 798aed05ba32825187ff935843b01ad94ebd8234;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Ceil, floor for some timeunit return wrong results or fail with CompileException",FLINK-24648,13408414,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,26/Oct/21 11:13,04/Nov/21 01:57,13/Jul/23 08:12,03/Nov/21 07:55,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"There are issues
1. for {{TIMESTAMP WITHOUT TIMEZONE}} and {{DATE}} it returns wrong result for queries
{code:sql}
select ceil(timstamp'2020-26-10 12:12:12' to decade);
select ceil(timstamp'2020-26-10 12:12:12' to century);
select ceil(timstamp'2020-26-10 12:12:12' to millennium);
{code}

same for {{FLOOR}} and {{DATE}}
2. for {{TIMESTAMP WITH TIMEZONE}} it throws exception below.

Expected for the query
{code:sql}
select floor(date '2021-10-07' to decade) as floor_decade,      
         ceil(date '2021-10-07' to decade) as ceil_decade,
         floor(date '2021-10-07' to century) as floor_century,
         ceil(date '2021-10-07' to century) as ceil_century,
         floor(date '2021-10-07' to millennium) as floor_millennium,
         ceil(date '2021-10-07' to millennium) as ceil_millennium;

{code}
is
{noformat}
 floor_decade ceil_decade floor_century ceil_century floor_millennium ceil_millennium
   2020-01-01  2030-01-01    2001-01-01   2101-01-01       2001-01-01      3001-01-01
{noformat}
based on  PostgreSQL[1] and Vertica[2] defibitions 
And for both the definition is the same
{{DECADE}} - The year field divided by 10
{{CENTURY}} - The first century starts at 0001-01-01 00:00:00 AD, although they did not know it at the time. This definition applies to all Gregorian calendar countries. There is no century number 0, you go from -1 century to 1 century.
{{MILLENNIUM}} - The millennium number, where the first millennium is 1 and each millenium starts on 01-01-y001. For example, millennium 2 starts on 01-01-1001.
from
[1] https://www.postgresql.org/docs/14/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT
[2] https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/SQLReferenceManual/Functions/Date-Time/DATE_PART.htm

{noformat}
[ERROR] Could not execute SQL statement. Reason:
org.codehaus.commons.compiler.CompileException: Line 57, Column 0: No applicable constructor/method found for actual parameters ""org.apache.flink.table.data.TimestampData, org.apache.flink.table.data.TimestampData""; candidates are: ""public static int org.apache.calcite.runtime.SqlFunctions.ceil(int, java.math.BigDecimal)"", ""public static java.math.BigDecimal org.apache.calcite.runtime.SqlFunctions.ceil(java.math.BigDecimal, int)"", ""public static java.math.BigDecimal org.apache.calcite.runtime.SqlFunctions.ceil(java.math.BigDecimal, java.math.BigDecimal)"", ""public static short org.apache.calcite.runtime.SqlFunctions.ceil(short, short)"", ""public static java.math.BigDecimal org.apache.calcite.runtime.SqlFunctions.ceil(java.math.BigDecimal)"", ""public static double org.apache.calcite.runtime.SqlFunctions.ceil(double)"", ""public static float org.apache.calcite.runtime.SqlFunctions.ceil(float)"", ""public static byte org.apache.calcite.runtime.SqlFunctions.ceil(byte, byte)"", ""public static long org.apache.calcite.runtime.SqlFunctions.ceil(long, long)"", ""public static int org.apache.calcite.runtime.SqlFunctions.ceil(int, int)""
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:9263)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9123)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9025)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5062)
	at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423)
	at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3792)
	at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3754)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3734)
	at org.codehaus.janino.Java$Assignment.accept(Java.java:4477)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)
	at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553)
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$Block.accept(Java.java:2779)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476)
	at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86)
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74)
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102)
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83)
	at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40)
	at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:81)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperator(OperatorChain.java:712)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:686)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:626)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:187)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.<init>(RegularOperatorChain.java:63)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:675)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:661)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:960)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:929)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:753)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:574)
	at java.base/java.lang.Thread.run(Thread.java:834)

{noformat}",,jark,libenchao,lzljs3620320,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 03 07:55:37 UTC 2021,,,,,,,,,,"0|z0w5vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Oct/21 13:23;Sergey Nuyanzin;The PR contains changes in DateTimeUtils
The similar changes recently were added in Avatica https://github.com/apache/calcite-avatica/commit/e462b09d4d8b6dd64a9a9c0562a6d11868751a06;;;","03/Nov/21 07:55;lzljs3620320;master: ac12dc2b02aaaf6cdd1354d72a17391fb137ae0a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClusterUncaughtExceptionHandler does not log the exception,FLINK-24647,13408402,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fpaul,fpaul,fpaul,26/Oct/21 09:55,15/Dec/21 01:44,13/Jul/23 08:12,26/Oct/21 14:30,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"If an uncaught exception occurs and the uncaught exception handler is configured to log it swallows the exception stacktrace.

Example stacktrace https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25416&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=19759",,fpaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 26 14:30:39 UTC 2021,,,,,,,,,,"0|z0w5sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Oct/21 14:30;arvid;Merged into master as d50d9b3027f954fd77e3ef85dd64c0858d5326a6, and into 1.14 as b68d189cd96896f93ba6401cf0fa9a626ed73ef9.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Unknown variable or type ""org.apache.flink.table.utils.DateTimeUtils""",FLINK-24638,13408306,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,slinkydeveloper,Sergey Nuyanzin,Sergey Nuyanzin,25/Oct/21 20:29,28/Oct/21 10:11,13/Jul/23 08:12,28/Oct/21 10:11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,,,,,"The problem is not constantly reproduced

however it is reproduced almost every 2-nd query via FlinkSqlClient containing {{current_timestamp}}, {{current_date}}
e.g.
{code:sql}
select extract(millennium from current_date);
select extract(millennium from current_timestamp);
select floor(current_timestamp to day);
select ceil(current_timestamp to day);
{code}

trace 
{noformat}
[ERROR] Could not execute SQL statement. Reason:
org.codehaus.commons.compiler.CompileException: Line 59, Column 16: Unknown variable or type ""org.apache.flink.table.utils.DateTimeUtils""
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
	at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6860)
	at org.codehaus.janino.UnitCompiler.access$13600(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$22.visitPackage(UnitCompiler.java:6472)
	at org.codehaus.janino.UnitCompiler$22.visitPackage(UnitCompiler.java:6469)
	at org.codehaus.janino.Java$Package.accept(Java.java:4248)
	at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
	at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6855)
	at org.codehaus.janino.UnitCompiler.access$14200(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$22$2$1.visitAmbiguousName(UnitCompiler.java:6497)
	at org.codehaus.janino.UnitCompiler$22$2$1.visitAmbiguousName(UnitCompiler.java:6494)
	at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4224)
	at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6494)
	at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6490)
	at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148)
	at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6490)
	at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6469)
	at org.codehaus.janino.Java$Rvalue.accept(Java.java:4116)
	at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9026)
	at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:7106)
	at org.codehaus.janino.UnitCompiler.access$15800(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$22$2.visitMethodInvocation(UnitCompiler.java:6517)
	at org.codehaus.janino.UnitCompiler$22$2.visitMethodInvocation(UnitCompiler.java:6490)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)
	at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6490)
	at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6469)
	at org.codehaus.janino.Java$Rvalue.accept(Java.java:4116)
	at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:9237)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9123)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9025)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5062)
	at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423)
	at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3792)
	at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3754)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3734)
	at org.codehaus.janino.Java$Assignment.accept(Java.java:4477)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)
	at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553)
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$Block.accept(Java.java:2779)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476)
	at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86)
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74)
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102)
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83)
	at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40)
	at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:81)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperator(OperatorChain.java:712)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:686)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:626)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:187)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.<init>(RegularOperatorChain.java:63)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:667)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:653)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:960)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:929)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:753)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:574)
	at java.base/java.lang.Thread.run(Thread.java:834)

{noformat}",,Sergey Nuyanzin,slinkydeveloper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 28 09:51:37 UTC 2021,,,,,,,,,,"0|z0w57k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Oct/21 06:38;slinkydeveloper;Hi, are you testing with master? Because if that's the case, it means there was something wrong with https://github.com/apache/flink/pull/17454;;;","26/Oct/21 07:30;Sergey Nuyanzin;yes I'm testing with master, probably you are right.
I tried without this PR and no issue.;;;","26/Oct/21 09:13;slinkydeveloper;Ok, thank you for reporting, I'll take care of it;;;","27/Oct/21 15:47;slinkydeveloper;Hi, the log should include the bad generated class, can you provide it?;;;","27/Oct/21 16:00;slinkydeveloper;I'm trying to reproduce it, without any success. Perhaps you need to rebuild the sql client? Does it reproduces using latest flink-table-api-java snapshots?;;;","28/Oct/21 09:51;Sergey Nuyanzin;I've checked with the current master commit fcdba9932cca5ea996c990bee1d2f3f007056e5a

Now it is not reproduced for as well..
Probably we can close if you don't mind;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sorting start-time/duration/end-time not working under pages of vertex taskManagers and sub-tasks,FLINK-24629,13408191,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,junhan,junhan,junhan,25/Oct/21 08:24,16/Feb/22 08:41,13/Jul/23 08:12,16/Feb/22 08:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Web Frontend,,,,,0,pull-request-available,stale-assigned,,,"Based on the definitions of `VertexTaskManagerDetailInterface` and `JobSubTaskInterface` interfaces, the sorting functions of start-time, duration and end-time are incorrectly stated as `details.XXX`. !image-2021-10-25-16-23-55-894.png|width=552,height=73!",,junhan,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/21 08:23;junhan;image-2021-10-25-16-23-55-894.png;https://issues.apache.org/jira/secure/attachment/13035300/image-2021-10-25-16-23-55-894.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 02 10:39:46 UTC 2021,,,,,,,,,,"0|z0w4i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/21 11:26;junhan;Hi, [~airblader]. I found a small bug regarding the Frontend Web after those PRs of bumping Angular & NG-Zorro. Nevertheless, the bug only affects the capability of sorting three columns, i.e. 'start-time', 'end-time' and 'duration'.;;;","02/Dec/21 10:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation on orc supported data types is outdated,FLINK-24613,13407861,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sujun1020,sujun1020,sujun1020,22/Oct/21 02:06,15/Dec/21 01:44,13/Jul/23 08:12,25/Oct/21 01:36,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Documentation,,,,,0,pull-request-available,,,,"Orc already supports complex types, and the documentation has not been updated in time.

 

The document url is: https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/connectors/table/formats/orc/",,jark,lzljs3620320,sujun1020,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 25 01:36:00 UTC 2021,,,,,,,,,,"0|z0w2go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Oct/21 02:11;sujun1020;hi [~jark] , The Data Type Mapping chapter should add ARRAY, MAP, ROW three types, you can assign this issue to me, I can update it in time;;;","22/Oct/21 02:15;jark;[~sujun1020] they are supported in which issue?;;;","22/Oct/21 02:48;lzljs3620320;https://issues.apache.org/jira/browse/FLINK-17783
https://issues.apache.org/jira/browse/FLINK-15390
[~jark];;;","25/Oct/21 01:36;lzljs3620320;Fixed via:
master: 955bd585b618e841fe2ff3d3f69bf4f06108aa84
release-1.14: 55f878385c55ad497bd97bce7b5212fca9e5208f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka test container creates a large amount of logs,FLINK-24612,13407780,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fpaul,fpaul,fpaul,21/Oct/21 13:56,15/Dec/21 01:44,13/Jul/23 08:12,01/Nov/21 08:09,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"When we use a testcontainer setup we try to forward all container STDOUT logs to the surrounding test logger. Unfortunately, Kafka loggers are by default writing a large number of logs because some of the internal loggers are defaulting to TRACE logging.

A good example is this test job 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25084&view=logs&j=32a18cd8-d404-5807-996d-abcee436b891
where one of the test was stuck and the generated artifact is ~25GB. This makes debugging very hard because the file is hard to parse.",,fpaul,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 01 08:08:56 UTC 2021,,,,,,,,,,"0|z0w1yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/21 14:09;arvid;Merged into master as 21fbd59e627c2926e18628817918318d9a20c7d7.;;;","01/Nov/21 08:08;fpaul;merged into 1.14 8ef4c1bb387d413f7207e282c43216facf7e0c99;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sinks built with the unified sink framework do not receive timestamps when used in Table API,FLINK-24608,13407684,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,matriv,fpaul,fpaul,21/Oct/21 06:46,15/Dec/21 01:44,13/Jul/23 08:12,22/Nov/21 06:59,1.13.3,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Connectors / Common,Table SQL / Planner,,,,0,pull-request-available,,,,"All sinks built with the unified sink framework extract the timestamp from the internal {{StreamRecord}}. The Table API does not facilitate the timestamp field in the {{StreamRecord}}  but extracts the timestamp from the actual data. 

We either have to use a dedicated operator before all the sinks to simulate the behavior or allow a customizable timestamp extraction during the sink translation.

",,airblader,fpaul,matriv,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24596,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 22 06:59:30 UTC 2021,,,,,,,,,,"0|z0w1dc:",9223372036854775807,This adds an additional operator to the topology if the new sink interfaces are used (e.g. for Kafka). It could cause issues in 1.14.1 when restoring from a 1.14 savepoint. A workaround is to cast the time attribute to a regular timestamp in the SQL statement closely before the sink.,,,,,,,,,,,,,,,,,,,"08/Nov/21 14:50;matriv;After discussion with [~twalthr] it seems there are 2 ways to implement this.

There is an issue with *SinkUpsertMaterializer* which keeps a state, and its state is a leas of `RowData` so any information on the *StreamRecord* is lost, so:
 # Keep any constraint validation operator before the *SinkUpsertMaterializer* and add a new operator at the end (after the {*}SinkUpsertMaterializer{*}) to set the timestamp on the *StreamRecord*
 # Combine the constraint validation operator with this new one which sets the timestamp, and apply it after the *SinkUpsertMaterializer*

(Currently the constraint validation in place is only the {*}SinkNotNullEnforcer{*})

 

With *1* we could already filter out records, or shorter the length of char/varchar fields of a record (soon to be implemented) before the *SinkUpsertMaterializer* thus decreasing the size of its state.

With *2* we loose this possible memory gain from *1* but we avoid introducing yet another operator in the pipeline.

 

[~fpaul], [~jark] Could you please share your opinion on this?

 ;;;","09/Nov/21 09:25;twalthr;I would vote for option 1. In theory, when shortening CHAR/VARCHAR this could even affect the primary key (e.g. ""asdf"" and ""aaa"" could become ""a"" with VARCHAR(1)). It is safer to do that before the `SinkUpsertMaterializer` and users can enforce another keyBy and upsert materialize step to normalize the output. Having another operator in the pipeline seems to be unavoidable unless we only pass the {{rowtimeIndex}} directly to the sink and every sink implements custom logic for reading the timestamp.;;;","09/Nov/21 09:25;fpaul;I do not fully see the connection between the *SinkUpsertMaterializer* and the issue described in the ticket. Is the *SinkUpsertMaterializer* suffering from the same problem as the unified Sinks? AFAICT the *SinkUpsertMaterializer* is not accessing the timestamp of the StreamRecord.


WDYT about an alternative 3 keeping one constraint operator before the *SinkUpsertMaterializer* with different functions i.e. not null enforcer, timestamp extractor ... and always use the StreamRecord#timestamp in the *SinkUpsertMaterializer*. It has the benefit that it is guaranteed that there is always a timestamp when using the materialize and you do not have to deal with null checks.


;;;","09/Nov/21 09:58;matriv;The problem with *SinkUpsertMaterializer* is that currently it uses a state which is a *List<RowData>* and not a *List<StreamRecord>,* therefore it ""looses"" the timestamp info.

If we set the *timestamp* to the stream record before *SinkUpsertMaterializer* then we need to either store *List<StreamRecord>,* thus increasing the size of its state, or pass the column index of the timestamp

to *SinkUpsertMaterializer* so it can retrieve the *timestamp* and set it to the result {*}StreamRecord{*}s, which somehow negates the point of setting the timestamp beforehand.;;;","09/Nov/21 10:36;matriv;I would also vote for option 1, so get the benefits of filtering records before needing to buffer them in {*}SinkUpsertMaterializer{*}, + the possible changed to CHAR/VARCHAR types as mentioned by [~twalthr] ,

and then have a dedicated operator to handle the timestamp in one place, (keep in mind that *SinkUpsertMaterilizer* is not always applied).;;;","22/Nov/21 06:59;twalthr;Fixed in master: 548b96e9cb226aaf8d919d900c9326520a5b6dc8
Fixed in 1.14.1: 3719c0402ec979c619371fcde9f2e7d2c46d69ed

For now I did not back port this fix to 1.13. 1.14 is still not adopted by many users whereas 1.13 is. The change could affect the savepoint restoration.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceCoordinator may miss to close SplitEnumerator when failover frequently,FLINK-24607,13407655,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,becket_qin,jark,jark,21/Oct/21 03:33,24/Feb/22 01:55,13/Jul/23 08:12,24/Feb/22 01:55,1.13.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.7,1.14.4,1.15.0,,,Connectors / Common,,,,,0,pull-request-available,,,,"We are having a connection leak problem when using mysql-cdc [1] source. We observed that many enumerators are not closed from the JM log.

{code}
➜  test123 cat jobmanager.log | grep ""SourceCoordinator \[\] - Restoring SplitEnumerator"" | wc -l
     264
➜  test123 cat jobmanager.log | grep ""SourceCoordinator \[\] - Starting split enumerator"" | wc -l
     264
➜  test123 cat jobmanager.log | grep ""MySqlSourceEnumerator \[\] - Starting enumerator"" | wc -l
     263
➜  test123 cat jobmanager.log | grep ""SourceCoordinator \[\] - Closing SourceCoordinator"" | wc -l
     264
➜  test123 cat jobmanager.log | grep ""MySqlSourceEnumerator \[\] - Closing enumerator"" | wc -l
     195
{code}

We added ""Closing enumerator"" log in {{MySqlSourceEnumerator#close()}}, and ""Starting enumerator"" in {{MySqlSourceEnumerator#start()}}. From the above result you can see that SourceCoordinator is restored and closed 264 times, split enumerator is started 264 but only closed 195 times. It seems that {{SourceCoordinator}} misses to close enumerator when job failover frequently. 

I also went throught the code of {{SourceCoordinator}} and found some suspicious point:

The {{started}} flag and  {{enumerator}} is assigned in the main thread, however {{SourceCoordinator#close()}} is executed async by {{DeferrableCoordinator#closeAsync}}.  That means the close method will check the {{started}} and {{enumerator}} variable async. Is there any concurrency problem here which mean lead to dirty read and miss to close the {{enumerator}}? 

I'm still not sure, because it's hard to reproduce locally, and we can't deploy a custom flink version to production env. 


[1]: https://github.com/ververica/flink-cdc-connectors",,becket_qin,dmvk,jark,Jocean,leonard,libenchao,mason6345,renqs,tartarus,Terry1897,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/21 03:33;jark;jobmanager.log;https://issues.apache.org/jira/secure/attachment/13035175/jobmanager.log",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 24 01:55:17 UTC 2022,,,,,,,,,,"0|z0w16w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/22 13:44;dmvk;[~jqin] Do you have any thoughts on this one?;;;","13/Jan/22 02:52;becket_qin;[~dmvk] I am not sure about the exact cause of this issue. But I went through the SourceCoordinator protocol a couple of days earlier and there might be a threading bug. I will verify that first and see if that is related to this ticket.;;;","09/Feb/22 06:16;becket_qin;[~jark] [~dmvk] Sorry I did not notice the attached log file earlier. From the log, it looks that the split enumerators are sometimes not closed because the coordinator closing sequence timed out before it reaches the code block that closes the enumerator.

More specifically, either the main executor or worker executor is performing some time-consuming task which causes the executor shutdown to take more than 60 seconds. After that closing sequence will be interrupted and the rest of the closing sequence are skipped.

What we can do here is to put the split enumerator closing sequence in the finally block in {{{}SourceCoordinator.close(){}}}. It ensures that {{enumerator.close()}} will be invoked. Also, we will need to make sure that the executors actually exits even if the split enumerator executes some tasks running indefinitely.

I'll submit a patch shortly.;;;","24/Feb/22 01:55;becket_qin;Patch merged.

Master: 0f19c2472c54aac97e4067f5398731ab90036d1a

release-1.14: 0a76d632f33d9a69df87457a63043bd7f609ed40

release-1.13: 6fb7807b20cba05437a7b570c704d643432745eb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failing tests for casting decimals to boolean,FLINK-24604,13407566,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,matriv,matriv,matriv,20/Oct/21 14:55,20/Oct/21 17:34,13/Jul/23 08:12,20/Oct/21 17:34,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Table SQL / Planner,Table SQL / Runtime,,,,0,pull-request-available,,,,"Currently some tests in *CalcITCase.scala, SimplifyJoinConditionRuleTest.scala* and *FlinkRexUtilTest.scala* are failing because of the merge of [https://github.com/apache/flink/pull/17311] and [https://github.com/apache/flink/pull/17439] where the first one adds some tests with decimal to boolean cast and the latter drops this support.",,matriv,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 20 17:34:47 UTC 2021,,,,,,,,,,"0|z0w0n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/21 17:34;twalthr;Fixed in master: 474b241eb1257f978a83aac934cf6601570d05aa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate 99th percentile displayed in checkpoint summary,FLINK-24600,13407449,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,txdong-sz,txdong-sz,txdong-sz,20/Oct/21 08:04,15/Dec/21 01:44,13/Jul/23 08:12,20/Oct/21 10:18,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,," 

flink checkpoints page has two p99 which is duplicated

!image-2021-10-20-16-03-43-437.png!",,txdong-sz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/21 08:03;txdong-sz;image-2021-10-20-16-03-43-437.png;https://issues.apache.org/jira/secure/attachment/13035137/image-2021-10-20-16-03-43-437.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 20 10:18:42 UTC 2021,,,,,,,,,,"0|z0vzx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/21 10:18;chesnay;master: a9af9bd9be084579ce61aea88d90a686d360f06a
1.14: 95d844deb9f3dceac5d2492629c7bdbe37f3b75f ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksdbStateBackend getKeysAndNamespaces would return duplicate data when using MapState ,FLINK-24597,13407432,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mayuehappy,mayuehappy,mayuehappy,20/Oct/21 06:43,15/Dec/21 01:44,13/Jul/23 08:12,05/Nov/21 03:03,1.12.4,1.13.3,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,API / State Processor,Runtime / State Backends,,,,1,pull-request-available,,,,"For example, in RocksdbStateBackend , if we worked in VoidNamespace , and And use the ValueState like below .
{code:java}
            // insert record
            for (int i = 0; i < 3; ++i) {
                keyedStateBackend.setCurrentKey(i);
                testValueState.update(String.valueOf(i));
            }
{code}
Then we get all the keysAndNamespace according the method RocksDBKeyedStateBackend#getKeysAndNamespaces().The result of the traversal is
 <1,VoidNamespace>,<2,VoidNamespace>,<3,VoidNamespace> ,which is as expected.

Thus，if we use MapState , and update the MapState with different user key, the getKeysAndNamespaces would return duplicate data with same keyAndNamespace.
{code:java}
            // insert record
            for (int i = 0; i < 3; ++i) {
                keyedStateBackend.setCurrentKey(i);
                mapState.put(""userKeyA_"" + i, ""userValue"");
                mapState.put(""userKeyB_"" + i, ""userValue"");
            }
{code}
The result of the traversal is
 <1,VoidNamespace>,<1,VoidNamespace>,<2,VoidNamespace>,<2,VoidNamespace>,<3,VoidNamespace>,<3,VoidNamespace>.

By reading the code, I found that the main reason for this problem is in the implementation of _RocksStateKeysAndNamespaceIterator_.
In the _hasNext_ method, when a new keyAndNamespace is created, there is no comparison with the previousKeyAndNamespace. So we can refer to RocksStateKeysIterator to implement the same logic should solve this problem.
  
  ",,klion26,libenchao,mayuehappy,roman,wind_ljy,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/21 06:19;mayuehappy;image-2021-11-01-14-19-58-372.png;https://issues.apache.org/jira/secure/attachment/13035545/image-2021-11-01-14-19-58-372.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 05 03:03:04 UTC 2021,,,,,,,,,,"0|z0vztc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Oct/21 12:30;klion26;[~mayuehappy] thanks for reporting this issue, I think this is valid. [~sjwiesman] could you please help to have a double check about this, thanks.;;;","25/Oct/21 05:52;mayuehappy;[~roman] could you please help to take a look at this issue  , thx ? ;;;","25/Oct/21 14:24;roman;Thanks for pulling me in [~mayuehappy], I think the issue is valid and the fix seems correct.

 

[~yunta] do you have any concerns regarding it? 

In the iterator.hasNext, we'd call equals for (key, ns) each time, which can have some performance implications.;;;","29/Oct/21 08:11;yunta;I think this is valid, and current {{RocksStateKeysIterator}}, which used in {{#getKeys}}, would filter same primary key via {{!Objects.equals(previousKey, currentKey)}}, we at least need to do simlar things during {{#getKeysAndNamespaces}}. For performance things, since we could have unfixed length key-serializer, it might not be easy to avoid the deserialization.;;;","01/Nov/21 06:20;mayuehappy;[~roman] So, can this ticket be assigned to me ?;;;","01/Nov/21 06:41;yunta;[~mayuehappy] Already assigned to you, please go ahead.;;;","01/Nov/21 17:57;mayuehappy;[~yunta] [~roman] 
https://github.com/apache/flink/pull/17525
have updated the description and added unit test . would you please take a review ?   thanks;;;","05/Nov/21 03:03;yunta;Merged
master: a907d92673a711612b287d184c00dad7fa42269f
release-1.14: 547b3befcff50bf8fc2bef4e596cd39a55c7d4b2
release-1.13: 77824d10edb3fd299c242865a3a03fe08e540a85;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bugs in sink.buffer-flush before upsert-kafka,FLINK-24596,13407411,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fpaul,lzljs3620320,lzljs3620320,20/Oct/21 03:05,15/Dec/21 01:44,13/Jul/23 08:12,01/Dec/21 08:06,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"There is no ITCase for sink.buffer-flush before upsert-kafka. We should add it.
FLINK-23735 brings some bugs:
* SinkBufferFlushMode bufferFlushMode not Serializable
* Function<RowData, RowData> valueCopyFunction not Serializable
* Planner dose not support DataStreamProvider with new Sink",,fpaul,jark,leonard,lzljs3620320,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24608,,,,,,,,,,,,,,,,,FLINK-23735,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 01 08:06:03 UTC 2021,,,,,,,,,,"0|z0vzoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/21 03:06;lzljs3620320;CC: [~arvid] [~fpaul];;;","20/Oct/21 06:31;fpaul;I can take it.;;;","29/Nov/21 16:05;fpaul;Fixed on master: 84f9baaf31ddcb3a7b4bb730b2112cfa4ed36526;;;","01/Dec/21 08:06;fpaul;Fixed on release-1.14: 66cf720388c5697d5bf98c10b3dd348a0704a710;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ElasticsearchWriterITCase.testWriteOnBulkIntervalFlush timeout on azure,FLINK-24583,13407191,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,alexanderpreuss,xtsong,xtsong,19/Oct/21 03:00,24/Nov/21 07:59,13/Jul/23 08:12,24/Nov/21 07:59,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Connectors / ElasticSearch,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25191&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=f53023d8-92c3-5d78-ec7e-70c2bf37be20&l=12452

{code}
Oct 18 23:47:27 [ERROR] Tests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 22.228 s <<< FAILURE! - in org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriterITCase
Oct 18 23:47:27 [ERROR] testWriteOnBulkIntervalFlush  Time elapsed: 2.032 s  <<< ERROR!
Oct 18 23:47:27 java.util.concurrent.TimeoutException: Condition was not met in given timeout.
Oct 18 23:47:27 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:166)
Oct 18 23:47:27 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:144)
Oct 18 23:47:27 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:136)
Oct 18 23:47:27 	at org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriterITCase.testWriteOnBulkIntervalFlush(ElasticsearchWriterITCase.java:139)
{code}",,gaoyunhaii,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 24 07:59:38 UTC 2021,,,,,,,,,,"0|z0vybs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/21 06:56;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26568&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=12761];;;","24/Nov/21 07:59;arvid;Merged into master as 349d57baf9492978cdfcfbcb6e5223bb30746622.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kinesis connect time out error is not handled as recoverable,FLINK-24580,13407169,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jkarp,jkarp,jkarp,18/Oct/21 22:21,04/Feb/22 11:38,13/Jul/23 08:12,04/Feb/22 11:38,1.11.4,1.12.5,1.13.3,1.14.2,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Connectors / Kinesis,,,,,0,pull-request-available,stale-assigned,,,"Several times a day, transient Kinesis errors cause our Flink job to fail:
{noformat}
org.apache.flink.kinesis.shaded.com.amazonaws.SdkClientException: Unable to execute HTTP request: Connect to kinesis.us-east-1.amazonaws.com:443 [kinesis.us-east-1.amazonaws.com/3.91.171.253] failed: connect timed out
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1207)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1153)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at org.apache.flink.kinesis.shaded.com.amazonaws.services.kinesis.AmazonKinesisClient.doInvoke(AmazonKinesisClient.java:2893)
	at org.apache.flink.kinesis.shaded.com.amazonaws.services.kinesis.AmazonKinesisClient.invoke(AmazonKinesisClient.java:2860)
	at org.apache.flink.kinesis.shaded.com.amazonaws.services.kinesis.AmazonKinesisClient.invoke(AmazonKinesisClient.java:2849)
	at org.apache.flink.kinesis.shaded.com.amazonaws.services.kinesis.AmazonKinesisClient.executeGetRecords(AmazonKinesisClient.java:1319)
	at org.apache.flink.kinesis.shaded.com.amazonaws.services.kinesis.AmazonKinesisClient.getRecords(AmazonKinesisClient.java:1288)
	at org.apache.flink.streaming.connectors.kinesis.proxy.KinesisProxy.getRecords(KinesisProxy.java:292)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisher.getRecords(PollingRecordPublisher.java:168)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisher.run(PollingRecordPublisher.java:113)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisher.run(PollingRecordPublisher.java:102)
	at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.run(ShardConsumer.java:114)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.flink.kinesis.shaded.org.apache.http.conn.ConnectTimeoutException: Connect to kinesis.us-east-1.amazonaws.com:443 [kinesis.us-east-1.amazonaws.com/3.91.171.253] failed: connect timed out
	at org.apache.flink.kinesis.shaded.org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:151)
	at org.apache.flink.kinesis.shaded.org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:374)
	at jdk.internal.reflect.GeneratedMethodAccessor168.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.conn.ClientConnectionManagerFactory$Handler.invoke(ClientConnectionManagerFactory.java:76)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.conn.$Proxy47.connect(Unknown Source)
	at org.apache.flink.kinesis.shaded.org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393)
	at org.apache.flink.kinesis.shaded.org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)
	at org.apache.flink.kinesis.shaded.org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)
	at org.apache.flink.kinesis.shaded.org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)
	at org.apache.flink.kinesis.shaded.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)
	at org.apache.flink.kinesis.shaded.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1331)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	... 22 more
Caused by: java.net.SocketTimeoutException: connect timed out
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:609)
	at org.apache.flink.kinesis.shaded.org.apache.http.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:368)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.conn.ssl.SdkTLSSocketFactory.connectSocket(SdkTLSSocketFactory.java:142)
	at org.apache.flink.kinesis.shaded.org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142)
	... 37 more
{noformat}
This creates operational noise for us, and having to lose all progress since the last checkpoint is inefficient for a simple transient issue.

I think this could be solved if KinesisProxy.isRecoverableSdkClientException(e) recognized connect errors as being recoverable?",,dannycranmer,jkarp,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 04 10:36:44 UTC 2022,,,,,,,,,,"0|z0vy6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/21 07:25;martijnvisser;[~dannycranmer] Any thoughts on this one?;;;","19/Oct/21 07:51;dannycranmer;[~MartijnVisser] yes agree, we have also received the same feedback. I will add this to my queue and pick it up if still unclaimed. ;;;","26/Oct/21 22:32;jkarp;I've tested out this patch, it seems to work in my manual testing: https://github.com/apache/flink/compare/master...john-karp:retry-connect?expand=1

(The resulting log message is ""Got recoverable SdkClientException. Backing off for 261 millis (com.amazonaws.SdkClientException: Unable to execute HTTP request: Connect to kinesis.us-east-1.amazonaws.com:443 [kinesis.us-east-1.amazonaws.com/3.227.250.226] failed: connect timed out)"")

If the patch makes sense, I can add any appropriate unit tests.;;;","14/Dec/21 23:20;dannycranmer;Merged. [~jkarp] thank you for the contribution!;;;","04/Feb/22 10:36;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkSQL does not print correctly content of arrays nested into maps,FLINK-24579,13407112,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Sergey Nuyanzin,Sergey Nuyanzin,18/Oct/21 16:12,08/Nov/21 09:11,13/Jul/23 08:12,08/Nov/21 09:11,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"Some samples of queries to reproduce
{code:sql}
select map[array[2], 1];
select map[1, array[2]];
select map[array['q'], array[2]];
{code}",,libenchao,Sergey Nuyanzin,slinkydeveloper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21456,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 08 09:11:34 UTC 2021,,,,,,,,,,"0|z0vxu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/21 15:26;slinkydeveloper;Hi! Thanks for the PR but we are already extensively reworking the PrintUtils class to unify the code with the casting provided by the runtime, here are the details: https://issues.apache.org/jira/browse/FLINK-21456;;;","08/Nov/21 09:11;Sergey Nuyanzin;I'm closing this issue as after merging of [https://github.com/apache/flink/pull/17405]
it is not reproduced anymore;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Comparing timstamp_ltz with random string throws NullPointerException,FLINK-24563,13406713,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,15/Oct/21 07:00,15/Dec/21 01:44,13/Jul/23 08:12,31/Oct/21 07:42,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"Add the following test case to {{org.apache.flink.table.planner.runtime.batch.sql.CalcITCase}} to reproduce this issue.
{code:scala}
@Test
def myTest(): Unit = {
  val data: Seq[Row] = Seq(row(
    LocalDateTime.of(2021, 10, 15, 0, 0, 0).toInstant(ZoneOffset.UTC)))
  val dataId = TestValuesTableFactory.registerData(data)
  val ddl =
    s""""""
       |CREATE TABLE MyTable (
       |  ltz TIMESTAMP_LTZ
       |) WITH (
       |  'connector' = 'values',
       |  'data-id' = '$dataId',
       |  'bounded' = 'true'
       |)
       |"""""".stripMargin
  tEnv.executeSql(ddl)

  checkResult(""SELECT ltz = uuid() FROM MyTable"", Seq(row(null)))
}
{code}

The exception stack is
{code}
java.lang.RuntimeException: Failed to fetch next result

	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
	at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:370)
	at java.util.Iterator.forEachRemaining(Iterator.java:115)
	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:109)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:300)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:140)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:106)
	at org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.myTest(CalcITCase.scala:88)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
Caused by: java.io.IOException: Failed to fetch job execution result
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
	... 41 more
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
	... 43 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:614)
	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1983)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:134)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:181)
	... 43 more
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:228)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:218)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:209)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:678)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:537)
	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Caused by: java.lang.NullPointerException
	at BatchExecCalc$7.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:99)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:80)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.run(FromElementsFunction.java:231)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:116)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:73)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:330)
{code}

This is because {{ScalarOperatorGens#generateCast}} will generate the following code when casting strings to timestamp_ltz.
{code:java}
result$6 = org.apache.flink.table.data.TimestampData.fromEpochMillis(org.apache.flink.table.runtime.functions.SqlDateTimeUtils.toTimestamp(result$3.toString(), timeZone));
{code}

{{org.apache.flink.table.runtime.functions.SqlDateTimeUtils.toTimestamp}} might returns {{null}} while {{org.apache.flink.table.data.TimestampData.fromEpochMillis}} only accepts primitive long values, thus causing this issue.

What we need to do is to check the result of {{toTimestamp}}.",,jark,libenchao,lzljs3620320,TsReaper,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24424,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Oct 31 07:42:26 UTC 2021,,,,,,,,,,"0|z0vvdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/21 07:50;twalthr;Tests re-enabled in master: 34de7d1038f1078980cc539273b724ce7c85696a;;;","31/Oct/21 07:42;lzljs3620320;master: 75e5a89529f4c1bec87d7090e527ce64e9188bc1
release-1.14: 51c0af2ebe259034906a0e4054ae5c488a62782b
release-1.13: a0ce50e3d225e3f6f6dec7a2cda2fd74da248ed5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YarnResourceManagerDriverTest should not use ContainerStatusPBImpl.newInstance,FLINK-24562,13406712,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,15/Oct/21 06:51,30/Nov/21 20:38,13/Jul/23 08:12,20/Oct/21 02:21,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Deployment / YARN,,,,,0,pull-request-available,,,,"In YarnResourceManagerDriverTest, we create ContainerStatus with the static method ContainerStatusPBImpl{{.newInstance}}, which is annotated as private and unstable.

Although this method is still available in the latest version of yarn, some third-party versions of yarn may modify it. In fact, this method was modified in the internal version provided by our yarn team, which caused flink-1.14.0 to fail to compile.

Moreover, there is already an org.apache.flink.yarn.TestingContainerStatus, I think we should use it directly.
 ",,Feifan Wang,guoyangze,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 20 02:21:10 UTC 2021,,,,,,,,,,"0|z0vvdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Oct/21 07:49;guoyangze;I think it is a valid issue. Would you like to prepare a PR for it?
If not, I'd like to fix that issue.

cc [~trohrmann];;;","15/Oct/21 08:15;Feifan Wang;Thanks for reply [~guoyangze], I'm glad to open a PR for it.;;;","15/Oct/21 08:50;Feifan Wang;Hi [~guoyangze], I open a [PR|https://github.com/apache/flink/pull/17496] for it. Can you assigned this issue to me and help review the code ?;;;","15/Oct/21 09:18;guoyangze;[~trohrmann] Would you like to assign this ticket to Feifan?;;;","18/Oct/21 12:16;Feifan Wang;Hi [~xtsong], thanks for assign this ticket to me, and can you help review this [pull request|https://github.com/apache/flink/pull/17496] ?;;;","20/Oct/21 01:16;Feifan Wang;Hi [~xtsong], I hive modify the pr follow your comment.;;;","20/Oct/21 02:21;xtsong;Fixed in
- master (1.15): a08e7c6577557dce874b30df9c8b3692d9575282;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ineffective buffer debloat configuration in randomized tests,FLINK-24552,13406580,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,dwysakowicz,dwysakowicz,14/Oct/21 13:59,15/Dec/21 01:44,13/Jul/23 08:12,12/Nov/21 15:26,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Runtime / Configuration,Runtime / Task,,,,0,pull-request-available,,,,"The randomization in {{TestStreamEnvironment#setAsContext}} is ineffective, it is not used. 

The problem is that the buffer debloat can be configure only through the tasks manager configuration. Configuring through the {{StreamExecutionEnvironment}} is not possible.

We should either:
1. Fix the randomization
2. Implement configuring buffer debloating through {{StreamExecutionEnvironment#configure}}",,dwysakowicz,RocMarshal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 12 15:26:39 UTC 2021,,,,,,,,,,"0|z0vuk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Nov/21 15:26;dwysakowicz;Fixed:
* master
** d687f6527948cea33e80655b27499960ea2f414e
* 1.14.1
** e94f4de24b47c8b231ceb209b096e24dcfe2524e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BUFFER_DEBLOAT_SAMPLES property is taken from the wrong configuration,FLINK-24551,13406576,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,akalashnikov,akalashnikov,14/Oct/21 13:52,15/Dec/21 01:44,13/Jul/23 08:12,05/Nov/21 08:13,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,,,,,Runtime / Task,,,,,0,pull-request-available,,,,"Right now, StreamTask receives the BUFFER_DEBLOAT_SAMPLES property from the taskConfiguration which is wrong. The right place for the debloat configuration is taskManagerConfiguration.

 

 

Original task of this problem is https://issues.apache.org/jira/browse/FLINK-23726 but unfortunately, not all properties were fixed there.",,akalashnikov,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24470,,,,,,,,,,,,,,,,,,,,,FLINK-23726,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 05 07:11:37 UTC 2021,,,,,,,,,,"0|z0vujc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/21 09:00;akalashnikov;Wait for https://issues.apache.org/jira/browse/FLINK-24470. Highly likely this problem will be resolved there.;;;","05/Nov/21 05:26;ym;merged commit 3e341d0 into apache:release-1.14

[~akalashnikov] do you need to put this change into master as well?;;;","05/Nov/21 07:11;akalashnikov;[~ym], No, this doesn't need to be merged to master because this problem was resolved differently by another ticket in master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not access job information from a standby jobmanager UI,FLINK-24550,13406573,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,dwysakowicz,dwysakowicz,14/Oct/21 13:48,15/Feb/22 14:29,13/Jul/23 08:12,05/Nov/21 13:43,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Runtime / Coordination,Runtime / Web Frontend,,,,0,pull-request-available,,,,"One can not access the ""running jobs"" section (if a job is running) or if the job is completed it can not access the job page. Moreover the overview section does not work in the standby manager if a job is running. The active jobmanager UI works just fine.

{code}
2021-10-14 15:45:11,483 ERROR org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler [] - Unhandled exception.
java.util.concurrent.CancellationException: null
        at java.util.concurrent.CompletableFuture.cancel(CompletableFuture.java:2263) ~[?:1.8.0_231]
        at org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCache.getExecutionGraphInternal(DefaultExecutionGraphCache.java:98) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCache.getExecutionGraphInfo(DefaultExecutionGraphCache.java:67) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.handleRequest(AbstractExecutionGraphHandler.java:81) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.rest.handler.AbstractRestHandler.respondToRequest(AbstractRestHandler.java:83) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.rest.handler.AbstractHandler.respondAsLeader(AbstractHandler.java:195) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.lambda$channelRead0$0(LeaderRetrievalHandler.java:83) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
        at java.util.Optional.ifPresent(Optional.java:159) [?:1.8.0_231]
        at org.apache.flink.util.OptionalConsumer.ifPresent(OptionalConsumer.java:45) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:80) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:49) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.rest.handler.router.RouterHandler.routed(RouterHandler.java:115) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:94) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:55) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:238) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:71) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_231]
{code}

It seems to be working just fine in 1.13.2

Reported in the ML:  https://lists.apache.org/thread.html/r69646f1c943846ed07f9ff80232c8d0cea31222191354871f914484c%40%3Cuser.flink.apache.org%3E",,dwysakowicz,stuxnet,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24496,,,,,,,,,,,,,,,FLINK-24706,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 15 14:29:36 UTC 2022,,,,,,,,,,"0|z0vuio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/21 11:42;chesnay;I was able to reproduce the issue immediately; currently checking since when this issue exists.;;;","29/Oct/21 12:34;chesnay;The deserialization of the ArchivedExecutionGraph fails on standby job managers due to a classloading issue:

{code}
Caused by: org.apache.flink.runtime.rpc.exceptions.RpcException: Could not deserialize the serialized payload of RPC method : requestExecutionGraphInfo
	... 41 more
Caused by: java.lang.ClassNotFoundException: org.apache.commons.math3.stat.descriptive.rank.Percentile
{code};;;","05/Nov/21 13:43;chesnay;master: 9b1529cbd37cba866917f76cc224bd7d1e637c3d
1.14: 341c13aed36f8d3d633298645002af516dec9050;;;","15/Feb/22 14:29;stuxnet;Hello, is this issue really resolved ? We're still experiencing the same with Flink 1.14.3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Acknowledged description miss on Monitoring Checkpointing page,FLINK-24546,13406544,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,camilesing,camilesing,camilesing,14/Oct/21 11:32,20/Oct/21 06:10,13/Jul/23 08:12,20/Oct/21 06:10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Documentation,,,,,0,pull-request-available,,,,"!image-2021-10-14-19-26-33-289.png!

Acknowledged description miss on Monitoring Checkpointing page",,Alston Williams,camilesing,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/21 11:26;camilesing;image-2021-10-14-19-26-33-289.png;https://issues.apache.org/jira/secure/attachment/13034962/image-2021-10-14-19-26-33-289.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 20 06:10:51 UTC 2021,,,,,,,,,,"0|z0vuc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/21 12:24;Alston Williams;I can fix it. Please assign it to me. [~jark];;;","20/Oct/21 06:10;yunta;[~Alston Williams], thanks for your enthusiasm!
As I wrote in [previous comment](https://github.com/apache/flink/pull/17482#issuecomment-947302284), I think you could create issue to help improve documentation of subtask details in history tab which missed the description of Processed (persisted) data.;;;","20/Oct/21 06:10;yunta;merged in master: 6b405f6318b82d759fbd93f9a6af213cda72374d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
	Zookeeper connection issue causes inconsistent state in Flink,FLINK-24543,13406512,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dmvk,qinjunjerry,qinjunjerry,14/Oct/21 08:26,05/Jul/22 07:06,13/Jul/23 08:12,26/Nov/21 17:06,1.13.3,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"Env: Flink 1.13.2 with Zookeeper HA

Here is what happened:
{code:bash}
# checkpoint 1116 was triggered
2021-10-09 00:16:49,555 [Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1116 (type=CHECKPOINT) @ 1633738609538 for job a8a4fb85b681a897ba118db64333c9e5.

# a few seconds later, zookeeper connection suspended, it turned out to be a disk issue at zookeeper side caused slow fsync and commit)
2021-10-09 00:16:58,563 [Curator-ConnectionStateManager-0] WARN  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver [] - Connection to ZooKeeper suspended. Can no longer retrieve the leader from ZooKeeper.
2021-10-09 00:16:58,563 [Curator-ConnectionStateManager-0] WARN  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver [] - Connection to ZooKeeper suspended. The contender LeaderContender: DefaultDispatcherRunner no longer participates in the leader election.

# job was switching to suspended
2021-10-09 00:16:58,564 [flink-akka.actor.default-dispatcher-61] INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager b79b79fe513fb5f47e7bf447b7d9448c@akka.tcp://flink@flink-...-jobmanager:50010/user/rpc/jobmanager_3 for job a8a4fb85b681a897ba118db64333c9e5 from the resource manager.

2021-10-09 00:16:58,565 [flink-akka.actor.default-dispatcher-92] INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager b79b79fe513fb5f47e7bf447b7d9448c@akka.tcp://flink@flink-...-jobmanager:50010/user/rpc/jobmanager_3 for job a8a4fb85b681a897ba118db64333c9e5.


2021-10-09 00:16:58,565 [flink-akka.actor.default-dispatcher-90] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job Flink ...(a8a4fb85b681a897ba118db64333c9e5).

2021-10-09 00:16:58,567 [flink-akka.actor.default-dispatcher-90] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Flink ... (a8a4fb85b681a897ba118db64333c9e5) switched from state RUNNING to SUSPENDED.


2021-10-09 00:16:58,570 [flink-akka.actor.default-dispatcher-86] INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver [] - Closing ZookeeperLeaderRetrievalDriver{retrievalPath='/leader/a8a4fb85b681a897ba118db64333c9e5/job_manager_lock'}.


2021-10-09 00:16:58,667 [flink-akka.actor.default-dispatcher-92] INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job a8a4fb85b681a897ba118db64333c9e5 reached terminal state SUSPENDED.

# zookeeper connector restored
2021-10-09 00:17:08,225 [Curator-ConnectionStateManager-0] INFO  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver [] - Connection to ZooKeeper was reconnected. Leader election can be restarted.

# received checkpoint acknowledgement, trying to finalize it, then failed to add to zookeeper due to KeeperException$NodeExistsException
2021-10-09 00:17:14,382 [flink-akka.actor.default-dispatcher-90] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: ... (1/5) (09d25852e3e206d6b7fe0d6bc965870f) switched from RUNNING to CANCELING.
2021-10-09 00:17:14,382 [jobmanager-future-thread-1] WARN  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Error while processing AcknowledgeCheckpoint message
org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete the pending checkpoint 1116. Failure reason: Failure to finalize checkpoint.
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1227) 
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:1072) 
	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$acknowledgeCheckpoint$1(ExecutionGraphHandler.java:89) 
	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119) 
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:834) [?:?]
Caused by: org.apache.flink.runtime.persistence.StateHandleStore$AlreadyExistException: ZooKeeper node /0000000000000001116 already exists.
	at org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStore.lambda$addAndLock$0(ZooKeeperStateHandleStore.java:179) 
	at java.util.Optional.map(Optional.java:265) ~[?:?]
	at org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStore.addAndLock(ZooKeeperStateHandleStore.java:177) 
	at org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore.addCheckpoint(DefaultCompletedCheckpointStore.java:182) 
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1209) 
	... 9 more
Caused by: org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists
	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:122) ~[flink-shaded-zookeeper-3.4.14.jar:3.4.14-13.0-stream1]
	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:1015) ~[flink-shaded-zookeeper-3.4.14.jar:3.4.14-13.0-stream1]
	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:919) ~[flink-shaded-zookeeper-3.4.14.jar:3.4.14-13.0-stream1]
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorTransactionImpl.doOperation(CuratorTransactionImpl.java:197) ~[flink-shaded-zookeeper-3.4.14.jar:3.4.14-13.0-stream1]
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorTransactionImpl.access$000(CuratorTransactionImpl.java:37) ~[flink-shaded-zookeeper-3.4.14.jar:3.4.14-13.0-stream1]
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorTransactionImpl$2.call(CuratorTransactionImpl.java:130) ~[flink-shaded-zookeeper-3.4.14.jar:3.4.14-13.0-stream1]
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorTransactionImpl$2.call(CuratorTransactionImpl.java:126) ~[flink-shaded-zookeeper-3.4.14.jar:3.4.14-13.0-stream1]
	at org.apache.flink.shaded.curator4.org.apache.curator.connection.StandardConnectionHandlingPolicy.callWithRetry(StandardConnectionHandlingPolicy.java:64) ~[flink-shaded-zookeeper-3.4.14.jar:3.4.14-13.0-stream1]
	at org.apache.flink.shaded.curator4.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:100) ~[flink-shaded-zookeeper-3.4.14.jar:3.4.14-13.0-stream1]
	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorTransactionImpl.commit(CuratorTransactionImpl.java:123) ~[flink-shaded-zookeeper-3.4.14.jar:3.4.14-13.0-stream1]
	at org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStore.writeStoreHandleTransactionally(ZooKeeperStateHandleStore.java:204) 
	at org.apache.flink.runtime.zookeeper.ZooKeeperStateHandleStore.addAndLock(ZooKeeperStateHandleStore.java:165) 
	at org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore.addCheckpoint(DefaultCompletedCheckpointStore.java:182) 
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1209) 
... 9 more

# checkpoint coordinator was stopping
2021-10-09 00:17:14,385 [flink-akka.actor.default-dispatcher-90] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job a8a4fb85b681a897ba118db64333c9e5.
2021-10-09 00:17:14,401 [flink-akka.actor.default-dispatcher-90] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job a8a4fb85b681a897ba118db64333c9e5 has been suspended.

# clean up
2021-10-09 00:17:14,403 [AkkaRpcService-Supervisor-Termination-Future-Executor-thread-1] INFO  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver [] - Closing ZooKeeperLeaderElectionDriver{leaderPath='/leader/a8a4fb85b681a897ba118db64333c9e5/job_manager_lock'}
2021-10-09 00:17:14,404 [cluster-io-thread-2] INFO  org.apache.flink.runtime.jobmanager.DefaultJobGraphStore     [] - Released job graph a8a4fb85b681a897ba118db64333c9e5 from ZooKeeperStateHandleStore{namespace='flink/flink-.../jobgraphs'}.

# however, during recovery, checkpoint 1116 was found in zookeeper, but the metadata file /mnt/flink/.flink/ha/flink-.../completedCheckpoint42683d1121c7 was cleaned up due to the KeeperException$NodeExistsException happened before
2021-10-09 00:18:18,678 [jobmanager-future-thread-1] INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore [] - Recovering checkpoints from ZooKeeperStateHandleStore{namespace='flink/flink-.../checkpoints/a8a4fb85b681a897ba118db64333c9e5'}.
2021-10-09 00:18:18,686 [jobmanager-future-thread-1] INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore [] - Found 4 checkpoints in ZooKeeperStateHandleStore{namespace='flink/flink-.../checkpoints/a8a4fb85b681a897ba118db64333c9e5'}.
2021-10-09 00:18:18,686 [jobmanager-future-thread-1] INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore [] - Trying to fetch 4 checkpoints from storage.
2021-10-09 00:18:18,686 [jobmanager-future-thread-1] INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore [] - Trying to retrieve checkpoint 1113.
2021-10-09 00:18:18,689 [jobmanager-future-thread-1] INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore [] - Trying to retrieve checkpoint 1114.
2021-10-09 00:18:18,691 [jobmanager-future-thread-1] INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore [] - Trying to retrieve checkpoint 1115.
2021-10-09 00:18:18,693 [jobmanager-future-thread-1] INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore [] - Trying to retrieve checkpoint 1116.

2021-10-09 00:18:18,700 [flink-akka.actor.default-dispatcher-18] ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Fatal error occurred in the cluster entrypoint.
org.apache.flink.util.FlinkException: JobMaster for job a8a4fb85b681a897ba118db64333c9e5 failed.
	at org.apache.flink.runtime.dispatcher.Dispatcher.jobMasterFailed(Dispatcher.java:873) 
	at org.apache.flink.runtime.dispatcher.Dispatcher.jobManagerRunnerFailed(Dispatcher.java:459) 
	at org.apache.flink.runtime.dispatcher.Dispatcher.handleJobManagerRunnerResult(Dispatcher.java:436) 
	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$runJob$3(Dispatcher.java:415) 
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930) ~[?:?]
	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907) ~[?:?]
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440) 
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208) 
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77) 
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) 
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) 
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) 
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) 
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) 
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) 
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) 
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) 
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) 
	at akka.actor.Actor.aroundReceive(Actor.scala:517) 
	at akka.actor.Actor.aroundReceive$(Actor.scala:515) 
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) 
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) 
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) 
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) 
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) 
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) 
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) 
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) 
Caused by: org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
	at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.lambda$new$0(DefaultJobMasterServiceProcess.java:97) 
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) ~[?:?]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) ~[?:?]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) ~[?:?]
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1705) ~[?:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
	at java.lang.Thread.run(Thread.java:834) ~[?:?]
Caused by: java.util.concurrent.CompletionException: java.lang.RuntimeException: org.apache.flink.util.FlinkException: Could not retrieve checkpoint 1116 from state handle under /0000000000000001116. This indicates that the retrieved state handle is broken. Try cleaning the state handle store.
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:314) ~[?:?]
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:319) ~[?:?]
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1702) ~[?:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
	at java.lang.Thread.run(Thread.java:834) ~[?:?]
Caused by: java.lang.RuntimeException: org.apache.flink.util.FlinkException: Could not retrieve checkpoint 1116 from state handle under /0000000000000001116. This indicates that the retrieved state handle is broken. Try cleaning the state handle store.
	at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:316) 
	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:114) 
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700) ~[?:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
	at java.lang.Thread.run(Thread.java:834) ~[?:?]
Caused by: org.apache.flink.util.FlinkException: Could not retrieve checkpoint 1116 from state handle under /0000000000000001116. This indicates that the retrieved state handle is broken. Try cleaning the state handle store.
	at org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore.retrieveCompletedCheckpoint(DefaultCompletedCheckpointStore.java:309) 
	at org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore.recover(DefaultCompletedCheckpointStore.java:151) 
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreLatestCheckpointedStateInternal(CheckpointCoordinator.java:1513) 
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreInitialCheckpointIfPresent(CheckpointCoordinator.java:1476) 
	at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:134) 
	at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:342) 
	at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:190) 
	at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:122) 
	at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:132) 
	at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:110) 
	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:340) 
	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:317) 
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:107) 
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:95) 
	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112) 
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700) ~[?:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
	at java.lang.Thread.run(Thread.java:834) ~[?:?]
Caused by: java.io.FileNotFoundException: /mnt/flink/.flink/ha/flink-.../completedCheckpoint42683d1121c7 (No such file or directory)
	at java.io.FileInputStream.open0(Native Method) ~[?:?]
	at java.io.FileInputStream.open(FileInputStream.java:219) ~[?:?]
	at java.io.FileInputStream.<init>(FileInputStream.java:157) ~[?:?]
	at org.apache.flink.core.fs.local.LocalDataInputStream.<init>(LocalDataInputStream.java:50) 
	at org.apache.flink.core.fs.local.LocalFileSystem.open(LocalFileSystem.java:134) 
	at org.apache.flink.runtime.state.filesystem.FileStateHandle.openInputStream(FileStateHandle.java:68) 
	at org.apache.flink.runtime.state.RetrievableStreamStateHandle.openInputStream(RetrievableStreamStateHandle.java:66) 
	at org.apache.flink.runtime.state.RetrievableStreamStateHandle.retrieveState(RetrievableStreamStateHandle.java:58) 
	at org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore.retrieveCompletedCheckpoint(DefaultCompletedCheckpointStore.java:298) 
	at org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore.recover(DefaultCompletedCheckpointStore.java:151) 
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreLatestCheckpointedStateInternal(CheckpointCoordinator.java:1513) 
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreInitialCheckpointIfPresent(CheckpointCoordinator.java:1476) 
	at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:134) 
	at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:342) 
	at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:190) 
	at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:122) 
	at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:132) 
	at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:110) 
	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:340) 
	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:317) 
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:107) 
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:95) 
	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112) 
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700) ~[?:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
	at java.lang.Thread.run(Thread.java:834) ~[?:?]

 {code}",,Brian Zhou,claudio.fahey@emc.com,dmvk,guoyangze,klion26,mzuehlke,qinjunjerry,trohrmann,victorunique,zhuzh,,,,,,,,,,,,,,,,,,,,FLINK-24707,,,,,,,,,,,,,,,,,,,,FLINK-22494,CURATOR-584,FLINK-25265,FLINK-25098,FLINK-25018,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 26 17:06:51 UTC 2021,,,,,,,,,,"0|z0vu54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/21 09:03;qinjunjerry;One thought after discussed with [~trohrmann]: the reason for the KeeperException$NodeExistsException may be due to the zookeeper client's retries as it might not get the response from the zookeeper server in time. This should be handled correctly by curator transactions but it seems not the case. ;;;","14/Oct/21 09:35;victorunique;After the KeeperException$NodeExistsException was thrown, the *ZooKeeperStateHandleStore.indicatesPossiblyInconsistentState(e)* returned *false* so the state cleaning logic cleaned both the metadata file and the corresponding checkpoints data files which then led to the FileNotFoundException:
{noformat}
Caused by: java.io.FileNotFoundException: /mnt/flink/.flink/ha/flink-.../completedCheckpoint42683d1121c7 (No such file or directory)
{noformat}
The reason was that *NodeExistsException* was included in the PRE_COMMIT_EXCEPTIONS. But shouldn't we consider this NodeExistsException as an inconsistent state and remove it from the PRE_COMMIT_EXCEPTIONS? It's different from other ZK commit exceptions (e.g. AuthFailedException, BadVersionException, etc.) as others mean that the ZK write is failed but this one means the ZK node is already there, though we don't know if it's the correct one or not. So I think we should mark it as an inconsistent state and don't remove the corresponding metadata and checkpoint data files.

 

 ;;;","14/Oct/21 09:51;qinjunjerry;It looks like the latest Curator 5.2.0 (released Jul 26, 2021), with contribution from Indeed, fixed exactly this problem: https://issues.apache.org/jira/browse/CURATOR-584 

See also their blog post: [https://medium.com/@jmslocum16/idempotent-fault-tolerant-writes-in-curator-5-2-0-eadf7c12c814];;;","19/Oct/21 08:14;trohrmann;Good findings [~qinjunjerry]. CURATOR-584 should indeed solve our problems.;;;","19/Oct/21 08:35;chesnay;Curator 5 no longer supports Zookeeper 3.4 though. We can bundle it exclusively with the ZK 3.5, but I suppose we should think about dropping 3.4 support and/or making 3.5 the default.;;;","19/Oct/21 10:57;dmvk;I think we should be fairly safe dropping the 3.4 support here. The very last past release two 3.4.x branch was more then two years ago (2 April, 2019) and there are already three newer minor versions available (3.5.x,3.6.x,3.7.x).;;;","26/Nov/21 17:06;trohrmann;Fixed via

1.15.0: 3d7ad08ea24cb632d0d3db6dc942bb052ab9b464
1.14.1: 0a5481b2717dfe4d2e7c15ee98df36f85ff94d70
1.13.4: 07945ea8b346476ef17bd560671f8d7bfb14f0ee;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConfigurationUtils#assembleDynamicConfigsStr should consider special characters,FLINK-24541,13406479,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,guoyangze,guoyangze,14/Oct/21 06:31,27/Oct/21 03:14,13/Jul/23 08:12,27/Oct/21 03:14,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / Configuration,,,,,0,pull-request-available,,,,"Without quoting, some special characters will be misunderstood by shell, e.g. ';' used in list type options.",,guoyangze,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 27 03:14:28 UTC 2021,,,,,,,,,,"0|z0vtxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/21 06:32;guoyangze;I'd like to take this ticket.;;;","21/Oct/21 07:05;guoyangze;cc [~xtsong];;;","27/Oct/21 03:14;xtsong;Fixed via:
- master (1.15): 44e75ecdf9ca15f8a028f77efd6cc7da5a7e4549;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Resource leak due to Files.list ,FLINK-24540,13406474,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,xiaoheipangzi,xiaoheipangzi,xiaoheipangzi,14/Oct/21 05:52,15/Dec/21 01:44,13/Jul/23 08:12,15/Oct/21 06:49,1.13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,,,,,,0,pull-request-available,,,,Files.list will open dir and we should close it,,xiaoheipangzi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 15 06:49:08 UTC 2021,,,,,,,,,,"0|z0vtwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Oct/21 06:49;chesnay;master: d1425bebd9ae5e326a381baf06227f42a128a47a
1.14: f37c6ad71f9ca0108d5216110a5942bb5f3dadfa ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperLeaderElectionTest.testLeaderShouldBeCorrectedWhenOverwritten fails with NPE,FLINK-24538,13406469,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,autophagy,xtsong,xtsong,14/Oct/21 05:07,14/Mar/22 08:21,13/Jul/23 08:12,14/Mar/22 08:21,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.5,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25020&view=logs&j=f2b08047-82c3-520f-51ee-a30fd6254285&t=3810d23d-4df2-586c-103c-ec14ede6af00&l=7573

{code}
Oct 13 22:26:04 [ERROR] Tests run: 8, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 12.355 s <<< FAILURE! - in org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest
Oct 13 22:26:04 [ERROR] testLeaderShouldBeCorrectedWhenOverwritten  Time elapsed: 1.138 s  <<< ERROR!
Oct 13 22:26:04 java.lang.NullPointerException
Oct 13 22:26:04 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.testLeaderShouldBeCorrectedWhenOverwritten(ZooKeeperLeaderElectionTest.java:434)
{code}",,gaoyunhaii,mapohl,xmarker,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 11 16:00:24 UTC 2022,,,,,,,,,,"0|z0vtvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Oct/21 15:33;xmarker;I investigate the issue related code and i think the issue may be occur with flowing scene:

1.When call `retrievalEventHandler.waitForNewLeader(timeout)` at line 434, in TestingRetrievalBase.waitForNewLeader wait a correct leader information

2. But when return `leader.getLeaderAddress()` in TestingRetrievalBase the leaderRetrievalDriver was notified a empty leader information(may be zookeeper connection suspend or lost, see ZooKeeperLeaderRetrievalDriver.handleStateChange)

3. So we can use a local variable to store leaderEventQueue.poll 's result in case the object field variable change .

[~wangyang0918]   do you have any good advice ? ;;;","15/Oct/21 16:27;xmarker;I have push some code( [https://github.com/apache/flink/pull/17502] ) ,can someone help with the code review ?;;;","24/Feb/22 07:26;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32098&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=7158;;;","08/Mar/22 06:38;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32649&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=7254;;;","10/Mar/22 09:20;mapohl;I re-assigned the issue to [~autophagy]. She's going to pick up the work to provide another PR.;;;","11/Mar/22 16:00;mapohl;master: f787a220d51cad6cdefdad49086d46dc6376b682
1.14: faad65c0b1ed5ec1438eaec3eb417c29c282dc0b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointingTimeBenchmark can fail with connection timed out,FLINK-24531,13406316,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,pnowojski,pnowojski,13/Oct/21 10:20,14/Oct/21 08:23,13/Jul/23 08:12,14/Oct/21 08:23,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Benchmarks,,,,,0,pull-request-available,,,,"There is a race condition between obtaining REST API port and actually starting up the REST API server. If REST API server start is delayed, the {{CheckpointingTimeBenchmark}} can fail with

{noformat}
java.util.concurrent.ExecutionException: org.apache.flink.shaded.netty4.io.netty.channel.ConnectTimeoutException: connection timed out: localhost/127.0.0.1:60503
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
	at org.apache.flink.benchmark.CheckpointingTimeBenchmark$CheckpointEnvironmentContext.waitForBackpressure(CheckpointingTimeBenchmark.java:251)
	at org.apache.flink.benchmark.CheckpointingTimeBenchmark$CheckpointEnvironmentContext.setUp(CheckpointingTimeBenchmark.java:216)
	at org.apache.flink.benchmark.generated.CheckpointingTimeBenchmark_checkpointSingleInput_jmhTest._jmh_tryInit_f_checkpointenvironmentcont
{noformat}
",,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 14 08:23:13 UTC 2021,,,,,,,,,,"0|z0vsyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/21 08:23;pnowojski; merged commit 6bb3bff into apache/flink-benchmarks:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkKafkaProducer example is not compiling due to incorrect constructer signature used,FLINK-24509,13405967,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,alibahadirzeybek,alibahadirzeybek,alibahadirzeybek,11/Oct/21 15:16,15/Dec/21 01:41,13/Jul/23 08:12,03/Nov/21 11:04,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,,,,,Documentation,,,,,0,pull-request-available,,,,"The version of the constructor that is used in the example (https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/connectors/datastream/kafka/#kafka-producer) does not exist, so the code example does not compile.",,alibahadirzeybek,danderson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 03 11:04:18 UTC 2021,,,,,,,,,,"0|z0vqsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/21 11:04;danderson;Fixed in release-1.13 by 

124ffefd4430bb8a86db7815280d5f2d1e16b076;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
checkpoint directory is not configurable through the Flink configuration passed into the StreamExecutionEnvironment,FLINK-24506,13405915,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,11/Oct/21 11:03,15/Dec/21 01:44,13/Jul/23 08:12,03/Dec/21 09:39,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Runtime / Configuration,Runtime / State Backends,,,,0,pull-request-available,,,,"FLINK-19463 introduced the separation of {{StateBackend}} and {{{}CheckpointStorage{}}}. Before that, both were included in the same interface implementation [AbstractFileStateBackend|https://github.com/apache/flink/blob/0a76daba0a428a322f0273d7dc6a70966f62bf26/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/AbstractFileStateBackend.java]. {{FsStateBackend}} was used as a default implementation pre-1.13.

pre-{{{}1.13{}}} initialized the checkpoint directory when instantiating the state backend (see [FsStateBackendFactory|https://github.com/apache/flink/blob/release-1.12/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsStateBackendFactory.java#L46]). Starting from {{1.13}} loading the {{CheckpointStorage}} is done by the {{CheckpointStorageLoader.load}} method that is called in various places:
 * Savepoint Disposal (through {{{}Checkpoints.loadCheckpointStorage{}}}) where it only relies on the configuration passed in by the cluster configuration (no application checkpoint storage is passed)
 * {{SchedulerBase}} initialization (through DefaultExecutionGraphBuilder) where it’s based on the cluster’s configuration but also the application configuration (i.e. the {{{}JobGraph{}}}’s setting) that would be considered if {{CheckpointConfig#configure}} would have the checkpoint storage included
 * {{StreamTask}} on the {{{}TaskManager{}}}’s side where it’s based on the configuration passed in by the {{JobVertex}} for the application’s {{CheckpointStorage}} and the {{{}TaskManager{}}}’s configuration (coming from the session cluster) for the fallback {{CheckpointStorage}}

The issue is that we don't set the checkpoint directory in the {{{}CheckpointConfig{}}}. Hence, it's not going to get picked up as a job-related property. Flink always uses the fallback provided by the session cluster configuration.",,dmvk,mapohl,maver1ck,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19463,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 03 09:39:25 UTC 2021,,,,,,,,,,"0|z0vqhc:",9223372036854775807,FLINK-19463 introduced a change in Flink 1.13+ where the configuration passed in through the StreamExecutionEnvironment wouldn't be considered for the state.checkpoints.dir parameter. This is fixed now bringing back the old behavior which was available in 1.12-.,,,,,,,,,,,,,,,,,,,"01/Dec/21 08:54;mapohl;Both, {{StreamPlanEnvironment}} and {{StreamContextEnvironment}} are intialized using the Flink configuration (which includes the checkpoint directory). Both classes derive from {{{}StreamExecutionEnvironment{}}}. The {{StreamExecutionEnvironment}} initializes the {{CheckpointConfig}} (see [StreamExecutionEnvironment:975|https://github.com/apache/flink/blob/b4c385e41832f16e39d5cbe4fb69ead9bbe077b2/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java#L975]). But it doesn’t set the checkpoint directory in [CheckpointConfig#configure|https://github.com/apache/flink/blob/cd01d4c02793d1b29618093f730b3bc521152b62/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/CheckpointConfig.java#L753]. This was previously set when initializing the {{StateBackend}} in [StreamExecutionEnvironment#configure|https://github.com/apache/flink/blob/b4c385e41832f16e39d5cbe4fb69ead9bbe077b2/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java#L919] which calls [StateBackendLoader#loadStateBackendFromConfig|https://github.com/apache/flink/blob/641c31e92fd8ff3702d2ac3510a63b0653802a2e/flink-runtime/src/main/java/org/apache/flink/runtime/state/StateBackendLoader.java#L146].

{{FsStateBackend}} was initialized using the checkpoint directory. {{HashMapStateBackend}} does not include this pointer anymore but relies on the {{CheckpointStorage}} instead that is loaded through [CheckpointStorageLoader.load|https://github.com/apache/flink/blob/658fac3736b73adf54b629242ede91313947e7e1/flink-runtime/src/main/java/org/apache/flink/runtime/state/CheckpointStorageLoader.java#L158] which is called when creating the ExecutionGraph in [DefaultExecutionGraphBuilder:269|https://github.com/apache/flink/blob/eba8f574c550123004ed4f557cef28ff557cd88e/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/DefaultExecutionGraphBuilder.java#L269]. The {{CheckpointStorage}} is either loaded from the JobManager configuration (i.e. the Session cluster’s configuration) or from the application (i.e. the {{{}JobGraph{}}}). But the {{JobGraph}} does not have this setting set due to it not being written [CheckpointConfig#configure|https://github.com/apache/flink/blob/cd01d4c02793d1b29618093f730b3bc521152b62/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/CheckpointConfig.java#L753] as written earlier already.

The {{CheckpointStorage}} is loaded in three locations:
 * Savepoint Disposal (through [Checkpoints.loadCheckpointStorage|https://github.com/apache/flink/blob/4597d5557c640e0ef5a526cbb6d46686be5dd813/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/Checkpoints.java#L351]) where it only relies on the configuration passed in by the cluster configuration (no application checkpoint storage is passed)

 * Scheduler initialization (through [DefaultExecutionGraphBuilder|https://github.com/apache/flink/blob/f7bedb0603c33cb4e25c62c9899edb709b264371/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/DefaultExecutionGraphBuilder.java#L268]) where it’s based on the cluster’s configuration but also the application configuration (i.e. the JobGraph’s setting) that would be considered if [CheckpointConfig#configure|https://github.com/apache/flink/blob/cd01d4c02793d1b29618093f730b3bc521152b62/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/CheckpointConfig.java#L753] would have the checkpoint storage included

 * [StreamTask|https://github.com/apache/flink/blob/79a801a7bf669813d88784fd642d724d6dab69f4/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java#L1505] on the TaskManager’s side where it’s based on the configuration passed in by the {{JobVertex}} for the application’s {{CheckpointStorage}} and the TaskManager’s configuration (coming from the session cluster) for the fallback {{CheckpointStorage}}

For the latter two, we could solve the issue of not having the checkpoint directory being loaded from the job spec by considering the option in [CheckpointConfig#configure|https://github.com/apache/flink/blob/cd01d4c02793d1b29618093f730b3bc521152b62/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/CheckpointConfig.java#L753]

The Savepoint Disposal is save even with this change because it relies on an external pointer for the savepoint and does not use the checkpoint directory at all.;;;","03/Dec/21 09:39;chesnay;master: 83cad724890e4904758b46f0e292a64fc1a5d1d4
1.14: 7e4cc6fc23732a151e5e4eab58988b4e99b290cf
1.13: 98c49d63efa7723ed5394e503f7f0c3f54061456;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unexpected behavior of cumulate window aggregate for late event after recover from sp/cp,FLINK-24501,13405889,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingzhang,jingzhang,jingzhang,11/Oct/21 08:50,10/Nov/21 07:16,13/Jul/23 08:12,10/Nov/21 07:16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"*Problem description*

After recover from savepoint or checkpoint, unexpected behavior of cumulate window aggregate for late event may happened.

*Bug analyze*

Currently, for cumulate window aggregate, late events belongs to the cleaned slice would be merged into the merged window state, and would be counted into the later slice.

For example, for a CUMULATE window, step is 1 minute, size is 1 day.
{code:java}
SELECT window_start, window_end, COUNT(USER_ID)
  FROM TABLE(
    CUMULATE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL '1' MINUTES, INTERVAL '1' DAY))
  GROUP BY window_start, window_end;{code}
When the watermark already comes to 11:01, result of window [00:00, 11:01) would be emitted. Let's assume the result is INSERT (00:00, 11:01, 4)

Then if a late record which event time is 11:00 comes, it would be merged into merged state, and would be counted into the later slice, for example, for window [00:00, 11:02), [00:00, 11:03)... But the emitted window result INSERT (00:00, 11:01, 4) would not be retracted and updated.

The behavior would be different if the job recover from savepoint/checkpoint.

Let's do a savepoint after watermark comes to 11:01 and emit (00:00, 11:01, 4).

Then recover the job from savepoint. Watermarks are not checkpointed and they need to be repopulated again. So after recovered, the watermark may rollback to 11:00, then if a record which event time is 11:00 comes, it would not be processed as late event, after watermark comes to 11:01 again, a window result INSERT (00:00, 11:01, 5)  would be emitted to downstream.

So the downstream operator would receive two INSERT record for WINDOW (00:00, 11:01) which may leads to wrong result.

 

*Solution*

There are two solutions for the problem:
 # save watermark to state in slice shared operator. (Prefered)
 # update the behavior for late event. For example, retract the emitted result and send the updated result. It needs to change the behavior of slice state clean mechanism because we clean the slice state after watermark exceeds the slice end currently.

 ",,godfreyhe,icshuo,jark,Jiangang,jingzhang,leonard,libenchao,lzljs3620320,wenlong.lwl,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21305,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 10 07:16:35 UTC 2021,,,,,,,,,,"0|z0vqbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/21 12:43;Jiangang;+1 for the first solution. Recomputing watermark can cause the watermark's backward. The late events last time may be not late after restarting which can cause data inconsistent. The problem  occurs in other situations too.;;;","11/Oct/21 12:50;jingzhang;There are two solutions for the problem:
 # save watermark to state in slice shared operator. (Prefered)
 # update the behavior for late event. For example, retract the emitted result and send the updated result. It needs to change the behavior of slice state clean mechanism because we clean the slice state after watermark exceeds the slice end currently.

{color:#ff0000}*Those two solutions could solve current problem based on a minor update of SliceSharedWindowOperator. But could not avoid other problems caused by the watermark repopulated after recover job from savepoint/checkpoint.*{color}

For example, for Tumble window which size is 1min.

When the watermark already comes to 11:01, result of window [00:00, 11:01) would be emitted. Let's assume the result is INSERT (00:00, 11:01, 4)

Then if a late record which event time is 11:00 comes, it would be dropped as late event.

The behavior would be different if the job recover from savepoint/checkpoint.

Let's do a savepoint after watermark comes to 11:01 and emit (00:00, 11:01, 4).

Then recover the job from savepoint. Watermarks are not checkpointed and they need to be repopulated again. So after recovered, the watermark may rollback to 11:00, then if a record which event time is 11:00 comes, it would not be processed as late event, after watermark comes to 11:01 again, a window result INSERT (00:00, 11:01, 1)  would be emitted to downstream.

So the downstream operator would receive two INSERT record for WINDOW (00:00, 11:01) which may leads to wrong result. For example, the downstream is a KV sink, which key is [window_start, window_end], the later result may override the previous result.

So I'm wonder should we look for a better solution which could solve all the problem caused by the watermark repopulated. For example, save the watermark to state for all operators?

What do you think [~twalthr],[~jark] [~godfrey] [~lzljs3620320] ?

Any suggestion is helpful. 

 ;;;","12/Oct/21 10:31;lzljs3620320;[~qingru zhang] Thanks! This is a good problem and good summary.
Does old window have this problem too?;;;","12/Oct/21 12:45;jingzhang;[~lzljs3620320] I think all operators which depend on event-time would have similar problem, including Group Window Aggregate/Window TVF/Interval Join/Window Join/Window Rank in Flink SQL and Window Operator in DataStream. After recover from savepoint/checkpoint, the late event maybe not processed as late event any more because of watermark repopulated.

 ;;;","21/Oct/21 03:56;wenlong.lwl;I think it may be better to make sure that watermark would not reduce after restoring from a checkpoint/savepoint instead of modifying the manner of operator to cover such abnormal case. 
For example, add an operator state in watermark assigner, to avoid it producing wrong watermark after restore?;;;","28/Oct/21 03:13;jingzhang;[~wenlong.lwl] Thank a lot for your attention to this issue.

> add an operator state in watermark assigner, to avoid it producing wrong watermark after restore?

It is a rejected alternatives because of the following reasons, please correct me if I'm wrong.
 # I think we should not use operator state to store watermark.  Because if the job parallelism is changed when restore from a checkpoint/savepoint, how to merge each old watermark to a new watermark. If takes min value,  watermark may be reduce again after restoring from a checkpoint/savepoint. If takes max value, some normal data events would be mistaken processed as late event.
 # So we need keyed state to store watermark because of the above reason. But the watermark assigner operator does not and should not require inputs data to be keyedStream. Besides, if use keyed state to store watermark, we need read/write this state frequently. I am very worried that update this general operator (watermark assigner operator) will cause bad effect on performance.

So I decide to temporarily correct this bug in a small scope until we have a better general solution. 

Any better idea?;;;","28/Oct/21 03:20;wenlong.lwl;[~qingru zhang] I think watermark should be the min of all subtask, and nothing wrong would happen, because downstream operator such as window, also treat min of watermark of its upstream as its watermark, the logic is aligned.;;;","28/Oct/21 05:01;jingzhang;[~wenlong.lwl] If we took min watermark of all subtask as new watermark, we still could not avoid the bug in this Jira because some late data would be regarded as normal data after recover from sp/cp.;;;","28/Oct/21 06:16;wenlong.lwl;[~qingru zhang] in you case，if the watermark can be restore，the late event is still late event when restoring from sp/cp，because its event time is earlier than watermark restored. 
On the other hand, introducing per-key watermark state could increase the state size of window operator in long run and affects performance, because the progress state should never be clean up in order to recognize later event in restore in the future. such side effect could be worse when the window size is small because the space namespace will increase quickly.;;;","28/Oct/21 06:46;jingzhang;[~wenlong.lwl] 

> in you case，if the watermark can be restore，the late event is still late event when restoring from sp/cp，because its event time is earlier than watermark restored.

I don't understand, could you explain more? If parallelism is changed, and we took took min watermark of all subtask as new watermark, is there any possible that some late data would be regarded as normal data because watermark maybe degrade??

> On the other hand, introducing per-key watermark state could increase the state size of window operator in long run and affects performance, because the progress state should never be clean up in order to recognize later event in restore in the future. such side effect could be worse when the window size is small because the space namespace will increase quickly.

I admit read/write state would effects performance. But using operator state, we still need to read/write state.

About state size, keyed state which stores progress would be cleaned when the window finishes.;;;","10/Nov/21 07:16;lzljs3620320;master: cba6b2c89a0ecd8858c1849a9b06350db48fb450;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python installdeps hangs,FLINK-24495,13405842,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,xtsong,xtsong,11/Oct/21 02:32,23/Nov/21 08:48,13/Jul/23 08:12,23/Nov/21 08:48,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,API / Python,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24922&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23587

{code}
Oct 10 02:30:01 py38-cython create: /__w/1/s/flink-python/.tox/py38-cython
Oct 10 02:30:04 py38-cython installdeps: pytest, apache-beam==2.27.0, cython==0.29.16, grpcio>=1.29.0,<2, grpcio-tools>=1.3.5,<=1.14.2, apache-flink-libraries
Oct 10 02:45:22 ==============================================================================
Oct 10 02:45:22 Process produced no output for 900 seconds.
Oct 10 02:45:22 ==============================================================================
{code}",,dianfu,gaoyunhaii,hxbks2ks,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24934,,,,,,,,,,,,,,,FLINK-24764,FLINK-23493,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 23 08:48:19 UTC 2021,,,,,,,,,,"0|z0vq14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/21 02:33;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24878&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23585;;;","11/Oct/21 02:34;xtsong;cc [~dianfu] [~hxbks2ks];;;","14/Oct/21 02:00;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24995&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23587;;;","07/Nov/21 15:36;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26083&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23641;;;","08/Nov/21 07:45;trohrmann;[~dianfu] can you take a look?;;;","09/Nov/21 08:27;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26188&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23642

The exit code 143 could point towards a SIGTERM and this could point towards a memory issue.;;;","10/Nov/21 08:10;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26236&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23641;;;","11/Nov/21 08:26;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26316&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=22042

Any help with fixing this problem would be highly appreciated.;;;","11/Nov/21 08:29;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26326&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23650;;;","12/Nov/21 10:48;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26386&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23649;;;","15/Nov/21 08:51;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26496&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23652];;;","16/Nov/21 06:55;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26568&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23586];;;","17/Nov/21 08:38;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26624&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23583];;;","18/Nov/21 07:08;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26676&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23594];;;","19/Nov/21 06:03;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26722&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23596];;;","21/Nov/21 10:56;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26774&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23596;;;","21/Nov/21 11:10;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26783&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23596];;;","23/Nov/21 08:48;hxbks2ks;Merged into master via 50cfd6c84568b51c8e5e96e6ea789ac9d7fe0cc7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incorrect implicit type conversion between numeric and (var)char,FLINK-24492,13405764,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,xuyangzhong,xuyangzhong,xuyangzhong,09/Oct/21 09:47,15/Dec/21 01:44,13/Jul/23 08:12,31/Oct/21 13:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"The result of the sql ""select 1 = '1'"" is false. This is caused by the CodeGen. CodeGen  incorrectly transform this ""="" to ""BinaryStringData.equals (int 1)"". And ""<>"" has the same wrong result.

In my opinion, ""="" should have the same behavior with "">"" and ""<"", which have the correct results. So before calcite solves this bug or flink supports this kind of implicit type conversion, we'd better temporarily forbidding this implicit type conversion in ""="" and ""<>"".",,godfreyhe,libenchao,xuyangzhong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Oct 31 13:46:46 UTC 2021,,,,,,,,,,"0|z0vpjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/21 09:49;xuyangzhong;link https://issues.apache.org/jira/browse/FLINK-18234;;;","31/Oct/21 13:46;godfreyhe;Fixed in 1.15.0: 25dc6eacd38bd7a70883fc8e54f0bec8dc5861c0
Fixed in 1.14.1: 5fcc4a10eaabceeb131ddeb29379bd1c73b1303a
Fixed in 1.13.4: 5cbaa28d688e5e3f75aa28d5b69110497415df4b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutionGraphInfo may not be archived when the dispatcher terminates,FLINK-24491,13405743,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,Thesharing,Thesharing,09/Oct/21 07:04,03/Aug/22 11:48,13/Jul/23 08:12,08/Jun/22 08:58,1.13.6,1.14.4,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.5,1.15.1,1.16.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"When a job finishes, its JobManagerRunnerResult will be processed in the callback of {{Dispatcher#runJob}}. In the callback, ExecutionGraphInfo will be archived by HistoryServerArchivist asynchronously. However, the CompletableFuture of the archiving is ignored. The job may be removed before the archiving is finished. For the batch job running in the per-job/application mode, the dispatcher will terminate itself once the job is finished. In this case, ExecutionGraphInfo may not be archived when the dispatcher terminates.

If the ExecutionGraphInfo is lost, users are not able to know whether the batch job is finished normally or not. They have to refer to the logs for the result.

The session mode is not affected, since the dispatcher won't terminate itself once the job is finished. The HistoryServerArchivist gets enough time to archive the ExcutionGraphInfo.",,aitozi,danderson,maguowei,mapohl,Thesharing,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28531,,,,,,,,,,,,,,,FLINK-26772,,,,,,,,,FLINK-26976,FLINK-26984,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 08 08:58:28 UTC 2022,,,,,,,,,,"0|z0vpf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/22 02:43;Thesharing;Hi, [~zhuzh], I'd like to take this JIRA. Would you mind assigning it to me?;;;","30/Mar/22 02:45;zhuzh;Thanks for reporting this problem and offering to fix it. [~Thesharing]
I have assigned the ticket to you.;;;","26/Apr/22 07:02;mapohl;master: b3a9dcbd65719c742fe4907ec17de396b188d378
1.15: 12db77280c6314ffddda7bfc47a44bfed687538d;;;","27/Apr/22 09:20;mapohl;Reopening the ticket to also add a 1.14 backport.;;;","28/Apr/22 12:02;mapohl;1.14: f9d2c1c0d3aa7e72e95f1c75bcf7c77c1fceac22

The 1.14 backport required 1c492ed97fe2876041804b944c6cb370430b3519 and e0fb11741a000478852d51fa9f2823208e6717c6 to be added as backported commits as well;;;","08/Jun/22 08:53;danderson;[~mapohl] Can this issue be resolved/fixed/closed? It looks like you finished the 1.14 backport.;;;","08/Jun/22 08:58;mapohl;Yes, it appears that I forgot to resolve the issue after merging the 1.14 backport. Thanks for pinging me on it, [~danderson] (y);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaRecordSerializationSchemaBuilder does not forward timestamp,FLINK-24488,13405637,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xmarker,fpaul,fpaul,08/Oct/21 14:18,15/Dec/21 01:44,13/Jul/23 08:12,28/Oct/21 14:07,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"When building a KafkaRecordSerializationSchema with its builder the extracted event time timestamp is currently not set at the resulting ProducerRecord.

https://github.com/apache/flink/blob/026675a5cb8a3704c51802fb549d6b0bc4759835/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaRecordSerializationSchemaBuilder.java#L328",,fpaul,xmarker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 28 14:07:28 UTC 2021,,,,,,,,,,"0|z0vorc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/21 15:04;xmarker;May be KafkaRecordSerializationSchemaWrapper.serialize called a incorrect ProducerRecord construct method,I can help fix this issue;;;","13/Oct/21 15:10;fpaul;[~xmarker] yes you are right. We need to always use a constructor with a timestamp and set it. AFAIK the constructor fields of the ProducerRecord are all nullable. I assign you the ticket and I can help to review it.;;;","14/Oct/21 07:32;xmarker;OK ,think you !;;;","26/Oct/21 09:03;arvid;Merged into master as 0df9e1e6b85238458a65b14ac31e8fe48f6087f5.;;;","28/Oct/21 14:07;arvid;Merged into 1.14 as h8fffeb653587913f487776531b857cc5b8f5b54f.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EqualiserCodeGeneratorTest fails on azure,FLINK-24480,13405550,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,airblader,xtsong,xtsong,08/Oct/21 06:12,15/Dec/21 01:44,13/Jul/23 08:12,18/Oct/21 15:57,1.12.5,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.6,1.14.3,1.15.0,,Table SQL / Planner,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24809&view=logs&j=955770d3-1fed-5a0a-3db6-0c7554c910cb&t=14447d61-56b4-5000-80c1-daa459247f6a&l=42615

{code}
Oct 07 01:11:46 [ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 8.236 s <<< FAILURE! - in org.apache.flink.table.planner.codegen.EqualiserCodeGeneratorTest
Oct 07 01:11:46 [ERROR] testManyFields(org.apache.flink.table.planner.codegen.EqualiserCodeGeneratorTest)  Time elapsed: 8.21 s  <<< FAILURE!
Oct 07 01:11:46 java.lang.AssertionError: Expected compilation to succeed
Oct 07 01:11:46 	at org.junit.Assert.fail(Assert.java:88)
Oct 07 01:11:46 	at org.apache.flink.table.planner.codegen.EqualiserCodeGeneratorTest.testManyFields(EqualiserCodeGeneratorTest.java:102)
{code}",,airblader,dwysakowicz,jark,trohrmann,twalthr,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13374,,,,FLINK-22788,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 18 15:57:59 UTC 2021,,,,,,,,,,"0|z0vo80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Oct/21 06:25;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24830&view=logs&j=955770d3-1fed-5a0a-3db6-0c7554c910cb&t=14447d61-56b4-5000-80c1-daa459247f6a&l=42433;;;","08/Oct/21 06:29;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24831&view=logs&j=955770d3-1fed-5a0a-3db6-0c7554c910cb&t=14447d61-56b4-5000-80c1-daa459247f6a&l=42615;;;","08/Oct/21 09:40;trohrmann;Another instance: https://dev.azure.com/ververica-dev/daplatform-flink/_build/results?buildId=1557&view=logs&j=d1352042-8a7d-50b6-3946-a85d176b7981&t=b2322052-d503-5552-81e2-b3a532a1d7e8&l=43078;;;","08/Oct/21 09:41;trohrmann;Another instance: https://dev.azure.com/ververica-dev/daplatform-flink/_build/results?buildId=1555&view=logs&j=d1352042-8a7d-50b6-3946-a85d176b7981&t=b2322052-d503-5552-81e2-b3a532a1d7e8&l=43406;;;","09/Oct/21 02:56;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24879&view=logs&j=955770d3-1fed-5a0a-3db6-0c7554c910cb&t=14447d61-56b4-5000-80c1-daa459247f6a&l=42433;;;","09/Oct/21 03:06;xtsong;cc [~twalthr] [~jark];;;","09/Oct/21 03:18;jark;[~airblader] could you help to have a look at this?;;;","09/Oct/21 03:49;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24880&view=logs&j=955770d3-1fed-5a0a-3db6-0c7554c910cb&t=14447d61-56b4-5000-80c1-daa459247f6a&l=42615;;;","09/Oct/21 08:24;airblader;[~jark] I'm currently on vacation but can have a look when I'm back.;;;","11/Oct/21 02:27;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24923&view=logs&j=955770d3-1fed-5a0a-3db6-0c7554c910cb&t=14447d61-56b4-5000-80c1-daa459247f6a&l=42433;;;","11/Oct/21 02:36;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24924&view=logs&j=955770d3-1fed-5a0a-3db6-0c7554c910cb&t=14447d61-56b4-5000-80c1-daa459247f6a&l=42615;;;","11/Oct/21 03:26;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24931&view=logs&j=955770d3-1fed-5a0a-3db6-0c7554c910cb&t=14447d61-56b4-5000-80c1-daa459247f6a&l=42433;;;","12/Oct/21 02:00;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24966&view=logs&j=955770d3-1fed-5a0a-3db6-0c7554c910cb&t=14447d61-56b4-5000-80c1-daa459247f6a&l=42433;;;","14/Oct/21 02:05;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24997&view=logs&j=955770d3-1fed-5a0a-3db6-0c7554c910cb&t=14447d61-56b4-5000-80c1-daa459247f6a&l=42615;;;","14/Oct/21 02:46;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25018&view=logs&j=955770d3-1fed-5a0a-3db6-0c7554c910cb&t=14447d61-56b4-5000-80c1-daa459247f6a&l=42433;;;","14/Oct/21 03:25;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25019&view=logs&j=955770d3-1fed-5a0a-3db6-0c7554c910cb&t=14447d61-56b4-5000-80c1-daa459247f6a&l=42615;;;","14/Oct/21 06:43;airblader;The actual exception is unfortunately discarded. I've opened [PR #17469|https://github.com/apache/flink/pull/17469] to get a better insight here. Would appreciate if someone could push this through.;;;","14/Oct/21 11:20;xtsong;Fix the exception discarded issue in:
- master (1.15): 66e35c9bec1132d22b34f53adc43848d3fd4246f
- release-1.14: f849e7df44eac468ca6198b3efd79b6e7d2c7365
- release-1.13: c336698fa8275bc39f164bd8c9850fed989913b9
- release-1.12: 2dabdd95c15ccae2a97a0e898d1acfc958a0f7f3;;;","15/Oct/21 07:26;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25066&view=logs&j=955770d3-1fed-5a0a-3db6-0c7554c910cb&t=14447d61-56b4-5000-80c1-daa459247f6a&l=44454;;;","15/Oct/21 07:35;airblader;OK, so we seem to be running into an issue with Janino. Spark had the same issue before, too, and the discussion there is helpful: https://issues.apache.org/jira/browse/SPARK-25987. We could reduce the number of fields in the test case to workaround this, but ultimately it still means we're hitting a limitation in Janino when using many fields.;;;","15/Oct/21 12:29;airblader;I've opened a PR that works around this by reducing the number of fields in the test. I think that's the best course of action for the time being.;;;","18/Oct/21 06:12;xtsong;[~airblader],
Does this align with the observation that this is only reported for the 1.12 & 1.13 branches?;;;","18/Oct/21 06:38;airblader;Ah, I hadn't noticed that this only affects 1.12 & 1.13. Let me see if I can reproduce the issue there.;;;","18/Oct/21 08:30;airblader;I can reproduce this locally (even on master) by running the test with -Xss228k, which shows that it is indeed related to the stack size. I _think_ the reason it only fails on 1.12 / 1.13 might be due to the new code split infrastructure. Reducing the number of fields as I did in the PR does allow the test to pass again.;;;","18/Oct/21 08:56;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25115&view=logs&j=955770d3-1fed-5a0a-3db6-0c7554c910cb&t=14447d61-56b4-5000-80c1-daa459247f6a&l=44126;;;","18/Oct/21 09:10;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25122&view=logs&j=955770d3-1fed-5a0a-3db6-0c7554c910cb&t=14447d61-56b4-5000-80c1-daa459247f6a&l=44454;;;","18/Oct/21 09:12;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25129&view=logs&j=955770d3-1fed-5a0a-3db6-0c7554c910cb&t=14447d61-56b4-5000-80c1-daa459247f6a&l=44126;;;","18/Oct/21 09:15;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25130&view=logs&j=955770d3-1fed-5a0a-3db6-0c7554c910cb&t=14447d61-56b4-5000-80c1-daa459247f6a&l=44454;;;","18/Oct/21 15:57;twalthr;Fixed in master: 639824faccf8678a441d882e501e5044ff37c01b
Fixed in 1.14: 3f78a081aa563a8d935cc94a8d239b2208636271
Fixed in 1.13: a21ae65d2fb5eb648dc78716f88bd30fc54247ef
Fixed in 1.12: ef9f717b02d9b1abc4951248e4ef4d042a8ccb4b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove no longer used NestedMap* classes,FLINK-24475,13405408,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Zakelly,pnowojski,pnowojski,07/Oct/21 13:40,08/Nov/21 15:07,13/Jul/23 08:12,08/Nov/21 03:03,1.13.2,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,After FLINK-21935 all of the {{NestedMapsStateTable}} classes are no longer used in the production code. They are still however being used in benchmarks in some tests. Benchmarks/tests should be migrated to {{CopyOnWrite}} versions while the {{NestedMaps}} classes should be removed.,,liyu,pnowojski,roman,yunta,Zakelly,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24826,,,FLINK-21935,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 08 15:07:40 UTC 2021,,,,,,,,,,"0|z0vncg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Oct/21 10:28;liyu;Thanks for creating the ticket [~pnowojski], and yes this is some consensus decision as discussed in the FLINK-21935 PR.

[~Zakelly] Please take care of this task, thanks.;;;","08/Oct/21 10:30;Zakelly;Thanks [~liyu], I'll work on it.;;;","08/Nov/21 03:03;yunta;Merged in master: fc4f255644a64bb556b0dcefb165a9c772164c5b;;;","08/Nov/21 15:07;roman;It looks like this change caused performance regression. I've opened FLINK-24826 for investigation and probably fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DispatcherTest  is unstable,FLINK-24472,13405384,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,pnowojski,pnowojski,07/Oct/21 11:45,07/Oct/21 23:40,13/Jul/23 08:12,07/Oct/21 17:15,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/pnowojski/Flink/_build/results?buildId=534&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=511d2595-ec54-5ab7-86ce-92f328796f20

testCancellationOfNonCanceledTerminalJobFailsWithAppropriateException from DispatcherTest can fail with:

{noformat}
Oct 07 10:31:18 Expected: A CompletableFuture that failed with: org.apache.flink.runtime.messages.FlinkJobTerminatedWithoutCancellationException
Oct 07 10:31:18      but: Future is not completed.
Oct 07 10:31:18 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Oct 07 10:31:18 	at org.junit.Assert.assertThat(Assert.java:964)
Oct 07 10:31:18 	at org.junit.Assert.assertThat(Assert.java:930)
Oct 07 10:31:18 	at org.apache.flink.runtime.dispatcher.DispatcherTest.testCancellationOfNonCanceledTerminalJobFailsWithAppropriateException(DispatcherTest.java:442)
Oct 07 10:31:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Oct 07 10:31:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Oct 07 10:31:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Oct 07 10:31:18 	at java.lang.reflect.Method.invoke(Method.java:498)
(...)
{noformat}
",,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24275,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 07 17:15:53 UTC 2021,,,,,,,,,,"0|z0vn74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Oct/21 17:15;chesnay;master: e38cd1be718a207125e8116667d3e2675258d2b9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when notifyNewBufferSize,FLINK-24468,13405362,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,akalashnikov,akalashnikov,07/Oct/21 09:24,15/Dec/21 01:44,13/Jul/23 08:12,21/Oct/21 15:39,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"In my opinion, we have two problem there:
 # The exception itself(see below)
 # Ignoring the exception and stopping rescheduling of the calculation of the buffer size.

Of course, we need to fix this NPE and we need to think what we want to do if the buffer debloat fails with error.

{noformat}

java.lang.NullPointerException: null
 at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.notifyNewBufferSize(CreditBasedPartitionRequestClientHandler.java:135) ~[flink-dist_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
 at org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClient.notifyNewBufferSize(NettyPartitionRequestClient.java:203) ~[flink-dist_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
 at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.notifyNewBufferSize(RemoteInputChannel.java:330) ~[flink-dist_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
 at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.announceBufferSize(RemoteInputChannel.java:299) ~[flink-dist_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
 at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.announceBufferSize(SingleInputGate.java:389) ~[flink-dist_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
 at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.announceBufferSize(InputGateWithMetrics.java:102) ~[flink-dist_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
 at org.apache.flink.streaming.runtime.tasks.bufferdebloat.BufferDebloater.recalculateBufferSize(BufferDebloater.java:118) ~[flink-dist_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
 at org.apache.flink.streaming.runtime.tasks.StreamTask.debloat(StreamTask.java:795) ~[flink-dist_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
 at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$null$4(StreamTask.java:784) ~[flink-dist_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
 at org.apache.flink.util.function.FunctionUtils.lambda$asCallable$5(FunctionUtils.java:126) ~[flink-dist_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
 at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_282]
 at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-dist_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
 at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) ~[flink-dist_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
 at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:353) ~[flink-dist_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
 at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:317) ~[flink-dist_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
 at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201) ~[flink-dist_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
 at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:814) ~[flink-dist_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761) ~[flink-dist_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
 at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) [flink-dist_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
 at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) [flink-dist_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:754) [flink-dist_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) [flink-dist_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
 at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]

{noformat}",,akalashnikov,dwysakowicz,pnowojski,Thesharing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23974,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 21 15:39:39 UTC 2021,,,,,,,,,,"0|z0vn28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/21 15:39;dwysakowicz;Fixed in:
* master
** 4648e8af115410b4d1a5aefed44cdb6bd9d07ebb..60f39ed62b4d64f34c5d709253265add2831e662
* 1.14.1
** de8484691f1422dffd38853d13d2b79c0c4b2a6d..9ed6ad8c904d8bda6287ed4b8bac83323ce52f0e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set min and max buffer size even if the difference less than threshold,FLINK-24467,13405361,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,akalashnikov,akalashnikov,07/Oct/21 09:10,15/Dec/21 01:44,13/Jul/23 08:12,12/Oct/21 13:54,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"Right now, we apply a new buffer size only if it differs from the old buffer size more than the configured threshold but if the old buffer size is close to the max or min value less than this threshold we are always stuck on this value. For example, if we have the old buffer size 22k and our threshold is 50% then the value which we can apply should 33k but this is impossible because the max value is 32k so once we calculate the buffer size to 22k it is impossible to increase it.

The suggestion is to apply the changes every time when we calculate the new value to min or max size and the old value was different.",,akalashnikov,dwysakowicz,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23974,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 12 13:54:38 UTC 2021,,,,,,,,,,"0|z0vn20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/21 13:54;dwysakowicz;Implemented in:
* master
** 747895f1bde53f358df9517e0313c679cb4151bc..58197bebb554df7df1dd3dc27cc3e71cc3ad5b49
* 1.14
** 28690757444e63d67eb3cdcba0c4fb2f99d9d8e4..1cd2a749d6f7a48a2c0e901438c47d2510a628cf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong javadoc and documentation for buffer timeout,FLINK-24465,13405327,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,07/Oct/21 07:09,15/Dec/21 01:44,13/Jul/23 08:12,18/Oct/21 14:52,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Documentation,Runtime / Configuration,Runtime / Network,,,0,pull-request-available,,,,"The javadoc for {{setBufferTimeout}} and similarly the documentation for {{execution.buffer-timeout}} claims:

{code}
    /**
     * Sets the maximum time frequency (milliseconds) for the flushing of the output buffers. By
     * default the output buffers flush frequently to provide low latency and to aid smooth
     * developer experience. Setting the parameter can result in three logical modes:
     *
     * <ul>
     *   <li>A positive integer triggers flushing periodically by that integer
     *   <li>0 triggers flushing after every record thus minimizing latency
     *   <li>-1 triggers flushing only when the output buffer is full thus maximizing throughput
     * </ul>
     *
     * @param timeoutMillis The maximum time between two output flushes.
     */
{code}

which is not true.

The {{-1}} value translates to 100ms. See {{org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator#checkAndResetBufferTimeout}}",,dwysakowicz,fanrui,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 18 14:52:19 UTC 2021,,,,,,,,,,"0|z0vmug:",9223372036854775807,Configuring buffer timeout to be equal to `-1` to flush only when buffers were full was broken in FLINK-18832. In Stream execution mode it was overwritten with 100ms. Fortunately it should not have had a visible impact as `100ms` should still be enough time to fill in buffers before sending them downstream.,,,,,,,,,,,,,,,,,,,"12/Oct/21 08:45;pnowojski;As {{-1}} is not supported for already over a year and there were no complaints, I would be in favour of officially dropping this feature. Users can always use Integer.MAX_VALUE to achieve a very similar behaviour.

Let's drop runtime code for {{-1}} support and let's not mention it anywhere in the docs/java docs, but to keep compatibility with the previous versions, let's still translate {{-1}} to {{100ms}} on the config level.;;;","14/Oct/21 07:30;dwysakowicz;Unfortunately, we cannot drop the {{-1}} option. We use internally for blocking exchanges. We set that e.g. in the SQL planner and {{RuntimeExecutionMode.BATCH}}

I think the problem is that we started treating {{-1}} as an undefined value in the configuration layer and as {{DISABLED}} in the runtime layer. We should revert that change and let -1 pass from configuration to runtime in all cases (not just for blocking exchanges).;;;","18/Oct/21 14:52;dwysakowicz;Fixed in:
* master
** da9e1831e755a3b98588cf7954c2ca2901d2a2a6
* 1.14.1
** c337fff9d59142fd76fc67f75df7c4f50ea243bc
* 1.13.4
** 3209129b676605b573cc1fa67ef649cabb158807;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OperatorCoordinatorSchedulerTest#testGlobalFailoverDoesNotNotifyLocalRestore fails with IllegalState,FLINK-24444,13404735,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,dwysakowicz,dwysakowicz,04/Oct/21 07:43,16/Jan/22 11:31,13/Jul/23 08:12,16/Jan/22 11:31,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.4,1.15.0,,,,Runtime / Coordination,,,,,0,auto-deprioritized-major,pull-request-available,test-stability,,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24727&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8053,,dmvk,dwysakowicz,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 16 11:31:25 UTC 2022,,,,,,,,,,"0|z0vj7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/21 09:41;mapohl;The link to the build failure in the ticket description and the ticket title are a bit misleading. The actual failure is caused by {{testGlobalFailoverDoesNotNotifyLocalRestore}} (see [same build in line 7565|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24727&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=7565]):
{code}
Oct 04 02:09:28 java.lang.IllegalStateException: Valve closed for different checkpoint: closed for = -9223372036854775808, expected = 1
Oct 04 02:09:28 	at org.apache.flink.runtime.operators.coordination.OperatorEventValve.openValveAndUnmarkCheckpoint(OperatorEventValve.java:147)
Oct 04 02:09:28 	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.lambda$afterSourceBarrierInjection$5(OperatorCoordinatorHolder.java:358)
Oct 04 02:09:28 	at org.apache.flink.core.testutils.ManuallyTriggeredScheduledExecutorService.trigger(ManuallyTriggeredScheduledExecutorService.java:195)
Oct 04 02:09:28 	at org.apache.flink.core.testutils.ManuallyTriggeredScheduledExecutorService.triggerAll(ManuallyTriggeredScheduledExecutorService.java:180)
Oct 04 02:09:28 	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorSchedulerTest.closeScheduler(OperatorCoordinatorSchedulerTest.java:873)
Oct 04 02:09:28 	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorSchedulerTest.shutdownScheduler(OperatorCoordinatorSchedulerTest.java:122)
Oct 04 02:09:28 	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
Oct 04 02:09:28 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Oct 04 02:09:28 	at java.lang.reflect.Method.invoke(Method.java:498)
Oct 04 02:09:28 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Oct 04 02:09:28 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Oct 04 02:09:28 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Oct 04 02:09:28 	at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
Oct 04 02:09:28 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
Oct 04 02:09:28 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Oct 04 02:09:28 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Oct 04 02:09:28 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Oct 04 02:09:28 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Oct 04 02:09:28 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Oct 04 02:09:28 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Oct 04 02:09:28 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Oct 04 02:09:28 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Oct 04 02:09:28 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Oct 04 02:09:28 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Oct 04 02:09:28 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Oct 04 02:09:28 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Oct 04 02:09:28 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Oct 04 02:09:28 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Oct 04 02:09:28 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Oct 04 02:09:28 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Oct 04 02:09:28 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
Oct 04 02:09:28 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Oct 04 02:09:28 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Oct 04 02:09:28 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Oct 04 02:09:28 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
Oct 04 02:09:28 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Oct 04 02:09:28 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Oct 04 02:09:28 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Oct 04 02:09:28 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Oct 04 02:09:28 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Oct 04 02:09:28 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
Oct 04 02:09:28 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
{code}

I'm going to update the ticket title accordingly.;;;","11/Oct/21 09:43;mapohl;We observed the same issue in the VVP fork of Flink 1.14: https://dev.azure.com/ververica-dev/daplatform-flink/_build/results?buildId=1566&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=511d2595-ec54-5ab7-86ce-92f328796f20&l=8740;;;","11/Dec/21 10:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Dec/21 10:39;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","04/Jan/22 17:24;dmvk;This is caused by the fact, that the checkpoint can complete before opening the operator event valve (valve is opened by calling `afterSourceBarrierInjection` on `OperatorCoordinatorCheckpointContext`).;;;","16/Jan/22 11:31;chesnay;master: f5429e8b4ecd80043cabf0a1bf8141fa9e171e93
1.14: 97fe1723d7e9071a0a3c93612a50a2dc9652e1d6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IntervalJoinITCase.testRowTimeInnerJoinWithEquiTimeAttrs fail with output mismatch,FLINK-24443,13404732,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,slinkydeveloper,dwysakowicz,dwysakowicz,04/Oct/21 07:35,12/Oct/21 08:44,13/Jul/23 08:12,12/Oct/21 08:44,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Table SQL / Planner,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24716&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=9811

{code}
Oct 02 01:08:36 [ERROR] Tests run: 42, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 23.361 s <<< FAILURE! - in org.apache.flink.table.planner.runtime.stream.sql.IntervalJoinITCase
Oct 02 01:08:36 [ERROR] testRowTimeInnerJoinWithEquiTimeAttrs[StateBackend=ROCKSDB]  Time elapsed: 0.408 s  <<< FAILURE!
Oct 02 01:08:36 java.lang.AssertionError: expected:<List(K1,1000,L1,R1, K1,1000,L1,R2, K1,1000,L1,R3, K1,1000,L2,R1, K1,1000,L2,R2, K1,1000,L2,R3, K1,1000,L3,R1, K1,1000,L3,R2, K1,1000,L3,R3, K1,4000,L5,R5, K1,5001,L8,R7, K1,6000,L7,R6)> but was:<List(K1,1000,L1,R1, K1,1000,L1,R2, K1,1000,L1,R3, K1,1000,L2,R1, K1,1000,L2,R2, K1,1000,L2,R3, K1,1000,L3,R1, K1,1000,L3,R2, K1,1000,L3,R3, K1,1000,should-be-discarded,R1, K1,1000,should-be-discarded,R2, K1,1000,should-be-discarded,R3, K1,4000,L5,R5, K1,5001,L8,R7, K1,6000,L7,R6)>
Oct 02 01:08:36 	at org.junit.Assert.fail(Assert.java:89)
Oct 02 01:08:36 	at org.junit.Assert.failNotEquals(Assert.java:835)
{code}",,dwysakowicz,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 12 08:44:36 UTC 2021,,,,,,,,,,"0|z0vj6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Oct/21 07:37;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24720&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=9811;;;","04/Oct/21 07:39;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24724&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=9811;;;","04/Oct/21 14:59;twalthr;[~slinkydeveloper] it seems the test is unstable. Can you take a look?;;;","04/Oct/21 15:28;twalthr;Test disabled in 9ab85e1d10f0848ce43c01bfdb6788d7288dada7.;;;","12/Oct/21 08:44;twalthr;Fixed in master: 7596152c9396580dc00de85a387f910917039d81;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FsStateChangelogWriter#lastAppendedSequenceNumber return different seq number with no writes,FLINK-24436,13404425,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,ym,ym,01/Oct/21 07:13,07/Feb/22 12:14,13/Jul/23 08:12,07/Feb/22 12:14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,,,,,,0,,,,,"Test code: 

[https://github.com/apache/flink/pull/16606/commits/3d32e902cee493a984bc052b76dfec984743921f]

LOG:
{code:java}
2157 [main] INFO  org.apache.flink.changelog.fs.FsStateChangelogWriter [] - append to 00000000-0000-0000-0000-000000000000: keyGroup=-1 844 bytes
2163 [main] INFO  org.apache.flink.changelog.fs.FsStateChangelogWriter [] - append to 00000000-0000-0000-0000-000000000000: keyGroup=8 17 bytes
2163 [main] INFO  org.apache.flink.state.changelog.ChangelogStateBackendTestUtils [] - direct append, last appended to 0
2163 [main] INFO  org.apache.flink.changelog.fs.FsStateChangelogWriter [] - append to 00000000-0000-0000-0000-000000000000: keyGroup=3 17 bytes
2163 [main] INFO  org.apache.flink.state.changelog.ChangelogStateBackendTestUtils [] - direct append, last appended to 1
2163 [main] INFO  org.apache.flink.state.changelog.ChangelogStateBackendTestUtils [] - direct append, last appended to 2
2163 [main] INFO  org.apache.flink.state.changelog.ChangelogStateBackendTestUtils [] - direct append, last appended to 2
2163 [main] INFO  org.apache.flink.changelog.fs.FsStateChangelogWriter [] - append to 00000000-0000-0000-0000-000000000000: keyGroup=3 17 bytes
2163 [main] INFO  org.apache.flink.state.changelog.ChangelogStateBackendTestUtils [] - direct append, last appended to 2
2163 [main] INFO  org.apache.flink.changelog.fs.FsStateChangelogWriter [] - append to 00000000-0000-0000-0000-000000000000: keyGroup=5 17 bytes
2163 [main] INFO  org.apache.flink.state.changelog.ChangelogStateBackendTestUtils [] - direct append, last appended to 3
2163 [main] INFO  org.apache.flink.state.changelog.ChangelogStateBackendTestUtils [] - direct append, last appended to 4
2164 [main] INFO  org.apache.flink.state.changelog.ChangelogStateBackendTestUtils [] - direct append, last appended to 4
{code}

Problem:
1. getLastAppendedTo() does not return the same seq number if no writes.
2. Materialization depends on `if (upTo.compareTo(changelogSnapshotState.lastMaterializedTo()) > 0)` to decide whether really perform materialisation.  This will cause some undefined behavior if I call getLastAppendedTo() twice ",,roman,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21353,,,,FLINK-24435,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 07 12:14:59 UTC 2022,,,,,,,,,,"0|z0vhag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Oct/21 08:46;roman;To clarify, there is no assumption that getLastAppendedTo does NOT increment in that case, so correctness not affected. It could cause one unnecessary materialization when it's triggered without state changes.;;;","01/Oct/21 08:48;ym;This is because `rollover()` increases the `activeSequenceNumber`

{code:java}
private void rollover() {
    if (activeChangeSet.isEmpty()) {
        return;
    }
    notUploaded.put(
            activeSequenceNumber,
            new StateChangeSet(logId, activeSequenceNumber, activeChangeSet));
    activeSequenceNumber = activeSequenceNumber.next();
    LOG.debug(""bump active sqn to {}"", activeSequenceNumber);
    activeChangeSet = new ArrayList<>();
    activeChangeSetSize = 0;
}
{code}

while
{code}
public SequenceNumber lastAppendedSequenceNumber() {
    LOG.debug(""query {} sqn: {}"", logId, activeSequenceNumber);
    SequenceNumber tmp = activeSequenceNumber;
    // the returned current sequence number must be able to distinguish between the changes
    // appended before and after this call so we need to use the next sequence number
    // At the same time, we don't want to increment SQN on each append (to avoid too many
    // objects and segments in the resulting file).
    rollover();
    return tmp;
}
{code}

return the old tmp value first time `lastAppendedSequenceNumber()` is called.;;;","01/Oct/21 08:49;ym;Don't we agree `getLastAppendedTo` should not change without appends? 
If we do not agree with that, then code is too difficult to reason about.

It is not just unnecessary materialization, it is also the undefined `empty materialization` caused by this. In many cases, we can not guarantee in a multi-thread environment whether `getLastAppendedTo` is called the first time or the second time.
;;;","01/Oct/21 14:56;roman;I'm just saying that it doesn't affect correctness. I'm not saying it should not be fixed.

""Empty materialization"" is what we'd have without an optimization of SQN checking: uploading the same unchanged state twice. That's how non-incremental backends work now without the changelog.;;;","07/Feb/22 12:14;roman;Fixed as part of FLINK-25492.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FsStateChangelogWriter#lastAppendedSequenceNumber return different seq number with no writes,FLINK-24435,13404424,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,ym,ym,01/Oct/21 07:12,01/Oct/21 08:14,13/Jul/23 08:12,01/Oct/21 08:14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24436,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 01 08:13:39 UTC 2021,,,,,,,,,,"0|z0vha8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Oct/21 08:13;ym;duplicate ticket as https://issues.apache.org/jira/browse/FLINK-24436;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink YARN per-job on Docker test fails on Azure,FLINK-24434,13404411,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,dwysakowicz,dwysakowicz,01/Oct/21 06:28,26/Jan/23 07:51,13/Jul/23 08:12,27/Jul/22 04:37,1.14.4,1.15.0,1.16.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,API / Python,Deployment / YARN,,,,0,stale-major,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24669&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=23186
{code}
Sep 30 18:20:22 Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=mapred, access=WRITE, inode=""/"":hdfs:hadoop:drwxr-xr-x
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:318)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1663)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1647)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1606)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3039)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1079)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:652)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
Sep 30 18:20:22 	at java.security.AccessController.doPrivileged(Native Method)
Sep 30 18:20:22 	at javax.security.auth.Subject.doAs(Subject.java:422)
Sep 30 18:20:22 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)
Sep 30 18:20:22 
Sep 30 18:20:22 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
Sep 30 18:20:22 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
Sep 30 18:20:22 	at com.sun.proxy.$Proxy12.mkdirs(Unknown Source)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:583)
Sep 30 18:20:22 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Sep 30 18:20:22 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Sep 30 18:20:22 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Sep 30 18:20:22 	at java.lang.reflect.Method.invoke(Method.java:498)
Sep 30 18:20:22 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
Sep 30 18:20:22 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
Sep 30 18:20:22 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
Sep 30 18:20:22 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
Sep 30 18:20:22 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
Sep 30 18:20:22 	at com.sun.proxy.$Proxy13.mkdirs(Unknown Source)
Sep 30 18:20:22 	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2472)
Sep 30 18:20:22 	... 17 more

{code}",,dianfu,dwysakowicz,gaoyunhaii,hxbks2ks,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28680,,,,,,,,FLINK-30793,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 27 04:37:34 UTC 2022,,,,,,,,,,"0|z0vh7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/21 10:44;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","09/Dec/21 10:44;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","18/Apr/22 07:17;gaoyunhaii;1.14: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34744&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=75f4c82e-ad02-5844-81c9-d16399e3372d&l=25825;;;","22/Apr/22 05:59;gaoyunhaii;1.14: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34929&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=26142;;;","19/May/22 07:38;martijnvisser;1.14: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35816&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=75f4c82e-ad02-5844-81c9-d16399e3372d&l=21957;;;","18/Jul/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Jul/22 02:05;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38467&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;","21/Jul/22 02:06;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38471&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;","21/Jul/22 02:49;hxbks2ks;
{code:java}
2022-07-20T13:36:55.8006262Z Caused by: java.io.IOException: No space left on device
2022-07-20T13:36:55.8035687Z 	at java.io.FileOutputStream.writeBytes(Native Method)
2022-07-20T13:36:55.8041296Z 	at java.io.FileOutputStream.write(FileOutputStream.java:326)
2022-07-20T13:36:55.8041843Z 	at org.apache.flink.util.IOUtils.copyBytes(IOUtils.java:63)
2022-07-20T13:36:55.8042288Z 	at org.apache.flink.util.IOUtils.copyBytes(IOUtils.java:86)
2022-07-20T13:36:55.8045626Z 	at org.apache.flink.python.util.CompressionUtils.extractZipFileWithPermissions(CompressionUtils.java:223)
2022-07-20T13:36:55.8046269Z 	at org.apache.flink.python.util.CompressionUtils.extractFile(CompressionUtils.java:61)
2022-07-20T13:36:55.8063141Z 	at org.apache.flink.client.python.PythonEnvUtils.lambda$preparePythonEnvironment$1(PythonEnvUtils.java:199)
2022-07-20T13:36:55.8066510Z 	... 53 more
{code}
The root cause is still no space left

;;;","21/Jul/22 11:40;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38507&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;","27/Jul/22 04:37;hxbks2ks;Since https://issues.apache.org/jira/browse/FLINK-28680 has been resolved, I think we can close the JIRA.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksIteratorWrapper.seekToLast() calls the wrong RocksIterator method,FLINK-24432,13404380,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,victorunique,victorunique,victorunique,30/Sep/21 21:09,15/Dec/21 01:44,13/Jul/23 08:12,15/Oct/21 02:19,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"The RocksIteratorWrapper is a wrapper of RocksIterator to do additional status check for all the methods. However, there's a typo that RocksIteratorWrapper.*seekToLast*() method calls RocksIterator's *seekToFirst*(), which is obviously wrong. I guess this issue wasn't found before as it was only referenced in the RocksTransformingIteratorWrapper.seekToLast() method and nowhere else.
{code:java}
@Override
public void seekToFirst() {
 iterator.seekToFirst();
 status();
}

@Override
public void seekToLast() {
 iterator.seekToFirst();
 status();
}{code}",,nkruber,victorunique,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24479,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 15 02:19:37 UTC 2021,,,,,,,,,,"0|z0vh0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Oct/21 02:19;yunta;merged in master: ae531b5888667abd0b194e0372bdde03581de97c

merged in release-1.14: ba6a8cd72c0abe12e50d09a5f42d74d2bea27e42;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Kinesis][EFO] EAGER registration strategy does not work when job fails over,FLINK-24431,13404333,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rudi.kershaw,dannycranmer,dannycranmer,30/Sep/21 15:55,15/Dec/21 01:44,13/Jul/23 08:12,11/Oct/21 14:26,1.12.5,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.3,1.15.0,,Connectors / Kinesis,,,,,0,pull-request-available,,,,"*Background*
The EFO Kinesis connector will register and de-register stream consumers based on the configured [registration strategy|https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/connectors/datastream/kinesis/#efo-stream-consumer-registrationderegistration]. When {{EAGER}} is used, the client (usually job manager) will register the consumer and then the task managers will de-register the consumer when job stops/fails. If the job is configured to restart on fail, then the consumer will not exist and the job will continuously fail over.

*Solution*
The proposal is to [not deregister the stream consumer|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/util/StreamConsumerRegistrarUtil.java#L88] when {{EAGER}} is used. The documentation should be updated to reflect this.",,dannycranmer,rudi.kershaw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 07 14:00:15 UTC 2021,,,,,,,,,,"0|z0vgq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Oct/21 08:14;rudi.kershaw;Pull request for 1.15.0 has been raised against {{master}}. Once the change has been approved I will back-port it to hotfixes for previous versions. 

Can I confirm which branches pull requests should be raised against for {{1.12.6}}, {{1.13.3}}, {{1.14.1}} respectively?;;;","07/Oct/21 14:00;dannycranmer;Thanks [~rudi.kershaw], {{master}} is merged. Other versions should target the {{release-x}} branch, for example for 1.14 it would be {{release-1.14}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
retract  stream  join  on topN   error,FLINK-24412,13404220,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,sandyfog,sandyfog,30/Sep/21 08:02,18/Sep/22 13:12,13/Jul/23 08:12,18/Sep/22 13:12,1.12.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.16.0,,,,,Table SQL / Runtime,,,,,0,,,,,"I can  reappear this error in follow sql：
 create table user_info(
 name string,
 age int,
 primary key(name) not enforced
 ) whith(
 'connector'='jdbc',
 'url'='jdbc:mysql...',
 ...
 'lookup.cache.max-rows'='0',
 'lookup.cache.ttl'='1 s'
 );

create table user_action(
 name string,
 app string,
 dt string,
 proctime as proctime()
 )whith(
 'connector'='kafka',
 ...
 );

create view v_user_action as select * from(
 select name,app,proctime,row_number() over(partition by name,app order by dt desc) as rn from user_action
 )t where rn=1;

create view user_out as select a.name,a.app,b.age from v_user_action a left join user_info
 for system_time as of a.proctime as b on a.name=b.name;

select * from (
 select name,app,age ,row_number() over(partition by name,app order by age desc) as rn from user_out
 ) t where rn=1;
  
 *first :*
 {color:#de350b} user_action  got data  \{""name"":""11"",""app"":""app"",""dt"":""2021-09-10""}{color}

{color:#de350b}user_info   got data  \{""name"":""11"",""age"":11}{color}

at the moment  sql can  successful run.

*{color:#de350b}then :{color}*

{color:#de350b}user_action  got data  \{""name"":""11"",""app"":""app"",""dt"":""2021-09-20""}{color}

{color:#de350b}user_info   got data  \{""name"":""11"",""age"":11}  \{""name"":""11"",""age"":22} {color}

now, TopN query on last sql, the TopN operator will thrown exception: {{Caused by: java.lang.RuntimeException: Can not retract a non-existent record. This should never happen.}}",,jark,libenchao,sandyfog,tartarus,wenlong.lwl,zhouqi,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28568,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 30 08:13:32 UTC 2021,,,,,,,,,,"0|z0vg0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/21 08:06;sandyfog;[~jark]  Would you help me look this issue?;;;","30/Sep/21 08:13;wenlong.lwl;hi, [~sandyfog], Lookup join can not work well on retract/upsert stream if the source table could change. you can try mysql cdc source(https://github.com/ververica/flink-cdc-connectors/issues) and regular table join instead.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka topics with periods in their names generate a constant stream of errors,FLINK-24409,13404207,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fpaul,jherico,jherico,30/Sep/21 07:41,15/Dec/21 01:44,13/Jul/23 08:12,17/Nov/21 16:57,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"Attempting to port a job that worked on 1.13.2 over to 1.14.0 I encountered [this|https://gist.github.com/jherico/1f2d1cf441a4a9724ae370c375c90fde] error.  Going into the code I discovered that the reason seems to be [this logic|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/metrics/KafkaSourceReaderMetrics.java#L297].  But the tags being examined here are generated at least partly by [this code|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L1937].  The tags always translate periods in the topic name to underscores, but the code in KafkaSourceReaderMetrics uses the untransformed topic name in the filter.  Therefore if the topic name has a period in it the filtering logic will _always_ fail.

 The code that's generating the error is apparently very recent, being merged in this PR [https://github.com/apache/flink/pull/16838]

 

 ",,fpaul,jherico,mason6345,maver1ck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24497,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 17 16:56:53 UTC 2021,,,,,,,,,,"0|z0vfy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/21 16:56;fpaul;Merged
- master: da8ef265e3f78ec71aad43b4f8209884eaf3a860
- release-1.14 e68e30dc8c355f88169f6a9ac1c7044c895aff30;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"org.codehaus.janino.InternalCompilerException: Compiling ""StreamExecValues$200"": Code of method ""nextRecord(Ljava/lang/Object;)Ljava/lang/Object;"" of class ""StreamExecValues$200"" grows beyond 64 KB",FLINK-24408,13404204,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,shengkui,shengkui,30/Sep/21 07:23,15/Dec/21 01:44,13/Jul/23 08:12,31/Oct/21 07:45,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"I build a large SQL in application, and meet the issue ""Code of method method  grows beyond 64 KB"". This bug should be fixed refer to #FLINK-22903.

 
{quote}{{ java.lang.RuntimeException: Could not instantiate generated class 'StreamExecValues$200'    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:75) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.operators.values.ValuesInputFormat.open(ValuesInputFormat.java:60) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.operators.values.ValuesInputFormat.open(ValuesInputFormat.java:35) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:84) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:116) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:73) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:323) [flink-dist_2.11-1.14.0.jar:1.14.0]Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:76) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69) [flink-table_2.11-1.14.0.jar:1.14.0]    ... 6 moreCaused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69) [flink-table_2.11-1.14.0.jar:1.14.0]    ... 6 moreCaused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:89) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69) [flink-table_2.11-1.14.0.jar:1.14.0]    ... 6 moreCaused by: org.codehaus.janino.InternalCompilerException: Compiling ""StreamExecValues$200"": Code of method ""nextRecord(Ljava/lang/Object;)Ljava/lang/Object;"" of class ""StreamExecValues$200"" grows beyond 64 KB    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69) [flink-table_2.11-1.14.0.jar:1.14.0]    ... 6 moreCaused by: org.codehaus.janino.InternalCompilerException: Code of method ""nextRecord(Ljava/lang/Object;)Ljava/lang/Object;"" of class ""StreamExecValues$200"" grows beyond 64 KB    at org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:1048) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.CodeContext.write(CodeContext.java:925) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.writeOpcode(UnitCompiler.java:12291) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.referenceThis(UnitCompiler.java:10103) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4488) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.access$10000(UnitCompiler.java:215) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$16.visitQualifiedThisReference(UnitCompiler.java:4437) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$16.visitQualifiedThisReference(UnitCompiler.java:4396) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.Java$QualifiedThisReference.accept(Java.java:4407) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileContext2(UnitCompiler.java:4336) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:215) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$15$1.visitFieldAccess(UnitCompiler.java:4273) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$15$1.visitFieldAccess(UnitCompiler.java:4268) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.Java$FieldAccess.accept(Java.java:4310) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$15.visitLvalue(UnitCompiler.java:4268) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$15.visitLvalue(UnitCompiler.java:4264) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileContext(UnitCompiler.java:4264) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5661) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5145) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3783) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.access$5900(UnitCompiler.java:215) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3762) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3734) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.Java$Block.accept(Java.java:2779) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.fakeCompile(UnitCompiler.java:1529) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2434) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2181) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.access$2400(UnitCompiler.java:215) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$6.visitSwitchStatement(UnitCompiler.java:1500) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$6.visitSwitchStatement(UnitCompiler.java:1487) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.Java$SwitchStatement.accept(Java.java:3391) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) [flink-dist_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102) [flink-table_2.11-1.14.0.jar:1.14.0]    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69) ~[flink-table_2.11-1.14.0.jar:1.14.0]    ... 6 more}}
{quote}
 ","* Flink v1.14.0
 * Hudi trunk(v0.10.0 snapshot)
 * Hadoop v2.9.2",airblader,jark,lzljs3620320,shengkui,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/21 01:05;shengkui;HudiBenchS3.java;https://issues.apache.org/jira/secure/attachment/13034673/HudiBenchS3.java","08/Oct/21 01:02;shengkui;sql.txt;https://issues.apache.org/jira/secure/attachment/13034672/sql.txt",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Oct 31 07:45:46 UTC 2021,,,,,,,,,,"0|z0vfxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Oct/21 13:49;jark;Thanks for reporting this [~shengkui], could you also share your SQL if possible, so that we can reproduce this problem. ;;;","08/Oct/21 01:06;shengkui;[~jark] , the SQL can be fount in the attachment: [^sql.txt]

 

I built it with this code: [^HudiBenchS3.java]

 ;;;","28/Oct/21 04:04;TsReaper;Hi!

Thanks for raising this issue. This is because Java code splitter currently does not deal with {{switch}} statements which is used by {{VALUES}}. I'll fix this in a couple of days.;;;","31/Oct/21 07:45;lzljs3620320;master: 39ec06aa304504777a8769a10350360fcb439d56
release-1.14: 992a1437ef79fb032cb0dccd3c7b362a368a6ab6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar connector chinese document link to Pulsar document location incorrectly.,FLINK-24407,13404185,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Aiden Gong,Aiden Gong,Aiden Gong,30/Sep/21 03:41,23/Feb/22 13:43,13/Jul/23 08:12,23/Feb/22 13:42,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.4,1.15.0,,,,Documentation,,,,,0,pull-request-available,stale-assigned,,,"Pulsar connector chinese document link to Pulsar document location incorrectly.

!企业微信截图_16329715717678.png!",,Aiden Gong,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/21 03:41;Aiden Gong;企业微信截图_16329715717678.png;https://issues.apache.org/jira/secure/attachment/13034394/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_16329715717678.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,ch,Fri Feb 18 13:44:32 UTC 2022,,,,,,,,,,"0|z0vft4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/21 03:43;Aiden Gong;Hi,[~jark]

I will submit a pr to fix this issue.Please assign to me.;;;","03/Dec/21 10:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","14/Dec/21 14:42;Aiden Gong;Hi, [~jark] 

 I submit a pr,please cc.Thank you~;;;","18/Feb/22 13:44;leonard;master(1.15): 9f60c506366ae14405f0da487b3267aa336ec284

release-1.14:fdc6d6c083eb349998818b06e41ffa07bcdf6209;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaWriterITCase.testLingeringTransaction fails on azure,FLINK-24405,13404173,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fpaul,xtsong,xtsong,30/Sep/21 01:53,15/Dec/21 01:44,13/Jul/23 08:12,07/Oct/21 07:37,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Connectors / Kafka,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24595&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=e424005a-b16e-540f-196d-da062cc19bdf&l=7344

{code}
Sep 28 22:36:05 [ERROR] Tests run: 13, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 35.773 s <<< FAILURE! - in org.apache.flink.connector.kafka.sink.KafkaWriterITCase
Sep 28 22:36:05 [ERROR] testLingeringTransaction  Time elapsed: 3.07 s  <<< FAILURE!
Sep 28 22:36:05 java.lang.AssertionError: 
Sep 28 22:36:05 
Sep 28 22:36:05 Expected: a collection with size <1>
Sep 28 22:36:05      but: collection size was <0>
Sep 28 22:36:05 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Sep 28 22:36:05 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
Sep 28 22:36:05 	at org.apache.flink.connector.kafka.sink.KafkaWriterITCase.testLingeringTransaction(KafkaWriterITCase.java:213)
{code}",,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 07 07:37:18 UTC 2021,,,,,,,,,,"0|z0vfqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Oct/21 08:53;arvid;Merged into master as ade011ec139f8258db356abd65e6e83601cd22e1..0a2325c322f49adc989450db329f5db8d81791f8.;;;","07/Oct/21 07:37;arvid;Merged into 1.14 as 8eaee0ffe47b21d01f5c013d0bf9beec98a33821..e8638b614c438e879435819d1a993ec65e39fd44.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TM cannot exit after Metaspace OOM,FLINK-24401,13403992,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,29/Sep/21 03:54,15/Dec/21 01:44,13/Jul/23 08:12,08/Nov/21 07:51,1.12.0,1.13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Runtime / Coordination,Runtime / Task,,,,0,pull-request-available,,,,"Hi masters, from the code and log, we can see that OOM will terminateJVM directly, but Metaspace OutOfMemoryError will graceful shutdown. The code comment mentions: {{_it does not usually require more class loading to fail again with the Metaspace OutOfMemoryError_.}}.

But we encountered: after Metaspace OutOfMemoryError, {{_java.lang.NoClassDefFoundError: Could not initialize class org.apache.flink.runtime.taskexecutor.TaskManagerRunner$Result_.}}, makes Tm unable to exit, keeps trying again, keeps NoClassDefFoundError, keeps class loading failure, until kill tm by manually.

I want to add a catch Throwable in the onFatalError method, and directly terminateJVM() in the catch. Is there any problem with this strategy? 

 

[code link |https://github.com/apache/flink/blob/4fe9f525a92319acc1e3434bebed601306f7a16f/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskManagerRunner.java#L312]

picture:

 

!image-2021-09-29-12-00-44-812.png|width=1337,height=692!

  !image-2021-09-29-12-00-28-510.png!

 

 ",,fanrui,jackwangcs,mason6345,pnowojski,trohrmann,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/21 04:00;fanrui;image-2021-09-29-12-00-28-510.png;https://issues.apache.org/jira/secure/attachment/13034350/image-2021-09-29-12-00-28-510.png","29/Sep/21 04:00;fanrui;image-2021-09-29-12-00-44-812.png;https://issues.apache.org/jira/secure/attachment/13034351/image-2021-09-29-12-00-44-812.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 08 07:51:46 UTC 2021,,,,,,,,,,"0|z0vem8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/21 13:23;pnowojski;[~fanrui], can you post the actual second's OOM stack trace?

As the error was caused by {{org.apache.flink.runtime.taskexecutor.TaskManagerRunner.Result}}, I wonder if we should actually just treat all OOMs the sam way, regardless if it's meta space or not. [~trohrmann], what do you think? I'm asking as you were doing the review of that change, and I don't know the motivation behind it. Was it just best effort thing that we added, just because we thought we could? Or was it addressing some problem that users were complaining about?
{quote}
In case of Metaspace OOM error, we try a graceful TM shutdown to notify JM because it is not expected to require class loading for that and cause further failures.
{quote}
After all this assumption is clearly in the wrong.;;;","19/Oct/21 02:41;fanrui;Hi [~pnowojski] , thanks for you reply. I'm sorry, I didn't save the stack trace of TM, I just save the log. ;;;","02/Nov/21 14:34;trohrmann;I think the assumption was that a meta space OOM mainly occurs when loading user code (additional classes). That's why we thought that most of Flink related things can still work because they were loaded before. Clearly, this does not seem to hold true. I'd be fine with failing hard in case of a meta space OOM. If we want to still provide the old behaviour, then we could make the exit behaviour configurable with default to fail hard.;;;","03/Nov/21 08:02;pnowojski;Thanks for the explanation. At this moment I do not see a good justification for keeping ""failing soft"" code path, so I would vote for the sake of simplicity to just remove it.;;;","03/Nov/21 08:03;pnowojski;[~fanrui], is this something that you would like to work on and contribute?;;;","03/Nov/21 12:40;fanrui;Of course. I'd like to contribute. Please assign this issue to me. Thanks a lot.;;;","08/Nov/21 07:51;pnowojski;Merged to master/release-1.14/release-1.13 as 365d12d499a/8f73e6c38cc/3b5b5428271 respectively

Thanks again [~fanrui] for reporting and fixing the issue!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python 'build_wheels mac' fails on azure,FLINK-24390,13403691,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dianfu,xtsong,xtsong,28/Sep/21 02:50,15/Dec/21 01:40,13/Jul/23 08:12,11/Oct/21 09:47,1.12.5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,,,,,API / Python,Build System,,,,0,pull-request-available,,,,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24547&view=logs&j=33dd8067-7758-552f-a1cf-a8b8ff0e44cd&t=789348ee-cf3e-5c4b-7c78-355970e5f360&l=17982,,dianfu,dwysakowicz,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24336,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 11 09:46:58 UTC 2021,,,,,,,,,,"0|z0vcrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/21 02:50;xtsong;cc [~dianfu]
Might relate to FLINK-24336.;;;","30/Sep/21 01:50;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24596&view=logs&j=33dd8067-7758-552f-a1cf-a8b8ff0e44cd&t=789348ee-cf3e-5c4b-7c78-355970e5f360&l=17992;;;","30/Sep/21 01:57;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24637&view=logs&j=33dd8067-7758-552f-a1cf-a8b8ff0e44cd&t=789348ee-cf3e-5c4b-7c78-355970e5f360&l=27866;;;","01/Oct/21 06:29;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24683&view=logs&j=33dd8067-7758-552f-a1cf-a8b8ff0e44cd&t=789348ee-cf3e-5c4b-7c78-355970e5f360&l=27875;;;","04/Oct/21 07:32;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24717&view=logs&j=33dd8067-7758-552f-a1cf-a8b8ff0e44cd&t=789348ee-cf3e-5c4b-7c78-355970e5f360;;;","04/Oct/21 07:38;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24721&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=33dd8067-7758-552f-a1cf-a8b8ff0e44cd;;;","04/Oct/21 07:38;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24725&view=results;;;","05/Oct/21 07:06;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24761&view=results;;;","06/Oct/21 06:10;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24782&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=33dd8067-7758-552f-a1cf-a8b8ff0e44cd;;;","08/Oct/21 02:36;dianfu;It should be caused that the latest grpcio (1.41.0 which was released in 9.28) has dropped the support for Python 3.5. We could simply limit the grpcio version.;;;","08/Oct/21 06:04;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24808&view=logs&j=33dd8067-7758-552f-a1cf-a8b8ff0e44cd&t=789348ee-cf3e-5c4b-7c78-355970e5f360&l=27884;;;","08/Oct/21 06:25;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24830&view=logs&j=33dd8067-7758-552f-a1cf-a8b8ff0e44cd&t=789348ee-cf3e-5c4b-7c78-355970e5f360&l=17996;;;","09/Oct/21 02:57;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24879&view=logs&j=33dd8067-7758-552f-a1cf-a8b8ff0e44cd&t=789348ee-cf3e-5c4b-7c78-355970e5f360&l=27869;;;","09/Oct/21 06:50;dianfu;Fixed in release-1.12 via 5dd50ef0e69eb104ad99a1a09f4e0e15b16cf647;;;","11/Oct/21 02:30;xtsong;[~dianfu]
New instance, on the exactly same commit of the fix.
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24923&view=logs&j=33dd8067-7758-552f-a1cf-a8b8ff0e44cd&t=789348ee-cf3e-5c4b-7c78-355970e5f360&l=27875;;;","11/Oct/21 03:27;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24931&view=logs&j=33dd8067-7758-552f-a1cf-a8b8ff0e44cd&t=789348ee-cf3e-5c4b-7c78-355970e5f360&l=27882;;;","11/Oct/21 09:46;dianfu;Merged the following-up fix to release-1.12 via d9d4d3c34bfd70902fe37fec7eb814e40634a2b8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getDescription() in `CatalogTableImpl` should check null,FLINK-24389,13403685,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nizhikov,nlu90,nlu90,28/Sep/21 01:39,09/Oct/21 14:03,13/Jul/23 08:12,09/Oct/21 14:03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Table SQL / API,,,,,0,pull-request-available,starter,,,"```
@Override
public Optional<String> getDescription() {
    return Optional.of(getComment());
}
```

If the table comment is not set, then `getDescription` will throw NullPointerException 

[https://github.com/apache/flink/blame/5b9e7882207357120717966d8bf7efd53c53ede5/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/catalog/CatalogTableImpl.java#L69]

 ",,airblader,jark,nizhikov,nlu90,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 09 14:03:46 UTC 2021,,,,,,,,,,"0|z0vcq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/21 07:18;airblader;+1. Would you like to send a PR for that?;;;","08/Oct/21 13:09;nizhikov;Patch [1] for the issue is ready.
Please, review.

[1] https://github.com/apache/flink/pull/17438;;;","09/Oct/21 14:03;jark;Fixed in master: 2d38d8bd72d9b80227071dde77dcad688d78a47f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table API exceptions may leak sensitive configuration values,FLINK-24381,13403431,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,ana4,ana4,ana4,27/Sep/21 06:56,15/Dec/21 01:44,13/Jul/23 08:12,02/Nov/21 11:56,1.13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"This following is error message. Password is 'bar' and is displayed.

Could we hidden it to password='******' or password='<hidden>' inspired by Apache Kafka source code.
{code:java}
Missing required options are:

hosts 

Unable to create a sink for writing table 'default_catalog.default_database.dws'.

Table options are:

'connector'='elasticsearch7-x'
'index'='foo'
'password'='bar'
	at org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:208)
	at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:369)
	at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:221)
	at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:159
{code}",,airblader,ana4,jark,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24734,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 02 11:56:58 UTC 2021,,,,,,,,,,"0|z0vb5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/21 09:15;twalthr;+1, how about we make this functionality available to all Flink users by adding a declaration flag like {{isSecret}} to {{ConfigOption}}? This was {{FactoryUtil}} can be one consumer of this flag among others.;;;","27/Sep/21 09:40;ana4;I agree with [~twalthr]. I want to fix this.;;;","27/Sep/21 09:54;chesnay;I think adding ConfigOption#isSecret is an orthogonal issues.
We need to check {{GlobalConfiguration#isSensitive}} in any case whenever a option value is printed.;;;","25/Oct/21 11:24;ana4;As [~chesnay] comment in PR, could I directly use GlobalConfiguration#isSensitive to judge sensitive options in FactoryUtil? I don't have to add a isSecret flag.;;;","25/Oct/21 11:51;jark;I think {{isSecret}} is useful for extension than hardcode way. Because many connector may use some company-wise key names which are very different. ;;;","25/Oct/21 18:38;chesnay;At the same time we shouldn't rely exclusively on the {{isSecret}} flag from the options because it should also work with existing connectors.

So let's split this ticket.

a) Adjust the Table API to use GlobalConfiguration#isSensitive where appropriate.
b) Add ConfigOption#isSecret and adjust _all_ existing usages to check for _both_ (e.g., by having ConfigOptions#isSecret call GlobalConfiguration#isSensitive by default).;;;","26/Oct/21 02:26;ana4;So I create a new ticket and PR to add support for using GlobalConfiguration#isSensitive, or in this PR add this.;;;","26/Oct/21 12:53;chesnay;hmm I would use this ticket to have the Table API check isSensitive, and then a new ticket for adding ConfigOption#isSecret.;;;","28/Oct/21 11:58;ana4;Could I take 2 PR to 1.12-release and 1.13-release? [~chesnay];;;","28/Oct/21 12:10;chesnay;master: 937d6b72a0dc62772d0b3a014d763f20aa7691ff
1.14: d9bae5672fd126c4d00457d472ff56bb5876e743 
1.13: 50121c301ac24a04a470c3653f2877c22e5862e7 ;;;","28/Oct/21 16:51;chesnay;[~ana4] I have already backported the PR to 1.13. 1.12 is no longer being supported.;;;","02/Nov/21 11:44;jark;The release-1.13 branch keeps failing on the new introduced test:

https://dev.azure.com/apache-flink/apache-flink/_build?definitionId=1&_a=summary&repositoryFilter=1&branchFilter=11516%2C11516%2C11516%2C11516

{code}
2021-11-01T10:44:56.1806689Z Nov 01 10:44:56 [ERROR] Failures: 
2021-11-01T10:44:56.1807226Z Nov 01 10:44:56 [ERROR]   FactoryUtilTest.testSecretOption 
2021-11-01T10:44:56.1808215Z Nov 01 10:44:56 Expected: (an instance of org.apache.flink.table.api.ValidationException and Expected failure cause is <org.apache.flink.table.api.ValidationException: Table options are:
2021-11-01T10:44:56.1809278Z Nov 01 10:44:56 
2021-11-01T10:44:56.1810394Z Nov 01 10:44:56 'buffer-size'='1000'
2021-11-01T10:44:56.1810967Z Nov 01 10:44:56 'connector'='test-connector'
2021-11-01T10:44:56.1811459Z Nov 01 10:44:56 'key.format'='test-format'
2021-11-01T10:44:56.1811990Z Nov 01 10:44:56 'key.test-format.delimiter'=','
2021-11-01T10:44:56.1812682Z Nov 01 10:44:56 'password'='******'
2021-11-01T10:44:56.1813467Z Nov 01 10:44:56 'property-version'='1'
2021-11-01T10:44:56.1813906Z Nov 01 10:44:56 'value.format'='test-format'
2021-11-01T10:44:56.1814681Z Nov 01 10:44:56 'value.test-format.delimiter'='|'
2021-11-01T10:44:56.1815202Z Nov 01 10:44:56 'value.test-format.fail-on-missing'='true'>)
2021-11-01T10:44:56.1815728Z Nov 01 10:44:56      but: Expected failure cause is <org.apache.flink.table.api.ValidationException: Table options are:
2021-11-01T10:44:56.1816352Z Nov 01 10:44:56 
2021-11-01T10:44:56.1816764Z Nov 01 10:44:56 'buffer-size'='1000'
2021-11-01T10:44:56.1817319Z Nov 01 10:44:56 'connector'='test-connector'
2021-11-01T10:44:56.1817772Z Nov 01 10:44:56 'key.format'='test-format'
2021-11-01T10:44:56.1818274Z Nov 01 10:44:56 'key.test-format.delimiter'=','
2021-11-01T10:44:56.1818869Z Nov 01 10:44:56 'password'='******'
2021-11-01T10:44:56.1819309Z Nov 01 10:44:56 'property-version'='1'
2021-11-01T10:44:56.1819886Z Nov 01 10:44:56 'value.format'='test-format'
2021-11-01T10:44:56.1820357Z Nov 01 10:44:56 'value.test-format.delimiter'='|'
2021-11-01T10:44:56.1821233Z Nov 01 10:44:56 'value.test-format.fail-on-missing'='true'> The throwable <org.apache.flink.table.api.ValidationException: Unable to create a source for reading table 'default.default.t1'.
2021-11-01T10:44:56.1821796Z Nov 01 10:44:56 
2021-11-01T10:44:56.1822044Z Nov 01 10:44:56 Table options are:
2021-11-01T10:44:56.1822416Z Nov 01 10:44:56 
2021-11-01T10:44:56.1822809Z Nov 01 10:44:56 'buffer-size'='1000'
2021-11-01T10:44:56.1823327Z Nov 01 10:44:56 'connector'='test-connector'
2021-11-01T10:44:56.1823794Z Nov 01 10:44:56 'key.format'='test-format'
2021-11-01T10:44:56.1824244Z Nov 01 10:44:56 'key.test-format.delimiter'=','
2021-11-01T10:44:56.1824689Z Nov 01 10:44:56 'password'='******'
2021-11-01T10:44:56.1825124Z Nov 01 10:44:56 'property-version'='1'
2021-11-01T10:44:56.1825558Z Nov 01 10:44:56 'value.format'='test-format'
2021-11-01T10:44:56.1826034Z Nov 01 10:44:56 'value.test-format.delimiter'='|'
2021-11-01T10:44:56.1826869Z Nov 01 10:44:56 'value.test-format.fail-on-missing'='true'> does not contain the expected failure cause <org.apache.flink.table.api.ValidationException: Table options are:
2021-11-01T10:44:56.1827422Z Nov 01 10:44:56 
2021-11-01T10:44:56.1827827Z Nov 01 10:44:56 'buffer-size'='1000'
2021-11-01T10:44:56.1828263Z Nov 01 10:44:56 'connector'='test-connector'
2021-11-01T10:44:56.1828789Z Nov 01 10:44:56 'key.format'='test-format'
2021-11-01T10:44:56.1829244Z Nov 01 10:44:56 'key.test-format.delimiter'=','
2021-11-01T10:44:56.1829690Z Nov 01 10:44:56 'password'='******'
2021-11-01T10:44:56.1830104Z Nov 01 10:44:56 'property-version'='1'
2021-11-01T10:44:56.1830551Z Nov 01 10:44:56 'value.format'='test-format'
2021-11-01T10:44:56.1831018Z Nov 01 10:44:56 'value.test-format.delimiter'='|'
2021-11-01T10:44:56.1831504Z Nov 01 10:44:56 'value.test-format.fail-on-missing'='true'>
2021-11-01T10:44:56.1832345Z Nov 01 10:44:56 Stacktrace was: org.apache.flink.table.api.ValidationException: Unable to create a source for reading table 'default.default.t1'.
2021-11-01T10:44:56.1832822Z Nov 01 10:44:56 
2021-11-01T10:44:56.1833070Z Nov 01 10:44:56 Table options are:
2021-11-01T10:44:56.1833332Z Nov 01 10:44:56 
2021-11-01T10:44:56.1833726Z Nov 01 10:44:56 'buffer-size'='1000'
2021-11-01T10:44:56.1834177Z Nov 01 10:44:56 'connector'='test-connector'
2021-11-01T10:44:56.1834615Z Nov 01 10:44:56 'key.format'='test-format'
2021-11-01T10:44:56.1835077Z Nov 01 10:44:56 'key.test-format.delimiter'=','
2021-11-01T10:44:56.1835521Z Nov 01 10:44:56 'password'='******'
2021-11-01T10:44:56.1835935Z Nov 01 10:44:56 'property-version'='1'
2021-11-01T10:44:56.1836387Z Nov 01 10:44:56 'value.format'='test-format'
2021-11-01T10:44:56.1836841Z Nov 01 10:44:56 'value.test-format.delimiter'='|'
2021-11-01T10:44:56.1837349Z Nov 01 10:44:56 'value.test-format.fail-on-missing'='true'
2021-11-01T10:44:56.1837871Z Nov 01 10:44:56 	at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:140)
2021-11-01T10:44:56.1838644Z Nov 01 10:44:56 	at org.apache.flink.table.factories.utils.FactoryMocks.createTableSource(FactoryMocks.java:58)
2021-11-01T10:44:56.1839648Z Nov 01 10:44:56 	at org.apache.flink.table.factories.FactoryUtilTest.testError(FactoryUtilTest.java:374)
2021-11-01T10:44:56.1840250Z Nov 01 10:44:56 	at org.apache.flink.table.factories.FactoryUtilTest.testSecretOption(FactoryUtilTest.java:162)
2021-11-01T10:44:56.1840845Z Nov 01 10:44:56 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-11-01T10:44:56.1841351Z Nov 01 10:44:56 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-11-01T10:44:56.1841909Z Nov 01 10:44:56 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-11-01T10:44:56.1842511Z Nov 01 10:44:56 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-11-01T10:44:56.1843031Z Nov 01 10:44:56 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-11-01T10:44:56.1843585Z Nov 01 10:44:56 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-11-01T10:44:56.1844165Z Nov 01 10:44:56 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-11-01T10:44:56.1844737Z Nov 01 10:44:56 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-11-01T10:44:56.1845314Z Nov 01 10:44:56 	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
2021-11-01T10:44:56.1845853Z Nov 01 10:44:56 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-11-01T10:44:56.1846320Z Nov 01 10:44:56 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-11-01T10:44:56.1846827Z Nov 01 10:44:56 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-11-01T10:44:56.1847383Z Nov 01 10:44:56 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-11-01T10:44:56.1847882Z Nov 01 10:44:56 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-11-01T10:44:56.1848368Z Nov 01 10:44:56 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-11-01T10:44:56.1848944Z Nov 01 10:44:56 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-11-01T10:44:56.1849425Z Nov 01 10:44:56 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-11-01T10:44:56.1849948Z Nov 01 10:44:56 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-11-01T10:44:56.1850615Z Nov 01 10:44:56 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-11-01T10:44:56.1851135Z Nov 01 10:44:56 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-11-01T10:44:56.1851720Z Nov 01 10:44:56 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-11-01T10:44:56.1852374Z Nov 01 10:44:56 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-11-01T10:44:56.1852953Z Nov 01 10:44:56 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-11-01T10:44:56.1853562Z Nov 01 10:44:56 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-11-01T10:44:56.1854159Z Nov 01 10:44:56 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-11-01T10:44:56.1854723Z Nov 01 10:44:56 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-11-01T10:44:56.1855266Z Nov 01 10:44:56 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-11-01T10:44:56.1855815Z Nov 01 10:44:56 Caused by: org.apache.flink.table.api.ValidationException: One or more required options are missing.
2021-11-01T10:44:56.1856232Z Nov 01 10:44:56 
2021-11-01T10:44:56.1856505Z Nov 01 10:44:56 Missing required options are:
2021-11-01T10:44:56.1856793Z Nov 01 10:44:56 
2021-11-01T10:44:56.1857029Z Nov 01 10:44:56 target
2021-11-01T10:44:56.1857549Z Nov 01 10:44:56 	at org.apache.flink.table.factories.FactoryUtil.validateFactoryOptions(FactoryUtil.java:384)
2021-11-01T10:44:56.1858212Z Nov 01 10:44:56 	at org.apache.flink.table.factories.FactoryUtil.validateFactoryOptions(FactoryUtil.java:357)
2021-11-01T10:44:56.1858942Z Nov 01 10:44:56 	at org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.validate(FactoryUtil.java:718)
2021-11-01T10:44:56.1859611Z Nov 01 10:44:56 	at org.apache.flink.table.factories.TestDynamicTableFactory.createDynamicTableSource(TestDynamicTableFactory.java:80)
2021-11-01T10:44:56.1860254Z Nov 01 10:44:56 	at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:137)
2021-11-01T10:44:56.1860658Z Nov 01 10:44:56 	... 30 more
2021-11-01T10:44:56.1860902Z Nov 01 10:44:56 
2021-11-01T10:44:56.1861143Z Nov 01 10:44:56 [INFO] 
2021-11-01T10:44:56.1861484Z Nov 01 10:44:56 [ERROR] Tests run: 1396, Failures: 1, Errors: 0, Skipped: 0
{code};;;","02/Nov/21 11:56;chesnay;Reverted for 1.13.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink should handle the state transition of the pod from Pending to Failed,FLINK-24380,13403414,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,guoyangze,guoyangze,27/Sep/21 03:08,15/Dec/21 01:44,13/Jul/23 08:12,29/Sep/21 07:54,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.3,1.15.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"In K8s, there is five phases in pod's lifecycle: Pending, Running, Secceeded, Failed and Unknown. Currently, Flink does not handle the state transition of the pod from Pending to Failed. If a pod failed from Pending by `OutOfCPU` or `OutOfMem`, it will never be released and Flink keep waiting for it.

To fix this issue, Flink should terminate the pod in Failed phase proactively.",,guoyangze,Thesharing,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 29 07:53:53 UTC 2021,,,,,,,,,,"0|z0vb1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/21 03:09;guoyangze;[~wangyang0918] Would you assign this to me?;;;","27/Sep/21 03:14;wangyang0918;Thanks for creating this ticket. I have assigned it to you.;;;","29/Sep/21 07:53;wangyang0918;Fixed via:

master(1.15): b9fbdf8c906aea4fce8881a7b8fb13619f4157ea

release-1.14: ee4776799ddfca0a7554fbc2d78d640a16eb6ab4

release-1.13: 5452dccfb4e1e80a627647caea63aba3bda0fa79;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TM resource may not be properly released after heartbeat timeout,FLINK-24377,13403342,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xtsong,xtsong,xtsong,26/Sep/21 08:08,15/Dec/21 01:44,13/Jul/23 08:12,29/Sep/21 01:36,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.3,1.15.0,,,Deployment / Kubernetes,Deployment / YARN,Runtime / Coordination,,,0,pull-request-available,,,,"In native k8s and yarn deploy modes, RM disconnects a TM when its heartbeat times out. However, it does not actively release the pod / container of that TM. The releasing of pod / container relies on the TM to terminate itself after failing to re-register to the RM.

In some rare conditions, the TM process may not terminate and hang out for long time. In such cases, k8s / yarn sees the process running, thus will not release the pod / container. Neither will Flink's resource manager. Consequently, the resource is leaked until the entire application is terminated.

To fix this, we should make {{ActiveResourceManager}} to actively release the resource to K8s / Yarn after a TM heartbeat timeout.",,wanglijie,wangyang0918,xtsong,zlzhang0122,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 28 08:36:41 UTC 2021,,,,,,,,,,"0|z0vam0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/21 11:58;zlzhang0122;We also have encountered this situation too.;;;","28/Sep/21 08:36;xtsong;Fixed via:
- master (1.15): c81a530392b149141b1124bf83918f717d022111
- release-1.14: fcb19e6bb65128f24d80d68c29ce432d9bdcea22
- release-1.13: 1057f4171645b48c7743571e9f90e20b94a64900
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ManualCheckpointITCase.testTriggeringWhenPeriodicDisabled fails on azure,FLINK-24374,13403310,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dwysakowicz,xtsong,xtsong,26/Sep/21 02:15,28/Sep/21 20:12,13/Jul/23 08:12,28/Sep/21 10:46,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24489&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=5037

{code}
Sep 25 01:47:42 java.lang.AssertionError: 
Sep 25 01:47:42 
Sep 25 01:47:42 Expected: <1L>
Sep 25 01:47:42      but: was <0L>
Sep 25 01:47:42 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Sep 25 01:47:42 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
Sep 25 01:47:42 	at org.apache.flink.test.checkpointing.ManualCheckpointITCase.testTriggeringWhenPeriodicDisabled(ManualCheckpointITCase.java:106)
{code}",,dwysakowicz,roman,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24280,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 28 10:46:40 UTC 2021,,,,,,,,,,"0|z0vaew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/21 02:16;xtsong;cc [~dwysakowicz];;;","26/Sep/21 02:17;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24489&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10606;;;","26/Sep/21 02:17;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24489&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=5087;;;","27/Sep/21 03:01;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24511&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5305

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24511&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=5031

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24511&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10622;;;","28/Sep/21 05:06;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24546&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10969

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24546&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=5030;;;","28/Sep/21 06:58;roman;[https://dev.azure.com/khachatryanroman/flink/_build/results?buildId=1162&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=9a028d19-6c4b-5a4e-d378-03fca149d0b1&l=7630];;;","28/Sep/21 10:46;dwysakowicz;Fixed in 1261d66350dd3980eea41aa90cb155e304be2783;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support SinkWriter preCommit without the need of a committer,FLINK-24371,13403114,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fpaul,fpaul,fpaul,24/Sep/21 10:39,15/Dec/21 01:44,13/Jul/23 08:12,28/Sep/21 13:27,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,API / DataStream,Connectors / Common,,,,0,pull-request-available,,,,"For some sinks i.e. elasticsearch we only want to listen for the snapshot barrier once retrieved the sink flushes the buffered records.
These sinks do not snapshot any state thus do not implement the `snapshotState()` method. 

We already have a NoopCommitHandler which swallows the passed committables but it should at least call `preCommit()` on the SinkWriter.",,fpaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 28 13:27:39 UTC 2021,,,,,,,,,,"0|z0v97c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/21 13:27;arvid;Merged into master as 44bbd1df377e40f715cca529e5c2ab675c43ff36..4fe9f525a92319acc1e3434bebed601306f7a16f, into 1.14 as aacdf554ceaef950a7a8c478d549e720df27be6b..770684b71756f00c903574daa7096dd686591218.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unnecessary/misleading error message about failing restores when tasks are already canceled.,FLINK-24366,13402985,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,23/Sep/21 16:38,15/Dec/21 01:44,13/Jul/23 08:12,19/Nov/21 00:11,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Runtime / Task,,,,,0,,,,,"The following line is logged in all cases where the restore operation fails. The check whether the task is canceled comes only after that line.

The fix would be to move the log line to after the check.

{code}
Exception while restoring my-stateful-task from alternative (1/1), will retry while more alternatives are available.
{code}
",,liyu,pnowojski,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 19 00:11:02 UTC 2021,,,,,,,,,,"0|z0v8eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/21 00:11;sewen;Fixed
  - 1.13.4 (release-1.13) via a72bd1c5cde8e5a0bc25c4d2c78d890aa660bb1c
  - 1.14.1 (release-1.14) via 6a651fcc3c4a9f9b77df20a0083cacedb875350f
  - 1.15.0 (master) via e6798c3e40bffeccfc35c84021e63aae41a3ac48;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-avro-glue-schema-registry fails compiling with scala 2.12 due to dependency convergence,FLINK-24358,13402847,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xtsong,xtsong,xtsong,23/Sep/21 03:47,27/Sep/21 09:27,13/Jul/23 08:12,27/Sep/21 09:27,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Build System,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24405&view=logs&j=ed6509f5-1153-558c-557a-5ee0afbcdf24&t=241b1e5e-1a8e-5e6a-469a-a9b8cad87065&l=13506

{code}
[WARNING] 
Dependency convergence error for io.netty:netty-handler:4.1.63.Final paths to dependency are:
+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT
  +-software.amazon.glue:schema-registry-common:1.1.2
    +-software.amazon.awssdk:glue:2.16.92
      +-software.amazon.awssdk:netty-nio-client:2.16.92
        +-io.netty:netty-codec-http:4.1.63.Final
          +-io.netty:netty-handler:4.1.63.Final
and
+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT
  +-software.amazon.glue:schema-registry-common:1.1.2
    +-software.amazon.awssdk:glue:2.16.92
      +-software.amazon.awssdk:netty-nio-client:2.16.92
        +-io.netty:netty-codec-http2:4.1.63.Final
          +-io.netty:netty-handler:4.1.63.Final
and
+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT
  +-software.amazon.glue:schema-registry-common:1.1.2
    +-software.amazon.awssdk:glue:2.16.92
      +-software.amazon.awssdk:netty-nio-client:2.16.92
        +-io.netty:netty-handler:4.1.63.Final
and
+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT
  +-software.amazon.glue:schema-registry-common:1.1.2
    +-software.amazon.awssdk:glue:2.16.92
      +-software.amazon.awssdk:netty-nio-client:2.16.92
        +-com.typesafe.netty:netty-reactive-streams-http:2.0.5
          +-com.typesafe.netty:netty-reactive-streams:2.0.5
            +-io.netty:netty-handler:4.1.52.Final

[WARNING] 
Dependency convergence error for org.jetbrains.kotlin:kotlin-stdlib:1.3.50 paths to dependency are:
+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT
  +-software.amazon.glue:schema-registry-serde:1.1.2
    +-com.kjetland:mbknor-jackson-jsonschema_2.12:1.0.39
      +-org.jetbrains.kotlin:kotlin-scripting-compiler-embeddable:1.3.50
        +-org.jetbrains.kotlin:kotlin-scripting-compiler-impl-embeddable:1.3.50
          +-org.jetbrains.kotlin:kotlin-scripting-common:1.3.50
            +-org.jetbrains.kotlin:kotlin-reflect:1.3.50
              +-org.jetbrains.kotlin:kotlin-stdlib:1.3.50
and
+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT
  +-software.amazon.glue:schema-registry-serde:1.1.2
    +-com.kjetland:mbknor-jackson-jsonschema_2.12:1.0.39
      +-org.jetbrains.kotlin:kotlin-scripting-compiler-embeddable:1.3.50
        +-org.jetbrains.kotlin:kotlin-scripting-compiler-impl-embeddable:1.3.50
          +-org.jetbrains.kotlin:kotlin-scripting-common:1.3.50
            +-org.jetbrains.kotlin:kotlin-stdlib:1.3.50
and
+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT
  +-software.amazon.glue:schema-registry-serde:1.1.2
    +-com.kjetland:mbknor-jackson-jsonschema_2.12:1.0.39
      +-org.jetbrains.kotlin:kotlin-scripting-compiler-embeddable:1.3.50
        +-org.jetbrains.kotlin:kotlin-scripting-compiler-impl-embeddable:1.3.50
          +-org.jetbrains.kotlin:kotlin-scripting-jvm:1.3.50
            +-org.jetbrains.kotlin:kotlin-stdlib:1.3.50
and
+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT
  +-software.amazon.glue:schema-registry-serde:1.1.2
    +-com.kjetland:mbknor-jackson-jsonschema_2.12:1.0.39
      +-org.jetbrains.kotlin:kotlin-scripting-compiler-embeddable:1.3.50
        +-org.jetbrains.kotlin:kotlin-scripting-compiler-impl-embeddable:1.3.50
          +-org.jetbrains.kotlin:kotlin-stdlib:1.3.50
and
+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT
  +-software.amazon.glue:schema-registry-serde:1.1.2
    +-com.kjetland:mbknor-jackson-jsonschema_2.12:1.0.39
      +-org.jetbrains.kotlin:kotlin-scripting-compiler-embeddable:1.3.50
        +-org.jetbrains.kotlin:kotlin-scripting-compiler-impl-embeddable:1.3.50
          +-org.jetbrains.kotlinx:kotlinx-coroutines-core:1.1.1
            +-org.jetbrains.kotlin:kotlin-stdlib:1.3.20
and
+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT
  +-software.amazon.glue:schema-registry-serde:1.1.2
    +-com.kjetland:mbknor-jackson-jsonschema_2.12:1.0.39
      +-org.jetbrains.kotlin:kotlin-scripting-compiler-embeddable:1.3.50
        +-org.jetbrains.kotlin:kotlin-stdlib:1.3.50

[WARNING] 
Dependency convergence error for io.netty:netty-codec-http:4.1.63.Final paths to dependency are:
+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT
  +-software.amazon.glue:schema-registry-common:1.1.2
    +-software.amazon.awssdk:glue:2.16.92
      +-software.amazon.awssdk:netty-nio-client:2.16.92
        +-io.netty:netty-codec-http:4.1.63.Final
and
+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT
  +-software.amazon.glue:schema-registry-common:1.1.2
    +-software.amazon.awssdk:glue:2.16.92
      +-software.amazon.awssdk:netty-nio-client:2.16.92
        +-io.netty:netty-codec-http2:4.1.63.Final
          +-io.netty:netty-codec-http:4.1.63.Final
and
+-org.apache.flink:flink-avro-glue-schema-registry_2.12:1.15-SNAPSHOT
  +-software.amazon.glue:schema-registry-common:1.1.2
    +-software.amazon.awssdk:glue:2.16.92
      +-software.amazon.awssdk:netty-nio-client:2.16.92
        +-com.typesafe.netty:netty-reactive-streams-http:2.0.5
          +-io.netty:netty-codec-http:4.1.52.Final

[WARNING] Rule 0: org.apache.maven.plugins.enforcer.DependencyConvergence failed with message:
Failed while enforcing releasability. See above detailed error message.
{code}",,dannycranmer,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23389,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 27 09:27:24 UTC 2021,,,,,,,,,,"0|z0v7k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/21 03:49;xtsong;It's wired this does not fail other stages.
cc [~chesnay], any insights?;;;","23/Sep/21 06:02;chesnay;Maybe there is some Scala dependency that has a slightly different dependency set between 2.11/2.12? I can't really think of anything else. In any case it would be good to resolve the conflicts.;;;","23/Sep/21 07:37;xtsong;It turns out this module is only included with scala 2.12.;;;","23/Sep/21 07:41;xtsong;cc [~hhkkxxx133] [~dannycranmer];;;","23/Sep/21 08:25;dannycranmer;Thanks [~xtsong], why did this not fail when building locally, or in the PR CI? Once the AVRO dependencies are fixed there may be a similar issue in the JSON module;;;","23/Sep/21 08:28;chesnay;PRs only run Scala 2.11.;;;","23/Sep/21 08:48;dannycranmer;Thanks [~chesnay], how would we reproduce this locally? Build passes for me when running 

{code}mvn clean install -Pscala-2.12 -Dscala-2.12{code} 

From 
- {{flink-avro-glue-schema-registry}}
- {{flink-formats}};;;","23/Sep/21 08:50;chesnay;add {{-Dcheck-convergence}};;;","23/Sep/21 08:53;dannycranmer;ok, so the json module fails too, I will see if the PR fixes both and if not, create a new PR;;;","23/Sep/21 09:09;xtsong;[~dannycranmer],
No, the PR doesn't fix for the json module. I'll update the PR. I'm updating it for the CI tests anyway.


;;;","23/Sep/21 09:20;xtsong;PR updated.
Building {{flink-formats}} succeed for me locally.;;;","23/Sep/21 14:14;dannycranmer;CI passed, thanks.;;;","24/Sep/21 01:38;xtsong;Fixed via:
- master (1.15): 61c1ea444f49b5c15adbed78b6b3b447a45c52bd;;;","27/Sep/21 03:05;xtsong;Reopen for the same problem in e2e test modules.;;;","27/Sep/21 09:27;xtsong;Fixed via:
- master (1.15): 6c2e605701d44da90e72ad7df6e1370c50984a24;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperLeaderElectionConnectionHandlingTest#testLoseLeadershipOnLostConnectionIfTolerateSuspendedConnectionsIsEnabled fails with an Unhandled error,FLINK-24357,13402724,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,pnowojski,pnowojski,22/Sep/21 15:18,15/Dec/21 01:44,13/Jul/23 08:12,08/Oct/21 15:10,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"In a [private azure|https://dev.azure.com/pnowojski/637f6470-2732-4605-a304-caebd40e284b/_apis/build/builds/517/logs/155] build when testing my own PR I've noticed the following error that looks unrelated to any of my changes (modifications to {{Task}} class error/cancellation handling logic):

{noformat}

2021-09-22T08:09:16.6244936Z Sep 22 08:09:16 [ERROR] testLoseLeadershipOnLostConnectionIfTolerateSuspendedConnectionsIsEnabled  Time elapsed: 28.753 s  <<< FAILURE!
2021-09-22T08:09:16.6245821Z Sep 22 08:09:16 java.lang.AssertionError: The TestingFatalErrorHandler caught an exception.
2021-09-22T08:09:16.6246513Z Sep 22 08:09:16 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource.after(TestingFatalErrorHandlerResource.java:78)
2021-09-22T08:09:16.6247281Z Sep 22 08:09:16 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource.access$300(TestingFatalErrorHandlerResource.java:33)
2021-09-22T08:09:16.6248167Z Sep 22 08:09:16 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$1.evaluate(TestingFatalErrorHandlerResource.java:57)
2021-09-22T08:09:16.6248862Z Sep 22 08:09:16 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2021-09-22T08:09:16.6249620Z Sep 22 08:09:16 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-09-22T08:09:16.6250210Z Sep 22 08:09:16 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2021-09-22T08:09:16.6250773Z Sep 22 08:09:16 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2021-09-22T08:09:16.6251375Z Sep 22 08:09:16 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2021-09-22T08:09:16.6251951Z Sep 22 08:09:16 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2021-09-22T08:09:16.6252562Z Sep 22 08:09:16 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2021-09-22T08:09:16.6253415Z Sep 22 08:09:16 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2021-09-22T08:09:16.6254469Z Sep 22 08:09:16 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2021-09-22T08:09:16.6255039Z Sep 22 08:09:16 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2021-09-22T08:09:16.6256238Z Sep 22 08:09:16 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2021-09-22T08:09:16.6257109Z Sep 22 08:09:16 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2021-09-22T08:09:16.6257766Z Sep 22 08:09:16 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2021-09-22T08:09:16.6258406Z Sep 22 08:09:16 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2021-09-22T08:09:16.6259050Z Sep 22 08:09:16 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2021-09-22T08:09:16.6259827Z Sep 22 08:09:16 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2021-09-22T08:09:16.6260963Z Sep 22 08:09:16 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2021-09-22T08:09:16.6261796Z Sep 22 08:09:16 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
2021-09-22T08:09:16.6262428Z Sep 22 08:09:16 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2021-09-22T08:09:16.6263268Z Sep 22 08:09:16 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2021-09-22T08:09:16.6263875Z Sep 22 08:09:16 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2021-09-22T08:09:16.6265025Z Sep 22 08:09:16 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
2021-09-22T08:09:16.6265940Z Sep 22 08:09:16 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2021-09-22T08:09:16.6266767Z Sep 22 08:09:16 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2021-09-22T08:09:16.6267470Z Sep 22 08:09:16 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2021-09-22T08:09:16.6268165Z Sep 22 08:09:16 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2021-09-22T08:09:16.6269341Z Sep 22 08:09:16 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2021-09-22T08:09:16.6269928Z Sep 22 08:09:16 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2021-09-22T08:09:16.6270951Z Sep 22 08:09:16 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
2021-09-22T08:09:16.6271683Z Sep 22 08:09:16 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
2021-09-22T08:09:16.6274483Z Sep 22 08:09:16 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
2021-09-22T08:09:16.6275060Z Sep 22 08:09:16 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
2021-09-22T08:09:16.6275819Z Sep 22 08:09:16 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
2021-09-22T08:09:16.6276557Z Sep 22 08:09:16 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
2021-09-22T08:09:16.6277234Z Sep 22 08:09:16 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
2021-09-22T08:09:16.6277770Z Sep 22 08:09:16 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
2021-09-22T08:09:16.6278345Z Sep 22 08:09:16 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2021-09-22T08:09:16.6278909Z Sep 22 08:09:16 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-09-22T08:09:16.6279433Z Sep 22 08:09:16 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-09-22T08:09:16.6279939Z Sep 22 08:09:16 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-09-22T08:09:16.6280627Z Sep 22 08:09:16 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-09-22T08:09:16.6281222Z Sep 22 08:09:16 Caused by: org.apache.flink.runtime.leaderelection.LeaderElectionException: Unhandled error in ZooKeeperLeaderElectionDriver: Ensure path threw exception
2021-09-22T08:09:16.6281884Z Sep 22 08:09:16 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.unhandledError(ZooKeeperLeaderElectionDriver.java:295)
2021-09-22T08:09:16.6282527Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl$6.apply(CuratorFrameworkImpl.java:713)
2021-09-22T08:09:16.6283172Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl$6.apply(CuratorFrameworkImpl.java:709)
2021-09-22T08:09:16.6283803Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:100)
2021-09-22T08:09:16.6284454Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.shaded.com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:30)
2021-09-22T08:09:16.6285302Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:92)
2021-09-22T08:09:16.6286162Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.logError(CuratorFrameworkImpl.java:708)
2021-09-22T08:09:16.6286836Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.NamespaceImpl.fixForNamespace(NamespaceImpl.java:100)
2021-09-22T08:09:16.6287505Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.fixForNamespace(CuratorFrameworkImpl.java:731)
2021-09-22T08:09:16.6288396Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.WatcherRemovalFacade.fixForNamespace(WatcherRemovalFacade.java:170)
2021-09-22T08:09:16.6289236Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:295)
2021-09-22T08:09:16.6289862Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:35)
2021-09-22T08:09:16.6290487Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.cache.TreeCache$TreeNode.doRefreshData(TreeCache.java:287)
2021-09-22T08:09:16.6291319Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.cache.TreeCache$TreeNode.refreshData(TreeCache.java:266)
2021-09-22T08:09:16.6291975Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.cache.TreeCache$TreeNode.refresh(TreeCache.java:250)
2021-09-22T08:09:16.6292615Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.cache.TreeCache$TreeNode.wasCreated(TreeCache.java:316)
2021-09-22T08:09:16.6293277Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.cache.TreeCache.handleStateChange(TreeCache.java:819)
2021-09-22T08:09:16.6293918Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.cache.TreeCache.access$1800(TreeCache.java:75)
2021-09-22T08:09:16.6294555Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.cache.TreeCache$1.stateChanged(TreeCache.java:543)
2021-09-22T08:09:16.6295373Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.state.ConnectionStateManager.lambda$processEvents$1(ConnectionStateManager.java:280)
2021-09-22T08:09:16.6296073Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.listen.MappingListenerManager.lambda$forEach$0(MappingListenerManager.java:93)
2021-09-22T08:09:16.6296829Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.listen.MappingListenerManager.forEach(MappingListenerManager.java:90)
2021-09-22T08:09:16.6297559Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.listen.StandardListenerManager.forEach(StandardListenerManager.java:89)
2021-09-22T08:09:16.6298402Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:280)
2021-09-22T08:09:16.6299099Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.state.ConnectionStateManager.access$000(ConnectionStateManager.java:44)
2021-09-22T08:09:16.6300215Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.state.ConnectionStateManager$1.call(ConnectionStateManager.java:133)
2021-09-22T08:09:16.6318237Z Sep 22 08:09:16 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-09-22T08:09:16.6318982Z Sep 22 08:09:16 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2021-09-22T08:09:16.6323311Z Sep 22 08:09:16 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2021-09-22T08:09:16.6326943Z Sep 22 08:09:16 	at java.lang.Thread.run(Thread.java:748)
2021-09-22T08:09:16.6327544Z Sep 22 08:09:16 Caused by: org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /flink
2021-09-22T08:09:16.6328187Z Sep 22 08:09:16 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:102)
2021-09-22T08:09:16.6328785Z Sep 22 08:09:16 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:54)
2021-09-22T08:09:16.6329526Z Sep 22 08:09:16 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1111)
2021-09-22T08:09:16.6330053Z Sep 22 08:09:16 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1139)
2021-09-22T08:09:16.6330609Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.utils.ZKPaths.mkdirs(ZKPaths.java:291)
2021-09-22T08:09:16.6331177Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.NamespaceImpl$1.call(NamespaceImpl.java:90)
2021-09-22T08:09:16.6331842Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.connection.StandardConnectionHandlingPolicy.callWithRetry(StandardConnectionHandlingPolicy.java:64)
2021-09-22T08:09:16.6332463Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:100)
2021-09-22T08:09:16.6333051Z Sep 22 08:09:16 	at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.NamespaceImpl.fixForNamespace(NamespaceImpl.java:83)
2021-09-22T08:09:16.6333499Z Sep 22 08:09:16 	... 22 more
{noformat}

{noformat}
2021-09-22T08:29:57.2073586Z Sep 22 08:29:57 [ERROR] Failures: 
2021-09-22T08:29:57.2074275Z Sep 22 08:29:57 [ERROR]   ZooKeeperLeaderElectionConnectionHandlingTest.testLoseLeadershipOnLostConnectionIfTolerateSuspendedConnectionsIsEnabled The TestingFatalErrorHandler caught an exception.
{noformat}
",,pnowojski,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24182,,,,,,,,,,,,,,,,"30/Sep/21 14:52;pnowojski;logs-ci_build-test_ci_build_core-1632297723.zip;https://issues.apache.org/jira/secure/attachment/13034426/logs-ci_build-test_ci_build_core-1632297723.zip",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 08 15:10:38 UTC 2021,,,,,,,,,,"0|z0v6so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/21 15:20;pnowojski;Very unlikely, but potentially related to FLINK-24182, as it was first spotted in a PR for this issue.;;;","30/Sep/21 14:44;trohrmann;[~pnowojski] do you have the logs for the test run? If yes, then please upload them to this ticket. This will help debugging the problem.;;;","30/Sep/21 14:53;pnowojski;I've attached the logs extracted from [the build|https://dev.azure.com/pnowojski/Flink/_build/results?buildId=517&view=results];;;","30/Sep/21 15:14;trohrmann;I think the problem is a test instability. In the test we stop the ZooKeeper server in order to provoke a lost connection. This of course can result into some {{KeeperException$ConnectionLossException}} if we did not manage to create the directories for the {{TreeCache}} in the meantime. This should not be problematic and should be handled in the test case.;;;","08/Oct/21 15:10;trohrmann;Fixed via

1.15.0: b3ad16cabd2fc5bac34318d373d379d20b139966
1.14.1: f3bbad73fde42653e37b1219b7dc277e4338f477;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not able to cancel delayed message using remote stateful function mode,FLINK-24356,13402678,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,igal,Pedrycz,Pedrycz,22/Sep/21 11:05,23/Sep/21 11:50,13/Jul/23 08:12,23/Sep/21 11:49,statefun-3.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,statefun-3.2.0,,,,,Stateful Functions,,,,,1,pull-request-available,,,,"My use case is processing delayed messages with ability to cancel them and update their delay. Whole logic is to wait with sending messages from ingress to egress. 

At first I worked with embedded mode, with org.apache.flink.statefun.sdk.Context. Just for testing I’ve made a function that invoked cancelDelayedMessage() method and straight after sendAfter() method. I used message id as cancellation token. My function worked just fine, message was overridden (cancelled and send with new delay).

After that I wanted to switch to remote mode, so I used org.apache.flink.statefun.sdk.java.Context. I’ve made similar function as previously, but it did not work. Just to be sure that cancellation works, I invoked sendAfter() and straight after that cancelDelayedMessage() method. Messages were still appearing on egress topic after original delay. 

Questions:

1. Whether cancelDelayedMessage() is working in remote mode? 

2. Should it work the same like in embedded mode?

3. Should it work as expected if we call cancelDelayedMessage() in a moment after sendAfter()? (for testing purposes)",,igal,Pedrycz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Thu Sep 23 11:49:44 UTC 2021,,,,,,,,,,"0|z0v6ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/21 11:29;igal;Hello [~Pedrycz], thank you for reporting this. Indeed this look like a bug, and it will be fixed in the upcoming release.

 ;;;","22/Sep/21 11:58;Pedrycz;Thank you for quick response. What is the ETA for the upcoming release? ;;;","22/Sep/21 12:00;igal;We will try to kick off the release process by the end of the month as we've already have some stuff accumulated. 

The PR for the fix will be open soon, is there a way for you to try it out?;;;","22/Sep/21 13:32;Pedrycz;I see PR, I will try to build and test it. ;;;","23/Sep/21 10:04;Pedrycz;It works like a charm, thank you kindly.;;;","23/Sep/21 11:49;igal;Fixed at 2600dfb077d05338d6dbebee3cba6d92ef75aa25;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bash scripts do not respect dynamic configurations when calculating memory sizes,FLINK-24353,13402631,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xtsong,xtsong,xtsong,22/Sep/21 07:26,15/Dec/21 01:44,13/Jul/23 08:12,23/Sep/21 05:53,1.13.2,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.3,1.15.0,,,Deployment / Scripts,,,,,0,pull-request-available,,,,"Dynamic configurations (the '-D' arguments) are lost due to changes in FLINK-21128.

Consequently, dynamic configurations like the following commands will not take effect.
{code}
./bin/taskmanager.sh start -D taskmanager.memory.task.off-heap.size=128m
{code}",,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 23 05:53:11 UTC 2021,,,,,,,,,,"0|z0v680:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/21 07:36;chesnay;Shouldn't it be {{./bin/taskmanager.sh start -Dtaskmanager.memory.task.off-heap.size=128m}}?;;;","22/Sep/21 08:19;xtsong;I think both `{{-Dkey=value}}` and `{{-D key=value}}` are expected to work.

I tried this on {{taskmanager.numberOfTaskSlots}} and both ways work.

However, for memory options, -only `{{-Dkey=value}}` works but not `{{-D key=value}}`- neither way works.;;;","23/Sep/21 05:53;xtsong;Fixed via:
- master (1.15): 1d564a95581f5b4b1d8ae343676474a4fa570ffa
- release-1.14: f990cd168d00b4d0afe98d7f1748d5e74d072dba
- release-1.13: b8c1a597533e2d9091caa8f3d864e24be4fb8ad2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lookup join + WITH clause throws NPE,FLINK-24352,13402626,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yuval.Itzchakov,TsReaper,TsReaper,22/Sep/21 07:13,31/Dec/21 09:09,13/Jul/23 08:12,31/Dec/21 02:53,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Add the following SQL to {{org.apache.flink.table.api.TableEnvironmentITCase}} to reproduce this bug.
{code:scala}
@Test
def myTest(): Unit = {
  tEnv.executeSql(
    """"""
      |CREATE TABLE T1 (
      |  a INT,
      |  b STRING,
      |  proctime AS PROCTIME()
      |) WITH (
      |  'connector' = 'values',
      |  'bounded' = 'true'
      |)
      |"""""".stripMargin)
  tEnv.executeSql(
    """"""
      |CREATE TABLE T2 (
      |  a INT,
      |  b STRING
      |) WITH (
      |  'connector' = 'values',
      |  'bounded' = 'true'
      |)
      |"""""".stripMargin)
  tEnv.explainSql(
    """"""
      |WITH MyView(a, b) AS (SELECT a, b FROM T2)
      |SELECT * FROM T1 AS T
      |LEFT JOIN MyView FOR SYSTEM_TIME AS OF T.proctime AS D
      |ON T.a = D.a
      |"""""".stripMargin)
}
{code}

The exception stack is
{code}
org.apache.flink.table.api.ValidationException: SQL validation failed. null

	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:165)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:107)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:217)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:101)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainSql(TableEnvironmentImpl.java:686)

// IDEA and Junit stacks are omitted

Caused by: java.lang.NullPointerException
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSnapshot(SqlValidatorImpl.java:4714)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:986)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3085)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3070)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3133)
	at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:117)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3076)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3335)
	at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:975)
	at org.apache.calcite.sql.validate.WithNamespace.validateImpl(WithNamespace.java:57)
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateWith(SqlValidatorImpl.java:3744)
	at org.apache.calcite.sql.SqlWith.validate(SqlWith.java:71)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:952)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:704)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:160)
	... 43 more
{code}

However if we use {{CREATE VIEW}} statement to create this view it will run successfully.",,airblader,godfreyhe,jackwangcs,jark,jingzhang,libenchao,martijnvisser,TsReaper,Yuval.Itzchakov,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24956,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 31 09:09:28 UTC 2021,,,,,,,,,,"0|z0v66w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/21 07:19;godfreyhe;good catch~;;;","19/Nov/21 01:35;TsReaper;I see [~Yuval.Itzchakov] is willing to solve this. [~Yuval.Itzchakov] if you would like to solve this please reply to this issue so a committer can assign this to you. Also when your github PR is ready ping me (@tsreaper) so that I can help for the review.;;;","21/Nov/21 12:45;Yuval.Itzchakov;[~TsReaper] It's ready: [https://github.com/apache/flink/pull/17845]

Not entirely sure that's the way we should approach this but let's get the ball rolling.;;;","31/Dec/21 02:53;godfreyhe;Fixed in 1.15.0: 1a0e058f88874ac303d9a22105d44700285b1288;;;","31/Dec/21 09:09;Yuval.Itzchakov;[~godfreyhe] Thanks for merging! I'd like to backport this to 1.14.X as well. Do I just open a PR agains't the relevant tag?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Kafka ITCases (e.g. KafkaTableITCase) fail with ""ContainerLaunch Container startup failed""",FLINK-24348,13402455,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,martijnvisser,dwysakowicz,dwysakowicz,21/Sep/21 12:06,30/Dec/21 09:22,13/Jul/23 08:12,29/Dec/21 14:44,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Connectors / Kafka,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24338&view=logs&j=b0097207-033c-5d9a-b48c-6d4796fbe60d&t=8338a7d2-16f7-52e5-f576-4b7b3071eb3d&l=7140

{code}
Sep 21 02:44:33 org.testcontainers.containers.ContainerLaunchException: Container startup failed
Sep 21 02:44:33 	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:334)
Sep 21 02:44:33 	at org.testcontainers.containers.KafkaContainer.doStart(KafkaContainer.java:97)
Sep 21 02:44:33 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase$1.doStart(KafkaTableTestBase.java:71)
Sep 21 02:44:33 	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:315)
Sep 21 02:44:33 	at org.testcontainers.containers.GenericContainer.starting(GenericContainer.java:1060)
Sep 21 02:44:33 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:29)
Sep 21 02:44:33 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Sep 21 02:44:33 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Sep 21 02:44:33 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Sep 21 02:44:33 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Sep 21 02:44:33 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Sep 21 02:44:33 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Sep 21 02:44:33 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Sep 21 02:44:33 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
Sep 21 02:44:33 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Sep 21 02:44:33 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Sep 21 02:44:33 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Sep 21 02:44:33 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
Sep 21 02:44:33 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Sep 21 02:44:33 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Sep 21 02:44:33 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Sep 21 02:44:33 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Sep 21 02:44:33 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Sep 21 02:44:33 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
Sep 21 02:44:33 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
Sep 21 02:44:33 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
Sep 21 02:44:33 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
Sep 21 02:44:33 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
Sep 21 02:44:33 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
Sep 21 02:44:33 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
Sep 21 02:44:33 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
Sep 21 02:44:33 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
Sep 21 02:44:33 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
Sep 21 02:44:33 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Sep 21 02:44:33 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Sep 21 02:44:33 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Sep 21 02:44:33 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Sep 21 02:44:33 Caused by: org.rnorth.ducttape.RetryCountExceededException: Retry limit hit with exception
Sep 21 02:44:33 	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:88)
Sep 21 02:44:33 	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:327)
Sep 21 02:44:33 	... 36 more
Sep 21 02:44:33 Caused by: org.testcontainers.containers.ContainerLaunchException: Could not create/start container
Sep 21 02:44:33 	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:523)
Sep 21 02:44:33 	at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:329)
Sep 21 02:44:33 	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)
Sep 21 02:44:33 	... 37 more
Sep 21 02:44:33 Caused by: java.lang.IllegalStateException: Container did not start correctly.
Sep 21 02:44:33 	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:461)
Sep 21 02:44:33 	... 39 more

{code}",,dwysakowicz,gaoyunhaii,leonard,mapohl,martijnvisser,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25088,,,,,,,,,,,FLINK-25088,,,,FLINK-24740,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 30 09:22:54 UTC 2021,,,,,,,,,,"0|z0v554:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Oct/21 09:56;mapohl;Same issue was observed on VVP's Flink 1.14 fork: https://dev.azure.com/ververica-dev/daplatform-flink/_build/results?buildId=1588&view=logs&j=d543d572-9428-5803-a30c-e8e09bf70915&t=4e4199a3-fbbb-5d5b-a2be-802955ffb013&l=7130;;;","12/Nov/21 09:45;mapohl;Failure of {{FlinkKafkaInternalProducerITCase}} caused by a problem during the container startup as well:  https://dev.azure.com/mapohl/flink/_build/results?buildId=478&view=logs&j=d543d572-9428-5803-a30c-e8e09bf70915&t=4e4199a3-fbbb-5d5b-a2be-802955ffb013&l=7152

I updated the issue's title accordingly;;;","29/Nov/21 11:25;trohrmann;Any updates for this ticket [~arvid]?;;;","30/Nov/21 06:29;gaoyunhaii;I copied some detailed log found be [~arvid]  in another repeat issue here:
{code:java}
03:37:59,944 [                main] INFO  🐳 [confluentinc/cp-kafka:5.5.2]                             [] - Creating container for image: confluentinc/cp-kafka:5.5.2
03:38:00,065 [                main] INFO  🐳 [confluentinc/cp-kafka:5.5.2]                             [] - Starting container with ID: 9f6a0e7664040345d6dc9a4189aea0bfcc2f191b31545e0ea0d191382454138c
03:38:00,537 [                main] INFO  🐳 [confluentinc/cp-kafka:5.5.2]                             [] - Container confluentinc/cp-kafka:5.5.2 is starting: 9f6a0e7664040345d6dc9a4189aea0bfcc2f191b31545e0ea0d191382454138c
03:38:00,640 [docker-java-stream-1817108049] INFO  org.apache.flink.connector.kafka.sink.KafkaSinkITCase        [] - STDERR: sh: 1: /testcontainers_start.sh: Text file busy
03:38:00,800 [                main] ERROR 🐳 [confluentinc/cp-kafka:5.5.2]                             [] - Could not start container
java.lang.IllegalStateException: Container did not start correctly.
	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:461) ~[testcontainers-1.16.0.jar:?]
	at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:329) ~[testcontainers-1.16.0.jar:?]
	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81) ~[duct-tape-1.0.8.jar:?]
	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:327) ~[testcontainers-1.16.0.jar:?]
	at org.testcontainers.containers.KafkaContainer.doStart(KafkaContainer.java:97) ~[kafka-1.16.0.jar:?]
	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:315) ~[testcontainers-1.16.0.jar:?]
	at org.testcontainers.containers.GenericContainer.starting(GenericContainer.java:1060) ~[testcontainers-1.16.0.jar:?]
	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:29) ~[testcontainers-1.16.0.jar:?]
	at org.junit.rules.RunRules.evaluate(RunRules.java:20) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43) ~[junit-vintage-engine-5.7.2.jar:5.7.2]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) ~[?:1.8.0_292]
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_292]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_292]
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ~[?:1.8.0_292]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ~[?:1.8.0_292]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_292]
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ~[?:1.8.0_292]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ~[?:1.8.0_292]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_292]
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485) ~[?:1.8.0_292]
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82) ~[junit-vintage-engine-5.7.2.jar:5.7.2]
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73) ~[junit-vintage-engine-5.7.2.jar:5.7.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220) ~[junit-platform-launcher-1.3.1.jar:1.3.1]
	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188) ~[junit-platform-launcher-1.3.1.jar:1.3.1]
	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202) [junit-platform-launcher-1.3.1.jar:1.3.1]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181) [junit-platform-launcher-1.3.1.jar:1.3.1]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128) [junit-platform-launcher-1.3.1.jar:1.3.1]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150) [surefire-junit-platform-2.22.2.jar:2.22.2]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120) [surefire-junit-platform-2.22.2.jar:2.22.2]
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) [surefire-booter-2.22.2.jar:2.22.2]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) [surefire-booter-2.22.2.jar:2.22.2]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) [surefire-booter-2.22.2.jar:2.22.2]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418) [surefire-booter-2.22.2.jar:2.22.2]
03:38:00,830 [                main] ERROR 🐳 [confluentinc/cp-kafka:5.5.2]                             [] - Log output from the failed container:
sh: 1: /testcontainers_start.sh: Text file busy {code};;;","30/Nov/21 07:12;arvid;I have not found out much yesterday. It is a standard Unix error but I'm unsure if it is a general issue with test containers or with the specific kafka container.;;;","30/Nov/21 07:51;arvid;Okay the script ""testcontainers_start.sh"" is generated in KafkaContainer in test containers 1.16.0. With https://github.com/testcontainers/testcontainers-java/pull/2078 that was changed and either it fixes the issue or should provide us better logs, so I'm proposing to upgrade to 1.16.2.;;;","30/Nov/21 08:04;martijnvisser;[~arvid] We have master already on 1.16.2 via https://issues.apache.org/jira/browse/FLINK-24740 

Given that this ticket was created for 1.14 and [~mapohl] also mentioned it for a 1.14 fork, it makes sense to backport it to 1.14. I can do that.;;;","30/Nov/21 09:27;arvid;Please do, since that is a test-only backport I expect little implications.;;;","30/Nov/21 19:19;martijnvisser;The backport is there, it just needs to be merged.;;;","01/Dec/21 16:30;martijnvisser;https://issues.apache.org/jira/browse/FLINK-24740 is merged to both master and release-1.14. This should resolve this issue. If it re-appears, please re-open the ticket and let me know. ;;;","16/Dec/21 07:53;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28157&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=35599

Hi [~MartijnVisser] it seems reproduced, could you have a double look~?;;;","16/Dec/21 08:20;martijnvisser;[~gaoyunhaii] I'll have a look;;;","21/Dec/21 20:19;martijnvisser;I did an investigation and I suspect that it's Zookeeper that's causing the issues. This is what's in the log for the failed run:


{code:bash}
===> Running preflight checks ... 
===> Launching ... 
===> Launching kafka ... 
[2021-12-15 10:21:42,557] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2021-12-15 10:21:43,120] INFO starting (kafka.server.KafkaServer)
[2021-12-15 10:21:43,120] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2021-12-15 10:21:43,142] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2021-12-15 10:21:43,167] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2021-12-15 10:21:43,195] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2021-12-15 10:22:01,168] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient)
[2021-12-15 10:22:01,289] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient)
[2021-12-15 10:22:01,291] ERROR Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)
kafka.zookeeper.ZooKeeperClientTimeoutException: Timed out waiting for connection while in state: CONNECTING
	at kafka.zookeeper.ZooKeeperClient.waitUntilConnected(ZooKeeperClient.scala:271)
	at kafka.zookeeper.ZooKeeperClient.<init>(ZooKeeperClient.scala:125)
	at kafka.zk.KafkaZkClient$.apply(KafkaZkClient.scala:1948)
	at kafka.server.KafkaServer.createZkClient$1(KafkaServer.scala:431)
	at kafka.server.KafkaServer.initZkClient(KafkaServer.scala:456)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:191)
	at kafka.Kafka$.main(Kafka.scala:109)
	at kafka.Kafka.main(Kafka.scala)
[2021-12-15 10:22:01,294] INFO shutting down (kafka.server.KafkaServer)
[2021-12-15 10:22:01,301] INFO shut down completed (kafka.server.KafkaServer)
[2021-12-15 10:22:01,302] ERROR Exiting Kafka. (kafka.Kafka$)
[2021-12-15 10:22:01,303] INFO shutting down (kafka.server.KafkaServer)
{code}

During successful runs, the logs look like:

{code:bash}
10:21:21,002 [docker-java-stream-1918861308] INFO  org.apache.flink.connector.kafka.sink.KafkaSinkITCase        [] - STDOUT: ===> Running preflight checks ... 
10:21:21,018 [docker-java-stream-1918861308] INFO  org.apache.flink.connector.kafka.sink.KafkaSinkITCase        [] - STDOUT: ===> Launching ... 
10:21:21,030 [docker-java-stream-1918861308] INFO  org.apache.flink.connector.kafka.sink.KafkaSinkITCase        [] - STDOUT: ===> Launching kafka ... 
10:21:21,800 [docker-java-stream-1918861308] INFO  org.apache.flink.connector.kafka.sink.KafkaSinkITCase        [] - STDOUT: [2021-12-15 10:21:21,796] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
10:21:22,305 [docker-java-stream-1918861308] INFO  org.apache.flink.connector.kafka.sink.KafkaSinkITCase        [] - STDOUT: [2021-12-15 10:21:22,304] INFO starting (kafka.server.KafkaServer)
10:21:22,305 [docker-java-stream-1918861308] INFO  org.apache.flink.connector.kafka.sink.KafkaSinkITCase        [] - STDOUT: [2021-12-15 10:21:22,305] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
10:21:22,324 [docker-java-stream-1918861308] INFO  org.apache.flink.connector.kafka.sink.KafkaSinkITCase        [] - STDOUT: [2021-12-15 10:21:22,323] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
10:21:22,347 [docker-java-stream-1918861308] INFO  org.apache.flink.connector.kafka.sink.KafkaSinkITCase        [] - STDOUT: [2021-12-15 10:21:22,346] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
10:21:22,377 [docker-java-stream-1918861308] INFO  org.apache.flink.connector.kafka.sink.KafkaSinkITCase        [] - STDOUT: [2021-12-15 10:21:22,376] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
10:21:22,417 [docker-java-stream-1918861308] INFO  org.apache.flink.connector.kafka.sink.KafkaSinkITCase        [] - STDOUT: [2021-12-15 10:21:22,411] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
10:21:22,554 [docker-java-stream-1918861308] INFO  org.apache.flink.connector.kafka.sink.KafkaSinkITCase        [] - STDOUT: [2021-12-15 10:21:22,552] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)
10:21:22,570 [docker-java-stream-1918861308] INFO  org.apache.flink.connector.kafka.sink.KafkaSinkITCase        [] - STDOUT: [2021-12-15 10:21:22,570] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener)
{code}

When digging a bit deeper through the logs, it appears that Zookeeper and/or Kafka runs into network issues and therefore shuts down. I'm leaning towards network hickups on the CI machines that cause these occurrences which we can't really resolve unless we decide to really investigate what's causing network connectivity on the CI machines. ;;;","23/Dec/21 10:25;trohrmann;Hmm, doesn't all the communication happen locally because the test process and the test containers run on the same machine? Would this mean that there is some problem with the Docker setup causing network issues?;;;","23/Dec/21 11:19;martijnvisser;The test process runs as a Docker image itself, there can be 7 workers running on one CI machine. This means that when running a {{docker ps}} on a CI machine, it displays multiple running containers, all running the image {{chesnay/flink-ci:java_8_11_17}}. 

In that running container, Testcontainer would spin up its own container is how I understand it. That's described in more detail at https://www.testcontainers.org/supported_docker_environment/continuous_integration/dind_patterns/

The netwerk communication is then also only the Docker network on which the communication (and potential issues) happen is how I understand it. 

I also have a feeling that we are seeing network issues happen more with other tests that are also using Testcontainers (like Cassandra or Elasticsearch)
;;;","23/Dec/21 11:48;trohrmann;Hmm, this sounds like a serious issue to look into since we've already adopted testcontainers at various places and plan to use it even more. +1 for looking into this after the Xmas break or asap.;;;","29/Dec/21 14:44;martijnvisser;[~trohrmann] [~gaoyunhaii] I'm closing this issue because I think this has been resolved by reducing the number of workers on the CI machines.;;;","30/Dec/21 09:16;trohrmann;Have we understood what the current limitations (resource wise) are on our CI machines and what test containers we can use [~MartijnVisser]? If not, then I fear that it will only be a matter of time until we face the same issue again.;;;","30/Dec/21 09:22;martijnvisser;[~trohrmann] No we haven't, which is why I've created https://issues.apache.org/jira/browse/FLINK-25480. But I need help from the community to see how we can achieve that. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSource cannot checkpoint if the parallelism is higher than the partition number,FLINK-24347,13402446,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fpaul,fpaul,fpaul,21/Sep/21 11:17,21/Sep/21 23:15,13/Jul/23 08:12,21/Sep/21 23:14,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"The KafkaSourceEnumerator signals all the readers if there are no more splits available and the readers shut down. In case the parallelism is higher than the partitions of the consumed topic there are not enough partitions to distribute and reader subtasks go immediately into FINISHED state.
Currently, it is not possible to checkpoint if parts of the job are finished. (this is lifted once FLIP-147 is by default enabled Flink 1.15+)

We should only signal to the readers to go into finished if the job is in bounded execution mode and keep them idling otherwise.
",,fpaul,sewen,thw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 21 23:14:45 UTC 2021,,,,,,,,,,"0|z0v534:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/21 16:29;thw;Thanks for fixing this. In unbounded executions readers should stay alive because new partitions/splits can be discovered dynamically. ;;;","21/Sep/21 23:14;sewen;Fixed in
  - master via d753cf20826c66f20bfbd3772c58709e00d9bcb7
  - 1.14.0 (release-1.14) via a595fbbfe0087181f1b92d2d991a885911f290a2
  - 1.13.3 (release-1.13) via 28211c0dec1dfb706c5d9583fd757cdd4de38ce8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handling of IOExceptions when triggering checkpoints doesn't cause job failover,FLINK-24344,13402419,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,pnowojski,pnowojski,21/Sep/21 08:38,15/Dec/21 01:44,13/Jul/23 08:12,23/Sep/21 12:10,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"When running {{CheckpointCoordinatorTest#testTriggerCheckpointAfterIOException}}, it simulates an {{IOException}} thrown from {{CheckpointCoordinator#initializeCheckpoint}}. Which makes sense but:
# This exception never reaches {{CheckpointFailureManager}} because of the logic in {{CheckpointCoordinator#onTriggerFailure()}}. So the main purpose of the FLINK-23189 ticket is not working as intended.
# it would be  however much much better to throw {{IOException}} from {{checkpointStorageView.initializeLocationForCheckpoint}} rather from {{checkpointIdCounter.getAndIncrement}}. The latter could be refactored out, while the production {{IOException}} can be thrown in reality from {{initializeLocationForCheckpoint}})",,fanrui,jackwangcs,pnowojski,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23189,FLINK-24249,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 23 12:10:17 UTC 2021,,,,,,,,,,"0|z0v4x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/21 12:10;pnowojski;Merged to master as 815191a6f5e^, 815191a6f5e
Merged to release-1.14 as 464849d74bd^ and 464849d74bd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filesystem sink does not escape right bracket in partition name,FLINK-24342,13402385,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,trushev,trushev,trushev,21/Sep/21 04:31,02/Aug/22 08:50,13/Jul/23 08:12,02/Aug/22 08:50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.16.0,,,,,Connectors / FileSystem,,,,,0,auto-deprioritized-minor,pull-request-available,,,"h3. How to reproduce the problem

In the following code snippet filesystem sink creates a partition named ""\{date\}"" and writes value ""1"" to file.

{code:sql}
create table sink (
  val int,
  part string
) partitioned by (part) with (
  'connector' = 'filesystem',
  'path' = '/tmp/sink',
  'format' = 'csv'
);

insert into sink values (1, '{date}');
{code}

h3. Expected behavior

Escaped ""\{"" and ""\}"" in partition name

{code}
$ ls /tmp/sink/
part=%7Bdate%7D
{code}

h3. Actual behavior
Escaped only ""\{"" in partition name

{code}
$ ls /tmp/sink/
part=%7Bdate}
{code}
",,libenchao,martijnvisser,trushev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 02 08:50:29 UTC 2022,,,,,,,,,,"0|z0v4pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","17/Apr/22 22:38;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","18/Jul/22 03:17;trushev;Can this ticket be reviewed please;;;","02/Aug/22 08:50;martijnvisser;Fixed in master: 484d4b2542831ca7bac7c87b545496b7ce3b0fd4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Misleading exception logged finished/canceled job restarted once,FLINK-24340,13402151,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,20/Sep/21 10:38,21/Sep/21 13:37,13/Jul/23 08:12,21/Sep/21 13:37,1.13.3,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"In FLINK-24015, code was introduced to ensure exceptions are logged on the JM (Dispatcher) so we have them at least once in case they arise from the initialization.

To this end the code checks whether the failureInfo is set in the ExecutionGraph. On the face of it this makes sense, but this failureInfo is also set when a job is _restarted_, and never reset.

As a result we may print the exception that caused the last restart when the job is finished/canceled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24015,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 21 13:37:14 UTC 2021,,,,,,,,,,"0|z0v39k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/21 13:37;chesnay;master: c0769cce4fcc753c02f359f27b06b69100a0f2e0

1.14: 40b0a317a17da3f1cd0b06a421edb6a050c63911 

1.13: 82118dd297d0dbd51a3527a754662c55123d3d7d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink TableEnvironment executes the SQL randomly MalformedURLException with the configuration for 'pipeline.classpaths',FLINK-24336,13401985,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,is,nicholasjiang,nicholasjiang,18/Sep/21 08:58,15/Dec/21 01:44,13/Jul/23 08:12,24/Sep/21 01:45,1.13.2,1.14.0,1.14.1,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.3,1.15.0,,API / Python,,,,,0,pull-request-available,,,,"When I run flink client to submit a python based workflow, I got the MalformedURLException like this:

https://gist.github.com/is/faabafc7f8750f3f3161fbb6517ed6ff

After some debug work, I found the problem is related with 
TableEvneriontment.execute_sql. The root cause is  
TableEvenriontment._add_jars_to_j_env_config in pyflink/table/TableEnverionment.py.

```
if j_configuration.containsKey(config_key):
  for url in j_configuration.getString(config_key, """").split("";""):
    jar_urls_set.add(url)
```

In our case, pipeline.classpaths was set by empty list value
from FromProgramOption, so the upper code block will
introduce a empty string ("""") into pipeline.classpaths, for example
""a.jar;b.jar;;c.jar"", and it will cause the according exception.

Another problem, the order of string set in python is not
determinate, so "";"".join(jar_urls_set) does NOT keep the
classpaths order. The list is more suiteable in this case.",,dianfu,is,nicholasjiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24390,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 24 01:45:29 UTC 2021,,,,,,,,,,"0|z0v28o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Sep/21 09:08;is;i will submit a PR ASAP.;;;","18/Sep/21 09:09;dianfu;[~is] Thanks, have assigned it to you~;;;","24/Sep/21 01:45;dianfu;Merged to:
- master via ac0d22e152bbe681ec6879db551aa57a2c4d0a41
- release-1.14 via 85d4f68e24ebac891c32f0fe672da65e534ea74d
- release-1.13 via 26616d9e85e1a6b9240536e3ed50dadd17f1b880
- release-1.12 via f1886efe75f400abf8136b4371cd43a12684b7f6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configuration kubernetes.flink.log.dir not working,FLINK-24334,13401981,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ouyangwuli,wangyang0918,wangyang0918,18/Sep/21 08:27,24/Jan/22 02:39,13/Jul/23 08:12,24/Jan/22 02:39,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.4,1.15.0,,,Deployment / Kubernetes,,,,,0,pull-request-available,stale-assigned,,,"After FLINK-21128, {{kubernetes.flink.log.dir}} could not take effect.",,ouyangwuli,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 24 02:39:32 UTC 2022,,,,,,,,,,"0|z0v27s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Oct/21 09:41;ouyangwuli;[~wangyang0918] I think, Solving this problem can be divided into two steps:

   1. Check whether'kubernetes.flink.log.dir' is configured. If configured, it will be converted to '{{env.log.dir}}', but '{{env.log.dir}}' has priority.

   2. The document states that the'kubernetes.flink.log. dir' parameter is a deprecated parameter, and next version will cancelled.;;;","08/Oct/21 03:13;wangyang0918;[~ouyangwuli] I think your suggestion makes sense and I have assigned this ticket to you. Thanks for working on this ticket.;;;","05/Dec/21 10:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","24/Jan/22 02:39;wangyang0918;Fixed via:

master(1.15): 33678dacbbc91ed25efa0fc0fec85eb3869ef0cc

release-1.14: eff530f2487e8be07ec0fadbeeca0a95369c59e9

release-1.13: e778992553475812c4ba241961a1f03039abddc1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"PartiallyFinishedSourcesITCase fails with ""No downstream received 0 from xxx;""",FLINK-24331,13401951,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,xtsong,xtsong,18/Sep/21 03:16,15/Dec/21 01:44,13/Jul/23 08:12,22/Oct/21 10:26,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,API / DataStream,Runtime / Task,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24287&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10945

{code}
Sep 18 02:21:08 [ERROR] Tests run: 12, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 224.44 s <<< FAILURE! - in org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase
Sep 18 02:21:08 [ERROR] test[complex graph SINGLE_SUBTASK, failover: true, strategy: region]  Time elapsed: 28.807 s  <<< FAILURE!
Sep 18 02:21:08 java.lang.AssertionError: No downstream received 0 from 00000000000000000000000000000003[0]; received: {0=OperatorFinished 00000000000000000000000000000007/0, 1=OperatorFinished 00000000000000000000000000000007/1, 2=OperatorFinished 00000000000000000000000000000007/2, 3=OperatorFinished 00000000000000000000000000000007/3}
Sep 18 02:21:08 	at org.junit.Assert.fail(Assert.java:89)
Sep 18 02:21:08 	at org.junit.Assert.assertTrue(Assert.java:42)
Sep 18 02:21:08 	at org.apache.flink.runtime.operators.lifecycle.validation.TestJobDataFlowValidator.lambda$checkDataFlow$1(TestJobDataFlowValidator.java:96)
Sep 18 02:21:08 	at java.util.HashMap.forEach(HashMap.java:1289)
Sep 18 02:21:08 	at org.apache.flink.runtime.operators.lifecycle.validation.TestJobDataFlowValidator.checkDataFlow(TestJobDataFlowValidator.java:94)
Sep 18 02:21:08 	at org.apache.flink.runtime.operators.lifecycle.validation.TestJobDataFlowValidator.checkDataFlow(TestJobDataFlowValidator.java:62)
Sep 18 02:21:08 	at org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase.test(PartiallyFinishedSourcesITCase.java:139)
{code}",,dwysakowicz,mapohl,pnowojski,roman,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21090,,,,,,,,,,,,,,,,,,,,"11/Oct/21 09:53;mapohl;logs-ci_build-test_ci_build_finegrained_resource_management-1633890853.zip;https://issues.apache.org/jira/secure/attachment/13034781/logs-ci_build-test_ci_build_finegrained_resource_management-1633890853.zip",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 22 10:26:20 UTC 2021,,,,,,,,,,"0|z0v214:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Oct/21 10:04;trohrmann;Another instance: https://dev.azure.com/ververica-dev/daplatform-flink/_build/results?buildId=1539&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=a9a20597-291c-5240-9913-a731d46d6dd1&l=11075;;;","08/Oct/21 10:05;trohrmann;cc [~roman], [~pnowojski];;;","11/Oct/21 09:51;mapohl;We're experiencing this issue also on the VVP Flink 1.14 fork: https://dev.azure.com/ververica-dev/daplatform-flink/_build/results?buildId=1566&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=a9a20597-291c-5240-9913-a731d46d6dd1&l=11085

I'm going to add the affected version accordingly and add the build's artifact for investigation.;;;","22/Oct/21 10:26;dwysakowicz;Fixed in:
* master
** f070f0d9ae19275b1e32c2587209d7d8ae87b57e
* 1.14.1
** 471d682eedabb06fe795adf2dc583ce0e6081ec0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Casting a number to boolean has different results between 'select' fields and 'where' condition,FLINK-24318,13401810,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,xuyangzhong,xuyangzhong,xuyangzhong,17/Sep/21 08:20,12/Aug/22 09:26,13/Jul/23 08:12,20/Oct/21 09:33,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"The same cast in the following two sql:
{code:java}
// SQL 1
SELECT cast(0.1 as boolean)

// SQL 2
SELECT * from test2 where cast(0.1 as boolean)
{code}
has different results.

The cast result in SQL 1 is true and the cast in SQL 2 is false.",,catyee,godfreyhe,jark,libenchao,xuyangzhong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 12 09:26:46 UTC 2022,,,,,,,,,,"0|z0v15s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/21 09:33;godfreyhe;Fixed in 1.15.0: 32f7cc9e34be67eaf1b746697f2fabefcd5f46c5
Fixed in 1.14.1: 309dc0479172b979ee3c951893a04304b5416a08
Fixed in 1.13.4: 1d92c9e36ff87240a49d32d7a50c11e95251c3db;;;","12/Aug/22 09:26;catyee;In 1.13.4, correct commit is: ea53e4a6cd1d39592cf3a6ae911fed332234d00e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot rebuild watcher thread while the K8S API server is unavailable,FLINK-24315,13401783,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,ouyangwuli,ouyangwuli,17/Sep/21 06:04,15/Dec/21 01:44,13/Jul/23 08:12,23/Sep/21 13:28,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.3,1.15.0,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,"In native k8s integration, Flink will try to rebuild the watcher thread if the API server is temporarily unavailable. However, if the jitter is longer than the web socket timeout, the rebuilding of the watcher will timeout and Flink cannot handle the pod event correctly.
",,guoyangze,ouyangwuli,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 23 13:28:19 UTC 2021,,,,,,,,,,"0|z0v0zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Sep/21 06:23;guoyangze;[~wangyang0918] I think we can add a retry logic in building the watcher and throw a fatal error if the watcher cannot be rebuilt. Could you assign this to me?;;;","17/Sep/21 06:54;ouyangwuli;[~guoyangze]  Can we configure parameters ‘kubernetes.watch.reconnectInterval’ and ‘kubernetes.watch.reconnectLimit’ to fix this problem ？;;;","17/Sep/21 07:15;guoyangze;Thanks for the information, I think we can leverage ‘kubernetes.watch.reconnectInterval’ and ‘kubernetes.watch.reconnectLimit’ to implement the retry logic. However, the key point of this issue is that we need to handle the failure of rebuilding the watcher.;;;","18/Sep/21 06:31;wangyang0918;I think we need to retry in the {{FlinkKubeClient#watchPodsAndDoCallback}}.;;;","23/Sep/21 13:28;wangyang0918;Fixed via

master: 36ff71f5ff63a140acc634dd1d98b2bb47a76ba5

release-1.14: a179cd1948fb08053670447e3be0d9e3783cf1d2

release-1.13: 901f834f57d705dac6302a706a5d612f4507556a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A bug in the BufferingSink example in the doc,FLINK-24310,13401611,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qinjunjerry,qinjunjerry,qinjunjerry,16/Sep/21 08:54,15/Dec/21 01:44,13/Jul/23 08:12,29/Nov/21 11:04,1.13.3,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Documentation,,,,,0,pull-request-available,stale-assigned,,,"The following line in the BufferingSink on [this page|https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/fault-tolerance/state/#operator-state] has a bug:
{code:java}
if (bufferedElements.size() == threshold) {{code}
It should be {{>=}} instead of {{==}} , because when restoring from a checkpoint during downscaling, the task may get more elements than the threshold. ",,fpaul,qinjunjerry,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 29 11:04:20 UTC 2021,,,,,,,,,,"0|z0uzxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/21 12:04;fpaul;Fix on master with e40b5c83d43a041b48839008cd6b5fa4b24b4132 and 1.13 fec8431385da4a4d291ab12a08cd553f8fa88a2c;;;","29/Nov/21 10:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","29/Nov/21 11:04;fpaul;Fixed 1.14: da39169cd8337b1a667ab07c31a592d727f56df7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some lines in Scala code example end with unnecessary semicolons. Also some Scala code blocks are wrongly typed with Java blocks,FLINK-24307,13401542,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,paul8263,paul8263,paul8263,16/Sep/21 06:41,18/Feb/22 07:20,13/Jul/23 08:12,18/Feb/22 07:20,1.13.2,shaded-14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Documentation,,,,,0,pull-request-available,stale-assigned,,,"In Flink docs, some sample of Scala codes end with semicolons.

Also in some of the markdown files, several the Scala code block types are Java. They should be Scala.

!image-2021-09-16-14-39-05-065.png!",,paul8263,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/21 06:39;paul8263;image-2021-09-16-14-39-05-065.png;https://issues.apache.org/jira/secure/attachment/13033650/image-2021-09-16-14-39-05-065.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 18 07:20:00 UTC 2022,,,,,,,,,,"0|z0uzi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/21 06:41;paul8263;Could anyone assign this issue to me pls?;;;","08/Nov/21 10:43;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","08/Feb/22 06:06;paul8263;Hi [~twalthr] ,

Could you please review this issue? Thanks.;;;","18/Feb/22 07:20;chesnay;master: e74ed86c30cb475a24ecd367ddfeefc01208fb8e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BatchPandasUDAFITTests.test_over_window_aggregate_function fails on azure,FLINK-24305,13401492,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hxbks2ks,xtsong,xtsong,16/Sep/21 02:25,15/Dec/21 01:40,13/Jul/23 08:12,16/Sep/21 07:46,1.12.5,1.13.2,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.0,1.15.0,,API / Python,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24170&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=23011

{code}
Sep 15 20:40:43 cls = <class 'pyflink.table.tests.test_pandas_udaf.BatchPandasUDAFITTests'>
Sep 15 20:40:43 actual = JavaObject id=o8666
Sep 15 20:40:43 expected = ['+I[1, 4.3333335, 13, 5.5, 3.0, 3.0, 4.3333335, 8.0, 5.0, 5.0]', '+I[1, 4.3333335, 5, 4.3333335, 3.0, 3.0, 2.5, 4.333....0, 4.0, 2.0]', '+I[2, 2.0, 9, 2.0, 4.0, 4.0, 2.0, 2.0, 4.0, 4.0]', '+I[3, 2.0, 3, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0]']
Sep 15 20:40:43 
Sep 15 20:40:43     @classmethod
Sep 15 20:40:43     def assert_equals(cls, actual, expected):
Sep 15 20:40:43         if isinstance(actual, JavaObject):
Sep 15 20:40:43             actual_py_list = cls.to_py_list(actual)
Sep 15 20:40:43         else:
Sep 15 20:40:43             actual_py_list = actual
Sep 15 20:40:43         actual_py_list.sort()
Sep 15 20:40:43         expected.sort()
Sep 15 20:40:43         assert len(actual_py_list) == len(expected)
Sep 15 20:40:43 >       assert all(x == y for x, y in zip(actual_py_list, expected))
Sep 15 20:40:43 E       AssertionError: assert False
Sep 15 20:40:43 E        +  where False = all(<generator object PyFlinkTestCase.assert_equals.<locals>.<genexpr> at 0x7f792d98b900>)
{code}",,dianfu,dwysakowicz,hxbks2ks,jingzhang,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 16 07:52:08 UTC 2021,,,,,,,,,,"0|z0uz74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/21 02:35;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24171&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=26255

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24171&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=25944

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24171&view=logs&j=bdd9ea51-4de2-506a-d4d9-f3930e4d2355&t=dd50312f-73b5-56b5-c172-4d81d03e2ef1&l=26108

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24171&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=26510

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24171&view=logs&j=ff2e2ea5-07e3-5521-7b04-a4fc3ad765e9&t=1ec6382b-bafe-5817-63ae-eda7d4be718e&l=25486

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24171&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=24576

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24171&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=22622;;;","16/Sep/21 02:37;xtsong;cc [~dian.fu] [~hxbks2ks];;;","16/Sep/21 02:45;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24172&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3&l=20571

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24172&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=22748

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24172&view=logs&j=bdd9ea51-4de2-506a-d4d9-f3930e4d2355&t=98717c4f-b888-5636-bb1e-db7aca25755e&l=21566

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24172&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=f5211ead-5e53-5af8-f827-4dbf08df26bb&l=21659

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24172&view=logs&j=ff2e2ea5-07e3-5521-7b04-a4fc3ad765e9&t=613f538c-bcef-59e6-f9cd-9714bec9fb97&l=22617

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24172&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=d59eb898-29f7-5a99-91a7-b2dfc3e8a653&l=21726;;;","16/Sep/21 03:54;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24173&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3&l=22090

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24173&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=23451

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24173&view=logs&j=bdd9ea51-4de2-506a-d4d9-f3930e4d2355&t=98717c4f-b888-5636-bb1e-db7aca25755e&l=23287

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24173&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=f5211ead-5e53-5af8-f827-4dbf08df26bb&l=21959

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24173&view=logs&j=ff2e2ea5-07e3-5521-7b04-a4fc3ad765e9&t=613f538c-bcef-59e6-f9cd-9714bec9fb97&l=24718

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24173&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=d59eb898-29f7-5a99-91a7-b2dfc3e8a653&l=22094

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24173&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=45a89cfc-9ff2-5909-6443-6c732efcf06b&l=23426;;;","16/Sep/21 05:21;dianfu;Thanks [~xtsong], [~hxbks2ks] is currently looking into this issue. We suspect that it's caused by protobuf 3.18.0 which was released today. We are still confirming that. If this is true, we need to limit the version of protobuf.;;;","16/Sep/21 05:25;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24174&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=22918

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24174&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=25895

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24174&view=logs&j=bdd9ea51-4de2-506a-d4d9-f3930e4d2355&t=dd50312f-73b5-56b5-c172-4d81d03e2ef1&l=26081

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24174&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=24835

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24174&view=logs&j=ff2e2ea5-07e3-5521-7b04-a4fc3ad765e9&t=1ec6382b-bafe-5817-63ae-eda7d4be718e&l=23411

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24174&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=24234

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24174&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=24259;;;","16/Sep/21 05:26;xtsong;Thanks for the info, [~dianfu].

I've labeled this as a 1.14 release blocker for now. Feel free to update the ticket if the conclusion is otherwise.;;;","16/Sep/21 07:05;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24178&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3;;;","16/Sep/21 07:46;hxbks2ks;Merged into master via 7487c309039da1f9d8b75bbe077cc5fab972972e
Merged into release-1.14 via 4ebd0579f002790e2d7436ca6c3afd7f36edbf3d
Merged into release-1.13 via 050f025f5857bce608545c9bf5148ab01511346c
Merged into release-1.12 via 66ddb98430af4b26b8ff9245f1e7a0081b6731b2;;;","16/Sep/21 07:49;xtsong;Thanks for the fix, [~hxbks2ks].
A quick question: do you think it's necessary to cancel 1.14.0-rc1 due to this?;;;","16/Sep/21 07:52;hxbks2ks;[~xtsong] Yes. I think it is necessary to cancel 1.14.0-rc1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceCoordinator exception may fail Session Cluster,FLINK-24303,13401464,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sewen,sjwiesman,sjwiesman,15/Sep/21 20:34,23/Sep/21 09:55,13/Jul/23 08:12,21/Sep/21 10:15,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Connectors / Common,,,,,0,pull-request-available,,,,"The SourceCoordinator currently forwards all exceptions from `Source#createEnumerator` up the stack triggering a JobMaster failover. However, JobMaster failover only works if HA is enabled[1]. If HA is not enabled the fatal error handler will simply exit the JM process killing the entire cluster. This is problematic in the case of a session cluster where there may be multiple jobs running. It also does not play well with external tooling that does not expect job failure to cause a full cluster failure. 

 

It would be preferable if failure to create an enumerator did not take down the entire cluster, but instead failed that particular job. 

 

[1] [https://github.com/apache/flink/blob/7f69331294ab2ab73f77b40a4320cdda53246afe/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L898-L903]",,airblader,fpaul,knaufk,leonard,libenchao,martijnvisser,qinjunjerry,sewen,sjwiesman,stevenz3wu,Thesharing,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24363,,,,,,,,FLINK-24343,,,,,,,FLINK-24343,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 22 06:38:11 UTC 2021,,,,,,,,,,"0|z0uz0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/21 11:01;sewen;The issue is relatively easy to fix on the level of the {{SourceCoordinator}}.

Why not fix it for all {{OperatorCoordinators}} on the generic level? Meaning, trigger failover when the start method of {{OperatorCoordinator}} produces an exception? I was looking to do that initially, but the scheduler is not prepared to handle global failures in cases where the starting procedure wasn't completed.

I investigated whether re-instantiating the source enumerator when restoring from a checkpoint has the same problem. It behaves already correctly, resulting in a global failover rather than a JM crash. I am adding a test to guard this behavior.;;;","20/Sep/21 11:48;sewen;In the longer term, I would like to change the Source Coordinator code such that all enumerator creation and restore happens actually in the enumerator thread. That makes the failure handling cleaner (no exceptions can happen during the startup phase) and also solves the issue what we move expensive enumerator initialization out of the JobManager startup phase.;;;","21/Sep/21 10:15;sewen;Fixed in
  - master (1.15.0) via d18d1f4b7acd683dfdfc8a5218892c17b42b6701
  - release-1.14 (1.14.1) via 36c1b5bd76140aa6c54e4da0af97d69c57397072
  - release-1.13 (1.13.2) via 600cde46a72bd78ac3aefffde7ae936e57624131;;;","21/Sep/21 10:16;sewen;This issue here tracks a potentially different long-term approach: https://issues.apache.org/jira/browse/FLINK-24343;;;","22/Sep/21 06:38;airblader;Since this canceled the 1.14.0 RC, shouldn't the fix version be 1.14.0 instead of 1.14.1?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MultipleInputOperator is running much more slowly in TPCDS,FLINK-24300,13401421,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,Thesharing,Thesharing,15/Sep/21 15:22,26/Jan/22 14:00,13/Jul/23 08:12,17/Sep/21 19:21,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,"When we are running TPCDS with release 1.14 we find that the job with {{MultipleInputOperator}} is running much more slowly than before. With a binary search among the commits, we find that the issue may be introduced by FLINK-23408. 

At the commit 64570e4c56955713ca599fd1d7ae7be891a314c6, the job in TPCDS runs normally, as the image below illustrates:

!64570e4c56955713ca599fd1d7ae7be891a314c6.png|width=600!

At the commit e3010c16947ed8da2ecb7d89a3aa08dacecc524a, the job q2.sql gets stuck for a pretty long time (longer than half an hour), as the image below illustrates:

!e3010c16947ed8da2ecb7d89a3aa08dacecc524a.png|width=600!

The detail of the job is illustrated below:

!detail-of-the-job.png|width=600!

The job uses a {{MultipleInputOperator}} with one normal input and two chained FileSource. It has finished reading the normal input and start to read the chained source. Each chained source has one source data fetcher.

We capture the jstack of the stuck tasks and attach the file below. From the [^jstack.txt] we can see the main thread is blocked on waiting for the lock, and the lock is held by a source data fetcher. The source data fetcher is still running but the stack keeps on {{CompletableFuture.cleanStack}}.

This issue happens in a batch job. However, from where it get blocked, it seems also affects the streaming jobs.

For the reference, the code of TPCDS we are running is located at [https://github.com/ververica/flink-sql-benchmark/tree/dev].",,dwysakowicz,gaoyunhaii,guoyangze,libenchao,okowr,pnowojski,Thesharing,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23408,,,,,,,,FLINK-25728,FLINK-25827,,,,,,,,,,,"15/Sep/21 14:59;Thesharing;64570e4c56955713ca599fd1d7ae7be891a314c6.png;https://issues.apache.org/jira/secure/attachment/13033595/64570e4c56955713ca599fd1d7ae7be891a314c6.png","15/Sep/21 15:10;Thesharing;detail-of-the-job.png;https://issues.apache.org/jira/secure/attachment/13033593/detail-of-the-job.png","15/Sep/21 15:02;Thesharing;e3010c16947ed8da2ecb7d89a3aa08dacecc524a.png;https://issues.apache.org/jira/secure/attachment/13033594/e3010c16947ed8da2ecb7d89a3aa08dacecc524a.png","15/Sep/21 16:25;gaoyunhaii;jstack-2.txt;https://issues.apache.org/jira/secure/attachment/13033603/jstack-2.txt","15/Sep/21 15:16;Thesharing;jstack.txt;https://issues.apache.org/jira/secure/attachment/13033592/jstack.txt",,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 17 19:21:16 UTC 2021,,,,,,,,,,"0|z0uyrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/21 15:32;gaoyunhaii;From the attached _jstack-2.txt_ (see this one since jstack.txt is running with alibaba jdk, which has some slight difference in line numbers), the stack of a single task is like
{code:java}
""Source Data Fetcher for MultipleInput(readOrder=[0,1,1], members=[\nHashJoin(joinType=[InnerJoin], where=[(d_date_sk = ws_sold_date_sk)], select=[ws_sold_date_sk, ws_ext_sales_price, d_date_sk, d_week_seq, d_day_name], isBroadcast=[true], build=[right])\n:- Union(all=[true], union=[ws_sold_date_sk, ws_ext_sales_price])\n:  :- [#2] TableSourceScan(table=[[hive, tpcds_bin_orc_10000, web_sales, project=[ws_sold_date_sk, ws_ext_sales_price]]], fields=[ws_sold_date_sk, ws_ext_sales_price])\n:  +- [#3] TableSourceScan(table=[[hive, tpcds_bin_orc_10000, catalog_sales, project=[cs_sold_date_sk, cs_ext_sales_price]]], fields=[cs_sold_date_sk, cs_ext_sales_price])\n+- [#1] Exchange(distribution=[broadcast])\n]) [Source: HiveSource-tpcds_bin_orc_10000.web_sales, Source: HiveSource-tpcds_bin_orc_10000.catalog_sales] -> Calc(select=[d_week_seq, CASE((d_day_name = _UTF-16LE'Sunday':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), ws_ext_sales_price, null:DOUBLE) AS $f1, CASE((d_day_name = _UTF-16LE'Monday':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), ws_ext_sales_price, null:DOUBLE) AS $f2, CASE((d_day_name = _UTF-16LE'Tuesday':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), ws_ext_sales_price, null:DOUBLE) AS $f3, CASE((d_day_name = _UTF-16LE'Wednesday':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), ws_ext_sales_price, null:DOUBLE) AS $f4, CASE((d_day_name = _UTF-16LE'Thursday':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), ws_ext_sales_price, null:DOUBLE) AS $f5, CASE((d_day_name = _UTF-16LE'Friday':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), ws_ext_sales_price, null:DOUBLE) AS $f6, CASE((d_day_name = _UTF-16LE'Saturday':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), ws_ext_sales_price, null:DOUBLE) AS $f7]) -> LocalHashAggregate(groupBy=[d_week_seq], select=[d_week_seq, Partial_SUM($f1) AS sum$0, Partial_SUM($f2) AS sum$1, Partial_SUM($f3) AS sum$2, Partial_SUM($f4) AS sum$3, Partial_SUM($f5) AS sum$4, Partial_SUM($f6) AS sum$5, Partial_SUM($f7) AS sum$6]) (548/1050)#0"" #217 prio=5 os_prio=0 tid=0x00007fcda518f000 nid=0x1632d runnable [0x00007fcc9e0ed000]
   java.lang.Thread.State: RUNNABLE
	at java.util.concurrent.CompletableFuture.cleanStack(CompletableFuture.java:497)
	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:567)
	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:1068)
	at java.util.concurrent.CompletableFuture$OrRelay.tryFire(CompletableFuture.java:1549)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.connector.base.source.reader.synchronization.FutureCompletingBlockingQueue.moveToAvailable(FutureCompletingBlockingQueue.java:173)
	at org.apache.flink.connector.base.source.reader.synchronization.FutureCompletingBlockingQueue.enqueue(FutureCompletingBlockingQueue.java:347)
	at org.apache.flink.connector.base.source.reader.synchronization.FutureCompletingBlockingQueue.put(FutureCompletingBlockingQueue.java:211)
--
""Source Data Fetcher for MultipleInput(readOrder=[0,1,1], members=[\nHashJoin(joinType=[InnerJoin], where=[(d_date_sk = ws_sold_date_sk)], select=[ws_sold_date_sk, ws_ext_sales_price, d_date_sk, d_week_seq, d_day_name], isBroadcast=[true], build=[right])\n:- Union(all=[true], union=[ws_sold_date_sk, ws_ext_sales_price])\n:  :- [#2] TableSourceScan(table=[[hive, tpcds_bin_orc_10000, web_sales, project=[ws_sold_date_sk, ws_ext_sales_price]]], fields=[ws_sold_date_sk, ws_ext_sales_price])\n:  +- [#3] TableSourceScan(table=[[hive, tpcds_bin_orc_10000, catalog_sales, project=[cs_sold_date_sk, cs_ext_sales_price]]], fields=[cs_sold_date_sk, cs_ext_sales_price])\n+- [#1] Exchange(distribution=[broadcast])\n]) [Source: HiveSource-tpcds_bin_orc_10000.web_sales, Source: HiveSource-tpcds_bin_orc_10000.catalog_sales] -> Calc(select=[d_week_seq, CASE((d_day_name = _UTF-16LE'Sunday':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), ws_ext_sales_price, null:DOUBLE) AS $f1, CASE((d_day_name = _UTF-16LE'Monday':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), ws_ext_sales_price, null:DOUBLE) AS $f2, CASE((d_day_name = _UTF-16LE'Tuesday':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), ws_ext_sales_price, null:DOUBLE) AS $f3, CASE((d_day_name = _UTF-16LE'Wednesday':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), ws_ext_sales_price, null:DOUBLE) AS $f4, CASE((d_day_name = _UTF-16LE'Thursday':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), ws_ext_sales_price, null:DOUBLE) AS $f5, CASE((d_day_name = _UTF-16LE'Friday':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), ws_ext_sales_price, null:DOUBLE) AS $f6, CASE((d_day_name = _UTF-16LE'Saturday':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), ws_ext_sales_price, null:DOUBLE) AS $f7]) -> LocalHashAggregate(groupBy=[d_week_seq], select=[d_week_seq, Partial_SUM($f1) AS sum$0, Partial_SUM($f2) AS sum$1, Partial_SUM($f3) AS sum$2, Partial_SUM($f4) AS sum$3, Partial_SUM($f5) AS sum$4, Partial_SUM($f6) AS sum$5, Partial_SUM($f7) AS sum$6]) (548/1050)#0"" #216 prio=5 os_prio=0 tid=0x00007fcda51be000 nid=0x16327 waiting on condition [0x00007fcc9e1ee000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000005067ffcf0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)
	at org.apache.flink.connector.file.src.util.Pool.pollEntry(Pool.java:82)
	at org.apache.flink.orc.AbstractOrcFileInputFormat$OrcVectorizedReader.getCachedEntry(AbstractOrcFileInputFormat.java:292)
	at org.apache.flink.orc.AbstractOrcFileInputFormat$OrcVectorizedReader.readBatch(AbstractOrcFileInputFormat.java:256)
	at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:67)
--
""MultipleInput(readOrder=[0,1,1], members=[\nHashJoin(joinType=[InnerJoin], where=[(d_date_sk = ws_sold_date_sk)], select=[ws_sold_date_sk, ws_ext_sales_price, d_date_sk, d_week_seq, d_day_name], isBroadcast=[true], build=[right])\n:- Union(all=[true], union=[ws_sold_date_sk, ws_ext_sales_price])\n:  :- [#2] TableSourceScan(table=[[hive, tpcds_bin_orc_10000, web_sales, project=[ws_sold_date_sk, ws_ext_sales_price]]], fields=[ws_sold_date_sk, ws_ext_sales_price])\n:  +- [#3] TableSourceScan(table=[[hive, tpcds_bin_orc_10000, catalog_sales, project=[cs_sold_date_sk, cs_ext_sales_price]]], fields=[cs_sold_date_sk, cs_ext_sales_price])\n+- [#1] Exchange(distribution=[broadcast])\n]) [Source: HiveSource-tpcds_bin_orc_10000.web_sales, Source: HiveSource-tpcds_bin_orc_10000.catalog_sales] -> Calc(select=[d_week_seq, CASE((d_day_name = _UTF-16LE'Sunday':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), ws_ext_sales_price, null:DOUBLE) AS $f1, CASE((d_day_name = _UTF-16LE'Monday':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), ws_ext_sales_price, null:DOUBLE) AS $f2, CASE((d_day_name = _UTF-16LE'Tuesday':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), ws_ext_sales_price, null:DOUBLE) AS $f3, CASE((d_day_name = _UTF-16LE'Wednesday':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), ws_ext_sales_price, null:DOUBLE) AS $f4, CASE((d_day_name = _UTF-16LE'Thursday':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), ws_ext_sales_price, null:DOUBLE) AS $f5, CASE((d_day_name = _UTF-16LE'Friday':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), ws_ext_sales_price, null:DOUBLE) AS $f6, CASE((d_day_name = _UTF-16LE'Saturday':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), ws_ext_sales_price, null:DOUBLE) AS $f7]) -> LocalHashAggregate(groupBy=[d_week_seq], select=[d_week_seq, Partial_SUM($f1) AS sum$0, Partial_SUM($f2) AS sum$1, Partial_SUM($f3) AS sum$2, Partial_SUM($f4) AS sum$3, Partial_SUM($f5) AS sum$4, Partial_SUM($f6) AS sum$5, Partial_SUM($f7) AS sum$6]) (548/1050)#0"" #200 prio=5 os_prio=0 tid=0x00007fcda63ee800 nid=0x1624d waiting on condition [0x00007fcc9edfa000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000004fd5fce68> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
	at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209)
	at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285)
	at org.apache.flink.connector.base.source.reader.synchronization.FutureCompletingBlockingQueue.poll(FutureCompletingBlockingQueue.java:257)
{code};;;","16/Sep/21 05:32;xtsong;I'm upgrading this to blocker for now. We can downgrade it if the conclusion turns out to be otherwise.;;;","16/Sep/21 07:46;gaoyunhaii;With some offline discussion with  [~pnowojski] and [~dwysakowicz] and some more verification, the cause of this issue should be:

1. For multi-input operator with input selection, it might call _StreamMultipleInputProcess#fullCheckAndSetAvailable_ for each record when input selection is enabled, which future calls _SourceOperator#getAvailableFuture()_.
2. In the current implementation each call to _SourceOperator#getAvailableFuture()_ would add a new stage to the source reader's available future via _CompletableFuture.anyOf(sourceReaderAvailable, forceStop)_.
3. Therefore, if an input get available for a while, the available future might attach too many stages. In the above case it might chained with more than 200000 stages. When complete it, it need to complete all these stages. Since completing each stage involves some operation to the volatile fields, it would takes a long time to complete all of them. ;;;","17/Sep/21 19:21;dwysakowicz;Fixed in:
* master
** 7c8fefae4a782297e46f0b883fb2152791b79cf0
* 1.14
** 791c1b9c0816a0a990c2e705b0cafc474fef97ca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resources leak in the StreamTask constructor,FLINK-24294,13401346,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,akalashnikov,akalashnikov,15/Sep/21 09:57,25/Nov/21 08:01,13/Jul/23 08:12,25/Nov/21 08:01,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / Task,,,,,0,pull-request-available,,,,"Since we are initializing a lot of resources in the StreamTask constructor like RecordWriter, timerServices, etc. it is possible that some of these resources remain open if the exception happens below the initialization in the same constructor.
So in my opinion, we have two choices here: 
* Avoiding allocation of resources in the constructor which allows us to do something like:
{noformat}
StreamTask task = new StreamTask(); //no leaks if it fails
try { 
  task.init();
  ....
} finally {
  task.cleanUp();
}
{noformat}
*  or we can rewrite a code in such a way that exception in any constructor(ex. StreamTask) guarantee releasing the earlier allocated resources in this constructor. But it is not so easy to implement(see. initialization of recordWriter in StreamTask constructor)

So perhaps it makes sense to separate creating object from initialization(allocation resources)",,akalashnikov,dwysakowicz,Feifan Wang,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 25 08:01:31 UTC 2021,,,,,,,,,,"0|z0uyao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/21 08:53;pnowojski;Aren't we already doing exactly [the first option mentioned by you|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java#L914:L946]? ;;;","18/Oct/21 09:17;akalashnikov;[~pnowojski] , Not exactly. We had two problems:
 * Possible resource leak if the exception happens after the StreamTask was created.
 * Possible resource leak if the exception happens inside of the constructor of the StreamTask after some resources were allocated

The first one is the more possible problem which has been reported a couple of times already and the fix for it was kind of straightforward. It is exactly what I fixed [here|[https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java#L914:L946].]

The second one is more tricky to resolve. I wanted to fix it at the same PR but after discussion, we decided to split that task into two because this problem requires a more complex solution and it is not so urgent as the first one. So this is ticket exactly about the problem inside of the constructor.;;;","18/Oct/21 10:43;pnowojski;Ok, thanks for the explanation. Can you also elaborate what do you mean by:
{quote}
But it is not so easy to implement(see. initialization of recordWriter in StreamTask constructor)
{quote}
about releasing resources if a constructor throws an exception? Why is not to easy to implement?;;;","21/Oct/21 09:22;akalashnikov;It is not easy because the easiest way to do so is to surround all resources allocation with try-catch block and if something goes wrong just close them before leaving the method. But we don't really know where exactly we failed so we need to check every resource at least for null and only then close it which lead to a high number of null-check condition and also it is not clear how to guarantee that the newly added resources will be handled at the same way. 
We also can surround every resource with try-catch block but it is even more tricky because for example  RecordWriters are collected to the list and if something happens when the list isn't empty we should close all RecordWriters correctly.

In conclusion, it is not a difficult problem but it is something that we should think about how to do it correctly.;;;","21/Oct/21 09:45;chesnay;Isn't this something where the guava Closer can help? You can have a single try-catch block, each resource being created is added to the closer, on exception you close the closer which takes care of everything.;;;","25/Oct/21 12:59;pnowojski;I also had something like guava's {{Closer}} or our internal {{org.apache.flink.core.fs.CloseableRegistry}} in mind while asking my previous question.;;;","25/Nov/21 08:01;dwysakowicz;Fixed in:
* master
** 0be100bca749a8bf20dc1690632480aa93e8a5e8..249683b25655e1c8c17f3ac01e7739e558df1f9b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decimal precision is lost when deserializing in test cases,FLINK-24291,13401284,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,xuyangzhong,xuyangzhong,xuyangzhong,15/Sep/21 07:45,15/Dec/21 01:44,13/Jul/23 08:12,09/Oct/21 13:12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.3,1.15.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"When added the test case following into FileSystemItCaseBase:
{code:java}
// create table
tableEnv.executeSql(
  s""""""
     |create table test2 (
     |  c0 decimal(10,0), c1 int
     |) with (
     |  'connector' = 'filesystem',
     |  'path' = '/Users/zhongxuyang/test/test',
     |  'format' = 'testcsv'
     |)
   """""".stripMargin
)

//test file content is:
//2113554011,1
//2113554022,2
{code}
and
{code:java}
// select sql
@Test
def myTest2(): Unit={
  check(
    ""SELECT c0 FROM test2"",
    Seq(
      row(2113554011),
      row(2113554022)
    ))
}
{code}
i got an exception :

{code}
java.lang.RuntimeException: Failed to fetch next resultjava.lang.RuntimeException: Failed to fetch next result
 at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109) at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80) at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:370) at java.util.Iterator.forEachRemaining(Iterator.java:115) at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:109) at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:300) at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:140) at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:106) at org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase.check(BatchFileSystemITCaseBase.scala:46) at org.apache.flink.table.planner.runtime.FileSystemITCaseBase$class.myTest2(FileSystemITCaseBase.scala:128) at org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase.myTest2(BatchFileSystemITCaseBase.scala:33) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)

 

Caused by: java.io.IOException: Failed to fetch job execution result at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184) at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121) at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106) ... 43 more

 

Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928) at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182) ... 45 more

 

Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144) at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137) at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616) at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:250) at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389) at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93) at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92) at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47) at akka.dispatch.OnComplete.internal(Future.scala:300) at akka.dispatch.OnComplete.internal(Future.scala:297) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224) at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65) at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68) at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284) at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284) at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284) at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621) at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24) at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23) at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532) at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29) at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63) at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12) at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81) at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48) at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1067) at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1703) at java.util.concurrent.For
{code}

 This is cause by the unexpected error in _AbstractBinaryWriter.writeDecimal.assert._ When i debug here, i found that the expected value in file 2113554011 will be read to 2113554011.000000000000000000, the precision of which is 38  and cases the following judge is false. (value.precision() is 38 and precision is 10)
{code:java}
assert value == null || (value.precision() == precision)
{code}
 I think this is because when the source reads values from fileSystem, it will treat the decimal  as BigDecimal, and doesn't convert it to the precision we wished. Maybe it's a bug.

 

 

 ",,godfreyhe,jark,libenchao,xuyangzhong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 09 13:12:34 UTC 2021,,,,,,,,,,"0|z0uxx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/21 13:12;godfreyhe;Fixed in 1.15.0: 4d69f7f844725534b66e8cc9f5e64d6d10226055
Fixed in 1.14.1: 3e0c2a12d2d270162c3d8a4a77451020926293a0
Fixed in 1.13.3: 87a3e2e227b0acf741f93dd14133f0e7ba6dbfef;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar connector won't use given hash ranges in Key_Shared mode,FLINK-24283,13401029,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,syhily,syhily,syhily,14/Sep/21 14:01,14/Sep/21 19:54,13/Jul/23 08:12,14/Sep/21 19:54,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Connectors / Pulsar,,,,,0,pull-request-available,,,,"Pulsar broker will keep the old consumer select if the consumer has been closed. This would lead to the sticky key range won't take effect.

We should use a sticky hash range when seeking the initial position in the source enumerator.

https://github.com/apache/pulsar/pull/12035",,dwysakowicz,syhily,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 14 19:54:05 UTC 2021,,,,,,,,,,"0|z0uwcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/21 14:32;syhily;[~dwysakowicz] Can you assign this issue to me.;;;","14/Sep/21 14:45;dwysakowicz;Sure;;;","14/Sep/21 15:10;syhily;[~dwysakowicz] Can you review the PR for this issue. :D;;;","14/Sep/21 19:54;dwysakowicz;Fixed in:
* master
** 724fb3d8701a31349a1f65dee496d0a62916d0ed
* 1.14
** 5f2ec45efc319e8ae713415e7c443b76c1bdf6a3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaRecordSerializationSchema TopicSelector is not serializable,FLINK-24282,13401028,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fpaul,fpaul,fpaul,14/Sep/21 14:01,14/Sep/21 19:54,13/Jul/23 08:12,14/Sep/21 19:54,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,"To dynamically calculate the outgoing topic we allow passing a lambda. Unfortunately, it is currently not marked as serializable hence the following code fails in during closure cleaning when used within a job.
 
{code:java}
KafkaRecordSerializationSchema.builder()
        .setTopic(topic)
        .setValueSerializationSchema(serSchema)
        .setPartitioner(partitioner)
        .build())
{code}

 ",,dwysakowicz,fpaul,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 14 19:54:56 UTC 2021,,,,,,,,,,"0|z0uwc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/21 19:54;dwysakowicz;Fixed in:
* master
** 2ae710671442e382888b615c65f8aed5781bfaa4
* 1.14
** 8c1033d9dfc3133891ee543f4c6b4ea568789ab0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Offset commit should be disabled if consumer group ID is not specified in KafkaSource,FLINK-24277,13400932,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,renqs,renqs,renqs,14/Sep/21 06:57,15/Dec/21 01:40,13/Jul/23 08:12,18/Sep/21 13:54,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.0,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"FLINK-24051 made ""group.id"" an optional configuration in KafkaSource. However, KafkaSource will generate a random group id if user doesn't specify one, and this random ID is inconsistent after failover, and not even human readable.

A solution will be adding a configuration for offset commit on checkpoint, make it as true by default, and disable offset commit if group id is not specified. ",,becket_qin,libenchao,renqs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Sep 18 13:54:00 UTC 2021,,,,,,,,,,"0|z0uvqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Sep/21 13:54;becket_qin;Fixed in the following branches:

Master: 2da73edba95685537040305f30ee9d6dfd8d6c02
release-1.14: cc19997d6124e0b4f8c905601a3c98b328014f1d
release-1.13: c4c91ec579d6aaa377e1c1979ac3cb09b345c531
release-1.12: 944138eef08b8d1bb1573770865209a89e1a58bc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink Table API example from docs does not work.,FLINK-24267,13400720,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dianfu,xtsong,xtsong,13/Sep/21 06:37,14/Sep/21 06:37,13/Jul/23 08:12,14/Sep/21 06:37,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,API / Python,Documentation,,,,0,pull-request-available,,,,"I followed the following documentation to build and install pyflink from sources.
https://ci.apache.org/projects/flink/flink-docs-master/docs/flinkdev/building/#build-pyflink

Then I tried the WordCount.py example from the following documentation.
https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/python/table_api_tutorial/

This results in the following error.
{code}
$ python WordCount.py
Traceback (most recent call last):
  File ""WordCount.py"", line 2, in <module>
    from pyflink.table.descriptors import Schema, FileSystem
ImportError: cannot import name 'FileSystem' from 'pyflink.table.descriptors' (/usr/local/opt/python@3.8/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyflink/table/descriptors.py)
{code}",,aomidvar,dianfu,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23821,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 14 06:37:58 UTC 2021,,,,,,,,,,"0|z0uufs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/21 06:37;dianfu;Merged to:
- master via e4b2f0cf200d4190a5a29a37d839e2717401f5a6
- release-1.14 via beb700978f6ce9e8bc8c1fc2c7aae738071bd2f2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
py35-cython fails due to could not find requests>=2.26.0,FLINK-24260,13400684,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dianfu,xtsong,xtsong,13/Sep/21 01:50,15/Dec/21 01:40,13/Jul/23 08:12,13/Sep/21 06:13,1.12.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,,,,,API / Python,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23942&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3&l=21015

{code}
Collecting requests>=2.26.0 (from apache-flink==1.12.dev0)
  Could not find a version that satisfies the requirement requests>=2.26.0 (from apache-flink==1.12.dev0) (from versions: 0.2.0, 0.2.1, 0.2.2, 0.2.3, 0.2.4, 0.3.0, 0.3.1, 0.3.2, 0.3.3, 0.3.4, 0.4.0, 0.4.1, 0.5.0, 0.5.1, 0.6.0, 0.6.1, 0.6.2, 0.6.3, 0.6.4, 0.6.5, 0.6.6, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.7.4, 0.7.5, 0.7.6, 0.8.0, 0.8.1, 0.8.2, 0.8.3, 0.8.4, 0.8.5, 0.8.6, 0.8.7, 0.8.8, 0.8.9, 0.9.0, 0.9.1, 0.9.2, 0.9.3, 0.10.0, 0.10.1, 0.10.2, 0.10.3, 0.10.4, 0.10.6, 0.10.7, 0.10.8, 0.11.1, 0.11.2, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.13.4, 0.13.5, 0.13.6, 0.13.7, 0.13.8, 0.13.9, 0.14.0, 0.14.1, 0.14.2, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.1.0, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 2.0.0, 2.0.1, 2.1.0, 2.2.0, 2.2.1, 2.3.0, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0, 2.6.1, 2.6.2, 2.7.0, 2.8.0, 2.8.1, 2.9.0, 2.9.1, 2.9.2, 2.10.0, 2.11.0, 2.11.1, 2.12.0, 2.12.1, 2.12.2, 2.12.3, 2.12.4, 2.12.5, 2.13.0, 2.14.0, 2.14.1, 2.14.2, 2.15.1, 2.16.0, 2.16.1, 2.16.2, 2.16.3, 2.16.4, 2.16.5, 2.17.0, 2.17.1, 2.17.2, 2.17.3, 2.18.0, 2.18.1, 2.18.2, 2.18.3, 2.18.4, 2.19.0, 2.19.1, 2.20.0, 2.20.1, 2.21.0, 2.22.0, 2.23.0, 2.24.0, 2.25.0, 2.25.1)
No matching distribution found for requests>=2.26.0 (from apache-flink==1.12.dev0)
{code}",,dianfu,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23345,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 13 06:13:18 UTC 2021,,,,,,,,,,"0|z0uu7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/21 01:51;xtsong;cc [~dianfu];;;","13/Sep/21 02:06;dianfu;[~xtsong] Thanks. I will fix it ASAP;;;","13/Sep/21 02:14;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23951&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3&l=21017;;;","13/Sep/21 02:20;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23956&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3&l=21016;;;","13/Sep/21 06:13;dianfu;Fixed in release-1.12 via 16c2bcc6330c95385a8e360aad96dbde0230ed6c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Environment / Mini Cluster do not forward configuration.,FLINK-24255,13400485,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,10/Sep/21 14:33,15/Dec/21 01:44,13/Jul/23 08:12,19/Nov/21 00:10,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"When using {{StreamExecutionEnvironment getExecutionEnvironment(Configuration)}}, the config should determine the characteristics of the execution.

The config is for example passed to the local environment in the local execution case, and used during the instantiation of the MiniCluster.

But when using the {{TestStreamEnvironment}} and the {{MiniClusterWithClientRule}}, the config is ignored.

The issue is that the {{StreamExecutionEnvironmentFactory}} in {{TestStreamEnvironment}} ignores the config that is passed to it.",,leonard,sewen,Thesharing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 19 00:10:40 UTC 2021,,,,,,,,,,"0|z0uszk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/21 00:10;sewen;Fixed
  - 1.13.4 (release-1.13) via 20323e32ac01a6ecd4a8ca1e3f3d2291e4119661
  - 1.14.1 (release-1.14) via dd17a319b7ad338c4d5a12ebe29a8f46bb987228
  - 1.15.0 (master) via 57253c5fc880fff880526a8954446c8189ac7c72;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-clients dependency missing in Gradle Example,FLINK-24248,13400415,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,daisyt,knaufk,knaufk,10/Sep/21 09:43,17/Sep/21 14:27,13/Jul/23 08:12,17/Sep/21 14:27,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Documentation,,,,,0,pull-request-available,,,,"The Gradle example on the ""Project Configuration"" page misses 

```
  compile ""org.apache.flink:flink-clients_${scalaBinaryVersion}:${flinkVersion}""
```

in order to be able to run the program locally. ",,knaufk,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,https://github.com/apache/flink/pull/17263,,,,,,,,,,9223372036854775807,,,,Fri Sep 17 14:27:07 UTC 2021,,,,,,,,,,"0|z0usk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Sep/21 14:27;rmetzger;Merged to master in https://github.com/apache/flink/commit/0bbc91a2a960cac7e9849eba7ff3e6d8085812be;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the problem caused by multiple jobs sharing the loopback mode address stored in the environment variable in PyFlink,FLINK-24245,13400401,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,10/Sep/21 08:34,14/Sep/21 02:06,13/Jul/23 08:12,13/Sep/21 07:31,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,API / Python,,,,,0,pull-request-available,,,,"In loopback mode, we store the loopback address in Environment which will cause other jobs will also run in the loopback.",,dianfu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24137,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 13 07:31:22 UTC 2021,,,,,,,,,,"0|z0usgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/21 07:31;hxbks2ks;Merged into master via a6fc9d1c395db45581cb0858a001b7d240cad0a0
Merged into release-1.14 via 23d63edb58b91d282c6355793851bce3406d946e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Receiving new buffer size before network reader configured,FLINK-24233,13400285,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,akalashnikov,akalashnikov,09/Sep/21 16:25,17/Sep/21 18:21,13/Jul/23 08:12,17/Sep/21 18:18,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Network,,,,,0,pull-request-available,,,,"It happened on the big cluster(parallelism = 75, TM=5, task=8) just on the initialization moment.
{noformat}
2021-09-09 14:36:42,383 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Map -> Flat Map (71/75)#0 (7a5b971e0cd57aa5d057a114e2679b03) switched from RUNNING to FAILED with failure c
ause: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Fatal error at remote task manager 'ip-172-31-22-183.eu-central-1.compute.internal/172.31.22.183:42085'.
        at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.decodeMsg(CreditBasedPartitionRequestClientHandler.java:339)
        at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.channelRead(CreditBasedPartitionRequestClientHandler.java:240)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
        at org.apache.flink.runtime.io.network.netty.NettyMessageClientDecoderDelegate.channelRead(NettyMessageClientDecoderDelegate.java:112)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:795)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:480)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalStateException: No reader for receiverId = 296559f497c54a82534945f4549b9e2d exists.
        at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.obtainReader(PartitionRequestQueue.java:194)
        at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.notifyNewBufferSize(PartitionRequestQueue.java:188)
        at org.apache.flink.runtime.io.network.netty.PartitionRequestServerHandler.channelRead0(PartitionRequestServerHandler.java:134)
        at org.apache.flink.runtime.io.network.netty.PartitionRequestServerHandler.channelRead0(PartitionRequestServerHandler.java:42)
at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:311)
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:432)
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276)
        ... 13 more


{noformat}",,akalashnikov,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24328,,,,,,,,,,,,,,,,,,FLINK-24328,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 17 18:21:05 UTC 2021,,,,,,,,,,"0|z0urr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Sep/21 18:18;pnowojski;Hotfix:
Merged to master as 7bacef09622
Merged to release-1.14 as 63fa7d07452;;;","17/Sep/21 18:21;pnowojski;As part of this ticket we have merged a code that just ignores new buffer size announcements if NetworkSequenceViewReader is not ready. However if that was a last buffer size adjustment for some period of time, this buffer size adjustment will be ignored and previous or default value will be still in use. FLINK-24328 will plan how to address this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Archiving of suspended jobs prevents breaks subsequent archive attempts,FLINK-24232,13400249,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,09/Sep/21 14:10,16/Dec/21 20:41,13/Jul/23 08:12,16/Dec/21 20:41,1.12.5,1.13.1,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"To archive a job we write a file that uses the job ID as the name. Since suspended jobs are handled like other terminal jobs they are also being archived.

When that job then later resumes any attempt to archive the job on termination will fail because an archive already exists.

The simplest option is to add a suffix if an archive already exists, like ""_1"".",,jingzhang,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22434,,,,,,,,FLINK-20195,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 14 10:25:24 UTC 2021,,,,,,,,,,"0|z0urj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/21 16:19;chesnay;Some additional thoughts need to be given for how the HistoryServer is supposed to load these archives, because again it assumes for a given job ID to only be used once.;;;","08/Dec/21 16:12;trohrmann;Hmm, I do see that archiving a suspended job can be helpful for looking at the history of a job. Maybe we either need to introduce a notion of job history or as a quick fix we don't archive suspended jobs.;;;","14/Dec/21 10:25;chesnay;Archiving of suspended jobs has been disabled.

master: efa336294f4280401d9933158dd499f377a63c2a
1.14: 069d629c53d4310bc1d45c278c2703452fab96d5 
1.13: ca3798c6228fb8806a5954acfde098f060a2081b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock in QueryableState Client,FLINK-24213,13399987,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,dwysakowicz,dwysakowicz,08/Sep/21 12:56,15/Dec/21 01:40,13/Jul/23 08:12,10/Sep/21 05:30,1.12.6,1.13.3,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.0,,,Runtime / Queryable State,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23750&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=15476

{code}
 Found one Java-level deadlock:
Sep 08 11:12:50 =============================
Sep 08 11:12:50 ""Flink Test Client Event Loop Thread 0"":
Sep 08 11:12:50   waiting to lock monitor 0x00007f4e380309c8 (object 0x0000000086b2cd50, a java.lang.Object),
Sep 08 11:12:50   which is held by ""main""
Sep 08 11:12:50 ""main"":
Sep 08 11:12:50   waiting to lock monitor 0x00007f4ea4004068 (object 0x0000000086b2cf50, a java.lang.Object),
Sep 08 11:12:50   which is held by ""Flink Test Client Event Loop Thread 0""

{code}",,dwysakowicz,xmarker,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-9925,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 10 02:18:32 UTC 2021,,,,,,,,,,"0|z0upww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/21 06:02;xmarker;The dead lock seems occur in following scene:

1.When establishedConnection handle data failed ,call stack  in netty bootstrap thread is: 

establishedConnection.onFailure() -> establishedConnection.close() -> hold establishedConnection.lock -> when channel closed : hold serverConnection.connectionLock 

2.When client.shutdown() ,call stack in main thread is :

serverConnection.close() -> hold serverConnection.connectionLock-  >  establishedConnection.close() ->
 hold establishedConnection.lock 

so ,two thread with different direction wants hold opposition's lock ,dead lock occurred.

 

May be ServerConnection.close should call internalConnection.close first without lock ,because  internalConnection.close is thread safe, has any better way?;;;","09/Sep/21 07:32;chesnay;[~xmarker] concurrency analysis is correct.

I see 2 options here. Either we enforce a strict order such that we always start close() calls from the ServerConnection, or we merge the locks.
Due to how the connections are constructed the first option cannot be implemented without larger refactorings, so I'm inclined to go with the latter; it should also make it in general easier to reason about, and the separation of locks doesn't provide any real benefit as far as I can tell.;;;","09/Sep/21 12:38;chesnay;master:
b5ac92eb5282ecdedf2daf89c1e9a1a737499836
2e721ab3fa501ce5c4ac4f9fe032a770aa66bc9a
1.14:
a2185c0f9bef64b103a4e1ae2c33619d83d54921
2459a3c3001c2ef263536276b7d7109752f93286
1.13:
640797f39454223503c097f4762ae4f8d5d2e768
4db7f4c502ba6428bf4f3f7d52b00a8cbada29fa
1.12:
fa2ab610c1c20ff828db0fc928b6328c2f440e9d
de69c3e3e7a4c29e99acd91129937db24afaefb2;;;","10/Sep/21 02:18;xtsong;Instance on 1.12:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23877&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=15504;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"kerberos krb5.conf file is mounted as empty directory, not the expected file",FLINK-24212,13399963,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,LiuZeshan,LiuZeshan,LiuZeshan,08/Sep/21 11:10,30/Nov/21 20:37,13/Jul/23 08:12,14/Sep/21 01:59,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,"From FLINK-18971，we can mount kerberos krb5 conf file to pod with path /etc/krb5.conf，however if the krb5 conf file is not named krb5.conf (e.g named mykrb5.conf)，the mount path /etc/krb5.conf in pod will be an empty directory, not a file that we expect.
{code:java}
root@mykrb5-conf-test-6dd5c76f87-vfwh5:/# ls /etc/krb5.conf/ -la
total 8
drwxrwxrwx 2 root root 4096 Sep  8 10:42 .
drwxr-xr-x 1 root root 4096 Sep  8 10:42 ..{code}
 

 The reason is that, the code  in [KerberosMountDecrator#decroateFlinkPod|https://github.com/apache/flink/blob/7e91e82fca999ddefa7ebdf198b8cfd3b6998b8b/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/decorators/KerberosMountDecorator.java#L110], we create the deployment like this: 
{code:java}
...
        volumeMounts:
        - mountPath: /etc/krb5.conf
          name: my-krb5conf-volume
          subPath: krb5.conf    
...
      volumes:
      - configMap:
          defaultMode: 420
          items:
          - key: mykrb5.conf
            path: mykrb5.conf
          name: my-krb5conf
        name: my-krb5conf-volume
{code}
path value should be set to const value ""krb5.conf"", not the file name that user provide (path: mykrb5.conf).

 we can use the yaml description file attachment to reproduce the problem.  [^mykrb5conf.yaml]

 ",,guoyangze,LiuZeshan,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/21 11:10;LiuZeshan;mykrb5conf.yaml;https://issues.apache.org/jira/secure/attachment/13033204/mykrb5conf.yaml",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 14 01:59:08 UTC 2021,,,,,,,,,,"0|z0uprk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/21 11:14;LiuZeshan;Could you [~guoyangze] [~xtsong] have a look at this.

Assign to me if possible.;;;","09/Sep/21 06:52;guoyangze;I'll take a look.;;;","14/Sep/21 01:59;xtsong;Fixed via
- master (1.15): d532f5fc81f9acf611541cab440eb3d5a9de08cc
- release-1.14: 742ba0a171fc909a6e65b9581ce76ea35862c341;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Window related serailizer should not return 0 as its serialized length,FLINK-24210,13399942,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lijinzhong,yunta,yunta,08/Sep/21 09:56,20/Jan/22 02:52,13/Jul/23 08:12,20/Jan/22 02:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,API / Type Serialization System,Runtime / State Backends,,,,0,pull-request-available,stale-assigned,,,"TimeWindow serializer return 0 as its length for serialization, this is certatinately not correct.
{code:java}
public static class Serializer extends TypeSerializerSingleton<TimeWindow> {
....
        @Override
        public int getLength() {
            return 0;
        }
}
{code}
Current namespace serializer in state backend does not depends on this interface so that no obvious bug has ever reported.

Moreover, this bug also occurs in other window related serializer.",,lijinzhong,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24797,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 20 02:51:59 UTC 2022,,,,,,,,,,"0|z0upmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/21 10:43;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","20/Jan/22 02:51;yunta;Merged in master: 573d2b038f8fd5dc1177733ee3087c5b6c847fa4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jdbc connector uses postgres testcontainers in compile scope,FLINK-24209,13399939,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,dwysakowicz,dwysakowicz,08/Sep/21 09:45,09/Sep/21 05:24,13/Jul/23 08:12,09/Sep/21 05:24,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,,,,,Connectors / JDBC,,,,,0,pull-request-available,,,,testcontainer dependencies should only ever be used in the test scope. It has been fixed at least on 1.14 branch (I have not checked on master),,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22462,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 09 05:24:27 UTC 2021,,,,,,,,,,"0|z0upm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/21 05:24;chesnay;master/1.14 are not affected.

1.13: c97ddb8181e6ad16510f4e6b558f9441c8f1f1d7
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"PulsarSourceITCase fails with ""Consumer not found""",FLINK-24206,13399915,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,dwysakowicz,dwysakowicz,08/Sep/21 06:58,14/Sep/21 01:42,13/Jul/23 08:12,14/Sep/21 01:42,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,Connectors / Pulsar,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23732&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=24650

{code}
Sep 08 05:08:35 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
Sep 08 05:08:35 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
Sep 08 05:08:35 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
Sep 08 05:08:35 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:228)
Sep 08 05:08:35 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:218)
Sep 08 05:08:35 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:209)
Sep 08 05:08:35 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:679)
Sep 08 05:08:35 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
Sep 08 05:08:35 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
Sep 08 05:08:35 	at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
Sep 08 05:08:35 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Sep 08 05:08:35 	at java.lang.reflect.Method.invoke(Method.java:498)
Sep 08 05:08:35 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
Sep 08 05:08:35 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
Sep 08 05:08:35 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
Sep 08 05:08:35 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
Sep 08 05:08:35 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
Sep 08 05:08:35 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
Sep 08 05:08:35 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
Sep 08 05:08:35 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
Sep 08 05:08:35 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
Sep 08 05:08:35 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
Sep 08 05:08:35 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
Sep 08 05:08:35 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
Sep 08 05:08:35 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
Sep 08 05:08:35 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
Sep 08 05:08:35 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
Sep 08 05:08:35 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
Sep 08 05:08:35 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
Sep 08 05:08:35 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
Sep 08 05:08:35 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
Sep 08 05:08:35 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
Sep 08 05:08:35 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
Sep 08 05:08:35 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
Sep 08 05:08:35 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Sep 08 05:08:35 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Sep 08 05:08:35 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Sep 08 05:08:35 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Sep 08 05:08:35 Caused by: org.apache.pulsar.client.api.PulsarClientException$BrokerMetadataException: Consumer not found
Sep 08 05:08:35 	at org.apache.pulsar.client.api.PulsarClientException.unwrap(PulsarClientException.java:987)
Sep 08 05:08:35 	at org.apache.pulsar.client.impl.PulsarClientImpl.close(PulsarClientImpl.java:658)
Sep 08 05:08:35 	at org.apache.flink.connector.pulsar.source.reader.source.PulsarSourceReaderBase.close(PulsarSourceReaderBase.java:83)
Sep 08 05:08:35 	at org.apache.flink.connector.pulsar.source.reader.source.PulsarOrderedSourceReader.close(PulsarOrderedSourceReader.java:170)
Sep 08 05:08:35 	at org.apache.flink.streaming.api.operators.SourceOperator.close(SourceOperator.java:324)
Sep 08 05:08:35 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:141)
Sep 08 05:08:35 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:127)
Sep 08 05:08:35 	at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:1020)
Sep 08 05:08:35 	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:864)
Sep 08 05:08:35 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:752)
Sep 08 05:08:35 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
Sep 08 05:08:35 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
Sep 08 05:08:35 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
Sep 08 05:08:35 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
Sep 08 05:08:35 	at java.lang.Thread.run(Thread.java:748)

{code}",,dwysakowicz,syhily,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24263,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 14 01:42:06 UTC 2021,,,,,,,,,,"0|z0upgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Sep/21 06:07;syhily;This could be some bugs on the Pulsar side. I'll keep this issue unresolved until I finally find the cause.;;;","13/Sep/21 08:21;syhily;It's a race condition on pulsar source close. I have found the solution on how to properly closed the pulsar client.;;;","13/Sep/21 08:29;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23958&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=24936;;;","13/Sep/21 08:43;syhily;[~xtsong] Can you review this PR? https://github.com/apache/flink/pull/17254;;;","13/Sep/21 08:57;xtsong;[~syhily],
The changes in the PR look good to me. However, shouldn't the PR be opened against the master branch?

Usually, contributors only need to open PRs against the master branch. If the commits need to be ported to other branches, the committers can do that while closing the PR.
Exceptions are:
* If the problem that the PR tries to fix only exist in the release branches, but not in the master branch, the PR should be opened against only the release branches.
* If the PR opened against the master branch can not be directly applied to the release branches, or if we are unsure about the changes and wants to trigger CI tests on the release branches, the PR should be opened against the release branches in addition to the master branch.;;;","13/Sep/21 08:59;syhily;[~xtsong] OK, I'll open a new PR that targets the master.;;;","14/Sep/21 01:42;xtsong;Fixed via
- master (1.15): 5474f89c8e39d97b6f1b94cbbae3d2c9dd116bf6
- release-1.14: 28677f34bad1ffb4f405a4550536969eb9621d05;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractServerTest fails due to port conflict,FLINK-24203,13399889,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xmarker,xtsong,xtsong,08/Sep/21 04:04,08/Sep/21 11:12,13/Jul/23 08:12,08/Sep/21 11:12,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / Queryable State,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23718&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=41463ccd-0694-5d4d-220d-8f771e7d098b&l=15081

{code}
Sep 08 00:18:11 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.19 s <<< FAILURE! - in org.apache.flink.queryablestate.network.AbstractServerTest
Sep 08 00:18:11 [ERROR] testServerInitializationFailure  Time elapsed: 0.026 s  <<< FAILURE!
Sep 08 00:18:11 java.lang.AssertionError: 
Sep 08 00:18:11 
Sep 08 00:18:11 Expected: (an instance of org.apache.flink.util.FlinkRuntimeException and exception with message a string containing ""Unable to start Test Server 2. All ports in provided range are occupied."")
Sep 08 00:18:11      but: exception with message a string containing ""Unable to start Test Server 2. All ports in provided range are occupied."" message was ""Unable to start Test Server 1. All ports in provided range are occupied.""
Sep 08 00:18:11 Stacktrace was: org.apache.flink.util.FlinkRuntimeException: Unable to start Test Server 1. All ports in provided range are occupied.
Sep 08 00:18:11 	at org.apache.flink.queryablestate.network.AbstractServerBase.start(AbstractServerBase.java:209)
Sep 08 00:18:11 	at org.apache.flink.queryablestate.network.AbstractServerTest.testServerInitializationFailure(AbstractServerTest.java:73)
{code}",,xmarker,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 08 11:12:05 UTC 2021,,,,,,,,,,"0|z0upb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/21 04:12;xmarker;[~xtsong] I can help resolve the issue;;;","08/Sep/21 11:12;chesnay;master: 7c65adb8f028e90350b766c630d7434c8b999a27;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BoundedSourceITCase hangs on azure,FLINK-24202,13399887,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,xtsong,xtsong,08/Sep/21 03:56,08/Sep/21 16:10,13/Jul/23 08:12,08/Sep/21 16:10,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,API / DataStream,,,,,0,test-stability,,,,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23718&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=5389,,gaoyunhaii,roman,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24160,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 08 16:09:37 UTC 2021,,,,,,,,,,"0|z0upao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/21 03:56;xtsong;cc [~roman];;;","08/Sep/21 03:57;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23718&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10982;;;","08/Sep/21 08:35;roman;This is the same issue as FLINK-24160 (MemCheckpointStreamFactory default limit exceeded).

Should be fixed by [https://github.com/apache/flink/pull/17165];;;","08/Sep/21 16:09;gaoyunhaii;Fixed on master via c740f9269ab2836a69cc3fac9dc4a670f6aa86d2
Fixed on release-1.14 via b56041b9acc8fafa2ab1500e9162bed7bbcb333d

I'll close the issue for now, we could reopen it if it reproduced~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointBarrierTrackerTest.testTwoLastBarriersOneByOne fails on azure,FLINK-24200,13399878,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,xtsong,xtsong,08/Sep/21 02:48,15/Dec/21 01:40,13/Jul/23 08:12,01/Oct/21 02:40,1.12.5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23719&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=0dbaca5d-7c38-52e6-f4fe-2fb69ccb3ada&l=8983

{code}
[ERROR] Tests run: 14, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.893 s <<< FAILURE! - in org.apache.flink.streaming.runtime.io.CheckpointBarrierTrackerTest
[ERROR] testTwoLastBarriersOneByOne(org.apache.flink.streaming.runtime.io.CheckpointBarrierTrackerTest)  Time elapsed: 0.093 s  <<< FAILURE!
java.lang.AssertionError: 

Expected: a value less than or equal to <30L>
     but: <33L> was greater than <30L>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.junit.Assert.assertThat(Assert.java:956)
	at org.junit.Assert.assertThat(Assert.java:923)
	at org.apache.flink.streaming.runtime.io.CheckpointBarrierTrackerTest.testTwoLastBarriersOneByOne(CheckpointBarrierTrackerTest.java:616)
{code}",,pnowojski,xtsong,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 01 02:40:28 UTC 2021,,,,,,,,,,"0|z0up8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Oct/21 02:40;ym;merged commit [{{1a9d782}}|https://github.com/apache/flink/commit/1a9d78259668f3d9c1a52dd278e8502d1449f780] into apache:release-1.12;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Job submission can fail with: ""RestClientException: [File upload failed.]""",FLINK-24197,13399823,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,dwysakowicz,dwysakowicz,07/Sep/21 16:47,06/Dec/22 22:14,13/Jul/23 08:12,20/Sep/21 08:38,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Connectors / FileSystem,Tests,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23672&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=11040

{code}
Caused by: org.apache.flink.util.FlinkException: Failed to execute job 'StreamingFileSinkProgram'.
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2056)
	at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:137)
	at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:76)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1917)
	at FileSinkProgram.main(FileSinkProgram.java:105)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
	... 8 more
Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
	at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$11(RestClusterClient.java:433)
	at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884)
	at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:373)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575)
	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:943)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.runtime.rest.util.RestClientException: [File upload failed.]
	at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:532)
	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:512)
	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966)
	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
	... 4 more

{code}",,dwysakowicz,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23557,,,,FLINK-24339,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 20 08:38:41 UTC 2021,,,,,,,,,,"0|z0uowg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/21 12:19;chesnay;I was finally able to reproduce it. It seems depend on the size of the FileUpload; I have found a payload range of 3 bytes range (5043442-5043445) where it can be reliably reproduced.;;;","09/Sep/21 05:31;chesnay;I believe this to be an exotic bug in netty, and have filed a ticket: https://github.com/netty/netty/issues/11668;;;","09/Sep/21 06:30;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23764&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=10890;;;","09/Sep/21 13:49;chesnay;Got the confirmation that this is a Netty bug.

Will look into a workaround for 1.14; we can hopefully fix it properly by bumping Netty in 1.15.;;;","16/Sep/21 05:30;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24174&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=cc5499f8-bdde-5157-0d76-b6528ecd808e&l=9866;;;","20/Sep/21 08:38;chesnay;master: 0f165565ba02df3ebcbeed0c38293ec17e32ebd3

1.14:  69b6e60da295a47eb449811c64c8a19733b27673 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable single rowtime column check for collect/print,FLINK-24186,13399730,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,twalthr,twalthr,07/Sep/21 09:08,10/Dec/21 13:43,13/Jul/23 08:12,10/Dec/21 13:43,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Table SQL / API,,,,,0,pull-request-available,stale-assigned,,,"As seen in FLINK-23751, the single rowtime column check can occur also during collecting and printing which is not important there as watermarks as not used.

The exception is also misleading as it references a {{DataStream}}:
{code:java}
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.TableException: Found more than one rowtime field: [bidtime, window_time] in the query when insert into 'default_catalog.default_database.Unregistered_Collect_Sink_8'.
Please select the rowtime field that should be used as event-time timestamp for the DataStream by casting all other fields to TIMESTAMP.
{code}",,martijnvisser,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23751,FLINK-23749,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 10 13:43:38 UTC 2021,,,,,,,,,,"0|z0uobs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/21 10:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","10/Dec/21 13:43;twalthr;Fixed in master: f27e53a03516ca7de7ec6c86a905f7d8a88b1271;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential race condition leading to incorrectly issued interruptions,FLINK-24184,13399728,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,pnowojski,pnowojski,pnowojski,07/Sep/21 09:04,13/Sep/21 07:28,13/Jul/23 08:12,13/Sep/21 07:27,1.10.3,1.11.4,1.12.5,1.13.2,1.14.0,1.8.3,1.9.3,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,Runtime / Task,,,,,0,pull-request-available,,,,"There is a  race condition in disabling interrupts while closing resources. Currently this is guarded by a volatile variable, but there might be a race condition when:
1. interrupter thread first checked the shouldInterruptOnCancel flag
2. shouldInterruptOnCancel flag switched to false as Task/StreamTask entered cleaning up phase
3. interrupter issued an interrupt while Task/StreamTask are closing/releasing resources, potentially causing a memory leak",,pnowojski,Thesharing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-9776,,,,,,,,FLINK-24182,,,,,,FLINK-21821,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 13 07:27:47 UTC 2021,,,,,,,,,,"0|z0uobc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/21 07:27;pnowojski;merged commit 4ea9908 into apache:master
merged commit 79a801a7bf6 into apache:release-1.14

as this is an old lowest priority bug fix, that has never been observed and there are merge conflicts with 1.13 branch, I'm not backporting it to 1.13.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-s3-fs-base contains copied codes not listed in NOTICE file,FLINK-24183,13399727,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,xtsong,xtsong,07/Sep/21 08:59,15/Dec/21 01:40,13/Jul/23 08:12,08/Sep/21 08:18,1.10.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.0,,,FileSystems,,,,,0,legal,pull-request-available,,,{{com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser}} is copied from {{aws-java-sdk-s3}}.,,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16014,,,,,,,,FLINK-24074,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 08 08:18:39 UTC 2021,,,,,,,,,,"0|z0uob4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/21 08:18;chesnay;master: 92812c0db1f1b67c830703d18f5d4e0e307171dd
1.14: a4cd43bbd573b38690ed7cab1f3f199510f047bc 
1.13: 256c82526517546493ba9d0f10cd3fa0028cbca1 
1.12: b682eed2ea7787996322b6a5e04f746bb82e3071 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rowtime type is not correct for windowTableFunction or OverAggregate follows after Match because the output type does not updated after input rowtime attribute changed from rowtime to rowtime_ltz,FLINK-24168,13399581,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jingzhang,jingzhang,jingzhang,06/Sep/21 12:55,18/Oct/21 09:07,13/Jul/23 08:12,18/Sep/21 03:03,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Rowtime type is not correct for windowTableFunction or OverAggregate on Match because the output type does not updated after input rowtime attribute changed from rowtime to rowtime_ltz in `RelTimeIndicator`.

The bug could be reproduced by the following two cases:
{code:java}
@Test
def testWindowTVFOnMatchRecognizeOnRowtimeLTZ(): Unit = {
  val sqlQuery =
    s""""""
       |SELECT
       |  *
       |FROM Ticker
       |MATCH_RECOGNIZE (
       |  PARTITION BY symbol
       |  ORDER BY ts_ltz
       |  MEASURES
       |    A.price as price,
       |    A.tax as tax,
       |    MATCH_ROWTIME() as matchRowtime
       |  ONE ROW PER MATCH
       |  PATTERN (A)
       |  DEFINE
       |    A AS A.price > 0
       |) AS T
       |"""""".stripMargin
  val table = util.tableEnv.sqlQuery(sqlQuery)
  util.tableEnv.registerTable(""T"", table)
  val sqlQuery1 =
    s""""""
       |SELECT *
       |FROM TABLE(TUMBLE(TABLE T, DESCRIPTOR(matchRowtime), INTERVAL '3' second))
       |"""""".stripMargin
  util.verifyRelPlanWithType(sqlQuery1)
}

@Test
def testOverWindowOnMatchRecognizeOnRowtimeLTZ(): Unit = {
  val sqlQuery =
    s""""""
       |SELECT
       |  *
       |FROM Ticker
       |MATCH_RECOGNIZE (
       |  PARTITION BY symbol
       |  ORDER BY ts_ltz
       |  MEASURES
       |    A.price as price,
       |    A.tax as tax,
       |    MATCH_ROWTIME() as matchRowtime
       |  ONE ROW PER MATCH
       |  PATTERN (A)
       |  DEFINE
       |    A AS A.price > 0
       |) AS T
       |"""""".stripMargin
  val table = util.tableEnv.sqlQuery(sqlQuery)
  util.tableEnv.registerTable(""T"", table)
  val sqlQuery1 =
    """"""
      |SELECT
      |  symbol,
      |  price,
      |  tax,
      |  matchRowtime,
      |  SUM(price) OVER (
      |    PARTITION BY symbol ORDER BY matchRowtime RANGE UNBOUNDED PRECEDING) as price_sum
      |FROM T
    """""".stripMargin
  util.verifyRelPlanWithType(sqlQuery1)
}

{code}
 ",,godfreyhe,jark,jingzhang,leonard,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 22 12:32:07 UTC 2021,,,,,,,,,,"0|z0uneo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/21 01:32;jingzhang;[~Leonard Xu] [~twalthr] I try to fix the bug by updating the row type of OverAggregate and TableFunction if the input row type is changed. But I think it's not enough yet. After change a field type from rowtime to rowtime_ltz, in theory we need to propagate the change toward to sink, including deriving all  RelNode type, deriving all RexNode type (including all RexCall because it's input type maybe changed). `RelTimeIndicatorConverter` does not cover all of this propagating.

BTW, this should not be the responsibility of the `RelTimeIndicatorConverter`. 

I wonder if  we have better solution to infer result type of `MATCH_ROWTIME`. WDYT?  [~twalthr] [~Leonard Xu];;;","07/Sep/21 07:47;twalthr;[~qingru zhang] maybe the logic would be easier if we simply provide dedicated function named `MATCH_ROWTIME_LTZ()` or we do a similar trick as with the `CURRENT_WATERMARK` function that simply takes the time attribute as a parameter. In the end, we should ensure a consistent behavior.;;;","07/Sep/21 08:00;jingzhang;[~twalthr] Thanks for response. I totally agree with you to introduce `MATCH_ROWTIME_LTZ()` or pass the time attributes as input parameter. 

I'm not very sure how to ensure the consistent behavior for old `MATCH_ROWTIME` function.

Could we simply just throw an exception if we found `MATCH_ROWTIME` input row type is LTZ?;;;","07/Sep/21 09:32;leonard;Thanks [~qingru zhang] for picking up this, *MATCH_ROWTIME()* is a no-args function, I think a clean way is to determine the return type eagerly by _TypeInference_ before transfer the _SQL_ to _RelNode_, but one flaw in this approach is to get the rowtime field from the complex input. Changing the function to *MATCH_ROWTIME(rowtime)* is a better option in my opinion, one minor concern is the compatibility.;;;","07/Sep/21 10:20;jingzhang;[~Leonard Xu] Thanks for response.

It's a perfect solution if we could derive the return type of *MATCH_ROWTIME()*  in `SqlToRelConverter`_._ But *MATCH_ROWTIME()* is a no-args function. We have to deal with WindowJoin/IntervalJoin/or other RelNodes which could contain mutiple fields with Rowtime attribute, we have to take those into consideration when get the rowtime field from the input type.

I agree with you that *MATCH_ROWTIME(rowtime)* is a better option.

About compatibility, how about keep *MATCH_ROWTIME()* version with no-args function at the same time. And add a check to throw an exception and suggest to pass in time attribute field if we found `MATCH_ROWTIME` input row type is LTZ. So that we could not effect the user case which does not use LTZ timestamp. WDYT [~Leonard Xu];;;","07/Sep/21 10:26;leonard;I think the the propagation is complex, but to check the input rowtime attribute is timestamp or timestamp_ltz is easy.

For the compatibility, I think we can support *MATCH_ROWTIME()* which can accept zero or one time attribute argument, for timestamp, if the input contains timestamp time attribute, user can use *MATCH_ROWTIME()* or  *MATCH_ROWTIME(ts)*, if the input contains timestamp_ltz time attribute, we can check it and tell user to use   *MATCH_ROWTIME(ts_ltz).* I supposed the *MATCH_ROWTIME()* with timestamp_ltz is less used. [~qingru zhang] [~twalthr] 

 ;;;","07/Sep/21 10:28;twalthr;Syntactically, I would also prefer {{MATCH_ROWTIME(rowtime)}}. However, we need to check whether the semantics are correct in this case. The documentation states:

{noformat}
Returns the timestamp of the last row that was mapped to the given pattern.
{noformat}

If we just pass {{rowtime}} what does this mean? 

{noformat}
If no pattern variable is specified (e.g. SUM(price)), an expression references the default pattern variable * which references all variables in the pattern. In other words, it creates a list of all the rows mapped so far to any variable plus the current row.
{noformat}

Sounds ok to me. But please double check.
;;;","07/Sep/21 10:31;twalthr;Regarding the exception, we can access the ordering column in the `RelTimeIndicatorConverter` if {{MATCH_ROWTIME()}} (without args) makes sense in this case.;;;","07/Sep/21 11:26;jingzhang;[~twalthr] Your description about *MATCH_ROWTIME(rowtime)* sounds reasonable. But I have to admit pass a time attribute may confuse users a bit.

How about *MATCH_ROWTIME_LTZ* directly? The syntax of *MATCH_ROWTIME_LTZ* is same with *MATCH_ROWTIME* except it's input row time attribute and output type. We could check in `RelTimeIndicatorConverter` that  *MATCH_ROWTIME_LTZ* AND *MATCH_ROWTIME* should match it's input time attribute. But this solution is not so friendly to users because they need choose the right function based on the input rowtime attribute.;;;","07/Sep/21 13:10;twalthr;I vote for `MATCH_ROWTIME(rowtime)` for consistency. We are doing the same for `CURRENT_WATERMARK()` and to be honest we should have done the same for `SOURCE_WATERMARK()`. It solves not only the LTZ problem but also the precision one, in case we would like to increase the precision from 3 to 9 at some point. What do others think? [~jark];;;","08/Sep/21 04:09;jark;I also vote for {{MATCH_ROWTIME(rowtime)}} and deprecate {{MATCH_ROWTIME()}}. It provides a consistent way to derive rowtime in MATCH. 

Regarding the semantics problem of {{MATCH_ROWTIME(rowtime)}}, I don't think it is a problem. {{MATCH_ROWTIME}} is more like an aggregate function instead of scalar function. It collects all the rows so far and returns the last row, just like a {{LAST_VALUE(..)}} function. Due to only matched events will trigger the output, so it makes sense that {{MATCH_ROWTIME(rowtime)}} returns the timestamp of the last row that was mapped to **the given pattern**.

The next thing we need to discuss is do we need to block 1.14? It also breaks compatibility that we disallow {{MATCH_ROWTIME()}} for LTZ input, and we need 
 add this to release note. 




;;;","08/Sep/21 06:35;twalthr;We don't need to block 1.14. This issue was present in 1.13 already. But it would be great to fix it asap. I will reduce the priority to Critical.;;;","09/Sep/21 02:16;jingzhang;[~twalthr] [~jark] [~Leonard Xu] Thanks for all suggestions.

It would be better if we could fix it in 1.14 because *MATCH_ROWTIME()* with timestamp_ltz  is new introduced in Flink 1.13.

I open a [PR|https://github.com/apache/flink/pull/17205] , please have a look, thanks a lot. [~twalthr] [~jark] [~Leonard Xu] [~godfreyhe].;;;","09/Sep/21 02:24;jingzhang;One more question, the bug already exists in 1.13,  the result `MatchRecognizeITCase#testWindowedGroupingAppliedToMatchRecognize` is wrong since 1.13. Do we need to fix the problem in 1.13?

There is only a very small probability that a user would suffer the problem because the bug maybe trigger only all following condition satisfied:
 # Use CEP
 # Use timestamp_ltz as rowtime attribute
 # Use 1.13+;;;","09/Sep/21 02:30;godfreyhe;I would like to fix it only in master, not including 1.13 and 1.14. Because, NO users report such issue before, only MATCH with timestamp_ltz will trigger this bug which is rare. I will change the fix version to master;;;","09/Sep/21 02:35;jark;+1 to only fix in master. Actually, we also not fix in master, but just introduce another way (i.e. new feature) for users, I don't think this is applicable for minor versions. We can update 1.13 documentation to note this limitation and let users use TIMESTAMP as rowtime instead. 
;;;","09/Sep/21 09:31;twalthr;+1 for 1.15 only + documentation;;;","18/Sep/21 03:03;godfreyhe;Fixed in 1.15.0: 475a662c02be34159b8e7f7707dc9ae1a8bbb362;;;","22/Sep/21 12:32;leonard;[~qingru zhang] [~godfreyhe]  Should we also add documentation for 1.14 and 1.13 ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix typo in javadoc of KeyContext class,FLINK-24165,13399532,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,jianzhang.yjz,jianzhang.yjz,jianzhang.yjz,06/Sep/21 08:08,05/Nov/21 23:50,13/Jul/23 08:12,07/Sep/21 11:17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,API / DataStream,,,,,0,pull-request-available,,,,,"[_*org.apache.flink.streaming.api.operators.KeyContext*_|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/KeyContext.java] interface annotation word spelling  error. modify from *_Inteface_* to _*Interface.*_",jark,jianzhang.yjz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 07 11:17:33 UTC 2021,,,,,,,,,,"0|z0un3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/21 11:17;jark;Fixed in master: e94a3db55d565830c3cb2892fbe8c5698a352351;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PartiallyFinishedSourcesITCase fails due to timeout,FLINK-24163,13399498,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gaoyunhaii,xtsong,xtsong,06/Sep/21 03:08,25/Jan/22 06:45,13/Jul/23 08:12,25/Jan/22 06:45,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,API / DataStream,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23529&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10996

{code}
Sep 04 04:35:28 [ERROR] Tests run: 6, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 155.236 s <<< FAILURE! - in org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase
Sep 04 04:35:28 [ERROR] test[complex graph ALL_SUBTASKS, failover: false]  Time elapsed: 65.999 s  <<< ERROR!
Sep 04 04:35:28 java.util.concurrent.TimeoutException: Condition was not met in given timeout.
Sep 04 04:35:28 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:164)
Sep 04 04:35:28 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:142)
Sep 04 04:35:28 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:134)
Sep 04 04:35:28 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForSubtasksToFinish(CommonTestUtils.java:297)
Sep 04 04:35:28 	at org.apache.flink.runtime.operators.lifecycle.TestJobExecutor.waitForSubtasksToFinish(TestJobExecutor.java:219)
Sep 04 04:35:28 	at org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase.test(PartiallyFinishedSourcesITCase.java:82)
{code}",,dwysakowicz,gaoyunhaii,pnowojski,roman,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24160,,,,,FLINK-25395,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 25 06:44:47 UTC 2022,,,,,,,,,,"0|z0umw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/21 03:08;xtsong;cc [~roman];;;","06/Sep/21 03:32;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23544&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=5403;;;","06/Sep/21 03:32;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23544&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10983;;;","06/Sep/21 03:36;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23547&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10995;;;","06/Sep/21 09:41;gaoyunhaii;This one should be called by the test configuration. There are two problems:
 # The checkpoint timeout 5s is still not large enough, it might be caused when the unaligned checkpoint is enabled, the checkpoint performance id bounded by disk IO.
 # We also met with 
{code}
java.util.concurrent.ExecutionException: java.io.IOException: Size of the state is larger than the maximum permitted memory-backed state. Size=8641168, maxSize=5242880. Consider using a different checkpoint storage, like the FileSystemCheckpointStorage.
    at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_292]
    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) ~[?:1.8.0_292]
    at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:66) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.finalizeNonFinishedSnapshots(AsyncCheckpointRunnable.java:177) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:124) [flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
    at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
Caused by: java.io.IOException: Size of the state is larger than the maximum permitted memory-backed state. Size=8641168, maxSize=5242880. Consider using a different checkpoint storage, like the FileSystemCheckpointStorage.
    at org.apache.flink.runtime.state.memory.MemCheckpointStreamFactory.checkSize(MemCheckpointStreamFactory.java:63) ~[flink-runtime-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.runtime.state.memory.MemCheckpointStreamFactory$MemoryCheckpointOutputStream.closeAndGetBytes(MemCheckpointStreamFactory.java:140) ~[flink-runtime-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.runtime.state.memory.MemCheckpointStreamFactory$MemoryCheckpointOutputStream.closeAndGetHandle(MemCheckpointStreamFactory.java:120) ~[flink-runtime-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.finishWriteAndResult(ChannelStateCheckpointWriter.java:218) ~[flink-runtime-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.doComplete(ChannelStateCheckpointWriter.java:240) ~[flink-runtime-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.lambda$complete$5(ChannelStateCheckpointWriter.java:202) ~[flink-runtime-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.runWithChecks(ChannelStateCheckpointWriter.java:296) ~[flink-runtime-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.complete(ChannelStateCheckpointWriter.java:200) ~[flink-runtime-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.completeInput(ChannelStateCheckpointWriter.java:187) ~[flink-runtime-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.runtime.checkpoint.channel.CheckpointInProgressRequest.execute(ChannelStateWriteRequest.java:212) ~[flink-runtime-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatchInternal(ChannelStateWriteRequestDispatcherImpl.java:85) ~[flink-runtime-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatch(ChannelStateWriteRequestDispatcherImpl.java:62) ~[flink-runtime-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.loop(ChannelStateWriteRequestExecutorImpl.java:96) ~[flink-runtime-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.run(ChannelStateWriteRequestExecutorImpl.java:75) ~[flink-runtime-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
    ... 1 more
{code}
This might be caused by we have channel state with unaligned checkpoints.

I'll open a PR to increase the checkpoint timeout and change to another statebackend type. 
    ;;;","07/Sep/21 02:03;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23560&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10993;;;","07/Sep/21 02:04;xtsong;Thanks [~gaoyunhaii]. I've assigned you to the ticket.;;;","07/Sep/21 06:00;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23629&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10995;;;","08/Sep/21 04:00;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23718&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=4710;;;","08/Sep/21 07:02;dwysakowicz;[~gaoyunhaii] Did we fix this issue with the PR you merged?;;;","08/Sep/21 07:39;gaoyunhaii;Hi [~dwysakowicz] the issue would be fixed after we also merge [https://github.com/apache/flink/pull/17165], I'll have a double look ~;;;","08/Sep/21 16:11;gaoyunhaii;Fixed on master via 6eff01b07eb5704dda5c9a48dc75603ffb70049f and c740f9269ab2836a69cc3fac9dc4a670f6aa86d2
Fixed on release-1.14 via 3a8a6b0d998bc66195de93d8b92dceca9cf94e71 and b56041b9acc8fafa2ab1500e9162bed7bbcb333d

I'll close the issue for now, we could reopen it if it reproduced~;;;","18/Jan/22 08:14;trohrmann;It happened again: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29598&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=16008;;;","18/Jan/22 08:15;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29598&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=10402;;;","18/Jan/22 08:15;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29598&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=16828;;;","18/Jan/22 08:18;gaoyunhaii;l'll have a look~;;;","19/Jan/22 07:17;gaoyunhaii;This seems to be due to different reason. 

Hi [~roman] , by binary search it seems with https://issues.apache.org/jira/browse/FLINK-25395 the running time of PartiallyFinishedSourcesITCase#test[complex graph SINGLE_SUBTASK, failover: true, strategy: region] has increased from 2s to about 1 minute, the case is blocked on restoring state after failover:
{code:java}
""transform-2-keyed (1/4)#1"" #1517 prio=5 os_prio=31 tid=0x00007f862136a000 nid=0x10423 runnable [0x0000700011fee000]
   java.lang.Thread.State: RUNNABLE
	at java.io.FileInputStream.readBytes(Native Method)
	at java.io.FileInputStream.read(FileInputStream.java:255)
	at org.apache.flink.core.fs.local.LocalDataInputStream.read(LocalDataInputStream.java:73)
	at org.apache.flink.core.fs.FSDataInputStreamWrapper.read(FSDataInputStreamWrapper.java:60)
	at org.apache.flink.runtime.util.ForwardingInputStream.read(ForwardingInputStream.java:52)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.flink.api.java.typeutils.runtime.DataInputViewStream.read(DataInputViewStream.java:68)
	at com.esotericsoftware.kryo.io.Input.fill(Input.java:146)
	at org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.require(NoFetchingInput.java:77)
	at com.esotericsoftware.kryo.io.Input.readAscii_slow(Input.java:598)
	at com.esotericsoftware.kryo.io.Input.readAscii(Input.java:576)
	at com.esotericsoftware.kryo.io.Input.readString(Input.java:454)
	at com.esotericsoftware.kryo.serializers.DefaultSerializers$StringSerializer.read(DefaultSerializers.java:177)
	at com.esotericsoftware.kryo.serializers.DefaultSerializers$StringSerializer.read(DefaultSerializers.java:166)
	at com.esotericsoftware.kryo.Kryo.readObjectOrNull(Kryo.java:730)
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:113)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761)
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:402)
	at org.apache.flink.runtime.state.heap.StateTableByKeyGroupReaders.lambda$createV2PlusReader$0(StateTableByKeyGroupReaders.java:78)
	at org.apache.flink.runtime.state.heap.StateTableByKeyGroupReaders$$Lambda$2196/1169355256.readElement(Unknown Source)
	at org.apache.flink.runtime.state.KeyGroupPartitioner$PartitioningResultKeyGroupReader.readMappingsInKeyGroup(KeyGroupPartitioner.java:297)
	at org.apache.flink.runtime.state.heap.HeapRestoreOperation.readKeyGroupStateData(HeapRestoreOperation.java:258)
	at org.apache.flink.runtime.state.heap.HeapRestoreOperation.readStateHandleStateData(HeapRestoreOperation.java:220)
	at org.apache.flink.runtime.state.heap.HeapRestoreOperation.restore(HeapRestoreOperation.java:166)
	at org.apache.flink.runtime.state.heap.HeapRestoreOperation.restore(HeapRestoreOperation.java:62)
	at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.restoreState(HeapKeyedStateBackendBuilder.java:169)
	at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:106)
	at org.apache.flink.runtime.state.hashmap.HashMapStateBackend.createKeyedStateBackend(HashMapStateBackend.java:137) {code}
Could you help to have a look~?

The commit before the PR on the master branch is 265a0a0708ae743c63505bb02e0659984a565fbb and the commit right after the PR is 4691b66545010ed812624a259869c7a522663720 . ;;;","19/Jan/22 09:29;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29668&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=10402;;;","19/Jan/22 09:30;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29668&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=16828;;;","19/Jan/22 09:35;roman;Hi Yun, sure, I'll take a look.

As for the mentioned commit, it reverted RocksDB snapshotting back to the older behavior which is slower. So the increase there is expected.;;;","19/Jan/22 10:52;gaoyunhaii;Very thanks [~roman] for the explanation! If the behavior is expected, I think I could first try to modify the test to reduce the running time or extend the waiting time. And for the long run, does it affect normal jobs in realistic and do we have plan to reduce this part~?;;;","19/Jan/22 17:49;roman;[~gaoyunhaii], I found that the reason is actually the 2nd commit from FLINK-25395, which added keyed state to one of the test operators.

That state can grow significantly because the stream is keyed by all the fields of an element, including sequenceNumber (so each stream element creates a separate state entry which also probably causes memory pressure). 

I prepared a PR to fix keying: [https://github.com/apache/flink/pull/18403]. Locally, the runtime is reduced from ~14s to ~5sec.

Could you take a look at it?;;;","20/Jan/22 06:36;gaoyunhaii;Thanks [~roman]  for the PR! LGTM and merged via d33c39d974f08a5ac520f220219ecb0796c9448c. 

Locally the issue is fixed, if it does not happen in the following period I'll close this issue~;;;","25/Jan/22 06:44;gaoyunhaii;Would close this issue since it does not occur. We could reopen it if it re-occur;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PartiallyFinishedSourcesITCase fails due to assertion error in DrainingValidator.validateOperatorLifecycle,FLINK-24162,13399496,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,xtsong,xtsong,06/Sep/21 03:03,07/Sep/21 13:48,13/Jul/23 08:12,07/Sep/21 13:48,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,API / DataStream,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23526&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=4639

{code}
Sep 03 23:17:11 [ERROR] Tests run: 6, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 19.233 s <<< FAILURE! - in org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase
Sep 03 23:17:11 [ERROR] test[simple graph SINGLE_SUBTASK, failover: true]  Time elapsed: 2.27 s  <<< FAILURE!
Sep 03 23:17:11 java.lang.AssertionError
Sep 03 23:17:11 	at org.junit.Assert.fail(Assert.java:87)
Sep 03 23:17:11 	at org.junit.Assert.assertTrue(Assert.java:42)
Sep 03 23:17:11 	at org.junit.Assert.assertFalse(Assert.java:65)
Sep 03 23:17:11 	at org.junit.Assert.assertFalse(Assert.java:75)
Sep 03 23:17:11 	at org.apache.flink.runtime.operators.lifecycle.validation.DrainingValidator.validateOperatorLifecycle(DrainingValidator.java:56)
Sep 03 23:17:11 	at org.apache.flink.runtime.operators.lifecycle.validation.TestOperatorLifecycleValidator.lambda$checkOperatorsLifecycle$1(TestOperatorLifecycleValidator.java:52)
Sep 03 23:17:11 	at java.util.HashMap.forEach(HashMap.java:1289)
Sep 03 23:17:11 	at org.apache.flink.runtime.operators.lifecycle.validation.TestOperatorLifecycleValidator.checkOperatorsLifecycle(TestOperatorLifecycleValidator.java:47)
Sep 03 23:17:11 	at org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase.test(PartiallyFinishedSourcesITCase.java:94)
{code}",,gaoyunhaii,pnowojski,roman,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 07 13:48:58 UTC 2021,,,,,,,,,,"0|z0umvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/21 03:03;xtsong;cc [~roman];;;","06/Sep/21 03:11;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23529&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=4594;;;","06/Sep/21 03:34;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23544&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=4594;;;","06/Sep/21 03:37;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23547&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=4594;;;","06/Sep/21 03:44;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23557&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=4594;;;","06/Sep/21 16:06;gaoyunhaii;I should roughly get the reason: it is due to that the test verifies for each operator, it should only received max watermark and EndOfData once for all the attempts. However, this is in fact not ensured:  if a task get finished first, then get restarted due to some downstream tasks failed, it would still re-emit max watermark and EndOfData. This is required for the downstream tasks to align.

This only happen with adaptive scheduler since now the test is in fact using region failover, with the default scheduler the finish task would not restart in failover. Sorry I do not fully got why with adaptive scheduler the whole job is restarted, but from the log this should indeed happen.

I'll open a PR to fix the issue~;;;","06/Sep/21 16:33;roman;Thanks for looking into it [~gaoyunhaii] .

I can confirm that the task transitions to FINISHED twice: before and after a failure:
{code:java}
23:16:57,760 INFO org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Timestamps/Watermarks -> transform-1-forward -> Sink: Unnamed (1/4)#0 (4a3e2cd18c9e79b42dc8d6624fcbcde8) switched from RUNNING to FINISHED.
...
23:16:57,837 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 3 (type=CHECKPOI NT) @ 1630711017835 for job 3d9486075a07c60f7d6927cff31ab0db.
23:16:57,840 [jobmanager-io-thread-18] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 3 for job 3d94 86075a07c60f7d6927cff31ab0db (0 bytes, checkpointDuration=5 ms, finalizationTime=0 ms).
23:16:57,849 [Source: Custom Source -> Timestamps/Watermarks -> transform-1-forward -> Sink: Unnamed (3/4)#0] WARN  org.apache.flink.runtime.taskm anager.Task                    [] - Source: Custom Source -> Timestamps/Watermarks -> transform-1-forward -> Sink: Unnamed (3/4)#0 (f8498b498d21de 0ce1edd1175a20e5a6) switched from RUNNING to FAILED with failure cause: java.lang.RuntimeException: requested to fail
     at org.apache.flink.runtime.operators.lifecycle.graph.TestEventSource.run(TestEventSource.java:82)
     at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:116)
     at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:73)
     at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:323)
...
23:16:58,357 INFO  org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> Timestamps/Watermarks -> transform-1-forward -> Sink: Unnamed (1/4)#1 (4c131c07267e65d0365a4f2db71f41dc) switched from RUNNING to FINISHED.
{code}
There is a checkpoint (3) that is completed after finishing and is used for recovery.

You're right that the whole job is restarted. However, shouldn't it be always the case because?

TestJobBuilders#prepareEnv sets:
{code:java}
configuration.set(EXECUTION_FAILOVER_STRATEGY, ""full""); {code}
 

 

 ;;;","06/Sep/21 17:03;gaoyunhaii;Hi [~roman]~ I have also noticed the configuration ~ but it did not take effective since it is seemed to be filtered in and not pass to the JM. If we instead set the configuration directly to the minicluster, the test would fail deterministically~  I'm still checking if it is due to we are using TestStreamEnvironment or it is in fact common for all the StreamExecutionEnvironmennt~ ;;;","07/Sep/21 02:09;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23560&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=4639;;;","07/Sep/21 05:58;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23626&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=4639;;;","07/Sep/21 06:02;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23629&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=4639;;;","07/Sep/21 13:48;roman;The fix merged as 
ba749b79ead87e827a48eff95631a148ba7bf291..3c204392e7914822e319323513936f5a053db518 into 1.14
and c9282cf44ef7fb048771427b306cc7b0d84566be..10eecacac1c6362e7dcf9707ae37ff40173f5f95 into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not stop the job with savepoint while a task is finishing,FLINK-24161,13399488,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dwysakowicz,guoyangze,guoyangze,06/Sep/21 02:47,30/Nov/21 20:37,13/Jul/23 08:12,10/Sep/21 12:09,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"When stop the job with savepoint, if there is a task is finishing, the action will be timeout.

Testing job: https://github.com/KarmaGYZ/flink/blob/test-147/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/wordcount/WordCount.java

Flink conf:

{code:bash}
state.savepoints.dir: file:///tmp/flink-savepoints
state.backend: rocksdb
state.backend.incremental: true
state.checkpoints.dir: file:///tmp/flink-ckp/
execution.checkpointing.aligned-checkpoint-timeout: 30 s
execution.checkpointing.interval: 5 s
taskmanager.numberOfTaskSlots: 2
execution.checkpointing.checkpoints-after-tasks-finish.enabled: true
{code}

How to reproduce:

{code:bash}
bin/flink run -d -p 4 examples/streaming/WordCount.jar
# while one task is finishing
bin/flink stop $JOB_ID
{code}

Client log:

{code:bash}
------------------------------------------------------------
 The program finished with the following exception:

org.apache.flink.util.FlinkException: Could not stop with a savepoint job ""e139a2eba7f8dc0b07fab65e84421ee4"".
  at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:581)
  at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:1002)
  at org.apache.flink.client.cli.CliFrontend.stop(CliFrontend.java:569)
  at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1069)
  at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1132)
  at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
  at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1132)
Caused by: java.util.concurrent.TimeoutException
  at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1771)
  at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915)
  at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:579)
  ... 6 more
{code}
",,dwysakowicz,gaoyunhaii,guoyangze,pnowojski,sharp,Thesharing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-2491,,,,FLINK-23532,,,,,,,,,,,,,,,,"06/Sep/21 02:52;guoyangze;flink-yangze-standalonesession-0-IT-C02YV0L8LVDL.local.log;https://issues.apache.org/jira/secure/attachment/13033073/flink-yangze-standalonesession-0-IT-C02YV0L8LVDL.local.log","06/Sep/21 02:52;guoyangze;flink-yangze-taskexecutor-0-IT-C02YV0L8LVDL.local.log;https://issues.apache.org/jira/secure/attachment/13033072/flink-yangze-taskexecutor-0-IT-C02YV0L8LVDL.local.log","06/Sep/21 02:52;guoyangze;flink-yangze-taskexecutor-1-IT-C02YV0L8LVDL.local.log;https://issues.apache.org/jira/secure/attachment/13033071/flink-yangze-taskexecutor-1-IT-C02YV0L8LVDL.local.log",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 10 12:09:02 UTC 2021,,,,,,,,,,"0|z0umu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/21 02:52;guoyangze;cc [~gaoyunhaii];;;","06/Sep/21 06:26;gaoyunhaii;Very thanks [~guoyangze] for verifying the functionality and report the issue!

I confirmed with [~guoyangze] that the issue happens only if we enabled checkpoints after tasks finished (otherwise the savepoint would fail directly due to not all tasks are running). I checked the case and It should be caused by some deadlock if stop-with-savepoint happens right when the task is finishing:

# The source finished and reach `afterInvoke` to wait for the final checkpoint in a mailbox loop.
# The user triggers stop-with-savepoint, which could still succeed for we are still in a mailbox loop. But since it is without drain, it would not complete the `finalCheckpointCompleted`.
# However, when the savepoint is triggered, the CheckpointCoordinator would stop the periodic checkpoint trigger, the final checkpoint would never happen and thus the source would be blocked forever. 
;;;","06/Sep/21 14:37;pnowojski;I've deprioritized the issue for the time being, as it affects only {{execution.checkpointing.checkpoints-after-tasks-finish.enabled: true}};;;","07/Sep/21 06:24;dwysakowicz;I have not thought it entirely through, but from a brief look at the description from Yun, I think once we unify the with and w/o drain versions (FLINK-23532) the problem should go away.;;;","07/Sep/21 07:01;pnowojski;That might be a solution for 1.15.x, but not for 1.14.x. I would prefer to not risk changing the behaviour of the stop-with-savepoint. So unless the unification is simple/trivial, I would try to solve this for 1.14.x only by modifying the {{execution.checkpointing.checkpoints-after-tasks-finish.enabled: true}} code ;;;","07/Sep/21 12:02;gaoyunhaii;Perhaps for 1.14 we could first disable stop-with-savepoint without drain if the tasks are finishing ? We could fail the job if checkpoint after tasks finished is enabled and:
 # In {{endData()}} if there is an pending savepoint without drain, we fail the task by throw an exception.
 # When triggering stop-with-savepoint without drain, if {{endOfDataReceived = true}} , we then return false to reject the savepoint.

Both of the above 2 cases cause a failover. 

We could then provide the support for this case in 1.15 after we unified the process of savepoint~;;;","10/Sep/21 12:09;dwysakowicz;Fixed in:
* master
** f5ef5ae8f120f3e70ff3fe093fb4c45e92beaabd
* 1.14.0
** df3b7235699c44923966a2ba733576c6a61d3d3a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PartiallyFinishedSourcesITCase fails due to 'Size of the state is larger than the maximum permitted memory-backed state.',FLINK-24160,13399487,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,xtsong,xtsong,06/Sep/21 02:47,08/Sep/21 16:09,13/Jul/23 08:12,08/Sep/21 16:09,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,API / DataStream,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23526&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=5526

{code}
Sep 04 02:31:13 Caused by: java.io.IOException: Size of the state is larger than the maximum permitted memory-backed state. Size=6281708, maxSize=5242880. Consider using a different checkpoint storage, like the FileSystemCheckpointStorage.
Sep 04 02:31:13 	at org.apache.flink.runtime.state.memory.MemCheckpointStreamFactory.checkSize(MemCheckpointStreamFactory.java:63)
Sep 04 02:31:13 	at org.apache.flink.runtime.state.memory.MemCheckpointStreamFactory$MemoryCheckpointOutputStream.closeAndGetBytes(MemCheckpointStreamFactory.java:140)
Sep 04 02:31:13 	at org.apache.flink.runtime.state.memory.MemCheckpointStreamFactory$MemoryCheckpointOutputStream.closeAndGetHandle(MemCheckpointStreamFactory.java:120)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.finishWriteAndResult(ChannelStateCheckpointWriter.java:218)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.doComplete(ChannelStateCheckpointWriter.java:240)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.lambda$complete$5(ChannelStateCheckpointWriter.java:202)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.runWithChecks(ChannelStateCheckpointWriter.java:296)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.complete(ChannelStateCheckpointWriter.java:200)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.completeInput(ChannelStateCheckpointWriter.java:187)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.CheckpointInProgressRequest.execute(ChannelStateWriteRequest.java:212)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatchInternal(ChannelStateWriteRequestDispatcherImpl.java:85)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatch(ChannelStateWriteRequestDispatcherImpl.java:62)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.loop(ChannelStateWriteRequestExecutorImpl.java:96)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.run(ChannelStateWriteRequestExecutorImpl.java:75)
Sep 04 02:31:13 	... 1 more
{code}",,gaoyunhaii,roman,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24163,FLINK-24202,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 08 16:08:57 UTC 2021,,,,,,,,,,"0|z0umts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/21 02:56;xtsong;[~roman], is this necessary that the testing job needs more than the default 5MB memory for states?;;;","06/Sep/21 20:28;roman;It depends on the configuration, with unaligned checkpoints channel state exceeds 5Mb.

I opened a PR to use FileSystem checkpointing storage in test.;;;","08/Sep/21 16:08;gaoyunhaii;Fixed on master via c740f9269ab2836a69cc3fac9dc4a670f6aa86d2
Fixed on release-1.14 via b56041b9acc8fafa2ab1500e9162bed7bbcb333d;;;","08/Sep/21 16:08;gaoyunhaii;I'll close the issue for now, we could reopen it if it occured~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BlobServer crashes due to SocketTimeoutException in Java 11,FLINK-24156,13399325,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,scudellari,scudellari,scudellari,03/Sep/21 19:27,02/Mar/23 10:01,13/Jul/23 08:12,20/Sep/21 08:48,1.12.4,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,"h3. Overview

We have seen the BlobServer crash due to a *SocketTimeoutException* while running on JRE 11. This is likely caused by a [JDK bug present in JDK 11|https://bugs.openjdk.java.net/browse/JDK-8237858] (fixed in version 16) that erroneously throws _SocketTimeoutException_ when _ServerSocket.accept()_ is interrupted by any UNIX signal. The BlobServer calls _accept()_ when establishing connections with clients and is expected to block indefinitely. [The BlobServer currently shuts down when it catches a Throwable|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobServer.java#L267]. We do not see this behavior when running the same steps in JRE 8.
h3. Reproducing the issue

To reproduce, send a _SIGPIPE_ to the BlobServer _PID_. You will need to be running a Flink cluster on JRE 11 and have tools _jps_ and _jstack_ available to find the relevant pid.

One-liner:
{code:bash}
kill -SIGPIPE $(jstack $(jps -v | grep StandaloneApplicationClusterEntryPoint | cut -f 1 -d ' ') | grep BLOB | awk '\{print $13}' | awk -F'[=]' '\{print $2}' | xargs printf ""%d"")
{code}
 
 # Run
{code:bash}
jstack [PID] | grep BLOB
{code}
where *PID* is the process ID of the job manager.

 # Find the *nid=[HEX]* value and convert the HEX to decimal.
 # Run
{code:bash}
kill -SIGPIPE [DNID]
{code}
where *DNID* is the converted decimal value of *HEX nid* from the previous step.

 # Observe the following error in the job manager logs:
{noformat}
2021-09-03 09:56:12.517 [BLOB Server listener at 6124] ERROR org.apache.flink.runtime.blob.BlobServer  - BLOB server stopped working. Shutting down
  at java.base/java.net.PlainSocketImpl.socketAccept
  at java.base/java.net.AbstractPlainSocketImpl.accept
       at java.base/java.net.ServerSocket.implAccept
  at java.base/java.net.ServerSocket.accept
  at org.apache.flink.runtime.blob.BlobServer.run(BlobServer.java:266)
2021-09-03 09:56:12.527 [BLOB Server listener at 6124] INFO  org.apache.flink.runtime.blob.BlobServer  - Stopped BLOB server at 0.0.0.0:6124
{noformat}

h3. Proposed Fix

To protect ourselves from this JDK bug, we propose the workaround of catching _SocketTimeoutException_ and retrying the _ServerSocket.accept()_ call indefinitely.

 

Thanks to [~bsanders-wf] for helping track this down.","Java 11

CentOS 7.6",aitozi,scudellari,Thesharing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25316,FLINK-31298,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 30 08:06:21 UTC 2021,,,,,,,,,,"0|z0ults:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/21 12:32;chesnay;This sounds reasonable. [~scudellari] Would you like to provide a fix for the issue?

I guess all we need is a new utility method in the {{NetUtils}} to handle this case, to which we migrate all usages of {{ServerSocker#accept()}}.;;;","06/Sep/21 15:21;scudellari;Yes, I can provide a fix.

A utility method in {{NetUtils}} sounds good. Should I migrate the {{ServerSocker#accept()}} calls in the integration tests or can we assume they are only ever run using Java 8?;;;","06/Sep/21 17:27;chesnay;We should also migrate tests, because they are also run on Java 11 and for consistency.;;;","06/Sep/21 17:31;scudellari;(y);;;","15/Sep/21 14:56;scudellari;[~chesnay] [ https://github.com/apache/flink/pull/17227|https://github.com/apache/flink/pull/17227] should be ready for review. I believe I have addressed all comments. Let me know if you have questions!;;;","20/Sep/21 08:48;chesnay;master: 5ae8cb0503449b07f76d0ab621c3e81734496b26
1.14: 685bd8f9b917d567f5876a58355b9a947614806a ;;;","29/Sep/21 19:32;scudellari;[~chesnay] - would it be possible to backport this into the `1.14.x` line? I see that `1.14.0` was just released. What are the policies around including these sorts of fixes in a patch release?;;;","30/Sep/21 08:06;chesnay;For this change in particular I would like to give it some more time on master to see if anyone runs into issues.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to use SET command in Flink SQL Shell file submit mode,FLINK-24153,13399249,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,Nj-kol,Nj-kol,03/Sep/21 13:05,03/Sep/21 15:04,13/Jul/23 08:12,03/Sep/21 14:50,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,,,,,Table SQL / Client,,,,,0,,,,,"Let's say you created a Flink SQL job in a file called *sample.sql*

 
{code:java}
-- Properties that change the fundamental execution behavior of a table program.
SET 'execution.runtime-mode' = 'streaming';
SET 'sql-client.execution.result-mode' = 'tableau';
SET 'sql-client.execution.max-table-result.rows' = '10000';
SET 'parallelism.default' = '1';

-- Source table
CREATE TEMPORARY TABLE salesitems (
    seller_id VARCHAR,
    product VARCHAR,
    quantity INT,
    product_price DOUBLE,
    sale_ts BIGINT,
    proctime AS PROCTIME()
) WITH (
    'connector' = 'kafka',
    'topic' = 'salesitems',
    'properties.bootstrap.servers' = '10.129.35.58:29092',
    'properties.group.id' = 'sliding-windows',
    'scan.startup.mode' = 'latest-offset',
    'format' = 'json'
);

SELECT
  seller_id,
  HOP_START(proctime, INTERVAL '10' SECONDS, INTERVAL '30' SECONDS) AS window_start,
  HOP_END(proctime, INTERVAL '10' SECONDS, INTERVAL '30' SECONDS) AS window_end,
  SUM(quantity * product_price) AS window_sales
FROM salesitems
GROUP BY
  HOP(proctime, INTERVAL '10' SECONDS, INTERVAL '30' SECONDS),
  seller_id;
{code}
Now, if you submit the job as -
{code:java}
${FLINK_HOME}/bin/sql-client.sh embedded \
--jar ${FLINK_HOME}/lib/flink-sql-connector-kafka_2.11-1.13.1.jar \
-f sample.sql{code}
The error you get is -
{code:java}
[INFO] Executing SQL from file.
Flink SQL> -- Properties that change the fundamental execution behavior of a table program.
SET 'execution.runtime-mode' = 'streaming';
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.sql.parser.impl.ParseException: Encountered ""\'execution.runtime-mode\'"" at line 2, column 5.
Was expecting one of:
<BRACKET_QUOTED_IDENTIFIER> ...
<QUOTED_IDENTIFIER> ...
<BACK_QUOTED_IDENTIFIER> ...
<HYPHENATED_IDENTIFIER> ...
<IDENTIFIER> ...
<UNICODE_QUOTED_IDENTIFIER> ...
{code}
 ",,airblader,jark,Nj-kol,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 03 15:04:15 UTC 2021,,,,,,,,,,"0|z0ulcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Sep/21 13:17;airblader;In Flink 1.13.1 the SET and RESET commands cannot be used with quotes, this was only introduced in Flink 1.13.2. You need to use
{code:java}
SET execution.runtime-mode = streaming;
{code}
[~twalthr] Another duplicate of this issue, we should revert the docs changes for 1.13;;;","03/Sep/21 13:35;twalthr;I will prepare a PR for reverting 320ed880513377acc622daf7764a9be70e7704f0 did we have more commits around this topic?;;;","03/Sep/21 13:38;Nj-kol;[~airblader] I have checked that as well. It does not work either. Only, the error changes 

cc: [~twalthr]

If I change the above code to -
{code:java}
SET execution.runtime-mode = streaming;
SET sql-client.execution.result-mode = tableau;
SET sql-client.execution.max-table-result.rows = 10000;
SET parallelism.default = 1;{code}
The error becomes -
{code:java}
SET execution.runtime-mode = streaming;
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.sql.parser.impl.ParseException: Encountered ""-"" at line 2, column 22.
Was expecting one of:
 ""="" ...
 ""."" ...{code};;;","03/Sep/21 14:33;jark;[~twalthr] [~airblader] +1 to revert the doc changes for release-1.13;;;","03/Sep/21 14:38;jark;[~Nj-kol] please do not copy and paste the whole SET statements. Please execute them one by one. This is also a limitation in 1.13.1. ;;;","03/Sep/21 14:50;twalthr;Reverted in 1.13: c6863d184e0386ea69799a03f67dc218ae2a516c;;;","03/Sep/21 15:04;Nj-kol;[~jark] Yes, pasting the SET statements one by one works fine. However, this is an issue for submitting long-running jobs. If the long term goal is to be able to write a streaming job as a hive file, then this could pose an impediment as for production systems, you'd like to fine-tune your jobs(and stuff like version control the Flink SQL file)

Overall, I think this is a great job done by the Flink community in realizing this erstwhile impossible dream of having a streaming pipeline as a SQL query. 

I am hopeful this final gap will be closed in the near future, which will make Flink the platform of choice for building streaming pipleines;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Python tests fail with ""Exception in thread read_grpc_client_inputs""",FLINK-24137,13399091,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dianfu,dwysakowicz,dwysakowicz,03/Sep/21 06:35,14/Sep/21 02:06,13/Jul/23 08:12,14/Sep/21 02:06,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,API / Python,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23443&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=24681

{code}
Sep 01 02:26:21 E                   Caused by: java.lang.RuntimeException: Failed to create stage bundle factory! INFO:root:Initializing python harness: /__w/1/s/flink-python/pyflink/fn_execution/beam/beam_boot.py --id=1-1 --provision_endpoint=localhost:44544
Sep 01 02:26:21 E                   
Sep 01 02:26:21 E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.createStageBundleFactory(BeamPythonFunctionRunner.java:566)
Sep 01 02:26:21 E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.open(BeamPythonFunctionRunner.java:255)
Sep 01 02:26:21 E                   	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.open(AbstractPythonFunctionOperator.java:131)
Sep 01 02:26:21 E                   	at org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator.open(AbstractOneInputPythonFunctionOperator.java:116)
Sep 01 02:26:21 E                   	at org.apache.flink.streaming.api.operators.python.PythonProcessOperator.open(PythonProcessOperator.java:59)
Sep 01 02:26:21 E                   	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:110)
Sep 01 02:26:21 E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:691)
Sep 01 02:26:21 E                   	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
Sep 01 02:26:21 E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:667)
Sep 01 02:26:21 E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:639)
Sep 01 02:26:21 E                   	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
Sep 01 02:26:21 E                   	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
Sep 01 02:26:21 E                   	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
Sep 01 02:26:21 E                   	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
Sep 01 02:26:21 E                   	at java.lang.Thread.run(Thread.java:748)
Sep 01 02:26:21 E                   Caused by: org.apache.beam.vendor.guava.v26_0_jre.com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Process died with exit code 0
Sep 01 02:26:21 E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2050)
Sep 01 02:26:21 E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache.get(LocalCache.java:3952)
Sep 01 02:26:21 E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3974)
Sep 01 02:26:21 E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4958)
Sep 01 02:26:21 E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4964)
Sep 01 02:26:21 E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory.<init>(DefaultJobBundleFactory.java:451)
Sep 01 02:26:21 E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory.<init>(DefaultJobBundleFactory.java:436)
Sep 01 02:26:21 E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory.forStage(DefaultJobBundleFactory.java:303)
Sep 01 02:26:21 E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.createStageBundleFactory(BeamPythonFunctionRunner.java:564)
Sep 01 02:26:21 E                   	... 14 more
Sep 01 02:26:21 E                   Caused by: java.lang.IllegalStateException: Process died with exit code 0
Sep 01 02:26:21 E                   	at org.apache.beam.runners.fnexecution.environment.ProcessManager$RunningProcess.isAliveOrThrow(ProcessManager.java:75)
Sep 01 02:26:21 E                   	at org.apache.beam.runners.fnexecution.environment.ProcessEnvironmentFactory.createEnvironment(ProcessEnvironmentFactory.java:112)
Sep 01 02:26:21 E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$1.load(DefaultJobBundleFactory.java:252)
Sep 01 02:26:21 E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$1.load(DefaultJobBundleFactory.java:231)
Sep 01 02:26:21 E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3528)
Sep 01 02:26:21 E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2277)
Sep 01 02:26:21 E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2154)
Sep 01 02:26:21 E                   	at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2044)
{code}",,dianfu,dwysakowicz,hxbks2ks,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24245,,,,,,FLINK-24100,,,,,,,,,,,,,,,FLINK-24100,FLINK-24123,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 14 02:06:14 UTC 2021,,,,,,,,,,"0|z0ukds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/21 02:39;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23526&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=22488;;;","06/Sep/21 02:43;xtsong;FLINK-24100 and FLINK-24123 seem caused by the same problem.;;;","06/Sep/21 02:43;xtsong;cc [~dianfu][~hxbks2ks];;;","06/Sep/21 02:49;hxbks2ks;It is the same problem with FLINK-24100 failed in initializing python process, and FLINK-24123 is another problem.;;;","06/Sep/21 03:10;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23529&view=logs&j=ff2e2ea5-07e3-5521-7b04-a4fc3ad765e9&t=1ec6382b-bafe-5817-63ae-eda7d4be718e&l=23008;;;","06/Sep/21 03:41;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23557&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=22396;;;","06/Sep/21 03:55;xtsong;Thanks [~hxbks2ks]. Shall I assign this to you?;;;","06/Sep/21 06:03;dianfu;[~xtsong] I will take a look at this issue. I agree with [~hxbks2ks] that this is the same problem as FLINK-24100 and so I will close FLINK-24100 and try to fix this problem in this ticket since there are more instances and discussions here.;;;","07/Sep/21 02:12;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23578&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=22354;;;","07/Sep/21 03:44;dianfu;I have added some logs and triggered the tests more than 50 times in my personal azure pipeline. It's a pity that haven't reproduced this issue for now. :(

Concurrently the JM/TM logs wasn't uploaded for Python tests. It makes it difficult to figure out what happens for the failed tests. I will submit a PR soon to enable the JM/TM log uploading for Python tests.;;;","07/Sep/21 05:52;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23626&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=23142;;;","07/Sep/21 10:23;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23646&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=21929;;;","08/Sep/21 01:45;dianfu;Enable uploading JM/TM logs for Python tests:
- master: 5565590dc27a77330127cdc027d83ccd10400bed
- release-1.14: 6b4abd5af4f40d8496df7c8b5984fe5ffa00c902;;;","08/Sep/21 05:49;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23721&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=23146;;;","09/Sep/21 06:44;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23805&view=logs&j=bdd9ea51-4de2-506a-d4d9-f3930e4d2355&t=dd50312f-73b5-56b5-c172-4d81d03e2ef1&l=23602;;;","09/Sep/21 12:27;dianfu;Fixed in:
- master via 7a1b2b39c0f5d7b6651a349db8bcb51cc6c86115 and 0b4855f6b388f260230a62f95e074a318fb62a4d
- release-1.14 via 4d131d547fd510a08d15557995edc19078cd26e7 and 2db50d63a5e4a97868ef2c6d05163e78c85db21c;;;","13/Sep/21 03:40;xtsong;[~dianfu]
It looks like the problem still exist.
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23955&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=23820;;;","13/Sep/21 09:05;dianfu;[~xtsong] Thanks for reporting the new failed instance. After investigating the new instance, I think it should be fixed as part of the fixing in FLINK-24245. It's already been merged hours ago. Let's see if there are still failures.;;;","14/Sep/21 02:06;xtsong;Thanks [~dianfu]. I'm closing this ticket for now. We can reopen it if the problem happens again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Docker ""Enabling Python"" documentation results in failed Docker builds",FLINK-24134,13398981,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,dianfu,adamhp,adamhp,02/Sep/21 15:54,15/Dec/21 01:40,13/Jul/23 08:12,06/Sep/21 05:30,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.0,,,Documentation,,,,,0,pull-request-available,,,,"Following the instructions defined here: [https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/deployment/resource-providers/standalone/docker/#enabling-python]  

When running `docker build .` with that example, I get the follow error:
{code:java}
#5 0.249 Get:1 http://deb.debian.org/debian bullseye InRelease [113 kB]
#5 0.251 Get:2 http://security.debian.org/debian-security bullseye-security InRelease [44.1 kB]
#5 0.271 Get:3 http://deb.debian.org/debian bullseye-updates InRelease [36.8 kB]
#5 0.361 Get:4 http://security.debian.org/debian-security bullseye-security/main amd64 Packages [29.4 kB]
#5 0.455 Get:5 http://deb.debian.org/debian bullseye/main amd64 Packages [8178 kB]
#5 1.466 Fetched 8401 kB in 1s (6707 kB/s)
#5 1.466 Reading package lists...
#5 1.893 Reading package lists...
#5 2.300 Building dependency tree...
#5 2.416 Reading state information...
#5 2.493 Package python3.7 is not available, but is referred to by another package.
#5 2.493 This may mean that the package is missing, has been obsoleted, or
#5 2.493 is only available from another source
#5 2.493
#5 2.545 E: Package 'python3.7' has no installation candidate
#5 2.545 E: Unable to locate package python3.7-dev
#5 2.545 E: Couldn't find any package by glob 'python3.7-dev'
#5 2.545 E: Couldn't find any package by regex 'python3.7-dev'{code}
It could be that there is a different way to install python3.7 for Debian? 

Dockerfile reference: 
{code:java}
FROM flink:latest
# install python3 and pip3
RUN apt-get update -y && \
apt-get install -y python3.7 python3-pip python3.7-dev && rm -rf /var/lib/apt/lists/*
RUN ln -s /usr/bin/python3 /usr/bin/python
# install Python Flink
RUN pip3 install apache-flink
{code}
 

 

More context:

I am attempting to establish a basic example pipeline with a Python WordCount example, using the FlinkRunner deployed to Docker via Docker Compose. ","* macOS Big Sur 11.5.2
 * Docker version 20.10.8, build 3967b7d",adamhp,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23984,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 06 05:30:39 UTC 2021,,,,,,,,,,"0|z0ujpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Sep/21 01:58;dianfu;[~adamhp] Good catch, the documentation should be updated as of the reason found in FLINK-23984. ;;;","06/Sep/21 05:30;dianfu;Merged to:
- master via 7bee34d69f386e86616696c56029e21597dca61e
- release-1.14 via f6460e9341a155459541dbaee331ffd751dd6f7e
- release-1.13 via 649213ca823bfec89cfbb8f55d6c7a7aa7824903
- release-1.12 via d80269373b4ebf7ff36c0d7b96000cf23073504f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"KafkaSinkITCase ""Detected producer leak""",FLINK-24131,13398963,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,dwysakowicz,dwysakowicz,02/Sep/21 14:19,17/Nov/21 15:21,13/Jul/23 08:12,07/Sep/21 12:09,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,Connectors / Kafka,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23383&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7627

{code}
Sep 02 10:35:14 
Sep 02 10:35:14 [ERROR] testRecoveryWithAtLeastOnceGuarantee  Time elapsed: 4.322 s  <<< FAILURE!
Sep 02 10:35:14 java.lang.AssertionError: Detected producer leak. Thread name: kafka-producer-network-thread | producer-kafka-sink-2-2
Sep 02 10:35:14 	at org.junit.Assert.fail(Assert.java:89)
Sep 02 10:35:14 	at org.apache.flink.connector.kafka.sink.KafkaSinkITCase.checkProducerLeak(KafkaSinkITCase.java:594)
Sep 02 10:35:14 	at org.apache.flink.connector.kafka.sink.KafkaSinkITCase.testRecoveryWithAssertion(KafkaSinkITCase.java:321)
Sep 02 10:35:14 	at org.apache.flink.connector.kafka.sink.KafkaSinkITCase.testRecoveryWithAtLeastOnceGuarantee(KafkaSinkITCase.java:190)
Sep 02 10:35:14 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Sep 02 10:35:14 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Sep 02 10:35:14 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Sep 02 10:35:14 	at java.lang.reflect.Method.invoke(Method.java:498)
Sep 02 10:35:14 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Sep 02 10:35:14 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Sep 02 10:35:14 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Sep 02 10:35:14 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Sep 02 10:35:14 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Sep 02 10:35:14 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Sep 02 10:35:14 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Sep 02 10:35:14 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Sep 02 10:35:14 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Sep 02 10:35:14 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Sep 02 10:35:14 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Sep 02 10:35:14 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Sep 02 10:35:14 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Sep 02 10:35:14 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Sep 02 10:35:14 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Sep 02 10:35:14 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Sep 02 10:35:14 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Sep 02 10:35:14 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Sep 02 10:35:14 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Sep 02 10:35:14 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Sep 02 10:35:14 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Sep 02 10:35:14 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Sep 02 10:35:14 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
Sep 02 10:35:14 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Sep 02 10:35:14 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Sep 02 10:35:14 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Sep 02 10:35:14 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Sep 02 10:35:14 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Sep 02 10:35:14 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
Sep 02 10:35:14 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Sep 02 10:35:14 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Sep 02 10:35:14 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Sep 02 10:35:14 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
Sep 02 10:35:14 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Sep 02 10:35:14 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Sep 02 10:35:14 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Sep 02 10:35:14 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Sep 02 10:35:14 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Sep 02 10:35:14 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
Sep 02 10:35:14 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
Sep 02 10:35:14 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
Sep 02 10:35:14 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
Sep 02 10:35:14 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
Sep 02 10:35:14 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
Sep 02 10:35:14 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
Sep 02 10:35:14 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
Sep 02 10:35:14 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
Sep 02 10:35:14 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
Sep 02 10:35:14 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Sep 02 10:35:14 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Sep 02 10:35:14 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Sep 02 10:35:14 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

{code}",,dwysakowicz,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24151,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 07 20:48:37 UTC 2021,,,,,,,,,,"0|z0ujlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Sep/21 05:24;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23440&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=ffa8837a-b445-534e-cdf4-db364cf8235d&l=7369;;;","03/Sep/21 07:29;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23459&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7662;;;","03/Sep/21 07:30;dwysakowicz;cc [~fpaul] [~arvid];;;","06/Sep/21 03:11;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23529&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=ffa8837a-b445-534e-cdf4-db364cf8235d&l=7369;;;","06/Sep/21 03:36;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23547&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=f7d83ad5-3324-5307-0eb3-819065cdcb38&l=8405;;;","06/Sep/21 03:39;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23559&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19&l=6635;;;","06/Sep/21 09:27;arvid;The last error is coming from a different test. Neither the test nor the code under test is present in release-1.13.;;;","07/Sep/21 12:09;arvid;Merged into master as da0e7dd05f486fef8922f6677d25e1fe00ea90a5 .. 85684e4aa91208a7cc06f1425bc5b7d375a48637.
Merged into 1.14 as 39d303f7a2a865eb6c1dcf3b513dc38b10aff609 .. 510c2fc09ebb9d638d258bb51ddb21ccaddb1a77.

Also raised FLINK-24182 which caused the leaks in the first place.;;;","07/Sep/21 19:43;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23710&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7747;;;","07/Sep/21 20:48;arvid;Again wrong ticket, probably FLINK-22056 or even FLINK-18634.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RowDataSerializerTest fails on Azure,FLINK-24130,13398959,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,twalthr,dwysakowicz,dwysakowicz,02/Sep/21 14:14,30/Sep/21 11:50,13/Jul/23 08:12,30/Sep/21 11:50,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,API / Python,Table SQL / API,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23376&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=30176

{code}
Sep 02 10:12:18 [ERROR] testSerializedCopyAsSequence  Time elapsed: 0.009 s  <<< FAILURE!
Sep 02 10:12:18 java.lang.AssertionError: Exception in test: Row arity of input element does not match serializers.
Sep 02 10:12:18 	at org.junit.Assert.fail(Assert.java:89)
Sep 02 10:12:18 	at org.apache.flink.api.common.typeutils.SerializerTestBase.testSerializedCopyAsSequence(SerializerTestBase.java:429)
Sep 02 10:12:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Sep 02 10:12:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Sep 02 10:12:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Sep 02 10:12:18 	at java.lang.reflect.Method.invoke(Method.java:498)
Sep 02 10:12:18 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Sep 02 10:12:18 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Sep 02 10:12:18 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Sep 02 10:12:18 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Sep 02 10:12:18 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Sep 02 10:12:18 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Sep 02 10:12:18 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Sep 02 10:12:18 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Sep 02 10:12:18 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Sep 02 10:12:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Sep 02 10:12:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Sep 02 10:12:18 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Sep 02 10:12:18 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Sep 02 10:12:18 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Sep 02 10:12:18 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Sep 02 10:12:18 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Sep 02 10:12:18 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Sep 02 10:12:18 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Sep 02 10:12:18 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Sep 02 10:12:18 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Sep 02 10:12:18 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
Sep 02 10:12:18 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Sep 02 10:12:18 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Sep 02 10:12:18 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Sep 02 10:12:18 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
Sep 02 10:12:18 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Sep 02 10:12:18 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Sep 02 10:12:18 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Sep 02 10:12:18 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Sep 02 10:12:18 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Sep 02 10:12:18 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
Sep 02 10:12:18 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
Sep 02 10:12:18 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
Sep 02 10:12:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
Sep 02 10:12:18 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
Sep 02 10:12:18 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
Sep 02 10:12:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
Sep 02 10:12:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
Sep 02 10:12:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
Sep 02 10:12:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:116)
Sep 02 10:12:18 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Sep 02 10:12:18 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Sep 02 10:12:18 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Sep 02 10:12:18 	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.toBinaryRow(RowDataSerializer.java:199)
Sep 02 10:12:18 	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:103)
Sep 02 10:12:18 	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:48)
Sep 02 10:12:18 	at org.apache.flink.api.common.typeutils.SerializerTestBase$SerializerRunner.run(SerializerTestBase.java:580)

{code}",,dwysakowicz,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 30 11:50:58 UTC 2021,,,,,,,,,,"0|z0ujko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Sep/21 14:15;dwysakowicz;Could it be caused by FLINK-24054? [~twalthr];;;","02/Sep/21 14:15;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23381&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=30493;;;","02/Sep/21 14:16;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23377&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=30493;;;","02/Sep/21 14:17;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23383&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=30545;;;","02/Sep/21 14:21;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23387&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=30223;;;","02/Sep/21 14:56;twalthr;Fixed in master: e5b88a311ff1bc94b25e5c8c29c309940331f43c
Fixed in 1.14: ee0517145ce86300d88b413724c4240d1508085d;;;","30/Sep/21 11:50;twalthr;Fixed in 1.13: b0bf6687ac002dc87182dbd01a5fe47709ce0296;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TopicRangeTest.rangeCreationHaveALimitedScope fails on Azure,FLINK-24129,13398950,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,trohrmann,trohrmann,02/Sep/21 13:48,17/Sep/21 17:56,13/Jul/23 08:12,17/Sep/21 17:56,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,Connectors / Pulsar,,,,,0,pull-request-available,test-stability,,,"The test {{TopicRangeTest.rangeCreationHaveALimitedScope}} fails on Azure with

{code}
[ERROR] Tests run: 11, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.041 s <<< FAILURE! - in org.apache.flink.connector.pulsar.source.enumerator.topic.TopicRangeTest
2021-09-02T12:41:55.9478399Z Sep 02 12:41:55 [ERROR] rangeCreationHaveALimitedScope[4]  Time elapsed: 0.025 s  <<< FAILURE!
2021-09-02T12:41:55.9478983Z Sep 02 12:41:55 org.opentest4j.AssertionFailedError: Expected java.lang.IllegalArgumentException to be thrown, but nothing was thrown.
2021-09-02T12:41:55.9479519Z Sep 02 12:41:55 	at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:71)
2021-09-02T12:41:55.9479983Z Sep 02 12:41:55 	at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:37)
2021-09-02T12:41:55.9480449Z Sep 02 12:41:55 	at org.junit.jupiter.api.Assertions.assertThrows(Assertions.java:3007)
2021-09-02T12:41:55.9481013Z Sep 02 12:41:55 	at org.apache.flink.connector.pulsar.source.enumerator.topic.TopicRangeTest.rangeCreationHaveALimitedScope(TopicRangeTest.java:44)
2021-09-02T12:41:55.9482349Z Sep 02 12:41:55 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-09-02T12:41:55.9483361Z Sep 02 12:41:55 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-09-02T12:41:55.9483969Z Sep 02 12:41:55 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-09-02T12:41:55.9484594Z Sep 02 12:41:55 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-09-02T12:41:55.9485051Z Sep 02 12:41:55 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)
2021-09-02T12:41:55.9485595Z Sep 02 12:41:55 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2021-09-02T12:41:55.9486194Z Sep 02 12:41:55 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2021-09-02T12:41:55.9486952Z Sep 02 12:41:55 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2021-09-02T12:41:55.9487837Z Sep 02 12:41:55 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2021-09-02T12:41:55.9488774Z Sep 02 12:41:55 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)
2021-09-02T12:41:55.9489775Z Sep 02 12:41:55 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2021-09-02T12:41:55.9490737Z Sep 02 12:41:55 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2021-09-02T12:41:55.9491693Z Sep 02 12:41:55 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2021-09-02T12:41:55.9492584Z Sep 02 12:41:55 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2021-09-02T12:41:55.9493353Z Sep 02 12:41:55 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2021-09-02T12:41:55.9493957Z Sep 02 12:41:55 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2021-09-02T12:41:55.9494608Z Sep 02 12:41:55 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2021-09-02T12:41:55.9495132Z Sep 02 12:41:55 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2021-09-02T12:41:55.9495735Z Sep 02 12:41:55 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)
2021-09-02T12:41:55.9496357Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-09-02T12:41:55.9497188Z Sep 02 12:41:55 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)
2021-09-02T12:41:55.9497785Z Sep 02 12:41:55 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)
2021-09-02T12:41:55.9498381Z Sep 02 12:41:55 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)
2021-09-02T12:41:55.9499072Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
2021-09-02T12:41:55.9499682Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-09-02T12:41:55.9500273Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2021-09-02T12:41:55.9500827Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-09-02T12:41:55.9501367Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2021-09-02T12:41:55.9501978Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-09-02T12:41:55.9502558Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2021-09-02T12:41:55.9503199Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2021-09-02T12:41:55.9503836Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
2021-09-02T12:41:55.9504607Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:212)
2021-09-02T12:41:55.9505263Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:192)
2021-09-02T12:41:55.9505877Z Sep 02 12:41:55 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)
2021-09-02T12:41:55.9506514Z Sep 02 12:41:55 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)
2021-09-02T12:41:55.9507070Z Sep 02 12:41:55 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2021-09-02T12:41:55.9507555Z Sep 02 12:41:55 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2021-09-02T12:41:55.9508024Z Sep 02 12:41:55 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
2021-09-02T12:41:55.9508517Z Sep 02 12:41:55 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2021-09-02T12:41:55.9508986Z Sep 02 12:41:55 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2021-09-02T12:41:55.9509460Z Sep 02 12:41:55 	at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250)
2021-09-02T12:41:55.9509929Z Sep 02 12:41:55 	at java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:110)
2021-09-02T12:41:55.9510413Z Sep 02 12:41:55 	at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693)
2021-09-02T12:41:55.9510871Z Sep 02 12:41:55 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2021-09-02T12:41:55.9511340Z Sep 02 12:41:55 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2021-09-02T12:41:55.9511813Z Sep 02 12:41:55 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2021-09-02T12:41:55.9512328Z Sep 02 12:41:55 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2021-09-02T12:41:55.9512916Z Sep 02 12:41:55 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2021-09-02T12:41:55.9513486Z Sep 02 12:41:55 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2021-09-02T12:41:55.9514129Z Sep 02 12:41:55 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2021-09-02T12:41:55.9515028Z Sep 02 12:41:55 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2021-09-02T12:41:55.9515934Z Sep 02 12:41:55 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2021-09-02T12:41:55.9516590Z Sep 02 12:41:55 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2021-09-02T12:41:55.9517077Z Sep 02 12:41:55 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2021-09-02T12:41:55.9517593Z Sep 02 12:41:55 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2021-09-02T12:41:55.9518074Z Sep 02 12:41:55 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2021-09-02T12:41:55.9518548Z Sep 02 12:41:55 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2021-09-02T12:41:55.9519075Z Sep 02 12:41:55 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)
2021-09-02T12:41:55.9519687Z Sep 02 12:41:55 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)
2021-09-02T12:41:55.9520292Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
2021-09-02T12:41:55.9520901Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-09-02T12:41:55.9521499Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2021-09-02T12:41:55.9522062Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-09-02T12:41:55.9522715Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2021-09-02T12:41:55.9523332Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-09-02T12:41:55.9523909Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2021-09-02T12:41:55.9524537Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2021-09-02T12:41:55.9525073Z Sep 02 12:41:55 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2021-09-02T12:41:55.9525644Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
2021-09-02T12:41:55.9526348Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
2021-09-02T12:41:55.9526957Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-09-02T12:41:55.9527567Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2021-09-02T12:41:55.9528093Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-09-02T12:41:55.9528657Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2021-09-02T12:41:55.9529254Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-09-02T12:41:55.9529841Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2021-09-02T12:41:55.9530399Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2021-09-02T12:41:55.9531027Z Sep 02 12:41:55 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2021-09-02T12:41:55.9531610Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
2021-09-02T12:41:55.9532320Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
2021-09-02T12:41:55.9533058Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-09-02T12:41:55.9533669Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2021-09-02T12:41:55.9534282Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-09-02T12:41:55.9534844Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2021-09-02T12:41:55.9535450Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-09-02T12:41:55.9536040Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2021-09-02T12:41:55.9536595Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2021-09-02T12:41:55.9537248Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
2021-09-02T12:41:55.9537934Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
2021-09-02T12:41:55.9538560Z Sep 02 12:41:55 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)
2021-09-02T12:41:55.9539139Z Sep 02 12:41:55 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
2021-09-02T12:41:55.9539665Z Sep 02 12:41:55 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
2021-09-02T12:41:55.9540229Z Sep 02 12:41:55 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
2021-09-02T12:41:55.9540765Z Sep 02 12:41:55 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
2021-09-02T12:41:55.9541282Z Sep 02 12:41:55 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
2021-09-02T12:41:55.9541840Z Sep 02 12:41:55 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
2021-09-02T12:41:55.9542444Z Sep 02 12:41:55 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2021-09-02T12:41:55.9543081Z Sep 02 12:41:55 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-09-02T12:41:55.9543646Z Sep 02 12:41:55 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-09-02T12:41:55.9544154Z Sep 02 12:41:55 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-09-02T12:41:55.9544719Z Sep 02 12:41:55 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23392&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=24826",,dmvk,dwysakowicz,rmetzger,syhily,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 17 17:56:43 UTC 2021,,,,,,,,,,"0|z0ujio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Sep/21 13:48;trohrmann;cc [~arvid].;;;","06/Sep/21 09:14;dmvk;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23508&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461;;;","07/Sep/21 19:42;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23703&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=24713;;;","17/Sep/21 14:34;rmetzger;Merged to master in https://github.com/apache/flink/commit/e18d2731a637b6f6d7f984221e95f02fb68b4e20;;;","17/Sep/21 17:56;rmetzger;merged to release-1.14 in https://github.com/apache/flink/commit/64902ea420a7785383258b0d7fa7922f7cec2c85;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingFileCommiter receive commit message but checkpoint did't ready ,FLINK-24128,13398928,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zhengjun zhu,zhengjun zhu,02/Sep/21 11:41,30/Sep/21 08:09,13/Jul/23 08:12,30/Sep/21 08:09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / FileSystem,,,,,0,,,,,"Checkpoint(49) has not been snapshot. The watermark information is: \{47=1630562415012, 48=-9223372036854775808, 50=1630565549045}.

we find that watermarks did not match checkpoint id.

if EndCompact Message send by notify checkpoint,it will be better?",,zhengjun zhu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2021-09-02 11:41:39.0,,,,,,,,,,"0|z0ujds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use increment of bytes-consumed-total for updating numBytesIn in KafkaSource,FLINK-24126,13398887,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,02/Sep/21 08:01,08/Sep/21 13:51,13/Jul/23 08:12,08/Sep/21 13:51,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,,,renqs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 08 13:51:34 UTC 2021,,,,,,,,,,"0|z0uj4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/21 13:19;arvid;Merged into master as 0dc46ca03933890bc14b9025702b56e32ee8b40f.;;;","08/Sep/21 13:51;arvid;Merged into 1.14 as b434f0acd65d1cb654f080bf091f6585eeba7e16.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_dependency.py fails due to 'Failed to close remote bundle',FLINK-24123,13398850,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,xtsong,xtsong,02/Sep/21 05:34,10/Sep/21 06:06,13/Jul/23 08:12,10/Sep/21 06:06,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,API / Python,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23343&view=logs&j=bdd9ea51-4de2-506a-d4d9-f3930e4d2355&t=dd50312f-73b5-56b5-c172-4d81d03e2ef1&l=23922

{code}
Caused by: java.lang.RuntimeException: Error while waiting for BeamPythonFunctionRunner flush
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.invokeFinishBundle(AbstractPythonFunctionOperator.java:361)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.checkInvokeFinishBundleByCount(AbstractPythonFunctionOperator.java:321)
Sep 02 01:34:47 E                   	at org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.processElement(AbstractStatelessFunctionOperator.java:119)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
Sep 02 01:34:47 E                   	at SourceConversion$38.processElement(Unknown Source)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:116)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:73)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:323)
Sep 02 01:34:47 E                   Caused by: java.lang.RuntimeException: Failed to close remote bundle
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:377)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.flush(BeamPythonFunctionRunner.java:361)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.lambda$invokeFinishBundle$2(AbstractPythonFunctionOperator.java:340)
Sep 02 01:34:47 E                   	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
Sep 02 01:34:47 E                   	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Sep 02 01:34:47 E                   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Sep 02 01:34:47 E                   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Sep 02 01:34:47 E                   	at java.lang.Thread.run(Thread.java:748)
Sep 02 01:34:47 E                   Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error received from SDK harness for instruction 1: Traceback (most recent call last):
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 289, in _execute
Sep 02 01:34:47 E                       response = task()
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 362, in <lambda>
Sep 02 01:34:47 E                       lambda: self.create_worker().do_instruction(request), request)
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 606, in do_instruction
Sep 02 01:34:47 E                       return getattr(self, request_type)(
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 644, in process_bundle
Sep 02 01:34:47 E                       bundle_processor.process_bundle(instruction_id))
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 999, in process_bundle
Sep 02 01:34:47 E                       input_op_by_transform_id[element.transform_id].process_encoded(
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 228, in process_encoded
Sep 02 01:34:47 E                       self.output(decoded_value)
Sep 02 01:34:47 E                     File ""apache_beam/runners/worker/operations.py"", line 357, in apache_beam.runners.worker.operations.Operation.output
Sep 02 01:34:47 E                     File ""apache_beam/runners/worker/operations.py"", line 359, in apache_beam.runners.worker.operations.Operation.output
Sep 02 01:34:47 E                     File ""apache_beam/runners/worker/operations.py"", line 221, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
Sep 02 01:34:47 E                     File ""apache_beam/runners/worker/operations.py"", line 319, in apache_beam.runners.worker.operations.Operation.process
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 132, in process
Sep 02 01:34:47 E                       self._output_processor.process_outputs(o, self.process_element(value))
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/pyflink/fn_execution/table/operations.py"", line 84, in process_element
Sep 02 01:34:47 E                       return self.func(value)
Sep 02 01:34:47 E                     File ""<string>"", line 1, in <lambda>
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/pyflink/table/tests/test_dependency.py"", line 53, in plus_two
Sep 02 01:34:47 E                       from test_dependency_manage_lib import add_two
Sep 02 01:34:47 E                     File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
Sep 02 01:34:47 E                     File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
Sep 02 01:34:47 E                     File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/_pytest/assertion/rewrite.py"", line 161, in exec_module
Sep 02 01:34:47 E                       source_stat, co = _rewrite_test(fn, self.config)
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/_pytest/assertion/rewrite.py"", line 351, in _rewrite_test
Sep 02 01:34:47 E                       stat = os.stat(fn_)
Sep 02 01:34:47 E                   FileNotFoundError: [Errno 2] No such file or directory: '/tmp/python-dist-266a4f9f-c350-41b8-b437-69b3c67435de/python-files/blob_p-0f11eb68b0611db5e1812d04788ca9c96d8e519c-4ad7d15b4215b93fd53b7ca4ab4bef4e/test_dependency_manage_lib.py'
Sep 02 01:34:47 E                   
Sep 02 01:34:47 E                   	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Sep 02 01:34:47 E                   	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Sep 02 01:34:47 E                   	at org.apache.beam.sdk.util.MoreFutures.get(MoreFutures.java:60)
Sep 02 01:34:47 E                   	at org.apache.beam.runners.fnexecution.control.SdkHarnessClient$BundleProcessor$ActiveBundle.close(SdkHarnessClient.java:504)
Sep 02 01:34:47 E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory$1.close(DefaultJobBundleFactory.java:555)
Sep 02 01:34:47 E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:375)
Sep 02 01:34:47 E                   	... 7 more
Sep 02 01:34:47 E                   Caused by: java.lang.RuntimeException: Error received from SDK harness for instruction 1: Traceback (most recent call last):
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 289, in _execute
Sep 02 01:34:47 E                       response = task()
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 362, in <lambda>
Sep 02 01:34:47 E                       lambda: self.create_worker().do_instruction(request), request)
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 606, in do_instruction
Sep 02 01:34:47 E                       return getattr(self, request_type)(
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 644, in process_bundle
Sep 02 01:34:47 E                       bundle_processor.process_bundle(instruction_id))
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 999, in process_bundle
Sep 02 01:34:47 E                       input_op_by_transform_id[element.transform_id].process_encoded(
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 228, in process_encoded
Sep 02 01:34:47 E                       self.output(decoded_value)
Sep 02 01:34:47 E                     File ""apache_beam/runners/worker/operations.py"", line 357, in apache_beam.runners.worker.operations.Operation.output
Sep 02 01:34:47 E                     File ""apache_beam/runners/worker/operations.py"", line 359, in apache_beam.runners.worker.operations.Operation.output
Sep 02 01:34:47 E                     File ""apache_beam/runners/worker/operations.py"", line 221, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
Sep 02 01:34:47 E                     File ""apache_beam/runners/worker/operations.py"", line 319, in apache_beam.runners.worker.operations.Operation.process
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 132, in process
Sep 02 01:34:47 E                       self._output_processor.process_outputs(o, self.process_element(value))
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/pyflink/fn_execution/table/operations.py"", line 84, in process_element
Sep 02 01:34:47 E                       return self.func(value)
Sep 02 01:34:47 E                     File ""<string>"", line 1, in <lambda>
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/pyflink/table/tests/test_dependency.py"", line 53, in plus_two
Sep 02 01:34:47 E                       from test_dependency_manage_lib import add_two
Sep 02 01:34:47 E                     File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
Sep 02 01:34:47 E                     File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
Sep 02 01:34:47 E                     File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/_pytest/assertion/rewrite.py"", line 161, in exec_module
Sep 02 01:34:47 E                       source_stat, co = _rewrite_test(fn, self.config)
Sep 02 01:34:47 E                     File ""/__w/2/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/_pytest/assertion/rewrite.py"", line 351, in _rewrite_test
Sep 02 01:34:47 E                       stat = os.stat(fn_)
Sep 02 01:34:47 E                   FileNotFoundError: [Errno 2] No such file or directory: '/tmp/python-dist-266a4f9f-c350-41b8-b437-69b3c67435de/python-files/blob_p-0f11eb68b0611db5e1812d04788ca9c96d8e519c-4ad7d15b4215b93fd53b7ca4ab4bef4e/test_dependency_manage_lib.py'
Sep 02 01:34:47 E                   
Sep 02 01:34:47 E                   	at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:180)
Sep 02 01:34:47 E                   	at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:160)
Sep 02 01:34:47 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:251)
Sep 02 01:34:47 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
Sep 02 01:34:47 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.Contexts$ContextualizedServerCallListener.onMessage(Contexts.java:76)
Sep 02 01:34:47 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:309)
Sep 02 01:34:47 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:292)
Sep 02 01:34:47 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:782)
Sep 02 01:34:47 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
Sep 02 01:34:47 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
Sep 02 01:34:47 E                   	... 3 more
{code}",test_dependency,dianfu,hxbks2ks,Paul Lin,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24137,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 10 06:06:43 UTC 2021,,,,,,,,,,"0|z0uiwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Sep/21 05:35;xtsong;cc [~dianfu] [~hxbks2ks];;;","10/Sep/21 06:06;hxbks2ks;Merged into master via fb5d1b84a8faeada0d745204381b519ac689a348
Merged into release-1.14 via d9e6abf83d5187095403532ebbbd01aa6837a556;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"KafkaITCase.testTimestamps fails due to ""Topic xxx already exist""",FLINK-24119,13398832,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gaborgsomogyi,xtsong,xtsong,02/Sep/21 01:43,02/Dec/22 10:51,13/Jul/23 08:12,23/Nov/22 09:04,1.14.0,1.15.0,1.16.0,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.4,1.16.1,1.17.0,,,Connectors / Kafka,,,,,0,auto-deprioritized-critical,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23328&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7419

{code}
Sep 01 15:53:20 [ERROR] Tests run: 23, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 162.65 s <<< FAILURE! - in org.apache.flink.streaming.connectors.kafka.KafkaITCase
Sep 01 15:53:20 [ERROR] testTimestamps  Time elapsed: 23.237 s  <<< FAILURE!
Sep 01 15:53:20 java.lang.AssertionError: Create test topic : tstopic failed, org.apache.kafka.common.errors.TopicExistsException: Topic 'tstopic' already exists.
Sep 01 15:53:20 	at org.junit.Assert.fail(Assert.java:89)
Sep 01 15:53:20 	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.createTestTopic(KafkaTestEnvironmentImpl.java:226)
Sep 01 15:53:20 	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.createTestTopic(KafkaTestEnvironment.java:112)
Sep 01 15:53:20 	at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.createTestTopic(KafkaTestBase.java:212)
Sep 01 15:53:20 	at org.apache.flink.streaming.connectors.kafka.KafkaITCase.testTimestamps(KafkaITCase.java:191)
Sep 01 15:53:20 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Sep 01 15:53:20 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Sep 01 15:53:20 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Sep 01 15:53:20 	at java.lang.reflect.Method.invoke(Method.java:498)
Sep 01 15:53:20 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Sep 01 15:53:20 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Sep 01 15:53:20 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Sep 01 15:53:20 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Sep 01 15:53:20 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
Sep 01 15:53:20 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
Sep 01 15:53:20 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Sep 01 15:53:20 	at java.lang.Thread.run(Thread.java:748)
{code}",,akalashnikov,fpaul,gaborgsomogyi,gaoyunhaii,hxb,hxbks2ks,leonard,mapohl,martijnvisser,mason6345,pnowojski,renqs,samrat007,Sergey Nuyanzin,trohrmann,Weijie Guo,xtsong,yunta,,,,,,,,,,,,,,,,,,,,,FLINK-25279,,FLINK-29697,,,,,FLINK-29153,,,,,,,,FLINK-25455,FLINK-29018,FLINK-29914,FLINK-25438,,,,,,FLINK-29956,,FLINK-25249,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 23 11:01:42 UTC 2022,,,,,,,,,,"0|z0uisg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/21 06:02;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24111&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=e424005a-b16e-540f-196d-da062cc19bdf&l=7390;;;","07/Dec/21 11:41;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27648&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=35576;;;","15/Dec/21 07:29;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28133&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=918e890f-5ed9-5212-a25e-962628fb4bc5&l=7374;;;","15/Dec/21 07:33;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27995&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=e424005a-b16e-540f-196d-da062cc19bdf&l=35697;;;","17/Dec/21 05:00;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28260&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c;;;","30/Dec/21 08:25;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28725&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=35242;;;","30/Dec/21 08:25;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28723&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=918e890f-5ed9-5212-a25e-962628fb4bc5&l=7241;;;","17/Jan/22 07:35;gaoyunhaii;Another instance for FlinkKafkaInternalProducerITCase.testProducerWhenCommitEmptyPartitionsToOutdatedTxnCoordinator:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29491&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=576aba0a-d787-51b6-6a92-cf233f360582&l=7201;;;","17/Jan/22 09:01;trohrmann;Another instance for the {{KafkaITCase.testBigRecordJob}}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29512&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=576aba0a-d787-51b6-6a92-cf233f360582&l=35830;;;","19/Jan/22 18:30;trohrmann;Probably similar cause: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29725&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=35907;;;","19/Jan/22 18:30;trohrmann;Any updates on this issue [~fpaul]?;;;","21/Jan/22 09:07;trohrmann;Similar problem for {{KafkaShuffleExactlyOnceITCase.testFailureRecoveryEventTime}}: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=29839&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=f7d83ad5-3324-5307-0eb3-819065cdcb38&l=8573;;;","26/Jan/22 03:29;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30145&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5&l=6993;;;","26/Jan/22 10:01;trohrmann;Another instance for {{FlinkKafkaInternalProducerITCase.testProducerWhenCommitEmptyPartitionsToOutdatedTxnCoordinator}}: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30187&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=f7d83ad5-3324-5307-0eb3-819065cdcb38&l=8200

;;;","27/Jan/22 07:43;gaoyunhaii;FlinkKafkaInternalProducerITCase.testProducerWhenCommitEmptyPartitionsToOutdatedTxnCoordinator: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30258&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=35606;;;","27/Jan/22 07:44;gaoyunhaii; KafkaShuffleITCase.testSerDeProcessingTime on 1.13: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30259&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=ffa8837a-b445-534e-cdf4-db364cf8235d&l=7172;;;","31/Jan/22 06:09;gaoyunhaii;1.13 KafkaSourceLegacyITCase https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30464&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7646;;;","31/Jan/22 06:34;gaoyunhaii;11.3 KafkaShuffleITCase https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30464&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=ffa8837a-b445-534e-cdf4-db364cf8235d&l=7172;;;","02/Feb/22 13:58;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30543&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=918e890f-5ed9-5212-a25e-962628fb4bc5&l=7361;;;","11/Feb/22 08:08;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31211&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=35896;;;","22/Feb/22 11:33;akalashnikov;The same problem for KafkaShuffleITCase#testSimpleProcessingTime:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31963&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=36150
;;;","28/Feb/22 17:46;gaoyunhaii;The same issue for KafkaITCase: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32175&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7403;;;","28/Feb/22 17:54;gaoyunhaii;The same issue for KafkaITCase: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32259&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=36135;;;","09/Mar/22 09:08;gaoyunhaii;I'll first move some Kafka connector related instable test issues to the next cycle since the test infrastructure has been updated and we have not seen this issue for some time. If for a longer time it still not reproduce I think we could close this issue first.;;;","03/Apr/22 08:42;mapohl;{{FlinkKafkaInternalProducerITCase.testProducerWhenCommitEmptyPartitionsToOutdatedTxnCoordinator}} on branch based on {{master}}:
https://dev.azure.com/mapohl/flink/_build/results?buildId=919&view=logs&j=d543d572-9428-5803-a30c-e8e09bf70915&t=4e4199a3-fbbb-5d5b-a2be-802955ffb013&l=35725;;;","06/Jun/22 11:49;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36310&view=logs&j=b0097207-033c-5d9a-b48c-6d4796fbe60d&t=8338a7d2-16f7-52e5-f576-4b7b3071eb3d

Hi [~fpaul] , what's the status of the this issue?;;;","14/Jun/22 11:36;martijnvisser;[~renqs] Do you think you could help out with this problem? ;;;","30/Jun/22 07:23;martijnvisser;1.14: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37383&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7265;;;","19/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","27/Jul/22 22:38;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","16/Aug/22 07:56;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40021&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","09/Sep/22 16:34;mapohl;Here, {{KafkaITCase.testOneToOneSources}} fails with the same error: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40862&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37215;;;","13/Sep/22 02:28;renqs;Should be fixed by FLINK-29153. Please reopen the ticket if the case appears again. ;;;","20/Oct/22 09:55;martijnvisser;Re-opening due to https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42229&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&s=ae4f8708-9994-57d3-c2d7-b892156e7812&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37456

CC [~renqs];;;","21/Oct/22 03:38;hxb;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42266&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203]
{code:java}
2022-10-20T08:43:50.6008823Z Oct 20 08:43:50 [ERROR] org.apache.flink.streaming.connectors.kafka.shuffle.KafkaShuffleExactlyOnceITCase.testAssignedToPartitionFailureRecoveryEventTime  Time elapsed: 92.728 s  <<< FAILURE!
2022-10-20T08:43:50.6010487Z Oct 20 08:43:50 java.lang.AssertionError: Create test topic : partition_failure_recovery_EventTime failed, org.apache.kafka.common.errors.TopicExistsException: Topic 'partition_failure_recovery_EventTime' already exists.
2022-10-20T08:43:50.6011552Z Oct 20 08:43:50 	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.createTestTopic(KafkaTestEnvironmentImpl.java:207)
2022-10-20T08:43:50.6012448Z Oct 20 08:43:50 	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.createTestTopic(KafkaTestEnvironment.java:97)
2022-10-20T08:43:50.6013274Z Oct 20 08:43:50 	at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.createTestTopic(KafkaTestBase.java:217)
2022-10-20T08:43:50.6014297Z Oct 20 08:43:50 	at org.apache.flink.streaming.connectors.kafka.shuffle.KafkaShuffleExactlyOnceITCase.testAssignedToPartitionFailureRecovery(KafkaShuffleExactlyOnceITCase.java:158)
2022-10-20T08:43:50.6015529Z Oct 20 08:43:50 	at org.apache.flink.streaming.connectors.kafka.shuffle.KafkaShuffleExactlyOnceITCase.testAssignedToPartitionFailureRecoveryEventTime(KafkaShuffleExactlyOnceITCase.java:101) {code};;;","21/Oct/22 07:58;martijnvisser;[~renqs] There's still something wrong, can you take a look? What has changed is that the images for Confluent Platform have been upgraded to the same Kafka version as we have the client (via FLINK-28405), but I believe that's just made the issue visible again because apparently it was not resolved. ;;;","21/Oct/22 14:55;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42232&view=logs&j=d22373ad-b356-55ba-ef18-6ae7deba4552&s=7d4e458d-e0e0-5f89-c72d-7371ef61b09b&t=350c6121-8698-59d6-9a85-c8cf427aed84&l=37447;;;","24/Oct/22 06:07;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42326&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","24/Oct/22 06:19;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42337&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37457;;;","24/Oct/22 14:55;samrat007;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42354&view=logs&j=219f6d90-20a2-5863-7c1b-c80377a1018f&t=20186858-1485-5059-c9c6-446952519524&l=37146;;;","25/Oct/22 09:04;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42325&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37355;;;","26/Oct/22 08:15;martijnvisser;[~mason6345] Do you have the bandwidth to have a look at this issue? ;;;","26/Oct/22 17:47;mason6345;[~martijnvisser]  Sure, let me take a look later in the week;;;","27/Oct/22 02:47;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42455&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37386;;;","27/Oct/22 07:09;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42455&view=logs&j=219f6d90-20a2-5863-7c1b-c80377a1018f&t=20186858-1485-5059-c9c6-446952519524&l=37312;;;","27/Oct/22 11:29;yunta;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42466&view=logs&j=219f6d90-20a2-5863-7c1b-c80377a1018f&t=20186858-1485-5059-c9c6-446952519524;;;","27/Oct/22 13:19;leonard;I upgrade the ticket priority to blocker as to many failure continuously happening. [~renqs];;;","28/Oct/22 08:41;gaborgsomogyi;[~martijnvisser] I've seen this jira already and intentionally opened FLINK-29783. I think the main issue is that Kafka server remains live for some magical reason and topics getting stuck there. This is not necessarily coming from Flink code, Kafka has it's own issues. This jira is open for 1+ year and going to remain like this because one must invest several weeks to find out why this magic happens. I know what I'm talking about because I'm the Spark side Kafka connector owner.

All in all either somebody invest such amount of time or we try to add some random to the topic names. In think hardcoding topic names are not a hard requirement to test something but such way we can avoid annoying CI failures. I'm doing some experiments here but at the moment japicmp is blowing up everything: https://github.com/apache/flink/pull/21180

Later on I'm pretty sure I'm going to have time to invest the mentioned time but right now I'm working on time pressured features.
FYI [~mbalassi];;;","28/Oct/22 10:16;martijnvisser;[~gaborgsomogyi] If you look at the history of this ticket, it was recently closed but now is broken again. Next to that, it's been marked as a blocker. All in all, there's no value in creating another duplicated ticket and having discussions on how to resolve this go in multiple places. 

The japicmp thing is definitely a blocker, most likely because of the 1.16 release.

Edit: it was found and fixed via FLINK-29787;;;","28/Oct/22 10:17;renqs;Thanks for the input [~gaborgsomogyi] ! {{KafkaShuffleExactlyOnceITCase}} is configured to re-run on failure, and the topic name is not randomized, so we encounter this ""topic already exists"" exception whatever the root cause is. We have to dig into the log and find the actual failure reason in the first attempt (it's a nightmare to look through the CI log...)

I checked the latest two failure cases above, and both of them failed because of InitProducerId timeout, which is caused by a timeout on broker when requesting new producer id blocks in ProducerIdManager: 

 
{code:java}
org.apache.kafka.common.errors.TimeoutException: Timed out waiting for next producer ID block{code}
So, I assume this is an internal issue on Kafka broker and not related to the Flink Kafka shuffler. [~gaborgsomogyi] 's solution to randomize topic names on retry makes sense to me on this case, considering it's so hard to debug issues on Kafka broker on our side. Please ping me if your PR is ready to review. 

 ;;;","28/Oct/22 13:59;gaborgsomogyi;Re-executing failing test is good idea in general.
However no matter what the issue is here tests which are retries must randomize the topic name because in case of failure we can't be sure whether the topic is deleted or not.

I still think that 99% of the issues can be eliminated by adding random to the topic name where retry happens...
All I can say retry w/ the actual implementation is not much help (99% of the cases delete topic is never called which 100% makes the next round fail). My preliminary tests has shown good result, just creating a PR for all Kafka tests where we hardcoded the topic name.

It may happen that we face further issues but that will be definitely less than what we have...;;;","29/Oct/22 01:09;mason6345;I took a look and it does seem to be Kafka container issue. I went through Confluent's documentation on default configurations that should be applied for a single container environment and I am testing it in [https://github.com/apache/flink/pull/21190];;;","31/Oct/22 08:15;martijnvisser;Merged ""Set offsets.topic.replication.factor to 1"" to via f73189ddf5ae0457592ed1bf74b3d68f7c883655 as stated via https://issues.apache.org/jira/browse/FLINK-24119?focusedCommentId=17625970&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17625970;;;","31/Oct/22 14:15;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42608&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37440;;;","31/Oct/22 14:21;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42653&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37187;;;","01/Nov/22 08:33;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42680&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a&l=37512;;;","01/Nov/22 09:19;martijnvisser;1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42682&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc&l=37484;;;","02/Nov/22 08:10;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42724&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37293;;;","02/Nov/22 11:22;pnowojski;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42652&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","02/Nov/22 15:40;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42734&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37317;;;","02/Nov/22 15:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42746&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37358;;;","04/Nov/22 07:59;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42779&view=logs&j=219f6d90-20a2-5863-7c1b-c80377a1018f&t=20186858-1485-5059-c9c6-446952519524&l=37525;;;","07/Nov/22 08:56;mapohl;{{KafkaSinkITCase.testWriteRecordsToKafkaWithNoneGuarantee}} failed: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42868&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=35892
{code}
Nov 07 01:22:57 [ERROR]   KafkaSinkITCase.testWriteRecordsToKafkaWithNoneGuarantee Multiple Failures (2 failures)
Nov 07 01:22:57 	java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TopicExistsException: Topic '9a50fd2b-222f-4d34-8bf9-bf9e26eaff68' already exists.
Nov 07 01:22:57 	java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
{code};;;","07/Nov/22 14:12;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42876&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","08/Nov/22 13:18;pnowojski;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42931&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203

[~renqs], are you still working on this? Note, I'm reporting maybe every 10th occurrence that I see somewhere. ;;;","08/Nov/22 15:03;gaborgsomogyi;Hi All, I've found many different issues and made a stable solution in https://github.com/apache/flink/pull/21247
It's not super classic to stack-up different commits within a PR but they both needed to work correctly.;;;","09/Nov/22 07:53;mapohl;Thanks [~gaborgsomogyi], I'm gonna have a look.

In the meantime: 
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42940&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37961 (two failures both being caused by a topic already being created)
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42942&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37795
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42941&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37846
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42955&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a&l=38000
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42960&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37626

I assigned [~gaborgsomogyi] to the issue since he's working on it.;;;","10/Nov/22 18:19;martijnvisser;The following changes have been merged:

master:
[FLINK-29914][tests] Wait for Kafka topic creation/deletion c66ef088ea662c38a94225ea086fcb83de514c76
[FLINK-24119][tests] Add random to Kafka tests topic name 9b6bae4eb87e1f472fd0f6cf9403911a88ed89ce

release-1.16:
[FLINK-29914][tests] Wait for Kafka topic creation/deletion 0df3c6668eb399eaa0e5e95626b2794614566803
[FLINK-24119][tests] Add random to Kafka tests topic name 9e7ebdc671386a8127ddf5affb66e997d877cb7b

release-1.15:
[FLINK-29914][tests] Wait for Kafka topic creation/deletion f60e1fa78c51aa0c979adf82b424481657874914
[FLINK-24119][tests] Add random to Kafka tests topic name 0f2c6bc80cd8c8cb3dbc442f6c981ff4c095694a

With this, the Blocker should be resolved. I'll downgrade it to Critical and if nothing appears in the next couple of days, we can mark the ticket as resolved.;;;","23/Nov/22 09:01;gaborgsomogyi;Can we close this jira or the issue popped up again?;;;","23/Nov/22 09:05;martijnvisser;[~gaborgsomogyi] Closed it as fixed. Many thanks :);;;","23/Nov/22 11:01;gaborgsomogyi;[~martijnvisser] good to hear that it works :) Ping me any time when Kafka related knowledge is needed.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Outdated SQL Temporal Join Example,FLINK-24115,13398734,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,nkruber,nkruber,01/Sep/21 15:14,08/Sep/21 02:44,13/Jul/23 08:12,08/Sep/21 02:44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,1.15.0,,,Documentation,Table SQL / API,,,,0,pull-request-available,,,,"[https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/table/sql/queries/joins/#event-time-temporal-join] is missing a primary key in the Table DDL.

Also, the following note does not map the current example anymore:
{quote}
Note: The event-time temporal join requires the primary key contained in the equivalence condition of the temporal join condition, e.g., The primary key P.product_id of table product_changelog to be constrained in the condition O.product_id = P.product_id.
{quote}
 ",,jark,leonard,nkruber,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 08 02:44:17 UTC 2021,,,,,,,,,,"0|z0ui6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/21 16:11;leonard;I fixed similar issue in https://issues.apache.org/jira/browse/FLINK-24084, but the note I missed. I'd like to take this one, could you assign this to me? [~nkruber];;;","08/Sep/21 02:44;jark;Fixed in 
 - master: 9dfb1af5ca038b1a12669cc752fa873530feacbc
 - release-1.14: c10de14ce6777d2b10d1284b2fe70f3c658cb60a
 - release-1.13: cedc27dc75c214d112b94868597c3b1dc85aac7f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the streaming check in StreamTableEnvironment in PyFlink,FLINK-24097,13398584,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,01/Sep/21 06:33,02/Sep/21 01:57,13/Jul/23 08:12,02/Sep/21 01:57,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,API / Python,,,,,0,pull-request-available,,,,"Since it has supported to DataStream batch mode in StreamTableEnvironment in FLINK-20897, it should also work in PyFlink. Currently there are a few checks in the Python code and we should remove them.",,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23768,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 02 01:57:54 UTC 2021,,,,,,,,,,"0|z0uh9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Sep/21 01:57;dianfu;Fixed in
- master via f2982cafa1f478ef702d30e4fdbb51bad1968676
- release-1.14 via 3f348ba01be8727d81d3b1cd61533500c25eb31c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table planner tests fails due to NPE in FailingCollectionSource.notifyCheckpointComplete,FLINK-24096,13398577,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gaoyunhaii,xtsong,xtsong,01/Sep/21 06:17,06/Sep/21 06:57,13/Jul/23 08:12,06/Sep/21 06:57,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23250&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=9417

JoinITCase.testNullFullOuterJoin

{code}
Sep 01 01:57:41 Caused by: java.lang.NullPointerException
Sep 01 01:57:41 	at org.apache.flink.table.planner.runtime.utils.FailingCollectionSource.notifyCheckpointComplete(FailingCollectionSource.java:242)
Sep 01 01:57:41 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.notifyCheckpointComplete(AbstractUdfStreamOperator.java:126)
Sep 01 01:57:41 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.notifyCheckpointComplete(StreamOperatorWrapper.java:99)
Sep 01 01:57:41 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.notifyCheckpointComplete(RegularOperatorChain.java:152)
Sep 01 01:57:41 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointComplete(SubtaskCheckpointCoordinatorImpl.java:348)
Sep 01 01:57:41 	at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:1397)
Sep 01 01:57:41 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointAbortAsync$16(StreamTask.java:1355)
Sep 01 01:57:41 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$17(StreamTask.java:1377)
Sep 01 01:57:41 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
Sep 01 01:57:41 	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
Sep 01 01:57:41 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.drain(MailboxProcessor.java:177)
Sep 01 01:57:41 	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:834)
Sep 01 01:57:41 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:747)
Sep 01 01:57:41 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
Sep 01 01:57:41 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
Sep 01 01:57:41 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
Sep 01 01:57:41 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
Sep 01 01:57:41 	at java.lang.Thread.run(Thread.java:748)
{code}",,dwysakowicz,gaoyunhaii,jark,jingzhang,pnowojski,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24136,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 06 06:57:02 UTC 2021,,,,,,,,,,"0|z0uh7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/21 06:20;xtsong;WindowAggregateITCase.testEventTimeHopWindow_GroupingSets
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23250&view=logs&j=ed934a8e-982d-5d3f-03cf-c751f5bd1b22&t=972d3f6c-09f6-5149-9cf8-2eaaf718eb08&l=11006;;;","01/Sep/21 06:28;xtsong;cc [~jark] [~qingru zhang]
2 different tests from the same build fail on this NPE.;;;","01/Sep/21 07:06;jark;{{checkpointedEmittedNums.get(checkpointId)}} may return null and result in NPE. We may need to add a protect here. Do you want to fix this [~qingru zhang]?;;;","01/Sep/21 07:18;arvid;I think this is related to FLIP-147. I wouldn't guard against NPE since we should never receive a {{notifyCheckpointComplete}} without prior {{snapshotState}}. CC [~pnowojski].;;;","01/Sep/21 07:23;pnowojski;On a first glance it sounds like something similar to FLINK-23759. But to the best of my knowledge FLIP-147 is disabled on the master.;;;","01/Sep/21 07:29;jark;[~arvid] sounds right. It looks like there is a potential problem in the checkpoint. This NPE is never happended before. ;;;","01/Sep/21 07:37;gaoyunhaii;I'll have a look at this issue~;;;","01/Sep/21 08:24;arvid;If this is not only 1.15 but also 1.14, it may be a release blocker. I think this may break all 2pc sinks.;;;","01/Sep/21 08:32;xtsong;Upgraded to Blocker for now;;;","01/Sep/21 11:14;gaoyunhaii;I should have found the reason, I'll open a PR for this issue~;;;","01/Sep/21 11:59;jark;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23292&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","02/Sep/21 12:31;gaoyunhaii;Very thanks for reporting this issue! This should be because the previous fix in -FLINK-23759- does not cover all the cases. I opened a PR for the new fix~;;;","06/Sep/21 03:43;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23557&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=9862;;;","06/Sep/21 06:56;gaoyunhaii;Fix on master via c0c33d3f8dd39c312111de68d3c4230d89edbb10..888b3653bd8c7b0c9088d143227eacb82f480a4d

Fix on release-1.14 via f6460e9341a155459541dbaee331ffd751dd6f7e..9c8d21fbfdc71f7daab99a4a4c5b19f9b353757a;;;","06/Sep/21 06:57;gaoyunhaii;Since the PR is merged, I'll temporarily close the issue, and we could re-open it if later we found it re-occured. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskManagerProcessFailureBatchRecoveryITCase.testTaskManagerProcessFailure fails on azure,FLINK-24091,13398557,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,xtsong,xtsong,01/Sep/21 02:06,03/Sep/21 09:55,13/Jul/23 08:12,03/Sep/21 09:55,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23187&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=6112

{code}
Aug 31 10:04:21 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 28.263 s <<< FAILURE! - in org.apache.flink.test.recovery.TaskManagerProcessFailureBatchRecoveryITCase
Aug 31 10:04:21 [ERROR] testTaskManagerProcessFailure[0]  Time elapsed: 16.147 s  <<< FAILURE!
Aug 31 10:04:21 java.lang.AssertionError: The program encountered a ProgramInvocationException : Job failed (JobID: c68654bb03c0ec88707ef7ada2239f62)
Aug 31 10:04:21 	at org.junit.Assert.fail(Assert.java:89)
Aug 31 10:04:21 	at org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest.testTaskManagerProcessFailure(AbstractTaskManagerProcessFailureRecoveryTest.java:204)
{code}",,RocMarshal,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 03 09:55:51 UTC 2021,,,,,,,,,,"0|z0uh3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/21 12:00;trohrmann;I think the problem is that we restart the job before the killed {{TaskManager}} can be marked as unreachable. The heartbeat timeout is set to {{500ms}} and we need two heartbeats to fail. Moreover, we set the delay between restart to 1s. This means that we can miss the second heartbeat before restarting the job.;;;","03/Sep/21 09:55;trohrmann;Fixed via

1.15.0: 338ae25ca68c788a51a9f8a9ae322133a785d233
1.14.0: c1aa87540e580e556e37d63a21f06851adca8622;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Internal server error"" when quitting a SELECT query in the SqlClient",FLINK-24088,13398489,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,mapohl,mapohl,31/Aug/21 15:12,06/Sep/21 11:27,13/Jul/23 08:12,06/Sep/21 11:27,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,API / DataStream,Table SQL / API,,,,0,pull-request-available,,,,"Release testing revealed an Internal server error being reported when quitting a SELECT statement in the SQL Client (see details on how to reproduce it in the [comment of FLINK-23850|https://issues.apache.org/jira/browse/FLINK-23850?focusedCommentId=17407235&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17407235]):
{code}
2021-08-31 12:34:37,091 WARN  org.apache.flink.streaming.api.operators.collect.CollectResultFetcher [] - An exception occurred when fetching query results
java.util.concurrent.ExecutionException: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (3df388d05273cc9782458228081ea5bf)
        at org.apache.flink.runtime.dispatcher.Dispatcher.getJobMasterGateway(Dispatcher.java:909)
        at org.apache.flink.runtime.dispatcher.Dispatcher.performOperationOnJobMasterGateway(Dispatcher.java:923)
        at org.apache.flink.runtime.dispatcher.Dispatcher.deliverCoordinationRequestToCoordinator(Dispatcher.java:719)
        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at akka.actor.Actor.aroundReceive(Actor.scala:537)
        at akka.actor.Actor.aroundReceive$(Actor.scala:535)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
        at akka.actor.ActorCell.invoke(ActorCell.scala:548)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
        at akka.dispatch.Mailbox.run(Mailbox.scala:231)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

End of exception on server side>]
        at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_265]
        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) ~[?:1.8.0_265]
        at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sendRequest(CollectResultFetcher.java:163) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:128) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80) [flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:370) [flink-table_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.table.client.gateway.local.result.CollectResultBase$ResultRetrievalThread.run(CollectResultBase.java:74) [flink-sql-client_2.12-1.14.0.jar:1.14.0]
Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (3df388d05273cc9782458228081ea5bf)
        at org.apache.flink.runtime.dispatcher.Dispatcher.getJobMasterGateway(Dispatcher.java:909)
        at org.apache.flink.runtime.dispatcher.Dispatcher.performOperationOnJobMasterGateway(Dispatcher.java:923)
        at org.apache.flink.runtime.dispatcher.Dispatcher.deliverCoordinationRequestToCoordinator(Dispatcher.java:719)
        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at akka.actor.Actor.aroundReceive(Actor.scala:537)
        at akka.actor.Actor.aroundReceive$(Actor.scala:535)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
        at akka.actor.ActorCell.invoke(ActorCell.scala:548)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
        at akka.dispatch.Mailbox.run(Mailbox.scala:231)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

End of exception on server side>]
        at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:532) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:512) ~[flink-dist_2.12-1.14.0.jar:1.14.0]
        at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966) ~[?:1.8.0_265]
        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940) ~[?:1.8.0_265]
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) ~[?:1.8.0_265]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_265]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_265]
        at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_265]
{code}",,fsk119,lzljs3620320,mapohl,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23850,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 06 11:27:58 UTC 2021,,,,,,,,,,"0|z0ugo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/21 02:35;TsReaper;Hi!

From the discussion in FLINK-23850 this issue seems to happen when the job is submitted but not yet running (see {{Dispatcher#getJobMasterGateway}}, it only gets running jobs, not initializing or scheduling jobs). Normally this type of exception will not affect the data as the collect iterator is designed to deal with all kinds of errors and fail-overs.

I think we can eliminate this type of exception by starting the collection after the job has started, but this is not a very severe issue to me as this does not affect the correctness of data.

Please assign this issue to me and I'll fix it when I have time.;;;","06/Sep/21 11:27;lzljs3620320;master: 327a98f2ea5aa5ed54529bd126d777e1a6f16953;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka connector StartupMode requires a table dependency,FLINK-24087,13398484,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,leonard,twalthr,twalthr,31/Aug/21 14:49,01/Sep/21 11:11,13/Jul/23 08:12,01/Sep/21 11:11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,Connectors / Kafka,Table SQL / Ecosystem,,,,0,pull-request-available,,,,Table dependencies are marked as {{optional}} for the Kafka connector. We should move the {{StartupMode.java#L75}} to {{KafkaConnectorOptionsUtil}} to not require Table API for DataStream API users.,,airblader,leonard,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 01 11:11:27 UTC 2021,,,,,,,,,,"0|z0ugn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/21 15:08;leonard;nice catch! I'd like to help fix this one ;;;","31/Aug/21 16:08;airblader;Sorry, I didn't see your comment when I assigned myself. Thanks for taking this over!;;;","01/Sep/21 02:41;leonard;[~airblader] It doesn't matter, I just found the issue and want to make a quick fix. Could you help review ? Thanks.;;;","01/Sep/21 11:11;twalthr;Fixed in master: 61b5b0a514a6fbf4ecbb457e82d4ea761afb2876
Fixed in 1.14: e591455f4d6a5b93899069f323a5947dc5958b03;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The `versioned table` and `Timezone` pages missed the first class subject ,FLINK-24085,13398464,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,leonard,leonard,31/Aug/21 13:16,10/Sep/21 03:02,13/Jul/23 08:12,01/Sep/21 06:38,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,Documentation,,,,,0,pull-request-available,,,,,,jark,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24238,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 01 06:38:35 UTC 2021,,,,,,,,,,"0|z0ugio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/21 13:23;leonard;[~jark] Could you help assign this to me? appreciate if you can help review this one.;;;","01/Sep/21 06:38;jark;Fixed in
 - master: a3489f56612dc669da9b0bc292ee360dc4c62a09
 - release-1.14: b828ce204d4a9173073da1e309bc560e979e1785;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The example in sql join document is incorrect,FLINK-24084,13398460,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,leonard,leonard,31/Aug/21 12:50,01/Sep/21 09:34,13/Jul/23 08:12,31/Aug/21 13:52,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,Documentation,,,,,0,pull-request-available,,,,"The defined upsert-kafka table missed primary key constranit
[1] https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/table/sql/queries/joins/#temporal-joins
",,leonard,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 31 13:51:39 UTC 2021,,,,,,,,,,"0|z0ughs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/21 13:13;leonard;CC [~sjwiesman] Could you help assign this one to me? Appreciate if you can take a look the PR.;;;","31/Aug/21 13:51;sjwiesman;fixed in master: 04e7433cc4ea4ffc8f254212f0d6a4355a4c051a

release-1.14: 75088fc0f692308913271fab9d31380a96268954;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The result isn't as expected when the result type is generator of string for Python UDTF,FLINK-24083,13398455,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,31/Aug/21 12:22,02/Sep/21 02:05,13/Jul/23 08:12,02/Sep/21 02:05,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,API / Python,,,,,0,pull-request-available,,,,,,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 02 02:05:31 UTC 2021,,,,,,,,,,"0|z0uggo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Sep/21 02:05;dianfu;Fixed in
- master via da1912d546f24349ee687457b3bf8149051212d7
- release-1.14 via 5e7f0824e77c3e9d36b7a16f821c46b4bff48acb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python UDTF throws exception when the result type is generator of Row,FLINK-24082,13398454,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,31/Aug/21 12:20,02/Sep/21 11:10,13/Jul/23 08:12,02/Sep/21 11:10,1.13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,,,,,API / Python,,,,,0,pull-request-available,,,,"For job:
{code}
################################################################################
#  Licensed to the Apache Software Foundation (ASF) under one
#  or more contributor license agreements.  See the NOTICE file
#  distributed with this work for additional information
#  regarding copyright ownership.  The ASF licenses this file
#  to you under the Apache License, Version 2.0 (the
#  ""License""); you may not use this file except in compliance
#  with the License.  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
# limitations under the License.
################################################################################
import argparse
import logging
import sys

from pyflink.common import Row
from pyflink.table import (EnvironmentSettings, TableEnvironment, DataTypes)
from pyflink.table.expressions import lit, col
from pyflink.table.udf import udtf

word_count_data = [""To be, or not to be,--that is the question:--"",
                   ""Whether 'tis nobler in the mind to suffer"",
                   ""The slings and arrows of outrageous fortune"",
                   ""Or to take arms against a sea of troubles,"",
                   ""And by opposing end them?--To die,--to sleep,--"",
                   ""No more; and by a sleep to say we end"",
                   ""The heartache, and the thousand natural shocks"",
                   ""That flesh is heir to,--'tis a consummation"",
                   ""Devoutly to be wish'd. To die,--to sleep;--"",
                   ""To sleep! perchance to dream:--ay, there's the rub;"",
                   ""For in that sleep of death what dreams may come,"",
                   ""When we have shuffled off this mortal coil,"",
                   ""Must give us pause: there's the respect"",
                   ""That makes calamity of so long life;"",
                   ""For who would bear the whips and scorns of time,"",
                   ""The oppressor's wrong, the proud man's contumely,"",
                   ""The pangs of despis'd love, the law's delay,"",
                   ""The insolence of office, and the spurns"",
                   ""That patient merit of the unworthy takes,"",
                   ""When he himself might his quietus make"",
                   ""With a bare bodkin? who would these fardels bear,"",
                   ""To grunt and sweat under a weary life,"",
                   ""But that the dread of something after death,--"",
                   ""The undiscover'd country, from whose bourn"",
                   ""No traveller returns,--puzzles the will,"",
                   ""And makes us rather bear those ills we have"",
                   ""Than fly to others that we know not of?"",
                   ""Thus conscience does make cowards of us all;"",
                   ""And thus the native hue of resolution"",
                   ""Is sicklied o'er with the pale cast of thought;"",
                   ""And enterprises of great pith and moment,"",
                   ""With this regard, their currents turn awry,"",
                   ""And lose the name of action.--Soft you now!"",
                   ""The fair Ophelia!--Nymph, in thy orisons"",
                   ""Be all my sins remember'd.""]


def word_count(input_path, output_path):
    t_env = TableEnvironment.create(EnvironmentSettings.new_instance().in_streaming_mode().build())
    # write all the data to one file
    t_env.get_config().get_configuration().set_string(""parallelism.default"", ""1"")

    # define the source
    if input_path is not None:
        t_env.execute_sql(""""""
                CREATE TABLE source (
                    word STRING
                ) WITH (
                    'connector' = 'filesystem',
                    'path' = {},
                    'format' = 'csv'
                )
            """""".format(input_path))
        tab = t_env.from_path('source')
    else:
        print(""Executing word_count example with default input data set."")
        print(""Use --input to specify file input."")
        tab = t_env.from_elements(map(lambda i: (i,), word_count_data),
                                  DataTypes.ROW([DataTypes.FIELD('line', DataTypes.STRING())]))

    # define the sink
    if output_path is not None:
        t_env.execute_sql(""""""
            CREATE TABLE sink (
                word STRING,
                `count` BIGINT
            ) WITH (
                'connector' = 'filesystem',
                'path' = {},
                'format' = 'canal-json'
            )
        """""".format(output_path))
    else:
        print(""Printing result to stdout. Use --output to specify output path."")
        t_env.execute_sql(""""""
                    CREATE TABLE sink (
                        word STRING,
                        `count` BIGINT
                    ) WITH (
                        'connector' = 'print'
                    )
                """""")

    @udtf(result_types=[DataTypes.STRING()])
    def split(line: Row):
        for s in line[0].split():
            yield Row(s)

    # compute word count
    tab.flat_map(split).alias('word') \
       .group_by(col('word')) \
       .select(col('word'), lit(1).count) \
       .execute_insert('sink') \
       .wait()
    # remove .wait if submitting to a remote cluster, refer to
    # https://ci.apache.org/projects/flink/flink-docs-stable/docs/dev/python/faq/#wait-for-jobs-to-finish-when-executing-jobs-in-mini-cluster
    # for more details


if __name__ == '__main__':
    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=""%(message)s"")

    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--input',
        dest='input',
        required=False,
        help='Input file to process.')
    parser.add_argument(
        '--output',
        dest='output',
        required=False,
        help='Output file to write results to.')

    argv = sys.argv[1:]
    known_args, _ = parser.parse_known_args(argv)

    word_count(known_args.input, known_args.output)

{code}

It will throw the following exception:
{code}
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error received from SDK harness for instruction 1: Traceback (most recent call last):
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 289, in _execute
    response = task()
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 362, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 607, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 644, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 1000, in process_bundle
    element.data)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 228, in process_encoded
    self.output(decoded_value)
  File ""apache_beam/runners/worker/operations.py"", line 357, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 359, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 221, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File ""apache_beam/runners/worker/operations.py"", line 319, in apache_beam.runners.worker.operations.Operation.process
  File ""/Users/dianfu/code/src/apache/flink/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 75, in process
    self.process_element(value), output_stream, True)
  File ""/Users/dianfu/code/src/apache/flink/flink-python/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 708, in encode_to_stream
    self._value_coder.encode_to_stream(value, out, nested)
  File ""/Users/dianfu/code/src/apache/flink/flink-python/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 226, in encode_to_stream
    self._flatten_row_coder.encode_to_stream(value, out_stream, nested)
  File ""/Users/dianfu/code/src/apache/flink/flink-python/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 96, in encode_to_stream
    self._encode_one_row(value, out_stream, nested)
  File ""/Users/dianfu/code/src/apache/flink/flink-python/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 120, in _encode_one_row
    field_coders[i].encode_to_stream(item, data_out_stream, nested)
  File ""/Users/dianfu/code/src/apache/flink/flink-python/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 535, in encode_to_stream
    bytes_value = value.encode(""utf-8"")
  File ""/Users/dianfu/code/src/apache/flink/flink-python/pyflink/common/types.py"", line 196, in __getattr__
    idx = self._fields.index(item)
  File ""/Users/dianfu/code/src/apache/flink/flink-python/pyflink/common/types.py"", line 192, in __getattr__
    raise AttributeError(item)
AttributeError: _fields

	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.beam.sdk.util.MoreFutures.get(MoreFutures.java:60)
	at org.apache.beam.runners.fnexecution.control.SdkHarnessClient$BundleProcessor$ActiveBundle.close(SdkHarnessClient.java:504)
	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory$1.close(DefaultJobBundleFactory.java:555)
	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:381)
	... 30 more
Caused by: java.lang.RuntimeException: Error received from SDK harness for instruction 1: Traceback (most recent call last):
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 289, in _execute
    response = task()
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 362, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 607, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 644, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 1000, in process_bundle
    element.data)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 228, in process_encoded
    self.output(decoded_value)
  File ""apache_beam/runners/worker/operations.py"", line 357, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 359, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 221, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File ""apache_beam/runners/worker/operations.py"", line 319, in apache_beam.runners.worker.operations.Operation.process
  File ""/Users/dianfu/code/src/apache/flink/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 75, in process
    self.process_element(value), output_stream, True)
  File ""/Users/dianfu/code/src/apache/flink/flink-python/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 708, in encode_to_stream
    self._value_coder.encode_to_stream(value, out, nested)
  File ""/Users/dianfu/code/src/apache/flink/flink-python/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 226, in encode_to_stream
    self._flatten_row_coder.encode_to_stream(value, out_stream, nested)
  File ""/Users/dianfu/code/src/apache/flink/flink-python/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 96, in encode_to_stream
    self._encode_one_row(value, out_stream, nested)
  File ""/Users/dianfu/code/src/apache/flink/flink-python/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 120, in _encode_one_row
    field_coders[i].encode_to_stream(item, data_out_stream, nested)
  File ""/Users/dianfu/code/src/apache/flink/flink-python/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 535, in encode_to_stream
    bytes_value = value.encode(""utf-8"")
  File ""/Users/dianfu/code/src/apache/flink/flink-python/pyflink/common/types.py"", line 196, in __getattr__
    idx = self._fields.index(item)
  File ""/Users/dianfu/code/src/apache/flink/flink-python/pyflink/common/types.py"", line 192, in __getattr__
    raise AttributeError(item)
AttributeError: _fields

Process finished with exit code 1
{code}",,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 02 11:10:51 UTC 2021,,,,,,,,,,"0|z0uggg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Sep/21 11:10;dianfu;Fixed in release-1.13 via 62f3c9c9dbacf27e297dd2a64788a4d26efccf27;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OVER window throws exception when the rowtime is timestamp_ltz type,FLINK-24081,13398452,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,leonard,leonard,31/Aug/21 12:19,02/Sep/21 07:15,13/Jul/23 08:12,02/Sep/21 07:15,1.13.0,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,1.15.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,," The exception as following:
{code:java}
org.apache.flink.table.api.TableException: OVER windows' ordering in stream mode must be defined on a time attribute. at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecOverAggregate.translateToPlanInternal(StreamExecOverAggregate.java:158) at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134) at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:250) at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:88) at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134) at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:250)
{code}",,airblader,dianfu,jark,leonard,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 02 07:14:57 UTC 2021,,,,,,,,,,"0|z0ugg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/21 12:40;leonard;[~dianfu] Could you help assign this to me? Appreciate if you can have a look this PR。;;;","01/Sep/21 02:13;jark;Fixed in 
 - master: e68b3a4e6ba3ca8f6b9256a2a34c622d5ef7cda0
 - release-1.14: 5bd06e1c676ae7a468090528ce191aadded36204
 - release-1.13: ca9f9b2c6eff3a6de5b13c0bb80236a1b4455561;;;","02/Sep/21 06:20;airblader;[~jark] I think you forgot to actually close the issue? Closing it as fixed, please correct if that's wrong.;;;","02/Sep/21 06:20;airblader;Sorry, just saw there is one open PR still. Reopening.;;;","02/Sep/21 07:14;jark;Yes [~airblader] ;);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileCache can leak user code classloader,FLINK-24080,13398446,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,leishuiyu,leishuiyu,31/Aug/21 12:02,05/Sep/21 15:22,13/Jul/23 08:12,05/Sep/21 15:22,1.11.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.0,,,,,Runtime / Task,,,,,0,,,,,"* Symptom After submitting a batch task for several times, the task is abnormal ,Caused by: java.lang.OutOfMemoryError: Direct buffer memory
 * MAT analyzing memory find five ChildFirstClassLoader instances, submit five batch job",,FrankZou,leishuiyu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16245,,,,,,,,,,,,,,,,,FLINK-23147,,,,,,,,,,,,,,,,"31/Aug/21 12:03;leishuiyu;GC root.png;https://issues.apache.org/jira/secure/attachment/13032786/GC+root.png","31/Aug/21 12:13;leishuiyu;class add.png;https://issues.apache.org/jira/secure/attachment/13032787/class+add.png","31/Aug/21 12:03;leishuiyu;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13032785/screenshot-1.png",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 31 12:43:53 UTC 2021,,,,,,,,,,"0|z0ugeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/21 12:40;chesnay;Closely related to FLINK-23147.

Task#doRun sets the context class loader of the executing thread to the user code classloader, and it is still set when FileCache#releaseJob is called. This may create threads in the thread pool that backs the FileCache, which then continue to reference the user code classloader.;;;","31/Aug/21 12:43;chesnay;[~leishuiyu] Upgrading to 1.12 should resolve the issue due to FLINK-16245.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBaseConnectorITCase.testTableSink,FLINK-24077,13398436,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jingge,xtsong,xtsong,31/Aug/21 10:37,11/Jan/22 09:21,13/Jul/23 08:12,11/Dec/21 16:08,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.6,1.14.3,1.15.0,,Connectors / HBase,Table SQL / Ecosystem,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23160&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=12962

{code}
Aug 31 05:10:58 Tests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 73.758 sec <<< FAILURE! - in org.apache.flink.connector.hbase2.HBaseConnectorITCase
Aug 31 05:10:58 testTableSink(org.apache.flink.connector.hbase2.HBaseConnectorITCase)  Time elapsed: 6.516 sec  <<< FAILURE!
Aug 31 05:10:58 java.lang.AssertionError: 
Aug 31 05:10:58 Different elements in arrays: expected 8 elements and received 3
Aug 31 05:10:58  expected: [+I[1, 10, Hello-1, 100, 1.01, false, Welt-1], +I[2, 20, Hello-2, 200, 2.02, true, Welt-2], +I[3, 30, Hello-3, 300, 3.03, false, Welt-3], +I[4, 40, null, 400, 4.04, true, Welt-4], +I[5, 50, Hello-5, 500, 5.05, false, Welt-5], +I[6, 60, Hello-6, 600, 6.06, true, Welt-6], +I[7, 70, Hello-7, 700, 7.07, false, Welt-7], +I[8, 80, null, 800, 8.08, true, Welt-8]]
Aug 31 05:10:58  received: [+I[1, 10, Hello-1, 100, 1.01, false, Welt-1], +I[2, 20, Hello-2, 200, 2.02, true, Welt-2], +I[3, 30, Hello-3, 300, 3.03, false, Welt-3]] expected:<8> but was:<3>
Aug 31 05:10:58 	at org.junit.Assert.fail(Assert.java:89)
Aug 31 05:10:58 	at org.junit.Assert.failNotEquals(Assert.java:835)
Aug 31 05:10:58 	at org.junit.Assert.assertEquals(Assert.java:647)
Aug 31 05:10:58 	at org.apache.flink.test.util.TestBaseUtils.compareResult(TestBaseUtils.java:395)
Aug 31 05:10:58 	at org.apache.flink.test.util.TestBaseUtils.compareResultAsText(TestBaseUtils.java:347)
Aug 31 05:10:58 	at org.apache.flink.connector.hbase2.HBaseConnectorITCase.testTableSink(HBaseConnectorITCase.java:284)
{code}",,airblader,gaoyunhaii,jingge,leonard,matriv,trohrmann,twalthr,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19615,FLINK-22621,,,,,,,,,,,,,,FLINK-25343,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Dec 11 16:08:38 UTC 2021,,,,,,,,,,"0|z0ugcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/21 02:02;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23560&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=13205;;;","12/Oct/21 08:36;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24965&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=13307;;;","04/Nov/21 09:20;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25896&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=41463ccd-0694-5d4d-220d-8f771e7d098b&l=12573;;;","06/Nov/21 03:40;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26033&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=13012];;;","08/Nov/21 07:47;trohrmann;cc [~arvid], [~twalthr];;;","23/Nov/21 15:19;matriv;Haven't managed to have a failure locally, therefore I only have some guesses that might make the test deterministic:

Configure

[https://nightlies.apache.org/flink/flink-docs-release-1.12/dev/table/connectors/hbase.html#sink-buffer-flush-max-rows]

and/or

[https://nightlies.apache.org/flink/flink-docs-release-1.12/dev/table/connectors/hbase.html#sink-buffer-flush-interval]

 

Or maybe set [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/config/#table-dml-sync] to true

 

But let's see if the changes in [https://github.com/apache/flink/pull/17888] fix the issue already.;;;","26/Nov/21 08:08;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27093&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=41463ccd-0694-5d4d-220d-8f771e7d098b&l=12714;;;","29/Nov/21 11:28;trohrmann;[~jingge] could you give a quick update on the progress of this ticket?;;;","30/Nov/21 11:46;jingge;The flaky issue happened occasionally based on some unknown race conditions during the CI process. Since the DML(i.e. Select operation in this case) job is executed asynchronously, the tableResult collecting could be started after the first row has been arrived. It looks like that we collected only partial result(3, expected 8) sometimes while asserting. 

 

In order to make the test stable. I have done the following improvements:
 # After the Insert operation, run

{code:java}
batchEnv.sqlQuery(""SELECT COUNT(h.rowkey) FROM "" + TEST_TABLE_2 + "" AS h""); {code}
to check if the Insert operation works before testing the SELECT operation.

2. After executing the SELECT query, call
{code:java}
// wait to finish
tableResult2.getJobClient().get().getJobExecutionResult().get(); {code}
to wait to finish.

 

I have changed the test locally to be a parametrized test and repeat 20 times for each test run. Until now, it has been checked on my local laptop more than 500 times. Zero issue has been found.

 

 ;;;","07/Dec/21 08:46;jingge;After opening the INFO log, found that a Flink MiniCluster will be started/stopped automatically in the background while using TableEnvironment for each query in the ITCase. Since the shutdown of the MiniCluster will be called asynchronously, CollectResultFetcher will got data lost sometimes based on race conditions and the unchecked RuntimeException java.lang.IllegalStateException will be thrown that we were not aware of.

The solution is to control the lifecycle of the MiniCluster manually. The MiniClusterWithClientResource could be a good fit in this case.

 ;;;","07/Dec/21 09:49;twalthr;[~jingge] I agree. We should definitely add the cluster resource:

{code}
    @ClassRule
    public static MiniClusterWithClientResource miniClusterResource =
            new MiniClusterWithClientResource(
                    new MiniClusterResourceConfiguration.Builder()
                            .setNumberTaskManagers(1)
                            .setNumberSlotsPerTaskManager(1)
                            .build());
{code}

Actually all ITCases should do that. Maybe this could be another potential architectural test [~airblader]?;;;","07/Dec/21 10:00;airblader;Writing an architectural rule that verifies that all IT cases (within o.a.f.table.*?) have a public, static member of type MiniClusterWithClientResource annotated with ClassRule would certainly be possible.

If [~jingge] would like to add such a rule I'm happy to review it; I can also take over doing that, of course.;;;","07/Dec/21 14:05;arvid;Note that the cluster could also be non-static. We could of course, also change that through the rule.;;;","07/Dec/21 16:28;jingge;[~twalthr] [~arvid] I agree with you. Beyond solving the race condition issue, forcing all ITCase doing it will also help saving the CI time significantly. I was aware that it will take about 4s for each start and stop of a MiniCluster. Imaging how long it will take for all test queries to wait for new MiniClusters up and down. Because of this,  @ClassRule is generally recommended unless there is a technical reason for using @Rule which will use a non-static MiniCluster. The architectural rule should cover both cases.

[~airblader] I'd like to add the architectural rule and then ask for your review, many thanks.;;;","08/Dec/21 09:23;jingge;I've noted the [best practices|https://gejing.gitbook.io/101-building-modern-distributed-platform/101-getting-started-with-flink-development/debug-flink-itcase] found while working on this PR, hopefully it will help developers while working on ITCase in the future.;;;","09/Dec/21 10:01;arvid;Merged 7976be0f8675a8753a5bb7e7a44dda6b4a347247..fca04c3aaf6346d61cf9fe022a7ac77ab4d66c91 into master. [~jingge], could you please also create backports to 1.14, 1.13, and if it's an easy cherry-pick also to 1.12?;;;","11/Dec/21 16:08;arvid;Merged 4e962d9a7980dc143a2e4ace1211887314648d7d..cc78095923508299437eab0b39c78576b6a69c07 into 1.14, 2efb0df1fb2d2a7bf081ae205c78698f3244c625..9d9842fb610a95d9f31ad432e2546d8ce61b106e into 1.13, 8e3b291fb5b04b8e774d603a517f2dfadc6095ed..b6ae90384b47a6274e2f746c76aa31387c2d5ee8 into 1.12.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apache Flink 1.13.1 Docker images are published for linux/arm,FLINK-24075,13398407,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,mapohl,mapohl,31/Aug/21 09:02,31/Aug/21 10:07,13/Jul/23 08:12,31/Aug/21 10:07,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"The [Apache Flink 1.13.1 Docker images|https://hub.docker.com/r/apache/flink/tags?page=1&ordering=last_updated&name=1.13.1] are published for {{linux/arm}} instead of {{linux/amd64}}. This causes issues for users pulling these images resulting in a warning:

{code}
WARNING: The requested image's platform (linux/arm64/v8) does not match the detected host platform (linux/amd64) and no specific platform was requested
ERROR: ld.so: object '/usr/lib/x86_64-linux-gnu/libjemalloc.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
[...]
{code}",,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 31 10:07:25 UTC 2021,,,,,,,,,,"0|z0ug60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/21 10:07;chesnay;All 1.13.1 images have been rebuilt.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Statefun 2.2 documentation build fails on buildbot,FLINK-24073,13398398,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,31/Aug/21 08:44,02/Nov/21 08:07,13/Jul/23 08:12,02/Nov/21 08:07,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Documentation,Stateful Functions,,,,0,,,,,"https://ci2.apache.org/#/builders/10/builds/51
{{/bin/sh: 14: rvm: not found}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 02 08:07:01 UTC 2021,,,,,,,,,,"0|z0ug40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/21 08:07;chesnay;The 2.2 documentation builds have been removed, and we thus no longer need RVM.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flink 1.12  HDFS sink have a problem,reported as OOM  ",FLINK-24071,13398389,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zhengjun zhu,zhengjun zhu,31/Aug/21 07:55,30/Sep/21 08:08,13/Jul/23 08:12,30/Sep/21 08:08,1.12.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / FileSystem,,,,,0,,,,,"{code:java}
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:118) at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:80) at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:233) at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:224) at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:215) at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:669) at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:89) at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:449) at sun.reflect.GeneratedMethodAccessor144.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: java.lang.OutOfMemoryError: Java heap space at java.util.HashMap.entrySet(HashMap.java:1007) at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:80) at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:21) at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:523) at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:61) at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:495) at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:599) at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(KryoSerializer.java:316) at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:168) at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:46) at org.apache.flink.runtime.plugable.SerializationDelegate.write(SerializationDelegate.java:54) at org.apache.flink.runtime.io.network.api.writer.RecordWriter.serializeRecord(RecordWriter.java:130) at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:104) at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:54) at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:101) at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:87) at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:43) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:50) at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:28) at org.apache.flink.streaming.runtime.operators.TimestampsAndWatermarksOperator.processElement(TimestampsAndWatermarksOperator.java:104) at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:191) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:204) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:174) at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:396) at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$219/1969922351.runDefaultAction(Unknown Source) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:191) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:617) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:581) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570) at java.lang.Thread.run(Thread.java:748)
{code}
 

we find this problem when we tested  file compaction，and this problem may not produced by us so that we report this problem to find help.",,jark,zhengjun zhu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/21 12:10;zhengjun zhu;image-2021-09-07-20-10-04-824.png;https://issues.apache.org/jira/secure/attachment/13033156/image-2021-09-07-20-10-04-824.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 07 12:10:54 UTC 2021,,,,,,,,,,"0|z0ug20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/21 12:10;zhengjun zhu;we find that oom exception was happend in JobManager.Pmap result as follow

!image-2021-09-07-20-10-04-824.png!

And the configration is ""jobmanager.memory.process.size=40000m,yarn.containers.vcores=10,yarn.appmaster.vcores=20"";;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IgnoreInFlightDataITCase.testIgnoreInFlightDataDuringRecovery hangs on azure,FLINK-24069,13398380,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,xtsong,xtsong,31/Aug/21 06:50,07/Sep/21 06:12,13/Jul/23 08:12,07/Sep/21 06:12,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23144&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=12083

{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007f90bc00b800 nid=0x86d54 waiting on condition [0x00007f90c4e53000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000080f25d48> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1937)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1917)
	at org.apache.flink.test.checkpointing.IgnoreInFlightDataITCase.executeIgnoreInFlightDataDuringRecovery(IgnoreInFlightDataITCase.java:136)
	at org.apache.flink.test.checkpointing.IgnoreInFlightDataITCase.testIgnoreInFlightDataDuringRecovery(IgnoreInFlightDataITCase.java:107)
{code}",,dwysakowicz,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 07 06:12:44 UTC 2021,,,,,,,,,,"0|z0ug00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/21 08:34;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23253&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=11795;;;","07/Sep/21 06:12;dwysakowicz;Fixed in:
* master
** 2d2b719a93dcf07c01a5100c0f94520d7ed93bea..311eaaa9f71d5326c0c3631a0de8c05601ebcb84
* 1.14
** 24baf7f39a8471f3544f670699a334d188a42ee9..c1b6acdd0c54f31ab1c7ee8fc027af7d856f31d5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HybridSource recovery from savepoint fails,FLINK-24064,13398365,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,thw,thw,thw,31/Aug/21 06:06,06/Sep/21 16:40,13/Jul/23 08:12,06/Sep/21 16:40,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,1.15.0,,,Connectors / Common,,,,,0,pull-request-available,,,,Recovery fails because underlying source and split deserializers are not initialized in the restore code path. This requires deferred deserialization of underlying splits after the current source has been set.,,thw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 31 12:14:57 UTC 2021,,,,,,,,,,"0|z0ufwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/21 12:10;thw;Exception restoring coordinator:
{code:java}
ERROR org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator [] - Failed to reset the coordinator to checkpoint and start.2021-08-30 20:00:24,027 ERROR org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator [] - Failed to reset the coordinator to checkpoint and start.java.lang.NullPointerException: Source for index=1 not available at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:104) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at org.apache.flink.connector.base.source.hybrid.HybridSourceEnumeratorStateSerializer.lambda$serializerOf$0(HybridSourceEnumeratorStateSerializer.java:99) ~[?:?] at java.util.HashMap.computeIfAbsent(HashMap.java:1133) ~[?:?] at org.apache.flink.connector.base.source.hybrid.HybridSourceEnumeratorStateSerializer.serializerOf(HybridSourceEnumeratorStateSerializer.java:95) ~[?:?] at org.apache.flink.connector.base.source.hybrid.HybridSourceEnumeratorStateSerializer.deserializeV0(HybridSourceEnumeratorStateSerializer.java:89) ~[?:?] at org.apache.flink.connector.base.source.hybrid.HybridSourceEnumeratorStateSerializer.deserialize(HybridSourceEnumeratorStateSerializer.java:72) ~[?:?] at org.apache.flink.connector.base.source.hybrid.HybridSourceEnumeratorStateSerializer.deserialize(HybridSourceEnumeratorStateSerializer.java:34) ~[?:?] at org.apache.flink.runtime.source.coordinator.SourceCoordinator.deserializeCheckpoint(SourceCoordinator.java:398) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at org.apache.flink.runtime.source.coordinator.SourceCoordinator.resetToCheckpoint(SourceCoordinator.java:301) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$DeferrableCoordinator.resetAndStart(RecreateOnResetOperatorCoordinator.java:377) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.lambda$resetToCheckpoint$6(RecreateOnResetOperatorCoordinator.java:136) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at java.util.concurrent.CompletableFuture.uniRunNow(CompletableFuture.java:815) ~[?:?] at java.util.concurrent.CompletableFuture.uniRunStage(CompletableFuture.java:799) ~[?:?] at java.util.concurrent.CompletableFuture.thenRun(CompletableFuture.java:2121) ~[?:?] at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.resetToCheckpoint(RecreateOnResetOperatorCoordinator.java:131) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.resetToCheckpoint(OperatorCoordinatorHolder.java:273) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreStateToCoordinators(CheckpointCoordinator.java:1815) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreLatestCheckpointedStateInternal(CheckpointCoordinator.java:1577) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1642) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.tryRestoreExecutionGraphFromSavepoint(DefaultExecutionGraphFactory.java:163) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:138) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:342) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:190) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:122) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:132) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:110) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:340) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:317) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:107) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:95) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112) ~[flink-dist_2.12-1.13.2-stream1.jar:1.13.2-stream1] at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700) [?:?] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?] at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) [?:?] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?] at java.lang.Thread.run(Thread.java:829) [?:?] {code};;;","31/Aug/21 12:12;thw;Exception restoring task/operator:
{code:java}
java.lang.NullPointerException: Source for index=1 not availablejava.lang.NullPointerException: Source for index=1 not available at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:104) at org.apache.flink.connector.base.source.hybrid.HybridSourceSplitSerializer.lambda$serializerOf$0(HybridSourceSplitSerializer.java:91) at java.base/java.util.HashMap.computeIfAbsent(HashMap.java:1133) at org.apache.flink.connector.base.source.hybrid.HybridSourceSplitSerializer.serializerOf(HybridSourceSplitSerializer.java:87) at org.apache.flink.connector.base.source.hybrid.HybridSourceSplitSerializer.deserializeV0(HybridSourceSplitSerializer.java:81) at org.apache.flink.connector.base.source.hybrid.HybridSourceSplitSerializer.deserialize(HybridSourceSplitSerializer.java:68) at org.apache.flink.connector.base.source.hybrid.HybridSourceSplitSerializer.deserialize(HybridSourceSplitSerializer.java:35) at org.apache.flink.core.io.SimpleVersionedSerialization.readVersionAndDeSerialize(SimpleVersionedSerialization.java:165) at org.apache.flink.streaming.api.operators.util.SimpleVersionedListState$DeserializingIterator.next(SimpleVersionedListState.java:138) at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133) at org.apache.flink.util.CollectionUtil.iterableToList(CollectionUtil.java:95) at org.apache.flink.streaming.api.operators.SourceOperator.open(SourceOperator.java:251) at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:442) at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:582) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) at org.apache.flink.streaming.runtime.tasks.StreamTask.executeRestore(StreamTask.java:562) at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:647) at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:537) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:759) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) at java.base/java.lang.Thread.run(Thread.java:829) {code};;;","31/Aug/21 12:14;thw;[~xtsong] [~arvid] fyi I'm testing the fix for this currently and should have the PR open soon.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception encountered during timer serialization in Python DataStream API ,FLINK-24062,13398361,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,31/Aug/21 05:13,01/Sep/21 09:12,13/Jul/23 08:12,01/Sep/21 03:46,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,API / Python,,,,,0,pull-request-available,release-testing,,,"For the following example:
{code}
################################################################################
#  Licensed to the Apache Software Foundation (ASF) under one
#  or more contributor license agreements.  See the NOTICE file
#  distributed with this work for additional information
#  regarding copyright ownership.  The ASF licenses this file
#  to you under the Apache License, Version 2.0 (the
#  ""License""); you may not use this file except in compliance
#  with the License.  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
# limitations under the License.
################################################################################
import argparse
import logging
import sys

from pyflink.common import WatermarkStrategy, Encoder, Types
from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode
from pyflink.datastream.connectors import (FileSource, StreamFormat, FileSink, OutputFileConfig,
                                           RollingPolicy)


word_count_data = [""To be, or not to be,--that is the question:--"",
                   ""Whether 'tis nobler in the mind to suffer"",
                   ""The slings and arrows of outrageous fortune"",
                   ""Or to take arms against a sea of troubles,"",
                   ""And by opposing end them?--To die,--to sleep,--"",
                   ""No more; and by a sleep to say we end"",
                   ""The heartache, and the thousand natural shocks"",
                   ""That flesh is heir to,--'tis a consummation"",
                   ""Devoutly to be wish'd. To die,--to sleep;--"",
                   ""To sleep! perchance to dream:--ay, there's the rub;"",
                   ""For in that sleep of death what dreams may come,"",
                   ""When we have shuffled off this mortal coil,"",
                   ""Must give us pause: there's the respect"",
                   ""That makes calamity of so long life;"",
                   ""For who would bear the whips and scorns of time,"",
                   ""The oppressor's wrong, the proud man's contumely,"",
                   ""The pangs of despis'd love, the law's delay,"",
                   ""The insolence of office, and the spurns"",
                   ""That patient merit of the unworthy takes,"",
                   ""When he himself might his quietus make"",
                   ""With a bare bodkin? who would these fardels bear,"",
                   ""To grunt and sweat under a weary life,"",
                   ""But that the dread of something after death,--"",
                   ""The undiscover'd country, from whose bourn"",
                   ""No traveller returns,--puzzles the will,"",
                   ""And makes us rather bear those ills we have"",
                   ""Than fly to others that we know not of?"",
                   ""Thus conscience does make cowards of us all;"",
                   ""And thus the native hue of resolution"",
                   ""Is sicklied o'er with the pale cast of thought;"",
                   ""And enterprises of great pith and moment,"",
                   ""With this regard, their currents turn awry,"",
                   ""And lose the name of action.--Soft you now!"",
                   ""The fair Ophelia!--Nymph, in thy orisons"",
                   ""Be all my sins remember'd.""]


def word_count(input_path, output_path):
    env = StreamExecutionEnvironment.get_execution_environment()
    env.set_runtime_mode(RuntimeExecutionMode.BATCH)
    # write all the data to one file
    env.set_parallelism(1)

    # define the source
    if input_path is not None:
        ds = env.from_source(
            source=FileSource.for_record_stream_format(StreamFormat.text_line_format(),
                                                       input_path)
                             .process_static_file_set().build(),
            watermark_strategy=WatermarkStrategy.for_monotonous_timestamps(),
            source_name=""file_source""
        )
    else:
        print(""Executing word_count example with default input data set."")
        print(""Use --input to specify file input."")
        ds = env.from_collection(word_count_data)

    def split(line):
        yield from line.split()

    # compute word count
    ds = ds.flat_map(split) \
           .map(lambda i: (i, 1), output_type=Types.TUPLE([Types.STRING(), Types.INT()])) \
           .key_by(lambda i: i[0]) \
           .reduce(lambda i, j: (i[0], i[1] + j[1]))

    # define the sink
    if output_path is not None:
        ds.sink_to(
            sink=FileSink.for_row_format(
                base_path=output_path,
                encoder=Encoder.simple_string_encoder())
            .with_output_file_config(
                OutputFileConfig.builder()
                .with_part_prefix(""prefix"")
                .with_part_suffix("".ext"")
                .build())
            .with_rolling_policy(RollingPolicy.default_rolling_policy())
            .build()
        )
    else:
        print(""Printing result to stdout. Use --output to specify output path."")
        ds.print()

    # submit for execution
    env.execute()


if __name__ == '__main__':
    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=""%(message)s"")

    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--input',
        dest='input',
        required=False,
        help='Input file to process.')
    parser.add_argument(
        '--output',
        dest='output',
        required=False,
        help='Output file to write results to.')

    argv = sys.argv[1:]
    known_args, _ = parser.parse_known_args(argv)

    word_count(known_args.input, known_args.output)

{code}

It will throw the following exception:
{code}
Traceback (most recent call last):
  File ""pyflink/examples/datastream/word_count.py"", line 134, in <module>
    word_count(known_args.input, known_args.output)
  File ""pyflink/examples/datastream/word_count.py"", line 113, in word_count
    env.execute()
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/datastream/stream_execution_environment.py"", line 691, in execute
    return JobExecutionResult(self._j_stream_execution_environment.execute(j_stream_graph))
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1286, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/util/exceptions.py"", line 146, in deco
    return f(*a, **kw)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/py4j/protocol.py"", line 328, in get_return_value
    format(target_id, ""."", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o3.execute.
: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:250)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
	at akka.dispatch.OnComplete.internal(Future.scala:300)
	at akka.dispatch.OnComplete.internal(Future.scala:297)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1067)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1703)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:172)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:228)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:218)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:209)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:679)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:441)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:537)
	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	... 4 more
Caused by: java.lang.RuntimeException: Error while waiting for BeamPythonFunctionRunner flush
	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.invokeFinishBundle(AbstractPythonFunctionOperator.java:361)
	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.processElementsOfCurrentKeyIfNeeded(AbstractPythonFunctionOperator.java:257)
	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.setCurrentKey(AbstractPythonFunctionOperator.java:246)
	at org.apache.flink.streaming.api.operators.python.PythonKeyedProcessOperator.setCurrentKey(PythonKeyedProcessOperator.java:225)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.setKeyContextElement(AbstractStreamOperator.java:504)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.setKeyContextElement1(AbstractStreamOperator.java:491)
	at org.apache.flink.streaming.api.operators.OneInputStreamOperator.setKeyContextElement(OneInputStreamOperator.java:36)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:229)
	at org.apache.flink.streaming.api.operators.sort.SortingDataInput.emitNextSortedRecord(SortingDataInput.java:207)
	at org.apache.flink.streaming.api.operators.sort.SortingDataInput.emitNext(SortingDataInput.java:187)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:489)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:819)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:746)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:785)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:728)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:786)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:572)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: Failed to close remote bundle
	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:377)
	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.flush(BeamPythonFunctionRunner.java:361)
	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.lambda$invokeFinishBundle$2(AbstractPythonFunctionOperator.java:340)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.util.concurrent.ExecutionException: org.apache.beam.sdk.coders.CoderException: java.io.EOFException
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.beam.sdk.fn.data.CompletableFutureInboundDataClient.awaitCompletion(CompletableFutureInboundDataClient.java:48)
	at org.apache.beam.sdk.fn.data.BeamFnDataInboundObserver.awaitCompletion(BeamFnDataInboundObserver.java:73)
	at org.apache.beam.runners.fnexecution.control.SdkHarnessClient$BundleProcessor$ActiveBundle.close(SdkHarnessClient.java:542)
	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory$1.close(DefaultJobBundleFactory.java:555)
	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:375)
	... 7 more
Caused by: org.apache.beam.sdk.coders.CoderException: java.io.EOFException
	at org.apache.beam.sdk.coders.StringUtf8Coder.decode(StringUtf8Coder.java:104)
	at org.apache.beam.sdk.coders.StringUtf8Coder.decode(StringUtf8Coder.java:90)
	at org.apache.beam.runners.core.construction.Timer$Coder.decode(Timer.java:195)
	at org.apache.beam.runners.core.construction.Timer$Coder.decode(Timer.java:158)
	at org.apache.beam.sdk.fn.data.DecodingFnDataReceiver.accept(DecodingFnDataReceiver.java:43)
	at org.apache.beam.sdk.fn.data.DecodingFnDataReceiver.accept(DecodingFnDataReceiver.java:25)
	at org.apache.beam.sdk.fn.data.BeamFnDataInboundObserver.accept(BeamFnDataInboundObserver.java:65)
	at org.apache.beam.sdk.fn.data.BeamFnDataInboundObserver.accept(BeamFnDataInboundObserver.java:29)
	at org.apache.beam.sdk.fn.data.BeamFnDataGrpcMultiplexer$InboundObserver.onNext(BeamFnDataGrpcMultiplexer.java:178)
	at org.apache.beam.sdk.fn.data.BeamFnDataGrpcMultiplexer$InboundObserver.onNext(BeamFnDataGrpcMultiplexer.java:127)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:251)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.Contexts$ContextualizedServerCallListener.onMessage(Contexts.java:76)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:309)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:292)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:782)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
	... 3 more
Caused by: java.io.EOFException
	at org.apache.beam.sdk.util.VarInt.decodeLong(VarInt.java:73)
	at org.apache.beam.sdk.util.VarInt.decodeInt(VarInt.java:56)
	at org.apache.beam.sdk.coders.StringUtf8Coder.readString(StringUtf8Coder.java:55)
	at org.apache.beam.sdk.coders.StringUtf8Coder.decode(StringUtf8Coder.java:100)
	... 20 more
{code}",,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 01 03:46:57 UTC 2021,,,,,,,,,,"0|z0ufvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/21 03:46;dianfu;Fixed in
- master via 4ba14cc3ef786623885e3b92a2f949a83edab2aa
- release-1.14 via 9c3957f7a42b5378d4b3ccad281f052fe2285d61;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceReaderTestBase should allow NUM_SPLITS to be overridden in implementation,FLINK-24059,13398340,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Brian Zhou,Brian Zhou,Brian Zhou,31/Aug/21 01:58,15/Sep/21 12:43,13/Jul/23 08:12,15/Sep/21 12:43,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Connectors / Common,,,,,0,pull-request-available,,,,"Pravega Flink connector is trying to implement the FLIP-27 sources and trying to map the Pravega reader into the split. This leads to a one-to-one mapping for source reader and splits. For unit tests, Flink has offered the {{SourceReaderTestBase}} class to test more easily, but it has a {{final}} constraint in the NUM_SPLITS constant which the value is 10, which makes us hard to integrate.",,becket_qin,Brian Zhou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 15 12:43:57 UTC 2021,,,,,,,,,,"0|z0ufr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/21 12:43;becket_qin;Merged to master: d4c483fadd3df32045fbb2ee117d0a6eeab9276e

Cherry-picked to release-1.14: 8d148a8b7832fcefefa4818de8e700562f0ffd26;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskSlotTableImplTest.testMarkSlotActiveDeactivatesSlotTimeout fails on azure,FLINK-24058,13398337,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,xtsong,xtsong,31/Aug/21 01:36,15/Dec/21 01:44,13/Jul/23 08:12,02/Sep/21 07:18,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23083&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8583

{code}
Aug 30 10:55:16 [ERROR] Tests run: 20, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.243 s <<< FAILURE! - in org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImplTest
Aug 30 10:55:16 [ERROR] testMarkSlotActiveDeactivatesSlotTimeout  Time elapsed: 0.334 s  <<< FAILURE!
Aug 30 10:55:16 java.lang.AssertionError: The slot timeout should have been deactivated.
Aug 30 10:55:16 	at org.junit.Assert.fail(Assert.java:89)
Aug 30 10:55:16 	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImplTest.runDeactivateSlotTimeoutTest(TaskSlotTableImplTest.java:509)
Aug 30 10:55:16 	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImplTest.testMarkSlotActiveDeactivatesSlotTimeout(TaskSlotTableImplTest.java:472)
{code}",,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20683,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 02 07:18:54 UTC 2021,,,,,,,,,,"0|z0ufqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/21 10:38;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23162&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8577;;;","02/Sep/21 07:18;chesnay;master: 794295eff3993ee164e55795dd606f7949cde6f0..cc58c6cf3f0be23dec2eccc6be9ab9fef88e6e9c
1.14: 102f71a3ae516ec94cad94d750c8603d44f7c381..24d9856a9b01015bc6df4babf5404789a850bb56;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Let SinkUpsertMaterializer emit +U instead of only +I,FLINK-24054,13398213,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,twalthr,twalthr,twalthr,30/Aug/21 12:16,30/Sep/21 11:51,13/Jul/23 08:12,30/Sep/21 11:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"Currently, {{SinkUpsertMaterializer}} is not able to emit +U's but will always emit +I's. Thus, resulting changelogs are incorrect strictly speaking and only valid when treating +U and +I as similar changes in downstream operators.",,Terry1897,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23835,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 30 11:51:56 UTC 2021,,,,,,,,,,"0|z0ueyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/21 08:22;ykt836;In most cases, +I and -D are easier to handle, and are equivalant with +U and -U

Is it worth to treat this as blocker bug? I'm not even sure if this is a bug.;;;","31/Aug/21 10:12;twalthr;It really depends on the use case. For our current Kafka sink implementation, this might not be an issue. But strictly speaking the current behavior is incorrect. +I and +U might trigger different downstream logic. [See this example|https://github.com/apache/flink/blob/master/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/DataStreamJavaITCase.java#L643] I had to modify the materialization logic because +U requires a look up by primary key, whereas +I should simply be forwarded.;;;","31/Aug/21 10:19;ykt836;The example you gave is test, whose behavior could be changed & adopted easily. 

I think we need to think more with actual external systems, just like you said, kafka sink and others. ;;;","31/Aug/21 10:36;twalthr;In the first version, the test was failing because I did not perform a look up by key (why should I if it is just an insertion?). We recently exposed `toChangelogStream` and our changelog contains +I/+U/-U/-D for a reason. We could have discussed whether we only expose +I/+D when we introduced FLIP-95 but now we should adhere to our own semantics.;;;","31/Aug/21 10:44;arvid;If the external system strictly follows +I key +I key, you will get a primary key violation. This would be the case if CDC is used to update a database. Having +I key +U key is correct and wouldn't require workarounds downstream. 

However, for kafka eliminating superfluous tombstones records is most important (e.g. what BufferedSinkFunction/ReducingSinkWriter is doing). So I'm fine to solve this issue later. But it must be documented for now.;;;","01/Sep/21 01:01;ykt836;You opinions also make sense in some cases. That's exactly what I tried to mean at the first place, whether to improve this issue seems more discussion and thoughts. I didn't understand why we category this one into a bug immediately and marked it as release blocker. ;;;","02/Sep/21 07:31;twalthr;Fixed in master: 35f63758cfdeb030fb5a7e7a819f2cf342e26cd2
Fixed in 1.14: cb7c41b5f98c491eb855c5f8f02469c80d42277d;;;","02/Sep/21 13:35;twalthr;[~ykt836] sorry, I saw you comment to late. I just marked it as a blocker to fix it in this release that also introduced the SinkUpsertMaterializer. In the end, as you can see in the PR, the changes required were minimal.;;;","30/Sep/21 11:51;twalthr;Fixed in 1.13: fa16e70774b516ddd499400d9cafa0400d314bfc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TupleTypeInfo doesn't handle correctly for data types need conversion,FLINK-24049,13398118,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,30/Aug/21 06:33,15/Dec/21 01:40,13/Jul/23 08:12,31/Aug/21 02:04,1.12.0,1.13.0,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.0,,,API / Python,,,,,0,pull-request-available,,,,,,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 31 02:04:03 UTC 2021,,,,,,,,,,"0|z0ueds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/21 02:04;dianfu;Merged to
 - master via 117c40cef96a5cc8dcaaee5029deb6ca718c7eb1
 - release-1.14 via 024877b6a0c0f221fa0ee6aaada310ce30915bc4
 - release-1.13 via a90b2115f9cf923858cbda3b3eb33832aee95410
 - release-1.12 via 6f9cfe1609ac3950c7782c2e46e4b2a36f178728;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Slf4jReporter#report need catch NoSuchElementException,FLINK-24040,13398023,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,camilesing,camilesing,camilesing,29/Aug/21 14:07,10/Nov/21 12:28,13/Jul/23 08:12,10/Nov/21 12:28,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,,,,,,0,pull-request-available,,,,"Slf4jReporter#report need catch NoSuchElementException, like StatsDReporter,  instead of  only catch ConcurrentModificationException",,camilesing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 10 12:28:30 UTC 2021,,,,,,,,,,"0|z0udso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/21 12:28;chesnay;master: 881577c00a631a78bc4574bffb93f43972455621;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DispatcherResourceManagerComponent fails to deregister application if no leading ResourceManager,FLINK-24038,13397977,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,trohrmann,trohrmann,28/Aug/21 17:49,09/Jan/23 08:58,13/Jul/23 08:12,26/Jan/22 22:56,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"With FLINK-21667 we introduced a change that can cause the {{DispatcherResourceManagerComponent}} to fail when trying to stop the application. The problem is that the {{DispatcherResourceManagerComponent}} needs a leading {{ResourceManager}} to successfully execute the stop/deregister application call. If this is not the case, then it will fail fatally. In the case of multiple standby JobManager processes it can happen that the leading {{ResourceManager}} runs somewhere else.

I do see two possible solutions:

1. Run the leader election process for the whole JobManager process
2. Move the registration/deregistration of the application out of the {{ResourceManager}} so that it can be executed w/o a leader",,aitozi,dmvk,guoyangze,klion26,knaufk,mapohl,Thesharing,trohrmann,wangyang0918,xtsong,yunta,,,,,,,,,,,,,,,,,,,FLINK-23946,FLINK-25235,,,,,,,,,,,,,,,FLINK-21667,FLINK-25981,FLINK-26630,,FLINK-25432,,,,FLINK-27358,FLINK-25393,FLINK-25500,FLINK-25806,FLINK-25847,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 07 03:33:41 UTC 2022,,,,,,,,,,"0|z0udig:",9223372036854775807,"A new multiple component leader election service was implemented that only runs a single leader election per Flink process. If this should cause any problems, then you can set `high-availability.use-old-ha-services: true` in the `flink-conf.yaml` to use the old high availability services.",,,,,,,,,,,,,,,,,,,"28/Aug/21 17:50;trohrmann;cc [~xtsong].;;;","29/Aug/21 02:30;xtsong;I think option 2) should not work. To deregister an application, it can involve interactions with the underlying external resource manager. This is usually specific to the underlying system, and is better performed by the ResourceManagerDriver. Most importantly, deregistration of an application usually means all the process will be terminated, thus a non-leader JobManager process could kill a leader process if it is allowed to deregister, which is undesired.

Option 1) might work. I would need to look into it a bit more to be sure about that. Event this works, my gut feeling the efforts needed and the potential impacts on stabilities may not be trivial.

Alternatively, we may consider simply not throwing the error there's not a leading resource manager. To be specific, if there is a leading resource manager, errors occurred during the deregistration should still be considered fatal. But if there's not a leading resource manager, we simply don't do the deregistration. For standalone clusters, there should be no difference anyway, since the StandaloneResourceManager does not do anything for deregistration. For active resource managers, I think it's a good contract that only the leading resource manager interacts with the external resource manager (except for pure reading operations). The side effect would be, if Flink tries to deregister when there's no leader RM, the deregister cannot success and K8s/Yarn will bring up another JobManager process anyway, which is the same as how it is currently and IMHO not a big problem.;;;","30/Aug/21 14:35;trohrmann;I am not so sure about your verdict about option 2). At the moment it is correct that communication logic to interact with an external resource manager is encapsulated in the {{ResourceManagerDriver}}. However, this does not have to mean that the logic to register and deregister an application should also be the responsibility of the {{ResourceManager}}. I think this actually shows in the old code where we call into the {{ResourceManager}} to deregister the application independent of its leadership. Moreover, the decision whether to shut down the cluster or not is currently made by the {{Dispatcher}}. Hence, this component should be able to do this independent whether there is a {{RM}} running or not (also think about the hypothetical case where we split the {{Dispatcher}} and {{ResourceManager}} components into several processes).

Concerning your proposal of not doing the deregistration if there is no leading {{ResourceManager}}: How will this work if we use a K8s job? If I am not mistaken, then the return value of the process decides whether the job is restarted or not by K8s. So if we shut down normally but cannot deregister the application, then we will continue and stop with a zero exit code. So in this scenario, K8s will terminate the job but we won't clean up other K8s resources.

For Yarn, this proposal can work I believe, even though it is not super nice.

Are you taking care of this problem [~xtsong]?;;;","31/Aug/21 03:30;xtsong;I think you're correct, about option 2) and about the K8s job.

Let me re-list the options I see.

1) Allow {{Dispatcher}} to deregister the application independently. This is probably the least invasive solution. The only down side I see is that it introduces the complexity of dealing with different external resource managers to the dispatcher, which is probably not very complex with a proper unified abstraction.

2) Run the leader election process for the whole {{DispatcherResourceManagerComponent}}. This would guarantee we either have both a leading {{Dispatcher}} and a leading {{ResourceManager}}, or neither or them. This is IMHO too invasive to be done in the release stabilization phase. 

Speaking of splitting a JobManager process into multiple processes, I'm not entirely sure whether it is indeed needed. And even this is needed in future, I wonder would it be good enough to have the {{Dispatcher}} and the {{ResourceManager}} in one process, while each {{JobMaster}} in a separate process. I guess it depends on what essential demands we see in separating the process.

3) Make {{Dispatcher}} and {{ResourceManager}} talks to each other via RPC. This should work in scenarios where the leading RM is somewhere else, but not in scenarios where there's no leading RMs.

TBH, I'm not entirely sure which one of 1) & 2) is better. Maybe slightly learning towards 2), which in general simplifies things rather than complicates them, if we leave aside the topic of splitting the process. WDTY?

I'd be happy to look into this, but cannot promise to take care of it for 1.14.0. It really depends on to which direction we decide to go. For option 1), I can have a try. For option 2), I would suggest to leave with the problem for now and fix it for the next major release.;;;","31/Aug/21 03:31;xtsong;cc [~wangyang0918];;;","31/Aug/21 07:05;wangyang0918;[~trohrmann] Using a K8s job could help a bit when doing the deregistration failed. However, we still have the residual TaskManager pods and flink ConfigMaps. Maybe let the JobManager relaunched and recover the finished or failed jobs, then the dispatcher will deregister the application again. It is more reasonable.

For Yarn, I am afraid it is in the same situation. Even though the JobManager(application master) exit with zero code, it will be launched again when deregistering failed.

 

For the new options #1 [~xtsong] listed, do you mean let the leader dispatcher do the deregistration? If it is, what will happen without leader.;;;","31/Aug/21 07:24;xtsong;bq. For the new options #1 Xintong Song listed, do you mean let the leader dispatcher do the deregistration? If it is, what will happen without leader.

Yes, I meant let the leading dispatcher to do the deregistration. According to [~trohrmann], the decision whether to shut down the cluster or not is currently made by the Dispatcher. IIUC, that means there should not be a shutdown without a leading dispatcher, because there won't be any dispatcher to make that decision without obtaining leadership.;;;","31/Aug/21 13:31;trohrmann;It is a good question what the easiest solution is. I think the proper solution is option 1) because it does not only solve a symptom. Option 2) is desirable for other reasons as well (no spread leaders across different processes, less request load on HA system because there is only a single leader election) but it also solves the described problem here (even though it is more indirectly).

I do see that option 1) will complicate things a bit because we have to create new {{YarnClient}} and {{NamespacedKubernetesClient}} instances that are now nicely encapsulated in the {{ResourceManagerDriver}}. I do think that we can manage some of this complexity by choosing proper abstractions. But still, it will make the system slightly more complicated.

Maybe we can start by looking into option 2) first in order to better understand the scope of this change.;;;","01/Sep/21 01:44;xtsong;Alright, I'll take a look into option 2).;;;","13/Sep/21 10:06;dmvk;Hi [~xtsong] , is there any progress on the issue?;;;","13/Sep/21 10:36;xtsong;[~dmvk],
No, I haven't started on this.;;;","11/Oct/21 09:09;trohrmann;Quick check on the status [~xtsong]? The reason is that we see a high load of requests coming from Flink K8s HA services and we hope that by reducing the number of leader elections per JobManager process could mitigate the situation a bit.;;;","11/Oct/21 09:31;xtsong;I'm a bit overloaded recently, and probably won't have time for this in at least a month from now.
I'm un-assigning myself, in case someone else wants to work on this.
Will try to pick this up later if no one else work on it.;;;","14/Nov/21 10:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","14/Jan/22 12:08;trohrmann;I've finalized the PR. It is ready for review now.;;;","14/Jan/22 12:29;dmvk;That's a great news, I can do a review pass next week.;;;","24/Jan/22 11:00;mapohl;I changed the FLINK-25432 link from ""blocks"" to ""relates to"" because we're considering the order of cleanup (i.e. {{JobMaster}} needs to be closed before cleaning up the {{{}HighAvailabilityServices{}}}) in FLINK-25432 now.

This invariant can be removed as soon as we remove the old component-based leader election entirely. Currently, FLINK-24038 implements this feature in a way that it's configurable.;;;","26/Jan/22 22:53;trohrmann;Fixed via

5a2f220e31c
6a4eb9b815c
b609cb4462f
e8742f7f5ca
24fea2682ea
0afe3539d9e
bba7c417217
391ce7c6444
fef375e571e;;;","28/Jan/22 09:20;wangyang0918;After introducing the multiple component leader election service, we now only have two ConfigMaps.

1. ""<clusterId>-cluster-config-map""
 * Leader election for all the components(e.g. restserver, dispatcher, resourcemanager, jobmanager)
 * JobGraph store
 * Running job registry

2. ""<clusterId>-<jobId>-config-map""
 * Checkpoint store and counter

 

For jobmanager, the leader election and checkpoint store are using different ConfigMaps. I think it could happen the non-leader jobmanager overwrite the checkpoint store unexpectedly. Because we do not have the ""get-check_leadership-update"" transactional operation.

cc [~trohrmann] Do you think this is a valid concern?;;;","28/Jan/22 17:07;trohrmann;You are correct [~wangyang0918], that we no longer have the leader check when writing to the {{<clusterId>-<jobId>-config-map}} because there is no leader election happening for this map. 

I think it gives now effectively the same guarantees that we also have with the ZooKeeper HA implementation. If there is an old leader the leader can still write things into the config map. If I am not mistaken, then the danger is that checkpoints from the old leader can complete. If this is a problem then one should not use the multiple component leader election ha services. Alternatively, one could think about only using a single config map. However, this would increase the concurrent writes to it. Given that we have the same semantics with the Zk HA services, then I am not sure whether this is a real problem.;;;","07/Feb/22 03:33;wangyang0918;I will keep an eye on the multiple component leader election ha services when it is used in the production environment.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the deadlock issue caused by buffer listeners may not be notified,FLINK-24035,13397927,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,kevin.cyj,kevin.cyj,kevin.cyj,28/Aug/21 02:39,21/Dec/21 15:45,13/Jul/23 08:12,01/Sep/21 16:09,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,"The buffer listeners are not notified when the the local buffer pool receives available notification from the global pool. This may cause potential deadlock issue:
 # A LocalBufferPool is created, but there is no available buffers in the global NetworkBufferPool.
 # The LocalBufferPool registers an available buffer listener to the global NetworkBufferPool.
 # The BufferManager requests buffers from the LocalBufferPool but no buffer is available. As a result, it registers an available buffer listener to the LocalBufferPool.
 # A buffer is recycled to the global pool and the local buffer pool is notified about the available buffer.
 # The local buffer pool requests the available buffer from the global pool but the registered available buffer listener of BufferManager is not notified and it can never get a chance to be notified so deadlock occurs.

",,kevin.cyj,pnowojski,Thesharing,wind_ljy,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25185,FLINK-25407,,FLINK-12852,FLINK-13203,,,FLINK-23466,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 01 16:09:19 UTC 2021,,,,,,,,,,"0|z0ud7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/21 06:19;pnowojski;Merged to master as d82b69ef5db
Merged to release-1.14 as 851ed821bdf;;;","31/Aug/21 07:04;pnowojski;Solution here is to always request at least a single buffer.

This is not an issue on the output side, because there are no {{BufferListeners}} in that case. Also if buffer is requested on the output side, it will be used and eventually flushed.

On the input on the other hand, with exclusive buffers > 0, we are already requesting exclusive buffers in a blocking way with a timeout (FLINK-12852), so we know that task will be able to make progress regardless if we notify about more buffers or not. With exclusive buffers = 0, this solution requests a single floating buffer, so we will also be able to make a progress. Once data starts flowing/this single buffer will be recycled, listeners would be notified and more buffers would be requested.;;;","31/Aug/21 07:16;pnowojski;Re-opening as we want to refactor the code that we have just merged a bit.;;;","31/Aug/21 07:17;pnowojski;This is a similar problem as in FLINK-13203. We need to setup the input gates in a such way, that we are sure that they can always make progress.;;;","31/Aug/21 12:44;kevin.cyj;Thanks [~pnowojski]. I opened a refactor PR for this: [https://github.com/apache/flink/pull/17075.|https://github.com/apache/flink/pull/17075];;;","01/Sep/21 16:09;pnowojski;Follow up refactor
merged commit 75adb32 into apache:master now
merged as 4c826c8c3d9 into release-1.14;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Propagate unique keys for fromChangelogStream,FLINK-24033,13397847,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,twalthr,twalthr,twalthr,27/Aug/21 14:21,31/Aug/21 15:23,13/Jul/23 08:12,30/Aug/21 14:33,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,1.15.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Similar to FLINK-23915, we are not propagating unique keys for {{fromChangelogStream}} because it is not written into statistics.",,libenchao,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23835,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 30 14:35:49 UTC 2021,,,,,,,,,,"0|z0ucpk:",9223372036854775807,"Compared to 1.13.2, `StreamTableEnvironment.fromChangelogStream` might produce a different stream because primary keys were not properly considered before.",,,,,,,,,,,,,,,,,,,"30/Aug/21 14:33;twalthr;Fixed in master: 93c5357200e1bcf052872a13fa84c563dd967144
Fixed in 1.14: 488589305137c297e826b9294c871d52227f1ff0
Fixed in 1.13: dcfbf74c73c5dfe72c6fa057ba6ea4d35be19ab1;;;","30/Aug/21 14:35;twalthr;Even though this might have implications on existing pipelines, we merged this to the 1.13 branch because `StreamTableEnvironment.fromChangelogStream` is marked as {{@Experimental}} in 1.13.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix ""dept_id"" typo in SQL ""Getting Started"" page",FLINK-24029,13397797,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kylewang,kylewang,kylewang,27/Aug/21 11:49,01/Sep/21 09:36,13/Jul/23 08:12,01/Sep/21 06:48,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,1.15.0,,,Documentation,,,,,0,pull-request-available,,,,Current sql in doc of branch release-1.13 is still dep_id and we should fix it to dept_id,,jark,kylewang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 01 06:47:43 UTC 2021,,,,,,,,,,"0|z0uceg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/21 06:47;jark;Fixed in 
 - master: d2de24427477b4cb15c6062bee9756757d481d9d
 - release-1.14: ad44b550823a53ec00714959f11208086941dc03
 - release-1.13: 3f619ddcc247fdf95844eae3c16cc936560b451b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSystems list excessive dependencies in NOTICE,FLINK-24027,13397782,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,airblader,chesnay,chesnay,27/Aug/21 11:06,30/Aug/21 10:09,13/Jul/23 08:12,30/Aug/21 10:09,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Connectors / FileSystem,,,,,0,pull-request-available,,,,"The LicenseChecker finds several dependencies that are listed in the NOTICE but do not show up in the shade-plugin output. It could be that after the recent AWS/Hadoop bumps these are no longer being bundled (needs confirmation!).

{code}
17:05:14,651 WARN  NoticeFileChecker [] - Dependency com.fasterxml.jackson.core:jackson-annotations:2.12.1 is mentioned in NOTICE file /__w/1/s/flink-filesystems/flink-azure-fs-hadoop/src/main/resources/META-INF/NOTICE, but is not expected there
17:05:14,651 WARN  NoticeFileChecker [] - Dependency com.fasterxml.jackson.core:jackson-databind:2.12.1 is mentioned in NOTICE file /__w/1/s/flink-filesystems/flink-azure-fs-hadoop/src/main/resources/META-INF/NOTICE, but is not expected there

17:05:14,652 WARN  NoticeFileChecker [] - Dependency org.apache.hadoop.thirdparty:hadoop-shaded-protobuf_3_7:1.1.1 is mentioned in NOTICE file /__w/1/s/flink-filesystems/flink-s3-fs-presto/src/main/resources/META-INF/NOTICE, but is not expected there
17:05:14,652 WARN  NoticeFileChecker [] - Dependency org.apache.hadoop.thirdparty:hadoop-shaded-guava:1.1.1 is mentioned in NOTICE file /__w/1/s/flink-filesystems/flink-s3-fs-presto/src/main/resources/META-INF/NOTICE, but is not expected there
17:05:14,652 WARN  NoticeFileChecker [] - Dependency org.wildfly.openssl:wildfly-openssl:1.0.7.Final is mentioned in NOTICE file /__w/1/s/flink-filesystems/flink-s3-fs-presto/src/main/resources/META-INF/NOTICE, but is not expected there
17:05:14,652 WARN  NoticeFileChecker [] - Dependency commons-lang:commons-lang:2.6 is mentioned in NOTICE file /__w/1/s/flink-filesystems/flink-s3-fs-presto/src/main/resources/META-INF/NOTICE, but is not expected there

17:05:14,741 WARN  NoticeFileChecker [] - Dependency commons-lang:commons-lang:2.6 is mentioned in NOTICE file /__w/1/s/flink-filesystems/flink-fs-hadoop-shaded/src/main/resources/META-INF/NOTICE, but is not expected there
17:05:14,743 WARN  NoticeFileChecker [] - Dependency org.apache.hadoop.thirdparty:hadoop-shaded-protobuf_3_7:1.1.1 is mentioned in NOTICE file /__w/1/s/flink-filesystems/flink-s3-fs-hadoop/src/main/resources/META-INF/NOTICE, but is not expected there
17:05:14,743 WARN  NoticeFileChecker [] - Dependency org.apache.hadoop.thirdparty:hadoop-shaded-guava:1.1.1 is mentioned in NOTICE file /__w/1/s/flink-filesystems/flink-s3-fs-hadoop/src/main/resources/META-INF/NOTICE, but is not expected there
17:05:14,743 WARN  NoticeFileChecker [] - Dependency org.wildfly.openssl:wildfly-openssl:1.0.7.Final is mentioned in NOTICE file /__w/1/s/flink-filesystems/flink-s3-fs-hadoop/src/main/resources/META-INF/NOTICE, but is not expected there
17:05:14,743 WARN  NoticeFileChecker [] - Dependency commons-lang:commons-lang:2.6 is mentioned in NOTICE file /__w/1/s/flink-filesystems/flink-s3-fs-hadoop/src/main/resources/META-INF/NOTICE, but is not expected there

{code}",,airblader,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24028,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 30 10:09:03 UTC 2021,,,,,,,,,,"0|z0ucb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/21 11:07;chesnay;ping [~airblader];;;","27/Aug/21 19:33;airblader;Thanks, [~chesnay]. I'll look into this.;;;","30/Aug/21 10:09;chesnay;master: 401d7241c7e1eb432d988af3ef097dc5a633c9b4
1.14: 4c23df38555e364df9f14c67452d06de653ecdc2 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala checks not running in flink-training CI,FLINK-24022,13397759,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nkruber,nkruber,nkruber,27/Aug/21 09:39,11/Oct/21 21:22,13/Jul/23 08:12,30/Aug/21 08:53,1.13.3,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Documentation / Training / Exercises,,,,,0,pull-request-available,,,,"FLINK-23339 disabled Scala by default but therefore also disabled CI for newly checked-in changes on the Scala code.
We should run CI with Scala enabled",,nkruber,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23339,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 30 08:53:23 UTC 2021,,,,,,,,,,"0|z0uc60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/21 08:53;nkruber;Fixed on master via 65cde7910e5bff6283eef7ea536fde3ea5d4c9d4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential job unrecoverable due to Network failure,FLINK-24021,13397742,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,aitozi,aitozi,aitozi,27/Aug/21 08:25,01/Oct/21 08:14,13/Jul/23 08:12,03/Sep/21 13:09,1.12.5,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"Now we use zk to do leader election and retrieval for HA. And we register a fatalError handler in leaderElectionService and leaderRetrievalService to let jobManager or taskManager process exit at the time of some unexpected error.

But we don't do this at the time of curatorFrameworkClient#start in ZookeeperUtils. This may lead to some unexpected error like :

 
 # ZookeeperUtils start curator client, but failed by network loss, this will not throw exception now, because we do not register an error handler.
 # The network recover when master begin do leader election, so this will success
 # The leaderRetrieval begin to work by get_data, but this will not be executed, because the curator client start failed in phase 1.

 

So I think we should register a error handler in phase1 , so that we can fail fast. 

 ",,aitozi,klion26,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24437,FLINK-24117,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 03 13:09:54 UTC 2021,,,,,,,,,,"0|z0uc28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/21 08:26;aitozi;cc [~trohrmann@apache.org];;;","30/Aug/21 13:09;aitozi;I have create a fix [PR|https://github.com/apache/flink/pull/17053] , please help review [~trohrmann] when you are free.;;;","31/Aug/21 15:22;trohrmann;Thanks for reporting this problem [~aitozi]. I think you are right that this is a problem that we don't properly handle at the moment. I think it would be good to harden Flink against these kind of failures.;;;","03/Sep/21 13:09;trohrmann;Fixed via 

1.15.0: 6b4f8a4dce08b2852cfea9db8b5dffff0712aac6
1.14.0: 94387a4fde4ac1985deb0a96d8d0f7f0df874079;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dispatcher does not log JobMaster initialization error on info level,FLINK-24015,13397715,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,27/Aug/21 07:13,20/Sep/21 10:38,13/Jul/23 08:12,27/Aug/21 16:45,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"The {{Dispatcher}} does not log JobMaster initialization errors. This can make it very hard to understand why a job has failed if the client does not receive the {{JobResult}}. Therefore, I propose to log the failure cause for a job when it finishes on the {{Dispatcher}}.",,Thesharing,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24340,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 27 16:45:23 UTC 2021,,,,,,,,,,"0|z0ubw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/21 16:45;trohrmann;Fixed via

1.14.0: d7ef5a10fe54033592e7bc3a5e62e114ab1f8080
1.13.3: 218c0b6daa15768d33ced0745533db5b0dad1dd9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PulsarSourceITCase.testTaskManagerFailure fails due to NoResourceAvailableException,FLINK-24012,13397681,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,xtsong,xtsong,27/Aug/21 02:20,09/Sep/21 06:47,13/Jul/23 08:12,09/Sep/21 06:47,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Connectors / Pulsar,,,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22918&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=24431
",,syhily,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 09 06:47:46 UTC 2021,,,,,,,,,,"0|z0ubow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/21 15:35;syhily;[~xtsong] This issue is caused by https://issues.apache.org/jira/browse/FLINK-23807 which has been fixed.;;;","09/Sep/21 06:47;xtsong;Thanks [~syhily]. Closing the ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HybridSource needs to forward checkpoint notifications,FLINK-24010,13397657,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,thw,thw,thw,26/Aug/21 22:29,31/Aug/21 15:22,13/Jul/23 08:12,30/Aug/21 22:27,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,1.15.0,,,,,,,,0,pull-request-available,,,,"Since the reader currently swallows notifyCheckpointComplete, offset commit in contained Kafka consumer doesn't happen.",,thw,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 30 03:28:15 UTC 2021,,,,,,,,,,"0|z0ubjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/21 03:28;xtsong;[~thw]
FYI, the new branch `release-1.14` has been cut, and your PR #17006 is merged into `master` after the branch cutting.
Please add `1.15.0` as fix-versions. And if this PR is meant to appear in the 1.14 release (IIUC yes), please also port it to the release-1.14 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MailboxExecutorImplTest#testIsIdle does not test the correct behaviour,FLINK-24006,13397582,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fpaul,fpaul,fpaul,26/Aug/21 14:59,27/Aug/21 08:01,13/Jul/23 08:12,27/Aug/21 08:01,1.12.5,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Task,Tests,,,,0,pull-request-available,,,,"The test was introduced to ensure that the mailbox idleness is still counting new messages although the mailbox loop might have been stopped.

 

Unfortunately, the test does not stop the mailbox processor currently which leads to than the test even passes without the actual code changes of https://issues.apache.org/jira/browse/FLINK-19109",,fpaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 27 08:01:19 UTC 2021,,,,,,,,,,"0|z0ub2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/21 08:01;fpaul;Merged in master: e23b027940b1c4f3d201a9413ee9bcfaaeca0360;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resource requirements declaration may be incorrect if JobMaster disconnects with a TaskManager with available slots in the SlotPool,FLINK-24005,13397558,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,zhuzh,zhuzh,26/Aug/21 12:47,02/Sep/21 15:25,13/Jul/23 08:12,02/Sep/21 15:25,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"When a TaskManager disconnects with JobMaster, it will trigger the `DeclarativeSlotPoolService#decreaseResourceRequirementsBy()` for all the slots that are registered to the JobMaster from the TaskManager. If the slots are still available, i.e. not assigned to any task,  the `decreaseResourceRequirementsBy` may lead to incorrect resource requirements declaration.

For example, there is one job with 3 source tasks only. It requires 3 slots and declares for 3 slots. Initially all the tasks are running. Suddenly one task failed and waits for some delay before restarting. The previous slot is returned to the SlotPool. Now the job requires 2 slots and declares for 2 slots. At this moment, the TaskManager of that returned slot get lost. After the triggered `decreaseResourceRequirementsBy`, the job only declares for 1 slot. Finally, when the failed task starts to re-schedule, the job will declare for 2 slots while it actually needs 3 slots.

The attached log of a real job and logs of the added test in https://github.com/zhuzhurk/flink/commit/59ca0ac5fa9c77b97c6e8a43dcc53ca8a0ad6c37 can demonstrate this case.
Note that the real job is configured with a large ""restart-strategy.fixed-delay.delay"" and and large ""slot.idle.timeout"". So possibly in production it is a rare case.
",,guoyangze,klion26,maguowei,Thesharing,trohrmann,yunta,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/21 12:48;zhuzh;decrease_resource_requirements.log;https://issues.apache.org/jira/secure/attachment/13032548/decrease_resource_requirements.log",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 02 10:42:20 UTC 2021,,,,,,,,,,"0|z0uaxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/21 12:55;zhuzh;Would it work if we do not decrease resource requirements if a TM disconnects? If a slot is already assigned to tasks, it will trigger task failover and the slot release will trigger the decrease of its resource requirements. If it is not assigned, looks to me there is no need to decrease resource requirements for it.

cc [~chesnay] [~trohrmann];;;","26/Aug/21 13:03;chesnay;I would have to think things through a bit.

I'm not sure if it would work, because the release of slots in the slot pool is the very thing triggering the task failure. After that happens I'm not sure if anything even calls back into the slot pool in this scenario, so who would reduce the requirements?;;;","26/Aug/21 13:06;chesnay;What I don't understand yet is why the requirements are reduced at all when the TM in this scenario fails. If a TM fails the requirements are reduced by the previously fulfilled requirements; but if the task is restarted (aka, the requirement was reduced), then it shouldn't be fulfilling anything?;;;","27/Aug/21 03:41;zhuzh;That's also my question that why we need to decrease requirements when a TM disconnects. In my understanding, the requirement is from the job and should only change if the job status changes. If a TM disconnects with slots assigned to tasks, it will lead to FAILED task and the assigned slots releasing will decrease the requirements(in {{DeclarativeSlotPoolBridge#releaseSlot(...)}}).

2 side notes:
 - {{DeclarativeSlotPoolBridge#onFailAllocation()}} will decrease resource requirements. So it may lead to similar issue as  {{DeclarativeSlotPoolBridge#onReleaseTaskManager()}}.
 - The requirements decrease are only triggered in {{DeclarativeSlotPoolBridge}}, so this issue will not affect adaptive scheduler.;;;","27/Aug/21 06:34;chesnay;The current behavior is just an artifact of how the Bridge and legacy slot allocation works.

Conceptually you are right; requirements shouldn't change because a task fails. And with the AdaptiveScheduler this is also the case.
However, the legacy scheduler is unable to specify requirements directly, and only communicates with the slot pool via slot requests. To solve that if a slot is requested we increase the requirements. When the slot is freed (for whatever reason) we decrease them again, because if that slot is still required later on a new slot request will be issued.

So if a TM crashes we free all of it's slots, and we decrease the requirements for all slots that were currently in use, because we assume that they are later re-requested when the tasks are restarted, increasing the requirements again. And it seems here we do not identify correctly which slots are in use :/

;;;","27/Aug/21 20:40;trohrmann;This is a very good finding that we need to fix. I am also not sure why we can't simply rely on the {{DeclarativeSlotPoolBridge.releaseSlot(SlotRequestId slotRequestId, Throwable cause)}}. There must be something that prevents this method from being called if the slot is failed via {{DeclarativeSlotPoolService.releaseTaskManager}}.;;;","27/Aug/21 20:53;trohrmann;I think the problem is the following: When we call {{DeclarativeSlotPoolService.releaseTaskManager}}, then we release the slots in the {{DecalarativeSlotPool}}. This will remove all bookkeeping information from the pool and fail the payloads of the slots. Failing the payloads will eventually call {{DeclarativeSlotPoolBridge.releaseSlot(SlotRequestId, Throwable)}}. In this method we call call {{DeclarativeSlotPool.freeReservedSlot}} which returns the {{ResourceCounter}} if there was a reserved slot that we could free. However, in this case, we have already removed the slot and, thus, the {{DeclarativeSlotPool}} no longer knows about the released slot and their resource profile. I think that is why we reduce the resource requirements by all released slots when {{DeclarativeSlotPoolService.releaseTaskManager}} is called.

As shown by Zhu Zhu, this is not correct and we would actually only want to reduce the resource requirements by the number of assigned slots. Conceptually, we would like to let {{DeclarativeSlotPoolBridge.releaseSlot}} do the correct accounting if we knew the {{ResourceProfile}} of the previously released slots.;;;","28/Aug/21 07:44;trohrmann;One idea to solve this problem could be to move copy the mapping between {{AllocationID}} and {{ResourceProfile}} to the {{DeclarativeSlotPoolBridge}}. If this component would know about which slot  request was mapped to which {{ResourceProfile}}, it wouldn't need the information from the {{DeclarativeSlotPool.freeReservedSlot}} call to properly decrement the resource requirements.

We could achieve this if we store for a fulfilled request in the {{DeclarativeSlotPoolBridge}} the resource profile by which we have increased the requirements before. I think this information is already present in the {{PendingRequest}} class.;;;","29/Aug/21 22:17;chesnay;The analysis isn't quite correct.

The issue is that we do not correctly determine whether a slot was fulfilling a requirement or not for the DefaultScheduler.

When the task is failed then the slot is freed; the bridge decreases the requirements as it should and the pool holds on to the slot as it should.

However, the pool still has that slot in the {{slotToRequirementProfileMappings}}.

When the TM providing said slot now fails we look for which requirements the slots from said TM have been fulfilling, such that we can provide that information to the bridge so that it can reduce the requirements afterwards.

Which requirement the slot was fulfilling is derived from the {{slotToRequirementProfileMappings}}.
Because the slot is still in there it is considered to still be fulfilling a requirement, which we tell the bridge which in turn reduce the requirements, now for the second time.

That the slot is still kept in the {{slotToRequirementProfileMappings}} is generally fine. It is for example required by the AdaptiveScheduler because, in contrast to the DefaultScheduler, it is able to free slots without reducing requirements. But is also used when the slot is freed to figure out what requirement it was fulfilling.


Solution-wise I don't think [~zhuzh]s original suggestion of not decrementing requirements for lost slots could solve the issue. It wouldn't behave correctly in the case where the slots are being used at the moment. It would remove the slot from the backing pool, free the payload, but the call to freeReservedSlot would then be a no-op because the slot was already removed; we'd end up never reducing the requirements.

I would also prefer to not add more book-keeping into the bridge; we have enough as is and keeping all that in sync will just be annoying. It would in particular be unfortunate because so far the bridge never really had to deal with resource requirements, and I don't think we should start that.

What I will try tomorrow is to make {{DefaultDeclarativeSlotPool#releaseSlots}} a bit smarter such that it only determines the previous requirements for slots that are currently reserved. That should also solve the issue.;;;","30/Aug/21 06:19;zhuzh;Thanks for the explanation! [~trohrmann] [~chesnay]
I now understand why it was needed to decrease resource requirements for disconnected TMs. But I think it is more a like workaround. 
Conceptually, resource requirements should be from the job/scheduler instead of from the acquired slots. i.e.
 - Resource requirements should be increased when job requires for some slots, including reserving available slots({{DeclarativeSlotPoolBridge#reserveFreeSlotForResource()}}) or request new slots({{DeclarativeSlotPoolBridge#internalRequestNewAllocatedSlot()}})
 - Resource requirements should be decreased when job retract the requirements, including canceling pending requests({{DeclarativeSlotPoolBridge#cancelPendingRequests()}} & {{DeclarativeSlotPoolBridge#releaseSlot()}}) and freeing reserved slots({{DeclarativeSlotPoolBridge#releaseSlot()}}).

Therefore, I think what Till proposed might be a cleaner solution. However, it would need more work to maintain a mapping between {{AllocationID}} and {{ResourceProfile}}. The main obstacle I can see is that the {{ResourceProfile}} of {{PendingRequest}} may not be correct in all cases, considering {{DefaultDeclarativeSlotPool#adjustRequirements()}} may have been invoked. 
So at the moment, I'm also fine with that the fix proposed by [~chesnay] which looks simpler. ;;;","30/Aug/21 14:58;trohrmann;I am not sure whether making the {{DefaultDeclarativeSlotPool#releaseSlots}} smarter is the right solution here. The problem I see is that it moves responsibilities of the {{DeclarativeSlotPoolBridge}} into the {{DefaultDeclarativeSlotPool}}. W/o the bridge this specific functionality wouldn't be needed in the {{DefaultDeclarativeSlotPool}}. Instead, the whole logic to increment/decrement the resource requirements when allocation/releasing slots should live imo in the {{DeclarativeSlotPoolBridge}}.

I do see the problem caused by {{DefaultDeclarativeSlotPool#adjustRequirements}}. I think it can work if we return the actual {{ResourceProfile}} from the {{DeclarativeSlotPool.reserveFreeSlot}} method.

On a related note, isn't {{DefaultDeclarativeSlotPool#adjustRequirements}} only required by the default scheduler? Maybe this is something that should actually live in the {{DeclarativeSlotPoolBridge}}, too, because the {{AdaptiveScheduler}} does not need this remapping, if I am not mistaken.

Maybe we can make the {{releaseSlots}} smarter as a band aid for {{1.14.0}} and then fix the problem properly with {{1.14.1}} and {{1.15.0}}.;;;","30/Aug/21 16:20;chesnay;??On a related note, isn't DefaultDeclarativeSlotPool#adjustRequirements only required by the default scheduler???

In practice this is currently the case, but if the AdaptiveScheduler were to actually support resource profiles then it would also need it I believe.

??The problem I see is that it moves responsibilities of the DeclarativeSlotPoolBridge into the DefaultDeclarativeSlotPool.??

Generally yes but this was already the case ever since we added the DeclarativeSlotPool. This problem isn't new, and we made the conscious decision to limit any resource concerns to the pool. It certainly wasn't ideal, the so is that we still need the bridge in the first place.

??fix the problem properly with 1.14.1 and 1.15.0.??

I'm more inclined to leave things as is until we have a clear plan on how we resolve the duality of schedulers and slot-allocation-protocols. When we originally worked on the declarative resource management the intent was to write a new scheduler that supports all jobs; the DeclarativeSlotPoolBridge was only meant as a temporary measure.
We then limited the adaptive scheduler work to streaming jobs, but never really concluded what that meant for the bridge and DefaultScheduler.
Will we extend the adaptive scheduler to cover batch jobs?
Will we refactor the DefaultScheduler to directly work with declarative resource management (at the very least declaring requirements explicitly (which would solve _a lot_ of issues))?
Will we continue to have this compatibility layer?

Before we start any larger refactorings we should have a clear idea where we are even headed.
;;;","31/Aug/21 14:05;trohrmann;{quote}In practice this is currently the case, but if the AdaptiveScheduler were to actually support resource profiles then it would also need it I believe.{quote}
I am not so sure about this. The {{AdaptiveScheduler}} is in charge of the resource requirements and which slot it uses to fulfill what requirement. I would see it as the responsibility of the {{AdaptiveScheduler}} to adjust the requirements if it decides to {{Executions}} and {{Slots}} differently.

{quote}Generally yes but this was already the case ever since we added the DeclarativeSlotPool. This problem isn't new, and we made the conscious decision to limit any resource concerns to the pool. It certainly wasn't ideal, the so is that we still need the bridge in the first place.{quote}
I agree that the responsibilities are not well separated atm. From a maintenance perspective it would be desirable to move all the default scheduler specific logic into the {{DeclarativeSlotPoolBridge}}. Maybe it is now a bit clearer which responsibility should go into which class after having the {{AdaptiveScheduler}} written.

bq. Will we extend the adaptive scheduler to cover batch jobs?

Eventually this would be really nice. But I don't have a good idea how to do it atm.

bq. Will we refactor the DefaultScheduler to directly work with declarative resource management (at the very least declaring requirements explicitly (which would solve a lot of issues))?

I don't think so. The declarative resource management specific logic will probably be contained in the {{DeclarativeSlotPoolBridge}}.

bq. Will we continue to have this compatibility layer?

Yes I think so.

bq. Before we start any larger refactorings we should have a clear idea where we are even headed.

I agree. But we should acknowledge that the proposed fix will further entangle {{DeclarativeSlotPoolBridge}} and {{DefaultDeclarativeSlotPool}} which can make their maintenance harder in the future.
;;;","02/Sep/21 07:14;chesnay;I will open a separate ticket to rethink the relationship of the pool & bridge.

FLINK-24125;;;","02/Sep/21 07:15;chesnay;master: 0ff68d0e4aa80d7fa9c9ab4b9db65ca706a308f7
1.14: 0e413988a0784d3a15454924274b3e42991ebad3
1.13: c98babfa1751d2a4b2e9417534eb8bd20499943d  ;;;","02/Sep/21 10:40;trohrmann;Are you backporting this fix to {{1.13.3}} as well [~chesnay]?;;;","02/Sep/21 10:42;chesnay;Yes, just waiting for CI.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lookback mode doesn't work when mixing use of Python Table API and Python DataStream API,FLINK-24003,13397554,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hxbks2ks,dianfu,dianfu,26/Aug/21 12:33,31/Aug/21 15:13,13/Jul/23 08:12,31/Aug/21 06:11,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,API / Python,,,,,0,pull-request-available,,,,"For the following program:
{code}
import logging
import time

from pyflink.common.typeinfo import Types
from pyflink.datastream import StreamExecutionEnvironment, CoMapFunction
from pyflink.table import StreamTableEnvironment, DataTypes, Schema


def test_chaining():
    env = StreamExecutionEnvironment.get_execution_environment()
    t_env = StreamTableEnvironment.create(stream_execution_environment=env)
    t_env.get_config().get_configuration().set_boolean(""python.operator-chaining.enabled"", False)

    # 1. create source Table
    t_env.execute_sql(""""""
        CREATE TABLE datagen (
            id INT,
            data STRING
        ) WITH (
            'connector' = 'datagen',
            'rows-per-second' = '1000000',
            'fields.id.kind' = 'sequence',
            'fields.id.start' = '1',
            'fields.id.end' = '1000'
        )
    """""")

    # 2. create sink Table
    t_env.execute_sql(""""""
        CREATE TABLE print (
            id BIGINT,
            data STRING,
            flag STRING
        ) WITH (
            'connector' = 'blackhole'
        )
    """""")

    t_env.execute_sql(""""""
        CREATE TABLE print_2 (
            id BIGINT,
            data STRING,
            flag STRING
        ) WITH (
            'connector' = 'blackhole'
        )
    """""")

    # 3. query from source table and perform calculations
    # create a Table from a Table API query:
    source_table = t_env.from_path(""datagen"")

    ds = t_env.to_append_stream(
        source_table,
        Types.ROW([Types.INT(), Types.STRING()]))

    ds1 = ds.map(lambda i: (i[0] * i[0], i[1]))
    ds2 = ds.map(lambda i: (i[0], i[1][2:]))

    class MyCoMapFunction(CoMapFunction):

        def map1(self, value):
            print('hahah')
            return value

        def map2(self, value):
            print('hahah')
            return value

    ds3 = ds1.connect(ds2).map(MyCoMapFunction(), output_type=Types.TUPLE([Types.LONG(), Types.STRING()]))

    ds4 = ds3.map(lambda i: (i[0], i[1], ""left""),
                  output_type=Types.TUPLE([Types.LONG(), Types.STRING(), Types.STRING()]))

    ds5 = ds3.map(lambda i: (i[0], i[1], ""right""))\
             .map(lambda i: i,
                  output_type=Types.TUPLE([Types.LONG(), Types.STRING(), Types.STRING()]))

    schema = Schema.new_builder() \
        .column(""f0"", DataTypes.BIGINT()) \
        .column(""f1"", DataTypes.STRING()) \
        .column(""f2"", DataTypes.STRING()) \
        .build()

    result_table_3 = t_env.from_data_stream(ds4, schema)
    statement_set = t_env.create_statement_set()
    statement_set.add_insert(""print"", result_table_3)

    result_table_4 = t_env.from_data_stream(ds5, schema)
    statement_set.add_insert(""print_2"", result_table_4)

    statement_set.execute().wait()


if __name__ == ""__main__"":

    start_ts = time.time()
    test_chaining()
    end_ts = time.time()
    print(""--- %s seconds ---"" % (end_ts - start_ts))
{code}

Lookback mode doesn't work.",,dianfu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 31 06:12:00 UTC 2021,,,,,,,,,,"0|z0uawo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/21 06:12;hxbks2ks;Merged into master via cf08de4db65e0f224abf07673f8bffecda0013fb
Merged into release-1.14 via 851ed821bdfb68387327b9023e0652c90495cc30;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-dist NOTICE not properly checked by NoticeFileChecker,FLINK-23998,13397546,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,26/Aug/21 12:09,27/Aug/21 13:07,13/Jul/23 08:12,27/Aug/21 13:07,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Build System,,,,,0,pull-request-available,,,,"com.github.scopt:scopt_2.11:3.5.0 is still bundled by flink-dist because of the scala-shell, but is not mentioned in the NOTICE file.

We should add it, and check why the notice check did not catch it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18783,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 27 13:05:33 UTC 2021,,,,,,,,,,"0|z0uauw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/21 12:12;chesnay;The entry was removed in FLINK-18783 because I assumed it was only needed by the previous Akka version.;;;","27/Aug/21 13:05;chesnay;master: 03688f47d6f3b78084cc88825de30ed72c1f30a5
1.13: e34f67ee5945a93dfdf4e3f20ec361933ce6b535 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayDataSerializer and MapDataSerializer doesn't handle correctly for Null values,FLINK-23994,13397532,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,26/Aug/21 11:24,15/Dec/21 01:40,13/Jul/23 08:12,27/Aug/21 02:16,1.10.0,1.11.0,1.12.0,1.13.0,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.0,,,API / Python,,,,,0,pull-request-available,,,,,,dianfu,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 27 02:16:30 UTC 2021,,,,,,,,,,"0|z0uars:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/21 02:16;dianfu;Fixed in:
- master via 4b89ac81e869455fe30a495c55f92a9d366a87b0
- release-1.13 via 198628c454bff1fb434646ef03506af56d1e2349
- release-1.12 via cfe9519a8cf4220b48dc327c95d568dfb121a82f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update doc version in release-1.13 to 1.13.2,FLINK-23992,13397510,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,26/Aug/21 09:58,27/Aug/21 02:51,13/Jul/23 08:12,27/Aug/21 02:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,,,,,Documentation,,,,,0,pull-request-available,,,,Current version in doc of  branch release-1.13 is still 1.13.0 and we should fix it to 1.13.2,,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 27 02:51:16 UTC 2021,,,,,,,,,,"0|z0uamw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/21 02:51;yunta;fixed in release-1.13.2
b5e593453fd57911827aa185e46fcbb8f8495260;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Specifying yarn.staging-dir fail when staging scheme is different from default fs scheme,FLINK-23991,13397472,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zuston,zuston,26/Aug/21 08:46,23/Sep/21 08:33,13/Jul/23 08:12,23/Sep/21 08:29,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Deployment / YARN,,,,,0,pull-request-available,,,,"When the yarn.staging-dir path scheme is different from the default fs scheme, the client will fail fast.",,gaborgsomogyi,klion26,zuston,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 23 08:27:50 UTC 2021,,,,,,,,,,"0|z0uaeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/21 07:49;zuston;Could you help assign ticket to me? [~lirui] [~jark];;;","23/Sep/21 08:27;gaborgsomogyi;Merged to master in 5bf8d649627c801a5154bb78682308fcefe9210f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"'Run kubernetes pyflink application test' fail with ""Package 'python3.7' has no installation candidate"" ",FLINK-23984,13397411,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dianfu,xtsong,xtsong,26/Aug/21 05:00,15/Dec/21 01:40,13/Jul/23 08:12,27/Aug/21 14:50,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.0,,,API / Python,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22862&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=4201

{code}
Aug 26 03:34:14 Step 1/9 : FROM test_kubernetes_application-1
Aug 26 03:34:14  ---> 95da2424deaa
Aug 26 03:34:14 Step 2/9 : RUN apt-get update -y && apt-get install -y python3.7 python3-pip python3.7-dev && rm -rf /var/lib/apt/lists/*
Aug 26 03:34:17  ---> Running in 4751135e4612
Aug 26 03:34:17 Get:1 http://deb.debian.org/debian bullseye InRelease [113 kB]
Aug 26 03:34:17 Get:2 http://security.debian.org/debian-security bullseye-security InRelease [44.1 kB]
Aug 26 03:34:17 Get:3 http://deb.debian.org/debian bullseye-updates InRelease [36.8 kB]
Aug 26 03:34:17 Get:4 http://security.debian.org/debian-security bullseye-security/main amd64 Packages [27.5 kB]
Aug 26 03:34:17 Get:5 http://deb.debian.org/debian bullseye/main amd64 Packages [8178 kB]
Aug 26 03:34:19 Fetched 8399 kB in 2s (5266 kB/s)
Aug 26 03:34:19 Reading package lists...
Aug 26 03:34:20 Reading package lists...
Aug 26 03:34:20 Building dependency tree...
Aug 26 03:34:20 Reading state information...
Aug 26 03:34:20 Package python3.7 is not available, but is referred to by another package.
Aug 26 03:34:20 This may mean that the package is missing, has been obsoleted, or
Aug 26 03:34:20 is only available from another source
Aug 26 03:34:20 
Aug 26 03:34:20 E: Package 'python3.7' has no installation candidate
Aug 26 03:34:20 E: Unable to locate package python3.7-dev
Aug 26 03:34:20 E: Couldn't find any package by glob 'python3.7-dev'
Aug 26 03:34:20 E: Couldn't find any package by regex 'python3.7-dev'
The command '/bin/sh -c apt-get update -y && apt-get install -y python3.7 python3-pip python3.7-dev && rm -rf /var/lib/apt/lists/*' returned a non-zero code: 100
{code}",,dianfu,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24134,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 27 14:50:13 UTC 2021,,,,,,,,,,"0|z0ua0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/21 05:03;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22863&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=4065

;;;","26/Aug/21 05:03;xtsong;cc [~dianfu];;;","26/Aug/21 05:20;dianfu;[~xtsong] Thanks for reporting this issue. I'm looking into it~;;;","26/Aug/21 05:48;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22865&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=4157

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22868&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=4208
;;;","26/Aug/21 05:49;xtsong;Seems the master branch is broken on this one. It has been appearing in 4 consecutive builds.;;;","26/Aug/21 05:52;xtsong;The first commit with this problem is e11a5c52c613e121f7a7868cbbfd9e7c21551394.
;;;","26/Aug/21 05:55;dianfu;[~xtsong] I seems that the debian source has removed Python 3.7 packages. As a quick fix, I guess we need to disable the PyFlink tests for now and then look into how to fix it.;;;","26/Aug/21 05:58;xtsong;[~dianfu], thanks for the quick response. Let's disable the test for now, and keep this ticket open until we fix the problem properly.;;;","26/Aug/21 06:04;dianfu;Temporary disabled the PyFlink end to end tests in
- master via 3555741a12ba9fb65e8db9f731a131ab39d1cfe8
- release-1.13 via f998da0f4c711f179a766f6978b12fe0e3ad7fae
- release-1.12 via a5ec7a7222273368824ad6fa06e9dc51ec3af7c4;;;","27/Aug/21 14:50;dianfu;Fixed in:
- master via 83987dd090b7fed0158e6ab952cdc8669e08d029
- release-1.13 via 2bf0f369391a08eb0df5aba809cca12da1da98e9
- release-1.12 via adcd545d059d8dcb28d332fe4da56df209fb86b5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JVM crash when running RocksDBStateBackend tests,FLINK-23983,13397399,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yunta,xtsong,xtsong,26/Aug/21 03:15,07/Sep/21 02:30,13/Jul/23 08:12,07/Sep/21 02:30,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,Runtime / State Backends,Tests,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22855&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=11131

You would need to compare the mvn logs ""Running xxx"" with ""Test run xxx in xxx"" to find out the unfinished test.",,xtsong,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 07 02:30:18 UTC 2021,,,,,,,,,,"0|z0u9y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/21 03:15;xtsong;cc [~yunta];;;","26/Aug/21 03:21;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22855&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=9669;;;","30/Aug/21 03:36;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22981&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=11127;;;","30/Aug/21 03:50;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23011&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=11131;;;","01/Sep/21 12:11;chesnay;Found this in one of the .dumpstream files:
{code:java}
# Created at 2021-08-26T00:32:06.710
pure virtual method called# Created at 2021-08-26T00:32:06.710
terminate called without an active exception# Created at 2021-08-26T00:32:07.165
Aborted (core dumped) {code}
Quite possibly some issue with the cleanup of RocksDB.;;;","02/Sep/21 06:31;yunta;I could reproduced it locally by running RocksDBStateBackend tests repeatly. 
Since we bumped RocksDB recently, and no obvious tests changed under flink-statebackend-rocksdb module, I also doubted that some behavior has changed in RocksDB itself. It took me some time to figure out this might be caused by change in \{{StateBackendTestBase}} which changed the previous test by creating a new keyed state-backend but not disposing the older one (refer to https://github.com/apache/flink/commit/e8daf67ce5096da791e21d0915848c78c395822d cc @roman [~rkhachatryan]  ).;;;","07/Sep/21 02:30;yunta;Merged
master: 4f5978fd4f53428b6af85f487f3f2abbf2f590fd
release-1.14: 7bf42400d99a5d3421d85fa52309f7c9a870537a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PulsarSourceITCase.testIdleReader failed on azure,FLINK-23971,13397216,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,ruanhang1993,roman,roman,25/Aug/21 15:21,12/Jan/22 14:24,13/Jul/23 08:12,01/Sep/21 09:54,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Connectors / Pulsar,,,,,0,pull-request-available,test-stability,,,"{code:java}
[INFO] Running org.apache.flink.connector.pulsar.source.PulsarSourceITCase
[ERROR] Tests run: 8, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 353.527 s <<< FAILURE! - in org.apache.flink.connector.pulsar.source.PulsarSourceITCase
[ERROR] testIdleReader{TestEnvironment, ExternalContext}[2]  Time elapsed: 4.549 s  <<< FAILURE!
java.lang.AssertionError:

Expected: Records consumed by Flink should be identical to test data and preserve the order in multiple splits
     but: Unexpected record 'tj7MpFRWX95GzBpSF3CCjxKSal6bRhR0aF'
   at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
   at org.apache.flink.connectors.test.common.testsuites.SourceTestSuiteBase.testIdleReader(SourceTestSuiteBase.java:193)
{code}
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22819&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=24448]

This is the same error as in FLINK-23828 (kafka).",,becket_qin,dwysakowicz,gaoyunhaii,renqs,ruanhang1993,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23828,,,,,,,,,,,,"27/Aug/21 06:27;ruanhang1993;error.log;https://issues.apache.org/jira/secure/attachment/13032584/error.log",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 01 09:54:44 UTC 2021,,,,,,,,,,"0|z0u8ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/21 03:30;xtsong;cc [~syhily] [~renqs];;;","27/Aug/21 04:11;ruanhang1993;There is a problem in the test method _matchesMultipleSplitTestData._ When there are the same records between two split, the test may fail.

For example, there are totally two splits for the test. And the first split records is like \{a,b,b,c}, the second split records is like \{a,b,c,d}. The final result is \{a,a,b,c,d,b,b,c}. It is a right result, but the test will fail.

I think we need to add a prefix for each record in different splits, like ""0-XXXXXX"" for the first split. Then the test will return the right judgement.

The test should fail in both _testMultipleSplits_ and _testIdleReader_ test case. But only the latter test case occurs, and the problem probability is supposed to be very low. Maybe we need more evidence about it.;;;","27/Aug/21 06:33;ruanhang1993;After a lot of times to test, I reproduced the error and get the [^error.log] that I have provided in the attachments.

The log shows that there is a duplicate string ""3"" in both split 2 and split 3. When taking the next loop, the logic uses the ""3"" in split 2(we should take it from split 3), the next string ""aXf1QgrCBzQNlr"" in the result can not be found as the next string after doing that.;;;","30/Aug/21 03:52;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23011&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24445;;;","30/Aug/21 03:56;xtsong;[~ruanhang1993],
Thanks for looking into this. Would you like to provide a PR to fix this?

cc [~renqs], could you help verify [~ruanhang1993]'s findings?;;;","30/Aug/21 07:48;ruanhang1993;Fine, I will provide a PR to fix it.;;;","30/Aug/21 08:10;xtsong;Thanks [~ruanhang1993], you're assigned. Please move ahead.;;;","30/Aug/21 08:20;renqs;Thanks [~ruanhang1993] for the investigation! I double checked the code and [~ruanhang1993]'s assumption is reasonable.

Also I think this ticket shares the same cause as FLINK-23828. I'll mark it as related to this one. ;;;","01/Sep/21 09:54;becket_qin;PR merged to master: ad052cc056c7d6e63d8356dbd22d6a98b54743c3

Cherry-picked to release-1.14: 267b863683b23b8b3df29bee55ac58a25ca1fcd0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UpdateKind trait is not propagated properly in changeLog inference for DAG optimizing,FLINK-23962,13397085,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,icshuo,icshuo,icshuo,25/Aug/21 07:19,30/Aug/21 13:40,13/Jul/23 08:12,30/Aug/21 13:40,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,1.15.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"For sql jobs with multi-sinks, the plan is divided into relNode blocks, changeLog mode should be also inferred among blocks. Currently, updateKind trait is not propagated properly from parent block to child blocks for the following pattern.

 
                             -> block3
 block0 -> block1 -> block4
             -> block2

 

In the above example, if block3 requires UB and block2, block4 do not require UB, block1 only contains Calc node.

For Agg in block0, UB should be emitted, but the updateKind for block0 is inferred as ONLY_UPDATE_AFTER.",,godfreyhe,icshuo,libenchao,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 30 13:40:57 UTC 2021,,,,,,,,,,"0|z0u80o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/21 07:22;TsReaper;This seems to be a bug that affects the correctness of data. Shall we make it a blocker [~lzljs3620320] [~godfreyhe] ?;;;","25/Aug/21 07:28;lzljs3620320;Yes, it affects the correctness of data. I think it should be a blocker of 1.14.0.;;;","30/Aug/21 13:40;godfreyhe;Fixed in 1.15.0: ffd2520620069840b88cde72e2f02fd99a96ea70
Fixed in 1.14.0: d08e10849fd711cf375fcfbf05b9e1fe6d6ce94f
Fixed in 1.13.3: e81e73e55ccc6797a768e98f6ffaa135122a2bec;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamTaskTimerTest.checkScheduledTimestamps fails on azure,FLINK-23960,13397054,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,xtsong,xtsong,25/Aug/21 03:28,06/Sep/21 07:54,13/Jul/23 08:12,06/Sep/21 07:54,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,Runtime / Task,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22770&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=10736

{code}
Aug 24 22:32:06 [ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.421 s <<< FAILURE! - in org.apache.flink.streaming.runtime.operators.StreamTaskTimerTest
Aug 24 22:32:06 [ERROR] checkScheduledTimestamps  Time elapsed: 1.372 s  <<< FAILURE!
Aug 24 22:32:06 java.lang.AssertionError: expected:<1> but was:<0>
Aug 24 22:32:06 	at org.junit.Assert.fail(Assert.java:89)
Aug 24 22:32:06 	at org.apache.flink.streaming.runtime.operators.StreamTaskTimerTest.verifyNoException(StreamTaskTimerTest.java:166)
Aug 24 22:32:06 	at org.apache.flink.streaming.runtime.operators.StreamTaskTimerTest.checkScheduledTimestamps(StreamTaskTimerTest.java:129)
Aug 24 22:32:06 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 24 22:32:06 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 24 22:32:06 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 24 22:32:06 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
Aug 24 22:32:06 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 24 22:32:06 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 24 22:32:06 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 24 22:32:06 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 24 22:32:06 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Aug 24 22:32:06 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Aug 24 22:32:06 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
Aug 24 22:32:06 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
Aug 24 22:32:06 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
Aug 24 22:32:06 	at java.base/java.lang.Thread.run(Thread.java:829)
{code}",,akalashnikov,pnowojski,RocMarshal,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 06 07:54:39 UTC 2021,,,,,,,,,,"0|z0u7ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/21 03:47;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23008&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=9098;;;","31/Aug/21 17:00;akalashnikov;The cause of this problem is https://issues.apache.org/jira/browse/FLINK-23208.
After these changes, if we have processingTime equal to currentTime the result time will be 1 ms, but if we have procesingTime less than currentTime the result will be 0. So if we have a sequence like that:
{noformat}
registerTimer(currentTime, ...)
registerTimer(currentTime-1, ...)
{noformat}
According to the test, we should execute it in order of registration but in fact, we reorder it because of the reason which I described before.
I personally don't understand the semantic of ProcessingTimeServiceUtil#getProcessingTimeDelay and why we should increase processing time on 1 ms(especially why we should do it with `0`). So maybe, [~pnowojski] or [~wind_ljy] can you help with that? I mean it should be simple if we are sure about changes in FLINK-23208 then we should change the `checkScheduledTimestamps` test according to the new semantic.

P.S. It will be nice if you explain or give me some link with an explanation about why `A watermark T says we won't see elements in the future with a timestamp smaller or equal to T` and why processing time service should following the same rule.;;;","01/Sep/21 06:22;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23250&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=9095;;;","01/Sep/21 09:03;pnowojski;[~akalashnikov] for the reason behind the {{getProcessingTimeDelay}} you can check FLINK-9857 that introduced it. In short the issue is that we want timers for time {{t}} to fire AFTER all records with {{timestamp <= t}} have been already processed. Without this 1ms delay, if we had fired the timer for {{t}}  at the timestamp {{t}}, it would be possible that we would process another record for {{timestamp == t}} in the same millisecond. More concrete example, we could fire a window timer, produce records for that window and then we would receive another record that should have belong to that already emitted window.

As it was unclear to you, can you expand the comment {{getProcessingTimeDelay}} to avoid future confusions?

For adjust the tests, I think yes. We should adjust the expected sequence. ;;;","01/Sep/21 13:42;akalashnikov;[~pnowojski], thanks for explanation. But now I think that we have a problem with the design. SystemProcessingTimeService says nothing that it can process only records(in fact, it can process everything). So I was confused why in such based level service as SystemProcessingTimeService we have integrated logic for processing records. I believe it is better to have something like RecordProcessingTimeService which can do specific logic for record processing.

Anyway, I don't think that it is a big problem. So I just will fix the test and try to improve java-doc.;;;","01/Sep/21 14:30;pnowojski;I don't think that it's only record specific. Instead of records, think about any events. Some event happened and you are registering a timer for timestamp {{t}} that you want to trigger after that timestamp {{t}} has passed. We don't want the same event for the same timestamp {{t}} to register a second timer 0.01ms after firing the previous one prematurely.  To do so, we are firing timer for {{t}} at {{t+1}} if {{t}} is not in the past.;;;","06/Sep/21 07:54;pnowojski;merged to master as 25b985de06b^^..25b985de06b
merged to release-1.14 as 5a68bb88d92^^..5a68bb88d92;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python Client Executable priority incorrect for PYFLINK_CLIENT_EXECUTABLE environment variable,FLINK-23948,13396945,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,autophagy,autophagy,24/Aug/21 14:10,25/Aug/21 09:20,13/Jul/23 08:12,25/Aug/21 09:16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.0,1.14.0,,,,API / Python,Documentation,,,,0,pull-request-available,,,,"The [documentation|https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/python/python_config/#python-client-executable] for configuring the python client executable states: 
{quote}The priority is as following:
1. the command line option ""-pyclientexec"";
2. the environment variable PYFLINK_CLIENT_EXECUTABLE;
3. the configuration 'python.client.executable' defined in flink-conf.yaml
{quote}
I set \{{python.client.executable}} to point to Python 3.6, and submitted a job that contained Python 3.8 syntax. Running the job normally results in a Syntax Error as expected, and the \{{pyclientexec}} and \{{pyClientExecutable}} CLI flags let me override this setting and point to Python 3.8. However, setting the \{{PYFLINK_CLIENT_EXECUTABLE}} *did not overwrite the \{{python.client.executable}} setting*.

{code:bash}
export PYFLINK_CLIENT_EXECUTABLE=/usr/bin/python3.8
./bin/flink run --python examples/python/table/batch/python38_test.py
{code}

Still used Python 3.6 as the Python client interpreter.",,autophagy,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23823,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 25 09:16:25 UTC 2021,,,,,,,,,,"0|z0u75k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/21 09:16;hxbks2ks;Merged into master via 5388d44c3d74e6187903cbc3ea87177f08446b84
Merged into release-1.13 via 97382e1c5c5a0a27c36228b96c8fe7d613f73f2e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Application mode fails fatally when being shut down,FLINK-23946,13396933,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,trohrmann,trohrmann,24/Aug/21 12:48,01/Feb/22 14:31,13/Jul/23 08:12,01/Feb/22 14:31,1.12.5,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.4,1.15.0,,,,Runtime / Coordination,,,,,2,pull-request-available,stale-assigned,,,"The application mode fails fatally when being shut down (e.g. if the {{Dispatcher}} loses its leadership). The problem seems to be that the {{ApplicationDispatcherBootstrap}} cancels the {{applicationExecutionTask}} and {{applicationCompletionFuture}} that can trigger the execution of the fatal exception handler in the handler of the {{applicationCompletionFuture}}.

I suggest to only call the fatal exception handler if an unexpected exception occurs in the {{applicationCompletionFuture}} callback.",,aitozi,dmvk,knaufk,leonard,trohrmann,wangm92,,,,,,,,,,,,,,,,,,,,,,,FLINK-24038,,,,,,,,,,,,,,,,,,FLINK-25271,,,FLINK-25235,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 17 09:40:49 UTC 2022,,,,,,,,,,"0|z0u72w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/21 07:31;dmvk;Would result in test flakiness without resolving FLINK-24038 first.;;;","11/Nov/21 10:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","10/Dec/21 08:38;trohrmann;Merged into 1.14.1 via

ff96e93b60ce6be62147f61220bcd964de2ddd9c
4567d872e58464c3de3d2f15099adc44552da9cd
0a99256586918fa9205953faa27bab99366c1545
b194e83fc7dd47af2227e77fdebc7375c10c1680;;;","03/Jan/22 20:35;trohrmann;Merged into master via 9247bf28b6b69ddcd781ca0991adfa1e8f1d0764;;;","11/Jan/22 07:38;knaufk;[~dmvk] This is only missing backports to 1.13 and potentially 1.12, correct?;;;","17/Jan/22 09:19;trohrmann;Yes, it is missing backports to {{1.13}} and {{1.12}}.;;;","17/Jan/22 09:40;dmvk;I'll still try to do a backport for 1.13, unless it's too much effort (this code has evolved a lot during past couple of releases). 1.12 is not supported anymore by the community, so I don't think we need to invest in that.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot start the cluster using S3 as the file system,FLINK-23945,13396921,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,trohrmann,zhongyangyang,zhongyangyang,24/Aug/21 11:55,15/Dec/21 01:40,13/Jul/23 08:12,27/Aug/21 16:50,1.12.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.0,,,Connectors / FileSystem,,,,,0,pull-request-available,,,,"{{high-availability.storageDir: s3:///flink/recovery}}
*When I performed the above configuration, the following error was reported*

Could not start cluster entrypoint KubernetesSessionClusterEntrypoint.
org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint KubernetesSessionClusterEntrypoint.
at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:201) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:585) [flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.kubernetes.entrypoint.KubernetesSessionClusterEntrypoint.main(KubernetesSessionClusterEntrypoint.java:61) [flink-dist_2.12-1.12.3.jar:1.12.3]
Caused by: java.io.IOException: Could not create FileSystem for highly available storage path (s3:/flink/recovery/flink-native-k8s-session-1)
at org.apache.flink.runtime.blob.BlobUtils.createFileSystemBlobStore(BlobUtils.java:92) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.runtime.blob.BlobUtils.createBlobStoreFromConfig(BlobUtils.java:76) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:115) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:338) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:296) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:224) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:178) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_292]
at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_292]
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:175) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
... 2 more
Caused by: java.io.IOException: null uri host.
at org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.create(AbstractS3FileSystemFactory.java:162) ~[?:?]
at org.apache.flink.core.fs.PluginFileSystemFactory.create(PluginFileSystemFactory.java:62) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:507) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:408) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.core.fs.Path.getFileSystem(Path.java:274) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.runtime.blob.BlobUtils.createFileSystemBlobStore(BlobUtils.java:89) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.runtime.blob.BlobUtils.createBlobStoreFromConfig(BlobUtils.java:76) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:115) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:338) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:296) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:224) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:178) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_292]
at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_292]
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:175) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
... 2 more
Caused by: java.lang.NullPointerException: null uri host.
at java.util.Objects.requireNonNull(Objects.java:228) ~[?:1.8.0_292]
at org.apache.hadoop.fs.s3native.S3xLoginHelper.buildFSURI(S3xLoginHelper.java:72) ~[?:?]
at org.apache.hadoop.fs.s3a.S3AFileSystem.setUri(S3AFileSystem.java:467) ~[?:?]
at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:234) ~[?:?]
at org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.create(AbstractS3FileSystemFactory.java:123) ~[?:?]
at org.apache.flink.core.fs.PluginFileSystemFactory.create(PluginFileSystemFactory.java:62) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:507) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:408) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.core.fs.Path.getFileSystem(Path.java:274) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.runtime.blob.BlobUtils.createFileSystemBlobStore(BlobUtils.java:89) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.runtime.blob.BlobUtils.createBlobStoreFromConfig(BlobUtils.java:76) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:115) ~[flink-dist

 

 ",,leonard,trohrmann,zhongyangyang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/21 16:26;zhongyangyang;截屏2021-08-27 上午12.24.30.png;https://issues.apache.org/jira/secure/attachment/13032560/%E6%88%AA%E5%B1%8F2021-08-27+%E4%B8%8A%E5%8D%8812.24.30.png","26/Aug/21 16:39;zhongyangyang;截屏2021-08-27 上午12.31.10.png;https://issues.apache.org/jira/secure/attachment/13032561/%E6%88%AA%E5%B1%8F2021-08-27+%E4%B8%8A%E5%8D%8812.31.10.png","27/Aug/21 06:50;zhongyangyang;截屏2021-08-27 下午2.50.46.png;https://issues.apache.org/jira/secure/attachment/13032585/%E6%88%AA%E5%B1%8F2021-08-27+%E4%B8%8B%E5%8D%882.50.46.png","27/Aug/21 06:51;zhongyangyang;截屏2021-08-27 下午2.51.24.png;https://issues.apache.org/jira/secure/attachment/13032586/%E6%88%AA%E5%B1%8F2021-08-27+%E4%B8%8B%E5%8D%882.51.24.png",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 27 16:50:17 UTC 2021,,,,,,,,,,"0|z0u708:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Aug/21 11:59;zhongyangyang;*this is my k8s job configuation*

apiVersion: batch/v1
 kind: Job
 metadata:
 labels:
 cattle.io/creator: norman
 name: otsp
 namespace: otsp-yd03-flink
 spec:
 backoffLimit: 6
 completions: 1
 parallelism: 1
 selector:
 matchLabels:
 job-name: otsp
 template:
 metadata:
 labels:
 job-name: otsp
 spec:
 serviceAccount: otsp-flink-sa
 containers:
 - args:
 - -c
 - /opt/flink/bin/kubernetes-session.sh
 -Dkubernetes.cluster-id=flink-native-k8s-session-9
 -Dkubernetes.container.image=acpimagehub.cgb.cn/otsp_dev/flink-s3:1.12.3.10
 -Djobmanager.heap.size=4096m
 -Dtaskmanager.memory.process.size=4096m -Dtaskmanager.numberOfTaskSlots=16
 -Dkubernetes.jobmanager.cpu=4
 -Dkubernetes.taskmanager.cpu=16
 -Dkubernetes.namespace=otsp-yd03-flink
 -Dkubernetes.jobmanager.service-account=otsp-flink-sa
 -Dkubernetes.rest-service.exposed.type=ClusterIP
 -Dmetrics.reporter.promgateway.class=org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporter
 -Dmetrics.reporter.promgateway.host=21.96.246.38
 -Dmetrics.reporter.promgateway.port=9091
 -Dmetrics.reporter.promgateway.jobName=myJob
 -Dmetrics.reporter.promgateway.randomJobNameSuffix=true
 -Dmetrics.reporter.promgateway.deleteOnShutdown=false
 -Dhigh-availability=zookeeper
 -Dhigh-availability.zookeeper.quorum=21.96.18.110:2181,21.96.18.111:2181,21.96.33.90:2181
 -Dhigh-availability.zookeeper.path.root=/flink
 -Dhigh-availability.cluster-id=cluster_one
 -Dhigh-availability.storageDir=s3://flink/rocovery
 -Ds3.access-key=ABCDEFGHIJKLMNOPQRST
 -Ds3.secret-key=*******
 -Ds3.endpoint=[http://21.96.18.197:8082/]
 -Ds3.path.style.access=true
 command:
 - /bin/sh
 image: acpimagehub.cgb.cn/otsp_dev/flink-s3:1.12.3.10
 imagePullPolicy: Always
 name: otsp
 resources:
 limits:
 cpu: ""8""
 memory: 8000Mi
 requests:
 cpu: ""1""
 memory: 1000Mi
 volumeMounts:
 - name: log
 mountPath: /opt/flink/log
 subPath: otsp/flink
 volumes:
 - name: log
 persistentVolumeClaim:
 claimName: otsp-yd03-flink-pvcvof
 dnsPolicy: ClusterFirst
 restartPolicy: Never

 

*this is my k8s job configuration log*

2021-08-24 11:41:37,480 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: jobmanager.rpc.address, localhost
 2021-08-24 11:41:37,485 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: jobmanager.rpc.port, 6123
 2021-08-24 11:41:37,485 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: jobmanager.memory.process.size, 1600m
 2021-08-24 11:41:37,485 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: taskmanager.memory.process.size, 1728m
 2021-08-24 11:41:37,485 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1
 2021-08-24 11:41:37,485 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: parallelism.default, 1
 2021-08-24 11:41:37,485 INFO org.apache.flink.configuration.GlobalConfiguration [] - Loading configuration property: jobmanager.execution.failover-strategy, region
 2021-08-24 11:41:38,365 INFO org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils [] - The derived from fraction jvm overhead memory (160.000mb (167772162 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
 2021-08-24 11:41:38,531 INFO org.apache.flink.kubernetes.utils.KubernetesUtils [] - Kubernetes deployment requires a fixed port. Configuration blob.server.port will be set to 6124
 2021-08-24 11:41:38,531 INFO org.apache.flink.kubernetes.utils.KubernetesUtils [] - Kubernetes deployment requires a fixed port. Configuration taskmanager.rpc.port will be set to 6122
 2021-08-24 11:41:38,538 INFO org.apache.flink.kubernetes.utils.KubernetesUtils [] - Kubernetes deployment requires a fixed port. Configuration high-availability.jobmanager.port will be set to 6123
 2021-08-24 11:41:38,601 INFO org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils [] - The derived from fraction jvm overhead memory (160.000mb (167772162 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
 2021-08-24 11:41:39,386 WARN org.apache.flink.kubernetes.KubernetesClusterDescriptor [] - Please note that Flink client operations(e.g. cancel, list, stop, savepoint, etc.) won't work from outside the Kubernetes cluster since 'kubernetes.rest-service.exposed.type' has been set to ClusterIP.
 2021-08-24 11:41:39,612 INFO org.apache.flink.kubernetes.KubernetesClusterDescriptor [] - Create flink session cluster flink-native-k8s-session-9 successfully, JobManager Web Interface: [http://flink-native-k8s-session-9-rest.otsp-yd03-flink:8081|http://flink-native-k8s-session-9-rest.otsp-yd03-flink:8081/]
 2021-08-24 11:41:39,629 WARN org.apache.flink.kubernetes.KubernetesClusterDescriptor [] - Please note that Flink client operations(e.g. cancel, list, stop, savepoint, etc.) won't work from outside the Kubernetes cluster since 'kubernetes.rest-service.exposed.type' has been set to ClusterIP.

 

*this is my jobmanager log*

sed: couldn't open temporary file /opt/flink/conf/sedfb53FU: Read-only file system
 sed: couldn't open temporary file /opt/flink/conf/sed8NXv1X: Read-only file system
 /docker-entrypoint.sh: line 75: /opt/flink/conf/flink-conf.yaml: Read-only file system
 sed: couldn't open temporary file /opt/flink/conf/sedDnhwpZ: Read-only file system
 /docker-entrypoint.sh: line 90: /opt/flink/conf/flink-conf.yaml.tmp: Read-only file system
 Start command: $JAVA_HOME/bin/java -classpath $FLINK_CLASSPATH -Xmx1073741824 -Xms1073741824 -XX:MaxMetaspaceSize=268435456 -Dlog.file=/opt/flink/log/jobmanager.log -Dlogback.configurationFile=[file:/opt/flink/conf/logback-console.xml|file:///opt/flink/conf/logback-console.xml] -Dlog4j.configuration=[file:/opt/flink/conf/log4j-console.properties|file:///opt/flink/conf/log4j-console.properties] -Dlog4j.configurationFile=[file:/opt/flink/conf/log4j-console.properties|file:///opt/flink/conf/log4j-console.properties] org.apache.flink.kubernetes.entrypoint.KubernetesSessionClusterEntrypoint -D jobmanager.memory.off-heap.size=134217728b -D jobmanager.memory.jvm-overhead.min=201326592b -D jobmanager.memory.jvm-metaspace.size=268435456b -D jobmanager.memory.heap.size=1073741824b -D jobmanager.memory.jvm-overhead.max=201326592b
 2021-08-24 11:41:48,607 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - --------------------------------------------------------------------------------
 2021-08-24 11:41:48,611 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Starting KubernetesSessionClusterEntrypoint (Version: 1.12.3, Scala: 2.12, Rev:eae8bb1, Date:2021-04-23T11:49:05+02:00)
 2021-08-24 11:41:48,612 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - OS current user: flink
 2021-08-24 11:41:49,024 WARN org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
 2021-08-24 11:41:49,084 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Current Hadoop/Kerberos user: flink
 2021-08-24 11:41:49,085 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - JVM: OpenJDK 64-Bit Server VM - Oracle Corporation - 1.8/25.292-b10
 2021-08-24 11:41:49,085 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Maximum heap size: 989 MiBytes
 2021-08-24 11:41:49,085 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - JAVA_HOME: /usr/local/openjdk-8
 2021-08-24 11:41:49,086 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Hadoop version: 2.8.3
 2021-08-24 11:41:49,087 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - JVM Options:
 2021-08-24 11:41:49,087 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -Xmx1073741824
 2021-08-24 11:41:49,087 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -Xms1073741824
 2021-08-24 11:41:49,087 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -XX:MaxMetaspaceSize=268435456
 2021-08-24 11:41:49,087 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -Dlog.file=/opt/flink/log/jobmanager.log
 2021-08-24 11:41:49,087 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -Dlogback.configurationFile=[file:/opt/flink/conf/logback-console.xml|file:///opt/flink/conf/logback-console.xml]
 2021-08-24 11:41:49,087 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -Dlog4j.configuration=[file:/opt/flink/conf/log4j-console.properties|file:///opt/flink/conf/log4j-console.properties]
 2021-08-24 11:41:49,087 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -Dlog4j.configurationFile=[file:/opt/flink/conf/log4j-console.properties|file:///opt/flink/conf/log4j-console.properties]
 2021-08-24 11:41:49,087 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Program Arguments:
 2021-08-24 11:41:49,088 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -D
 2021-08-24 11:41:49,088 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - jobmanager.memory.off-heap.size=134217728b
 2021-08-24 11:41:49,088 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -D
 2021-08-24 11:41:49,088 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - jobmanager.memory.jvm-overhead.min=201326592b
 2021-08-24 11:41:49,089 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -D
 2021-08-24 11:41:49,089 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - jobmanager.memory.jvm-metaspace.size=268435456b
 2021-08-24 11:41:49,089 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -D
 2021-08-24 11:41:49,089 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - jobmanager.memory.heap.size=1073741824b
 2021-08-24 11:41:49,089 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - -D
 2021-08-24 11:41:49,089 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - jobmanager.memory.jvm-overhead.max=201326592b
 2021-08-24 11:41:49,089 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Classpath: /opt/flink/lib/flink-csv-1.12.3.jar:/opt/flink/lib/flink-json-1.12.3.jar:/opt/flink/lib/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:/opt/flink/lib/flink-shaded-zookeeper-3.4.14.jar:/opt/flink/lib/flink-table-blink_2.12-1.12.3.jar:/opt/flink/lib/flink-table_2.12-1.12.3.jar:/opt/flink/lib/log4j-1.2-api-2.12.1.jar:/opt/flink/lib/log4j-api-2.12.1.jar:/opt/flink/lib/log4j-core-2.12.1.jar:/opt/flink/lib/log4j-slf4j-impl-2.12.1.jar:/opt/flink/lib/flink-dist_2.12-1.12.3.jar:::
 2021-08-24 11:41:49,089 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - --------------------------------------------------------------------------------
 2021-08-24 11:41:49,090 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Registered UNIX signal handlers for [TERM, HUP, INT]
 2021-08-24 11:41:49,102 INFO org.apache.flink.configuration.GlobalConfiguration

 

*When I remove the HA and S3 configuration, I can start the cluster normally*;;;","24/Aug/21 12:40;leonard;[~zhongyangyang] please use English in Jira for all developers can understand your issue, feel free to reopen once you updated. ;;;","24/Aug/21 13:07;zhongyangyang; {{high-availability.storageDir: s3:///flink/recovery}}
*When I performed the above configuration, the following error was reported*

Could not start cluster entrypoint KubernetesSessionClusterEntrypoint.
org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint KubernetesSessionClusterEntrypoint.
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:201) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:585) [flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.kubernetes.entrypoint.KubernetesSessionClusterEntrypoint.main(KubernetesSessionClusterEntrypoint.java:61) [flink-dist_2.12-1.12.3.jar:1.12.3]
Caused by: java.io.IOException: Could not create FileSystem for highly available storage path (s3:/flink/recovery/flink-native-k8s-session-1)
 at org.apache.flink.runtime.blob.BlobUtils.createFileSystemBlobStore(BlobUtils.java:92) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.blob.BlobUtils.createBlobStoreFromConfig(BlobUtils.java:76) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:115) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:338) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:296) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:224) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:178) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_292]
 at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_292]
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
 at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:175) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 ... 2 more
Caused by: java.io.IOException: null uri host.
 at org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.create(AbstractS3FileSystemFactory.java:162) ~[?:?]
 at org.apache.flink.core.fs.PluginFileSystemFactory.create(PluginFileSystemFactory.java:62) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:507) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:408) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.core.fs.Path.getFileSystem(Path.java:274) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.blob.BlobUtils.createFileSystemBlobStore(BlobUtils.java:89) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.blob.BlobUtils.createBlobStoreFromConfig(BlobUtils.java:76) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:115) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:338) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:296) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:224) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:178) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_292]
 at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_292]
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
 at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:175) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 ... 2 more
Caused by: java.lang.NullPointerException: null uri host.
 at java.util.Objects.requireNonNull(Objects.java:228) ~[?:1.8.0_292]
 at org.apache.hadoop.fs.s3native.S3xLoginHelper.buildFSURI(S3xLoginHelper.java:72) ~[?:?]
 at org.apache.hadoop.fs.s3a.S3AFileSystem.setUri(S3AFileSystem.java:467) ~[?:?]
 at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:234) ~[?:?]
 at org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.create(AbstractS3FileSystemFactory.java:123) ~[?:?]
 at org.apache.flink.core.fs.PluginFileSystemFactory.create(PluginFileSystemFactory.java:62) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:507) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:408) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.core.fs.Path.getFileSystem(Path.java:274) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.blob.BlobUtils.createFileSystemBlobStore(BlobUtils.java:89) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.blob.BlobUtils.createBlobStoreFromConfig(BlobUtils.java:76) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:115) ~[flink-dist_2.;;;","24/Aug/21 13:35;chesnay;Can you enable debug logging, try again and look for messages from the S3FileSystemFactory?;;;","25/Aug/21 09:11;trohrmann;Can you try specifying the host of your s3 service like {{s3://21.96.18.197:8082/flink/recovery}}? Maybe the problem is that {{-Ds3.endpoint=http://21.96.18.197:8082/}} won't get picked up.

cc [~arvid] do you know about any problems with the s3a filesystem?;;;","25/Aug/21 09:31;zhongyangyang;*when I specifying the host of your s3 service like {{s3://21.96.18.197:8082/flink/recovery,the following error was reported}}*

org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint KubernetesSessionClusterEntrypoint.
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:201) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:585) [flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.kubernetes.entrypoint.KubernetesSessionClusterEntrypoint.main(KubernetesSessionClusterEntrypoint.java:61) [flink-dist_2.12-1.12.3.jar:1.12.3]
 Caused by: java.io.IOException: Could not create FileSystem for highly available storage path (s3://21.96.18.197:8082/flink/rocovery/flink-native-k8s-session-11)
 at org.apache.flink.runtime.blob.BlobUtils.createFileSystemBlobStore(BlobUtils.java:92) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.blob.BlobUtils.createBlobStoreFromConfig(BlobUtils.java:76) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:115) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:338) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:296) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:224) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:178) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_292]
 at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_292]
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
 at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:175) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 ... 2 more
 Caused by: java.io.FileNotFoundException: Bucket 21.96.18.197 does not exist
 at org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:374) ~[?:?]
 at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:308) ~[?:?]
 at org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.create(AbstractS3FileSystemFactory.java:123) ~[?:?]
 at org.apache.flink.core.fs.PluginFileSystemFactory.create(PluginFileSystemFactory.java:62) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:507) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:408) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.core.fs.Path.getFileSystem(Path.java:274) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.blob.BlobUtils.createFileSystemBlobStore(BlobUtils.java:89) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.blob.BlobUtils.createBlobStoreFromConfig(BlobUtils.java:76) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:115) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:338) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:296) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:224) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:178) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_292]
 at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_292]
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
 at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:175) ~[flink-dist_2.12-1.12.3.jar:1.12.3]
 ... 2 more

 

*I feel flink treats {{21.96.18.197}}* as a bucket

when I configure like *{{s3://flink/recovery}}*, I can successfully start my cluster,

I don’t understand why it fails to start when configured according to the official website *{{s3:///flink/recovery}}*, but it can start successfully when configured according to *{{s3://flink/recovery}}*

 ;;;","25/Aug/21 09:57;chesnay;I suppose it's expected that is interprets the endpoint as the bucket because s3.endpoint is still set and evaluated by the filesystem, so in the end you have the endpoint twice in the URL. 

It could make sense that s3:///flink/recovery, fails since s3.endpoint contains a trailing slash and is injected later on as-is (I suppose?), you could theoretically end up with an incorrect number of slashes.

I couldn't find this exact issue in the web search; I found _similar_ issues because of wrongly formatted/encoded credentials however.
This issue is a bit of a mystery TBH because we have e2e tests that do exactly the same thing.;;;","25/Aug/21 11:36;arvid;Your syntax is incorrect. s3://<bucket>/<path> is always setting the bucket at a given endpoint.

For minio, we explicitly set http://<ip>:<port> as an {{s3.endpoint}} and then use s3 protocol just for actual bucket.

 -There is also a combined syntax (Virtual Hosted-Style Requests) where you set it to http(s)://<bucket>.<ip>:<port>/<path> [1]-

-[1] [https://docs.aws.amazon.com/AmazonS3/latest/userguide/VirtualHosting.html|https://docs.aws.amazon.com/AmazonS3/latest/userguide/VirtualHosting.html-]- (cannot be used in Flink);;;","26/Aug/21 15:01;trohrmann;Can we close this ticket then with ""information provided"" or is there a bug we need to fix?;;;","26/Aug/21 16:26;zhongyangyang;But the example on the official website is like +high-availability.storageDir: s3:///flink/recovery+

. Please see the [https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/ha/kubernetes_ha.html|http://example.com/] . I am worried that my misconfiguration may cause a production problem, but it may also be because I don’t know much about s3. I will  read more information about s3.

I tried to debug using the source code, and then found that he  new a org.apache.flink.core.fs.Path by String ""+s3:///flink/recovery""+, and then threw a +_null uri host_+ exception when calling the getFileSystem method

Due to some company reasons, I cannot show my debug process. The following demo  are the main reasons I found during the debug process that caused the exception to be thrown.

  !截屏2021-08-27 上午12.24.30.png|width=435,height=272!

Is the s3 example on the official website wrong？;;;","26/Aug/21 16:40;zhongyangyang;When I use s3:*//*flink/recovery for configuration, I feel that he is communicating with the S3 file system through the endpoint. Is the example on the official website really wrong? Or there is a problem with my understanding

!截屏2021-08-27 上午12.31.10.png|width=665,height=415!;;;","26/Aug/21 16:44;zhongyangyang;My English is not good, so if my expression is vague or incomprehensible to you, I will once again describe my problem as clearly as possible;;;","27/Aug/21 06:36;arvid;{quote}
I tried to debug using the source code, and then found that he  new a org.apache.flink.core.fs.Path by String ""s3:///flink/recovery"", and then threw a null uri host exception when calling the getFileSystem method
{quote}

This is expected: you leave out the bucket name. It should be {{s3://<bucket>/<path>}}. Do we have ""s3:///"" somewhere on our documentation?;;;","27/Aug/21 06:52;zhongyangyang;@[~arvid]

yes，You can open the official website link below

[https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/ha/kubernetes_ha.html]

 

Below is my screenshot from the official website

!截屏2021-08-27 下午2.51.24.png|width=665,height=286!

  !截屏2021-08-27 下午2.50.46.png|width=1177,height=142!;;;","27/Aug/21 08:58;trohrmann;Hmm, this is then a documentation mistake. I'll create a fix for it. Thanks for reporting this issue [~zhongyangyang].;;;","27/Aug/21 16:50;trohrmann;Fixed via 

1.14.0: c411ab685ffade8c91ace00b6bf1212246621269
1.13.3: 3b8379b58c9406f1b294b30993ad35aeb5bd2724
1.12.6: 6e4f119d65f9f384344028a91e2a6a5fe4ff684e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyflink.datastream.connectors module not documented in PyDocs,FLINK-23941,13396899,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,chesnay,chesnay,24/Aug/21 10:05,26/Aug/21 11:54,13/Jul/23 08:12,26/Aug/21 05:15,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,API / Python,Documentation,,,,0,pull-request-available,,,,"I don't know why, but the PyDocs don't contain anything for the Python Datastream connectors. As a result there is no exhaustive list for which sources/sinks are actually supported.

Furthermore there is no information regarding custom sources written in Python.

There is this [note|https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/python/datastream/intro_to_datastream_api/#create-using-datastream-connectors] saying that add_source only Kafka is supported. But it does not explain why that is the case, or how another connector could be made compatible.",,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22589,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 26 05:15:10 UTC 2021,,,,,,,,,,"0|z0u6vc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/21 05:15;dianfu;Merged to
- master via e11a5c52c613e121f7a7868cbbfd9e7c21551394
- release-1.13 via cff74f4576ab4430a865f1ab593379cad073dcba;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python UDFs instances are reinitialized if there is no input for more than 1 minute,FLINK-23936,13396838,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,24/Aug/21 07:25,15/Dec/21 01:40,13/Jul/23 08:12,24/Aug/21 11:33,1.10.0,1.11.0,1.12.0,1.13.0,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.0,,,API / Python,,,,,0,pull-request-available,,,,"We receive this feedback from some PyFlink users. After some investigation, we find out that the root case is that there is a mechanism in Beam that it will released the BundleProcessors which are inactive for more than 1 minute: https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/worker/sdk_worker.py#L90",,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 24 11:33:36 UTC 2021,,,,,,,,,,"0|z0u6hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Aug/21 11:33;dianfu;Fixed in
- master via 418bce6257756192f9f94ab5b08f236010146511
- release-1.13 via f11ed5de028ffd35900fcf6353a51e7db7a43bd5
- release-1.12 via acea21c016d2d95c1a512c767437c88683eb986e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Chaining optimization doesn't handle properly for transformations with multiple outputs,FLINK-23929,13396796,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,24/Aug/21 02:05,24/Aug/21 06:24,13/Jul/23 08:12,24/Aug/21 06:24,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,API / Python,,,,,0,pull-request-available,,,,,,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23754,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 24 06:24:33 UTC 2021,,,,,,,,,,"0|z0u68g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Aug/21 06:24;dianfu;Fixed in master via 3412970839eebcbeb42d8d1851d5e2fcc95e73a5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix go.mod file for SDK release,FLINK-23927,13396756,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sjwiesman,sjwiesman,sjwiesman,23/Aug/21 17:51,24/Aug/21 08:06,13/Jul/23 08:12,24/Aug/21 08:06,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,statefun-3.1.0,,,,,Stateful Functions,,,,,0,pull-request-available,,,,"To release the go SDK, the go.mod file needs to be in the `v3` subdirectory. ",,sjwiesman,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 24 08:06:20 UTC 2021,,,,,,,,,,"0|z0u5zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Aug/21 08:06;tzulitai;flink-statefun/master: c85c6db515cc14c409259542639c68e3c3f1dff6
flink-statefun/release-3.1: 8da52790789e4e116afe4be6e25c3af91a2341f9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SchemaTranslator looses primary key if schema is inferred,FLINK-23920,13396662,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,twalthr,twalthr,23/Aug/21 10:40,25/Aug/21 09:53,13/Jul/23 08:12,25/Aug/21 09:53,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"Schema derivation with primary key does not work correctly as the primary key is not propagated:

{code}
tableEnv.toChangelogStream(...,
    Schema.newBuilder().primaryKey(""key"").build(),
    ChangelogMode.upsert())
{code}",,airblader,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23835,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 25 09:53:17 UTC 2021,,,,,,,,,,"0|z0u5eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/21 09:53;twalthr;Fixed in 1.13.3: 4b682e88d093c2d610f00e0c827a48c7408ee83d
Fixed in 1.14.0: cd52056489ce02300a2277a0c6a102ca5a37d5b0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PullUpWindowTableFunctionIntoWindowAggregateRule generates invalid Calc for Window TVF,FLINK-23919,13396658,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingzhang,Yuval.Itzchakov,Yuval.Itzchakov,23/Aug/21 10:26,15/Dec/21 01:44,13/Jul/23 08:12,01/Nov/21 09:52,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Given the following Window TVF:
{code:java}
SELECT window_time, 
       MIN(alert_timestamp) as start_time, 
       MAX(alert_timestamp) as end_time 
FROM TABLE(TUMBLE(TABLE alert_table, DESCRIPTOR(alert_timestamp), INTERVAL '3' MINUTE)) 
WHERE service_source = 'source' 
GROUP BY window_start, window_end, window_time
{code}
Where the schema of alert_table is:
{code:java}
alert_timestamp: TIMESTAMP(3) ROWTIME INDICATOR
service_source: VARCHAR{code}
The following generates an invalid RowType:
{code:java}
Error while applying rule PullUpWindowTableFunctionIntoWindowAggregateRule, args [rel#358:StreamPhysicalWindowAggregate.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=RelSubset#356,window=TUMBLE(win_start=[window_start], win_end=[window_end], size=[3 min]),select=MIN(alert_timestamp) AS start_time, MAX(alert_timestamp) AS end_time, start('w$) AS window_start, end('w$) AS window_end, rowtime('w$) AS window_time), rel#367:StreamPhysicalExchange.STREAM_PHYSICAL.single.None: 0.[NONE].[NONE](input=RelSubset#355,distribution=single), rel#354:StreamPhysicalCalc.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=RelSubset#353,select=window_start, window_end, window_time, CAST(alert_timestamp) AS alert_timestamp,where==(service_source, _UTF-16LE'my source':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"")), rel#352:StreamPhysicalWindowTableFunction.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=RelSubset#351,window=TUMBLE(time_col=[alert_timestamp], size=[3 min]))]Error while applying rule PullUpWindowTableFunctionIntoWindowAggregateRule, args [rel#358:StreamPhysicalWindowAggregate.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=RelSubset#356,window=TUMBLE(win_start=[window_start], win_end=[window_end], size=[3 min]),select=MIN(alert_timestamp) AS start_time, MAX(alert_timestamp) AS end_time, start('w$) AS window_start, end('w$) AS window_end, rowtime('w$) AS window_time), rel#367:StreamPhysicalExchange.STREAM_PHYSICAL.single.None: 0.[NONE].[NONE](input=RelSubset#355,distribution=single), rel#354:StreamPhysicalCalc.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=RelSubset#353,select=window_start, window_end, window_time, CAST(alert_timestamp) AS alert_timestamp,where==(service_source, _UTF-16LE'Microsoft Defender for Identity':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"")), rel#352:StreamPhysicalWindowTableFunction.STREAM_PHYSICAL.any.None: 0.[NONE].[NONE](input=RelSubset#351,window=TUMBLE(time_col=[alert_timestamp], size=[3 min]))] at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:256) at org.apache.calcite.plan.volcano.IterativeRuleDriver.drive(IterativeRuleDriver.java:58) at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:510) at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:312) at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:69) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:62) at scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196) at scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194) at scala.collection.Iterator.foreach(Iterator.scala:943) at scala.collection.Iterator.foreach$(Iterator.scala:943) at scala.collection.AbstractIterator.foreach(Iterator.scala:1431) at scala.collection.IterableLike.foreach(IterableLike.scala:74) at scala.collection.IterableLike.foreach$(IterableLike.scala:73) at scala.collection.AbstractIterable.foreach(Iterable.scala:56) at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199) at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:108) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:58) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:163) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:83) at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77) at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:279) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1518) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:740) at org.apache.flink.table.api.internal.StatementSetImpl.execute(StatementSetImpl.java:99) java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at java.base/java.lang.Thread.run(Thread.java:834)Caused by: java.lang.RuntimeException: Error occurred while applying rule PullUpWindowTableFunctionIntoWindowAggregateRule at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:161) at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:268) at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:283) at org.apache.flink.table.planner.plan.rules.physical.stream.PullUpWindowTableFunctionIntoWindowAggregateRule.onMatch(PullUpWindowTableFunctionIntoWindowAggregateRule.scala:143) at 
org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:229) ... 31 moreCaused by: org.apache.flink.table.api.ValidationException: Field names must be unique. Found duplicates: [alert_timestamp] at org.apache.flink.table.types.logical.RowType.validateFields(RowType.java:272) at org.apache.flink.table.types.logical.RowType.<init>(RowType.java:157) at org.apache.flink.table.types.logical.RowType.of(RowType.java:297) at org.apache.flink.table.types.logical.RowType.of(RowType.java:289) at org.apache.flink.table.planner.calcite.FlinkTypeFactory$.toLogicalRowType(FlinkTypeFactory.scala:657) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWindowAggregate.aggInfoList$lzycompute(StreamPhysicalWindowAggregate.scala:60) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWindowAggregate.aggInfoList(StreamPhysicalWindowAggregate.scala:59) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalWindowAggregate.explainTerms(StreamPhysicalWindowAggregate.scala:86) at org.apache.calcite.rel.AbstractRelNode.getDigestItems(AbstractRelNode.java:409) at org.apache.calcite.rel.AbstractRelNode.deepHashCode(AbstractRelNode.java:391) at org.apache.calcite.rel.AbstractRelNode$InnerRelDigest.hashCode(AbstractRelNode.java:443) at java.base/java.util.HashMap.hash(HashMap.java:339) at java.base/java.util.HashMap.get(HashMap.java:552) at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1150) at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:589) at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:604) at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:148)
{code}
Looking at the code, it seems that when PullUpWindowTableFunctionIntoWindowAggregateRule is building the new Calc in WindowUtil.buildNewProgramWithoutWindowColumns, it is adding the rowtime column from the input row to the new calc without checking to see if there are any name collisions. Also, TBH I'm not entirely sure yet why the rowtime column of the input table is being added to the projected output row like that?

!image-2021-08-23-13-31-24-052.png|width=887,height=163!  

[~jark] would appreciate your help with this.",,godfreyhe,jark,jingzhang,Yuval.Itzchakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25084,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/21 10:31;Yuval.Itzchakov;image-2021-08-23-13-31-24-052.png;https://issues.apache.org/jira/secure/attachment/13032307/image-2021-08-23-13-31-24-052.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 01 09:52:27 UTC 2021,,,,,,,,,,"0|z0u5ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Aug/21 06:22;jark;cc [~qingru zhang];;;","10/Oct/21 17:08;Yuval.Itzchakov;[~jark] [~qingru.zhang] Did one of you guys get a chance to peak at this? Or perhaps give me pointers and I can investigate further.;;;","28/Oct/21 13:18;Yuval.Itzchakov;Anyone?;;;","29/Oct/21 03:05;jingzhang;[~Yuval.Itzchakov] I'm very sorry for late response. I would follow this JIRA soon and give you response later today.;;;","29/Oct/21 07:26;jingzhang;[~Yuval.Itzchakov], I reproduce the problem in my local environment. This is a bug.  cc [~jark]

> it is adding the rowtime column from the input row to the new calc without checking to see if there are any name collisions.

I agree with you.

>  I'm not entirely sure yet why the rowtime column of the input table is being added to the projected output row like that?

Because after transpose Calc and window TVF, the time attribute index may be changed. When construct new Window TVF, we should use new time attribute index.

 

Would you like to fix this bug?:)

If you don't have time, I would fix this bug later.

 

 ;;;","01/Nov/21 09:52;godfreyhe;Fixed in 1.15.0: 003df215b482c246c48c147b63b56608c6557cba
Fixed in 1.14.1: 5b3e3a8fd1dec3a41a7ff41835dc11456ad6836b
Fixed in 1.13.4: 934aa94c8509149079e375879ff5d3d4b86e15ad;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Propagate unique keys for temporary tables,FLINK-23915,13396649,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,twalthr,twalthr,twalthr,23/Aug/21 09:36,26/Aug/21 10:39,13/Jul/23 08:12,26/Aug/21 10:39,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"{{DatabaseCalciteSchema#100}} only propagates unique keys if the table is not temporary. This makes primary keys unusable in many cases.

-We should fix FLINK-15123 on the way.- Postponed due to complexity.",,aitozi,jark,libenchao,lzljs3620320,nkruber,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23835,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 26 10:39:07 UTC 2021,,,,,,,,,,"0|z0u5bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/21 10:39;twalthr;Fixed in 1.13.3: 89b10a8166aa84682e433362d6783805184df973
Fixed in 1.14.0: 1d2c565f68b947058209969a45c9c745118e6463;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnalignedCheckpointITCase fails with exit code 137 (kernel oom) on Azure VMs,FLINK-23913,13396630,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,rmetzger,rmetzger,23/Aug/21 08:26,30/Aug/21 05:58,13/Jul/23 08:12,30/Aug/21 05:57,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Network,,,,,0,test-stability,,,,"Cases reported in FLINK-23525:
- https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22618&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10338
- https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22618&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=4743
- https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22605&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=4743
- ... there are a lot more cases.

The problem seems to have started occurring around August 20.",UnalignedCheckpointITCase,pnowojski,rmetzger,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23652,,,,FLINK-23525,FLINK-23796,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 30 05:57:31 UTC 2021,,,,,,,,,,"0|z0u57k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/21 13:49;pnowojski;Maybe this is related to FLINK-23796 and FLINK-23776? [~arvid]?;;;","24/Aug/21 03:16;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22684&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=4743;;;","24/Aug/21 03:17;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22684&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10338;;;","24/Aug/21 20:16;arvid;This is probably a duplicate of FLINK-23794.;;;","25/Aug/21 02:57;xtsong;EventTimeWindowCheckpointingITCase

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22756&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10467;;;","25/Aug/21 03:15;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22770&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10341;;;","27/Aug/21 02:14;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22918&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=4746;;;","30/Aug/21 05:57;arvid;Note that this failure predates the fix in FLINK-23794. If any new case happens now, it is caused for a different reason as we completely removed InMemoryReporter for this test case.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Projections not selecting any metadata columns cause all declared metadata columns to be applied to the source,FLINK-23911,13396609,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,airblader,airblader,airblader,23/Aug/21 06:37,30/Aug/21 06:50,13/Jul/23 08:12,25/Aug/21 13:13,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"h1. New Description

If a source implements SupportsReadingMetadata and SupportsProjectionPushDown, a table is created declaring some metadata columns and then queried with none of the metadata columns selected, #applyReadableMetadata will be called with all metadata keys declared in the schema. This causes unnecessary (and potentially expensive) calculations in the source.

 
----
h1. Original Description

Given a table with a declared schema containing some metadata columns, if we select only some of those metadata columns (or none), the interface of SupportsReadableMetadata states that the planner will perform the projection and only push required metadata keys into the source:
{quote}The planner will select required metadata columns (i.e. perform projection push down) and will call \{@link #applyReadableMetadata(List, DataType)} with a list of metadata keys.
{quote}
However, it seems that this doesn't happen, and the planner always applies all metadata declared in the schema instead. This can be a problem because the source has to do unnecessary work, and some metadata might be more expensive to compute than others.

For reference, SupportsProjectionPushDown can not be used to workaround this because it operates only on physical columns, i.e. #applyProjections will never be called with a projection for the metadata columns, even if they are selected.

The following test case can be executed to debug into #applyReadableMetadata of the values table source:
{code:java}
@Test
def test(): Unit = {
  val tableId = TestValuesTableFactory.registerData(Seq())

  tEnv.createTemporaryTable(""T"", TableDescriptor.forConnector(""values"")
    .schema(Schema.newBuilder()
      .column(""f0"", DataTypes.INT())
      .columnByMetadata(""m1"", DataTypes.STRING())
      .columnByMetadata(""m2"", DataTypes.STRING())
      .build())
    .option(""data-id"", tableId)
    .option(""bounded"", ""true"")
    .option(""readable-metadata"", ""m1:STRING,m2:STRING"")
    .build())

  tEnv.sqlQuery(""SELECT f0, m1 FROM T"").execute().collect().toList
}
{code}",,airblader,jark,jingzhang,libenchao,lzljs3620320,twalthr,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22600,,,,FLINK-23917,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 25 13:13:42 UTC 2021,,,,,,,,,,"0|z0u52w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/21 06:50;lzljs3620320;Thanks [~airblader]
It seems that there are two bugs:
* select all physical fields and partial meta columns, expect meta projection (applyReadableMetadata) but not.
* select partial physical fields and empty meta columns, expect meta projection (applyReadableMetadata) but not.;;;","23/Aug/21 07:30;airblader;I debugged a little bit more. Actually only the second case you mentioned is a problem (when no metadata columns remain after the projection). If any metadata columns are selected, #applyReadableMetadata is called twice and the second time the projection has been considered correctly. However, if only physical columns are selected, then #applyReadableMetadata is only called once with all metadata keys.

-I am also quite surprised that #applyReadableMetadata is called multiple times on a source. Depending on the implementation this can cause unexpected behavior, so we should probably document that this method (and those of all other abilities?) must be idempotent.- (Edit: it's called on different instances of the source)

-Edit: It seems this might also differ between 1.13 and master? In my 1.13 project I couldn't reproduce the same behavior, but continuing to look into this-

Edit 2: That was not correct, this just depends on SupportProjectionPushDown being implemented.;;;","23/Aug/21 07:40;airblader;So there actually are two problems (again):
 # If none of the metadata columns are selected, #applyReadableMetadata is called with _all_ metadata keys.
 # If the source does not implement SupportProjectionPushDown, #applyReadableMetadata is _always_ called with__ _all_ metadata keys. Unless I'm missing something, there should be no reason for this from the source perspective: projection push down is completely independent of projecting required metadata keys, right?;;;","23/Aug/21 07:49;lzljs3620320;I'm not sure if we need to separate Metadata projection push down from SupportProjectionPushDown. If we want to, we may need to introduce an interface for this.

The (physical/metadata) projection push down should an option. Because it may be a dangerous thing. Maybe we cannot start multiple instances of a same stream source. If the project causes the source to not be reused and multiple instances occur, this may lead to exceptions. For example Kafka consume group can not be reused by multiple instances.;;;","23/Aug/21 08:06;airblader;I think a source which implements SupportsReadableMetadata but not SupportsProjectionPushDown should still receive only the metadata keys which are actually required, since SupportsProjectionPushDown operates only on the physical columns. I don't think I understand why a separate interface would be needed here, could you maybe elaborate a bit on that? Thanks!

But if there's something I am missing (which is more than possible :)) we should at least point this out in the documentation.

 

I think the fix for the other case (no metadata columns selected) is pretty simple. We currently only apply the spec if any metadata columns have been selected, but we should just always apply it, even if the list is empty, right?;;;","23/Aug/21 08:13;airblader;We should also still document the fact that #applyReadableMetadata (and other abilities, I assume) should be idempotent. The source is copied before the specs are applied, but an implementation like this looks totally fine based on the interface docs, but is actually broken (pseudo-code):
{code:java}
class MySource implements SupportsReadableMetadata, SupportsProjectionPushDown {
  private final List<String> metadataKeys = new ArrayList<>();

  public MySource copy() {
    MySource copy = new MySource();
    copy.metadataKeys.addAll(metadataKeys);
    return copy;
  }

  public void applyReadableMetadata(List<String> metadataKeys, DataType producedType) {
    this.metadataKeys.addAll(metadataKeys);
  }
}{code}
Since the source is created and applyReadableMetadata is called with all metadata keys, copying it and then calling it with a subset of keys would only add duplicates.;;;","23/Aug/21 10:18;airblader;I've split this issue into the two cases since one is a bug and the other one an additional improvement that never worked before as far as I can tell.;;;","24/Aug/21 05:53;lzljs3620320;> I don't think I understand why a separate interface would be needed here

Hi [~airblader], Implements SupportsReadableMetadata does not mean the connector want to support metadata column projection push down.
For example:
CREATE TABLE kafka (..., meta1, meta2, meta3) WITH ('connector' = 'kafka', 'group.id' = 'a_id');
INSERT INTO sink_1 SELECT ..., meta1 FROM kafka;
INSERT INTO sink_2 SELECT ..., meta2 FROM kafka;
This job will failed because two kafka source instances use same group.id. Why not only one instance? Because their meta column projection are different.;;;","24/Aug/21 06:21;jark;[~lzljs3620320], I think your use case is another orthogonal topic. It's too complex for users to use if we introduce an option to disable/enable metadata pushdown, because this problem also exists in other interfaces, e.g. filter/projection pushdown. If users don't want to reuse same group.id, then the user should declare different group.id in query using table hints. ;;;","24/Aug/21 06:28;airblader;What if we just added a method in SupportsReadingMetadata which returns a boolean of whether metadata should be projected (if the source doesn't implement projection pushdown), with a default implementation preserving current behavior?;;;","24/Aug/21 07:04;lzljs3620320;[~jark] If we just modify this, it should be a behavior change. Then we should explain more...;;;","24/Aug/21 07:11;airblader;Just to be clear: this issue (FLINK-23911) should be safe to fix. We're talking about how to address FLINK-23917 now.;;;","24/Aug/21 07:15;jark;[~lzljs3620320] I think this just fixing the wrong behavior. The Javadoc already explains 

{code}
The planner will select required metadata columns (i.e. perform projection push down) and will
 * call {@link #applyReadableMetadata(List, DataType)} with a list of metadata keys. An
 * implementation must ensure that metadata columns are appended at the end of the physical row in
 * the order of the provided list after the apply method has been called.
{code};;;","24/Aug/21 07:22;lzljs3620320;> a boolean of whether metadata should be projected
It should be a valid way.;;;","24/Aug/21 07:25;lzljs3620320;[~jark] I'm not sure whether the right behavior is reasonable.. Just for metadata column projection, user needs to declare a new group.id and modify their sql using table hints.  (I mean the sql user instead of connector developer);;;","25/Aug/21 13:13;twalthr;Fixed in 1.14.0:

commit 97650c03ac93eeef81ff9f328578f5025fc5f4e3
[table-planner] Apply metadata spec even if no metadata were used

commit 27fcc4a1eca6313aa604dbc5cd72b88f14f38963
[table-planner] Make SupportsReadingMetadata visible in the plan digests

commit 50411b67fd5f240c84c1b0d236062dee5082f42a
[table-common] Emphasize that #applyReadableMetadata must be idempotent

We will not fix this for 1.13 because it causes plan changes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperLeaderRetrievalConnectionHandlingTest.testSuspendedConnectionDoesNotClearLeaderInformationIfClearanceOnLostConnection fails on azure,FLINK-23903,13396341,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,xtsong,xtsong,20/Aug/21 10:51,21/Aug/21 09:19,13/Jul/23 08:12,21/Aug/21 09:19,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22534&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=6647

{code}
Aug 20 07:54:25 [ERROR] Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 14.652 s <<< FAILURE! - in org.apache.flink.runtime.leaderelection.ZooKeeperLeaderRetrievalConnectionHandlingTest
Aug 20 07:54:25 [ERROR] testSuspendedConnectionDoesNotClearLeaderInformationIfClearanceOnLostConnection  Time elapsed: 2.244 s  <<< FAILURE!
Aug 20 07:54:25 java.lang.AssertionError: 
Aug 20 07:54:25 
Aug 20 07:54:25 Expected: is null
Aug 20 07:54:25      but: was <java.util.concurrent.CompletableFuture@2d6764b2[Completed normally]>
Aug 20 07:54:25 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Aug 20 07:54:25 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
Aug 20 07:54:25 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderRetrievalConnectionHandlingTest.testSuspendedConnectionDoesNotClearLeaderInformationIfClearanceOnLostConnection(ZooKeeperLeaderRetrievalConnectionHandlingTest.java:211)
Aug 20 07:54:25 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 20 07:54:25 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 20 07:54:25 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 20 07:54:25 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 20 07:54:25 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 20 07:54:25 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 20 07:54:25 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 20 07:54:25 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 20 07:54:25 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Aug 20 07:54:25 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Aug 20 07:54:25 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$CloseableStatement.evaluate(TestingFatalErrorHandlerResource.java:91)
Aug 20 07:54:25 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$CloseableStatement.access$200(TestingFatalErrorHandlerResource.java:83)
Aug 20 07:54:25 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$1.evaluate(TestingFatalErrorHandlerResource.java:55)
Aug 20 07:54:25 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
{code}",,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 21 09:19:36 UTC 2021,,,,,,,,,,"0|z0u3fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Aug/21 12:27;trohrmann;The problem is that the {{ZooKeeperLeaderRetrievalDriver}} resends the {{LeaderInformation}} if the ZooKeeper connection can be reconnected. In the problematic test run, the timing was so that this happened.;;;","21/Aug/21 09:19;trohrmann;Fixed via 9fa48e187c553de94cf6f1ed6c59072c5272ab97;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log levels not mapped correctly to akka log levels,FLINK-23898,13396329,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,chesnay,chesnay,20/Aug/21 10:03,22/Aug/21 13:10,13/Jul/23 08:12,22/Aug/21 13:10,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"WARN should be mapped to WARNING.
TRACE should be mapped to DEBUG.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 22 13:10:18 UTC 2021,,,,,,,,,,"0|z0u3co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/21 13:10;chesnay;master: 5af5dd144195052b447606435ecd4da18707c274;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix obsolete doc about creating hive table with flink dialect,FLINK-23897,13396322,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,20/Aug/21 09:15,31/Aug/21 14:53,13/Jul/23 08:12,30/Aug/21 13:01,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,Connectors / Hive,Documentation,,,,0,pull-request-available,,,,,,lirui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 30 13:01:58 UTC 2021,,,,,,,,,,"0|z0u3b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/21 13:01;lirui;Fixed in master: 5d5565523283332414fb25f4f8765a7268c11b83
Fixed in release-1.14: d3e55535417215fa47f8c1e283d88e87bc1fa508;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The new KafkaSink drops data if job fails between checkpoint and transaction commit.,FLINK-23896,13396314,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,dmvk,dmvk,20/Aug/21 08:57,17/Nov/21 15:21,13/Jul/23 08:12,01/Sep/21 06:50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"* Any time a new *transactional producer* is started, ""[KafkaProducer#initTransactions()|https://kafka.apache.org/24/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#initTransactions--]"" needs to be called in order to obtain new *ProducerId* from *TransactionCoordinator* (Kafka Broker component).
 ** *ProducerId* is increased every time a new producer with the same *TransactionalId* is registered.
 ** Publication of new ProducerId *FENCES* all prior ProducerIds and *ABORTS* all of uncommitted transactions assigned with them.
 * *KafkaCommitter* uses a separate producer, that hacks into Kafka internals and resumes transaction using epoch and producer, without actually assigning a new ProducerId for itself. This committer uses *ProducerId* that is stored in *KafkaComittable* state to commit transaction.
 * If a *new producer is started before committing the transaction* (this can happen in some failover scenarios), ProducerId stored in the state is already *FENCED* and commit fails with *ProducerFencedException*. Because we currently ignore this exception (we just log a warning), we effectively *DROP* all uncommitted data from the previous checkpoint.

Basically any job failure that happens between successfully taking a checkpoint and committing transactions, will trigger this behavior.",,dmvk,fpaul,Paul Lin,qinjunjerry,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23839,FLINK-23814,,,FLINK-23854,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 01 13:25:37 UTC 2021,,,,,,,,,,"0|z0u39c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Aug/21 09:12;fpaul;[~dmvk] thanks for looking into this. I am not sure your analysis is fully correct since in the current implementation it should be guaranteed that a transaction that is in the committer state is not reused by the KafkaWriter to write new records. This would cause the epoch to increase and fence the committer. 

Unfortunately, the current and old implementation has the downside that it indeed might lose data if the transaction is part of the checkpoint but not committed, the job fails and is not restarted until Kafka aborts the transactions. We recommend setting a high enough transaction timeout to alleviate this problem.;;;","20/Aug/21 09:41;dmvk;[~fpaul] We've already discussed this with [~arvid] a little bit. You're right that TransactionalId is unique, that something I've missed. Can you please see the attached test case that simulates the issue if you can get an explanation for the ProducerFencedException?

I'm aware of potential problem with transaction timeouts (TX expiring on the broker side).;;;","20/Aug/21 10:07;dmvk;After the hint with the transactional id uniqueness I did a second pass on the issue, the actual problem seems to be in the transaction abortion mechanism. Symptoms I've described are still valid here.

{code:java}
NEW PRODUCER kafka-sink-d8b78c12-f8ba-45ae-99f5-e3426c31ec91-0-1
NEW PRODUCER kafka-sink-d8b78c12-f8ba-45ae-99f5-e3426c31ec91-0-2
FAIL BEFORE AFTER CHECKPOINT / BEFORE COMMIT...
ABORT kafka-sink-d8b78c12-f8ba-45ae-99f5-e3426c31ec91-0-1
ABORT kafka-sink-d8b78c12-f8ba-45ae-99f5-e3426c31ec91-0-2
NEW PRODUCER kafka-sink-d8b78c12-f8ba-45ae-99f5-e3426c31ec91-0-3
NEW PRODUCER kafka-sink-d8b78c12-f8ba-45ae-99f5-e3426c31ec91-0-4
COMMIT kafka-sink-d8b78c12-f8ba-45ae-99f5-e3426c31ec91-0-1
FENCED kafka-sink-d8b78c12-f8ba-45ae-99f5-e3426c31ec91-0-1 <--------------- We need to commit this one.
COMMIT kafka-sink-d8b78c12-f8ba-45ae-99f5-e3426c31ec91-0-3
NEW PRODUCER kafka-sink-d8b78c12-f8ba-45ae-99f5-e3426c31ec91-0-5
COMMIT kafka-sink-d8b78c12-f8ba-45ae-99f5-e3426c31ec91-0-4
{code}
;;;","20/Aug/21 12:29;dmvk;NOTE: I've tried exactly the same testing scenario for the old *FlinkKafkaProducer* and it passes.;;;","20/Aug/21 12:36;fpaul;I think the seen problem is not only affecting KafkaSink but all sinks implementing FLIP-143. The interface claims that if committing fails the committables should be returned [1]. Unfortunately, this behaviour was not implemented, and returning failed committables runs into an UnsupportedOperationException[2]. This means all errors happening during committing cause potential data loss.

 

[1] [https://github.com/apache/flink/blob/273dce5b030e12dd3d7bebb2f51036a198d07112/flink-core/src/main/java/org/apache/flink/api/connector/sink/Committer.java#L39]

[2] https://github.com/apache/flink/blob/273dce5b030e12dd3d7bebb2f51036a198d07112/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/sink/AbstractStreamingCommitterHandler.java#L141;;;","23/Aug/21 15:25;fpaul;[~dmvk] after having a more careful look at the problem we have two different kind of problems in this ticket.
 # It is currently not guaranteed that transactional ids are not reused which is causing the fencing exceptions because the recovered sink writer will initialize a transaction id which the committer is still trying to commit. This problem will be solved by https://issues.apache.org/jira/browse/FLINK-23854 whereby the transactionalId is including the checkpoint counter so the same transactionalId is never reused.
 # FLIP-143 planned that sink-committer-operators can retry failed committables by simply returning the failed ones from the commit method. Unfortunately, this was never implemented but I opened a PR [https://github.com/apache/flink/pull/16945] to fix it. It also enables retries for the KafkaCommitter i.e. in case the network becomes unstable during committing the committable will be retried on the next checkpoint.

I think this should solve the found problem. Please correct me If I missed something.;;;","24/Aug/21 09:25;dmvk;[~fpaul]

For 1), if we guarantee this and fix the abortion mechanism (currently implemented in TransactionsToAbortChecker), that by itself should be enough to resolve this issue.

For 2), I think we already have a ""retry mechanism"" in terms of, if we're not able to commit, we just restart the job. But more fine-grained approach would be great here ;);;;","01/Sep/21 06:50;arvid;This issue has been solved as part of FLINK-23854 where we implemented retry logic on the SinkOperator level.;;;","01/Sep/21 13:25;arvid;[~ruanhang1993] manually verified that this particular issue was solved in FLINK-23814.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upsert materializer is not inserted for all sink providers,FLINK-23895,13396306,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,twalthr,twalthr,twalthr,20/Aug/21 08:43,30/Sep/21 07:17,13/Jul/23 08:12,30/Sep/21 07:14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,1.15.0,,,Table SQL / Planner,,,,,0,pull-request-available,,,,The new {{SinkUpsertMaterializer}} is not inserted for {{TransformationSinkProvider}} or {{DataStreamSinkProvider}} which means that neither {{toChangelogStream}} nor the current {{KafkaDynamicSink}} work correctly.,,jark,nkruber,qingyue,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23835,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 30 07:17:06 UTC 2021,,,,,,,,,,"0|z0u37k:",9223372036854775807,The 1.13.3 release aims to fix various primary key issues that effectively made it impossible to use this feature. The change might affect savepoint backwards compatibility for those incorrect pipelines. Also the resulting changelog stream might be different after these changes. Pipelines that were correct before should be restorable from a savepoint. ,,,,,,,,,,,,,,,,,,,"31/Aug/21 09:20;twalthr;Fixed in master: db5af67aee7fb449a9db8930213eec7dc925e58c
Fixed in 1.14: 6334323d486b413e91a04295516f2bbf898b653b;;;","30/Sep/21 07:14;twalthr;Since primary keys were effectively broken for `toChangelogStream` and sinks using`DataStreamSinkProvider`, we decided to also backport this change to the 1.13.3 branch. See also FLINK-20374.;;;","30/Sep/21 07:17;twalthr;Fixed in 1.13: 5027c73462ae130db6d523f89830272819b603ee;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fine-grained resource allocation may fail depending on slot allocation order,FLINK-23893,13396299,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,guoyangze,chesnay,chesnay,20/Aug/21 08:20,23/Aug/21 08:53,13/Jul/23 08:12,23/Aug/21 08:53,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"The fine-grained slot management is relatively simple in that it iterates over each registered TM in order for each required slot.
This means that you can pretty easily create scenarios where, depending on the slot order, the allocation of a slot may fail.

A trivial example, only using memory for conciseness:

2 TMs with 3 MB of memory each
2 slot-sharing groups with parallelism=2 and the following requirements:
 1) 1 MB memory
 2) 2 MB memory

If both sub-tasks of 1) are scheduled first on a single TM then the job cannot be scheduled.

It's not terrible for this limitation to exist in the first version, but it should be _explicitly_ documented.
",,guoyangze,klion26,Thesharing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 23 08:53:24 UTC 2021,,,,,,,,,,"0|z0u360:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Aug/21 08:29;guoyangze;A slot request contains multiple dimensions resources. So, it is indeed a multi-dimensional packing problem, which is NP-hard. I agree that we should document it and maybe find another suboptimal strategy in the future.;;;","23/Aug/21 08:53;chesnay;master: c0a41fc5361bc0219f23d646328abb8777f319bb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unnecessary logging from FineGrainedSlotManager,FLINK-23889,13396286,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,chesnay,chesnay,20/Aug/21 07:26,24/Aug/21 06:05,13/Jul/23 08:12,24/Aug/21 06:05,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"This is what the slot manager logs when creating a standalone cluster, without running any job:

{code}
2021-08-20 09:21:49,586 INFO  FineGrainedSlotManager [] - Registering task executor [::1]:45785-0effb8 under 71b35bc7267c2a5aa290fc5bc3789d56 at the slot manager.
2021-08-20 09:21:49,593 INFO  FineGrainedSlotManager [] - Scheduling the resource requirement check.
2021-08-20 09:21:49,660 INFO  FineGrainedSlotManager [] - Matching resource requirements against available resources.
2021-08-20 09:22:48,465 INFO  FineGrainedSlotManager [] - Release TaskManager 71b35bc7267c2a5aa290fc5bc3789d56 because it exceeded the idle timeout.
{code}

These are additional messages being logged when submitting a job:

{code}
2021-08-20 09:26:57,943 INFO  FineGrainedSlotManager [] - Allocating task managers for the resource requirements.
2021-08-20 09:26:58,803 INFO  FineGrainedSlotManager [] - Clearing resource requirements of job bdb66574e9629e9090aa173e408f9625
{code}

{{Scheduling the resource requirement check.}} is unnecessary information that provides no benefits. It is also logged repeatedly for some reason.

{{Matching resource requirements against available resources.}} should only be logged if there are any requirements to actually match against.

{{Release TaskManager XXX because it exceeded the idle timeout}} is not applicable in standalone mode; it is also logged repeatedly.

{{Allocating task managers for the resource requirements.}} is not applicable to standalone mode.

{{Clearing resource requirements of job XXX}} is logged twice for each job.
This also happens for the DeclarativeSlotManager.",,guoyangze,trohrmann,xtsong,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 24 06:05:42 UTC 2021,,,,,,,,,,"0|z0u334:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Aug/21 09:38;trohrmann;cc [~xtsong].;;;","23/Aug/21 02:54;guoyangze;> Clearing resource requirements of job XXX is logged twice for each job.
It's not only for the {{FineGrainedResourceManager}}. I think the root cause might come from the {SlotPool}. Would [~zhuzh] take a look?;;;","23/Aug/21 05:48;zhuzh;Looks to me that ""Clearing resource requirements ..."" is printed each time the resource manager receives an empty resource declaration.
I think if the previous resource requirements in SlotManager is already empty, there is no need to print this log.
In declarative slot allocation, I think it's acceptable that the JM can send an empty resource declaration to RM multiple times.;;;","23/Aug/21 06:38;zhuzh;Regarding why the empty resource declaration is sent twice, I think one is caused by the safety net in {{DeclarativeSlotPoolBridge#onClose()}} and the other is caused by {{DeclarativeSlotPoolBridge#onReleaseTaskManager()}}.
I think we should skip decreasing empty resources at JM side, because each TM release can trigger one such useless RPC. But still I think it's better to always do a final check in SlotManager.;;;","23/Aug/21 07:50;guoyangze;Personally, I think it might be good enough to skip decreasing empty resources at JM side. On RM side, check the diff between the two resource requirements each time might cost a lot for large-scale jobs. Also, I'd like to move the discussion to another ticket. WDYT? [~chesnay];;;","23/Aug/21 07:57;chesnay;We shouldn't check whether 2 requirements are different; but checking that they are empty should be fine. Feel free to create distinct issues.;;;","23/Aug/21 08:09;guoyangze;FYI, create FLINK-23912 for it.;;;","24/Aug/21 06:05;xtsong;Fixed via
- master (1.14): f69cc3deb818c949ee7f914ad067884dae346ccb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Benchmarks are not compiling,FLINK-23879,13396248,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,Thesharing,Thesharing,20/Aug/21 02:40,21/Aug/21 06:22,13/Jul/23 08:12,20/Aug/21 08:23,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Benchmarks,,,,,0,pull-request-available,,,,"The benchmarks are not compiling from Aug. 16th, 2021. The error is:
{noformat}
[2021-08-19T23:18:36.242Z] [ERROR] /home/jenkins/workspace/flink-scheduler-benchmarks/flink-benchmarks/src/main/java/org/apache/flink/benchmark/SortingBoundedInputBenchmarks.java:47:54: error: package org.apache.flink.streaming.runtime.streamstatus does not exist
[2021-08-19T23:18:36.242Z] [ERROR] /home/jenkins/workspace/flink-scheduler-benchmarks/flink-benchmarks/src/main/java/org/apache/flink/benchmark/SortingBoundedInputBenchmarks.java:350:40: error: cannot find symbol{noformat}
It seems to be introduced by FLINK-23767, in which {{StreamStatus}} is replaced with {{WatermarkStatus}}.",,Thesharing,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 21 06:22:18 UTC 2021,,,,,,,,,,"0|z0u2uo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Aug/21 03:08;zhuzh;cc [~arvid] [~dwysakowicz];;;","20/Aug/21 08:23;chesnay;benchmark-master: 076aee63e98236cc7687f171899f1da25c748d24;;;","21/Aug/21 06:22;arvid;Thanks for fixing [~chesnay]! Sorry for creating the mess - I keep forgetting about the benchmark repository. Do we actually still need the separation? I thought there was some clarification around JMH license that would make it possible to integrate into main repo?

For example, apache-commons seems to use it [1].

[1] http://commons.apache.org/proper/commons-rng/commons-rng-examples/commons-rng-examples-jmh/dependencies.html;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 ReducingUpsertSink can loose records during failover,FLINK-23875,13396102,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fpaul,fpaul,fpaul,19/Aug/21 11:48,17/Nov/21 15:21,13/Jul/23 08:12,31/Aug/21 11:29,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Connectors / Kafka,Table SQL / API,,,,0,pull-request-available,,,,"When trying to rework the Table API Kafka connector to make it compatible with the new KafkaSink I noticed that currently the buffer which is used to reduce the update-before and update-after calls is not snapshotted which can result in data loss if the job fails while the buffer is not empty.

 

Before 1.14 the equivalent class was called BufferedUpsertSinkFunction",,fpaul,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 31 14:10:53 UTC 2021,,,,,,,,,,"0|z0u1y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/21 11:26;arvid;Merged into master as 31e5fa196de6d695e062d942e9d952e300439840.;;;","31/Aug/21 11:29;fpaul;I created a separate ticket to fix it for Flink 1.13 https://issues.apache.org/jira/browse/FLINK-24079;;;","31/Aug/21 11:44;jark;Sorry [~fpaul] and [~fpaul], but I don't understand why we need to snapshot the buffer. In the original design, the buffer is not needed to be snapshotted, because we will flush buffer before checkpoint current operator. If the flushing fails, the current checkpoint is failed as well. If the checkpoint is successful, the buffer is already written to external system successfully, and no need to snapshot the buffer. 

The buffer can be very large and impact the checkpoint. It's also very complex to re-distribute the buffer data (may result in out-of-order updating) when sink parallelism rescales.;;;","31/Aug/21 11:57;fpaul;[~jark] Thanks for your fast reply you are right we missed that the buffer is cleared on every flush. We'll revert this commit and I close the backport ticket.;;;","31/Aug/21 14:10;arvid;Reverted in e45c72d1bc6f8c81ba8bb3659fd1cd58e56652d9. Thanks Jark for pointing it out. The commit was a no-op because of the flush (no data has been savepointed because of the flush).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dispatcher should handle finishing job exception when recover,FLINK-23871,13396077,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aitozi,aitozi,aitozi,19/Aug/21 09:41,23/Aug/21 08:46,13/Jul/23 08:12,23/Aug/21 08:46,1.12.5,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"The exception during run recovery job will trigger fatal error which is introduced in https://issues.apache.org/jira/browse/FLINK-9097.  If a job have reached a finished status. But crash at clean up phase or any other post phase. When recover job, it may recover a job in RunningJobsRegistry.JobSchedulingStatus.DONE status, this may lead to the dispatcher fatal again. 

I think we should deal with the  RunningJobsRegistry.JobSchedulingStatus.DONE with special exception like JobFinishingException, which represents the job/master crashed in job finishing phase. And only do the clean up work for this exception",,aitozi,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 23 08:46:06 UTC 2021,,,,,,,,,,"0|z0u1so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/21 09:51;aitozi;cc [~trohrmann];;;","23/Aug/21 08:46;trohrmann;Fixed via 

1.14.0: ee7cdcd1174bd83c495280c35ca7ded769a8f186
1.13.3: bffed23e17662b0787e2117b7c0c3942c51b9b42;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobExecutionResult printed even if suppressSysout is on,FLINK-23868,13396025,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Paul Lin,Paul Lin,Paul Lin,19/Aug/21 05:22,23/Aug/21 08:52,13/Jul/23 08:12,23/Aug/21 08:52,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Client / Job Submission,,,,,0,pull-request-available,,,,"Environments prints job execution results to stdout by default and provides a flag `suppressSysout` to disable the behavior. This flag is useful when submitting jobs through REST API or other programmatic approaches. However, JobExecutionResult is still printed when this flag is on, which looks like a bug to me.",,Paul Lin,tison,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 23 08:52:09 UTC 2021,,,,,,,,,,"0|z0u1h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/21 06:05;Paul Lin;Since this is a minor fix, to facilitate the discussion, I made a PR without issue assignment.;;;","22/Aug/21 14:18;tison;master via [https://github.com/apache/flink/commit/7f9587c723057e2b6cbaf748181c8c80a7f6703d];;;","23/Aug/21 08:52;trohrmann;Fixed via 

1.14.0: 7f9587c723057e2b6cbaf748181c8c80a7f6703d
1.13.3: 3ae08c03a15d949976b7383966f4b74e80de488f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaTransactionLogITCase.testGetTransactionsToAbort fails with IllegalStateException,FLINK-23866,13396018,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fpaul,xtsong,xtsong,19/Aug/21 04:00,20/Aug/21 18:25,13/Jul/23 08:12,20/Aug/21 18:25,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Connectors / Kafka,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22463&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=7098

{code}
Aug 18 23:14:24 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 67.32 s <<< FAILURE! - in org.apache.flink.connector.kafka.sink.KafkaTransactionLogITCase
Aug 18 23:14:24 [ERROR] testGetTransactionsToAbort  Time elapsed: 22.35 s  <<< ERROR!
Aug 18 23:14:24 java.lang.IllegalStateException: You can only check the position for partitions assigned to this consumer.
Aug 18 23:14:24 	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1717)
Aug 18 23:14:24 	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1684)
Aug 18 23:14:24 	at org.apache.flink.connector.kafka.sink.KafkaTransactionLog.hasReadAllRecords(KafkaTransactionLog.java:144)
Aug 18 23:14:24 	at org.apache.flink.connector.kafka.sink.KafkaTransactionLog.getTransactionsToAbort(KafkaTransactionLog.java:133)
Aug 18 23:14:24 	at org.apache.flink.connector.kafka.sink.KafkaTransactionLogITCase.testGetTransactionsToAbort(KafkaTransactionLogITCase.java:110)
Aug 18 23:14:24 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 18 23:14:24 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 18 23:14:24 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 18 23:14:24 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 18 23:14:24 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 18 23:14:24 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 18 23:14:24 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 18 23:14:24 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 18 23:14:24 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Aug 18 23:14:24 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 18 23:14:24 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Aug 18 23:14:24 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Aug 18 23:14:24 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Aug 18 23:14:24 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Aug 18 23:14:24 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Aug 18 23:14:24 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Aug 18 23:14:24 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Aug 18 23:14:24 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 18 23:14:24 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Aug 18 23:14:24 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
Aug 18 23:14:24 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Aug 18 23:14:24 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 18 23:14:24 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 18 23:14:24 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Aug 18 23:14:24 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Aug 18 23:14:24 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
Aug 18 23:14:24 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Aug 18 23:14:24 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Aug 18 23:14:24 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Aug 18 23:14:24 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
Aug 18 23:14:24 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Aug 18 23:14:24 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Aug 18 23:14:24 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Aug 18 23:14:24 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Aug 18 23:14:24 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Aug 18 23:14:24 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
Aug 18 23:14:24 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
Aug 18 23:14:24 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
Aug 18 23:14:24 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
Aug 18 23:14:24 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
Aug 18 23:14:24 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
Aug 18 23:14:24 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
Aug 18 23:14:24 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
Aug 18 23:14:24 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
Aug 18 23:14:24 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
Aug 18 23:14:24 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Aug 18 23:14:24 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Aug 18 23:14:24 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Aug 18 23:14:24 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,fpaul,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 20 18:25:52 UTC 2021,,,,,,,,,,"0|z0u1fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Aug/21 18:25;fpaul;Merged on master: 226858967eb341c37b44b2f62646798d42c383ff;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition while cancelling task during initialization,FLINK-23862,13395884,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,roman,roman,18/Aug/21 13:03,26/Aug/21 10:56,13/Jul/23 08:12,26/Aug/21 10:55,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Task,,,,,0,pull-request-available,,,,"While debugging the recent failures in FLINK-22889, I see that sometimes the operator chain is not closed if the task is cancelled while it's being initialized.

 

The reason is that on restore(), cleanUpInvoke() is only closed if there was an exception, including CancelTaskException.

The latter is only thrown if StreamTask.canceled is set, i.e. TaskCanceler has called StreamTask.cancel().

 

So if StreamTask is cancelled in between restore and normal invoke then it may not close the operator chain and not do other cleanup.

 

One solution is to make StreamTask.cleanup visible to and called from Task.

 

cc: [~akalashnikov], [~pnowojski]",,roman,Thesharing,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22889,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 26 10:55:13 UTC 2021,,,,,,,,,,"0|z0u0ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/21 10:55;roman;Fix merged into master (1.14) as 285b2c12d5a5673f4d041229e751b81e5581d370..a8ea3c3e81517f9c38f90b82c043136d8b56bf8b.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"insert overwirite table select * from t where 1 != 1, Unable to clear table data",FLINK-23857,13395872,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,leexu,leexu,18/Aug/21 12:27,07/Sep/21 06:54,13/Jul/23 08:12,31/Aug/21 05:53,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,Connectors / FileSystem,Connectors / Hive,,,,0,pull-request-available,,,,insert overwirite table select * from t where 1 != 1，Unable to clear table data，Unlike hive。,,jark,leexu,libenchao,lirui,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/21 01:46;luoyuxia;7F086A05-FE73-47F2-BA6C-95371E89116C.png;https://issues.apache.org/jira/secure/attachment/13033130/7F086A05-FE73-47F2-BA6C-95371E89116C.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 07 06:54:48 UTC 2021,,,,,,,,,,"0|z0u0j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/21 05:53;lirui;Fixed in master: 6f5995d783bc07b7bb26653555da366659255f45
Fixed in release-1.14: 2650fd63172b4d7b218187c7f65178d14fdaeb1b;;;","31/Aug/21 12:01;leexu;insert overwrite table t1 partition(f2=1) select * from t2 where 1 != 1

I merge code tests, unable to cleat partition data;;;;","01/Sep/21 03:39;lirui;bq. insert overwrite table t1 partition(f2=1) select * from t2 where 1 != 1

[~leexu] Are you able to clear partition data with this in Hive? And could you share the hive version you're using?;;;","01/Sep/21 03:44;leexu;hive 1.1.0;;;","04/Sep/21 00:00;luoyuxia;{code:java}
 insert overwrite table t1 partition(f2=1) select * from t2 where 1 != 1{code}
Yeah, I have tried, the sql can clear partition data in Hive.

Currently, the fix only works for no partition table. I would like to try to make it work even for partition table.;;;","05/Sep/21 01:56;jark;[~luoyuxia] Shouldn't the above query fail if target table is non-partitioned table? 
;;;","06/Sep/21 01:25;luoyuxia;[~jark] Yeah, the query will fail if target table is non-partitioned table,  but is fine for partitioned table.;;;","06/Sep/21 03:46;lirui;[~luoyuxia] Please also verify with different hive versions. IIRC hive didn't clear partition data when I tried with hive-2.3.4.;;;","07/Sep/21 01:46;luoyuxia;[~lirui] I tried with hive-2.3.2, hive-2.3.8,  hive-3.1.2. All of them clear partition data. 

!7F086A05-FE73-47F2-BA6C-95371E89116C.png|width=498,height=409!

Not sure whether need to clear parition data using hive dialect in flink to make the behaviour consistent with hive.;;;","07/Sep/21 06:54;lirui;OK, I tried with an empty source table w/o the {{1 != 1}} filter, and hive doesn't clear the partition data in that case. Maybe there's some inconsistency in hive. I guess it's more intuitive to clear the data since user intends to do {{OVERWRITE}}. And therefore I prefer not to make a difference on this between hive and default dialect. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSink error when restart from the checkpoint with a lower parallelism by exactly-once guarantee,FLINK-23854,13395837,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,ruanhang1993,ruanhang1993,18/Aug/21 09:20,29/Nov/22 03:53,13/Jul/23 08:12,01/Sep/21 06:29,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Connectors / Kafka,,,,,0,pull-request-available,release-testing,,,"The KafkaSink throws the exception when restarted with a lower parallelism and the exactly-once guarantee. The exception is like this.


{noformat}
java.lang.IllegalStateException: Internal error: It is expected that state from previous executions is distributed to the same subtask id.   
at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)   
at org.apache.flink.connector.kafka.sink.KafkaWriter.recoverAndInitializeState(KafkaWriter.java:178)   
at org.apache.flink.connector.kafka.sink.KafkaWriter.<init>(KafkaWriter.java:130)   
at org.apache.flink.connector.kafka.sink.KafkaSink.createWriter(KafkaSink.java:99)   
at org.apache.flink.streaming.runtime.operators.sink.SinkOperator.initializeState(SinkOperator.java:134)   
at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:118)   
at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:286)   
at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:109)   
at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:690)   
at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)   
at org.apache.flink.streaming.runtime.tasks.StreamTask.executeRestore(StreamTask.java:666)   
at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:785)   
at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:638)   
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)   
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:572)   
at java.lang.Thread.run(Thread.java:748)    
Suppressed: java.lang.NullPointerException       
at org.apache.flink.streaming.runtime.operators.sink.SinkOperator.close(SinkOperator.java:195)       
at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:141)       
at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:127)       
at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:1028)       
at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1014)       
at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:927)       
at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:797)        ... 4 more
{noformat}

I start the kafka cluster(kafka_2.13-2.8.0) and the flink cluster in my own mac. I change the parallelism from 4 to 2 and restart the job from some completed checkpoint. Then the error occurs. 

And the cli command and the code are as follows.
{code:java}
// cli command
./bin/flink run -d -c com.test.KafkaExactlyOnceScaleDownTest -s /Users/test/checkpointDir/ExactlyOnceTest1/67105fcc1724e147fc6208af0dd90618/chk-1 /Users/test/project/self/target/test.jar
{code}
{code:java}
public class KafkaExactlyOnceScaleDownTest { 
public static void main(String[] args) throws Exception { 
    final String kafkaSourceTopic = ""flinkSourceTest""; 
    final String kafkaSinkTopic = ""flinkSinkExactlyTest1""; 
    final String groupId = ""ExactlyOnceTest1""; 
    final String brokers = ""localhost:9092""; 
    final String ckDir = ""file:///Users/test/checkpointDir/"" + groupId; 
    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); 
    env.enableCheckpointing(60000); 
    env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);        
env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);
    env.getCheckpointConfig().setCheckpointStorage(ckDir); 
    env.setParallelism(4); 

    KafkaSource<String> source = KafkaSource.<String>builder() 
     .setBootstrapServers(brokers) 
     .setTopics(kafkaSourceTopic) 
     .setGroupId(groupId) 
     .setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest"") 
     .setValueOnlyDeserializer(new SimpleStringSchema()) 
     .build(); 

    DataStream<String> flintstones = env.fromSource(source, WatermarkStrategy.noWatermarks(), ""Kafka Source""); 
    DataStream<String> adults = flintstones.filter(s -> s != null && s.length() > 2); 
    Properties props = new Properties(); 
    props.setProperty(""transaction.timeout.ms"", ""900000""); 
    adults.sinkTo(KafkaSink.<String>builder() 
    .setBootstrapServers(brokers) 
    .setDeliverGuarantee(DeliveryGuarantee.EXACTLY_ONCE) 
    .setTransactionalIdPrefix(""tp-test-"") 
    .setKafkaProducerConfig(props) 
    .setRecordSerializer(new SelfSerializationSchema(kafkaSinkTopic, new SimpleStringSchema())) 
    .build()); 

    env.execute(""ScaleDownTest""); 
} 

static class SelfSerializationSchema implements KafkaRecordSerializationSchema<String> { private final SerializationSchema<String> valueSerialization; private String topic; SelfSerializationSchema(String topic, SerializationSchema<String> valueSerialization){ this.valueSerialization = valueSerialization; this.topic = topic; } @Override public void open(SerializationSchema.InitializationContext context, KafkaSinkContext sinkContext) throws Exception { KafkaRecordSerializationSchema.super.open(context, sinkContext); } @Override public ProducerRecord<byte[], byte[]> serialize(String s, KafkaSinkContext kafkaSinkContext, Long aLong) { final byte[] valueSerialized = valueSerialization.serialize(s); return new ProducerRecord<>(topic, valueSerialized); } } 
}
{code}",,ruanhang1993,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23896,FLINK-27787,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 01 06:29:59 UTC 2021,,,,,,,,,,"0|z0u0bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/21 12:39;ruanhang1993;After adding log in StatefulSinkWriterStateHandler class, the distribution of the operator state changes as follow. 
||parallelism 4(before)||parallelism 2(after)||
|subTask0 -> state0
 subTask1 -> state1
 subTask2 -> state2
 subTask3 -> state3|subTask0 -> state0 & state1
 subTask1 - > state2 & state3|

Here are the log and the reult after changing to 2 parallelism.
||Add log in initializeState method of StatefulSinkWriterStateHandle||the log of subtask0||the log of subtask1||
|{code:java}
public List<WriterStateT> initializeState(StateInitializationContext context) throws Exception {
    final ListState<byte[]> rawState =
            context.getOperatorStateStore().getListState(WRITER_RAW_STATES_DESC);
    writerState =
            new SimpleVersionedListState<>(rawState, writerStateSimpleVersionedSerializer);
    final List<WriterStateT> writerStates = CollectionUtil.iterableToList(writerState.get());
    final List<WriterStateT> states = new ArrayList<>(writerStates);

    // add log
    for(WriterStateT stateT : states) {
        LOG.info(""Stateful Sink Writer state handler:"" + stateT.toString());
    }

    for (String previousSinkStateName : previousSinkStateNames) {
        ......
    }
    return states;
}
{code}|2021-08-18 20:02:04,214 INFO org.apache.flink.streaming.runtime.operators.sink.StatefulSinkWriterStateHandler [] - Stateful Sink Writer state handler:KafkaWriterState\{subtaskId=1, transactionalIdOffset=8, transactionalIdPrefix='tp-test-'}
 2021-08-18 20:02:04,214 INFO org.apache.flink.streaming.runtime.operators.sink.StatefulSinkWriterStateHandler [] - Stateful Sink Writer state handler:KafkaWriterState\{subtaskId=0, transactionalIdOffset=8, transactionalIdPrefix='tp-test-'}|2021-08-18 20:02:04,220 INFO org.apache.flink.streaming.runtime.operators.sink.StatefulSinkWriterStateHandler [] - Stateful Sink Writer state handler:KafkaWriterState\{subtaskId=2, transactionalIdOffset=8, transactionalIdPrefix='tp-test-'}
 2021-08-18 20:02:04,220 INFO org.apache.flink.streaming.runtime.operators.sink.StatefulSinkWriterStateHandler [] - Stateful Sink Writer state handler:KafkaWriterState\{subtaskId=3, transactionalIdOffset=8, transactionalIdPrefix='tp-test-'}|;;;","18/Aug/21 12:53;ruanhang1993;And why do we need this check? I think the distribution behavior of the operator list state is fixed when the parallelism changes, just like the test shows.;;;","23/Aug/21 07:54;ruanhang1993;Maybe I can work on it. I think the check aims at generating an unique transaction id for the topic. Maybe adding the attemptNum to the transaction prefix is a good solution.;;;","01/Sep/21 06:29;arvid;Merged into master as f01c636aa4871590fb44f09e63aa97216277ff29..e396f4ab17f66ac6d631a2659746d1e55ad570a0.
Merged into 1.14 as 3076a23b1c7b14307160eba31af40bb4f4d72863..ba675e1ec8f2a6e90353988a80631ebad5668b56.

[~ruanhang1993] could you please retest with the current release-1.14. Sorry that it took so long to solve - we had to overhaul how we deal with lingering transactions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PulsarSourceITCase is failed on Azure,FLINK-23848,13395783,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,syhily,jark,jark,18/Aug/21 02:32,11/Nov/21 08:54,13/Jul/23 08:12,11/Nov/21 08:54,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Connectors / Pulsar,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22412&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461

{code}
2021-08-17T20:11:53.7228789Z Aug 17 20:11:53 [INFO] Running org.apache.flink.connector.pulsar.source.PulsarSourceITCase
2021-08-17T20:17:38.2429467Z Aug 17 20:17:38 [ERROR] Tests run: 8, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 344.515 s <<< FAILURE! - in org.apache.flink.connector.pulsar.source.PulsarSourceITCase
2021-08-17T20:17:38.2430693Z Aug 17 20:17:38 [ERROR] testMultipleSplits{TestEnvironment, ExternalContext}[2]  Time elapsed: 66.766 s  <<< ERROR!
2021-08-17T20:17:38.2431387Z Aug 17 20:17:38 java.lang.RuntimeException: Failed to fetch next result
2021-08-17T20:17:38.2432035Z Aug 17 20:17:38 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
2021-08-17T20:17:38.2433345Z Aug 17 20:17:38 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2021-08-17T20:17:38.2434175Z Aug 17 20:17:38 	at org.apache.flink.connectors.test.common.utils.TestDataMatchers$MultipleSplitDataMatcher.matchesSafely(TestDataMatchers.java:151)
2021-08-17T20:17:38.2435028Z Aug 17 20:17:38 	at org.apache.flink.connectors.test.common.utils.TestDataMatchers$MultipleSplitDataMatcher.matchesSafely(TestDataMatchers.java:133)
2021-08-17T20:17:38.2438387Z Aug 17 20:17:38 	at org.hamcrest.TypeSafeDiagnosingMatcher.matches(TypeSafeDiagnosingMatcher.java:55)
2021-08-17T20:17:38.2439100Z Aug 17 20:17:38 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:12)
2021-08-17T20:17:38.2439708Z Aug 17 20:17:38 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
2021-08-17T20:17:38.2440299Z Aug 17 20:17:38 	at org.apache.flink.connectors.test.common.testsuites.SourceTestSuiteBase.testMultipleSplits(SourceTestSuiteBase.java:156)
2021-08-17T20:17:38.2441007Z Aug 17 20:17:38 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-08-17T20:17:38.2441526Z Aug 17 20:17:38 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-08-17T20:17:38.2442068Z Aug 17 20:17:38 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-08-17T20:17:38.2442759Z Aug 17 20:17:38 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-08-17T20:17:38.2443247Z Aug 17 20:17:38 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)
2021-08-17T20:17:38.2443812Z Aug 17 20:17:38 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2021-08-17T20:17:38.2444441Z Aug 17 20:17:38 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2021-08-17T20:17:38.2445101Z Aug 17 20:17:38 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2021-08-17T20:17:38.2445688Z Aug 17 20:17:38 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2021-08-17T20:17:38.2446328Z Aug 17 20:17:38 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)
2021-08-17T20:17:38.2447303Z Aug 17 20:17:38 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2021-08-17T20:17:38.2448336Z Aug 17 20:17:38 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2021-08-17T20:17:38.2448999Z Aug 17 20:17:38 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2021-08-17T20:17:38.2449689Z Aug 17 20:17:38 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2021-08-17T20:17:38.2450363Z Aug 17 20:17:38 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2021-08-17T20:17:38.2451001Z Aug 17 20:17:38 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2021-08-17T20:17:38.2451614Z Aug 17 20:17:38 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2021-08-17T20:17:38.2452440Z Aug 17 20:17:38 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2021-08-17T20:17:38.2453087Z Aug 17 20:17:38 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)
2021-08-17T20:17:38.2453741Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-08-17T20:17:38.2454682Z Aug 17 20:17:38 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)
2021-08-17T20:17:38.2455320Z Aug 17 20:17:38 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)
2021-08-17T20:17:38.2456064Z Aug 17 20:17:38 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)
2021-08-17T20:17:38.2456715Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
2021-08-17T20:17:38.2457455Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-08-17T20:17:38.2458161Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2021-08-17T20:17:38.2458734Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-08-17T20:17:38.2459327Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2021-08-17T20:17:38.2459954Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-08-17T20:17:38.2460572Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2021-08-17T20:17:38.2461161Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2021-08-17T20:17:38.2461851Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
2021-08-17T20:17:38.2462757Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:212)
2021-08-17T20:17:38.2463458Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:192)
2021-08-17T20:17:38.2464110Z Aug 17 20:17:38 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)
2021-08-17T20:17:38.2464782Z Aug 17 20:17:38 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)
2021-08-17T20:17:38.2465360Z Aug 17 20:17:38 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2021-08-17T20:17:38.2465880Z Aug 17 20:17:38 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2021-08-17T20:17:38.2466376Z Aug 17 20:17:38 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
2021-08-17T20:17:38.2466889Z Aug 17 20:17:38 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2021-08-17T20:17:38.2467382Z Aug 17 20:17:38 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2021-08-17T20:17:38.2467953Z Aug 17 20:17:38 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2021-08-17T20:17:38.2468460Z Aug 17 20:17:38 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2021-08-17T20:17:38.2468980Z Aug 17 20:17:38 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2021-08-17T20:17:38.2469477Z Aug 17 20:17:38 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2021-08-17T20:17:38.2470008Z Aug 17 20:17:38 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2021-08-17T20:17:38.2470551Z Aug 17 20:17:38 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2021-08-17T20:17:38.2471207Z Aug 17 20:17:38 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2021-08-17T20:17:38.2471693Z Aug 17 20:17:38 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2021-08-17T20:17:38.2472443Z Aug 17 20:17:38 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2021-08-17T20:17:38.2472960Z Aug 17 20:17:38 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2021-08-17T20:17:38.2473476Z Aug 17 20:17:38 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2021-08-17T20:17:38.2473973Z Aug 17 20:17:38 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2021-08-17T20:17:38.2474585Z Aug 17 20:17:38 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2021-08-17T20:17:38.2475112Z Aug 17 20:17:38 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2021-08-17T20:17:38.2475634Z Aug 17 20:17:38 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2021-08-17T20:17:38.2476273Z Aug 17 20:17:38 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2021-08-17T20:17:38.2476862Z Aug 17 20:17:38 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)
2021-08-17T20:17:38.2477493Z Aug 17 20:17:38 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)
2021-08-17T20:17:38.2478211Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
2021-08-17T20:17:38.2478837Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-08-17T20:17:38.2479487Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2021-08-17T20:17:38.2480067Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-08-17T20:17:38.2480635Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2021-08-17T20:17:38.2481337Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-08-17T20:17:38.2481952Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2021-08-17T20:17:38.2482690Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2021-08-17T20:17:38.2483174Z Aug 17 20:17:38 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2021-08-17T20:17:38.2483802Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
2021-08-17T20:17:38.2484527Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
2021-08-17T20:17:38.2485170Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-08-17T20:17:38.2485803Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2021-08-17T20:17:38.2486390Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-08-17T20:17:38.2486958Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2021-08-17T20:17:38.2487672Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-08-17T20:17:38.2488281Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2021-08-17T20:17:38.2488873Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2021-08-17T20:17:38.2489349Z Aug 17 20:17:38 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2021-08-17T20:17:38.2490063Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
2021-08-17T20:17:38.2490785Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
2021-08-17T20:17:38.2491420Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-08-17T20:17:38.2492112Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2021-08-17T20:17:38.2492862Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2021-08-17T20:17:38.2493434Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2021-08-17T20:17:38.2494072Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2021-08-17T20:17:38.2494677Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2021-08-17T20:17:38.2495278Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2021-08-17T20:17:38.2495953Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
2021-08-17T20:17:38.2496698Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
2021-08-17T20:17:38.2497360Z Aug 17 20:17:38 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)
2021-08-17T20:17:38.2498003Z Aug 17 20:17:38 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
2021-08-17T20:17:38.2498573Z Aug 17 20:17:38 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
2021-08-17T20:17:38.2499152Z Aug 17 20:17:38 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
2021-08-17T20:17:38.2499724Z Aug 17 20:17:38 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
2021-08-17T20:17:38.2500254Z Aug 17 20:17:38 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
2021-08-17T20:17:38.2500852Z Aug 17 20:17:38 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
2021-08-17T20:17:38.2501472Z Aug 17 20:17:38 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2021-08-17T20:17:38.2502089Z Aug 17 20:17:38 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-08-17T20:17:38.2502846Z Aug 17 20:17:38 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-08-17T20:17:38.2503399Z Aug 17 20:17:38 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-08-17T20:17:38.2503903Z Aug 17 20:17:38 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-08-17T20:17:38.2504388Z Aug 17 20:17:38 Caused by: java.io.IOException: Failed to fetch job execution result
2021-08-17T20:17:38.2504963Z Aug 17 20:17:38 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:177)
2021-08-17T20:17:38.2505838Z Aug 17 20:17:38 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:120)
2021-08-17T20:17:38.2506827Z Aug 17 20:17:38 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2021-08-17T20:17:38.2507453Z Aug 17 20:17:38 	... 108 more
2021-08-17T20:17:38.2508033Z Aug 17 20:17:38 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-08-17T20:17:38.2508633Z Aug 17 20:17:38 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2021-08-17T20:17:38.2509149Z Aug 17 20:17:38 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2021-08-17T20:17:38.2509744Z Aug 17 20:17:38 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:175)
2021-08-17T20:17:38.2510302Z Aug 17 20:17:38 	... 110 more
2021-08-17T20:17:38.2510683Z Aug 17 20:17:38 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-08-17T20:17:38.2511235Z Aug 17 20:17:38 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2021-08-17T20:17:38.2511850Z Aug 17 20:17:38 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2021-08-17T20:17:38.2512616Z Aug 17 20:17:38 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2021-08-17T20:17:38.2513144Z Aug 17 20:17:38 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2021-08-17T20:17:38.2513677Z Aug 17 20:17:38 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2021-08-17T20:17:38.2514252Z Aug 17 20:17:38 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:134)
2021-08-17T20:17:38.2514944Z Aug 17 20:17:38 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:174)
2021-08-17T20:17:38.2515404Z Aug 17 20:17:38 	... 110 more
2021-08-17T20:17:38.2515817Z Aug 17 20:17:38 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2021-08-17T20:17:38.2516465Z Aug 17 20:17:38 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2021-08-17T20:17:38.2517209Z Aug 17 20:17:38 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2021-08-17T20:17:38.2517937Z Aug 17 20:17:38 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:228)
2021-08-17T20:17:38.2518557Z Aug 17 20:17:38 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:218)
2021-08-17T20:17:38.2519187Z Aug 17 20:17:38 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:209)
2021-08-17T20:17:38.2519817Z Aug 17 20:17:38 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:679)
2021-08-17T20:17:38.2520396Z Aug 17 20:17:38 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2021-08-17T20:17:38.2520981Z Aug 17 20:17:38 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:438)
2021-08-17T20:17:38.2521455Z Aug 17 20:17:38 	at sun.reflect.GeneratedMethodAccessor65.invoke(Unknown Source)
2021-08-17T20:17:38.2521940Z Aug 17 20:17:38 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-08-17T20:17:38.2522546Z Aug 17 20:17:38 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-08-17T20:17:38.2523083Z Aug 17 20:17:38 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
2021-08-17T20:17:38.2523703Z Aug 17 20:17:38 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2021-08-17T20:17:38.2524327Z Aug 17 20:17:38 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
2021-08-17T20:17:38.2524968Z Aug 17 20:17:38 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2021-08-17T20:17:38.2525553Z Aug 17 20:17:38 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2021-08-17T20:17:38.2526116Z Aug 17 20:17:38 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2021-08-17T20:17:38.2526626Z Aug 17 20:17:38 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2021-08-17T20:17:38.2527144Z Aug 17 20:17:38 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2021-08-17T20:17:38.2527661Z Aug 17 20:17:38 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2021-08-17T20:17:38.2528108Z Aug 17 20:17:38 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2021-08-17T20:17:38.2528590Z Aug 17 20:17:38 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2021-08-17T20:17:38.2529061Z Aug 17 20:17:38 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-08-17T20:17:38.2529553Z Aug 17 20:17:38 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2021-08-17T20:17:38.2530025Z Aug 17 20:17:38 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2021-08-17T20:17:38.2530474Z Aug 17 20:17:38 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2021-08-17T20:17:38.2530875Z Aug 17 20:17:38 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2021-08-17T20:17:38.2531325Z Aug 17 20:17:38 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2021-08-17T20:17:38.2531772Z Aug 17 20:17:38 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2021-08-17T20:17:38.2532202Z Aug 17 20:17:38 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2021-08-17T20:17:38.2532786Z Aug 17 20:17:38 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2021-08-17T20:17:38.2533211Z Aug 17 20:17:38 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2021-08-17T20:17:38.2533602Z Aug 17 20:17:38 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2021-08-17T20:17:38.2534039Z Aug 17 20:17:38 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2021-08-17T20:17:38.2534522Z Aug 17 20:17:38 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2021-08-17T20:17:38.2535026Z Aug 17 20:17:38 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2021-08-17T20:17:38.2535517Z Aug 17 20:17:38 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2021-08-17T20:17:38.2536042Z Aug 17 20:17:38 Caused by: java.lang.RuntimeException: One or more fetchers have encountered exception
2021-08-17T20:17:38.2536626Z Aug 17 20:17:38 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:199)
2021-08-17T20:17:38.2537278Z Aug 17 20:17:38 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:167)
2021-08-17T20:17:38.2537965Z Aug 17 20:17:38 	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:128)
2021-08-17T20:17:38.2538630Z Aug 17 20:17:38 	at org.apache.flink.connector.pulsar.source.reader.source.PulsarOrderedSourceReader.pollNext(PulsarOrderedSourceReader.java:108)
2021-08-17T20:17:38.2539265Z Aug 17 20:17:38 	at org.apache.flink.streaming.api.operators.SourceOperator.pollNext(SourceOperator.java:364)
2021-08-17T20:17:38.2539842Z Aug 17 20:17:38 	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:324)
2021-08-17T20:17:38.2540525Z Aug 17 20:17:38 	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
2021-08-17T20:17:38.2541133Z Aug 17 20:17:38 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
2021-08-17T20:17:38.2541698Z Aug 17 20:17:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:489)
2021-08-17T20:17:38.2542427Z Aug 17 20:17:38 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
2021-08-17T20:17:38.2542997Z Aug 17 20:17:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:819)
2021-08-17T20:17:38.2543541Z Aug 17 20:17:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:746)
2021-08-17T20:17:38.2544093Z Aug 17 20:17:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:785)
2021-08-17T20:17:38.2544719Z Aug 17 20:17:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:728)
2021-08-17T20:17:38.2545205Z Aug 17 20:17:38 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:786)
2021-08-17T20:17:38.2545642Z Aug 17 20:17:38 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:572)
2021-08-17T20:17:38.2546045Z Aug 17 20:17:38 	at java.lang.Thread.run(Thread.java:748)
2021-08-17T20:17:38.2546509Z Aug 17 20:17:38 Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
2021-08-17T20:17:38.2547099Z Aug 17 20:17:38 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:146)
2021-08-17T20:17:38.2547716Z Aug 17 20:17:38 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:101)
2021-08-17T20:17:38.2548241Z Aug 17 20:17:38 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2021-08-17T20:17:38.2548689Z Aug 17 20:17:38 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-08-17T20:17:38.2549159Z Aug 17 20:17:38 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2021-08-17T20:17:38.2549659Z Aug 17 20:17:38 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2021-08-17T20:17:38.2550030Z Aug 17 20:17:38 	... 1 more
2021-08-17T20:17:38.2550437Z Aug 17 20:17:38 Caused by: org.apache.pulsar.client.api.PulsarClientException$TimeoutException: 2 request timedout after ms 30000
2021-08-17T20:17:38.2551017Z Aug 17 20:17:38 	at org.apache.pulsar.client.api.PulsarClientException.unwrap(PulsarClientException.java:961)
2021-08-17T20:17:38.2551552Z Aug 17 20:17:38 	at org.apache.pulsar.client.impl.ConsumerBuilderImpl.subscribe(ConsumerBuilderImpl.java:97)
2021-08-17T20:17:38.2552171Z Aug 17 20:17:38 	at org.apache.flink.connector.pulsar.source.config.PulsarSourceConfigUtils.createConsumer(PulsarSourceConfigUtils.java:256)
2021-08-17T20:17:38.2552894Z Aug 17 20:17:38 	at org.apache.flink.connector.pulsar.source.config.PulsarSourceConfigUtils.createConsumer(PulsarSourceConfigUtils.java:229)
2021-08-17T20:17:38.2553632Z Aug 17 20:17:38 	at org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.lambda$createPulsarConsumer$1(PulsarPartitionSplitReaderBase.java:262)
2021-08-17T20:17:38.2554335Z Aug 17 20:17:38 	at org.apache.flink.connector.pulsar.common.utils.PulsarExceptionUtils.sneaky(PulsarExceptionUtils.java:69)
2021-08-17T20:17:38.2554961Z Aug 17 20:17:38 	at org.apache.flink.connector.pulsar.common.utils.PulsarExceptionUtils.sneakyClient(PulsarExceptionUtils.java:46)
2021-08-17T20:17:38.2555654Z Aug 17 20:17:38 	at org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.createPulsarConsumer(PulsarPartitionSplitReaderBase.java:261)
2021-08-17T20:17:38.2556427Z Aug 17 20:17:38 	at org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.createPulsarConsumer(PulsarPartitionSplitReaderBase.java:238)
2021-08-17T20:17:38.2557199Z Aug 17 20:17:38 	at org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.handleSplitsChanges(PulsarPartitionSplitReaderBase.java:178)
2021-08-17T20:17:38.2558037Z Aug 17 20:17:38 	at org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.handleSplitsChanges(PulsarOrderedPartitionSplitReader.java:52)
2021-08-17T20:17:38.2558793Z Aug 17 20:17:38 	at org.apache.flink.connector.base.source.reader.fetcher.AddSplitsTask.run(AddSplitsTask.java:49)
2021-08-17T20:17:38.2559363Z Aug 17 20:17:38 	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:138)
2021-08-17T20:17:38.2559776Z Aug 17 20:17:38 	... 6 more
2021-08-17T20:17:38.2560000Z Aug 17 20:17:38 
2021-08-17T20:17:38.6992614Z Aug 17 20:17:38 [INFO] 
2021-08-17T20:17:38.6993159Z Aug 17 20:17:38 [INFO] Results:
2021-08-17T20:17:38.6994058Z Aug 17 20:17:38 [INFO] 
2021-08-17T20:17:38.6994436Z Aug 17 20:17:38 [ERROR] Errors: 
2021-08-17T20:17:38.6996010Z Aug 17 20:17:38 [ERROR]   PulsarSourceITCase>SourceTestSuiteBase.testMultipleSplits:156 » Runtime Failed...
{code}",,dmvk,gaoyunhaii,jark,syhily,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24030,,,,,,,FLINK-20731,,,,FLINK-24872,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 11 08:54:03 UTC 2021,,,,,,,,,,"0|z0tzzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/21 03:54;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22359&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=24751;;;","18/Aug/21 05:42;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22415&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=24424;;;","19/Aug/21 04:02;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22463&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24426;;;","19/Aug/21 04:43;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22463&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=24746;;;","19/Aug/21 10:15;trohrmann;cc [~arvid];;;","20/Aug/21 03:27;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22511&view=logs&j=f450c1a5-64b1-5955-e215-49cb1ad5ec88&t=cc452273-9efa-565d-9db8-ef62a38a0c10&l=24267;;;","20/Aug/21 03:28;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22511&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=24746;;;","20/Aug/21 08:48;trohrmann;[~syhily] and [~Jianyun Zhao] could you take a look whether this is a Pulsar connector specific problem?;;;","20/Aug/21 09:00;syhily;[~trohrmann] I have read all the test report, some tests are failed because of 
{code:java}
org.apache.pulsar.client.api.PulsarClientException$BrokerMetadataException: Consumer not found
{code}

I can fixed this issue. Other may need more time to investigate.;;;","20/Aug/21 09:09;xtsong;Thanks [~syhily], I've assigned you to the ticket.;;;","23/Aug/21 02:40;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22577&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=24746;;;","23/Aug/21 02:47;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22605&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=24746;;;","23/Aug/21 02:49;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22605&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24426;;;","23/Aug/21 02:53;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22605&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=24746;;;","23/Aug/21 03:39;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22618&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=24746;;;","23/Aug/21 06:24;loyi;I met a similar issue.

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22612&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=25015]

 ;;;","23/Aug/21 08:56;trohrmann;Might be related: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22554&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461;;;","24/Aug/21 03:16;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22684&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24426;;;","24/Aug/21 03:19;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22684&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=24424;;;","24/Aug/21 07:43;dmvk;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22665&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461;;;","25/Aug/21 03:15;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22770&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24429;;;","25/Aug/21 03:31;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22770&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=24746;;;","26/Aug/21 03:25;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22855&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=24753;;;","03/Sep/21 05:23;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23440&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=24755;;;","03/Sep/21 07:58;syhily;[~xtsong] The test failure should be fixed in this [PR|https://github.com/apache/flink/pull/17119]. Tks for your comments.;;;","08/Sep/21 04:01;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23718&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=25068;;;","08/Sep/21 14:02;arvid;Merged into master as 1ca7353b8f91cee62bd78e24fd5af346fe5220ec..ab9b85beacbb3089b3fa1b9adde1adf4a0f7835e.;;;","10/Sep/21 02:59;xtsong;release-1.14: 4e732ad4509d719a2f0c5a81cd75ed002df25c2e;;;","07/Nov/21 15:52;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26086&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=25247]

 

[~syhily]  hello~ The issue seems reproduced on 1.14, could you have a double look~?;;;","08/Nov/21 07:44;trohrmann;cc [~arvid];;;","10/Nov/21 05:54;syhily;[~gaoyunhaii] I think this is a new issue caused by {{Exclusive consumer is already connected}}. Can you submit another issue?
[~trohrmann] I think we can close this issue and track it in a new issue.;;;","11/Nov/21 08:54;trohrmann;Aright, I've created FLINK-24872 for tracking the new issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clarify metric deletion guarantees for Prometheus PushGateway reporter on shutdown,FLINK-23845,13395774,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,camilesing,camilesing,camilesing,18/Aug/21 01:24,11/Oct/21 21:25,13/Jul/23 08:12,25/Aug/21 07:55,1.12.5,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Documentation,,,,,0,pull-request-available,stale-blocker,,,"see https://issues.apache.org/jira/browse/FLINK-20691 .  whatever the problem has always existed, we should avoid other guys met it",,camilesing,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 25 07:55:01 UTC 2021,,,,,,,,,,"0|z0tzxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/21 01:48;camilesing;https://github.com/apache/flink/pull/16823;;;","21/Aug/21 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","25/Aug/21 07:55;trohrmann;Fixed via

1.14.0: 18f176ce86900fd4e932c73f3d138912355c6880
1.13.3: fbc78c8eb1f5fa781363d70ff549920e2f48c238;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix spelling mistakes for ""async""",FLINK-23844,13395732,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hapihu,hapihu,hapihu,17/Aug/21 17:40,18/Aug/21 16:51,13/Jul/23 08:12,18/Aug/21 02:34,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,,,,,,0,pull-request-available,,,,"Fix spelling mistakes for ""async""

The 'aysnc' should be changed to 'async'.

 

1.  flink-connectors/flink-connector-hbase-2.2/src/main/java/org/apache/flink/connector/hbase2/source/HBaseRowDataAsyncLookupFunction.java
{code:java}
Line 111: 
hbase-aysnc-lookup-worker  ==>  hbase-async-lookup-worker
{code}
2. flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/async/AsyncWaitOperatorTest.java
{code:java}
Line 949: 
AysncWaitOperator  ==> AsyncWaitOperator{code}",,hapihu,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 18 02:34:23 UTC 2021,,,,,,,,,,"0|z0tzo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/21 17:49;hapihu;Hi,[~jark] 
Excuse me for taking up your time.
I created pull request for this issue.
Would you like to review it for me.
Thank you very much!;;;","18/Aug/21 02:34;jark;Fixed in master: d5cdd6f3b207c01b3ff7dd363d07527e0185f347;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[doc]Modify incorrect English statements for page 'execution_configuration',FLINK-23841,13395715,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,hapihu,hapihu,hapihu,17/Aug/21 16:05,15/Dec/21 01:40,13/Jul/23 08:12,20/Aug/21 08:43,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.0,,,Documentation,,,,,0,pull-request-available,,,,"[https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/datastream/execution/execution_configuration/]

 

The English statement in line 82 is not correct.
{code:java}
//line 82
Note that types registered with `registerKryoType()` are not available to Flink's Kryo serializer instance.
{code}
It should be modified as follows:
{code:java}
Note that types registered with `registerKryoType()` are not available to Flink's POJO serializer instance.
{code}",,hapihu,trohrmann,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 20 08:43:31 UTC 2021,,,,,,,,,,"0|z0tzk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/21 16:17;hapihu;Hi,[~jark] 
Excuse me for taking up your time.
I created pull request for this issue.
Would you like to review it for me.
Thank you very much!;;;","18/Aug/21 18:51;hapihu;Hi,[~chesnay]
Excuse me for taking up your time.
I created pull request for this issue.
Would you like to review it for me.
If there is any problem, please inform me in time.
I will actively revise it in time.
Thank you very much!;;;","20/Aug/21 07:20;yunta;I think the typo may be obvious, however, from my understanding of Flink and a test job. Even a pojo type registered with `registerKryoType()`, it could still be avaiable to POJO serializer instances unless we set `enableForceKryo`. It seems the statements of this part needs to be changed. cc [~trohrmann].;;;","20/Aug/21 08:33;trohrmann;I think the proposed change is correct. I think the idea is that {{reigsterKryoType}} won't be registered at the POJO serializer. However a type can still be a POJO. That's what it tries to say.

{quote} I think the typo may be obvious, however, from my understanding of Flink and a test job. Even a pojo type registered with `registerKryoType()`, it could still be avaiable to POJO serializer instances unless we set `enableForceKryo`. It seems the statements of this part needs to be changed. {quote}

I think that's what's also written in the documentation on the same page: 

{quote} If the type ends up being serialized with Kryo, then it will be registered at Kryo to make sure that only tags (integer IDs) are written. If a type is not registered with Kryo, its entire class-name will be serialized with every instance, leading to much higher I/O costs. {quote}

;;;","20/Aug/21 08:43;trohrmann;Fixed via

1.14.0: 45bd9a45c1a35b21026dff69d12e67e0349fe3b3
1.13.3: dea8bede2ebff4b46d8a1eccc2338952a2cdf58c
1.12.6: 49fedc121003d7e41f80041135c61d517daa52cc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cache of ShuffleDescriptors should be individually cleaned up,FLINK-23833,13395619,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Thesharing,Thesharing,Thesharing,17/Aug/21 06:59,30/Aug/21 07:50,13/Jul/23 08:12,30/Aug/21 07:50,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"{color:#172b4d}In FLINK-23005, we introduce the cache of compressed serialized value for ShuffleDescriptors to improve the performance of deployment. To make sure the cache wouldn't stay too long and become a burden for GC, the cache would be cleaned up when the partition is released or reset for new execution. In the implementation, the cache of the entire IntermediateResult is cleaned up because a partition is released only when the entire IntermediateResult is released. {color}

{color:#172b4d}However, after FLINK-22017, the BLOCKING result partition is allowed to be consumable individually. It also means that the result partition doesn't need to wait for other result partitions and can be released individually. After this change, there may be a scene: when a result partition is finished, the cache of IntermediateResult on the blob is deleted, while other result partitions corresponding to this IntermediateResult is just deployed to the TaskExecutor. Then when TaskExecutors are trying to download TDD from the blob, they will find the blob is deleted and get stuck.{color}

{color:#172b4d}This bug only happens for jobs with POINTWISE BLOCKING edge. Also, the {{blob.offload.minsize}} is set to be a extremely small value, since the size of  ShuffleDescriptors of POINTWISE BLOCKING edges is usually small. To solve this issue, we just need to clean up the cache of ShuffleDescriptors individually.{color}",,Thesharing,trohrmann,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23005,FLINK-22017,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 30 07:50:19 UTC 2021,,,,,,,,,,"0|z0tyyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/21 07:50;zhuzh;Fixed via:
master:
8d3ec4a2fb1c9e07ee386dced15e1f64f359c6a2
ef6ef809b17b4547194c247975fb4566a19679f0
eba8f574c550123004ed4f557cef28ff557cd88e
1b4340a416bb7da39bb194da2faf3fc5e83fc5d9
1d38803c408fd4f4984913605030394d56bb160e

release-1.14:
f1eaf314ef232c1e604a49122857bbd99ad71a97
d4f8bb18ebe98ab4fd0d08ec24b00794d732cb38
f7bedb0603c33cb4e25c62c9899edb709b264371
daf997b7a13a62e251a9275a1b631697dea413bd
be857d1fdf8853b0315152521e27714ced6b1204;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SavepointITCase JVM crash on azure,FLINK-23829,13395602,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gaoyunhaii,xtsong,xtsong,17/Aug/21 04:09,27/Aug/21 02:45,13/Jul/23 08:12,27/Aug/21 02:45,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Checkpointing,,,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22293&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5224

{code}
Aug 16 16:26:11 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.2:test (integration-tests) on project flink-tests: There are test failures.
Aug 16 16:26:11 [ERROR] 
Aug 16 16:26:11 [ERROR] Please refer to /__w/1/s/flink-tests/target/surefire-reports for the individual test results.
Aug 16 16:26:11 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
Aug 16 16:26:11 [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Aug 16 16:26:11 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-tests/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /__w/1/s/flink-tests/target/surefire/surefirebooter8870094541887019356.jar /__w/1/s/flink-tests/target/surefire 2021-08-16T15-45-06_363-jvmRun2 surefire8582412554358604743tmp surefire_2118489584967019297925tmp
Aug 16 16:26:11 [ERROR] Error occurred in starting fork, check output in log
Aug 16 16:26:11 [ERROR] Process Exit Code: 239
Aug 16 16:26:11 [ERROR] Crashed tests:
Aug 16 16:26:11 [ERROR] org.apache.flink.test.checkpointing.SavepointITCase
Aug 16 16:26:11 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Aug 16 16:26:11 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-tests/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /__w/1/s/flink-tests/target/surefire/surefirebooter8870094541887019356.jar /__w/1/s/flink-tests/target/surefire 2021-08-16T15-45-06_363-jvmRun2 surefire8582412554358604743tmp surefire_2118489584967019297925tmp
Aug 16 16:26:11 [ERROR] Error occurred in starting fork, check output in log
Aug 16 16:26:11 [ERROR] Process Exit Code: 239
Aug 16 16:26:11 [ERROR] Crashed tests:
Aug 16 16:26:11 [ERROR] org.apache.flink.test.checkpointing.SavepointITCase
Aug 16 16:26:11 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:510)
Aug 16 16:26:11 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:457)
Aug 16 16:26:11 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:298)
Aug 16 16:26:11 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)
Aug 16 16:26:11 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
Aug 16 16:26:11 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
Aug 16 16:26:11 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
Aug 16 16:26:11 [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
Aug 16 16:26:11 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
Aug 16 16:26:11 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
Aug 16 16:26:11 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
Aug 16 16:26:11 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
Aug 16 16:26:11 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
Aug 16 16:26:11 [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
Aug 16 16:26:11 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
Aug 16 16:26:11 [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
Aug 16 16:26:11 [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
Aug 16 16:26:11 [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
Aug 16 16:26:11 [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
Aug 16 16:26:11 [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
Aug 16 16:26:11 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 16 16:26:11 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 16 16:26:11 [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 16 16:26:11 [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
Aug 16 16:26:11 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
Aug 16 16:26:11 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
Aug 16 16:26:11 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
Aug 16 16:26:11 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Aug 16 16:26:11 [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Aug 16 16:26:11 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-tests/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /__w/1/s/flink-tests/target/surefire/surefirebooter8870094541887019356.jar /__w/1/s/flink-tests/target/surefire 2021-08-16T15-45-06_363-jvmRun2 surefire8582412554358604743tmp surefire_2118489584967019297925tmp
Aug 16 16:26:11 [ERROR] Error occurred in starting fork, check output in log
Aug 16 16:26:11 [ERROR] Process Exit Code: 239
Aug 16 16:26:11 [ERROR] Crashed tests:
Aug 16 16:26:11 [ERROR] org.apache.flink.test.checkpointing.SavepointITCase
Aug 16 16:26:11 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:669)
Aug 16 16:26:11 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)
Aug 16 16:26:11 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:444)
Aug 16 16:26:11 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:420)
Aug 16 16:26:11 [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Aug 16 16:26:11 [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Aug 16 16:26:11 [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Aug 16 16:26:11 [ERROR] at java.lang.Thread.run(Thread.java:748)
Aug 16 16:26:11 [ERROR] -> [Help 1]
Aug 16 16:26:11 [ERROR] 
Aug 16 16:26:11 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
Aug 16 16:26:11 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
Aug 16 16:26:11 [ERROR] 
Aug 16 16:26:11 [ERROR] For more information about the errors and possible solutions, please read the following articles:
Aug 16 16:26:11 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
{code}",,gaoyunhaii,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23884,,,,,,,,,,,,,FLINK-2491,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 27 02:45:12 UTC 2021,,,,,,,,,,"0|z0tyv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/21 07:31;chesnay;testStopWithSavepointForFlip27SourceWithDrain:
{code}
16:02:44,161 [flink-akka.actor.default-dispatcher-6] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.actor.default-dispatcher-6' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: org.apache.flink.util.WrappingRuntimeException: java.util.concurrent.CompletionException: java.util.concurrent.RejectedExecutionException: org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailbox$MailboxClosedException: Mailbox is in state QUIESCED, but is required to be in state OPEN for put operations.
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:888) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.uniExceptionallyStage(CompletableFuture.java:898) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.exceptionally(CompletableFuture.java:2209) ~[?:1.8.0_292]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.assertTriggeringCheckpointExceptions(StreamTask.java:1213) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.triggerStopWithSavepointWithDrainAsync(SourceOperatorStreamTask.java:130) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.triggerCheckpointAsync(SourceOperatorStreamTask.java:118) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.triggerCheckpointBarrier(Task.java:1309) ~[flink-runtime-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.triggerCheckpoint(TaskExecutor.java:956) ~[flink-runtime-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
Caused by: org.apache.flink.util.WrappingRuntimeException: java.util.concurrent.CompletionException: java.util.concurrent.RejectedExecutionException: org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailbox$MailboxClosedException: Mailbox is in state QUIESCED, but is required to be in state OPEN for put operations.
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$assertTriggeringCheckpointExceptions$12(StreamTask.java:1224) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884) ~[?:1.8.0_292]
	... 36 more
Caused by: java.util.concurrent.CompletionException: java.util.concurrent.RejectedExecutionException: org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailbox$MailboxClosedException: Mailbox is in state QUIESCED, but is required to be in state OPEN for put operations.
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1005) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2137) ~[?:1.8.0_292]
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.triggerStopWithSavepointWithDrainAsync(SourceOperatorStreamTask.java:133) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	... 32 more
Caused by: java.util.concurrent.RejectedExecutionException: org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailbox$MailboxClosedException: Mailbox is in state QUIESCED, but is required to be in state OPEN for put operations.
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.execute(MailboxExecutorImpl.java:78) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointAsync(StreamTask.java:1106) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.lambda$triggerStopWithSavepointWithDrainAsync$0(SourceOperatorStreamTask.java:135) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:995) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2137) ~[?:1.8.0_292]
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.triggerStopWithSavepointWithDrainAsync(SourceOperatorStreamTask.java:133) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	... 32 more
Caused by: org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailbox$MailboxClosedException: Mailbox is in state QUIESCED, but is required to be in state OPEN for put operations.
	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.checkPutStateConditions(TaskMailboxImpl.java:269) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.put(TaskMailboxImpl.java:197) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.execute(MailboxExecutorImpl.java:74) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointAsync(StreamTask.java:1106) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.lambda$triggerStopWithSavepointWithDrainAsync$0(SourceOperatorStreamTask.java:135) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:995) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2137) ~[?:1.8.0_292]
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.triggerStopWithSavepointWithDrainAsync(SourceOperatorStreamTask.java:133) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	... 32 more
{code};;;","18/Aug/21 05:41;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22415&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=5108;;;","18/Aug/21 09:02;trohrmann;Here is another instance of this problem https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22276&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5240.

{code}
3:24:33,647 [flink-akka.actor.default-dispatcher-9] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.actor.default-dispatcher-9' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: org.apache.flink.util.WrappingRuntimeException: java.util.concurrent.CompletionException: java.util.concurrent.RejectedExecutionException: org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailbox$MailboxClosedException: Mailbox is in state CLOSED, but is required to be in state OPEN for put operations.
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:888) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.uniExceptionallyStage(CompletableFuture.java:898) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.exceptionally(CompletableFuture.java:2209) ~[?:1.8.0_292]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.assertTriggeringCheckpointExceptions(StreamTask.java:1211) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.triggerStopWithSavepointWithDrainAsync(SourceOperatorStreamTask.java:130) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.triggerCheckpointAsync(SourceOperatorStreamTask.java:118) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.triggerCheckpointBarrier(Task.java:1309) ~[flink-runtime-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.triggerCheckpoint(TaskExecutor.java:956) ~[flink-runtime-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[flink-rpc-akka_d4ae6041-5c32-4e8b-8526-4885241e5468.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_d4ae6041-5c32-4e8b-8526-4885241e5468.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[flink-rpc-akka_d4ae6041-5c32-4e8b-8526-4885241e5468.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[flink-rpc-akka_d4ae6041-5c32-4e8b-8526-4885241e5468.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[flink-rpc-akka_d4ae6041-5c32-4e8b-8526-4885241e5468.jar:1.14-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_d4ae6041-5c32-4e8b-8526-4885241e5468.jar:1.14-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_d4ae6041-5c32-4e8b-8526-4885241e5468.jar:1.14-SNAPSHOT]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_d4ae6041-5c32-4e8b-8526-4885241e5468.jar:1.14-SNAPSHOT]
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_d4ae6041-5c32-4e8b-8526-4885241e5468.jar:1.14-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_d4ae6041-5c32-4e8b-8526-4885241e5468.jar:1.14-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_d4ae6041-5c32-4e8b-8526-4885241e5468.jar:1.14-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_d4ae6041-5c32-4e8b-8526-4885241e5468.jar:1.14-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_d4ae6041-5c32-4e8b-8526-4885241e5468.jar:1.14-SNAPSHOT]
	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_d4ae6041-5c32-4e8b-8526-4885241e5468.jar:1.14-SNAPSHOT]
	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_d4ae6041-5c32-4e8b-8526-4885241e5468.jar:1.14-SNAPSHOT]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_d4ae6041-5c32-4e8b-8526-4885241e5468.jar:1.14-SNAPSHOT]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_d4ae6041-5c32-4e8b-8526-4885241e5468.jar:1.14-SNAPSHOT]
	at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_d4ae6041-5c32-4e8b-8526-4885241e5468.jar:1.14-SNAPSHOT]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_d4ae6041-5c32-4e8b-8526-4885241e5468.jar:1.14-SNAPSHOT]
	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_d4ae6041-5c32-4e8b-8526-4885241e5468.jar:1.14-SNAPSHOT]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_d4ae6041-5c32-4e8b-8526-4885241e5468.jar:1.14-SNAPSHOT]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
Caused by: org.apache.flink.util.WrappingRuntimeException: java.util.concurrent.CompletionException: java.util.concurrent.RejectedExecutionException: org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailbox$MailboxClosedException: Mailbox is in state CLOSED, but is required to be in state OPEN for put operations.
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$assertTriggeringCheckpointExceptions$12(StreamTask.java:1222) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884) ~[?:1.8.0_292]
	... 36 more
Caused by: java.util.concurrent.CompletionException: java.util.concurrent.RejectedExecutionException: org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailbox$MailboxClosedException: Mailbox is in state CLOSED, but is required to be in state OPEN for put operations.
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1005) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2137) ~[?:1.8.0_292]
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.triggerStopWithSavepointWithDrainAsync(SourceOperatorStreamTask.java:133) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	... 32 more
Caused by: java.util.concurrent.RejectedExecutionException: org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailbox$MailboxClosedException: Mailbox is in state CLOSED, but is required to be in state OPEN for put operations.
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.execute(MailboxExecutorImpl.java:78) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointAsync(StreamTask.java:1104) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.lambda$triggerStopWithSavepointWithDrainAsync$0(SourceOperatorStreamTask.java:135) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:995) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2137) ~[?:1.8.0_292]
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.triggerStopWithSavepointWithDrainAsync(SourceOperatorStreamTask.java:133) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	... 32 more
Caused by: org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailbox$MailboxClosedException: Mailbox is in state CLOSED, but is required to be in state OPEN for put operations.
	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.checkPutStateConditions(TaskMailboxImpl.java:269) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.put(TaskMailboxImpl.java:197) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.execute(MailboxExecutorImpl.java:74) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointAsync(StreamTask.java:1104) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.lambda$triggerStopWithSavepointWithDrainAsync$0(SourceOperatorStreamTask.java:135) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:995) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2137) ~[?:1.8.0_292]
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.triggerStopWithSavepointWithDrainAsync(SourceOperatorStreamTask.java:133) ~[flink-streaming-java_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	... 32 more
{code}

To me this looks not like a test instability but more like a problem introduced by FLINK-2491 which might affect the overall stability of Flink.

cc [~pnowojski], [~gaoyunhaii].;;;","18/Aug/21 11:07;gaoyunhaii;Very thanks for reporting and checking this issue! I'll have a look~;;;","18/Aug/21 12:11;trohrmann;Great, thanks [~gaoyunhaii]. I've assigned you to this ticket.;;;","18/Aug/21 13:48;gaoyunhaii;Very thanks [~trohrmann]! I should have found the reason, this should be caused by the concurrency problem between _triggerStopWithSavepointWithDrainAsync_ in the akka thread and the main thread. I'll open a PR for this issue~;;;","19/Aug/21 02:24;xtsong;Thanks, [~gaoyunhaii].
BTW, could you please move the ticket to in-progress?;;;","19/Aug/21 02:27;gaoyunhaii;Ok, I changed the status~;;;","20/Aug/21 08:24;gaoyunhaii;I opened a PR for this issue: https://github.com/apache/flink/pull/16905

To avoid the case might have other causes, I opened a new issue to track the above fix~;;;","27/Aug/21 02:45;gaoyunhaii;Since the fix is merged onto master as 475d9cace988fc7a8b163df64b19bbd84c360bfc, I'll first close this issue for now, and we could reopen it if it re-occur.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ModifiedMonotonicity inference for some node,FLINK-23827,13395597,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,icshuo,icshuo,icshuo,17/Aug/21 03:48,15/Dec/21 01:44,13/Jul/23 08:12,22/Sep/21 08:34,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"ModifiedMonotonicity handler do not handle some node properly, such as IntermediateTableScan, Deduplicate and LookupJoin.",,godfreyhe,icshuo,libenchao,Terry1897,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 22 08:34:25 UTC 2021,,,,,,,,,,"0|z0tyu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/21 08:34;godfreyhe;Fixed in 1.15.0: b8c6edb3b68ae920e785ae50de7745d5a37ebe3f
Fixed in 1.14.1: ae7a7303f95669d7138221979efae5ff846dc92d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DeleteExecutor NPE,FLINK-23813,13395473,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,Akihito Liang,Akihito Liang,16/Aug/21 12:50,30/Aug/21 20:29,13/Jul/23 08:12,20/Aug/21 12:21,1.12.5,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Connectors / JDBC,,,,,0,pull-request-available,,,,"Encountered a situation where I get an NPE from JDBCUpsertOutputFormat.

This occurs when jdbc disconnected and try to reconnect.

I need to write data to mysql in upsert way in sql, So it must group by unique key and the JdbcBatchingOutputFormat of Jdbc sink would use TableJdbcUpsertOutputFormat.

 

Jdbc would disconnected when The data interval exceeds the set connection time.I see that when jdbc reconnect , only JdbcBatchingOutputFormat#jdbcStatementExecutor(insert) would prepareStatements but TableJdbcUpsertOutputFormat#deleteExecutor would not prepareStatements so that come up NPE.

if in JdbcBatchingOutputFormat have a protected function to reset PrepareStatement and TableJdbcUpsertOutputFormat override this function to reset deleteExecutor, it would work well.

prepareStatements ",,Akihito Liang,roman,roman_old,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/21 02:49;Akihito Liang;image-2021-08-18-10-49-43-941.png;https://issues.apache.org/jira/secure/attachment/13032088/image-2021-08-18-10-49-43-941.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Fri Aug 20 12:19:42 UTC 2021,,,,,,,,,,"0|z0ty2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/21 08:28;xtsong;cc [~roman_khachatryan]
Would you please help check on this?;;;","17/Aug/21 16:19;roman;Sure [~xtsong].

 

[~Akihito Liang],

> if in JdbcBatchingOutputFormat have a protected function to reset PrepareStatement 

I think that function is JdbcOutputFormat.updateExecutor. It is called if attemptFlush() fails; the latter is already overriden in TableJdbcUpsertOutputFormat. It can also override updateExecutor to update its own deleteExecutor.

WDYT?

 

However, could you please provide the stacktrace (or a test case), just to be sure we are fixing the right problem? 

 

cc: [~jark], [~arvid];;;","18/Aug/21 02:50;Akihito Liang;hi, [~roman_khachatryan].

this is the deepest stacktrace.

!image-2021-08-18-10-49-43-941.png|width=957,height=317!;;;","20/Aug/21 12:19;roman;Thanks for the details [~Akihito Liang].

 

The fix merged for 1.14 and 1.13 as dfac7629819fad231ea5f0a0a0aa574299dd083b and 669f3b56b4c2999fd76c42504dbfa246684dcf34 accordingly.

 

As for 1.12, it would require backporting qutie some refactoring. As Flink [update policy|https://flink.apache.org/downloads.html#update-policy-for-old-releases] ony requires backporting to a single release, I'm going to remove 1.12.6 from the Fix versions unless there are any concerns.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use RestClient to detect restarts in MiniClusterTestEnvironment#triggerTaskManagerFailover,FLINK-23807,13395439,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,renqs,arvid,arvid,16/Aug/21 09:22,09/Sep/21 06:47,13/Jul/23 08:12,08/Sep/21 13:53,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Connectors / Common,,,,,0,pull-request-available,,,,"{{MiniClusterTestEnvironment#triggerTaskManagerFailover}} checks the job status to detect a restart
{noformat}
        terminateTaskManager();
        CommonTestUtils.waitForJobStatus(
                jobClient,
                Arrays.asList(JobStatus.FAILING, JobStatus.FAILED, JobStatus.RESTARTING),
                Deadline.fromNow(Duration.ofMinutes(5)));
        afterFailAction.run();
        startTaskManager();
{noformat}
However, `waitForJobStatus` polls every 100ms while the restart can happen within 100ms and thus can easily miss the actual restart and wait forever (or when the next restart happens because slots are missing).

We should rather use the metric `numRestarts`, check before the induced error, and wait until the counter increased.

Here is an excerpt from a log where the restart was not detected in time.
{noformat}
42769 [flink-akka.actor.default-dispatcher-26] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job TaskManager Failover Test (543035cf9e19317f92ee559b70ac70bd) switched from state RUNNING to RESTARTING.
42774 [flink-akka.actor.default-dispatcher-26] INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [ead7cad050ec7a264c0dba0b6e6a6ad9].
42775 [flink-akka.actor.default-dispatcher-23] INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job 543035cf9e19317f92ee559b70ac70bd
42776 [flink-akka.actor.default-dispatcher-22] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:RELEASING, resource profile: ResourceProfile{taskHeapMemory=170.667gb (183251937962 bytes), taskOffHeapMemory=170.667gb (183251937962 bytes), managedMemory=13.333mb (13981013 bytes), networkMemory=10.667mb (11184810 bytes)}, allocationId: ead7cad050ec7a264c0dba0b6e6a6ad9, jobId: 543035cf9e19317f92ee559b70ac70bd).
43780 [flink-akka.actor.default-dispatcher-26] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job TaskManager Failover Test (543035cf9e19317f92ee559b70ac70bd) switched from state RESTARTING to RUNNING.
43783 [flink-akka.actor.default-dispatcher-26] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Restoring job 543035cf9e19317f92ee559b70ac70bd from Checkpoint 11 @ 1629093422900 for 543035cf9e19317f92ee559b70ac70bd located at <checkpoint-not-externally-addressable>.
43798 [flink-akka.actor.default-dispatcher-26] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - No master state to restore
43800 [SourceCoordinator-Source: Tested Source -> Sink: Data stream collect sink] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Recovering subtask 0 to checkpoint 11 for source Source: Tested Source -> Sink: Data stream collect sink to checkpoint.
43801 [flink-akka.actor.default-dispatcher-26] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Tested Source -> Sink: Data stream collect sink (1/1) (35c0ee7183308af02db4b09152f1457e) switched from CREATED to SCHEDULED.
{noformat}
 

UPDATE: A better implementation would be using RestClient to detect if tasks are failed",,Jiangang,syhily,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23969,,,,,FLINK-24012,,,,FLINK-24174,,,,,,,,,,,FLINK-23944,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 08 13:53:16 UTC 2021,,,,,,,,,,"0|z0txuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/21 03:10;Jiangang;I have met the same problem and it blocks my ITCase for pulsar. Hope it to be resolved quickly.;;;","08/Sep/21 13:53;arvid;Merged into master as ef082c9d091846a1659277aa313b1cc6f79eba94.
Merged into 1.14 as 21fd895073231a34fd2c2659f12caef03a05a2f6.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StackOverflowException can happen if a large scale job failed to acquire enough slots in time,FLINK-23806,13395438,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zhuzh,zhuzh,zhuzh,16/Aug/21 09:20,15/Dec/21 01:40,13/Jul/23 08:12,19/Aug/21 08:17,1.12.5,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"When requested slots are not fulfilled in time, task failure will be triggered and all related tasks will be canceled and restarted. However, in this process, if a task is already assigned a slot, the slot will be returned to the slot pool and it will be immediately used to fulfill pending slot requests of the tasks which will soon be canceled. The execution version of those tasks are already bumped in {{DefaultScheduler#restartTasksWithDelay(...)}} so that the assignment will fail immediately and the slot will be returned to the slot pool and again used to fulfill pending slot requests. StackOverflow can happen in this way when there are many vertices, and fatal error can happen and lead to JM crash. A sample call stack is attached below.

To fix the problem, one way is to cancel the pending requests of all the tasks which will be canceled soon(i.e. tasks with version bumped) before canceling these tasks.

{panel}
...
        at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl.cancelSlotRequest(PhysicalSlotProviderImpl.java:112) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator.releaseSharedSlot(SlotSharingExecutionSlotAllocator.java:242) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.SharedSlot.releaseExternally(SharedSlot.java:281) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.SharedSlot.removeLogicalSlotRequest(SharedSlot.java:242) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.SharedSlot.returnLogicalSlot(SharedSlot.java:234) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.lambda$returnSlotToOwner$0(SingleLogicalSlot.java:203) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:705) ~[?:1.8.0_102]
        at java.util.concurrent.CompletableFuture.uniRunStage(CompletableFuture.java:717) ~[?:1.8.0_102]
        at java.util.concurrent.CompletableFuture.thenRun(CompletableFuture.java:2010) ~[?:1.8.0_102]
        at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.returnSlotToOwner(SingleLogicalSlot.java:200) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.releaseSlot(SingleLogicalSlot.java:130) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.DefaultScheduler.releaseSlotIfPresent(DefaultScheduler.java:542) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$8(DefaultScheduler.java:505) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ~[?:1.8.0_102]
        at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ~[?:1.8.0_102]
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_102]
        at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) ~[?:1.8.0_102]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge$PendingRequest.fulfill(DeclarativeSlotPoolBridge.java:552) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge$PendingRequestSlotMatching.fulfillPendingRequest(DeclarativeSlotPoolBridge.java:587) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.newSlotsAreAvailable(DeclarativeSlotPoolBridge.java:171) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.lambda$freeReservedSlot$0(DefaultDeclarativeSlotPool.java:316) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at java.util.Optional.ifPresent(Optional.java:159) ~[?:1.8.0_102]
        at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.freeReservedSlot(DefaultDeclarativeSlotPool.java:313) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.releaseSlot(DeclarativeSlotPoolBridge.java:335) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl.cancelSlotRequest(PhysicalSlotProviderImpl.java:112) ~[flink-dist_2.11-1.13-vvr-4.0.7-SNAPSHOT.jar:1.13-vvr-4.0.7-SNAPSHOT]
...

{panel}
",,Thesharing,trohrmann,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 19 08:17:32 UTC 2021,,,,,,,,,,"0|z0txuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/21 08:17;zhuzh;Fixed via
master/release-1.14: f543e9a97e2d2dda340d4d1d54467ffe060666cb
release-1.13: de16f34193799e7f3aade15b9bc57549f8010621
release-1.12: 5e83f3e6f3d9bef893a28e68b6ed2534589f1e30;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SavepointITCase.testStopSavepointWithBoundedInput fails on azure,FLINK-23797,13395386,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,xtsong,xtsong,16/Aug/21 05:13,01/Sep/21 14:25,13/Jul/23 08:12,01/Sep/21 14:25,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22220&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=4945

{code}
Aug 15 22:57:53 [ERROR] Tests run: 15, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 58.799 s <<< FAILURE! - in org.apache.flink.test.checkpointing.SavepointITCase
Aug 15 22:57:53 [ERROR] testStopSavepointWithBoundedInput  Time elapsed: 4.431 s  <<< ERROR!
Aug 15 22:57:53 java.util.concurrent.ExecutionException: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint triggering task Sink: Unnamed (1/1) of job a27d76d1e3a0145b179b30b3a4cd6564 is not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
Aug 15 22:57:53 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Aug 15 22:57:53 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Aug 15 22:57:53 	at org.apache.flink.test.checkpointing.SavepointITCase.testStopSavepointWithBoundedInput(SavepointITCase.java:612)
Aug 15 22:57:53 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 15 22:57:53 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 15 22:57:53 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 15 22:57:53 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 15 22:57:53 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 15 22:57:53 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 15 22:57:53 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 15 22:57:53 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 15 22:57:53 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Aug 15 22:57:53 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Aug 15 22:57:53 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Aug 15 22:57:53 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Aug 15 22:57:53 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 15 22:57:53 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Aug 15 22:57:53 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Aug 15 22:57:53 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Aug 15 22:57:53 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Aug 15 22:57:53 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Aug 15 22:57:53 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Aug 15 22:57:53 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Aug 15 22:57:53 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 15 22:57:53 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Aug 15 22:57:53 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 15 22:57:53 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 15 22:57:53 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Aug 15 22:57:53 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Aug 15 22:57:53 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
Aug 15 22:57:53 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Aug 15 22:57:53 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Aug 15 22:57:53 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Aug 15 22:57:53 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
Aug 15 22:57:53 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Aug 15 22:57:53 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Aug 15 22:57:53 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Aug 15 22:57:53 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Aug 15 22:57:53 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Aug 15 22:57:53 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
Aug 15 22:57:53 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
Aug 15 22:57:53 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
Aug 15 22:57:53 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
Aug 15 22:57:53 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
Aug 15 22:57:53 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
Aug 15 22:57:53 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
Aug 15 22:57:53 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
Aug 15 22:57:53 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
Aug 15 22:57:53 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
Aug 15 22:57:53 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Aug 15 22:57:53 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Aug 15 22:57:53 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Aug 15 22:57:53 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Aug 15 22:57:53 Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint triggering task Sink: Unnamed (1/1) of job a27d76d1e3a0145b179b30b3a4cd6564 is not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
Aug 15 22:57:53 	at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.checkTasksStarted(DefaultCheckpointPlanCalculator.java:145)
Aug 15 22:57:53 	at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.lambda$calculateCheckpointPlan$1(DefaultCheckpointPlanCalculator.java:107)
Aug 15 22:57:53 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
Aug 15 22:57:53 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:455)
Aug 15 22:57:53 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
Aug 15 22:57:53 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:455)
Aug 15 22:57:53 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:213)
Aug 15 22:57:53 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
Aug 15 22:57:53 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
Aug 15 22:57:53 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
Aug 15 22:57:53 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
Aug 15 22:57:53 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
Aug 15 22:57:53 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
Aug 15 22:57:53 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
Aug 15 22:57:53 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
Aug 15 22:57:53 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
Aug 15 22:57:53 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
Aug 15 22:57:53 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
Aug 15 22:57:53 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
Aug 15 22:57:53 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
Aug 15 22:57:53 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
Aug 15 22:57:53 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
Aug 15 22:57:53 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
Aug 15 22:57:53 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
Aug 15 22:57:53 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
Aug 15 22:57:53 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Aug 15 22:57:53 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Aug 15 22:57:53 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Aug 15 22:57:53 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
{code}",,pnowojski,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22379,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 01 14:25:40 UTC 2021,,,,,,,,,,"0|z0txj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/21 13:44;arvid;[~trohrmann@apache.org] could that be connected to your recent changes to this test?;;;","17/Aug/21 15:40;trohrmann;I couldn't find a problem caused by my recent changes.;;;","01/Sep/21 14:25;pnowojski;merged commit c7bf8b3 into apache:master
merged as 856c927ba93 into release-1.14;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnalignedCheckpointRescaleITCase JVM crash on Azure,FLINK-23796,13395385,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,arvid,xtsong,xtsong,16/Aug/21 05:09,30/Aug/21 05:58,13/Jul/23 08:12,30/Aug/21 05:58,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Checkpointing,,,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22220&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=5182

{code}
Aug 16 01:03:17 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.2:test (integration-tests) on project flink-tests: There are test failures.
Aug 16 01:03:17 [ERROR] 
Aug 16 01:03:17 [ERROR] Please refer to /__w/1/s/flink-tests/target/surefire-reports for the individual test results.
Aug 16 01:03:17 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
Aug 16 01:03:17 [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Aug 16 01:03:17 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-tests/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:+UseG1GC -jar /__w/1/s/flink-tests/target/surefire/surefirebooter4077406518503777021.jar /__w/1/s/flink-tests/target/surefire 2021-08-15T23-59-56_973-jvmRun1 surefire4438021626717472043tmp surefire_1445134621790231688950tmp
Aug 16 01:03:17 [ERROR] Error occurred in starting fork, check output in log
Aug 16 01:03:17 [ERROR] Process Exit Code: 137
Aug 16 01:03:17 [ERROR] Crashed tests:
Aug 16 01:03:17 [ERROR] org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase
Aug 16 01:03:17 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Aug 16 01:03:17 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-tests/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:+UseG1GC -jar /__w/1/s/flink-tests/target/surefire/surefirebooter4077406518503777021.jar /__w/1/s/flink-tests/target/surefire 2021-08-15T23-59-56_973-jvmRun1 surefire4438021626717472043tmp surefire_1445134621790231688950tmp
Aug 16 01:03:17 [ERROR] Error occurred in starting fork, check output in log
Aug 16 01:03:17 [ERROR] Process Exit Code: 137
{code}",,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23652,,,,,,,,FLINK-23913,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 30 05:56:00 UTC 2021,,,,,,,,,,"0|z0txiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/21 05:10;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22220&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10331;;;","17/Aug/21 05:10;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22329&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=5182;;;","17/Aug/21 07:44;chesnay;hmm, it looks like the JVM is really just crashing all of a sudden. Maybe its crashing due to an OOM, i.e.,  we may have a memory leak.;;;","18/Aug/21 04:06;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22415&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=5142;;;","18/Aug/21 14:35;trohrmann;According to the internet, exit code 137 might indeed indicate an OOM problem. I am wondering why it fails always in the {{cron_azure}} job. Maybe it is something specific to the used images?;;;","18/Aug/21 14:48;trohrmann;Hmm when attaching JVisualVM to the test case, then it looks as if we are constantly loading more and more classes w/o unloading them. Maybe the problem is that we are exceeding our meta space size. [~arvid], [~pnowojski] do you know more about the specifics of this test case?;;;","18/Aug/21 15:03;trohrmann;Can this be caused by the {{InMemoryReporter}} that keeps amassing {{GenericMetricGroups}}?;;;","19/Aug/21 04:00;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22463&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=5142;;;","19/Aug/21 07:03;trohrmann;The test instability is probably caused by FLINK-23776 which causes new metric registrations for every record.;;;","19/Aug/21 10:15;arvid;I have merged FLINK-23776, let's check back if this issue persists.;;;","19/Aug/21 10:17;trohrmann;Thanks Arvid. I've assigned you to this ticket so that you can close it if there are no other occurrences.;;;","19/Aug/21 10:18;arvid;On a related note, shouldn't Flink more gracefully exit on OOM? I guess it's a matter of test setup in this case, right? 137 is usually the return code when the process is killed by another process (e.g. Yarn container).;;;","19/Aug/21 10:42;trohrmann;Yes, I think it is the external system killing the Flink process hard. Not sure whether Flink can do anything about it.;;;","24/Aug/21 20:12;arvid;I haven't seen any new reports. There is still the possibility that FLINK-23794 also causes this. But will close this ticket for the time being.;;;","25/Aug/21 03:13;xtsong;New instance after FLINK-23776 merged.
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22770&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=5142;;;","26/Aug/21 03:20;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22855&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=5145;;;","30/Aug/21 03:38;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22981&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=5145;;;","30/Aug/21 05:56;arvid;Note that this failure predates the fix in FLINK-23794. If any new case happens now, it is caused for a different reason as we completely removed InMemoryReporter for this test case.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InMemoryReporter leaks memory,FLINK-23794,13395382,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,arvid,xtsong,xtsong,16/Aug/21 04:49,01/Sep/21 12:02,13/Jul/23 08:12,27/Aug/21 09:25,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Tests,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22196&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=13960

{code}
Aug 14 22:56:30 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.2:test (default-test) on project flink-connector-jdbc_2.11: There are test failures.
Aug 14 22:56:30 [ERROR] 
Aug 14 22:56:30 [ERROR] Please refer to /__w/1/s/flink-connectors/flink-connector-jdbc/target/surefire-reports for the individual test results.
Aug 14 22:56:30 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
Aug 14 22:56:30 [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Aug 14 22:56:30 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-connectors/flink-connector-jdbc && /usr/lib/jvm/adoptopenjdk-11-hotspot-amd64/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /__w/1/s/flink-connectors/flink-connector-jdbc/target/surefire/surefirebooter3870491592340940577.jar /__w/1/s/flink-connectors/flink-connector-jdbc/target/surefire 2021-08-14T22-14-27_386-jvmRun2 surefire3999990822284944903tmp surefire_7612891660133211258241tmp
Aug 14 22:56:30 [ERROR] Error occurred in starting fork, check output in log
Aug 14 22:56:30 [ERROR] Process Exit Code: 239
Aug 14 22:56:30 [ERROR] Crashed tests:
Aug 14 22:56:30 [ERROR] org.apache.flink.connector.jdbc.xa.JdbcExactlyOnceSinkE2eTest
Aug 14 22:56:30 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Aug 14 22:56:30 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-connectors/flink-connector-jdbc && /usr/lib/jvm/adoptopenjdk-11-hotspot-amd64/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /__w/1/s/flink-connectors/flink-connector-jdbc/target/surefire/surefirebooter3870491592340940577.jar /__w/1/s/flink-connectors/flink-connector-jdbc/target/surefire 2021-08-14T22-14-27_386-jvmRun2 surefire3999990822284944903tmp surefire_7612891660133211258241tmp
Aug 14 22:56:30 [ERROR] Error occurred in starting fork, check output in log
Aug 14 22:56:30 [ERROR] Process Exit Code: 239
{code}",,roman,roman_old,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23525,,,,,,,FLINK-23652,,,,,,,,,,,,,,,,,,,,"23/Aug/21 11:34;roman;Screenshot_2021-08-23_13-34-31.png;https://issues.apache.org/jira/secure/attachment/13032310/Screenshot_2021-08-23_13-34-31.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 27 09:25:12 UTC 2021,,,,,,,,,,"0|z0txi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/21 05:16;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22220&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=13961;;;","23/Aug/21 03:39;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22618&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=13967;;;","23/Aug/21 06:47;trohrmann;cc [~roman_khachatryan];;;","23/Aug/21 11:36;roman;The crashes are caused by ""OOM: Heap space"";

When analyzing the heapdump, I see old tasks and all the related objects are still referred to, and thus not GCed. There are around 25K flink Task instances.

Those Tasks a referenced from InMemoryReporter:

 !Screenshot_2021-08-23_13-34-31.png! 

-The strange thing is that- MetricRegistryImpl of attempt 13 (task#1, MetricRegistryImpl#1) is used by e.g. attempt 4666 (task#19053).

edit:
The above is fine as MetricRegistryImpl and is its reporters are re-used across attempts.

What's not fine is that InMemoryReporter is not cleared until the test end.
I see that it's removedGroups set size is 202458.

cc: [~arvid];;;","23/Aug/21 12:28;roman;The lambda that references the task (TaskExecutor$$Lambda$1325#1) is likely
[task::isBackPressured|https://github.com/apache/flink/blob/48f531d290dae7783f44f29f3a7e7eec07a12313/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskExecutor.java#L739].

To sum up:
* TaskExecutor creates some metrics for each task that reference this task
* Those metrics are added to InMemoryReporter
* When the task is removed, the corresponding metrics are only scheduled for removal from InMemoryReporter
* With many restarts during the test, InMemoryReporter references all previous attempts and prevents GC

I see the following solutions:
1. Disable InMemoryReporter by default
2. Disable InMemoryReporter for this and similar tests
3. Try to cleanup references (i.e. null out task reference in metric when the task is removed)

The latter seems very fragile and modifying production code only for tests.

[~arvid] WDYT? ;;;","23/Aug/21 14:10;trohrmann;Why can't we remove the metrics right away? Is this some special behaviour for some tests that we need but for other tests this falls on our toes [~arvid]?;;;","23/Aug/21 14:12;arvid;Wow thanks for the in-depth analysis.
I think I'll fix that in the InMemoryReporter (remove all metrics eagerly unless explicitly stated in `InMemoryReporterRule`) or overhaul the whole mechanic (only register when there is a `InMemoryReporterRule`).
Can I take this ticket?;;;","23/Aug/21 14:26;roman;Sure, please take it :) ;;;","27/Aug/21 09:25;arvid;Merged into master as 3dbd9e785394590bf52fd7dc41c00c5254a4de24..126fafc25d8769e1cdcd8d88e153bc33dbc95c61.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unnecessary setTotalOrderForSeek for Rocks iterator,FLINK-23789,13395373,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,16/Aug/21 03:58,17/Aug/21 03:02,13/Jul/23 08:12,17/Aug/21 03:02,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"From FLINK-17800, we have to explicitly add setTotalOrderForSeek to ensure data correctness if user choose to set OptimizeForPointLookup.

However, this is unnecessary since we upgraded the RocksDB version (please refer to [rocksDB PR|https://github.com/facebook/rocksdb/pull/5165]) which removes the prefix extractor and hash-based indexing.
",,liyu,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17800,,,,,,,,,,,FLINK-14482,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 17 03:02:19 UTC 2021,,,,,,,,,,"0|z0txg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/21 03:02;yunta;merged in master:
ee74231c385e2c2d6b11cc5d4e5f557f03d0a30f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SinkITCase.testMetrics fails with ConcurrentModification on Azure,FLINK-23785,13395339,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,arvid,trohrmann,trohrmann,15/Aug/21 14:57,16/Aug/21 12:37,13/Jul/23 08:12,16/Aug/21 12:37,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,API / DataStream,,,,,0,pull-request-available,test-stability,,,"The {{SinkITCase.testMetrics}} fails with a ConcurrentModification

{code}
Aug 15 14:26:40 java.util.ConcurrentModificationException
Aug 15 14:26:40 	at java.util.HashMap$HashIterator.nextNode(HashMap.java:1445)
Aug 15 14:26:40 	at java.util.HashMap$KeyIterator.next(HashMap.java:1469)
Aug 15 14:26:40 	at java.util.AbstractSet.removeAll(AbstractSet.java:174)
Aug 15 14:26:40 	at org.apache.flink.runtime.testutils.InMemoryReporter.applyRemovals(InMemoryReporter.java:78)
Aug 15 14:26:40 	at org.apache.flink.runtime.testutils.InMemoryReporterRule.afterTestSuccess(InMemoryReporterRule.java:61)
Aug 15 14:26:40 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:57)
Aug 15 14:26:40 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Aug 15 14:26:40 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Aug 15 14:26:40 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Aug 15 14:26:40 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 15 14:26:40 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Aug 15 14:26:40 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Aug 15 14:26:40 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Aug 15 14:26:40 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Aug 15 14:26:40 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Aug 15 14:26:40 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Aug 15 14:26:40 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Aug 15 14:26:40 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 15 14:26:40 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22215&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5225",,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23793,,,,,,,FLINK-23652,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 16 12:37:51 UTC 2021,,,,,,,,,,"0|z0tx8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/21 14:57;trohrmann;Probably introduced recently with FLINK-23652. cc [~arvid];;;","15/Aug/21 18:31;arvid;Synchronizing access to the reporter unfortunately only solves the symptom and not the root cause. The main issue is that apparently some metrics are only unregistered after the test case has already finished.
That means that metrics from test case A can spill into test case B using the same mini cluster.

That means that {{env.execute}} returned while metric system has not converged to a stable state. It's similar to {{FLINK-23647}}. So maybe [~roman_khachatryan] is right that we should find a general solution and not workaround it in each and every test case.

I'll try to come up with a workaround for now.;;;","16/Aug/21 06:28;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22233&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5270;;;","16/Aug/21 12:37;arvid;Merged into master as 236bd652c9f2080c490935e3e0f2381fb7850b4a..cab4a9875b9ab5cd4c7da3a7e83ec77cb2a4838f.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression on 14.08.2021 in FLIP-27,FLINK-23776,13395296,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,pnowojski,pnowojski,14/Aug/21 20:43,03/Sep/21 08:40,13/Jul/23 08:12,03/Sep/21 08:40,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,API / DataStream,Benchmarks,,,,0,pull-request-available,,,,"http://codespeed.dak8s.net:8000/timeline/?ben=mapSink.F27_UNBOUNDED&env=2
http://codespeed.dak8s.net:8000/timeline/?ben=mapRebalanceMapSink.F27_UNBOUNDED&env=2


{noformat}
git ls 7b60a964b1..7f3636f6b4
7f3636f6b4f [2 days ago] [FLINK-23652][connectors] Adding common source metrics. [Arvid Heise]
97c8f72b813 [3 months ago] [FLINK-23652][connectors] Adding common sink metrics. [Arvid Heise]
48da20e8f88 [3 months ago] [FLINK-23652][test] Adding InMemoryMetricReporter and using it by default in MiniClusterResource. [Arvid Heise]
63ee60859ca [3 months ago] [FLINK-23652][core/metrics] Extract Operator(IO)MetricGroup interfaces and expose them in RuntimeContext [Arvid Heise]
5d5e39b614b [2 days ago] [refactor][connectors] Only use MockSplitReader.Builder for instantiation. [Arvid Heise]
b927035610c [3 months ago] [refactor][core] Extract common context creation in CollectionExecutor [Arvid Heise]
{noformat}
",,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23652,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 03 08:40:14 UTC 2021,,,,,,,,,,"0|z0twz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/21 05:18;arvid;Thanks for the report. I will investigate. ;;;","19/Aug/21 10:15;arvid;Merged into master as 976026b63175e63009749936416fc9581a0bf56b.;;;","23/Aug/21 10:17;pnowojski;Issue doesn't seem to be fully resolved: http://codespeed.dak8s.net:8000/timeline/?ben=mapSink.F27_UNBOUNDED&env=2;;;","27/Aug/21 18:29;arvid;Merged an improvement as fd429d084264357d3cfbfd8b2b62cf8327a8fd79.;;;","03/Sep/21 08:40;pnowojski;Basing on the recent results it looks like the regression is gone after the last update. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error doc's function name of datastream api,FLINK-23774,13395257,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,zhongjiajie,zhongjiajie,14/Aug/21 10:29,14/Aug/21 10:32,13/Jul/23 08:12,14/Aug/21 10:32,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Documentation,,,,,0,,,,,Doc correct datastream function name from _executeAysnc_ to _executeAsync._,,zhongjiajie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 14 10:32:32 UTC 2021,,,,,,,,,,"0|z0twqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/21 10:32;zhongjiajie;Found out typo could skip Jira ticket;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaPartitionSplitReader should remove empty splits from fetcher,FLINK-23773,13395229,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,14/Aug/21 03:54,14/Sep/21 05:47,13/Jul/23 08:12,14/Sep/21 05:47,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"Currently if a {{KafkaPartitionSplit}} is empty (startingOffset >= stoppingOffset), split reader only unsubscribes it from consumer, but doesn't remove it from SplitFetcher. This will lead to consumer complaining some partitions are not subscribed while fetching.",,becket_qin,renqs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 14 05:47:48 UTC 2021,,,,,,,,,,"0|z0twk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/21 05:47;becket_qin;PR merged to master: fe17ca6042c570ce603bf4308775f61db1d515c9
cherry-picked to release-1.14: b26f7e7f5a0f1accda991a9304afa49369f5c553
cherry-picked to release-1.13: 763ac52092ba70dfef989d18b711400b437e6e09

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_data_stream.py failed with flake8 checks,FLINK-23766,13395096,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,xtsong,xtsong,13/Aug/21 09:57,13/Aug/21 11:53,13/Jul/23 08:12,13/Aug/21 11:53,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,API / Python,,,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22050&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=21118

{code}
Aug 13 06:50:12 ================flake8 checks=================
Aug 13 06:50:14 ./pyflink/datastream/tests/test_data_stream.py:21:1: F401 'unittest' imported but unused
Aug 13 06:50:14 ==========flake8 checks... [FAILED]===========
{code}",,dianfu,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 13 11:53:23 UTC 2021,,,,,,,,,,"0|z0tvqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/21 10:00;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22051&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=21119;;;","13/Aug/21 10:02;xtsong;cc [~dian.fu] [~hxbks2ks];;;","13/Aug/21 11:53;dianfu;[~xtsong] Thanks for reporting this issue. It's already fixed in master via 470eaff41e0c0e1e54755964d7e62003b437be27.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PythonTableFunctionOperatorTestBase  java.lang.NullPointerException,FLINK-23765,13395092,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hxbks2ks,ym,ym,13/Aug/21 09:29,15/Dec/21 01:40,13/Jul/23 08:12,17/Aug/21 01:52,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.0,,,API / Python,,,,,0,pull-request-available,,,," 
 PythonTableFunctionOperatorTest>PythonTableFunctionOperatorTestBase.testFinishBundleTriggeredByTime:147 » NullPointer 
PythonTableFunctionOperatorTest>PythonTableFunctionOperatorTestBase.testFinishBundleTriggeredOnCheckpoint:93 » NullPointer 
 PythonTableFunctionOperatorTest>PythonTableFunctionOperatorTestBase.testLeftJoin:179 » NullPointer 
 PythonTableFunctionOperatorTest>PythonTableFunctionOperatorTestBase.testRetractionFieldKept:68 » NullPointer",,dianfu,gaoyunhaii,hxbks2ks,pnowojski,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23790,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 17 01:52:46 UTC 2021,,,,,,,,,,"0|z0tvps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/21 09:30;ym;[https://dev.azure.com/mymeiyuan/Flink/_build/results?buildId=313&view=logs&j=f3dc9b18-b77a-55c1-591e-264c46fe44d1&t=2d3cd81e-1c37-5c31-0ee4-f5d5cdb9324d&l=28528]

 

 ;;;","13/Aug/21 09:32;ym;It may be related to

https://issues.apache.org/jira/browse/FLINK-23309

 

 ;;;","13/Aug/21 09:34;ym;Aug 13 06:24:26 at org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperator.emitResult(PythonTableFunctionOperator.java:193)
Aug 13 06:24:26 at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.invokeFinishBundle(AbstractPythonFunctionOperator.java:351)
Aug 13 06:24:26 at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.finish

 

 

I think it may relate to how to handle data after finish

 

In the current contract, there should not be any more input data after function.finish() is called.

 

[~gaoyunhaii], please correct me if I am wrong;;;","13/Aug/21 09:38;gaoyunhaii;[~ym] I'll have a look~ Very thanks for reporting this issue~ ;;;","13/Aug/21 09:39;dianfu;cc [~hxbks2ks];;;","13/Aug/21 09:47;hxbks2ks;I have pushed a hotfix to ignore PythonTableFunctionOperatorTest temporarily.;;;","14/Aug/21 08:59;pnowojski;https://dev.azure.com/pnowojski/Flink/_build/results?buildId=475&view=logs&j=f3dc9b18-b77a-55c1-591e-264c46fe44d1&t=2d3cd81e-1c37-5c31-0ee4-f5d5cdb9324d;;;","17/Aug/21 01:52;hxbks2ks;Merged into master via 8128c04fc9c44a47d2e383597bf9ddd6cd86eb7b
Merged into release-1.13 via 5170e205e86cba5f6f524eefe2abd33fac5ff5f1
Merged into release-1.12 via 4380185253863dbcd4fef7ffde712e64d7316647;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SuccessAfterNetworkBuffersFailureITCase.testSuccessfulProgramAfterFailure fails due to AskTimeoutException,FLINK-23746,13394996,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,xtsong,xtsong,13/Aug/21 02:49,15/Aug/21 10:25,13/Jul/23 08:12,15/Aug/21 10:25,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22020&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10401

{code}
Aug 12 23:01:56 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 33.224 s <<< FAILURE! - in org.apache.flink.test.misc.SuccessAfterNetworkBuffersFailureITCase
Aug 12 23:01:56 [ERROR] testSuccessfulProgramAfterFailure  Time elapsed: 20.554 s  <<< ERROR!
Aug 12 23:01:56 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
Aug 12 23:01:56 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
Aug 12 23:01:56 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
Aug 12 23:01:56 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:250)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
Aug 12 23:01:56 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
Aug 12 23:01:56 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
Aug 12 23:01:56 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
Aug 12 23:01:56 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
Aug 12 23:01:56 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
Aug 12 23:01:56 	at akka.dispatch.OnComplete.internal(Future.scala:300)
Aug 12 23:01:56 	at akka.dispatch.OnComplete.internal(Future.scala:297)
Aug 12 23:01:56 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
Aug 12 23:01:56 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
Aug 12 23:01:56 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
Aug 12 23:01:56 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
Aug 12 23:01:56 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
Aug 12 23:01:56 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
Aug 12 23:01:56 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
Aug 12 23:01:56 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
Aug 12 23:01:56 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
Aug 12 23:01:56 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
Aug 12 23:01:56 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
Aug 12 23:01:56 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
Aug 12 23:01:56 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
Aug 12 23:01:56 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
Aug 12 23:01:56 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
Aug 12 23:01:56 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
Aug 12 23:01:56 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
Aug 12 23:01:56 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
Aug 12 23:01:56 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
Aug 12 23:01:56 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
Aug 12 23:01:56 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
Aug 12 23:01:56 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
Aug 12 23:01:56 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Aug 12 23:01:56 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Aug 12 23:01:56 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Aug 12 23:01:56 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Aug 12 23:01:56 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
Aug 12 23:01:56 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
Aug 12 23:01:56 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
Aug 12 23:01:56 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:228)
Aug 12 23:01:56 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:218)
Aug 12 23:01:56 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:209)
Aug 12 23:01:56 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:679)
Aug 12 23:01:56 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51)
Aug 12 23:01:56 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1465)
Aug 12 23:01:56 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1130)
Aug 12 23:01:56 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1070)
Aug 12 23:01:56 	at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:909)
Aug 12 23:01:56 	at org.apache.flink.runtime.executiongraph.Execution.lambda$deploy$5(Execution.java:613)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
Aug 12 23:01:56 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:455)
Aug 12 23:01:56 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
Aug 12 23:01:56 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:455)
Aug 12 23:01:56 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:213)
Aug 12 23:01:56 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
Aug 12 23:01:56 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
Aug 12 23:01:56 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
Aug 12 23:01:56 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
Aug 12 23:01:56 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
Aug 12 23:01:56 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
Aug 12 23:01:56 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
Aug 12 23:01:56 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
Aug 12 23:01:56 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
Aug 12 23:01:56 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
Aug 12 23:01:56 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
Aug 12 23:01:56 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
Aug 12 23:01:56 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
Aug 12 23:01:56 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
Aug 12 23:01:56 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
Aug 12 23:01:56 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
Aug 12 23:01:56 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
Aug 12 23:01:56 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
Aug 12 23:01:56 	... 4 more
Aug 12 23:01:56 Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Invocation of [LocalRpcInvocation(TaskExecutorGateway.submitTask(TaskDeploymentDescriptor, JobMasterId, Time))] at recipient [akka://flink/user/rpc/taskmanager_1] timed out.
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:326)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:338)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture.uniRelay(CompletableFuture.java:925)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:913)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
Aug 12 23:01:56 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:246)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
Aug 12 23:01:56 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1387)
Aug 12 23:01:56 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
Aug 12 23:01:56 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
Aug 12 23:01:56 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
Aug 12 23:01:56 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:45)
Aug 12 23:01:56 	at akka.dispatch.OnComplete.internal(Future.scala:299)
Aug 12 23:01:56 	at akka.dispatch.OnComplete.internal(Future.scala:297)
Aug 12 23:01:56 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
Aug 12 23:01:56 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
Aug 12 23:01:56 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
Aug 12 23:01:56 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
Aug 12 23:01:56 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
Aug 12 23:01:56 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
Aug 12 23:01:56 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
Aug 12 23:01:56 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
Aug 12 23:01:56 	at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:729)
Aug 12 23:01:56 	at akka.actor.Scheduler$$anon$7.run(Scheduler.scala:479)
Aug 12 23:01:56 	at akka.dispatch.internal.SameThreadExecutionContext$$anon$1.unbatchedExecute(SameThreadExecutionContext.scala:21)
Aug 12 23:01:56 	at akka.dispatch.BatchingExecutor.execute(BatchingExecutor.scala:133)
Aug 12 23:01:56 	at akka.dispatch.BatchingExecutor.execute$(BatchingExecutor.scala:124)
Aug 12 23:01:56 	at akka.dispatch.internal.SameThreadExecutionContext$$anon$1.execute(SameThreadExecutionContext.scala:20)
Aug 12 23:01:56 	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:365)
Aug 12 23:01:56 	at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:314)
Aug 12 23:01:56 	at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:318)
Aug 12 23:01:56 	at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:270)
Aug 12 23:01:56 	at java.lang.Thread.run(Thread.java:748)
Aug 12 23:01:56 Caused by: java.util.concurrent.TimeoutException: Invocation of [LocalRpcInvocation(TaskExecutorGateway.submitTask(TaskDeploymentDescriptor, JobMasterId, Time))] at recipient [akka://flink/user/rpc/taskmanager_1] timed out.
Aug 12 23:01:56 	at org.apache.flink.runtime.jobmaster.RpcTaskManagerGateway.submitTask(RpcTaskManagerGateway.java:60)
Aug 12 23:01:56 	at org.apache.flink.runtime.executiongraph.Execution.lambda$deploy$4(Execution.java:589)
Aug 12 23:01:56 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
Aug 12 23:01:56 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
Aug 12 23:01:56 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Aug 12 23:01:56 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
Aug 12 23:01:56 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
Aug 12 23:01:56 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Aug 12 23:01:56 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Aug 12 23:01:56 	... 1 more
Aug 12 23:01:56 Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/taskmanager_1#-791314098]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
{code}",,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 15 10:25:47 UTC 2021,,,,,,,,,,"0|z0tv4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/21 10:28;trohrmann;In the logs, it looks as if CI is simply slow. It takes more than 10s to deploy all the vertices for the connected components job. I will decrease the degree of parallelism to reduce the number of tasks to deploy in order to harden the test a bit. If this does not work, then I would suggest to increase the ask timeout as a next step.;;;","15/Aug/21 10:25;trohrmann;Hardened via 58abcafb151ebfede229b564b3edbde0a15f52c9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CliClientTest.testCancelExecutionInteractiveMode fails on azure,FLINK-23744,13394993,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,xtsong,xtsong,13/Aug/21 02:27,16/Aug/21 02:46,13/Jul/23 08:12,16/Aug/21 02:46,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Table SQL / Client,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21983&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=9760

{code}
Aug 12 13:14:02 [ERROR] Tests run: 12, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.496 s <<< FAILURE! - in org.apache.flink.table.client.cli.CliClientTest
Aug 12 13:14:02 [ERROR] testCancelExecutionInteractiveMode  Time elapsed: 0.078 s  <<< FAILURE!
Aug 12 13:14:02 java.lang.AssertionError
Aug 12 13:14:02 	at org.junit.Assert.fail(Assert.java:87)
Aug 12 13:14:02 	at org.junit.Assert.assertTrue(Assert.java:42)
Aug 12 13:14:02 	at org.junit.Assert.assertTrue(Assert.java:53)
Aug 12 13:14:02 	at org.apache.flink.table.client.cli.CliClientTest.testCancelExecutionInteractiveMode(CliClientTest.java:335)
Aug 12 13:14:02 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 12 13:14:02 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 12 13:14:02 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 12 13:14:02 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 12 13:14:02 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 12 13:14:02 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 12 13:14:02 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 12 13:14:02 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 12 13:14:02 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
Aug 12 13:14:02 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
Aug 12 13:14:02 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Aug 12 13:14:02 	at java.lang.Thread.run(Thread.java:748)
{code}",,fsk119,jark,roman,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23645,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 16 02:46:43 UTC 2021,,,,,,,,,,"0|z0tv3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/21 02:28;xtsong;cc [~jark] [~loyi]
Seems introduced by FLINK-23645.;;;","13/Aug/21 02:35;jark;cc [~fsk119];;;","15/Aug/21 21:31;roman;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22212&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=9760;;;","16/Aug/21 02:46;jark;Fixed in master: 5a43b0cef25e2b73dc3b79edbe92f402ae39ac1d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_keyed_co_process test failed in py36 and py37,FLINK-23742,13394990,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dianfu,hxbks2ks,hxbks2ks,13/Aug/21 01:49,13/Aug/21 05:50,13/Jul/23 08:12,13/Aug/21 05:50,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,API / Python,,,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22020&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a
",,dianfu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23745,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 13 05:50:11 UTC 2021,,,,,,,,,,"0|z0tv34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/21 01:54;dianfu;[~hxbks2ks] Thanks for reporting this issue. I will take a look at this issue. As a quick fix, I have disabled the failed test case for now via adcaafffc74c2942935905e4a2f8af141d145e2f;;;","13/Aug/21 05:50;dianfu;Fixed in master via 37df9b70d82ce73b43035530787630d0a707c16b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-147 Waiting for final checkpoint can deadlock job,FLINK-23741,13394941,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pnowojski,pnowojski,pnowojski,12/Aug/21 16:57,14/Aug/21 09:00,13/Jul/23 08:12,14/Aug/21 09:00,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Checkpointing,Runtime / Task,,,,0,pull-request-available,,,,"With {{ENABLE_CHECKPOINTS_AFTER_TASKS_FINISH}} enabled, final checkpoint can deadlock (or timeout after very long time) if there is a race condition between selecting tasks to trigger checkpoint on and finishing tasks. FLINK-21246 was supposed to handle it, but it doesn't work as expected, because futures from:
org.apache.flink.runtime.taskexecutor.TaskExecutor#triggerCheckpoint
and
org.apache.flink.streaming.runtime.tasks.StreamTask#triggerCheckpointAsync
are not linked together. TaskExecutor#triggerCheckpoint reports that checkpoint has been successfully triggered, while {{StreamTask}} might have actually finished.",,pnowojski,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21246,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 14 09:00:13 UTC 2021,,,,,,,,,,"0|z0tus8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/21 09:00;pnowojski;Merged to master as cc417cd0636 and edaf75ee072;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Delete unnecessary test line in page `Deployment/Resource Providers/Yarn`,FLINK-23736,13394861,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Liebing,Liebing,Liebing,12/Aug/21 10:00,16/Aug/21 02:54,13/Jul/23 08:12,16/Aug/21 02:53,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Documentation,,,,,0,pull-request-available,,,,"There is the following line in page `Deployment/Resource Providers/Yarn`:
{code:java}
[测试](\{{< downloads >}})
{code}
I think it was introduced by mistake.

It affected:

1. https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/deployment/resource-providers/yarn/

2. https://ci.apache.org/projects/flink/flink-docs-master/docs/deployment/resource-providers/yarn/",,jark,Liebing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23769,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 16 02:53:54 UTC 2021,,,,,,,,,,"0|z0tuag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/21 02:53;jark;Fixed in master: b8cdef00476b9a19c671c442054efeba83a14270;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StatefulJobSavepointMigrationITCase.testRestoreSavepoint fails due to LoggerInitializationException,FLINK-23732,13394803,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,xtsong,xtsong,12/Aug/21 02:38,23/Jan/22 11:12,13/Jul/23 08:12,15/Aug/21 10:26,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21905&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10229

{code}
Aug 11 13:18:49 [ERROR] Tests run: 40, Failures: 0, Errors: 1, Skipped: 20, Time elapsed: 105.47 s <<< FAILURE! - in org.apache.flink.api.scala.migration.StatefulJobSavepointMigrationITCase
Aug 11 13:18:49 [ERROR] testRestoreSavepoint[Migrate Savepoint / Backend: {1.13,rocksdb}]  Time elapsed: 43.482 s  <<< ERROR!
Aug 11 13:18:49 java.lang.Exception: Could not create actor system
Aug 11 13:18:49 	at org.apache.flink.runtime.rpc.akka.AkkaBootstrapTools.startLocalActorSystem(AkkaBootstrapTools.java:238)
Aug 11 13:18:49 	at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils$AkkaRpcServiceBuilder.createAndStart(AkkaRpcServiceUtils.java:349)
Aug 11 13:18:49 	at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils$AkkaRpcServiceBuilder.createAndStart(AkkaRpcServiceUtils.java:327)
Aug 11 13:18:49 	at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils$AkkaRpcServiceBuilder.createAndStart(AkkaRpcServiceUtils.java:247)
Aug 11 13:18:49 	at org.apache.flink.runtime.minicluster.MiniCluster.createLocalRpcService(MiniCluster.java:986)
Aug 11 13:18:49 	at org.apache.flink.runtime.minicluster.MiniCluster.start(MiniCluster.java:297)
Aug 11 13:18:49 	at org.apache.flink.runtime.testutils.MiniClusterResource.startMiniCluster(MiniClusterResource.java:203)
Aug 11 13:18:49 	at org.apache.flink.runtime.testutils.MiniClusterResource.before(MiniClusterResource.java:91)
Aug 11 13:18:49 	at org.apache.flink.test.util.MiniClusterWithClientResource.before(MiniClusterWithClientResource.java:64)
Aug 11 13:18:49 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:50)
Aug 11 13:18:49 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Aug 11 13:18:49 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Aug 11 13:18:49 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 11 13:18:49 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Aug 11 13:18:49 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Aug 11 13:18:49 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Aug 11 13:18:49 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Aug 11 13:18:49 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Aug 11 13:18:49 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Aug 11 13:18:49 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Aug 11 13:18:49 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 11 13:18:49 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Aug 11 13:18:49 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 11 13:18:49 	at org.junit.runners.Suite.runChild(Suite.java:128)
Aug 11 13:18:49 	at org.junit.runners.Suite.runChild(Suite.java:27)
Aug 11 13:18:49 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Aug 11 13:18:49 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Aug 11 13:18:49 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Aug 11 13:18:49 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 11 13:18:49 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Aug 11 13:18:49 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Aug 11 13:18:49 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Aug 11 13:18:49 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 11 13:18:49 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 11 13:18:49 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Aug 11 13:18:49 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Aug 11 13:18:49 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
Aug 11 13:18:49 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Aug 11 13:18:49 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Aug 11 13:18:49 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Aug 11 13:18:49 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
Aug 11 13:18:49 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Aug 11 13:18:49 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Aug 11 13:18:49 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Aug 11 13:18:49 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Aug 11 13:18:49 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Aug 11 13:18:49 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
Aug 11 13:18:49 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
Aug 11 13:18:49 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
Aug 11 13:18:49 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
Aug 11 13:18:49 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
Aug 11 13:18:49 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
Aug 11 13:18:49 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
Aug 11 13:18:49 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
Aug 11 13:18:49 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
Aug 11 13:18:49 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
Aug 11 13:18:49 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Aug 11 13:18:49 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Aug 11 13:18:49 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Aug 11 13:18:49 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Aug 11 13:18:49 Caused by: akka.ConfigurationException: Could not start logger due to [akka.ConfigurationException: Logger specified in config can't be loaded [akka.event.slf4j.Slf4jLogger] due to [akka.event.Logging$LoggerInitializationException: Logger log1-Slf4jLogger did not respond with LoggerInitialized, sent instead [TIMEOUT]]]
Aug 11 13:18:49 	at akka.event.LoggingBus.startDefaultLoggers(Logging.scala:165)
Aug 11 13:18:49 	at akka.event.LoggingBus.startDefaultLoggers$(Logging.scala:108)
Aug 11 13:18:49 	at akka.event.EventStream.startDefaultLoggers(EventStream.scala:26)
Aug 11 13:18:49 	at akka.actor.LocalActorRefProvider.init(ActorRefProvider.scala:609)
Aug 11 13:18:49 	at akka.actor.ActorSystemImpl.liftedTree2$1(ActorSystem.scala:1026)
Aug 11 13:18:49 	at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:1022)
Aug 11 13:18:49 	at akka.actor.ActorSystemImpl._start(ActorSystem.scala:1022)
Aug 11 13:18:49 	at akka.actor.ActorSystemImpl.start(ActorSystem.scala:1045)
Aug 11 13:18:49 	at org.apache.flink.runtime.rpc.akka.RobustActorSystem.create(RobustActorSystem.java:95)
Aug 11 13:18:49 	at org.apache.flink.runtime.rpc.akka.RobustActorSystem.create(RobustActorSystem.java:61)
Aug 11 13:18:49 	at org.apache.flink.runtime.rpc.akka.RobustActorSystem.create(RobustActorSystem.java:53)
Aug 11 13:18:49 	at org.apache.flink.runtime.rpc.akka.AkkaUtils.createActorSystem(AkkaUtils.java:418)
Aug 11 13:18:49 	at org.apache.flink.runtime.rpc.akka.AkkaBootstrapTools.startActorSystem(AkkaBootstrapTools.java:253)
Aug 11 13:18:49 	at org.apache.flink.runtime.rpc.akka.AkkaBootstrapTools.startLocalActorSystem(AkkaBootstrapTools.java:236)
Aug 11 13:18:49 	... 59 more
{code}",,dmvk,mapohl,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 23 11:12:48 UTC 2022,,,,,,,,,,"0|z0ttxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/21 11:27;dmvk;After quick check of the issue, it seems that there is not much we can do except increasing an already generous ""logger-startup-timeout"".

https://github.com/apache/flink/blob/938cf8db710409431902cb6cc2825fec289b8e5b/flink-rpc/flink-rpc-akka/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaUtils.java#L86;;;","13/Aug/21 10:39;trohrmann;I agree. In the logs it looks as if we have a gap of 30s of inactivity. This might explain the triggering of the timeout. I would suggest to increase the logger timeout to 50s as this should usually not have any negative effect.;;;","15/Aug/21 10:26;trohrmann;Hardened via c71d260fb3c4f883e36d71f64ea587607abefcef;;;","23/Jan/22 11:12;mapohl;I'm adding a build failure here without re-opening the ticket (just for the purpose of documenting): [https://dev.azure.com/mapohl/flink/_build/results?buildId=619&view=logs&j=43a593e7-535d-554b-08cc-244368da36b4&t=82d122c0-8bbf-56f3-4c0d-8e3d69630d0f]

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
state bootstrapping fails with new state backend factory stack,FLINK-23728,13394693,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sjwiesman,sjwiesman,sjwiesman,11/Aug/21 14:30,03/May/22 06:58,13/Jul/23 08:12,17/Aug/21 20:21,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,API / State Processor,,,,,1,pull-request-available,,,,"The state processor API should force checkpoint storage to be FileSystemCheckpointStorage, irregardless of what state backend is configured. ",,rgrebennikov,sjwiesman,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26584,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 17 20:21:36 UTC 2021,,,,,,,,,,"0|z0tt94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/21 20:21;sjwiesman;fixed in master: 80bd85d50ec8d3939ab8e826fd7b5dd472516b9f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Network buffer leak when ResultPartition is released (failover),FLINK-23724,13394645,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kevin.cyj,kevin.cyj,kevin.cyj,11/Aug/21 11:57,20/Aug/21 08:53,13/Jul/23 08:12,20/Aug/21 08:53,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,The BufferBuilders in BufferWritingResultPartition are not properly released when ResultPartition is released.,,gaoyunhaii,kevin.cyj,maguowei,pnowojski,tanyuxin,Thesharing,wind_ljy,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22376,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 20 08:53:37 UTC 2021,,,,,,,,,,"0|z0tsyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/21 04:12;kevin.cyj; [~pnowojski] [~akalashnikov], do you have time to take a look? If not, I can prepare a patch.

(I am guessing you are busy with FLIP-183);;;","12/Aug/21 06:37;pnowojski;Thanks [~kevin.cyj], that would be great.;;;","19/Aug/21 08:56;ym;*IMPORTANT* for reference (based on offline discussion) with [~kevin.cyj]

Why pulling {{closeBufferPool}} into the {{fail}} method and why not directly calling BufferWritingResultPartition#close() method in task cancelation (same reason why not put bufferbuilder#close in the BufferWritingResultPartition#releaseInternal method).

Because of potential race condition:

Task calculation thread and task thread may access {{bufferbuilder}} at the same time. In the current solution, bufferbuilder release is done in the task#doRun#finally only (task thread only).;;;","20/Aug/21 03:14;ym;[~kevin.cyj], would you please also backport this to 1.13 as well, since it is marked as affecting 1.13 as well.?;;;","20/Aug/21 08:53;ym;merged commit [{{08f98b7}}|https://github.com/apache/flink/commit/08f98b748a203097064028e0ea066939d63b18f0] into apache:master 

merged commit [{{a7093f1}}|https://github.com/apache/flink/commit/a7093f188dab148ed3df6588fc640f93c48683a4] into apache:release-1.13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SavepointITCase.testStopWithSavepointWithDrainGlobalFailoverIfSavepointAborted failed with ""Stop with savepoint operation could not be completed""",FLINK-23713,13394535,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,xtsong,xtsong,11/Aug/21 02:30,15/Aug/21 15:14,13/Jul/23 08:12,15/Aug/21 15:14,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21851&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=4858

{code}
Aug 10 22:14:12 [ERROR] Tests run: 15, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 65.911 s <<< FAILURE! - in org.apache.flink.test.checkpointing.SavepointITCase
Aug 10 22:14:12 [ERROR] testStopWithSavepointWithDrainGlobalFailoverIfSavepointAborted  Time elapsed: 2.389 s  <<< FAILURE!
Aug 10 22:14:12 java.lang.AssertionError: 
Aug 10 22:14:12 
Aug 10 22:14:12 Expected: a string starting with ""org.apache.flink.util.FlinkException: Inconsistent execution state after stopping with savepoint. At least one execution is still in one of the following states: FAILED. A global fail-over is triggered to recover the job""
Aug 10 22:14:12      but: was ""org.apache.flink.util.FlinkException: Stop with savepoint operation could not be completed.""
Aug 10 22:14:12 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Aug 10 22:14:12 	at org.junit.Assert.assertThat(Assert.java:964)
Aug 10 22:14:12 	at org.junit.Assert.assertThat(Assert.java:930)
Aug 10 22:14:12 	at org.apache.flink.test.checkpointing.SavepointITCase.testStopWithSavepointWithDrainGlobalFailoverIfSavepointAborted(SavepointITCase.java:744)
Aug 10 22:14:12 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 10 22:14:12 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 10 22:14:12 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 10 22:14:12 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 10 22:14:12 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 10 22:14:12 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 10 22:14:12 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 10 22:14:12 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 10 22:14:12 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Aug 10 22:14:12 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Aug 10 22:14:12 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Aug 10 22:14:12 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 10 22:14:12 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Aug 10 22:14:12 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Aug 10 22:14:12 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 10 22:14:12 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 10 22:14:12 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Aug 10 22:14:12 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Aug 10 22:14:12 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
Aug 10 22:14:12 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Aug 10 22:14:12 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Aug 10 22:14:12 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Aug 10 22:14:12 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
Aug 10 22:14:12 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Aug 10 22:14:12 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Aug 10 22:14:12 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Aug 10 22:14:12 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Aug 10 22:14:12 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Aug 10 22:14:12 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
Aug 10 22:14:12 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
Aug 10 22:14:12 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
Aug 10 22:14:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
Aug 10 22:14:12 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
Aug 10 22:14:12 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
Aug 10 22:14:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
Aug 10 22:14:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
Aug 10 22:14:12 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
Aug 10 22:14:12 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
Aug 10 22:14:12 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Aug 10 22:14:12 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Aug 10 22:14:12 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Aug 10 22:14:12 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,pnowojski,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 15 15:14:10 UTC 2021,,,,,,,,,,"0|z0tsa0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/21 03:24;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21932&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=4837;;;","12/Aug/21 06:51;trohrmann;[~pnowojski] could this be caused by the recent work on FLIP-147?;;;","12/Aug/21 07:59;pnowojski;I can not reproduce this locally. I have submitted a PR to rethrow the original exception because ""org.apache.flink.util.FlinkException: Stop with savepoint operation could not be completed."" doesn't tell us much. If I won't be able to reproduce it locally, we will need to wait for a next occurrence on azure after merging my PR. ;;;","12/Aug/21 11:13;pnowojski;I couldn't reproduce it locally after a couple of hours. Merged improvement in the tests as 938cf8d into apache:master. Let's hope for a next failure to give a better message.;;;","13/Aug/21 02:57;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22020&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=4837;;;","13/Aug/21 12:18;pnowojski;After second investigation my previous fix hasn't improved anything in this regard. This exception without any stack trace is being thrown from:
{{org.apache.flink.runtime.scheduler.adaptive.StopWithSavepoint#onLeave}}.

There might be more information in the logs.;;;","13/Aug/21 13:30;trohrmann;The problem is reproducible when enabling the adaptive scheduler. The problem is that the test tests for a specific exception message that is an implementation detail of the default scheduler but not the adaptive scheduler. That's why the test fails.;;;","15/Aug/21 15:14;trohrmann;Fixed via 1848a886d8af1d4bf457d61031edb8fc8565efd0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unregister MetricGroup after shard closes in Kinesis connector,FLINK-23705,13394406,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,elphastori,elphastori,elphastori,10/Aug/21 09:38,18/Aug/21 16:59,13/Jul/23 08:12,18/Aug/21 13:36,1.10.0,1.10.1,1.10.2,1.10.3,1.11.0,1.11.1,1.11.2,1.11.3,1.11.4,1.12.0,1.12.1,1.12.2,1.12.3,1.12.4,1.12.5,1.13.0,1.13.1,1.13.2,1.6.3,1.6.4,1.7.2,1.8.0,1.8.1,1.8.2,1.8.3,1.9.0,1.9.1,1.9.2,1.9.3,1.13.3,1.14.0,,,,Connectors / Kinesis,,,,,0,pull-request-available,,,,"Kinesis connector continues to report metrics for closed shards after resharding leading to incorrect millisBehindLatest metric reported on the Flink dashboard.

This is results in incorrect aggregated metrics for the entire stream even after restarting.",,dannycranmer,elphastori,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 11 07:47:52 UTC 2021,,,,,,,,,,"0|z0trhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/21 07:47;dannycranmer;Thanks [~elphastori]. The fix for this will be ported from [here|https://github.com/awslabs/amazon-kinesis-connector-flink/pull/43];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-27 sources are not generating LatencyMarkers,FLINK-23704,13394403,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,pnowojski,pnowojski,10/Aug/21 09:31,15/Dec/21 01:44,13/Jul/23 08:12,15/Oct/21 08:38,1.12.5,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.3,1.15.0,,API / DataStream,Runtime / Task,,,,0,pull-request-available,,,,Currently {{LatencyMarker}} is created only in {{StreamSource.LatencyMarksEmitter#LatencyMarksEmitter}}. FLIP-27 sources are never emitting it.,,nicholasjiang,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 15 08:38:06 UTC 2021,,,,,,,,,,"0|z0trgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/21 16:08;nicholasjiang;[~pnowojski], I have pushed the PR for the LatencyMarkerEmitter in SourceOperator. Please help to review this PR.;;;","15/Oct/21 08:38;pnowojski;Merged to master as fe59485be2a^^^^^..fe59485be2a
Merged to release-1.14 as 32d93097bb5^^^^^..32d93097bb5
Merged to release-1.13 as f1094a92a83^^^^..f1094a92a83
Merged to release-1.12 as 534afd637e4^^^^..534afd637e4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix Error translation in ""Builtin Watermark Generators"" page",FLINK-23701,13394398,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Aiden Gong,Aiden Gong,Aiden Gong,10/Aug/21 09:23,10/Aug/21 14:50,13/Jul/23 08:12,10/Aug/21 14:50,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Documentation,Documentation / Training,,,,0,pull-request-available,,,,"Error translation in ""Builtin Watermark Generators"" page [https://ci.apache.org/projects/flink/flink-docs-master/zh/docs/dev/datastream/event-time/built_in/].

The error translation is located by follow screenshot.
  ",,Aiden Gong,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/21 09:24;Aiden Gong;企业微信截图_16285874228277.png;https://issues.apache.org/jira/secure/attachment/13031706/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_16285874228277.png","10/Aug/21 09:24;Aiden Gong;企业微信截图_16285874495585.png;https://issues.apache.org/jira/secure/attachment/13031707/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_16285874495585.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 10 14:50:18 UTC 2021,,,,,,,,,,"0|z0trfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/21 09:29;Aiden Gong;Hi,[~jark]. Please assigne to me,I have fixed.Thanks!


 ;;;","10/Aug/21 14:50;jark;Fixed in master: ffc9637e2c1f42d6adb825148a0213d34bead8d3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RMQSourceTest.testRedeliveredSessionIDsAck fails on azure,FLINK-23696,13394312,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,cmick,xtsong,xtsong,10/Aug/21 03:16,15/Dec/21 01:44,13/Jul/23 08:12,10/Nov/21 15:22,1.12.5,1.13.2,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Connectors/ RabbitMQ,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21792&view=logs&j=e1276d0f-df12-55ec-86b5-c0ad597d83c9&t=906e9244-f3be-5604-1979-e767c8a6f6d9&l=13297

{code}
Aug 10 01:15:35 [ERROR] Tests run: 16, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 3.181 s <<< FAILURE! - in org.apache.flink.streaming.connectors.rabbitmq.RMQSourceTest
Aug 10 01:15:35 [ERROR] testRedeliveredSessionIDsAck(org.apache.flink.streaming.connectors.rabbitmq.RMQSourceTest)  Time elapsed: 0.269 s  <<< FAILURE!
Aug 10 01:15:35 java.lang.AssertionError: expected:<25> but was:<27>
Aug 10 01:15:35 	at org.junit.Assert.fail(Assert.java:88)
Aug 10 01:15:35 	at org.junit.Assert.failNotEquals(Assert.java:834)
Aug 10 01:15:35 	at org.junit.Assert.assertEquals(Assert.java:645)
Aug 10 01:15:35 	at org.junit.Assert.assertEquals(Assert.java:631)
Aug 10 01:15:35 	at org.apache.flink.streaming.connectors.rabbitmq.RMQSourceTest.testRedeliveredSessionIDsAck(RMQSourceTest.java:407)
Aug 10 01:15:35 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 10 01:15:35 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 10 01:15:35 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 10 01:15:35 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 10 01:15:35 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Aug 10 01:15:35 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 10 01:15:35 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Aug 10 01:15:35 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 10 01:15:35 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Aug 10 01:15:35 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Aug 10 01:15:35 	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
Aug 10 01:15:35 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Aug 10 01:15:35 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
Aug 10 01:15:35 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
Aug 10 01:15:35 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
Aug 10 01:15:35 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Aug 10 01:15:35 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Aug 10 01:15:35 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Aug 10 01:15:35 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Aug 10 01:15:35 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Aug 10 01:15:35 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
Aug 10 01:15:35 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Aug 10 01:15:35 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Aug 10 01:15:35 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Aug 10 01:15:35 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Aug 10 01:15:35 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Aug 10 01:15:35 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Aug 10 01:15:35 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Aug 10 01:15:35 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418){code}",,cmick,dianfu,dwysakowicz,fpaul,trohrmann,xtsong,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 10 15:22:52 UTC 2021,,,,,,,,,,"0|z0tqwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/21 06:34;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23803&view=logs&j=e1276d0f-df12-55ec-86b5-c0ad597d83c9&t=906e9244-f3be-5604-1979-e767c8a6f6d9&l=13325;;;","09/Sep/21 12:21;dianfu;https://dev.azure.com/dianfu/Flink/_build/results?buildId=527&view=logs&j=dafbab6d-4616-5d7b-ee37-3c54e4828fd7&t=e204f081-e6cd-5c04-4f4c-919639b63be9;;;","10/Sep/21 06:30;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23876&view=logs&j=e1276d0f-df12-55ec-86b5-c0ad597d83c9&t=66648bdf-9af9-503d-c9a7-11f783a19935&l=13913;;;","08/Oct/21 09:46;trohrmann;Another instance: https://dev.azure.com/ververica-dev/daplatform-flink/_build/results?buildId=1552&view=logs&j=dafbab6d-4616-5d7b-ee37-3c54e4828fd7&t=e204f081-e6cd-5c04-4f4c-919639b63be9&l=13978;;;","08/Oct/21 09:47;trohrmann;Another instance: https://dev.azure.com/ververica-dev/daplatform-flink/_build/results?buildId=1551&view=logs&j=dafbab6d-4616-5d7b-ee37-3c54e4828fd7&t=e204f081-e6cd-5c04-4f4c-919639b63be9&l=13979;;;","11/Oct/21 06:25;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24932&view=logs&j=e1276d0f-df12-55ec-86b5-c0ad597d83c9&t=906e9244-f3be-5604-1979-e767c8a6f6d9&l=13274;;;","13/Oct/21 04:44;ym;another case:

https://dev.azure.com/mymeiyuan/Flink/_build/results?buildId=342&view=logs&j=dafbab6d-4616-5d7b-ee37-3c54e4828fd7&t=e204f081-e6cd-5c04-4f4c-919639b63be9;;;","13/Oct/21 06:38;fpaul;[~cmick] do you have time to take a look?;;;","05/Nov/21 10:58;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25986&view=logs&j=e1276d0f-df12-55ec-86b5-c0ad597d83c9&t=66648bdf-9af9-503d-c9a7-11f783a19935&l=13978;;;","05/Nov/21 10:58;trohrmann;cc [~arvid] this test seems to be failing repeatedly. Can we make a plan how to handle the problem?;;;","05/Nov/21 11:01;fpaul;[~cmick] can you spare some time to help us debug it?;;;","08/Nov/21 00:31;cmick;Hi [~fpaul] , sorry for the delay. Yes. I think I know what causes the issue. It's a race condition between the message count checks and the message producer. I managed to reproduce it locally by adding an Thread.sleep. Moving the problematic asserts into a synchronized code block fixes the issue. Since, it's a minor change I will prepare a PR shortly.;;;","10/Nov/21 15:22;arvid;Merged into master as 4eaeb92691fc60b1680ffa7450a534a4bcf08533, into 1.14 as 71891823f2c82c12952c0dee52d6bc25c6e56fc9, and into 1.13 as 89acb9a5b36a36b4397310c935f6537a37bec885.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DispatcherFailoverITCase.testRecoverFromCheckpointAfterJobGraphRemovalOfTerminatedJobFailed fails due to timeout,FLINK-23695,13394303,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,xtsong,xtsong,10/Aug/21 01:48,11/Aug/21 12:54,13/Jul/23 08:12,11/Aug/21 12:54,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21790&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9573

{code}
Aug 09 23:09:32 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 60.331 s <<< FAILURE! - in org.apache.flink.runtime.dispatcher.DispatcherFailoverITCase
Aug 09 23:09:32 [ERROR] testRecoverFromCheckpointAfterJobGraphRemovalOfTerminatedJobFailed(org.apache.flink.runtime.dispatcher.DispatcherFailoverITCase)  Time elapsed: 60.265 s  <<< ERROR!
Aug 09 23:09:32 java.util.concurrent.TimeoutException: Condition was not met in given timeout.
Aug 09 23:09:32 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:154)
Aug 09 23:09:32 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:132)
Aug 09 23:09:32 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:124)
Aug 09 23:09:32 	at org.apache.flink.runtime.dispatcher.AbstractDispatcherTest.awaitStatus(AbstractDispatcherTest.java:81)
Aug 09 23:09:32 	at org.apache.flink.runtime.dispatcher.DispatcherFailoverITCase.testRecoverFromCheckpointAfterJobGraphRemovalOfTerminatedJobFailed(DispatcherFailoverITCase.java:137)
Aug 09 23:09:32 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 09 23:09:32 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 09 23:09:32 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 09 23:09:32 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 09 23:09:32 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 09 23:09:32 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 09 23:09:32 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 09 23:09:32 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 09 23:09:32 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Aug 09 23:09:32 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Aug 09 23:09:32 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Aug 09 23:09:32 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Aug 09 23:09:32 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$CloseableStatement.evaluate(TestingFatalErrorHandlerResource.java:91)
Aug 09 23:09:32 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$CloseableStatement.access$200(TestingFatalErrorHandlerResource.java:83)
Aug 09 23:09:32 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$1.evaluate(TestingFatalErrorHandlerResource.java:55)
Aug 09 23:09:32 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Aug 09 23:09:32 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Aug 09 23:09:32 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 09 23:09:32 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Aug 09 23:09:32 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Aug 09 23:09:32 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Aug 09 23:09:32 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Aug 09 23:09:32 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Aug 09 23:09:32 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Aug 09 23:09:32 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Aug 09 23:09:32 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 09 23:09:32 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Aug 09 23:09:32 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Aug 09 23:09:32 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Aug 09 23:09:32 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 09 23:09:32 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 09 23:09:32 	at org.junit.runners.Suite.runChild(Suite.java:128)
Aug 09 23:09:32 	at org.junit.runners.Suite.runChild(Suite.java:27)
Aug 09 23:09:32 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Aug 09 23:09:32 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Aug 09 23:09:32 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Aug 09 23:09:32 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 09 23:09:32 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Aug 09 23:09:32 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 09 23:09:32 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 09 23:09:32 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
Aug 09 23:09:32 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
Aug 09 23:09:32 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
Aug 09 23:09:32 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
Aug 09 23:09:32 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
Aug 09 23:09:32 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
Aug 09 23:09:32 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Aug 09 23:09:32 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Aug 09 23:09:32 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Aug 09 23:09:32 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,dmvk,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 11 12:54:43 UTC 2021,,,,,,,,,,"0|z0tqug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/21 07:06;trohrmann;[~dmvk] could you take a look at this problem?;;;","10/Aug/21 07:12;dmvk;Yes, I'll try to fix this today.;;;","11/Aug/21 02:24;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21851&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9858;;;","11/Aug/21 02:25;xtsong;Thanks [~dmvk], you're assigned.;;;","11/Aug/21 12:54;trohrmann;Fixed via 6ded624e176152fa1676c8ac6242df994d9cde46;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"KafkaSource metric ""commitsSucceeded"" should count per-commit instead of per-partition",FLINK-23686,13394144,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,09/Aug/21 07:11,31/Aug/21 02:25,13/Jul/23 08:12,31/Aug/21 02:25,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"Currently if a successful offset commit includes multiple topic partition (let's say 4), the counter will increase by 4 instead of 1",,becket_qin,renqs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 31 02:25:29 UTC 2021,,,,,,,,,,"0|z0tpv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/21 02:25;becket_qin;Merged to master / release-1.14: 62931a1665e6a6976d088ed49375f9fdf00229d9

Cherry picked to 1.13: 5ce61a31ff8a184ce3f8457471ffc6f5f4439b5d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamTaskTimerTest.testOpenCloseAndTimestamps fails on azure,FLINK-23680,13394118,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,xtsong,xtsong,09/Aug/21 03:22,17/Aug/21 04:09,13/Jul/23 08:12,17/Aug/21 04:09,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Task,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21728&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=8985

{code}
Aug 07 22:58:40 [ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.128 s <<< FAILURE! - in org.apache.flink.streaming.runtime.operators.StreamTaskTimerTest
Aug 07 22:58:40 [ERROR] testOpenCloseAndTimestamps(org.apache.flink.streaming.runtime.operators.StreamTaskTimerTest)  Time elapsed: 0.041 s  <<< FAILURE!
Aug 07 22:58:40 java.lang.AssertionError: expected:<2> but was:<1>
Aug 07 22:58:40 	at org.junit.Assert.fail(Assert.java:89)
Aug 07 22:58:40 	at org.junit.Assert.failNotEquals(Assert.java:835)
Aug 07 22:58:40 	at org.junit.Assert.assertEquals(Assert.java:647)
Aug 07 22:58:40 	at org.junit.Assert.assertEquals(Assert.java:633)
Aug 07 22:58:40 	at org.apache.flink.streaming.runtime.operators.StreamTaskTimerTest.testOpenCloseAndTimestamps(StreamTaskTimerTest.java:77)
Aug 07 22:58:40 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 07 22:58:40 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 07 22:58:40 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 07 22:58:40 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
Aug 07 22:58:40 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 07 22:58:40 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 07 22:58:40 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 07 22:58:40 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 07 22:58:40 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Aug 07 22:58:40 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Aug 07 22:58:40 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Aug 07 22:58:40 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Aug 07 22:58:40 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 07 22:58:40 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Aug 07 22:58:40 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Aug 07 22:58:40 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Aug 07 22:58:40 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Aug 07 22:58:40 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Aug 07 22:58:40 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Aug 07 22:58:40 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Aug 07 22:58:40 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 07 22:58:40 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Aug 07 22:58:40 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 07 22:58:40 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 07 22:58:40 	at org.junit.runners.Suite.runChild(Suite.java:128)
Aug 07 22:58:40 	at org.junit.runners.Suite.runChild(Suite.java:27)
Aug 07 22:58:40 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Aug 07 22:58:40 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Aug 07 22:58:40 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Aug 07 22:58:40 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 07 22:58:40 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Aug 07 22:58:40 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 07 22:58:40 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 07 22:58:40 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
Aug 07 22:58:40 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
Aug 07 22:58:40 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
Aug 07 22:58:40 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
Aug 07 22:58:40 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
Aug 07 22:58:40 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
Aug 07 22:58:40 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Aug 07 22:58:40 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Aug 07 22:58:40 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Aug 07 22:58:40 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,pnowojski,xtsong,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23452,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 17 04:09:01 UTC 2021,,,,,,,,,,"0|z0tppc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/21 03:23;xtsong;[~akalashnikov], this seems to be related to FLINK-23452. Could you please take a look?;;;","17/Aug/21 04:09;ym;merged commit [{{ff416c6}}|https://github.com/apache/flink/commit/ff416c644f6f49d14838b0bc664a1827707f5faa] into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSinkITCase.testWriteRecordsToKafkaWithExactlyOnceGuarantee fails on azure,FLINK-23678,13394115,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fpaul,xtsong,xtsong,09/Aug/21 02:56,17/Nov/21 15:21,13/Jul/23 08:12,11/Aug/21 20:11,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Connectors / Kafka,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21711&view=logs&j=b0097207-033c-5d9a-b48c-6d4796fbe60d&t=8338a7d2-16f7-52e5-f576-4b7b3071eb3d&l=6946

{code}
Aug 07 00:12:18 [ERROR] Tests run: 7, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 67.431 s <<< FAILURE! - in org.apache.flink.streaming.connectors.kafka.sink.KafkaSinkITCase
Aug 07 00:12:18 [ERROR] testWriteRecordsToKafkaWithExactlyOnceGuarantee(org.apache.flink.streaming.connectors.kafka.sink.KafkaSinkITCase)  Time elapsed: 7.001 s  <<< FAILURE!
Aug 07 00:12:18 java.lang.AssertionError: expected:<407799> but was:<407798>
Aug 07 00:12:18 	at org.junit.Assert.fail(Assert.java:89)
Aug 07 00:12:18 	at org.junit.Assert.failNotEquals(Assert.java:835)
Aug 07 00:12:18 	at org.junit.Assert.assertEquals(Assert.java:647)
Aug 07 00:12:18 	at org.junit.Assert.assertEquals(Assert.java:633)
Aug 07 00:12:18 	at org.apache.flink.streaming.connectors.kafka.sink.KafkaSinkITCase.writeRecordsToKafka(KafkaSinkITCase.java:334)
Aug 07 00:12:18 	at org.apache.flink.streaming.connectors.kafka.sink.KafkaSinkITCase.testWriteRecordsToKafkaWithExactlyOnceGuarantee(KafkaSinkITCase.java:173)
Aug 07 00:12:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 07 00:12:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 07 00:12:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 07 00:12:18 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 07 00:12:18 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 07 00:12:18 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 07 00:12:18 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 07 00:12:18 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 07 00:12:18 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Aug 07 00:12:18 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Aug 07 00:12:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Aug 07 00:12:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Aug 07 00:12:18 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Aug 07 00:12:18 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Aug 07 00:12:18 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 07 00:12:18 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Aug 07 00:12:18 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Aug 07 00:12:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Aug 07 00:12:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Aug 07 00:12:18 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Aug 07 00:12:18 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Aug 07 00:12:18 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Aug 07 00:12:18 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 07 00:12:18 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Aug 07 00:12:18 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
Aug 07 00:12:18 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Aug 07 00:12:18 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 07 00:12:18 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 07 00:12:18 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Aug 07 00:12:18 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Aug 07 00:12:18 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Aug 07 00:12:18 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Aug 07 00:12:18 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Aug 07 00:12:18 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Aug 07 00:12:18 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Aug 07 00:12:18 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,pnowojski,xtsong,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Aug/21 05:40;arvid;extracted.log;https://issues.apache.org/jira/secure/attachment/13031828/extracted.log",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 12 06:41:21 UTC 2021,,,,,,,,,,"0|z0tpoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/21 03:02;xtsong;[~fpaul], could you help take a look into this?

I'm making this a blocker for now, since the exactly-once consistency seems to be broken. Please downgrade if that is not the case.;;;","09/Aug/21 03:17;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21728&view=logs&j=b0097207-033c-5d9a-b48c-6d4796fbe60d&t=8338a7d2-16f7-52e5-f576-4b7b3071eb3d&l=6946;;;","09/Aug/21 03:36;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21738&view=logs&j=b0097207-033c-5d9a-b48c-6d4796fbe60d&t=8338a7d2-16f7-52e5-f576-4b7b3071eb3d&l=6946;;;","09/Aug/21 17:53;pnowojski;https://dev.azure.com/pnowojski/Flink/_build/results?buildId=451&view=logs&j=d543d572-9428-5803-a30c-e8e09bf70915&t=4e4199a3-fbbb-5d5b-a2be-802955ffb013;;;","10/Aug/21 01:46;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21790&view=logs&j=b0097207-033c-5d9a-b48c-6d4796fbe60d&t=8338a7d2-16f7-52e5-f576-4b7b3071eb3d&l=6946;;;","11/Aug/21 02:22;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21851&view=logs&j=b0097207-033c-5d9a-b48c-6d4796fbe60d&t=8338a7d2-16f7-52e5-f576-4b7b3071eb3d&l=6968;;;","11/Aug/21 02:23;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21851&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=576aba0a-d787-51b6-6a92-cf233f360582&l=7296;;;","11/Aug/21 03:11;ym;[https://dev.azure.com/mymeiyuan/Flink/_build/results?buildId=311&view=logs&j=d543d572-9428-5803-a30c-e8e09bf70915&t=4e4199a3-fbbb-5d5b-a2be-802955ffb013]

 

fails quite frequently  (11 fails out of 28 runs);;;","11/Aug/21 20:11;arvid;Merged into master as 58ff344e5e5fdd397e0f9276561b746ddabbadea.;;;","12/Aug/21 05:41;arvid;The new errors are related to FLINK-23408. We do not see a notifyCheckpointCompleted for checkpoint 2 (grep for notifyCheckpoint) even though we see Completed checkpoint 2. [~pnowojski] PTAL
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21936&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c [^extracted.log] ;;;","12/Aug/21 05:46;arvid;Ignoring test in ac42bb3d146 on master.;;;","12/Aug/21 06:41;pnowojski;[~arvid] have you enabled {{ExecutionCheckpointingOptions#ENABLE_CHECKPOINTS_AFTER_TASKS_FINISH}}? As of now it's off by default.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In many Chinese pages, the prompt or warning text block does not take effect",FLINK-23673,13394031,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hapihu,hapihu,hapihu,07/Aug/21 17:05,09/Aug/21 02:35,13/Jul/23 08:12,09/Aug/21 02:35,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Documentation,,,,,0,pull-request-available,,,,"In many Chinese pages, the prompt or warning text block does not take effect.

[https://ci.apache.org/projects/flink/flink-docs-master/zh/docs/libs/cep/]

 

 

 

The '\{% info 提示 %}' should be modified as follows:
{code:java}
{{< hint info >}}

{{< /hint >}}
{code}
 

The '\{% warn 注意 %}' should be modified as follows:
{code:java}
{{< hint warning >}}

{{< /hint >}}{code}
 ",,hapihu,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/21 17:05;hapihu;image-20210808005849891.png;https://issues.apache.org/jira/secure/attachment/13031624/image-20210808005849891.png","07/Aug/21 18:35;hapihu;image-20210808023456047.png;https://issues.apache.org/jira/secure/attachment/13031627/image-20210808023456047.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 09 02:35:34 UTC 2021,,,,,,,,,,"0|z0tp60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Aug/21 17:54;hapihu;The files needs to be modified are as follows:
{code:java}
docs/content.zh/docs/deployment/filesystems/s3.md
docs/content.zh/docs/dev/datastream/fault-tolerance/serialization/schema_evolution.md
docs/content.zh/docs/dev/table/concepts/versioned_tables.md
docs/content.zh/docs/dev/table/sql/queries/match_recognize.md
docs/content.zh/docs/flinkDev/building.md
docs/content.zh/docs/libs/cep.md
docs/content.zh/docs/try-flink/datastream.md
{code};;;","07/Aug/21 18:19;hapihu;Hi  [~jark]
Excuse me for taking up your time.
I created pull request for this issue.
Would you like to review it for me.
If there is any problem, please inform me in time.
I will actively revise it in time.
Thank you very much!;;;","09/Aug/21 02:35;jark;Fixed in master: 019111dc09303fd2398789015dcb1678d15f9463;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Scala formatting checks to training exercises,FLINK-23670,13393890,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nkruber,nkruber,nkruber-old,06/Aug/21 12:13,11/Oct/21 21:22,13/Jul/23 08:12,01/Sep/21 08:26,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Documentation / Training / Exercises,,,,,0,pull-request-available,,,,"Currently, there are no formatting checks for Scala code in the training exercises. We should employ the same checks that the main Flink project is using.",,nkruber,nkruber-old,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 01 08:26:01 UTC 2021,,,,,,,,,,"0|z0toao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/21 08:26;nkruber;Fixed on master via:
- bd4a088fd3aa23b13eea5e75307b26a15afb3608
- a22bc8f088b6f6794f931a1f6e7bf961729ad66b
- 177cadf30fb0f73341f89b3db1e2fc13dcedc2dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid using Scala >= 2.12.8 in Flink Training exercises,FLINK-23669,13393886,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nkruber,nkruber,nkruber-old,06/Aug/21 11:55,11/Oct/21 21:22,13/Jul/23 08:12,06/Aug/21 13:39,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Documentation / Training / Exercises,,,,,0,pull-request-available,,,,"The current IDE setup instructions of the Flink training exercises do not mention a specific Scala SDK to set up. For compatibility reasons described in https://ci.apache.org/projects/flink/flink-docs-stable/docs/dev/datastream/project-configuration/#scala-versions, we should also not use 2.12.8 and up.",,nkruber-old,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 06 13:39:56 UTC 2021,,,,,,,,,,"0|z0to9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/21 13:39;nkruber-old;Merged on master via 1704fd959122aa197b48803441338c1fb0470091;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix training exercises IDE setup description for Scala,FLINK-23667,13393883,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,nkruber,nkruber,nkruber-old,06/Aug/21 11:45,11/Oct/21 21:22,13/Jul/23 08:12,06/Aug/21 13:39,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Documentation / Training / Exercises,,,,,0,pull-request-available,,,,"If you follow the training exercises instructions to set up your IDE with code formatting and the Save Actions plugin while having Scala enabled, it will completely reformat your Scala code files instead of keeping them as is.

The instructions should be updated to match the ones used for the Flink main project.",,nkruber-old,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 06 13:39:19 UTC 2021,,,,,,,,,,"0|z0to94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/21 11:50;nkruber-old;The same actually also applies to the new {{CONTRIBUTING.md}} file;;;","06/Aug/21 13:39;nkruber-old;Merged on master via 08f5bf4d93d241f58d67148dc3e054a2325b64a0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add RocksDB packages to parent-first classloading patterns.,FLINK-23649,13393708,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,sewen,sewen,05/Aug/21 13:02,17/Aug/21 02:58,13/Jul/23 08:12,17/Aug/21 02:58,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,API / Core,,,,,0,pull-request-available,,,,"RocksDB classes are currently loaded child-first.

Because of that, it can happen that the RocksDB library is attempted to be loaded multiple times (by different classloaders).

That is prevented by JNI and results in an error as reported in this mail for example
https://lists.apache.org/x/thread.html/rbc3ca24efe13b25e802af9739a6877276503363ffbdc5914ffdad7be@%3Cuser.flink.apache.org%3E

We should prevent accidental repeated loading of RocksDB, because we rely on the fact that only one DB is created per task.",,jingzhang,sewen,Thesharing,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 17 02:58:12 UTC 2021,,,,,,,,,,"0|z0tn68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/21 02:58;yunta;Merged in master:
d57d9f84d84408547e00f3a715e8f05b4408af11;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drop StreamTwoInputProcessor in favour of StreamMultipleInputProcessor,FLINK-23648,13393707,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pnowojski,pnowojski,pnowojski,05/Aug/21 13:01,09/Aug/21 07:38,13/Jul/23 08:12,09/Aug/21 07:38,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Task,,,,,0,pull-request-available,,,,"StreamTwoInputProcessor is less efficient and can be simply replaced with StreamMultipleInputProcessor. This also solves a performance problem for FLINK-23541, where initial version was causing a performance regression in two input processor, while multiple input was working fine. Hence instead of optimising StreamTwoInputProcessor, it's just simpler to drop it. ",,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23541,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 09 07:38:32 UTC 2021,,,,,,,,,,"0|z0tn60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/21 07:38;pnowojski;merged commit fad1bc5 into apache:master now;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnalignedCheckpointStressITCase crashed on azure,FLINK-23647,13393701,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,roman,roman,05/Aug/21 12:20,15/Dec/21 01:44,13/Jul/23 08:12,28/Oct/21 13:57,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Runtime / Network,Tests,,,,0,pull-request-available,stale-assigned,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21539&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=4855

When testing DFS changelog implementation in FLINK-23279 and enabling it for all tests,
UnalignedCheckpointStressITCase crashed with the following exception
{code}
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 18.433 s <<< FAILURE! - in org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase
[ERROR] runStressTest(org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase)  Time elapsed: 17.663 s  <<< ERROR!
java.io.UncheckedIOException: java.nio.file.NoSuchFileException: /tmp/junit7860347244680665820/435237 d57439f2ceadfedba74dadd6fa/chk-16
   at java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:88)
   at java.nio.file.FileTreeIterator.hasNext(FileTreeIterator.java:104)
   at java.util.Iterator.forEachRemaining(Iterator.java:115)
   at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
   at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
   at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
   at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
   at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
   at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546)
   at org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase.discoverRetainedCheckpoint(UnalignedCheckpointStressITCase.java:288)
   at org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase.runAndTakeExternalCheckpoint(UnalignedCheckpointStressITCase.java:261)
   at org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase.runStressTest(UnalignedCheckpointStressITCase.java:157)
   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
   at java.lang.reflect.Method.invoke(Method.java:498)
   at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
   at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
   at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
   at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
   at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
   at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
   at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
   at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
   at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
   at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
   at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
   at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
   at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
   at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
   at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
   at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
   at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
   at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
   at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
   at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
   at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
   at org.junit.rules.RunRules.evaluate(RunRules.java:20)
   at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
   at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
   at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
   at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
   at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
   at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
   at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java :384)
   at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
   at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
   at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: java.nio.file.NoSuchFileException: /tmp/junit7860347244680665820/435237d57439f2ceadfedba74 dadd6fa/chk-16
   at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
   at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
   at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
   at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
   at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
   at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
   at java.nio.file.Files.readAttributes(Files.java:1737)
   at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219)
   at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276)
   at java.nio.file.FileTreeWalker.next(FileTreeWalker.java:372)
   at java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:84)
{code}
 
The referred checkpoint 16 was aborted and scheduled for deletion.
But the test does not wait for it to complete and proceeds to file listing.

I think this problem is also present in UnalignedCheckpointRescaleITCase (FLINK-22197) and probably in CoordinatedSourceRescaleITCase(FLINK-23577).

Patch to demonstrate it: https://github.com/rkhachatryan/flink/tree/f23647-demo
Corresponding [failure|https://dev.azure.com/khachatryanroman/flink/_build/results?buildId=1039&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=9a028d19-6c4b-5a4e-d378-03fca149d0b1&l=4870]
",,pnowojski,roman,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24938,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 28 13:57:56 UTC 2021,,,,,,,,,,"0|z0tn4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/21 17:58;roman;[~pnowojski], [~arvid] I remember you also encountered a similar issue.
Does the reasoning in the ticket make sense to you?
If so, do you have any idea of how to fix it?

IMO, the desired behavior is to wait for all checkpoints to be discarded before reporting job termination to the client.

However, CheckpointCleaner currently submits actions to JobManagerSharedServices.scheduledExecutorService; which is terminated with the last job (i.e. minicluster shutdown); and it still doesn't wait for all tasks termination on shutdown.
A simple workaround would be to wait in CheckpointsCleaner until numberOfCheckpointsToClean become 0 on termination. (and wait for executor tasks termination).

cc: [~trohrmann@apache.org];;;","05/Aug/21 18:27;arvid;I'm assuming it's more an implementation issue than a logic issue: 
- we are looking for the latest {{chk}} directory with a metadata file in it
- concurrently a {{chk}} directory is being deleted

since the directory under deletion will not contain a metadata file, it should be fine in theory. In practice, we need to use more atomic checks to avoid concurrency issues.
So rather than

{noformat}
Files.find(checkpointDir.toPath(), 2, this::isCompletedCheckpoint)
{noformat}

we should use the old-fashioned way that operators on snapshots
{noformat}
Arrays.stream(checkpointDir.listFiles()).filter(this::isCompletedCheckpoint)
{noformat}

isCompletedCheckpoint must deal with the given file being deleted at any time in-between.;;;","05/Aug/21 23:09;roman;I think that's another way to look at it :) 

I still think Flink should wait for checkpoints to be discarded but maybe test instability can be improved too.

 
{quote}isCompletedCheckpoint must deal with the given file being deleted at any time in-between.
{quote}
Do you mean catching NoSuchFileException as well?

And I'm not sure that listFiles itself won't fail; I think it should list each folder attributes (to make sure 'x' is set - where stream() fails currently); and then list files in a potentially deleted folder.

 

Also listing the files while deleting may fail deletion; but that shouldn't be an issue as the folder will removed by Junit Rule.

 ;;;","06/Aug/21 05:53;arvid;`listFiles` is an atomic operation afaik. The issue in your stacktrace shows that the error happens when the FileWalker assumes that the respective listed files can be queried for attributes. However, that doesn't hold in general (not sure why it's implemented in this way; it's a JDK bug in my book).

Afaik the old File API is sufficient. {{isDirectory}} doesn't seem to fail if the directory is deleted. In the same way, {{exists}} does not as well.;;;","06/Aug/21 08:52;roman;> `listFiles` is an atomic operation afaik.

I doubt it as listFiles needs to traverse an arbitrate number of files and directories, e.g. in case listFiles(""/""). The OS would have to provide some lock to prevent any FS changes then.

 

> Afaik the old File API is sufficient. {{isDirectory}} doesn't seem to fail if the directory is deleted.

The first thing isCompletedCheckpoint does is checking the attributes which can fail (or do you mean something else?). 

And exists wouldn't help as exists+readAttr is not atomic.;;;","06/Aug/21 10:01;arvid;Okay let us take the ticket, it takes too long to explain for a 5 min fix.;;;","06/Aug/21 10:23;roman;Thanks. I would also be fine to provide a fix once we agree on it.

The production code change is definetly not a 5 minute fix if we choose that way.;;;","10/Aug/21 12:54;trohrmann;Conceptually it would be nice if Flink only reported a job result to the client after the artefacts Flink leaves behind have converged (differently said that Flink leaves an immutable set of artefacts). I think that this could improve Flink operations since users can rely on the available artefacts and don't have to fear that something gets deleted at a later point in time.;;;","10/Aug/21 19:11;arvid;Yes, it would also avoid ugly hacks like https://github.com/AHeise/flink/blob/master/flink-tests/src/test/java/org/apache/flink/test/checkpointing/UnalignedCheckpointTestBase.java#L212-L212 .
However, when I just waited for a proper shutdown of the minicluster, it added 1-2s to each ITCase. So we need to be aware that some ITCases take 3x the time then.;;;","10/Aug/21 19:21;arvid;Here is my solution to solve the test instability in code https://github.com/apache/flink/commit/08b29798548ed6ee915113b2e000923d5d275259 with some explanations.;;;","16/Aug/21 17:28;roman;{quote}when I just waited for a proper shutdown of the minicluster, it added 1-2s to each ITCase.{quote}

From the code, it seems the delay was caused by GC. But do we have to wait for it?


{quote}Here is my solution to solve the test instability in code{quote}
 I think it's still a bit fragile:
 1. It assumes that the latest folder with _metadata file won't be removed; but I think it can be removed if adding to checkpointStore fails
 2. The assumption is easy to violate - by reading attrs of some previous checkpoint (which can be subsumed concurrently)
 3. I couldn't find any docs about the atomicity of listFiles - could you list any reference? 
 4. Reading the files while they are being deleted can prevent deletion
;;;","16/Aug/21 18:39;arvid;> I think it's still a bit fragile:
> 1. It assumes that the latest folder with _metadata file won't be removed; but I think it can be removed if adding to checkpointStore fails
I haven't thought about that one - or rather I didn't know it. When does it happen?
> 2. The assumption is easy to violate - by reading attrs of some previous checkpoint (which can be subsumed concurrently)
There is no harm in checking a file on existance - even if subsumed.
> 3. I couldn't find any docs about the atomicity of listFiles - could you list any reference?
The API is not allowing any exceptions at all. If you check the implementation, you can also see that no exceptions are thrown http://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/java.base/unix/native/libjava/UnixFileSystem_md.c#l310
> 4. Reading the files while they are being deleted can prevent deletion
Good thing that we are not reading them then unless we are sure that this is the correct checkpoint directory.;;;","16/Aug/21 19:12;roman;> 1. It assumes that the latest folder with _metadata file won't be removed; but I think it can be removed if adding to checkpointStore fails
 > I haven't thought about that one - or rather I didn't know it. When does it happen?

[Here|https://github.com/apache/flink/blob/d326b0574a373bd5eef63a44261f8762709265f8/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L1233] and (theorhetically) [here|https://github.com/apache/flink/blob/d326b0574a373bd5eef63a44261f8762709265f8/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L1207%C2%A0]

> 2. The assumption is easy to violate - by reading attrs of some previous checkpoint (which can be subsumed concurrently)
 > There is no harm in checking a file on existance - even if subsumed.
 I meant that if it's checked, then subsumed, then accessed; then there will be an exception again
  
> 3.
 > The API is not allowing any exceptions at all.
 You're right...but it says in case of any I/O error it returns null. So we need to check for null and retry.
  
> 4. Reading the files while they are being deleted can prevent deletion
 > Good thing that we are not reading them then unless we are sure that this is the correct checkpoint directory.
 I agree.;;;","24/Sep/21 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","19/Oct/21 10:20;pnowojski;Thanks [~roman] and [~arvid] for reporting and analysing this issue.
{quote}
IMO, the desired behavior is to wait for all checkpoints to be discarded before reporting job termination to the client.

However, CheckpointCleaner currently submits actions to JobManagerSharedServices.scheduledExecutorService; which is terminated with the last job (i.e. minicluster shutdown); and it still doesn't wait for all tasks termination on shutdown.
A simple workaround would be to wait in CheckpointsCleaner until numberOfCheckpointsToClean become 0 on termination. (and wait for executor tasks termination).
{quote}
I agree that there are two options to fix this
1. Wait for {{CheckpointCleaner}} to finish before job reaching terminal state
2. Wait for {{CheckpointCleaner}} to finish before shutting down {{AdaptiveScheduler}} and {{DefaultScheduler}}

The main advantage of 1. would be that files wouldn't be removed after reaching the terminal state, which would be still a possibility in the option 2. However I've chatted about this offline with [~trohrmann], and we are not sure if this is an issue. After all we are aiming for a clear ownership of checkpoint/savepoint files, and checkpoint files would be owned by Flink, so users shouldn't be concerned what happens with those files. So for the time being we would go with the easier to implement option 2.;;;","28/Oct/21 13:57;pnowojski;Merged to master as daa8ac9426e^..daa8ac9426e
Merged to release-1.14 as 1a33033000f^..1a33033000f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sqlclient doesn't response CTRL-C correctly before Executor ResultView opening.,FLINK-23645,13393682,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,loyi,loyi,loyi,05/Aug/21 10:21,13/Aug/21 02:28,13/Jul/23 08:12,12/Aug/21 04:12,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"If we press ctrl-c before the *ResultView* opened,the terminal will exits without any prompt.

 For example:

!image-2021-08-05-18-31-50-487.png!

After analsis, i found out the reason is Sqlclient doesn't register *Signal.INT* handler before *callOperation*, then jvm default handler executes and exit.

 

Suggestion:

We could just interupt the Executor when we receive  *Signal.INT*

 
{code:java}
private void callOperation(Operation operation, ExecutionMode mode) {
    validate(operation, mode);
    final Thread thread = Thread.currentThread();
    final Terminal.SignalHandler previousHandler =
            terminal.handle(Terminal.Signal.INT, (signal) -> thread.interrupt());
    try {
        if (operation instanceof QuitOperation) {
            // QUIT/EXIT
            callQuit();
        } else if (operation instanceof ClearOperation) {
            // CLEAR
            callClear();
        } else if (operation instanceof HelpOperation) {
            // HELP
            callHelp();
        } else if (operation instanceof SetOperation) {
            // SET
            callSet((SetOperation) operation);
        } else if (operation instanceof ResetOperation) {
            // RESET
            callReset((ResetOperation) operation);
        } else if (operation instanceof CatalogSinkModifyOperation) {
            // INSERT INTO/OVERWRITE
            callInsert((CatalogSinkModifyOperation) operation);
        } else if (operation instanceof QueryOperation) {
            // SELECT
            callSelect((QueryOperation) operation);
        } else if (operation instanceof ExplainOperation) {
            // EXPLAIN
            callExplain((ExplainOperation) operation);
        } else if (operation instanceof BeginStatementSetOperation) {
            // BEGIN STATEMENT SET
            callBeginStatementSet();
        } else if (operation instanceof EndStatementSetOperation) {
            // END
            callEndStatementSet();
        } else {
            // fallback to default implementation
            executeOperation(operation);
        }
    } finally {
        terminal.handle(Terminal.Signal.INT, previousHandler);
   }
}{code}
 

After fixed:

!image-2021-08-05-18-44-08-453.png!  ",,jark,loyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23744,,,,,,,,,,,,,,,,"05/Aug/21 10:31;loyi;image-2021-08-05-18-31-50-487.png;https://issues.apache.org/jira/secure/attachment/13031501/image-2021-08-05-18-31-50-487.png","05/Aug/21 10:44;loyi;image-2021-08-05-18-44-08-453.png;https://issues.apache.org/jira/secure/attachment/13031502/image-2021-08-05-18-44-08-453.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 12 04:12:34 UTC 2021,,,,,,,,,,"0|z0tn0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/21 12:35;jark;[~loyi] are you willing to contribute the fix?

cc [~fsk119];;;","06/Aug/21 02:23;loyi;[~jark]  yes ,please assign to me;;;","12/Aug/21 04:12;jark;Fixed in master: c54084a8bb124fb88b0effee4d01f4a1ceb02f17;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PrestoS3FileSystemITCase.testConfigKeysForwarding,FLINK-23635,13393608,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,airblader,xtsong,xtsong,05/Aug/21 02:01,05/Aug/21 11:52,13/Jul/23 08:12,05/Aug/21 07:14,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,FileSystems,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21522&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11407

{code}
Aug 04 16:45:11 [ERROR] Tests run: 6, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 104.563 s <<< FAILURE! - in org.apache.flink.fs.s3presto.PrestoS3FileSystemITCase
Aug 04 16:45:11 [ERROR] testConfigKeysForwarding[Scheme = s3](org.apache.flink.fs.s3presto.PrestoS3FileSystemITCase)  Time elapsed: 44.523 s  <<< ERROR!
Aug 04 16:45:11 com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@1e7f2e0f: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@55f45b92: Failed to connect to service endpoint: ]
Aug 04 16:45:11 	at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:136)
Aug 04 16:45:11 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1257)
Aug 04 16:45:11 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:833)
Aug 04 16:45:11 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:783)
Aug 04 16:45:11 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
Aug 04 16:45:11 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
Aug 04 16:45:11 	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
Aug 04 16:45:11 	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
Aug 04 16:45:11 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
Aug 04 16:45:11 	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
Aug 04 16:45:11 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5259)
Aug 04 16:45:11 	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5206)
Aug 04 16:45:11 	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1360)
Aug 04 16:45:11 	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1334)
Aug 04 16:45:11 	at com.facebook.presto.hive.s3.PrestoS3FileSystem.lambda$getS3ObjectMetadata$4(PrestoS3FileSystem.java:661)
Aug 04 16:45:11 	at com.facebook.presto.hive.RetryDriver.run(RetryDriver.java:139)
Aug 04 16:45:11 	at com.facebook.presto.hive.s3.PrestoS3FileSystem.getS3ObjectMetadata(PrestoS3FileSystem.java:658)
Aug 04 16:45:11 	at com.facebook.presto.hive.s3.PrestoS3FileSystem.getS3ObjectMetadata(PrestoS3FileSystem.java:642)
Aug 04 16:45:11 	at com.facebook.presto.hive.s3.PrestoS3FileSystem.getFileStatus(PrestoS3FileSystem.java:353)
Aug 04 16:45:11 	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1734)
Aug 04 16:45:11 	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.exists(HadoopFileSystem.java:165)
Aug 04 16:45:11 	at org.apache.flink.fs.s3presto.PrestoS3FileSystemITCase.testConfigKeysForwarding(PrestoS3FileSystemITCase.java:91)
Aug 04 16:45:11 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 04 16:45:11 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 04 16:45:11 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 04 16:45:11 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 04 16:45:11 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 04 16:45:11 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 04 16:45:11 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 04 16:45:11 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 04 16:45:11 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Aug 04 16:45:11 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Aug 04 16:45:11 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)

Aug 04 16:45:11 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 04 16:45:11 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Aug 04 16:45:11 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 04 16:45:11 	at org.junit.runners.Suite.runChild(Suite.java:128)
Aug 04 16:45:11 	at org.junit.runners.Suite.runChild(Suite.java:27)
Aug 04 16:45:11 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Aug 04 16:45:11 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Aug 04 16:45:11 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Aug 04 16:45:11 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 04 16:45:11 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Aug 04 16:45:11 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Aug 04 16:45:11 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Aug 04 16:45:11 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 04 16:45:11 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 04 16:45:11 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Aug 04 16:45:11 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Aug 04 16:45:11 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Aug 04 16:45:11 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Aug 04 16:45:11 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Aug 04 16:45:11 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Aug 04 16:45:11 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Aug 04 16:45:11 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Aug 04 16:45:11 	Suppressed: com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@1e7f2e0f: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@55f45b92: Failed to connect to service endpoint: ]
Aug 04 16:45:11 		... 62 more
Aug 04 16:45:11 	Suppressed: com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@1e7f2e0f: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@55f45b92: Failed to connect to service endpoint: ]
Aug 04 16:45:11 		... 62 more
Aug 04 16:45:11 	Suppressed: com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@1e7f2e0f: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@55f45b92: Failed to connect to service endpoint: ]
Aug 04 16:45:11 		... 62 more
Aug 04 16:45:11 	Suppressed: com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@1e7f2e0f: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@55f45b92: Failed to connect to service endpoint: ]
Aug 04 16:45:11 		... 62 more
Aug 04 16:45:11 	Suppressed: com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@1e7f2e0f: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@55f45b92: Failed to connect to service endpoint: ]
Aug 04 16:45:11 		... 62 more
{code}",,airblader,dwysakowicz,twalthr,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23487,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 05 07:45:29 UTC 2021,,,,,,,,,,"0|z0tmk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/21 02:02;xtsong;cc [~airblader] [~twalthr]
Could this relate to FLINK-23487?;;;","05/Aug/21 02:19;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21530&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11407;;;","05/Aug/21 02:20;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21533&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11407;;;","05/Aug/21 02:22;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21548&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11407;;;","05/Aug/21 02:24;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21554&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11407;;;","05/Aug/21 02:49;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21556&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11407

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21556&view=logs&j=e1276d0f-df12-55ec-86b5-c0ad597d83c9&t=66648bdf-9af9-503d-c9a7-11f783a19935&l=11152

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21556&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=f53023d8-92c3-5d78-ec7e-70c2bf37be20&l=11305

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21556&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=11607

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21556&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=4cf71635-d33f-53ff-7185-c5abb11ae3a0&l=12204

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21556&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=11375

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21556&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=41463ccd-0694-5d4d-220d-8f771e7d098b&l=11149;;;","05/Aug/21 05:38;airblader;Yes, that's very likely related. I'll look into this a bit as well.;;;","05/Aug/21 05:59;xtsong;Thank you, [~airblader]. You're assigned to the ticket.;;;","05/Aug/21 06:18;airblader;Preliminary finding, I can get the test to fail locally as well with the same error. It seems that changing the expected exception fixes the problem, see patch below. However, I'm not yet sure (a) why the thrown exception has changed (to justify it), or, more importantly, (b) why this would fail now, but not on the PR build (which was successful). I'll dig into this a bit more.
{code:java}
diff --git a/flink-filesystems/flink-s3-fs-presto/src/test/java/org/apache/flink/fs/s3presto/PrestoS3FileSystemITCase.java b/flink-filesystems/flink-s3-fs-presto/src/test/java/org/apache/flink/fs/s3presto/PrestoS3FileSystemITCase.java
index dae9e1f87c..4e0ef07c43 100644
--- a/flink-filesystems/flink-s3-fs-presto/src/test/java/org/apache/flink/fs/s3presto/PrestoS3FileSystemITCase.java
+++ b/flink-filesystems/flink-s3-fs-presto/src/test/java/org/apache/flink/fs/s3presto/PrestoS3FileSystemITCase.java
@@ -90,7 +90,7 @@ public class PrestoS3FileSystemITCase extends AbstractHadoopFileSystemITTest {
             try {
                 path.getFileSystem().exists(path);
                 fail(""should fail with an exception"");
-            } catch (IOException ignored) {
+            } catch (Exception ignored) {
             }
         }
 

{code};;;","05/Aug/21 06:40;airblader;OK, I can justify the change of exception now. Will open a PR.;;;","05/Aug/21 07:05;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21566&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11150

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21569&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11407;;;","05/Aug/21 07:08;airblader;-I've also reported this change in behavior to presto now ([https://github.com/prestodb/presto/issues/16569]).-;;;","05/Aug/21 07:14;twalthr;I reverted the affecting commit in 452fb6774521eec112941549dae679cf9df1a7b6 and reopened FLINK-23487.;;;","05/Aug/21 07:24;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21570&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11562;;;","05/Aug/21 07:24;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21571&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11368;;;","05/Aug/21 07:45;airblader;I also identified the reason for why the PR originally passed the CI. This particular test is not executed for pull requests:
{code:java}
2021-08-02T15:10:26.2137325Z Aug 02 15:10:26 [INFO] -------------------------------------------------------
2021-08-02T15:10:26.2137876Z Aug 02 15:10:26 [INFO]  T E S T S
2021-08-02T15:10:26.2138706Z Aug 02 15:10:26 [INFO] -------------------------------------------------------
2021-08-02T15:10:26.5995336Z Aug 02 15:10:26 [INFO] Running org.apache.flink.fs.s3presto.PrestoS3FileSystemITCase
2021-08-02T15:10:26.6275858Z Aug 02 15:10:26 [INFO] Running org.apache.flink.fs.s3presto.PrestoS3FileSystemBehaviorITCase
2021-08-02T15:10:27.3975610Z Aug 02 15:10:27 [WARNING] Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.768 s - in org.apache.flink.fs.s3presto.PrestoS3FileSystemBehaviorITCase
2021-08-02T15:10:27.4040650Z Aug 02 15:10:27 [WARNING] Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.803 s - in org.apache.flink.fs.s3presto.PrestoS3FileSystemITCase
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BatchingStateChangeUploaderTest.testDelay fails on azure,FLINK-23634,13393606,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,xtsong,xtsong,05/Aug/21 01:53,30/Aug/21 20:29,13/Jul/23 08:12,05/Aug/21 14:23,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21518&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=22202

{code}
Aug 04 16:31:14 [ERROR] Tests run: 6, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.616 s <<< FAILURE! - in org.apache.flink.changelog.fs.BatchingStateChangeUploaderTest
Aug 04 16:31:14 [ERROR] testDelay(org.apache.flink.changelog.fs.BatchingStateChangeUploaderTest)  Time elapsed: 0.198 s  <<< FAILURE!
Aug 04 16:31:14 java.lang.AssertionError: expected:<[logId=96102f85-c8a5-454a-9651-14bc1c6b0858, sequenceNumber=0, changes=[keyGroup=0, dataSize=4]]> but was:<[]>
Aug 04 16:31:14 	at org.junit.Assert.fail(Assert.java:89)
Aug 04 16:31:14 	at org.junit.Assert.failNotEquals(Assert.java:835)
Aug 04 16:31:14 	at org.junit.Assert.assertEquals(Assert.java:120)
Aug 04 16:31:14 	at org.junit.Assert.assertEquals(Assert.java:146)
Aug 04 16:31:14 	at org.apache.flink.changelog.fs.BatchingStateChangeUploaderTest.lambda$testDelay$4(BatchingStateChangeUploaderTest.java:105)
Aug 04 16:31:14 	at org.apache.flink.changelog.fs.BatchingStateChangeUploaderTest.withStore(BatchingStateChangeUploaderTest.java:210)
Aug 04 16:31:14 	at org.apache.flink.changelog.fs.BatchingStateChangeUploaderTest.testDelay(BatchingStateChangeUploaderTest.java:97)
Aug 04 16:31:14 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 04 16:31:14 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 04 16:31:14 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 04 16:31:14 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 04 16:31:14 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 04 16:31:14 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 04 16:31:14 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 04 16:31:14 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 04 16:31:14 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Aug 04 16:31:14 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Aug 04 16:31:14 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 04 16:31:14 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 04 16:31:14 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Aug 04 16:31:14 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Aug 04 16:31:14 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Aug 04 16:31:14 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Aug 04 16:31:14 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Aug 04 16:31:14 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Aug 04 16:31:14 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Aug 04 16:31:14 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,roman_old,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 05 14:23:49 UTC 2021,,,,,,,,,,"0|z0tmjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/21 01:54;xtsong;cc [~roman_khachatryan];;;","05/Aug/21 14:23;roman;Fix merged as 0d09f76fa3542afae9119e96a928e7f49a308a71.
And thanks for reporting and letting me know Xintong Song :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix broken links in documentations,FLINK-23631,13393541,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hapihu,hapihu,hapihu,04/Aug/21 17:26,10/Aug/21 09:03,13/Jul/23 08:12,10/Aug/21 09:02,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Documentation,,,,,0,pull-request-available,,,,"1.  [https://ci.apache.org/projects/flink/flink-docs-master/zh/docs/dev/dataset/examples/]

2. docs/content.zh/docs/dev/dataset/examples.md

3. The shortcode below contains both 'file' attributes and no 'name'
{code:java}
 {{< gh_link file=""flink-examples/flink-examples-batch"" file=""flink-examples-batch"" >}}
{code}
4. It should be modified as follows
{code:java}
 {{< gh_link file=""flink-examples/flink-examples-batch"" name=""flink-examples-batch"" >}}
{code}",,hapihu,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/21 17:26;hapihu;image-20210805011518012.png;https://issues.apache.org/jira/secure/attachment/13031448/image-20210805011518012.png","04/Aug/21 17:40;hapihu;image-20210805014000555.png;https://issues.apache.org/jira/secure/attachment/13031451/image-20210805014000555.png","07/Aug/21 16:24;hapihu;image-20210808002209467.png;https://issues.apache.org/jira/secure/attachment/13031623/image-20210808002209467.png",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 10 09:02:56 UTC 2021,,,,,,,,,,"0|z0tm54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/21 17:45;hapihu;[https://ci.apache.org/projects/flink/flink-docs-master/docs/ops/metrics/]

[https://ci.apache.org/projects/flink/flink-docs-master/zh/docs/ops/metrics/]

docs/content/docs/ops/metrics.md

docs/content.zh/docs/ops/metrics.md

 

1. The following syntax is incorrect, causing link failure
{code:java}
{% gh_link flink-metrics/flink-metrics-dropwizard/src/main/java/org/apache/flink/dropwizard/metrics/DropwizardHistogramWrapper.java ""Wrapper"" %} 
{code}
2. It should be modified as follows
{code:java}
{{< gh_link file=""flink-metrics/flink-metrics-dropwizard/src/main/java/org/apache/flink/dropwizard/metrics/DropwizardHistogramWrapper.java"" name=""Wrapper"" >}} 
{code}
 ;;;","07/Aug/21 16:24;hapihu;[https://ci.apache.org/projects/flink/flink-docs-master/zh/docs/deployment/memory/mem_migration/]

 

The shortcodes corresponding to link should be modified as follows
{code:java}
{%link }}   ==>    {{< ref """" >}}
{code};;;","09/Aug/21 15:17;hapihu;Hi  [~jark]
Excuse me for taking up your time.
I created pull request for this issue.
Would you like to review it for me.
If there is any problem, please inform me in time.
I will actively revise it in time.
Thank you very much!;;;","10/Aug/21 09:02;jark;Fixed in master: bfb0d79526c487f972a28041404bac79e0499371;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make EventTimeWindowCheckpointingITCase and LocalRecoveryITCase run on Windows.,FLINK-23630,13393532,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sewen,sewen,sewen,04/Aug/21 16:50,16/Aug/21 22:04,13/Jul/23 08:12,16/Aug/21 22:04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Tests,,,,,0,,,,,This need a fix in the test where it creates the paths for the checkpoint storage locations.,,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 16 22:04:11 UTC 2021,,,,,,,,,,"0|z0tm34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/21 22:04;sewen;Fixed via c84ebe839b3e1e936e68d2e67496cf7664b35287;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove redundant test cases in EventTimeWindowCheckpointingITCase,FLINK-23629,13393531,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sewen,sewen,sewen,04/Aug/21 16:49,17/Aug/21 10:57,13/Jul/23 08:12,16/Aug/21 22:03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Tests,,,,,0,pull-request-available,,,,"HashMap state store snapshots are always async right now, sync snapshots are no longer supported.

We should adjust the {{EventTimeWindowCheckpointingITCase}} to remove the now redundant cases {{MEM_ASYNC}} and {{FILE_ASYNC}} parameter runs.

The test is very time-intensive, so this is quite a time saver.",,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 16 22:03:45 UTC 2021,,,,,,,,,,"0|z0tm2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/21 22:03;sewen;Fixed via 31b45fc6c48dc0503f1c91ce1b6b0b00c6439a02;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation of the TRUNCATE SQL function is not correct,FLINK-23615,13393403,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,launche,TsReaper,TsReaper,04/Aug/21 07:10,11/Aug/21 03:15,13/Jul/23 08:12,11/Aug/21 03:15,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Documentation,,,,,0,pull-request-available,starter,,,"The documentation of the {{TRUNCATE}} SQL function states that ""E.g. 42.324.truncate(2) to 42.34"", which should be ""42.32"".

Some period in the document also lacks trailing spaces.",,jark,launche,TsReaper,,,,,,,,,,,,,,,,";09/Aug/21 09:52;launche;3600",,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 11 03:15:50 UTC 2021,,,,,,,,,,"0|z0tlag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/21 07:47;launche;hello, [~TsReaper].  I'm new to the community and I want to try to make a contribution to the community. Could you please assign this task to me?;;;","09/Aug/21 09:13;TsReaper;Hi [~launche]! Thanks for your interest in contributing to Flink. Sadly I'm not a committer and cannot assign issues to people. Maybe [~jark] can help.;;;","09/Aug/21 09:26;launche;Thanks for your advice,  [~TsReaper].  And i will ask [~jark]  if he can assign this task to me. Besides, I've already started doing this job.;;;","11/Aug/21 03:15;jark;Fixed in master: 0c1b2580f6c796d2b2a5daee7686693f59e75d66;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The resulting scale of TRUNCATE(DECIMAL, ...) is not correct",FLINK-23614,13393400,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,paul8263,TsReaper,TsReaper,04/Aug/21 07:08,15/Dec/21 01:44,13/Jul/23 08:12,07/Sep/21 09:13,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,1.15.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,starter,,,"Run the following SQL
{code:sql}
SELECT
  TRUNCATE(123.456, 2),
  TRUNCATE(123.456, 0),
  TRUNCATE(123.456, -2),
  TRUNCATE(CAST(123.456 AS DOUBLE), 2),
  TRUNCATE(CAST(123.456 AS DOUBLE), 0),
  TRUNCATE(CAST(123.456 AS DOUBLE), -2)
{code}

The result is
{code}
123.450
123.000
100.000
123.45
123.0
100.0
{code}

It seems that the resulting scale of {{TRUNCATE(DECIMAL, ...)}} is the same as that of the input decimal.",,airblader,godfreyhe,lzljs3620320,paul8263,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 29 02:35:17 UTC 2021,,,,,,,,,,"0|z0tl9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/21 03:10;TsReaper;This is because {{FlinkSqlOperatorTable#TRUNCATE}} is using the function definition from Calcite's {{SqlStdOperatorTable#TRUNCATE}}. The return type inference of this function is {{ARG0_NULLABLE}} which causes this issue.

Flink already has a self-defined return type inference for this ({{FlinkReturnTypes#ROUND_FUNCTION_NULLABLE}}). See its usage in {{FlinkSqlOperatorTable#ROUND}}. We just need to define our own truncate function and add some more tests.;;;","06/Aug/21 06:15;paul8263;I'm recently working on Flink SQL and could you please assign this to me?;;;","06/Aug/21 06:32;TsReaper;Hi [~paul8263]! I'm glad that you'd like to fix this issue and I'll also help for the review. [~lzljs3620320] would you help assign this issue?;;;","06/Aug/21 06:54;lzljs3620320;Thanks!;;;","06/Aug/21 10:27;paul8263;Hi [~TsReaper] [~lzljs3620320],

By the way I found another issue that some methods in SqlFunctionUtils.java are not needed by BuiltInMethods.scala. For example, a sround method in SqlFunctionUtils.java is defined as:

{code:java}
public static int sround(int b0, int b1) {
    return sround(BigDecimal.valueOf(b0), b1).intValue();
}
{code}

But in BuiltInMethods.scala as shown below, the sround method is loaded from SqlFunctions(in Calcite runtime), apparently not from SqlFunctionUtils in Flink.

{code:scala}
val ROUND_INT = Types.lookupMethod(classOf[SqlFunctions], ""sround"", classOf[Int], classOf[Int])
{code}

Do I need to create another ticket for cleaning up those ""unnecessary"" methods?
;;;","09/Aug/21 02:37;TsReaper;[~paul8263] I believe this class is copied from Calcite. However as it's not intended to replace the original class in Calcite I think removing unused methods are reasonable.

To check if a method is really unused it is not enough to see if it is called in the Flink code base. Some code generation may use these method as a string literal and we should really check that thoroughly. I think this is a good point but it may take some effort. What do you think [~lzljs3620320].;;;","07/Sep/21 09:13;lzljs3620320;master: e55e6649da41621872f3a847dcd1dad2d97b017a;;;","29/Oct/21 02:35;godfreyhe;1.14.1: 8c7cbcd9744378dccb4b6ab0493c7d273c69dfef;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SELECT ROUND(CAST(1.2345 AS FLOAT), 1) cannot compile",FLINK-23612,13393366,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,04/Aug/21 03:20,07/Sep/21 09:12,13/Jul/23 08:12,07/Sep/21 09:12,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"Run this SQL {{SELECT ROUND(CAST(1.2345 AS FLOAT), 1)}} and the following exception will be thrown:

{code}
java.lang.RuntimeException: Could not instantiate generated class 'ExpressionReducer$2'

	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:75)
	at org.apache.flink.table.planner.codegen.ExpressionReducer.reduce(ExpressionReducer.scala:108)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressionsInternal(ReduceExpressionsRule.java:759)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressions(ReduceExpressionsRule.java:699)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:306)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:87)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:282)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:165)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1702)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:833)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1301)
	at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:601)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:300)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:140)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:106)
	at org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.myTest2(CalcITCase.scala:1618)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:76)
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102)
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69)
	... 74 more
Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74)
	... 76 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:89)
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
	... 79 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 36, Column 23: Assignment conversion not possible from type ""double"" to type ""float""
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
	at org.codehaus.janino.UnitCompiler.assignmentConversion(UnitCompiler.java:11062)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3790)
	at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3754)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3734)
	at org.codehaus.janino.Java$Assignment.accept(Java.java:4477)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)
	at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553)
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$Block.accept(Java.java:2779)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476)
	at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86)
	... 85 more
{code}",,jark,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 07 09:12:20 UTC 2021,,,,,,,,,,"0|z0tl28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/21 03:00;TsReaper;This is because Calcite hasn't implemented the {{sround}} function for float. There is also no implementation for byte and short. I'm taking this issue.;;;","07/Sep/21 09:12;lzljs3620320;master: 075fd1b9c453899d26b26f03f053acf6455050a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefaultSchedulerTest.testProducedPartitionRegistrationTimeout fails on azure,FLINK-23610,13393362,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,xtsong,xtsong,04/Aug/21 02:48,28/Aug/21 13:10,13/Jul/23 08:12,06/Aug/21 05:24,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21438&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=7834

{code}
Aug 03 23:05:35 [ERROR] Tests run: 40, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.43 s <<< FAILURE! - in org.apache.flink.runtime.scheduler.DefaultSchedulerTest
Aug 03 23:05:35 [ERROR] testProducedPartitionRegistrationTimeout(org.apache.flink.runtime.scheduler.DefaultSchedulerTest)  Time elapsed: 0.137 s  <<< FAILURE!
Aug 03 23:05:35 java.lang.AssertionError: 
Aug 03 23:05:35 
Aug 03 23:05:35 Expected: a collection with size <2>
Aug 03 23:05:35      but: collection size was <0>
Aug 03 23:05:35 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Aug 03 23:05:35 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
Aug 03 23:05:35 	at org.apache.flink.runtime.scheduler.DefaultSchedulerTest.testProducedPartitionRegistrationTimeout(DefaultSchedulerTest.java:1391)
Aug 03 23:05:35 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 03 23:05:35 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 03 23:05:35 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 03 23:05:35 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
Aug 03 23:05:35 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 03 23:05:35 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 03 23:05:35 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 03 23:05:35 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 03 23:05:35 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Aug 03 23:05:35 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Aug 03 23:05:35 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Aug 03 23:05:35 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Aug 03 23:05:35 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 03 23:05:35 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Aug 03 23:05:35 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Aug 03 23:05:35 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Aug 03 23:05:35 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Aug 03 23:05:35 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Aug 03 23:05:35 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Aug 03 23:05:35 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Aug 03 23:05:35 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 03 23:05:35 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Aug 03 23:05:35 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Aug 03 23:05:35 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Aug 03 23:05:35 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 03 23:05:35 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 03 23:05:35 	at org.junit.runners.Suite.runChild(Suite.java:128)
Aug 03 23:05:35 	at org.junit.runners.Suite.runChild(Suite.java:27)
Aug 03 23:05:35 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Aug 03 23:05:35 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Aug 03 23:05:35 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Aug 03 23:05:35 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Aug 03 23:05:35 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Aug 03 23:05:35 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Aug 03 23:05:35 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Aug 03 23:05:35 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
Aug 03 23:05:35 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
Aug 03 23:05:35 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
Aug 03 23:05:35 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
Aug 03 23:05:35 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
Aug 03 23:05:35 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
Aug 03 23:05:35 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Aug 03 23:05:35 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Aug 03 23:05:35 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Aug 03 23:05:35 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 06 05:24:54 UTC 2021,,,,,,,,,,"0|z0tl1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/21 08:27;chesnay;Caused by the {{Thread.sleep}}; if you remove it the test fails reliably.;;;","06/Aug/21 05:24;chesnay;master: 4e69d07193fc26db163e6755f622d2eba4f63c7f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unstable test_from_and_to_data_stream_event_time failed,FLINK-23594,13393202,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,03/Aug/21 06:14,03/Aug/21 09:52,13/Jul/23 08:12,03/Aug/21 06:56,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,API / Python,,,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21337&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901
",,dianfu,dwysakowicz,guoyangze,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 03 09:52:30 UTC 2021,,,,,,,,,,"0|z0tk20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/21 06:31;guoyangze;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21348&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901;;;","03/Aug/21 06:56;hxbks2ks;After a brief analysis, the problem should be the custom window used in the test.I need to spend more time to be able to find out the problem with the custom window. Due to the frequency of the failure, I temporarily skip this test.

Merged into master via 7e2f01a1a13d6ea6058868374eeb96e68e1926a1
;;;","03/Aug/21 09:40;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21345&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=21791;;;","03/Aug/21 09:49;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21352&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=22254;;;","03/Aug/21 09:52;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21358&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=22244;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[DOCS]The link to the 'deployment/config' page is invalid because several characters are written in page 'deployment/memory/mem_tuning/',FLINK-23591,13393130,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hapihu,hapihu,hapihu,02/Aug/21 17:36,28/Aug/21 13:07,13/Jul/23 08:12,03/Aug/21 02:45,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Documentation,,,,,0,pull-request-available,,,,"1、 [https://ci.apache.org/projects/flink/flink-docs-master/zh/docs/deployment/memory/mem_tuning/]

2、 docs/content.zh/docs/deployment/memory/mem_tuning.md

 

3、The wrong link is below：

{code:java}
[`jobmanager.memory.flink.size`]({{< ref ""docs/deployment/config"" >}}#jobmanager-memory-flink-size"" >}})
{code}

4、It should be modified as follows

{code:java}
[`jobmanager.memory.flink.size`]({{< ref ""docs/deployment/config"" >}}#jobmanager-memory-flink-size)
{code}
 

 

 ",,hapihu,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/21 17:34;hapihu;image-20210803011947329.png;https://issues.apache.org/jira/secure/attachment/13031391/image-20210803011947329.png","02/Aug/21 17:34;hapihu;image-20210803012556316.png;https://issues.apache.org/jira/secure/attachment/13031390/image-20210803012556316.png","02/Aug/21 18:06;hapihu;image-20210803014949126.png;https://issues.apache.org/jira/secure/attachment/13031393/image-20210803014949126.png",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 03 02:45:17 UTC 2021,,,,,,,,,,"0|z0tjm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/21 18:08;hapihu; 
 5、The absence of a blank line after the div tag invalidates the MarkDown tag and invalidates the link
{code:java}
<div class=""alert alert-warning"">
  <strong>Warning:</strong> If Flink or user code allocates unmanaged off-heap (native) memory beyond the container size
  the job can fail because the deployment environment can kill the offending containers.
</div>
See also description of [container memory exceeded]({{< ref ""docs/deployment/memory/mem_trouble"" >}}#container-memory-exceeded) failure.
{code}
It should be modified as follows:
{code:java}
<div class=""alert alert-warning"">
  <strong>Warning:</strong> If Flink or user code allocates unmanaged off-heap (native) memory beyond the container size
  the job can fail because the deployment environment can kill the offending containers.
</div>

See also description of [container memory exceeded]({{< ref ""docs/deployment/memory/mem_trouble"" >}}#container-memory-exceeded) failure.
{code}
 

 ;;;","02/Aug/21 18:22;hapihu;Hi  [~jark] 
Excuse me for taking up your time.
I created pull request for this issue.
Would you like to review it for me.
If there is any problem, please inform me in time. 
I will actively revise it in time.
Thank you very much!;;;","03/Aug/21 02:45;jark;Fixed in master: 11aeacab6fb9a95efb57709dee59c9162c04f7f9
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamTaskTest#testProcessWithUnAvailableInput is flaky,FLINK-23590,13393101,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,akalashnikov,dmvk,dmvk,02/Aug/21 15:00,28/Aug/21 13:09,13/Jul/23 08:12,04/Aug/21 17:43,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Task,,,,,0,pull-request-available,test-stability,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21218&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb]

 
{code:java}
java.lang.AssertionError: 
Expected: a value equal to or greater than <222222L>
     but: <217391L> was less than <222222L>	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.junit.Assert.assertThat(Assert.java:964)
	at org.junit.Assert.assertThat(Assert.java:930)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskTest.testProcessWithUnAvailableInput(StreamTaskTest.java:1561)
	at jdk.internal.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.lang.Thread.run(Thread.java:829){code}",,dmvk,dwysakowicz,pnowojski,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23452,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 04 17:43:21 UTC 2021,,,,,,,,,,"0|z0tjfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/21 09:53;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21358&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9136;;;","04/Aug/21 01:52;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21370&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=10777;;;","04/Aug/21 02:44;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21438&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=9087;;;","04/Aug/21 05:54;zhuzh;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21450&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","04/Aug/21 06:48;pnowojski;bumping to release blocker due to frequency of occurrences. ;;;","04/Aug/21 16:43;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21503&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=10511;;;","04/Aug/21 17:43;pnowojski;Merged to master as 97727afa449;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The link to the Flink download page is broken,FLINK-23581,13393015,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hapihu,hapihu,hapihu,02/Aug/21 08:06,28/Aug/21 13:09,13/Jul/23 08:12,05/Aug/21 06:50,1.13.0,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Documentation,,,,,0,pull-request-available,,,,"*1. The url of the download link is not written correctly, so the link is invalid.*

 

The wrong link information is as follows：
{code:java}
 [download page]({{ site.download_url }}) 
{code}
It should be modified as follows：
{code:java}
 [download page](https://flink.apache.org/downloads.html) 
{code}
 

*2.  The pages involved are as follows*

1> [https://ci.apache.org/projects/flink/flink-docs-master/zh/docs/deployment/resource-providers/standalone/overview/]

2> [https://ci.apache.org/projects/flink/flink-docs-master/zh/docs/deployment/resource-providers/yarn/]

3> [https://ci.apache.org/projects/flink/flink-docs-master/docs/deployment/resource-providers/standalone/overview/]

4> [https://ci.apache.org/projects/flink/flink-docs-master/docs/deployment/resource-providers/yarn/]

 

*3、The documents involved are as follows：*
{code:java}
1. docs/content.zh/docs/deployment/resource-providers/standalone/overview.md
2. docs/content.zh/docs/deployment/resource-providers/yarn.md
3. docs/content/docs/deployment/resource-providers/standalone/overview.md
4. docs/content/docs/deployment/resource-providers/yarn.md
{code}
*4、 The modifications involved are as follows：*
{code:java}
1.  [download page]({{ site.download_url }})   
Modified as follows: 
[download page](https://flink.apache.org/downloads.html)
 
2. [下载页面]({{ site.zh_download_url }}) 
Modified as follows: 
[下载页面](https://flink.apache.org/zh/downloads.html)
  
3. [Downloads / Additional Components]({{site.download_url}}#additional-components)
Modified as follows: 
[Downloads / Additional Components](https://flink.apache.org/downloads.html#additional-components)
{code}",,hapihu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/21 08:01;hapihu;image-20210802152418242.png;https://issues.apache.org/jira/secure/attachment/13031370/image-20210802152418242.png","02/Aug/21 08:01;hapihu;image-20210802153010175.png;https://issues.apache.org/jira/secure/attachment/13031369/image-20210802153010175.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 05 06:50:06 UTC 2021,,,,,,,,,,"0|z0tiwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/21 18:22;hapihu;Hi  [~jark] 
Excuse me for taking up your time.
I created pull request for this issue.
Would you like to review it for me.
If there is any problem, please inform me in time. 
I will actively revise it in time.
Thank you very much!;;;","05/Aug/21 06:50;chesnay;master: 2ae30249719dd090ee76be7fb3576bf85561af05
1.13: b3b7ae9bad42b7cbc89680b80a79933287c237a5 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SELECT SHA2(CAST(NULL AS VARBINARY), CAST(NULL AS INT)) AS ref0 can't compile",FLINK-23579,13393002,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,xiaojin.wy,xiaojin.wy,02/Aug/21 06:16,24/Aug/21 08:07,13/Jul/23 08:12,24/Aug/21 08:07,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"Running the sql of SELECT SHA2(CAST(NULL AS VARBINARY), CAST(NULL AS INT)) AS ref0 will get the error below:

{code}
java.lang.RuntimeException: Could not instantiate generated class 'ExpressionReducer$3'

	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:75)
	at org.apache.flink.table.planner.codegen.ExpressionReducer.reduce(ExpressionReducer.scala:108)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressionsInternal(ReduceExpressionsRule.java:759)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressions(ReduceExpressionsRule.java:699)
	at org.apache.calcite.rel.rules.ReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:306)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:87)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:46)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:282)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:165)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1702)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:833)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1301)
	at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:601)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:300)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:140)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:106)
	at org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.myTest(CalcITCase.scala:1614)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:76)
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102)
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69)
	... 74 more
Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74)
	... 76 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:89)
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
	... 79 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 55, Column 85: No applicable constructor/method found for actual parameters ""byte[], java.security.MessageDigest""; candidates are: ""public static org.apache.flink.table.data.binary.BinaryStringData org.apache.flink.table.data.binary.BinaryStringDataUtil.hash(org.apache.flink.table.data.binary.BinaryStringData, java.lang.String) throws java.security.NoSuchAlgorithmException"", ""public static org.apache.flink.table.data.binary.BinaryStringData org.apache.flink.table.data.binary.BinaryStringDataUtil.hash(org.apache.flink.table.data.binary.BinaryStringData, java.security.MessageDigest)""
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:9263)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9123)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9025)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5062)
	at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423)
	at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3792)
	at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3754)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3734)
	at org.codehaus.janino.Java$Assignment.accept(Java.java:4477)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)
	at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553)
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$Block.accept(Java.java:2779)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476)
	at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86)
	... 85 more
{code}
",,libenchao,lzljs3620320,TsReaper,xiaojin.wy,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 09 03:09:29 UTC 2021,,,,,,,,,,"0|z0titk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/21 03:17;TsReaper;Hi! Thanks for raising this issue. This is because the hashing function does not have a corresponding version which reads in a byte array. I'm taking this issue.;;;","09/Aug/21 03:09;lzljs3620320;master: 0d2b945729df8f0a0149d02ca24633ae52a1ef21
release-1.13: 5d4b25dd6756ecb0ececf8c54d0125dfb6342c5c
release-1.12: 8a83d8b46e10231c8c129b80ba951c23c1ee52ef;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tests fail with AdaptiveScheduler due to exceptions in logs trying to offer slots after JobMaster shutdown,FLINK-23573,13392983,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xtsong,xtsong,xtsong,02/Aug/21 04:11,28/Aug/21 13:06,13/Jul/23 08:12,03/Aug/21 01:34,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20267&view=logs&j=4dd4dbdd-1802-5eb7-a518-6acd9d24d0fc&t=8d6b4dd3-4ca1-5611-1743-57a7d76b395a&l=412

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20396&view=logs&j=4dd4dbdd-1802-5eb7-a518-6acd9d24d0fc&t=8d6b4dd3-4ca1-5611-1743-57a7d76b395a&l=2390

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20454&view=logs&j=4dd4dbdd-1802-5eb7-a518-6acd9d24d0fc&t=8d6b4dd3-4ca1-5611-1743-57a7d76b395a&l=371

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21228&view=logs&j=4dd4dbdd-1802-5eb7-a518-6acd9d24d0fc&t=7c4a8fb8-eeee-5a77-f518-4176bfae300b&l=2437

The test failed due to exceptions in logs. I executed the following command from flink-end-to-end-tests/test-scripts/common.sh on the logs, and it points to the RecipientUnreachableException in TM logs. The problem is that, TM received extra slot requests from RM after the tasks are finished and slots are freed, while the JobMaster it tried to offer slots to had already shutdown.

{code}
$ grep -rv ""GroupCoordinatorNotAvailableException"" . \
   | grep -v ""RetriableCommitFailedException"" \
   | grep -v ""NoAvailableBrokersException"" \
   | grep -v ""Async Kafka commit failed"" \
   | grep -v ""DisconnectException"" \
   | grep -v ""Cannot connect to ResourceManager right now"" \
   | grep -v ""AskTimeoutException"" \
   | grep -v ""WARN  akka.remote.transport.netty.NettyTransport"" \
   | grep -v  ""WARN  org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline"" \
   | grep -v 'INFO.*AWSErrorCode' \
   | grep -v ""RejectedExecutionException"" \
   | grep -v ""CancellationException"" \
   | grep -v ""An exception was thrown by an exception handler"" \
   | grep -v ""Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.exceptions.YarnException"" \
   | grep -v ""Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.conf.Configuration"" \
   | grep -v ""java.lang.NoClassDefFoundError: org/apache/hadoop/yarn/exceptions/YarnException"" \
   | grep -v ""java.lang.NoClassDefFoundError: org/apache/hadoop/conf/Configuration"" \
   | grep -v ""java.lang.Exception: Execution was suspended"" \
   | grep -v ""java.io.InvalidClassException: org.apache.flink.formats.avro.typeutils.AvroSerializer"" \
   | grep -v ""Caused by: java.lang.Exception: JobManager is shutting down"" \
   | grep -v ""java.lang.Exception: Artificial failure"" \
   | grep -v ""org.apache.flink.runtime.checkpoint.CheckpointException"" \
   | grep -v ""org.elasticsearch.ElasticsearchException"" \
   | grep -v ""Elasticsearch exception"" \
   | grep -v ""org.apache.flink.runtime.JobException: Recovery is suppressed"" \
   | grep -v ""WARN  akka.remote.ReliableDeliverySupervisor"" \
   | grep -i ""exception""
./flink-vsts-taskexecutor-0-fv-az217-107.log:org.apache.flink.runtime.rpc.exceptions.RecipientUnreachableException: Could not send message [RemoteFencedMessage(00000000000000000000000000000000, RemoteRpcInvocation(null.offerSlots(ResourceID, Collection, Time)))] from sender [Actor[akka.tcp://flink@10.1.0.175:38955/temp/$0b]] to recipient [Actor[akka://flink/user/rpc/jobmanager_2#1483449133]], because the recipient is unreachable. This can either mean that the recipient has been terminated or that the remote RpcService is currently not reachable.
{code}",,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21751,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 03 01:34:30 UTC 2021,,,,,,,,,,"0|z0tipc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/21 04:13;xtsong;The tests can be easily fixed by adding the `RecipientUnreachableException` to the whitelist. However, IIUC the problem should not happen after FLINK-21751. Maybe [~chesnay] can check if everything works as expected before we add it to the whitelist.;;;","02/Aug/21 09:40;chesnay;If I remember correctly then this behavior is expected. FLINK-21751 did not really fix the race condition; it just made sure the slots are released properly/faster than they usually would in the case of a JM shutdown.
This looks more of a side-effect of FLINK-23202 with which we actively listen for messages that could not be delivered.

I'd say it is fine to add it to the whitelist. I would imagine this can occur in any case during the shutdown of the cluster if at exactly that time some communication is initiated (e.g., heartbeats).;;;","02/Aug/21 11:27;xtsong;Thanks [~chesnay]. I'll add it to the whitelist.;;;","03/Aug/21 01:34;xtsong;Fixed via
- master (1.14): e45723b503b9cc793317a6cad7c0d4c8075c0d16;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The internal query-start options missed when convert exec graph to transformation,FLINK-23571,13392914,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,leonard,leonard,01/Aug/21 08:25,28/Aug/21 13:07,13/Jul/23 08:12,02/Aug/21 16:32,1.13.3,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"The internal query-start configuration options is missed when convert exec graph to transformation, please see:
{code:java}
// org.apache.flink.table.planner.delegation.PlannerBase

 translateToPlan(execGraph: ExecNodeGraph)

{code}",,jark,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 02 16:32:33 UTC 2021,,,,,,,,,,"0|z0tia0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/21 16:32;jark;Fixed in 
 - master: db68796883dce12fce47bd05f117ff2f101c8dd5
 - release-1.13: 2b486c5c88e5b5013c53cc629979316314a484cb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation lists incorrect scala suffixes,FLINK-23570,13392912,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,erickliu,erickliu,erickliu,01/Aug/21 07:42,28/Aug/21 13:10,13/Jul/23 08:12,06/Aug/21 05:42,1.13.0,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Documentation,,,,,0,pull-request-available,,,,"Some of the maven dependencies in the documentation seem to have some problems and cannot be used directly.

Page: DataStream Connectors -> File Sink/Streaming FIle Sink

!image-2021-08-01-15-26-34-346.png!

!image-2021-08-01-15-30-39-859.png!

 ",,erickliu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/21 07:26;erickliu;image-2021-08-01-15-26-34-346.png;https://issues.apache.org/jira/secure/attachment/13031324/image-2021-08-01-15-26-34-346.png","01/Aug/21 07:30;erickliu;image-2021-08-01-15-30-39-859.png;https://issues.apache.org/jira/secure/attachment/13031323/image-2021-08-01-15-30-39-859.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 06 05:42:55 UTC 2021,,,,,,,,,,"0|z0ti9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/21 05:42;chesnay;master: 84f4cc2ddf01057a50da8d64b8e3200138bdcfff
1.13: c1f39df092d9e261312eeb97795bc8ac5a7c530b ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
There is a spelling error in a word in the document,FLINK-23569,13392876,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,erickliu,erickliu,erickliu,31/Jul/21 16:47,28/Aug/21 13:07,13/Jul/23 08:12,03/Aug/21 10:55,1.13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Documentation,,,,,0,pull-request-available,,,,"I was browsing the documentation and found a word misspelled. This causes an exception when introducing libs in the pom.

!image-2021-08-01-00-43-40-919.png!

I checked the latest code and the problem still exists.

!image-2021-08-01-00-45-58-930.png!",,erickliu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/21 16:43;erickliu;image-2021-08-01-00-43-40-919.png;https://issues.apache.org/jira/secure/attachment/13031309/image-2021-08-01-00-43-40-919.png","31/Jul/21 16:46;erickliu;image-2021-08-01-00-45-58-930.png;https://issues.apache.org/jira/secure/attachment/13031308/image-2021-08-01-00-45-58-930.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 03 10:55:58 UTC 2021,,,,,,,,,,"0|z0ti1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/21 10:55;chesnay;master: babb692e63321296d8d1c8125418f3ed667ceda1
1.13: d89b39ddf7e760a93445d7e86215ad186fe1d715;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive 1.1.0 failed to write using flink sql 1.13.1 because the JSON class was not found,FLINK-23567,13392844,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lirui,wuyang09,wuyang09,31/Jul/21 02:39,28/Aug/21 13:07,13/Jul/23 08:12,03/Aug/21 03:06,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"*First：I added the flink-sql-connector-hive-1.2.2_2.11-1.13.1.jar under the Lib directory, the following error is prompted when publishing the task of Flink SQL：*

java.lang.NoClassDefFoundError: org/json/JSONException
 at org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.analyzeCreateTable(HiveParserDDLSemanticAnalyzer.java:646)
 at org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.analyzeInternal(HiveParserDDLSemanticAnalyzer.java:373)
 at org.apache.flink.table.planner.delegation.hive.HiveParser.processCmd(HiveParser.java:235)
 at org.apache.flink.table.planner.delegation.hive.HiveParser.parse(HiveParser.java:217)
 at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:724)
 at me.ddmc.bigdata.sqlsubmit.helper.SqlSubmitHelper.callSql(SqlSubmitHelper.java:201)
 at me.ddmc.bigdata.sqlsubmit.helper.SqlSubmitHelper.callCommand(SqlSubmitHelper.java:182)
 at me.ddmc.bigdata.sqlsubmit.helper.SqlSubmitHelper.run(SqlSubmitHelper.java:124)
 at me.ddmc.bigdata.sqlsubmit.SqlSubmit.main(SqlSubmit.java:34)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)

at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
 at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)
 at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:812)
 at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:246)
 at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1054)
 at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1132)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1754)
 at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
 at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1132)
 Caused by: java.lang.ClassNotFoundException: org.json.JSONException
 at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
 at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
 ... 25 

 

*Second:  After investigation, it is found that the exclude is added to the POM in the flink-sql-connector-hive-1.2.2 module, but other hive connectors are not.*

!image-2021-07-31-10-40-07-070.png!

*But I didn't understand this remark. Is this a problem？*

 

 

 ",,lirui,wuyang09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/21 02:39;wuyang09;image-2021-07-31-10-39-52-126.png;https://issues.apache.org/jira/secure/attachment/13031300/image-2021-07-31-10-39-52-126.png","31/Jul/21 02:40;wuyang09;image-2021-07-31-10-40-07-070.png;https://issues.apache.org/jira/secure/attachment/13031301/image-2021-07-31-10-40-07-070.png","02/Aug/21 05:55;wuyang09;image-2021-08-02-13-55-26-467.png;https://issues.apache.org/jira/secure/attachment/13031361/image-2021-08-02-13-55-26-467.png","02/Aug/21 06:00;wuyang09;image-2021-08-02-14-00-26-096.png;https://issues.apache.org/jira/secure/attachment/13031364/image-2021-08-02-14-00-26-096.png","02/Aug/21 06:24;wuyang09;image-2021-08-02-14-24-36-210.png;https://issues.apache.org/jira/secure/attachment/13031365/image-2021-08-02-14-24-36-210.png",,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 03 03:07:15 UTC 2021,,,,,,,,,,"0|z0thug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/21 02:01;wuyang09;[~lirui] Please explain this filter comment,thanks;;;","02/Aug/21 03:29;lirui;[~wuyang09] The filter is there because the JSON dependency's license doesn't allow us to include it in our distribution. Could you share the DDL that triggers this issue?;;;","02/Aug/21 05:59;wuyang09;[~lirui]  Like the screenshot, but I removed this filter and repacked it and it succeeded.

CREATE CATALOG hive_catalog WITH (
 'type' = 'hive',
 'default-database' = 'tmp',
 'hive-conf-dir' = '/etc/hive/conf'
 );
 – set the HiveCatalog as the current catalog of the session
 USE CATALOG hive_catalog;

– 要指定 hive 方言，不然 hive 表创建不成功
 SET table.sql-dialect=hive; 
 drop table if exists tmp.flink_sql_sink_hive_hi;
 CREATE TABLE `tmp.flink_sql_sink_hive_hi`(
 `log_timestamp` BIGINT COMMENT '事件时间',
 `name` STRING COMMENT '姓名',
 `age` STRING COMMENT '年龄',
 `sex` STRING COMMENT '性别',
 `hometown` STRING COMMENT '家乡',
 `work` STRING COMMENT '工作'
 )PARTITIONED BY (`dt` STRING COMMENT '天',`hh` STRING COMMENT '小时') STORED AS PARQUET LOCATION 'hdfs://ddNS/user/hive/warehouse/tmp.db/flink_sql_sink_hive_hi' 
 TBLPROPERTIES (
 --'partition.time-extractor.timestamp-pattern'='$dt $hh', – hive 分区提取器
 'sink.partition-commit.trigger'='partition-time', – 分区触发提交 
 'sink.partition-commit.delay'='1 h', – 提交延迟
 'sink.partition-commit.policy.kind'='metastore,success-file' – 提交类型
 );

!image-2021-08-02-14-24-36-210.png!

!image-2021-08-02-14-00-26-096.png!;;;","03/Aug/21 03:06;lirui;Fixed in master: 48cd889b1de39080ad883da982287faf904bc520
Fixed in release-1.13: ef752202a32ee4c308d37d28e0163c9c7bec4fc6;;;","03/Aug/21 03:07;lirui;[~wuyang09] I have pushed a fix for this issue. Please try it out and feel free to reopen this ticket if the issue persists.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mysql 8.0  Public Key Retrieval is not allowed,FLINK-23566,13392782,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mrbiot2021,mrbiot2021,30/Jul/21 14:43,16/Aug/21 08:55,13/Jul/23 08:12,16/Aug/21 08:55,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,mysql 8.0 这个问题怎么解决呀,,jark,mrbiot2021,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jul 31 06:30:48 UTC 2021,,,,,,,,,,"0|z0thgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jul/21 06:30;jark;[~mrbiot2021], please use English in description and please add more information what exception you meeted or what feature you are looking for. From the title and description, we can't know what you want. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression on 29.07.2021,FLINK-23560,13392703,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,akalashnikov,pnowojski,pnowojski,30/Jul/21 09:11,28/Aug/21 13:11,13/Jul/23 08:12,16/Aug/21 09:03,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Benchmarks,,,,,0,pull-request-available,,,,"http://codespeed.dak8s.net:8000/timeline/?ben=remoteFilePartition&env=2
http://codespeed.dak8s.net:8000/timeline/?ben=uncompressedMmapPartition&env=2
http://codespeed.dak8s.net:8000/timeline/?ben=compressedFilePartition&env=2
http://codespeed.dak8s.net:8000/timeline/?ben=tupleKeyBy&env=2
http://codespeed.dak8s.net:8000/timeline/?ben=arrayKeyBy&env=2
http://codespeed.dak8s.net:8000/timeline/?ben=uncompressedFilePartition&env=2
http://codespeed.dak8s.net:8000/timeline/?ben=sortedTwoInput&env=2
http://codespeed.dak8s.net:8000/timeline/?ben=sortedMultiInput&env=2
http://codespeed.dak8s.net:8000/timeline/?ben=globalWindow&env=2
(And potentially other benchmarks)",,akalashnikov,pnowojski,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23452,,,,,,,,,,,,,,,,,,,,"30/Jul/21 13:51;pnowojski;Screenshot 2021-07-30 at 15.46.54.png;https://issues.apache.org/jira/secure/attachment/13031282/Screenshot+2021-07-30+at+15.46.54.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 16 09:01:49 UTC 2021,,,,,,,,,,"0|z0tgzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jul/21 09:15;pnowojski;
{noformat}
git ls c3088af325..230f659e79
230f659e790 [3 days ago] [FLINK-23452][streaming] Integration ThroughputCalculator in StreamTask for the calculation of the subtask level throughput [Anton Kalashnikov]
29d7bcd2bb3 [3 days ago] [hotfix] Stopping the StreamTask#systemTimerService explicitly after the invoke [Anton Kalashnikov]
45890c0a001 [3 days ago] [FLINK-23452][core] Added specific configuration for the calculation of the throughput [Anton Kalashnikov]
fe26c9d0644 [3 days ago] [FLINK-23452][runtime] Added extra classes for the ability to calculate throughput [Anton Kalashnikov]
3cbdd783e4e [3 days ago] [FLINK-23452][refactor] Extracted PeriodTimer interface for the abstraction of idle/backpressure time management [Anton Kalashnikov]
5c475d41fea [2 weeks ago] [FLINK-23354][blob] Limit the size of ShuffleDescriptors in PermanentBlobCache on TaskExecutor [Thesharing]
d879d3e46bf [25 hours ago] [FLINK-23517][build] Add error messages for bannedDependencies rules [Chesnay Schepler]
{noformat}

CC [~akalashnikov];;;","30/Jul/21 13:49;pnowojski;I've commented out whole content of the {{StreamTask#throughputCalculationSetup()}} and results were back to normal on the benchmark request machine. So it's indeed caused by FLINK-23452. 

 !Screenshot 2021-07-30 at 15.46.54.png|width=600! 
;;;","04/Aug/21 10:12;akalashnikov;[~pnowojski] My conclusion why it happens:

First of all, there is a little java theory: if a synchronization block is used in only one thread it works more effectively rather than it would be used in different threads. It happens because when only one thread is able to own the mutex the information about this owner thread is located inside of the header of this object but when the several threads want to own the object then the java create the table with this information in a separate place which requires extra hops on each synchronization.

One more important notice is the all problematic benchmarks don't have any timers or checkpoints. So it mostly has only one thread during the execution. More precisely: *LegacySourceFunctionThread* does main execution(evict the data under the checkpoint lock) and *MailboxProcessorThread* do nothing until *LegacySourceFunctionThread* is finished and then *MailboxProcessorThread* just finishes the task.

FLINK-23452 has introduced the action which is submitted to the mailbox for execution. It is not really important what is this task doing(performance drop reproduces even for empty action). More important here is that during the *LegacySourceFunctionThread* does its job(emitting records under checkpoint lock synchronization)*,* the *MailboxProcessorThread* wakes up and executes the action via *SynchronizedStreamTaskActionExecutor*(synchronized version of *StreamTaskActionExecutor*) which uses for synchronization the same checkpoint lock which uses *LegacySourceFunctionThread*. So as soon as *MailboxProcessorThread* takes the lock for the first time  *LegacySourceFunctionThread* becomes slower because of more expensive synchronization which I mention at the beginning.

I proved the above-described assumption by adding the simple code before the execution of mainOperator inside of SourceStreamTask.LegacySourceFunctionThread#run(in fact it can be added to any place in the code):

 
{code:java}
new Thread(() -> {
   synchronized (lock) {
   }
}).start();
{code}
These small changes lead to the same degradation as FLINK-23452

 

In general, it means that currently, our benchmarks are not so relevant because in real cases we usually use the checkpoint or timeService which  weren't used for these benchmarks. But also we can turn off the feature from FLINK-23452 for sources(input gates == 0) because, in fact, the calculation of the throughput for sources doesn't make sense.

 ;;;","04/Aug/21 10:26;pnowojski;Great analysis and good find [~akalashnikov].
{quote}
In general, it means that currently, our benchmarks are not so relevant because in real cases we usually use the checkpoint or timeService which  weren't used for these benchmarks.
{quote}
This is not true for batch jobs, where are no timers and checkpointing is disabled. In those cases performance drop caught by this JIRA ticket would have been visible to the users. But indeed we can:
{quote}
But also we can turn off the feature from FLINK-23452 for sources(input gates == 0) because, in fact, the calculation of the throughput for sources doesn't make sense.
{quote}
as this should solve the problem.;;;","06/Aug/21 07:15;pnowojski;merged to master as a8db36bf07e;;;","06/Aug/21 15:21;pnowojski;The issue in 
http://codespeed.dak8s.net:8000/timeline/?ben=sortedTwoInput&env=2
http://codespeed.dak8s.net:8000/timeline/?ben=sortedMultiInput&env=2
still persists. It looks like there were two different performance regressions in the same commit range. Which makes sense, because my and [~akalashnikov]'s fix for the first regression couldn't explain those {{sortedInput}} benchmarks as:
# those benchmarks are so slow (2M records/s) and involve spilling to disk, that any regression around enqueuing mailbox action shouldnt’ be visible
# those benchmarks are using FLIP-27 sources, so no problem with biased locking as described by [~akalashnikov];;;","16/Aug/21 09:01;ym;closed with

merged commit [{{a8db36b}}|https://github.com/apache/flink/commit/a8db36bf07eb475434da8215762f5129f55c5b53] into apache:master 

merged commit [{{6d89c1d}}|https://github.com/apache/flink/commit/6d89c1de833ddde82d85826e428242c74de77034] into apache:master 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E2e tests fail because of quiesced system timers service,FLINK-23558,13392691,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,akalashnikov,dwysakowicz,dwysakowicz,30/Jul/21 07:56,09/Nov/21 11:30,13/Jul/23 08:12,05/Aug/21 03:29,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Task,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21180&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=2a8cc459-df7a-5e6f-12bf-96efcc369aa9&l=10484

{code}
Jul 29 21:41:15 Caused by: org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailbox$MailboxClosedException: Mailbox is in state QUIESCED, but is required to be in state OPEN for put operations.
Jul 29 21:41:15 	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.checkPutStateConditions(TaskMailboxImpl.java:269) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
Jul 29 21:41:15 	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.put(TaskMailboxImpl.java:197) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
Jul 29 21:41:15 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.execute(MailboxExecutorImpl.java:74) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
Jul 29 21:41:15 	at org.apache.flink.runtime.mailbox.MailboxExecutor.submit(MailboxExecutor.java:163) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
Jul 29 21:41:15 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$throughputCalculationSetup$3(StreamTask.java:688) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
Jul 29 21:41:15 	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService$ScheduledTask.run(SystemProcessingTimeService.java:317) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
Jul 29 21:41:15 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_302]
Jul 29 21:41:15 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_302]
Jul 29 21:41:15 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_302]
Jul 29 21:41:15 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) ~[?:1.8.0_302]
Jul 29 21:41:15 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_302]
Jul 29 21:41:15 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_302]
{code}",,dwysakowicz,pnowojski,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23575,FLINK-23665,,,,,,FLINK-23452,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 05 03:29:51 UTC 2021,,,,,,,,,,"0|z0tgwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jul/21 07:57;dwysakowicz;cc [~pnowojski] [~akalashnikov];;;","30/Jul/21 08:04;dwysakowicz;Could it be related to 29d7bcd2bb3347caec25b3aa04b508968ac2a4cd FLINK-23452;;;","03/Aug/21 08:07;dwysakowicz;Copied over from the duplicate:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21255&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=10654
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21246&view=logs&j=d8d26c26-7ec2-5ed2-772e-7a1a1eb8317c&t=ec8797b0-5eee-5a0e-f936-8db65cff44cc&l=10540
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21246&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=4938;;;","05/Aug/21 03:29;ym;merged commit [{{fb2d690}}|https://github.com/apache/flink/commit/fb2d690f2c91c459e3a8ae1fa4b9c84ea4198956] into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SQLClientSchemaRegistryITCase fails with "" Subject ... not found""",FLINK-23556,13392685,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,bgeng777,dwysakowicz,dwysakowicz,30/Jul/21 07:37,31/Aug/21 06:41,13/Jul/23 08:12,31/Aug/21 06:41,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Table SQL / Ecosystem,,,,,0,pull-request-available,stale-blocker,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21129&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=cc5499f8-bdde-5157-0d76-b6528ecd808e&l=25337
{code}
Jul 28 23:37:48 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 209.44 s <<< FAILURE! - in org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase
Jul 28 23:37:48 [ERROR] testWriting(org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase)  Time elapsed: 81.146 s  <<< ERROR!
Jul 28 23:37:48 io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException: Subject 'test-user-behavior-d18d4af2-3830-4620-9993-340c13f50cc2-value' not found.; error code: 40401
Jul 28 23:37:48 	at io.confluent.kafka.schemaregistry.client.rest.RestService.sendHttpRequest(RestService.java:292)
Jul 28 23:37:48 	at io.confluent.kafka.schemaregistry.client.rest.RestService.httpRequest(RestService.java:352)
Jul 28 23:37:48 	at io.confluent.kafka.schemaregistry.client.rest.RestService.getAllVersions(RestService.java:769)
Jul 28 23:37:48 	at io.confluent.kafka.schemaregistry.client.rest.RestService.getAllVersions(RestService.java:760)
Jul 28 23:37:48 	at io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient.getAllVersions(CachedSchemaRegistryClient.java:364)
Jul 28 23:37:48 	at org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase.getAllVersions(SQLClientSchemaRegistryITCase.java:230)
Jul 28 23:37:48 	at org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase.testWriting(SQLClientSchemaRegistryITCase.java:195)
Jul 28 23:37:48 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 28 23:37:48 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 28 23:37:48 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 28 23:37:48 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 28 23:37:48 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jul 28 23:37:48 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jul 28 23:37:48 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jul 28 23:37:48 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jul 28 23:37:48 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
Jul 28 23:37:48 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
Jul 28 23:37:48 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Jul 28 23:37:48 	at java.lang.Thread.run(Thread.java:748)
Jul 28 23:37:48 

{code}",,akalashnikov,bgeng777,dwysakowicz,gaoyunhaii,jark,pnowojski,roman,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23666,,,,,,,,,,,FLINK-20410,FLINK-20498,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 31 06:40:26 UTC 2021,,,,,,,,,,"0|z0tgvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/21 09:40;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21335&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=25340;;;","04/Aug/21 02:40;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21438&view=logs&j=4dd4dbdd-1802-5eb7-a518-6acd9d24d0fc&t=7c4a8fb8-eeee-5a77-f518-4176bfae300b&l=15539;;;","04/Aug/21 12:58;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21478&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=24526;;;","05/Aug/21 01:55;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21522&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=25251;;;","06/Aug/21 03:04;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21579&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=25185;;;","06/Aug/21 03:06;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21584&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=25156;;;","06/Aug/21 03:07;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21589&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=24594;;;","06/Aug/21 03:07;xtsong;Upgrade to Blocker due to frequency of failure.;;;","06/Aug/21 03:14;xtsong;[~jark], any idea who would make a best candidate for this?;;;","06/Aug/21 16:37;pnowojski;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21671&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c;;;","07/Aug/21 12:30;roman;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21676&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=24045];;;","08/Aug/21 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","09/Aug/21 02:52;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21711&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=cc5499f8-bdde-5157-0d76-b6528ecd808e&l=25258;;;","09/Aug/21 09:30;xtsong;Offline reached out to @bgeng777 and he's willing to help look into this.;;;","10/Aug/21 01:45;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21790&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=75f4c82e-ad02-5844-81c9-d16399e3372d&l=25449;;;","10/Aug/21 15:48;bgeng777;I have read the test codes and I doubt that the port in the `avro-confluent.url` may be wrong due to the port mapping of the container. I will discuss with [~renqs] to verify the guess.;;;","10/Aug/21 16:28;akalashnikov;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21826&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=23940;;;","12/Aug/21 06:03;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21916&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c];;;","13/Aug/21 02:32;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21986&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=24948;;;","14/Aug/21 07:34;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22139&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=24225];;;","14/Aug/21 09:02;pnowojski;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22139&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c;;;","14/Aug/21 09:21;pnowojski;Test disabled on master via 2957818d645;;;","16/Aug/21 02:58;xtsong;Before disabled:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22066&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=24583

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22150&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=24885

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22168&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=25026

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22179&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=24674;;;","17/Aug/21 08:15;xtsong;[~bgeng777], I've linked two more tickets that might be related to this, see if they provide any useful insights.;;;","17/Aug/21 16:08;bgeng777;hi [~xtsong], thanks a lot for your support! I have dived into this IT case for some days and create a [PR|[https://github.com/apache/flink/pull/16864]] to make it more stable. But my current fix does not actually guarantee the success of this case. It should just make it more stable.

My summary:
 * AFAIK, the logic and implementation of this case is correct.
 * Due to the complexity of this case, the running time of this IT case should not be limited to 120 seconds. Due to my own tests, even only the running time of the internal flink job can exceed the limit easily.
 * I believe the failure of this case should not be caused by flink. Instead, I doubt the network issue of test containers(i.e. a kafka container, a flink container and a schemaRegistry container) in this case is more likely be the root cause. But I need more time to verify this guess.

 ;;;","18/Aug/21 02:06;xtsong;Thanks for the updates, [~bgeng777].

Do you need a review for the PR, or you want o spend more time confirming your hypothesis？;;;","18/Aug/21 16:09;bgeng777;Hi, [~xtsong] . I may need one more day to verify the PR after today's discussion with [~renqs] . 
 Once I think it is ready, I will contact you for a review.

Thanks!;;;","19/Aug/21 02:20;xtsong;Sounds good, thanks.
BTW, could you move the ticket to in-progress?;;;","22/Aug/21 02:40;bgeng777;hi [~xtsong], I think my pull request is ready. I tried 3 times of e2e tests and this case works fine. I would appreciate a lot if you can help me to find a reviewer for it. Thanks! 

The pr link: [https://github.com/apache/flink/pull/16952];;;","25/Aug/21 03:52;jark;Fixed in maste: 92e65333cc7ecbd7886f6a348c3f21d3fe85a942

This should only improve the stable of the test and adds logs, but may still fail. 
We can close this issue, if not failed for some days. ;;;","31/Aug/21 06:40;xtsong;It has been about one week without new instance reported. I'm closing this ticket for now.
Thanks [~bgeng777] and [~jark] for fixing this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlCli throws an exception and hang ,FLINK-23554,13392668,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,fsk119,jingzhang,jingzhang,30/Jul/21 06:15,31/Jan/23 07:36,13/Jul/23 08:12,31/Jan/23 07:36,1.11.3,1.12.4,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Client,,,,,0,,,,,"SqlCli would throws an exception like the following, and SqlCli would hang forever until kill the process outside.

You could reproduce the exception by the following step:
 # submit a SQL command in SQLCli, (for example {{SELECT 'Hello World';}})
 # does not wait for it response, input another SQL command in SQL Cli (for example:  {{SET execution.result-mode=table;}})
 # an exception would be thrown out, and SQLCli would hang

!image-2021-07-30-14-12-07-817.png|width=1706,height=527!",,fsk119,jark,jingzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/21 06:12;jingzhang;image-2021-07-30-14-12-07-817.png;https://issues.apache.org/jira/secure/attachment/13031239/image-2021-07-30-14-12-07-817.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 07:35:42 UTC 2023,,,,,,,,,,"0|z0tgrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jul/21 06:27;fsk119;Thanks for [~qingru zhang]'s report. I will fix it soon. :);;;","31/Jan/23 07:35;fsk119;I test this with the latest sql client and sql cilent exits without hanging. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StateFun does not execute integration tests,FLINK-23547,13392567,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sjwiesman,sjwiesman,sjwiesman,29/Jul/21 15:46,02/Aug/21 14:44,13/Jul/23 08:12,02/Aug/21 14:44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,statefun-3.1.0,,,,,,,,,,0,pull-request-available,,,,"Stateful Functions is missing the surefire maven plugin and integration tests, marked with ITCase suffix, are not executed on install. ",,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 02 14:44:09 UTC 2021,,,,,,,,,,"0|z0tg54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/21 14:44;sjwiesman;fixed in master: 99ada68b9d65c18e9f59f35e6e8147946c2c699a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stop-cluster.sh produces warning on macOS 11.4,FLINK-23546,13392555,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,rmetzger,rmetzger,rmetzger,29/Jul/21 14:32,28/Aug/21 13:06,13/Jul/23 08:12,30/Jul/21 17:50,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Deployment / Scripts,,,,,0,pull-request-available,,,,"Since FLINK-17470, we are stopping daemons with a timeout, to SIGKILL them if they are not gracefully stopping.

I noticed that this mechanism causes warnings on macOS:

{code}
❰robert❙/tmp/flink-1.14-SNAPSHOT❱✔≻ ./bin/start-cluster.sh
Starting cluster.
Starting standalonesession daemon on host MacBook-Pro-2.localdomain.
Starting taskexecutor daemon on host MacBook-Pro-2.localdomain.
❰robert❙/tmp/flink-1.14-SNAPSHOT❱✔≻ ./bin/stop-cluster.sh
Stopping taskexecutor daemon (pid: 50044) on host MacBook-Pro-2.localdomain.
tail: illegal option -- -
usage: tail [-F | -f | -r] [-q] [-b # | -c # | -n #] [file ...]
Stopping standalonesession daemon (pid: 49812) on host MacBook-Pro-2.localdomain.
tail: illegal option -- -
usage: tail [-F | -f | -r] [-q] [-b # | -c # | -n #] [file ...]
{code}
",,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 30 17:50:19 UTC 2021,,,,,,,,,,"0|z0tg2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/21 14:33;rmetzger;The error is probably coming from here: https://github.com/apache/flink/blame/master/flink-dist/src/main/flink-bin/bin/flink-daemon.sh#L100;;;","30/Jul/21 17:50;rmetzger;Merged to master in https://github.com/apache/flink/commit/3b115544b04572831e162288097105c63ca5e048
merged to release-1.13 in https://github.com/apache/flink/commit/d5bf26448780d2bfc3ec4db28c8f8c91b1435487;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot build succesfully flink runtime project,FLINK-23540,13392478,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,yulei0824,yulei0824,29/Jul/21 07:37,29/Jul/21 09:12,13/Jul/23 08:12,29/Jul/21 09:12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Network,,,,,0,,,,,"Using ""mvn clean -DskipTests -Drat.skip=true install"" under the flink parent project after pulling latest code on master branch, I encounter the following error:

[WARNING] Rule 0: org.apache.maven.plugins.enforcer.BannedDependencies failed with message:
Found Banned Dependency: org.clapper:grizzled-slf4j_2.12:jar:1.3.2
Found Banned Dependency: org.scala-lang.modules:scala-parser-combinators_2.12:jar:1.1.2
Found Banned Dependency: com.typesafe.akka:akka-stream_2.12:jar:2.6.15
Found Banned Dependency: com.typesafe.akka:akka-pki_2.12:jar:2.6.15
Found Banned Dependency: com.typesafe.akka:akka-actor_2.12:jar:2.6.15
Found Banned Dependency: com.typesafe.akka:akka-protobuf-v3_2.12:jar:2.6.15
Found Banned Dependency: com.typesafe:ssl-config-core_2.12:jar:0.4.2
Found Banned Dependency: org.scala-lang.modules:scala-java8-compat_2.12:jar:0.8.0
Found Banned Dependency: com.typesafe.akka:akka-slf4j_2.12:jar:2.6.15
Found Banned Dependency: com.typesafe.akka:akka-remote_2.12:jar:2.6.15

Use 'mvn dependency:tree' to locate the source of the banned dependencies.
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Flink : 1.14-SNAPSHOT:
[INFO]
[INFO] Flink : ............................................ SUCCESS [02:06 min]
[INFO] Flink : Annotations ................................ SUCCESS [ 13.750 s]
[INFO] Flink : Test utils : ............................... SUCCESS [ 0.729 s]
[INFO] Flink : Test utils : Junit ......................... SUCCESS [ 9.651 s]
[INFO] Flink : Metrics : .................................. SUCCESS [ 0.657 s]
[INFO] Flink : Metrics : Core ............................. SUCCESS [ 6.995 s]
[INFO] Flink : Core ....................................... SUCCESS [01:45 min]
[INFO] Flink : Java ....................................... SUCCESS [ 24.425 s]
[INFO] Flink : Scala ...................................... SUCCESS [02:03 min]
[INFO] Flink : FileSystems : .............................. SUCCESS [ 0.826 s]
[INFO] Flink : FileSystems : Hadoop FS .................... SUCCESS [ 12.479 s]
[INFO] Flink : FileSystems : Mapr FS ...................... SUCCESS [ 3.243 s]
[INFO] Flink : FileSystems : Hadoop FS shaded ............. SUCCESS [ 16.769 s]
[INFO] Flink : FileSystems : S3 FS Base ................... SUCCESS [ 6.317 s]
[INFO] Flink : FileSystems : S3 FS Hadoop ................. SUCCESS [ 15.672 s]
[INFO] Flink : FileSystems : S3 FS Presto ................. SUCCESS [ 20.762 s]
[INFO] Flink : FileSystems : OSS FS ....................... SUCCESS [ 13.750 s]
[INFO] Flink : FileSystems : Azure FS Hadoop .............. SUCCESS [ 34.133 s]
[INFO] Flink : RPC : ...................................... SUCCESS [ 0.521 s]
[INFO] Flink : RPC : Core ................................. SUCCESS [ 2.976 s]
[INFO] Flink : RPC : Akka ................................. SUCCESS [ 46.822 s]
[INFO] Flink : RPC : Akka-Loader .......................... SUCCESS [ 7.125 s]
[INFO] Flink : Queryable state : .......................... SUCCESS [ 0.522 s]
[INFO] Flink : Queryable state : Client Java .............. SUCCESS [ 4.074 s]
{color:#de350b}[INFO] Flink : Runtime .................................... FAILURE [02:06 min]{color}",,martijnvisser,yulei0824,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jul/21 08:40;yulei0824;ssmaster.PNG;https://issues.apache.org/jira/secure/attachment/13031194/ssmaster.PNG",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 29 09:10:41 UTC 2021,,,,,,,,,,"0|z0tflc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/21 07:52;yulei0824;Hi [~chesnay], does the FLINK-18783 cause this problem ?;;;","29/Jul/21 08:03;martijnvisser;Most probably this is related to https://issues.apache.org/jira/browse/FLINK-23534;;;","29/Jul/21 08:12;chesnay;I can't reproduce the issue locally, so I'd suggest to make sure you really are on the latest master.;;;","29/Jul/21 08:41;yulei0824;Hi [~chesnay]，

!ssmaster.PNG!;;;","29/Jul/21 08:47;chesnay;so it's not the latest master then. https://github.com/apache/flink/commits/master

Make sure you have this commit: abf9752c9de3810e10b9f7bbb937cbdf288d2789;;;","29/Jul/21 09:10;yulei0824;Hi [~chesnay], this issue has been fixed in latest master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InfluxDBReporter should filter characters in tags,FLINK-23539,13392471,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yulei0824,yulei0824,yulei0824,29/Jul/21 06:38,15/Dec/21 01:40,13/Jul/23 08:12,04/Aug/21 07:03,1.10.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.0,,,Runtime / Metrics,,,,,0,pull-request-available,,,,"I find that the job cannot report any metrics to influxdb when a SQL statement contains '\n', because '\n' has the special meaning to influxdb .

From time to time, many strange tables that use partial SQL statement as name are created in influxdb. ",,jark,yulei0824,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23618,FLINK-23628,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 04 07:03:02 UTC 2021,,,,,,,,,,"0|z0tfjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/21 06:58;yulei0824;Hi [~jark],  should we replace '\n' that is contained by the job, task and operator name with ‘ ’ before reporting any metrics to external system ?;;;","29/Jul/21 07:14;jark;I think this should be normalized by metric registry in flink runtime. cc [~chesnay];;;","29/Jul/21 07:42;chesnay;This should be handled by the reporter; I'll adjust the ticket accordingly.;;;","29/Jul/21 07:53;yulei0824;Hi [~chesnay], could you please assign this to me ?;;;","04/Aug/21 07:03;chesnay;master: 1c7100a8e528d620c4ce77673630f8bf8f8e6966
1.13: 9cbf65692aa388995cfded8429f76536d12445ec
1.12: 14bdf8acfaaebbd4736a449a380e8c41569b0fb5 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Banned dependencies in flink-statebackend-changelog,FLINK-23534,13392457,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,dwysakowicz,dwysakowicz,29/Jul/21 05:03,23/Sep/21 18:10,13/Jul/23 08:12,29/Jul/21 13:23,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21129&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7&l=34482

{code}
[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-versions) @ flink-statebackend-changelog ---
[WARNING] Rule 0: org.apache.maven.plugins.enforcer.BannedDependencies failed with message:
Found Banned Dependency: org.apache.flink:flink-streaming-java_2.11:jar:1.14-SNAPSHOT
Found Banned Dependency: org.apache.flink:flink-streaming-java_2.11:test-jar:tests:1.14-SNAPSHOT
Found Banned Dependency: com.twitter:chill_2.11:jar:0.7.6
Found Banned Dependency: org.apache.flink:flink-scala_2.11:jar:1.14-SNAPSHOT
Found Banned Dependency: org.apache.flink:flink-dstl-dfs_2.11:jar:1.14-SNAPSHOT
Use 'mvn dependency:tree' to locate the source of the banned dependencies.

{code}",,dwysakowicz,roman_old,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21352,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 29 13:23:58 UTC 2021,,,,,,,,,,"0|z0tfgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/21 05:03;dwysakowicz;cc [~roman_khachatryan] [~ym];;;","29/Jul/21 06:51;ym;It is very likely because of [FLINK-21353][tests] Use FS store in ChangelogBackend tests

[~roman_khachatryan];;;","29/Jul/21 13:23;roman;Thanks for reporting,

The fix merged into master as 6f441284e81ee7174c578035bcb67d8b3bcf7b88.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stop-with-savepoint can fail with FlinkKinesisConsumer,FLINK-23528,13392317,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,kdziolak,pnowojski,pnowojski,28/Jul/21 10:28,22/Feb/23 10:37,13/Jul/23 08:12,25/Jul/22 09:01,1.11.3,1.12.4,1.13.1,1.14.3,1.15.1,,,,,,,,,,,,,,,,,,,,,,,,,1.15.2,1.16.0,,,,Connectors / Kinesis,,,,,1,pull-request-available,stale-assigned,,,"{{FlinkKinesisConsumer#cancel()}}  (inside {{KinesisDataFetcher#shutdownFetcher()}}) shouldn't be interrupting source thread. Otherwise, as described in FLINK-23527, network stack can be left in an invalid state and downstream tasks can encounter deserialisation errors.",,danderson,dannycranmer,pnowojski,qinjunjerry,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23538,,FLINK-29015,,,,,,,,,FLINK-21028,,,,FLINK-23527,FLINK-31183,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 25 09:00:57 UTC 2022,,,,,,,,,,"0|z0telk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Aug/21 09:01;arvid;Merged into master as 273dce5b030e12dd3d7bebb2f51036a198d07112.;;;","23/Aug/21 10:27;pnowojski;I think one test should be re-enabled https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kinesis/src/test/java/org/apache/flink/streaming/connectors/kinesis/FlinkKinesisITCase.java#L56 ;;;","01/Sep/21 08:25;arvid;Thank you for reminding me. I'll try to run it locally a few hundred times.;;;","08/Nov/21 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","23/Feb/22 08:48;dannycranmer;[~arvid] any update on this? I have removed the 1.15 fix version, feel free to revert if this is on target;;;","28/Jun/22 07:59;arvid;Hi [~dannycranmer], sorry for the late reply. The main problem with completing the ticket was the lack of test coverage for EFO. See your comment in https://github.com/apache/flink/pull/17189#discussion_r705274129.
I'm positive that the PR works for non-EFO cases.;;;","08/Jul/22 10:14;dannycranmer;Hey [~arvid], we are finishing up this issue, but have a question around one of the changes you made in the original PR:
- https://github.com/apache/flink/pull/17189/commits/3bae510b7af508ecf7c0569df34c307cacf08c8f

This change looks a little out of place in this PR, do you remember why you made this change?;;;","08/Jul/22 16:01;arvid;As far as I remember, I needed to fix this for the tests to succeed reliably. ;;;","25/Jul/22 09:00;dannycranmer;Merged:
- 1.15.x: [7cd5fe9|https://github.com/apache/flink/commit/7cd5fe93035c5d875d10d9f3cb1d84d4ed045dc0]
- master: [4bbf319|https://github.com/apache/flink/commit/4bbf3194dc3816dcaa6a76ed7adf0fd228b16293];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clarify `SourceFunction#cancel()` contract about interrupting,FLINK-23527,13392309,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sewen,pnowojski,pnowojski,28/Jul/21 10:06,14/Mar/23 12:38,13/Jul/23 08:12,07/Sep/21 15:30,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,API / DataStream,Documentation,,,,0,pull-request-available,,,,"We should clarify the contract of {{SourceFunction#cancel()}}

# source itself shouldn’t be interrupting the source thread
# interrupt shouldn’t be expected in the clean cancellation case

Interrupting the code on the clean shutdown path can cause failures when doing `stop-with-savepoint`. If source thread is interrupted during backpressure, this leaves network stack in invalid state, making it impossible to send {{EndOfPartitionEvent}} to complete the shutdown.

In a bit more detail, when source thread is backpressured, network stack might have already sent a partial record and it could be waiting for a buffer to finish writing/serialising that record. If network stack is interrupted while waiting for that buffer, it will never resume writing/serialisation of the remaining part of that record, while downstream node will be expecting those bytes. If in this situation we attempt to emit anything (like {{EndOfPartitionEvent}}), this will most likely cause deserialisation errors on the downstream nodes.",,pnowojski,qinjunjerry,sewen,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23528,FLINK-21028,,,FLINK-24182,FLINK-28758,,,,,,,,,,,"10/Nov/21 10:31;master_preferences;https://issues.apache.org/jira/secure/attachment/13035910/master_preferences",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 07 15:30:43 UTC 2021,,,,,,,,,,"0|z0tejs:",9223372036854775807,"Contract of the SourceFunction#cancel() method with respect to interruptions has been clarified:
- source itself shouldn’t be interrupting the source thread
- interrupt shouldn’t be expected in the clean cancellation case",,,,,,,,,,,,,,,,,,,"07/Sep/21 15:30;sewen;Fixed in
 - master (1.15.0) via 9136c4030baba575cfcccef5558a9f2b079bdc23
 - release-1.14 (1.14.0) via c56ecf3e0acb917f3e302eda7c5f92b73a881c76
 - release-1.13 (1.13.3) via 57efc6980c4a2792e73b270a8dd39d95c9ed6de6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveSchedulerClusterITCase fails on Azure,FLINK-23524,13392257,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,dwysakowicz,dwysakowicz,28/Jul/21 06:34,14/Mar/22 11:05,13/Jul/23 08:12,04/Aug/21 06:59,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21053&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=8177

{code}
Jul 27 22:09:39 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 19.558 s <<< FAILURE! - in org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase
Jul 27 22:09:39 [ERROR] testAutomaticScaleUp(org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase)  Time elapsed: 14.541 s  <<< FAILURE!
Jul 27 22:09:39 java.lang.AssertionError: expected:<6> but was:<7>
Jul 27 22:09:39 	at org.junit.Assert.fail(Assert.java:89)
Jul 27 22:09:39 	at org.junit.Assert.failNotEquals(Assert.java:835)
Jul 27 22:09:39 	at org.junit.Assert.assertEquals(Assert.java:647)
Jul 27 22:09:39 	at org.junit.Assert.assertEquals(Assert.java:633)
Jul 27 22:09:39 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.testAutomaticScaleUp(AdaptiveSchedulerClusterITCase.java:121)
Jul 27 22:09:39 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 27 22:09:39 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 27 22:09:39 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 27 22:09:39 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 27 22:09:39 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jul 27 22:09:39 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jul 27 22:09:39 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jul 27 22:09:39 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jul 27 22:09:39 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Jul 27 22:09:39 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jul 27 22:09:39 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Jul 27 22:09:39 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 27 22:09:39 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jul 27 22:09:39 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jul 27 22:09:39 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Jul 27 22:09:39 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Jul 27 22:09:39 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Jul 27 22:09:39 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Jul 27 22:09:39 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Jul 27 22:09:39 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Jul 27 22:09:39 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Jul 27 22:09:39 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 27 22:09:39 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Jul 27 22:09:39 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Jul 27 22:09:39 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Jul 27 22:09:39 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Jul 27 22:09:39 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Jul 27 22:09:39 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jul 27 22:09:39 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jul 27 22:09:39 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jul 27 22:09:39 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

{code}",,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22406,,,,FLINK-26500,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 04 06:59:46 UTC 2021,,,,,,,,,,"0|z0te88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/21 10:03;chesnay;This is pretty much the same issue as FLINK-22406, just that it apply to the adaptive scheduler and isn't specific to reactive mode.
The job restarts more often then we expected, which throws off the counts.;;;","04/Aug/21 06:59;chesnay;master: 2ca91780afb2006a1e28595636afccb912b063b6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperLeaderElectionConnectionHandlingTest hangs on Azure,FLINK-23518,13392134,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,dwysakowicz,dwysakowicz,27/Jul/21 13:39,28/Aug/21 13:06,13/Jul/23 08:12,02/Aug/21 06:43,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20964&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9511

{code}
Jul 26 11:05:26 ""main"" #1 prio=5 os_prio=0 tid=0x00007efd7000b800 nid=0x1769 waiting on condition [0x00007efd771ea000]
Jul 26 11:05:26    java.lang.Thread.State: WAITING (parking)
Jul 26 11:05:26 	at sun.misc.Unsafe.park(Native Method)
Jul 26 11:05:26 	- parking to wait for  <0x0000000086681f00> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
Jul 26 11:05:26 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Jul 26 11:05:26 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
Jul 26 11:05:26 	at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)
Jul 26 11:05:26 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest$QueueLeaderElectionListener.next(ZooKeeperLeaderElectionConnectionHandlingTest.java:324)
Jul 26 11:05:26 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest$QueueLeaderElectionListener.next(ZooKeeperLeaderElectionConnectionHandlingTest.java:318)
Jul 26 11:05:26 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionConnectionHandlingTest.testConnectionSuspendedHandlingDuringInitialization(ZooKeeperLeaderElectionConnectionHandlingTest.java:114)
Jul 26 11:05:26 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 26 11:05:26 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 26 11:05:26 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 26 11:05:26 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 26 11:05:26 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jul 26 11:05:26 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jul 26 11:05:26 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jul 26 11:05:26 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jul 26 11:05:26 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Jul 26 11:05:26 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Jul 26 11:05:26 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jul 26 11:05:26 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Jul 26 11:05:26 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 26 11:05:26 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jul 26 11:05:26 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jul 26 11:05:26 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Jul 26 11:05:26 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Jul 26 11:05:26 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Jul 26 11:05:26 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Jul 26 11:05:26 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Jul 26 11:05:26 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Jul 26 11:05:26 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Jul 26 11:05:26 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 26 11:05:26 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Jul 26 11:05:26 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Jul 26 11:05:26 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Jul 26 11:05:26 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Jul 26 11:05:26 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Jul 26 11:05:26 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jul 26 11:05:26 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jul 26 11:05:26 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jul 26 11:05:26 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

{code}",,dwysakowicz,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 02 06:43:39 UTC 2021,,,,,,,,,,"0|z0tdgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/21 13:41;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20986&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9516;;;","27/Jul/21 13:55;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21018&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9618;;;","28/Jul/21 06:44;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21053&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9443;;;","28/Jul/21 14:54;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21103&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9862;;;","30/Jul/21 07:46;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21160&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=10237;;;","30/Jul/21 07:46;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21165&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=10240;;;","02/Aug/21 05:26;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21246&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9978;;;","02/Aug/21 05:40;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21255&view=logs&j=d8d26c26-7ec2-5ed2-772e-7a1a1eb8317c&t=ec8797b0-5eee-5a0e-f936-8db65cff44cc&l=8555;;;","02/Aug/21 06:43;chesnay;master: c8644c8f0a21e6052e4a7515b794984920d65073;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkKafkaInternalProducer overrides static final ProducerIdAndEpoch#NONE during transaction recovery (fails),FLINK-23509,13392037,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fpaul,Matthias Schwalbe,Matthias Schwalbe,27/Jul/21 06:51,15/Dec/21 01:40,13/Jul/23 08:12,06/Aug/21 12:06,1.12.5,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.0,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"When recovering Kafka transactions from a snapshot, FlinkKafkaInternalProducer overrides static final ProducerIdAndEpoch#NONE here:

[FlinkKafkaInternalProducer#resumeTransaction|https://github.com/apache/flink/blob/f06faf13930f2e8acccf1e04e2c250b85bdbf48e/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/FlinkKafkaInternalProducer.java#L229]

and consequently TransactionManager initializes transactions as new transactions instead of recovered ones. Here:

[TransactionManager#initializeTransactions|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java#L332]

TransactionManager log (edited for readability):
{quote}{{[Sink: trxRollupKafkaSink (1/1)#3|#3] INFO org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-Sink: trxRollupKafkaSink-...8b6-2, transactionalId=Sink: trxRollupKafkaSink-...8b6-2] Overriding the default enable.idempotence to true since transactional.id is specified.
 [Sink: trxRollupKafkaSink (1/1)#3|#3] INFO org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-Sink: trxRollupKafkaSink-...8b6-2, transactionalId=Sink: trxRollupKafkaSink-...8b6-2] Instantiated a transactional producer.
 [Sink: trxRollupKafkaSink (1/1)#3|#3] INFO org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-Sink: trxRollupKafkaSink-...8b6-2, transactionalId=Sink: trxRollupKafkaSink-...8b6-2] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.
 [Sink: trxRollupKafkaSink (1/1)#3|#3] INFO org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-Sink: trxRollupKafkaSink-...8b6-2, transactionalId=Sink: trxRollupKafkaSink-...8b6-2] Overriding the default acks to all since idempotence is enabled.
 [Sink: trxRollupKafkaSink (1/1)#3|#3] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-Sink: trxRollupKafkaSink-...8b6-2, transactionalId=Sink: trxRollupKafkaSink-...8b6-2] Transition from state UNINITIALIZED to INITIALIZING
 [Sink: trxRollupKafkaSink (1/1)#3|#3] INFO org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-Sink: trxRollupKafkaSink-...8b6-2, transactionalId=Sink: trxRollupKafkaSink-...8b6-2] Invoking InitProducerId for the first time in order to acquire a producer ID
 [Sink: trxRollupKafkaSink (1/1)#3|#3] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-Sink: trxRollupKafkaSink-...8b6-2, transactionalId=Sink: trxRollupKafkaSink-...8b6-2] Enqueuing transactional request InitProducerIdRequestData(transactionalId='Sink: trxRollupKafkaSink-...8b6-2', transactionTimeoutMs=60000, producerId=1545118, producerEpoch=17)
 [kafka-producer-network-thread | producer-Sink: trxRollupKafkaSink-...8b6-2] TRACE org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-Sink: trxRollupKafkaSink-...8b6-2, transactionalId=Sink: trxRollupKafkaSink-...8b6-2] Request InitProducerIdRequestData(transactionalId='Sink: trxRollupKafkaSink-...8b6-2', transactionTimeoutMs=60000, producerId=1545118, producerEpoch=17) dequeued for sending
 [kafka-producer-network-thread | producer-Sink: trxRollupKafkaSink-...8b6-2] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-Sink: trxRollupKafkaSink-...8b6-2, transactionalId=Sink: trxRollupKafkaSink-...8b6-2] Enqueuing transactional request FindCoordinatorRequestData(key='Sink: trxRollupKafkaSink-...8b6-2', keyType=1)
 [kafka-producer-network-thread | producer-Sink: trxRollupKafkaSink-...8b6-2] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-Sink: trxRollupKafkaSink-...8b6-2, transactionalId=Sink: trxRollupKafkaSink-...8b6-2] Enqueuing transactional request InitProducerIdRequestData(transactionalId='Sink: trxRollupKafkaSink-...8b6-2', transactionTimeoutMs=60000, producerId=1545118, producerEpoch=17)
 [kafka-producer-network-thread | producer-Sink: trxRollupKafkaSink-...8b6-2] TRACE org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-Sink: trxRollupKafkaSink-...8b6-2, transactionalId=Sink: trxRollupKafkaSink-...8b6-2] Request FindCoordinatorRequestData(key='Sink: trxRollupKafkaSink-...8b6-2', keyType=1) dequeued for sending
 [kafka-producer-network-thread | producer-Sink: trxRollupKafkaSink-...8b6-2] TRACE org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-Sink: trxRollupKafkaSink-...8b6-2, transactionalId=Sink: trxRollupKafkaSink-...8b6-2] Received transactional response FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='NONE', nodeId=3, host='ulxxtkafbrk03.adgr.net', port=9093) for request FindCoordinatorRequestData(key='Sink: trxRollupKafkaSink-...8b6-2', keyType=1)
 [kafka-producer-network-thread | producer-Sink: trxRollupKafkaSink-...8b6-2] INFO org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-Sink: trxRollupKafkaSink-...8b6-2, transactionalId=Sink: trxRollupKafkaSink-...8b6-2] Discovered transaction coordinator ulxxtkafbrk03.adgr.net:9093 (id: 3 rack: null)
 [kafka-producer-network-thread | producer-Sink: trxRollupKafkaSink-...8b6-2] TRACE org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-Sink: trxRollupKafkaSink-...8b6-2, transactionalId=Sink: trxRollupKafkaSink-...8b6-2] Request InitProducerIdRequestData(transactionalId='Sink: trxRollupKafkaSink-...8b6-2', transactionTimeoutMs=60000, producerId=1545118, producerEpoch=17) dequeued for sending
 [kafka-producer-network-thread | producer-Sink: trxRollupKafkaSink-...8b6-2] TRACE org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-Sink: trxRollupKafkaSink-...8b6-2, transactionalId=Sink: trxRollupKafkaSink-...8b6-2] Received transactional response InitProducerIdResponseData(throttleTimeMs=0, errorCode=47, producerId=-1, producerEpoch=-1) for request InitProducerIdRequestData(transactionalId='Sink: trxRollupKafkaSink-...8b6-2', transactionTimeoutMs=60000, producerId=1545118, producerEpoch=17)
 [kafka-producer-network-thread | producer-Sink: trxRollupKafkaSink-...8b6-2] DEBUG org.apache.kafka.clients.producer.internals.TransactionManager - [Producer clientId=producer-Sink: trxRollupKafkaSink-...8b6-2, transactionalId=Sink: trxRollupKafkaSink-...8b6-2] Transition from state INITIALIZING to error state FATAL_ERROR
 [Sink: trxRollupKafkaSink (1/1)#3|#3] INFO org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-Sink: trxRollupKafkaSink-...8b6-2, transactionalId=Sink: trxRollupKafkaSink-...8b6-2] Closing the Kafka producer with timeoutMillis = 0 ms.
 org.apache.kafka.common.KafkaException: Unexpected error in InitProducerIdResponse; Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer's transaction has been expired by the broker.
 at org.apache.kafka.clients.producer.internals.TransactionManager$InitProducerIdHandler.handleResponse(TransactionManager.java:1352)
 at org.apache.kafka.clients.producer.internals.TransactionManager$TxnRequestHandler.onComplete(TransactionManager.java:1260)
 at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
 at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:572)
 at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:564)
 at org.apache.kafka.clients.producer.internals.Sender.maybeSendAndPollTransactionalRequest(Sender.java:414)
 at org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:312)
 at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:239)
 at java.lang.Thread.run(Thread.java:748)
 }}
{quote}
 Notice here ""Invoking InitProducerId for the first time in order to acquire a producer ID"" indicates a request for a new transaction (-1, -1) but below we see instead: ""Enqueuing transactional request InitProducerIdRequestData(transactionalId='Sink: ...', ..., producerId=1545118, producerEpoch=17)"" because of changed ProducerIdAndEpoch#NONE

 

TransactionManager#initializeTransactions variables:

  !2021-07-26_16-47-48.png!

Notice the values above which should be -1, -1.

Stack trace of TransactionManager#initializeTransactions:
{quote}initializeTransactions:314, TransactionManager (org.apache.kafka.clients.producer.internals)
 initializeTransactions:310, TransactionManager (org.apache.kafka.clients.producer.internals)
 initTransactions:591, KafkaProducer (org.apache.kafka.clients.producer)
 initTransactions:88, FlinkKafkaInternalProducer (org.apache.flink.streaming.connectors.kafka.internals)
 recoverAndAbort:1060, FlinkKafkaProducer (org.apache.flink.streaming.connectors.kafka)
 recoverAndAbort:99, FlinkKafkaProducer (org.apache.flink.streaming.connectors.kafka)
 initializeState:371, TwoPhaseCommitSinkFunction (org.apache.flink.streaming.api.functions.sink)
 initializeState:1195, FlinkKafkaProducer (org.apache.flink.streaming.connectors.kafka)
 tryRestoreFunction:189, StreamingFunctionUtils (org.apache.flink.streaming.util.functions)
 restoreFunctionState:171, StreamingFunctionUtils (org.apache.flink.streaming.util.functions)
 initializeState:96, AbstractUdfStreamOperator (org.apache.flink.streaming.api.operators)
 initializeOperatorState:118, StreamOperatorStateHandler (org.apache.flink.streaming.api.operators)
 initializeState:290, AbstractStreamOperator (org.apache.flink.streaming.api.operators)
 initializeStateAndOpenOperators:436, OperatorChain (org.apache.flink.streaming.runtime.tasks)
 restoreGates:574, StreamTask (org.apache.flink.streaming.runtime.tasks)
 call:-1, 412600778 (org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$745)
 call:55, StreamTaskActionExecutor$1 (org.apache.flink.streaming.runtime.tasks)
 restore:554, StreamTask (org.apache.flink.streaming.runtime.tasks)
 doRun:756, Task (org.apache.flink.runtime.taskmanager)
 run:563, Task (org.apache.flink.runtime.taskmanager)
 run:748, Thread (java.lang)
{quote}
 

Stack trace of FlinkKafkaInternalProducer#resumeTransaction when the values are overridden:
{quote}resumeTransaction:204, FlinkKafkaInternalProducer (org.apache.flink.streaming.connectors.kafka.internals)
 resumeTransaction:196, KafkaProducerJobSink$$anon$1$$anon$2 (ch.viseca.flink.connectors.kafka.sinks)
 recoverAndCommit:1029, FlinkKafkaProducer (org.apache.flink.streaming.connectors.kafka)
 recoverAndCommit:99, FlinkKafkaProducer (org.apache.flink.streaming.connectors.kafka)
 recoverAndCommitInternal:414, TwoPhaseCommitSinkFunction (org.apache.flink.streaming.api.functions.sink)
 initializeState:364, TwoPhaseCommitSinkFunction (org.apache.flink.streaming.api.functions.sink)
 initializeState:1195, FlinkKafkaProducer (org.apache.flink.streaming.connectors.kafka)
 tryRestoreFunction:189, StreamingFunctionUtils (org.apache.flink.streaming.util.functions)
 restoreFunctionState:171, StreamingFunctionUtils (org.apache.flink.streaming.util.functions)
 initializeState:96, AbstractUdfStreamOperator (org.apache.flink.streaming.api.operators)
 initializeOperatorState:118, StreamOperatorStateHandler (org.apache.flink.streaming.api.operators)
 initializeState:290, AbstractStreamOperator (org.apache.flink.streaming.api.operators)
 initializeStateAndOpenOperators:436, OperatorChain (org.apache.flink.streaming.runtime.tasks)
 restoreGates:574, StreamTask (org.apache.flink.streaming.runtime.tasks)
 call:-1, 412600778 (org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$745)
 call:55, StreamTaskActionExecutor$1 (org.apache.flink.streaming.runtime.tasks)
 restore:554, StreamTask (org.apache.flink.streaming.runtime.tasks)
 doRun:756, Task (org.apache.flink.runtime.taskmanager)
 run:563, Task (org.apache.flink.runtime.taskmanager)
 run:748, Thread (java.lang)
{quote}
 

Background:
 * we recently upgraded from Flink 1.8.0 to 1.13.0
 * FlinkKafkaInternalProducer#resumeTransaction has not changed between these versions, however
 * in Flink 1.8.0 we never observed any resumable transaction as part of a checkpoint
 * we could not determine what actually made the change that causes the failure, however:
 * it would probably be much saver to instead of assigning new values to an arbitrary ProducerIdAndEpoch held by TransactionManager to directly assign a fresh ProducerIdAndEpoch and thus avoid overriding ProducerIdAndEpoch#NONE

Sample workaround (scala):
{quote}val sink = new FlinkKafkaProducer[T](val sink = new FlinkKafkaProducer[T](  defaultTopic,  schema,  getProperties,  getSemantic,  getProducerPoolSize) {
   override protected def createProducer: FlinkKafkaInternalProducer[Array[Byte], Array[Byte]] =
Unknown macro: \{ new FlinkKafkaInternalProducer[Array[Byte], Array[Byte]](this.producerConfig) Unknown macro}
      override def resumeTransaction(producerId: Long, epoch: Short): Unit =
 Unknown macro: \{        val transactionManager = FlinkKafkaInternalProducer.getField(kafkaProducer, ""transactionManager"")        transactionManager.synchronized Unknown macro}
         super.resumeTransaction(producerId, epoch)      }    }  }}
{quote}
 ",,fpaul,martijnvisser,Matthias Schwalbe,Paul Lin,qinjunjerry,swhelan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23674,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/21 06:57;Matthias Schwalbe;2021-07-26_16-47-48.png;https://issues.apache.org/jira/secure/attachment/13031092/2021-07-26_16-47-48.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 06 12:06:00 UTC 2021,,,,,,,,,,"0|z0tcvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/21 08:37;fpaul;Hi [~Matthias Schwalbe],

Unfortunately, we cannot easily set a new epoch and producer id because it might implicate data loss. It is important to commit all pending transactions from either previous checkpoints (which have not been completed successfully yet) or from other subtasks in case of a scale-in event. 

I think the problem you are seeing is related to https://issues.apache.org/jira/browse/FLINK-16419 ;;;","27/Jul/21 09:23;Matthias Schwalbe;Hi [~fpaul],

I completely agree with your points, and the code does a good job in concluding phase 2 of the 2-phase-commit-protocol, after recovering from an interrupt. However ProducerIdAndEpoch tuple class has value semantics, i.e. it should never directly be changed but rather replaced with a new ProducerIdAndEpoch containing the correct values.
 * race conditions out of our control lead to the fact that
 * TransactionManager sometimes directly holds ProducerIdAndEpoch#NONE
 * ProducerIdAndEpoch#NONE is used for comparison whether we work on a new transaction or an existing one
 * ProducerIdAndEpoch#NONE is especially sensitive to being changed by means of reflection
 * overriding ProducerIdAndEpoch#NONE can be avoided by changing [FlinkKafkaInternalProducer#resumeTransaction|https://github.com/apache/flink/blob/f06faf13930f2e8acccf1e04e2c250b85bdbf48e/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/FlinkKafkaInternalProducer.java#L229] to assign a new ProducerIdAndEpoch instead of overriding an arbitrary ProducerIdAndEpoch held by TransactionManager
 * all remaining protocol stays intact
 * the workaround given above completely fixes this, the approach could also be integrated

 

It took me a while to track down the root cause:
 * at no time ProducerIdAndEpoch#NONE should contain values other that (-1, -1),
 * which can be violated due to race conditions by [FlinkKafkaInternalProducer#resumeTransaction|https://github.com/apache/flink/blob/f06faf13930f2e8acccf1e04e2c250b85bdbf48e/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/FlinkKafkaInternalProducer.java#L229]

 

Should be unite the two tickets?

- https://issues.apache.org/jira/browse/FLINK-16419  proposes ignoreFailuresAfterTransactionTimeout() which only helps a little:

-  ProducerIdAndEpoch#NONE would still contain values other that (-1, -1) which seems to be the rot couse

 

Looking forward to your response :);;;","27/Jul/21 10:33;arvid;Hi Matthias, good observation and I see your point. It may also solve http://deprecated-apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/recover-from-svaepoint-td44081.html#a44157 .;;;","27/Jul/21 11:22;Matthias Schwalbe;Hi [~arvid],

Yes, this exactly matches my observation.

The fix would be to replace 3 lines with 2 different lines in [FlinkKafkaInternalProducer#resumeTransaction|https://github.com/apache/flink/blob/f06faf13930f2e8acccf1e04e2c250b85bdbf48e/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/FlinkKafkaInternalProducer.java#L229] and we would be on the save side, no matter how the specific version of Kafka client initializes TransactionManager#producerIdAndEpoch (cf. to Tia Zhaos comment here: [http://deprecated-apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/recover-from-svaepoint-tp44081p44162.html];;;","06/Aug/21 12:06;fpaul;Fixed in:
 * release-1.12: adf2f29416eaf6f819cbb2a7944ecec4c1be0eb2
 * release-1.13: e13ae99953d2702f45db9a42d8714b6da93cd0ff
 * master: 543959ee6707b6e93412e809e9082227dd95efe0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The node IP obtained in NodePort mode is a VIP,FLINK-23507,13392013,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,long jiang,long jiang,27/Jul/21 03:37,15/Feb/22 03:51,13/Jul/23 08:12,13/Aug/21 14:44,1.13.0,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,"When the three masters of k8s turn on the load balancing mode, when the job is submitted to the k8s cluster and the parameter -Dkubernetes.rest-service.exposed.type=NodePort is used, the web ui address of the job obtained at this time is Get the address of the k8s master, but this address is a VIP. VIP does not belong to the k8s cluster, so it cannot be accessed. The node ip we should get is the IP of any machine in the k8s cluster.
like this：

!image-2021-07-27-11-36-56-280.png!",,long jiang,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16601,,,,FLINK-23043,,,,,,,,,,,,,,,FLINK-22975,,,,,,,,,,,,"27/Jul/21 03:36;long jiang;image-2021-07-27-11-36-56-280.png;https://issues.apache.org/jira/secure/attachment/13031084/image-2021-07-27-11-36-56-280.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 13 14:44:37 UTC 2021,,,,,,,,,,"0|z0tcq0:",9223372036854775807,"When using ""kubernetes.rest-service.exposed.type=NodePort"", connection string for Rest gateway is now correctly constructed in form of ""<nodeIp>:<nodePort>"" instead of ""<kubernetesApiServerUrl>:<nodePort>"". This may be a breaking change for some users.

This also introduces a new config option ""kubernetes.rest-service.exposed.node-port-address-type"" that lets you select ""<nodeIp>"" from a desired range.",,,,,,,,,,,,,,,,,,,"13/Aug/21 14:44;trohrmann;Fixed via d9830ca3a9eae1d3bc7ff367f5f728b717971aaf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to deserialize csv format data with double quotes,FLINK-23503,13391912,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,empcl,empcl,empcl,26/Jul/21 15:18,15/Dec/21 01:40,13/Jul/23 08:12,03/Aug/21 03:16,1.12.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,,,,,"Currently flink 1.12 provides the `csv.disable-quote-character` configuration to solve the problem of double quotation marks in the CSV field. However, if the field data is: ""n2""a2, this will cause an exception. ",,empcl,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21207,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/21 15:17;empcl;f1.png;https://issues.apache.org/jira/secure/attachment/13031066/f1.png","26/Jul/21 15:17;empcl;f2.png;https://issues.apache.org/jira/secure/attachment/13031065/f2.png","26/Jul/21 15:17;empcl;f3.png;https://issues.apache.org/jira/secure/attachment/13031064/f3.png",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 03 03:16:34 UTC 2021,,,,,,,,,,"0|z0tc3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/21 15:29;empcl;This kind of problem occurs if the field starts with double quotes.;;;","27/Jul/21 02:46;TsReaper;Hi!

This issue should be fixed by FLINK-21207, however it's only in 1.13 and higher versions.

Shall we cherry-pick that to 1.12? cc. release manager [~lzljs3620320].;;;","27/Jul/21 02:49;lzljs3620320;We can cherry-pick that to 1.12, and included in 1.12.6. This can not be a blocker for 1.12.5.;;;","27/Jul/21 16:08;empcl;Hi ,[~lzljs3620320] .

I want to do it for 1.12. Thanks.;;;","03/Aug/21 01:58;empcl;Hi, [~lzljs3620320]

I have cherry-pick to version 1.12, could you review the code for me? thanks

[Fix source table with 'csv.disable-quote-character' = 'true' can not take effect in flink 1.12. |https://github.com/apache/flink/pull/16614];;;","03/Aug/21 03:15;lzljs3620320;[~empcl] Thanks!;;;","03/Aug/21 03:16;lzljs3620320;release-1.12: c8f4d8038e345ae3083ce5c32d5f54ecd812792b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Benchmarks use outdated flink-shaded-netty-dynamic version,FLINK-23502,13391887,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,chesnay,chesnay,26/Jul/21 12:39,26/Jul/21 12:46,13/Jul/23 08:12,26/Jul/21 12:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Benchmarks,Build System,,,,0,,,,,"The benchmarks use the native netty library from flink-shaded 1.10, which we release for Flink 1.10.
This version is incompatible with the latest netty version that Flink uses.

It also somewhat invalidates all SSL benchmarks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 26 12:46:15 UTC 2021,,,,,,,,,,"0|z0tby0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/21 12:46;chesnay;benchmark-master: f88469b5987610c3e40a3db916e48cab6297a5f0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AkkaRpcSystemLoader fails if tmp directory does not exist,FLINK-23500,13391876,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,26/Jul/21 11:29,28/Aug/21 12:23,13/Jul/23 08:12,27/Jul/21 09:34,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"The {{AkkaRpcSystemLoader}} requires the configured tmp directory to already exist, which is unnecessarily strict.",,bhagi__R,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 27 09:34:48 UTC 2021,,,,,,,,,,"0|z0tbvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/21 11:38;bhagi__R;Hi team,
Please unsubscribe me from all emails...

Bhagi.ramahalla@gmail.com

---------- Forwarded message ----------
From: *Chesnay Schepler (Jira)* <jira@apache.org>
Date: Monday, July 26, 2021
Subject: [jira] [Created] (FLINK-23500) AkkaRpcSystemLoader fails if tmp
directory does not exist
To: dev@flink.apache.org


Chesnay Schepler created FLINK-23500:
----------------------------------------

             Summary: AkkaRpcSystemLoader fails if tmp directory does not
exist
                 Key: FLINK-23500
                 URL: https://issues.apache.org/jira/browse/FLINK-23500
             Project: Flink
          Issue Type: Bug
          Components: Runtime / Coordination
    Affects Versions: 1.14
            Reporter: Chesnay Schepler
            Assignee: Chesnay Schepler
             Fix For: 1.14.0


The {{AkkaRpcSystemLoader}} requires the configured tmp directory to
already exist, which is unnecessarily strict.



--
This message was sent by Atlassian Jira
(v8.3.4#803005)



-- 
Thanks & Regards,
Bhagi
;;;","27/Jul/21 09:34;chesnay;master: 7ac1fb9fe4d1dd3e7ad8e816df63958fb81da058;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-table-planner does not compile on scala 2.12,FLINK-23497,13391860,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,twalthr,chesnay,chesnay,26/Jul/21 10:15,28/Aug/21 12:24,13/Jul/23 08:12,28/Jul/21 12:03,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20961&view=logs&j=b9f58bb2-ed4a-500b-bef9-cc3cf2248e69&t=e6d8efc2-861e-5470-71ae-bbaad6c133d3

{code}
2021-07-26T09:10:26.7655809Z [INFO] --- scala-maven-plugin:3.2.2:compile (scala-compile-first) @ flink-table-planner_2.12 ---
2021-07-26T09:10:26.8567767Z [INFO] /__w/1/s/flink-table/flink-table-planner/src/main/java:-1: info: compiling
2021-07-26T09:10:26.8568668Z [INFO] /__w/1/s/flink-table/flink-table-planner/src/main/scala:-1: info: compiling
2021-07-26T09:10:26.8571934Z [INFO] Compiling 903 source files to /__w/1/s/flink-table/flink-table-planner/target/classes at 1627290626856
2021-07-26T09:10:31.0803565Z [ERROR] /__w/1/s/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/expressions/PlannerExpressionParserImpl.scala:32: error: object parsing is not a member of package util
2021-07-26T09:10:31.0804449Z [ERROR] import _root_.scala.util.parsing.combinator.{JavaTokenParsers, PackratParsers}
2021-07-26T09:10:31.0804904Z [ERROR]                          ^
2021-07-26T09:10:31.1130420Z [ERROR] /__w/1/s/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/expressions/PlannerExpressionParserImpl.scala:59: error: not found: type JavaTokenParsers
2021-07-26T09:10:31.1131245Z [ERROR] object PlannerExpressionParserImpl extends JavaTokenParsers
2
...
{code}",,dwysakowicz,hackergin,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 28 12:03:26 UTC 2021,,,,,,,,,,"0|z0tbs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/21 10:21;dwysakowicz;cc [~twalthr][~jark];;;","26/Jul/21 15:09;twalthr;Did this work before? Or are we aiming to support this for the first time? In any case, we could think about dropping this class finally.;;;","26/Jul/21 21:54;chesnay;??Did this work before? Or are we aiming to support this for the first time???

I must be missing something; wouldn't it _had_ to have worked before, since it existed before the 1.13 release, for which we deployed scala 2.12 versions?

AFAIK started failing sometime last week, and could very well be _somehow_ related to the scala removal from flink-runtime.

This currently blocks the 2.12 snapshot releases so it would be good to resolve it quickly. Naturally it would block the 1.14 release.;;;","27/Jul/21 06:30;twalthr;That's why I'm also confused. This class hasn't been touched for quite some time. I will investigate.;;;","27/Jul/21 13:43;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20996&view=results;;;","27/Jul/21 13:51;twalthr;I could reproduce it locally. I will likely open a PR today.;;;","28/Jul/21 06:38;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21053&view=results;;;","28/Jul/21 12:03;twalthr;Fixed in 1.14.0: 5c5e353b03bce6e073ef4482ee7ff7c47e0184c0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-rpc-akka gets stuck during dependency reduction,FLINK-23494,13391807,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,dwysakowicz,dwysakowicz,26/Jul/21 06:56,26/Jul/21 10:27,13/Jul/23 08:12,26/Jul/21 10:26,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Build System,,,,,0,,,,,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20904&view=logs&j=585d8b77-fa33-51bc-8163-03e54ba9ce5b,,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 26 10:26:57 UTC 2021,,,,,,,,,,"0|z0tbg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/21 06:57;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20896&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=585d8b77-fa33-51bc-8163-03e54ba9ce5b;;;","26/Jul/21 07:00;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20925&view=results;;;","26/Jul/21 10:17;chesnay;The issue with flink-rpc-akka getting stuck has been resolved; it was caused by duplicate dependencies in pom.xml. Now we run into a compilation issue in flink-table-planner on scala 2.12, see FLINK-23497.;;;","26/Jul/21 10:26;chesnay;I've modified this issue to be purely about the flink-rpc-akka issue. Remaining work is (hopefully) covered by FLINK-23497.

master:
63dc59dad53a5c811ec93889b370cf550c289d0f
b2255483c4a99f933b34282bda4f638706e171ef;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
python tests hang on Azure,FLINK-23493,13391803,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hxbks2ks,dwysakowicz,dwysakowicz,26/Jul/21 06:42,30/Dec/21 08:32,13/Jul/23 08:12,28/Dec/21 06:18,1.12.4,1.13.1,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.6,1.14.3,1.15.0,,API / Python,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20898&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=22829
",,dianfu,dwysakowicz,gaoyunhaii,hxbks2ks,trohrmann,xtsong,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23574,FLINK-21004,,,,,,,,,,FLINK-24495,FLINK-24764,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 30 08:32:45 UTC 2021,,,,,,,,,,"0|z0tbfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/21 07:03;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20925&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295;;;","27/Jul/21 13:52;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20998&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490;;;","30/Jul/21 07:48;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21181&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490;;;","02/Aug/21 06:38;dianfu;New instances as reported in FLINK-23574:
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21229&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=21404]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21256&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=20834]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21255&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23335];;;","02/Aug/21 06:39;dianfu;cc [~hxbks2ks] Could you help to take a look at this issue?;;;","05/Aug/21 03:44;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21558&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=21353;;;","09/Aug/21 02:45;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21712&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=20845;;;","09/Aug/21 02:51;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21713&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=21910;;;","09/Aug/21 02:53;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21711&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23195;;;","09/Aug/21 03:28;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21739&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=21421;;;","12/Aug/21 03:14;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21934&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=21912;;;","13/Aug/21 03:05;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22022&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=22441;;;","16/Aug/21 03:41;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22198&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=20787;;;","16/Aug/21 03:51;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22196&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=21538;;;","17/Aug/21 04:54;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22330&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=21150;;;","17/Aug/21 05:08;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22329&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=21932;;;","17/Aug/21 07:34;hxbks2ks;Let me synchronize the results of the survey.

1. The reason for hang is that the process started on the java side is waiting for the python side to start the client to connect to it, but I don’t know why the python side has not been able to connect to it. In fact, if there is an error, python process will throw the error message and exit. So from the perspective of the phenomenon, we can know there is no error in starting python process. On the other hand, in lack of info logs, I haven't discover the root cause of Python side can't connect to java side.

2. Another phenomenon is that all the hangs tests are in `cron_azure`. Compared with the stages `cron_hadoop241`, `cron_hadoop313`, and `cron_scala212` which will also test python related tests, `cron_azure` will use Azure vm to test. But `cron_hadoop241`, `cron_hadoop313`, and `cron_scala212` will use the Alibaba Cloud host to test directly. 

3. From https://issues.apache.org/jira/browse/FLINK-22856, we update the vm version from `ubuntu-16` to `ubuntu-20`, and I'm not sure whether it is relevant to this.;;;","17/Aug/21 08:00;xtsong;So what do you suggest for next? Is there any way to get more inputs about what's going on in the python process?;;;","17/Aug/21 08:27;hxbks2ks;I have reproduced this problem in my private Azure pipeline yesterday and I am trying to add some logs to find the root cause.;;;","18/Aug/21 02:55;hxbks2ks;https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_build/results?buildId=22416&tracking_data=ew0KICAic291cmNlIjogIlNsYWNrUGlwZWxpbmVzQXBwIiwNCiAgInNvdXJjZV9ldmVudF9uYW1lIjogIm1zLnZzcy1waXBlbGluZXMucnVuLXN0YXRlLWNoYW5nZWQtZXZlbnQiDQp9;;;","19/Aug/21 02:19;xtsong;[~hxbks2ks], please move the ticket to in-progress, since you have started investigating it.;;;","20/Aug/21 02:36;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22513&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=22274;;;","20/Aug/21 03:19;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22511&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=22588;;;","23/Aug/21 02:29;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22578&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=20588;;;","23/Aug/21 02:30;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22579&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=22441;;;","23/Aug/21 02:44;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22606&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=21422;;;","23/Aug/21 02:47;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22605&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23931;;;","23/Aug/21 02:54;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22619&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=22626;;;","24/Aug/21 01:59;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22685&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=20847;;;","30/Aug/21 03:51;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23011&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23310;;;","31/Aug/21 06:47;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23144&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23769;;;","31/Aug/21 06:50;dianfu;[~xtsong] I'd like to downgrade the issue as just as [~hxbks2ks] investigated, it's strange that the failed test cases only occurred in stage ""cron_azure"" and so I also suspect it's more like an environment issue. Besides, it only failed once in the last week. Personally I think this should not be a release blocker for 1.14. Do you think it makes sense to downgrade it to critical? (PS: We will continue to investigate this issue);;;","31/Aug/21 06:55;xtsong;Thanks, [~dianfu] and [~hxbks2ks]. I'm fine with downgrading its priority to critical.;;;","02/Sep/21 01:44;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23344&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=20811;;;","02/Sep/21 05:35;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23345&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=22444;;;","06/Sep/21 03:15;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23546&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=21751;;;","09/Sep/21 06:33;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23803&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=22343;;;","10/Sep/21 06:37;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23879&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=23081;;;","10/Sep/21 06:41;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23879&view=logs&j=ff2e2ea5-07e3-5521-7b04-a4fc3ad765e9&t=1ec6382b-bafe-5817-63ae-eda7d4be718e&l=22838;;;","13/Sep/21 01:45;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23942&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=21839;;;","13/Sep/21 02:05;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23941&view=logs&j=ff2e2ea5-07e3-5521-7b04-a4fc3ad765e9&t=1ec6382b-bafe-5817-63ae-eda7d4be718e&l=23723;;;","13/Sep/21 02:15;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23951&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=21840;;;","13/Sep/21 02:18;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23950&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=22449;;;","13/Sep/21 02:19;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23953&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=23028;;;","13/Sep/21 03:44;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23955&view=logs&j=bdd9ea51-4de2-506a-d4d9-f3930e4d2355&t=dd50312f-73b5-56b5-c172-4d81d03e2ef1&l=22245;;;","13/Sep/21 06:04;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23957&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=22843;;;","13/Sep/21 06:12;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23958&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=21648;;;","14/Sep/21 03:51;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24036&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23728;;;","14/Sep/21 04:39;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24039&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=22038;;;","17/Sep/21 04:43;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24231&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=22447;;;","17/Sep/21 05:50;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24232&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23445;;;","18/Sep/21 03:50;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24289&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=21907;;;","24/Sep/21 03:09;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24457&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=22652;;;","26/Sep/21 02:19;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24491&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=23389;;;","26/Sep/21 02:20;xtsong;[~hxbks2ks],
Are you still looking into this? Any updates?;;;","27/Sep/21 04:53;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24513&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=23570;;;","01/Oct/21 06:35;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24685&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=22215;;;","04/Oct/21 07:32;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24717&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490;;;","04/Oct/21 07:33;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24716&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295;;;","04/Oct/21 07:36;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24718&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490;;;","04/Oct/21 07:36;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24720&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295;;;","04/Oct/21 07:39;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24724&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295;;;","05/Oct/21 07:07;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24762&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490;;;","06/Oct/21 06:13;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24784&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295;;;","08/Oct/21 06:10;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24809&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=23578
;;;","08/Oct/21 06:32;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24832&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=22772;;;","09/Oct/21 02:55;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24879&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=21965;;;","09/Oct/21 04:52;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24881&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23896;;;","11/Oct/21 02:40;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24930&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=22048;;;","12/Oct/21 08:37;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24965&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=22047;;;","14/Oct/21 02:05;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24997&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=22844;;;","15/Oct/21 07:26;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25065&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490;;;","18/Oct/21 08:55;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=25115&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490;;;","11/Nov/21 08:35;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26329&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23628;;;","11/Nov/21 08:35;trohrmann;[~hxbks2ks] and [~dianfu] what is the status of your investigation. We are seeing quite a high number of failing python builds on CI for quite some time now.;;;","11/Nov/21 08:36;trohrmann;Might be related to FLINK-24764.;;;","29/Nov/21 13:20;trohrmann;[~hxbks2ks] or [~dianfu] could give a quick update on the progress for this ticket?;;;","30/Nov/21 01:21;dianfu;It is caused by the same reason as FLINK-24764. [~hxbks2ks] is looking into it. [~hxbks2ks] Could you share some progress?;;;","30/Nov/21 02:55;hxbks2ks;I'm very sorry, because there is something wrong with my mail client, the mail was not received in time. Let me share with you the current progress.

The reasons for these hanging tests are the same. Let me first talk about the steps of the start-up phase of the operator that runs the Python UDF whichi can help know why the progress hangs:
    step 1: The Java operator will start a Grpc Server.
    step 2: The Java Operator will start a child process which is a shell script called `pyflink-udf-runner.sh`
    step 3: The script `pyflink-udf-runner.sh` will start a child process `python beam_boot.py`
    step 4: The `beam_boot.py` will also start a child process `python beam_sdk_worker_main.py`
    step 5: In the `beam_sdk_worker_main.py`, it will start a Grpc client  to connect to the Grpc Server run in the Java Operator (# step 1)

Now the phenomenon of hanging is that the Java Operators(If we set multiple paralism) has one concurrent Grpc Server that does not receive the connection from the Grpc client on the Python side. By grabbing the background process, we can see the following phenomenas:
    1. `pyflink-udf-runner.sh` hangs in running `python beam_boot.py` which can be explained.
    2. The process `python beam_sdk_worker_main.py` is not running, and through the log, we can confirm that the Grpc Client does not start up, which explains the phenomenon that the Java client hangs.
    3. According to the printed log, the execution of `python beam_boot.py` has been finished, but this can't explain why `python beam_sdk_worker_main.py` did not start successfully.
    4. Observing the process running in the background, `python beam_boot.py` is still running.
    5. Two `python beam_boot.py` process appeared in the background, and one of them was a child process of another one.

At present, the phenomenas 3, 4, and 5 have no way to explain clearly. so I have added the code to output the stack method of the python process to print out the status of `python beam_boot.py`. However, an experiment needs to trigger multiple Azure Labs at the same time. That is because it is not a stable trigger, so it takes a lot of time to get the results of an experiment. 





;;;","04/Dec/21 08:52;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27550&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23832]

Hi [~hxbks2ks]  it seems there is new occurrence of this issue~ Could you have another look~?;;;","05/Dec/21 13:27;gaoyunhaii;Another instance on 1.13:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27560&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=21908]

 ;;;","05/Dec/21 13:34;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27562&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23226;;;","06/Dec/21 03:12;hxbks2ks;[~gaoyunhaii] Thanks a lot for the reporting. I have tracked that Python process hangs in the progress of creating a child process. Similar problems have been reported in the Python community, but because there is no reproduction program provided, it is closed (https://bugs.python.org/issue39241 ). I am currently studying this part of the code of python subprocess.py.;;;","06/Dec/21 06:17;yunta;Another instance https://myasuka.visualstudio.com/flink/_build/results?buildId=358&view=logs&j=fba17979-6d2e-591d-72f1-97cf42797c11&t=727942b6-6137-54f7-1ef9-e66e706ea068;;;","06/Dec/21 09:42;hxbks2ks;Regarding the failure of python to create a child process, it seems to be related to the hardware system at present. I took the way of directly calling the module in the same process to replace the original way of launching the child process to solve this problem. In fact, even if this problem does not occur, the way of creating a child process is not a good choice, and calling it directly is a better way. I will provide a PR to fix this problem.;;;","07/Dec/21 02:16;hxbks2ks;Merged into master via fb38c99a38c63ba8801e765887f955522072615a
Merged into release-1.14 via b4fda42c59fff0c4e0065955312400c5d9fe8f69
Merged into release-1.13 via 10df24bbd87d2a16e6ea7b7c124807a167793a37
Merged into release-1.12 via b30e5b4704a10920b65b078611ed532c080ec656;;;","07/Dec/21 11:30;gaoyunhaii;Very thanks [~hxbks2ks] for the investigating!;;;","27/Dec/21 06:18;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28588&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23407

Hi [~hxbks2ks] there seems stall on 1.14, could you have another look? Also fyi currently the pipeline is not stable, there are also issues like https://issues.apache.org/jira/browse/FLINK-25292 and https://issues.apache.org/jira/browse/FLINK-25374;;;","27/Dec/21 06:40;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28592&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=23404;;;","27/Dec/21 06:49;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28593&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=22752;;;","27/Dec/21 11:32;hxbks2ks;[~gaoyunhaii] Thanks a lot for reporting. Your newly reported problem is that the installation depends on the network to get stuck, which is not the same as the original problem of JIRA. Let’s create a new JIRA to record it?;;;","28/Dec/21 06:18;gaoyunhaii;Thanks [~hxbks2ks] for the investigation! I'll create a new issue~;;;","30/Dec/21 08:30;trohrmann;[~gaoyunhaii] what is the new issue you've created? I think https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28724&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=22774 is another instance of it.;;;","30/Dec/21 08:31;trohrmann;I assume that it is FLINK-25464.;;;","30/Dec/21 08:32;gaoyunhaii;Hi [~trohrmann] , the issue is https://issues.apache.org/jira/browse/FLINK-25464 ~ ;;;"
JobVertexThreadInfoTrackerTest.testCachedStatsCleanedAfterCleanupInterval fails on Azure,FLINK-23492,13391802,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Nicolaus Weidner,dwysakowicz,dwysakowicz,26/Jul/21 06:40,28/Aug/21 13:10,13/Jul/23 08:12,05/Aug/21 07:09,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20898&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2&l=7017

{code}
Jul 23 22:19:29 [ERROR] Tests run: 6, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.967 s <<< FAILURE! - in org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTrackerTest
Jul 23 22:19:29 [ERROR] testCachedStatsCleanedAfterCleanupInterval(org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTrackerTest)  Time elapsed: 0.024 s  <<< FAILURE!
Jul 23 22:19:29 java.lang.AssertionError
Jul 23 22:19:29 	at org.junit.Assert.fail(Assert.java:86)
Jul 23 22:19:29 	at org.junit.Assert.assertTrue(Assert.java:41)
Jul 23 22:19:29 	at org.junit.Assert.assertTrue(Assert.java:52)
Jul 23 22:19:29 	at org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTrackerTest.assertExpectedEqualsReceived(JobVertexThreadInfoTrackerTest.java:231)
Jul 23 22:19:29 	at org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTrackerTest.doInitialRequestAndVerifyResult(JobVertexThreadInfoTrackerTest.java:224)
Jul 23 22:19:29 	at org.apache.flink.runtime.webmonitor.threadinfo.JobVertexThreadInfoTrackerTest.testCachedStatsCleanedAfterCleanupInterval(JobVertexThreadInfoTrackerTest.java:178)
Jul 23 22:19:29 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 23 22:19:29 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 23 22:19:29 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 23 22:19:29 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 23 22:19:29 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Jul 23 22:19:29 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jul 23 22:19:29 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Jul 23 22:19:29 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jul 23 22:19:29 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
Jul 23 22:19:29 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
Jul 23 22:19:29 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Jul 23 22:19:29 	at java.lang.Thread.run(Thread.java:748)

{code}",,dwysakowicz,Nicolaus Weidner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 05 07:09:33 UTC 2021,,,,,,,,,,"0|z0tbf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/21 14:43;Nicolaus Weidner;Looking into this a bit, I suspect it's a race condition: In the test, we set a cleanup interval of 10ms. This is probably low enough that sporadically, between querying that the result is now available and actually fetching the result, it is lost. According to the docs of the cache being used, elements are automatically removed after the interval. The code contains explicit cleanup calls, though I am not sure what purpose they serve (tests pass without).

I experimented locally and can only reproduce this exception when setting the cleanup interval to 1ms, but then it is pretty consistent.;;;","05/Aug/21 07:09;chesnay;master: ef99b99a9b997571576848ebb89d2878abb33d72
1.13: de6931625ea28a3ab774558faed02cd02060543e ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scheduler benchmarks are not running,FLINK-23491,13391776,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,Thesharing,Thesharing,26/Jul/21 03:09,26/Jul/21 12:47,13/Jul/23 08:12,26/Jul/21 12:47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Benchmarks,Runtime / Coordination,,,,0,pull-request-available,,,,"As shown in [codespeed|http://codespeed.dak8s.net:8000/timeline/#/?exe=5&env=2&revs=1000&equid=off&quarts=on&extr=on&ben=grid], we find that the scheduler benchmark are not running from July 22nd to July 25th. The last valid commit revision is fce9c1d8. I try to run the scheduler benchmarks locally and find that they work well. 

I've checked the log and find that the scheduler benchmarks are not running because:
{code:java}
java.lang.NoClassDefFoundError: org/apache/flink/runtime/jobmaster/slotpool/SlotPoolImpl
	at org.apache.flink.scheduler.benchmark.e2e.CreateSchedulerBenchmarkExecutor.setup(CreateSchedulerBenchmarkExecutor.java:51)
	at org.apache.flink.scheduler.benchmark.e2e.generated.CreateSchedulerBenchmarkExecutor_createScheduler_jmhTest._jmh_tryInit_f_createschedulerbenchmarkexecutor0_0(CreateSchedulerBenchmarkExecutor_createScheduler_jmhTest.java:342)
	at org.apache.flink.scheduler.benchmark.e2e.generated.CreateSchedulerBenchmarkExecutor_createScheduler_jmhTest.createScheduler_SingleShotTime(CreateSchedulerBenchmarkExecutor_createScheduler_jmhTest.java:294)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:453)
	at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:437)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	... 15 more{code}
It seems that the benchmarks are still calling {{SlotPoolImpl}}, which is removed in FLINK-22477. But it's weird that {{SlotPoolImpl}} is removed on July 13th, while the benchmark is not running until July 22nd. I think maybe it's related to JMH?",,pnowojski,Thesharing,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 26 12:47:39 UTC 2021,,,,,,,,,,"0|z0tb9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/21 12:47;chesnay;The problem was that the test-jars for both flink-runtime_2.11 and flink-runtime (without a suffix) were put on the classpath, because the poms were not updated after the removal of the suffix.;;;","26/Jul/21 12:47;chesnay;benchmark-master: a310cfe273c14b479d7590dee0621ac1e0c1acc9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"For page deployment/cli/,the name of the configuration file is flink-conf.yaml instead of flink-config.yaml",FLINK-23489,13391656,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,hapihu,hapihu,hapihu,24/Jul/21 06:28,28/Aug/21 12:21,13/Jul/23 08:12,26/Jul/21 02:37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Documentation,,,,,0,pull-request-available,,,,"The relevant pages are as follows：
 # [https://ci.apache.org/projects/flink/flink-docs-master/docs/deployment/cli/]
 # [https://ci.apache.org/projects/flink/flink-docs-master/zh/docs/deployment/cli/]

 

The related documents are as follows：
 # docs/content/docs/deployment/cli.md
 # docs/content.zh/docs/deployment/cli.md

 

In these pages, *conf/flink-config.yaml* should be changed to *conf/flink-conf.yaml.*

 

 ",,hapihu,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/21 06:28;hapihu;image-20210724141503058.png;https://issues.apache.org/jira/secure/attachment/13031003/image-20210724141503058.png","24/Jul/21 06:28;hapihu;image-20210724141542029.png;https://issues.apache.org/jira/secure/attachment/13031004/image-20210724141542029.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 26 02:37:13 UTC 2021,,,,,,,,,,"0|z0taio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/21 15:27;hapihu;Hi [~jark] 
Excuse me for taking up your time.
I created pull request for this issue.
Would you like to review it for me.
If there is any problem, please inform me in time. 
I active support the modification.
Thank you very much!;;;","26/Jul/21 02:37;jark;Fixed in master: 6df9bdf1251a50cafe8287885b7a9cd2c5b44aff;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IRSA doesn't work with S3,FLINK-23487,13391564,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,airblader,airblader,airblader,23/Jul/21 12:07,14/Jul/22 05:44,13/Jul/23 08:12,06/Aug/21 05:25,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,FileSystems,,,,,1,pull-request-available,,,,"Using IRSA (IAM Role for Service Account) currently doesn't work with s3 (both presto and hadoop) and fails with Access Denied. This has been brought up previously in FLINK-18676, but seems to be broken still or again.

We have tested a patch which updates the version for AWS and presto-hive in Flink, and doing so we successfully verified that it works with s3-presto. However, this didn't yet fix s3-hadoop. The patch will be posted in the comments.

Another curious finding was FLINK-17859 which states that presto-hive had been updated. However, this seems to be the case neither in the posted Flink 1.8 link nor current master.",,airblader,linqyd218,qinjunjerry,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23635,,,,,,,FLINK-18676,FLINK-17859,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 14 05:44:43 UTC 2022,,,,,,,,,,"0|z0t9y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/21 12:09;airblader;Patch which works at least for s3-presto. Note that we exclude aws-java-sdk-sts which is now a direct dependency of presto-hive as well, so we need to make sure to use our own version.
{code:java}
From 85a0c88e2bc81ee18e2fc6b06b760009aa723d60 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Ingo=20B=C3=BCrk?= <ingo.buerk@tngtech.com>
Date: Fri, 23 Jul 2021 08:48:12 +0200
Subject: [PATCH] Test---
 flink-filesystems/flink-s3-fs-base/pom.xml                  | 2 +-
 flink-filesystems/flink-s3-fs-presto/pom.xml                | 6 +++++-
 .../apache/flink/fs/s3presto/PrestoS3FileSystemITCase.java  | 3 ++-
 3 files changed, 8 insertions(+), 3 deletions(-)diff --git a/flink-filesystems/flink-s3-fs-base/pom.xml b/flink-filesystems/flink-s3-fs-base/pom.xml
index eb2b5b76582..78588c6bb18 100644
--- a/flink-filesystems/flink-s3-fs-base/pom.xml
+++ b/flink-filesystems/flink-s3-fs-base/pom.xml
@@ -31,7 +31,7 @@ under the License.
 	<packaging>jar</packaging>
 
 	<properties>
-		<fs.s3.aws.version>1.11.788</fs.s3.aws.version>
+		<fs.s3.aws.version>1.11.951</fs.s3.aws.version>
 	</properties>
 
 	<dependencies>
diff --git a/flink-filesystems/flink-s3-fs-presto/pom.xml b/flink-filesystems/flink-s3-fs-presto/pom.xml
index 4048ce59d22..11d4446cf8a 100644
--- a/flink-filesystems/flink-s3-fs-presto/pom.xml
+++ b/flink-filesystems/flink-s3-fs-presto/pom.xml
@@ -32,7 +32,7 @@ under the License.
 	<packaging>jar</packaging>
 
 	<properties>
-		<presto.version>0.187</presto.version>
+		<presto.version>0.257</presto.version>
 	</properties>
 
 	<dependencies>
@@ -67,6 +67,10 @@ under the License.
 					<groupId>com.amazonaws</groupId>
 					<artifactId>aws-java-sdk-s3</artifactId>
 				</exclusion>
+				<exclusion>
+					<groupId>com.amazonaws</groupId>
+					<artifactId>aws-java-sdk-sts</artifactId>
+				</exclusion>
 
 				<!-- lot's of unneeded stuff for the S3 file system -->
 				<exclusion>
diff --git a/flink-filesystems/flink-s3-fs-presto/src/test/java/org/apache/flink/fs/s3presto/PrestoS3FileSystemITCase.java b/flink-filesystems/flink-s3-fs-presto/src/test/java/org/apache/flink/fs/s3presto/PrestoS3FileSystemITCase.java
index 82ba609c0d0..49cc2166ea5 100644
--- a/flink-filesystems/flink-s3-fs-presto/src/test/java/org/apache/flink/fs/s3presto/PrestoS3FileSystemITCase.java
+++ b/flink-filesystems/flink-s3-fs-presto/src/test/java/org/apache/flink/fs/s3presto/PrestoS3FileSystemITCase.java
@@ -34,7 +34,6 @@ import java.util.Arrays;
 import java.util.List;
 import java.util.UUID;
 
-import static com.facebook.presto.hive.s3.PrestoS3FileSystem.S3_USE_INSTANCE_CREDENTIALS;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.fail;
 
@@ -49,6 +48,8 @@ import static org.junit.Assert.fail;
 @RunWith(Parameterized.class)
 public class PrestoS3FileSystemITCase extends AbstractHadoopFileSystemITTest {
 
+    private static final String S3_USE_INSTANCE_CREDENTIALS = ""presto.s3.use-instance-credentials"";
+
     @Parameterized.Parameter public String scheme;
 
     @Parameterized.Parameters(name = ""Scheme = {0}"")
-- 
2.32.0
{code};;;","04/Aug/21 12:42;twalthr;Fixed in 1.14.0: 48526e43bdbd4e837be863e4ac002ec7d8b03c64;;;","04/Aug/21 12:43;twalthr;This is quite a big change in dependencies which is why we haven't merged it to 1.13 yet. We might reconsider this based on user feedback.;;;","05/Aug/21 07:19;twalthr;Temporarily reverted in 452fb6774521eec112941549dae679cf9df1a7b6.;;;","06/Aug/21 05:25;twalthr;Hopefully fixed this time in 1.14.0: e8325ccc767740bcbe6505e2b2c64babb7714518;;;","14/Jul/22 05:44;linqyd218;Hey, when will the hadoop fix be available? Since the FileSink doesn't support presto, this issue doesn't get resolved for FileSink.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IncrementalAggregateJsonPlanTest.testIncrementalAggregateWithSumCountDistinctAndRetraction fail,FLINK-23479,13391482,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,dwysakowicz,dwysakowicz,23/Jul/21 05:52,09/Aug/21 12:41,13/Jul/23 08:12,09/Aug/21 12:41,1.13.1,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Table SQL / Planner,,,,,0,pull-request-available,stale-blocker,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20864&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=8904

{code}
Jul 23 04:21:56 [ERROR] testIncrementalAggregateWithSumCountDistinctAndRetraction(org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest)  Time elapsed: 0.067 s  <<< FAILURE!
Jul 23 04:21:56 org.junit.ComparisonFailure: expected:<...RyaWJ1dGVzcQB+AAFMAA[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')""}]},""description"":""LocalGroupAggregate(groupBy=[b, $f2], partialFinalType=[PARTIAL], select=[b, $f2, SUM_RETRACT(b1) AS (sum$0, count$1), COUNT_RETRACT(distinct$0 b1) AS count$2, COUNT_RETRACT(*) AS count1$3, DISTINCT(b1) AS distinct$0])""},{""class"":""org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecExchange"",""id"": 0,""inputProperties"":[{""requiredDistribution"":{""type"":""HASH"",""keys"":[0,1]},""damBehavior"":""PIPELINED"",""priority"":0}],""outputType"":{""type"":""ROW"",""nullable"":true,""fields"":[{""b"":""BIGINT NOT NULL""},{""$f2"":""INT NOT NULL""},{""sum$0"":""INT""},{""count$1"":""BIGINT""},{""count$2"":""BIGINT""},{""count1$3"":""BIGINT""},{""distinct$0"":""RAW('org.apache.flink.table.api.dataview.MapView', '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]9uAAAAAAAAAAASAAB4cQ...> but was:<...RyaWJ1dGVzcQB+AAFMAA[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')""}]},""description"":""LocalGroupAggregate(groupBy=[b, $f2], partialFinalType=[PARTIAL], select=[b, $f2, SUM_RETRACT(b1) AS (sum$0, count$1), COUNT_RETRACT(distinct$0 b1) AS count$2, COUNT_RETRACT(*) AS count1$3, DISTINCT(b1) AS distinct$0])""},{""class"":""org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecExchange"",""id"": 0,""inputProperties"":[{""requiredDistribution"":{""type"":""HASH"",""keys"":[0,1]},""damBehavior"":""PIPELINED"",""priority"":0}],""outputType"":{""type"":""ROW"",""nullable"":true,""fields"":[{""b"":""BIGINT NOT NULL""},{""$f2"":""INT NOT NULL""},{""sum$0"":""INT""},{""count$1"":""BIGINT""},{""count$2"":""BIGINT""},{""count1$3"":""BIGINT""},{""distinct$0"":""RAW('org.apache.flink.table.api.dataview.MapView', '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]9uAAAAAAAAAAASAAB4cQ...>
Jul 23 04:21:56 	at org.junit.Assert.assertEquals(Assert.java:117)
Jul 23 04:21:56 	at org.junit.Assert.assertEquals(Assert.java:146)
Jul 23 04:21:56 	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyJsonPlan(TableTestBase.scala:752)
Jul 23 04:21:56 	at org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalAggregateJsonPlanTest.testIncrementalAggregateWithSumCountDistinctAndRetraction(IncrementalAggregateJsonPlanTest.java:110)
Jul 23 04:21:56 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 23 04:21:56 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 23 04:21:56 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 23 04:21:56 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 23 04:21:56 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jul 23 04:21:56 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jul 23 04:21:56 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jul 23 04:21:56 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jul 23 04:21:56 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Jul 23 04:21:56 	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
Jul 23 04:21:56 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Jul 23 04:21:56 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Jul 23 04:21:56 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 23 04:21:56 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jul 23 04:21:56 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jul 23 04:21:56 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Jul 23 04:21:56 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Jul 23 04:21:56 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Jul 23 04:21:56 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Jul 23 04:21:56 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Jul 23 04:21:56 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Jul 23 04:21:56 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Jul 23 04:21:56 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 23 04:21:56 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Jul 23 04:21:56 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Jul 23 04:21:56 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Jul 23 04:21:56 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Jul 23 04:21:56 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Jul 23 04:21:56 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jul 23 04:21:56 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jul 23 04:21:56 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jul 23 04:21:56 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

{code}",,dwysakowicz,godfreyhe,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23434,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 09 12:41:14 UTC 2021,,,,,,,,,,"0|z0t9g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/21 06:22;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20863&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=8925;;;","23/Jul/21 10:50;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20881&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=9182;;;","23/Jul/21 10:51;dwysakowicz;cc [~godfrey] [~jark][~Leonard Xu];;;","23/Jul/21 12:45;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20879&view=results;;;","23/Jul/21 15:53;dwysakowicz;Reverted FLINK-23434 because of test instabilities:

*  master
** b62382e634015733754b05c2ad351d9748ced2fb
* 1.13
** ea2bddc8e82ab4b9797f40534b1e5637eef4c5b5

;;;","23/Jul/21 15:54;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20884&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=8925;;;","23/Jul/21 19:23;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20889&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=9182;;;","24/Jul/21 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","26/Jul/21 03:33;godfreyhe;Thanks for reporting this [~dwysakowicz], I will fix it ASAP.;;;","26/Jul/21 16:25;zhuzh;another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20976&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","09/Aug/21 12:41;godfreyhe;Fixed in 1.14.0: 785c72a5e9dc317584be92e9f174b652650ebe72
Fixed in 1.13.3: 68355c4702bc5e6e1e83d6404e7c6e4f0ccfcfd9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KubernetesSharedInformerITCase.testWatchWithBlockHandler fails on Azure,FLINK-23478,13391481,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yittg,dwysakowicz,dwysakowicz,23/Jul/21 05:50,28/Aug/21 12:22,13/Jul/23 08:12,11/Aug/21 07:35,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Deployment / Kubernetes,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20864&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=3190

{code}
Jul 23 04:41:49 java.util.concurrent.ExecutionException: java.lang.AssertionError
Jul 23 04:41:49 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Jul 23 04:41:49 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
Jul 23 04:41:49 	at org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformerITCase.testWatchWithBlockHandler(KubernetesSharedInformerITCase.java:140)
Jul 23 04:41:49 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 23 04:41:49 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 23 04:41:49 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 23 04:41:49 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 23 04:41:49 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jul 23 04:41:49 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jul 23 04:41:49 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jul 23 04:41:49 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jul 23 04:41:49 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Jul 23 04:41:49 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Jul 23 04:41:49 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jul 23 04:41:49 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Jul 23 04:41:49 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 23 04:41:49 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jul 23 04:41:49 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jul 23 04:41:49 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Jul 23 04:41:49 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Jul 23 04:41:49 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Jul 23 04:41:49 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Jul 23 04:41:49 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Jul 23 04:41:49 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Jul 23 04:41:49 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Jul 23 04:41:49 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Jul 23 04:41:49 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Jul 23 04:41:49 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 23 04:41:49 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Jul 23 04:41:49 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Jul 23 04:41:49 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Jul 23 04:41:49 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Jul 23 04:41:49 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Jul 23 04:41:49 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jul 23 04:41:49 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jul 23 04:41:49 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jul 23 04:41:49 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Jul 23 04:41:49 Caused by: java.lang.AssertionError
Jul 23 04:41:49 	at org.junit.Assert.fail(Assert.java:87)
Jul 23 04:41:49 	at org.junit.Assert.assertTrue(Assert.java:42)
Jul 23 04:41:49 	at org.junit.Assert.assertTrue(Assert.java:53)
Jul 23 04:41:49 	at org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformerITCase$TestingBlockCallbackHandler.lambda$onAddOrUpdated$0(KubernetesSharedInformerITCase.java:292)
Jul 23 04:41:49 	at org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformerITCase.check(KubernetesSharedInformerITCase.java:311)
Jul 23 04:41:49 	at org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformerITCase.access$800(KubernetesSharedInformerITCase.java:54)
Jul 23 04:41:49 	at org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformerITCase$TestingBlockCallbackHandler.onAddOrUpdated(KubernetesSharedInformerITCase.java:292)
Jul 23 04:41:49 	at org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformerITCase$TestingBlockCallbackHandler.onAdded(KubernetesSharedInformerITCase.java:277)
Jul 23 04:41:49 	at org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformer$EventHandler.lambda$null$1(KubernetesSharedInformer.java:227)
Jul 23 04:41:49 	at org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformer$WatchCallback.lambda$run$0(KubernetesSharedInformer.java:273)
Jul 23 04:41:49 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
Jul 23 04:41:49 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Jul 23 04:41:49 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Jul 23 04:41:49 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Jul 23 04:41:49 	at java.lang.Thread.run(Thread.java:748)

{code}",,dwysakowicz,trohrmann,wangyang0918,yittg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23481,,FLINK-23572,,,,,,,,,FLINK-22802,,,,,,,,,,,,,,,,"27/Jul/21 02:56;wangyang0918;mvn-1.log;https://issues.apache.org/jira/secure/attachment/13031083/mvn-1.log",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 11 07:35:12 UTC 2021,,,,,,,,,,"0|z0t9fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/21 06:45;wangyang0918;[~yittg] Could you please have a look on this failed test?;;;","23/Jul/21 07:48;yittg;[~fly_in_gis] yeah, i'll dig it;;;","23/Jul/21 09:32;yittg;[~fly_in_gis] The cause is that a simpler lock can not guarantee the absolute order event it is a ""fair"" lock,  because the later event can be executed earlier to the cached thread pool executor, so the lock attempt occures earlier also.

Maybe we still need to manually handle it in the {{WatchCallback}}, like submit a runnable if events come and terminate if all queued events are processed in a short period.

You can assign this issue to me, I'll try to resolve it ASAP. 

cc [~dmvk];;;","23/Jul/21 10:50;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20881&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=3176;;;","23/Jul/21 13:26;yittg;[~fly_in_gis] I've added a patch to fix this issue, please help to review it.

Sorry for the inconvenience.;;;","24/Jul/21 18:57;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20896&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=3156;;;","26/Jul/21 06:01;wangyang0918;Fixed in master via

2ab506208318ae87b97ec1b8b4b04e14af10b00e

ceb57439de0660d12f09c364473b7ece5203d26e;;;","26/Jul/21 06:50;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20904&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=89ed5489-a970-5ff2-67f7-d7391de0165f&l=3185;;;","26/Jul/21 07:01;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20925&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=2a8cc459-df7a-5e6f-12bf-96efcc369aa9&l=3173;;;","26/Jul/21 07:08;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20928&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=3194;;;","26/Jul/21 07:09;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20932&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=3184;;;","27/Jul/21 01:37;yittg;[~fly_in_gis]
Looks like the {{KubernetesSharedInformerITCase}} still has some other issues, would we create another ticket to track it?

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20995&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=3144;;;","27/Jul/21 02:57;wangyang0918;[~yittg] I have attached the mvn logs. It seems that the events still could not be processes orderly.;;;","27/Jul/21 03:17;yittg;Thanks [~fly_in_gis]. I'll try to find the root cause again. It's a sad test case, :(;;;","27/Jul/21 03:41;wangyang0918;However, I have run the test {{KubernetesSharedInformerITCase#testWatchWithBlockHandler}} more than 1000 times and could not reproduce the failure.;;;","27/Jul/21 04:12;yittg;Maybe we can make use of Azure to reproduce it by removing all other e2e tests and only running the {{KubernetesSharedInformerITCase}} multiple times with some useful log?;;;","27/Jul/21 05:14;wangyang0918;I agree with you that we may need to use the Azure to reproduce this failure. I will reopen this ticket and downgrade the priority since it happens rarely.

 

Feel free to upgrade the priority if we have some more other instances.;;;","27/Jul/21 05:21;yittg;I happened to reproduce the issue locally, it seems the indexer of {{SharedIndexInformer}} is incredibly not serial. 
 I need to further confirm this conclusion, hope i am wrong.

It can happen in {{EventHandler#onModified}} like following:
{code:java}
private class EventHandler {
...
        private void onModified(T obj) {
            System.out.printf(
                    ""M:%s:%s\n%s\n"",
                    obj.getMetadata().getName(), obj.getMetadata().getResourceVersion(), obj);
            this.callbacks.forEach(
                    (id, callback) -> callback.run(h -> h.onModified(wrapEvent(obj))));
        }
...
}

M:shared-informer-test-cluster-1627362096046:136167
ConfigMap(apiVersion=v1, binaryData=null, data={val=7},  ... name=shared-informer-test-cluster-1627362096046, namespace=default, ownerReferences=[], resourceVersion=136167, selfLink=/api/v1/namespaces/default/configmaps/shared-informer-test-cluster-1627362096046, uid=7ad189fc-52c2-4c36-a9f8-0da8ea71ae29, additionalProperties={}), additionalProperties={})

M:shared-informer-test-cluster-1627362096046:136168
ConfigMap(apiVersion=v1, binaryData=null, data={val=8}, ... name=shared-informer-test-cluster-1627362096046, namespace=default, ownerReferences=[], resourceVersion=136168, selfLink=/api/v1/namespaces/default/configmaps/shared-informer-test-cluster-1627362096046, uid=7ad189fc-52c2-4c36-a9f8-0da8ea71ae29, additionalProperties={}), additionalProperties={})

M:shared-informer-test-cluster-1627362096046:136167
ConfigMap(apiVersion=v1, binaryData=null, data={val=7}, ... name=shared-informer-test-cluster-1627362096046, namespace=default, ownerReferences=[], resourceVersion=136167, selfLink=/api/v1/namespaces/default/configmaps/shared-informer-test-cluster-1627362096046, uid=7ad189fc-52c2-4c36-a9f8-0da8ea71ae29, additionalProperties={}), additionalProperties={})

{code};;;","27/Jul/21 06:15;yittg;It is likely caused by the following logical, the first {{reListAndSync}} in schedule executor(can not be disabled by setting {{resyncPeriodMillis}} to a large value) conflicts with the outer list and watch without any guards. I'm confused about this implementation.
{code:java}
package io.fabric8.kubernetes.client.informers.cache;
...
public class Reflector<T extends HasMetadata, L extends KubernetesResourceList<T>> {
...
  public void listAndWatch() throws Exception {
    try {
      log.info(""Started ReflectorRunnable watch for {}"", apiTypeClass);
      reListAndSync();
      resyncExecutor.scheduleWithFixedDelay(this::reListAndSync, 0L, resyncPeriodMillis, TimeUnit.MILLISECONDS);
      startWatcher();
    } catch (Exception exception) {
      store.isPopulated(false);
      throw new RejectedExecutionException(""Error while starting ReflectorRunnable watch"", exception);
    }
  }
{code}

Maybe the {{SharedInformer#hasSynced}} can help, i will dive it deeper.;;;","27/Jul/21 07:05;wangyang0918;A side input: I am working on FLINK-22802 to bump the fabric8 Kubernetes client to 5.5.0, which could resolve the resync bugs.;;;","27/Jul/21 07:28;yittg;Glad to hear the news, i think it can be fixed after upgrading the client.;;;","30/Jul/21 09:20;dwysakowicz;I also faced the jvm crash with the test: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21196&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=3123;;;","01/Aug/21 02:52;wangyang0918;FLINK-22802 has been merged. Let's keep this ticket open and verify whether it is fixed truly.;;;","06/Aug/21 03:29;yittg;Hi [~dwysakowicz], [~chesnay],  [~xtsong], thanks for your report about the test error. Did you find more failed case after merging [FLINK-22802].

[~wangyang0918] We can keep this ticket OPEN until next Monday (almost one week) or longer if no more failed case?;;;","11/Aug/21 07:35;wangyang0918;I will close this ticket since we do not find more related instances. Feel free to reopen it if necessary.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Snapshot deployments are broken,FLINK-23476,13391431,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,22/Jul/21 20:25,23/Jul/21 06:31,13/Jul/23 08:12,22/Jul/21 20:38,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Build System / Azure Pipelines,,,,,0,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20855&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7
{code}
[ERROR]     'dependencies.dependency.version' for com.typesafe.akka:akka-testkit_2.11:jar must be a valid version but is '${akka.version}'. @ line 1212, column 15
{code}",,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18783,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 23 06:31:20 UTC 2021,,,,,,,,,,"0|z0t94o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/21 20:38;chesnay;master: b19a515ec3ec593c4e1c05c0dbf5c971e104fdf4 ;;;","23/Jul/21 05:37;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20855&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7;;;","23/Jul/21 06:28;chesnay;well that's the same build I linked :/;;;","23/Jul/21 06:31;dwysakowicz;I did not look into the description when going over the build failures, my bad, sorry.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InputStatus should not contain END_OF_RECOVERY,FLINK-23474,13391350,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,22/Jul/21 12:31,28/Aug/21 12:22,13/Jul/23 08:12,26/Jul/21 10:59,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,API / DataStream,Runtime / Task,,,,0,pull-request-available,,,,"We added the END_OF_RECOVERY enum value in order to support recovery of unaligned checkpoints with rescaling.

However the InputStatus is expose in a public interface via {{SourceReader}}. At the same time it is not a valid value which the {{SourceReader}} can return.

We should internally replace the InputStatus with an internal equivalent.",,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19801,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 26 10:59:26 UTC 2021,,,,,,,,,,"0|z0t8mo:",9223372036854775807,InputStatus.END_OF_RECOVERY was removed. It was an internal flag that should never be returned from SourceReaders. Returning that value in earlier versions might lead to misbehaviour.,,,,,,,,,,,,,,,,,,,"26/Jul/21 10:59;dwysakowicz;Fixed in master 684d56ebce83c9bf69a3bd070a8a0aefd3b380ff;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StateBackendContext does not close underlying MiniCluster,FLINK-23469,13391300,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,dwysakowicz,dwysakowicz,22/Jul/21 09:28,22/Jul/21 09:58,13/Jul/23 08:12,22/Jul/21 09:58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Benchmarks,,,,,0,pull-request-available,,,,"The StateBackendContext overrides the {{tearDown()}} method, but does not call the {{super.tearDown()}} which leaves the {{MiniCluster}} behind.

http://codespeed.dak8s.net:8080/job/flink-master-benchmarks/8123/consoleFull

{code}
10:00:29  Thread[flink-akka.actor.default-dispatcher-31,5,main]
10:00:29    at sun.misc.Unsafe.park(Native Method)
10:00:29    at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
10:00:29    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
10:00:29    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
10:00:29  
10:00:29  Thread[flink-metrics-6,1,main]
10:00:29    at sun.misc.Unsafe.park(Native Method)
10:00:29    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
10:00:29    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
10:00:29    at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
10:00:29    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
10:00:29    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
10:00:29    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
10:00:29    at java.lang.Thread.run(Thread.java:748)
10:00:29  
10:00:29  Thread[pool-4-thread-1,5,main]
10:00:29    at sun.misc.Unsafe.park(Native Method)
10:00:29    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
10:00:29    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
10:00:29    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
10:00:29    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
10:00:29    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
10:00:29    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
10:00:29    at java.lang.Thread.run(Thread.java:748)
10:00:29  
10:00:29  Thread[flink-akka.actor.default-dispatcher-33,5,main]
10:00:29    at sun.misc.Unsafe.park(Native Method)
10:00:29    at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
10:00:29    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
10:00:29    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
10:00:29  
10:00:29  
10:00:29  <shutdown timeout of 30 seconds expired, forcing forked VM to exit>
{code}",,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 22 09:58:47 UTC 2021,,,,,,,,,,"0|z0t8bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/21 09:58;chesnay;benchmark-master: jmh exception not failing benchmark;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Benchmarks networkThroughput for OpenSSL do not run,FLINK-23468,13391295,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pnowojski,dwysakowicz,dwysakowicz,22/Jul/21 09:03,01/Sep/21 09:44,13/Jul/23 08:12,01/Sep/21 06:17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Benchmarks,,,,,0,pull-request-available,,,,"Benchmarks for OpenSSL do not run because we do not have OpenSSL on the machines:

http://codespeed.dak8s.net:8080/job/flink-master-benchmarks/8123/consoleFull

{code}
10:28:52  # Warmup Iteration   1: <failure>
10:28:52  
10:28:52  java.lang.AssertionError: openSSL not available
10:28:52  	at org.junit.Assert.fail(Assert.java:88)
10:28:52  	at org.junit.Assert.assertTrue(Assert.java:41)
10:28:52  	at org.apache.flink.runtime.net.SSLUtilsTest.addSslProviderConfig(SSLUtilsTest.java:524)
10:28:52  	at org.apache.flink.runtime.net.SSLUtilsTest.createInternalSslConfigWithKeyAndTrustStores(SSLUtilsTest.java:489)
10:28:52  	at org.apache.flink.benchmark.StreamNetworkThroughputBenchmarkExecutor$MultiEnvironment.setUp(StreamNetworkThroughputBenchmarkExecutor.java:92)
10:28:52  	at org.apache.flink.benchmark.generated.StreamNetworkThroughputBenchmarkExecutor_networkThroughput_jmhTest._jmh_tryInit_f_multienvironment1_1(StreamNetworkThroughputBenchmarkExecutor_networkThroughput_jmhTest.java:351)
10:28:52  	at org.apache.flink.benchmark.generated.StreamNetworkThroughputBenchmarkExecutor_networkThroughput_jmhTest.networkThroughput_Throughput(StreamNetworkThroughputBenchmarkExecutor_networkThroughput_jmhTest.java:73)
10:28:52  	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
10:28:52  	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
10:28:52  	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
10:28:52  	at java.lang.reflect.Method.invoke(Method.java:498)
10:28:52  	at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:453)
10:28:52  	at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:437)
10:28:52  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
10:28:52  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
10:28:52  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
10:28:52  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
10:28:52  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
10:28:52  	at java.lang.Thread.run(Thread.java:748)
10:28:52  
{code}",,dwysakowicz,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13172,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 01 06:17:38 UTC 2021,,,,,,,,,,"0|z0t8ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/21 09:42;pnowojski;They are running, but testing non ssl code? 

Besides fixing this SSL issue we should fix the benchmarks that the build should be failing if something like that is happening :/;;;","31/Aug/21 13:41;pnowojski;After checking the underlying exception:

{noformat}
java.lang.UnsatisfiedLinkError: /tmp/liborg_apache_flink_shaded_netty4_netty_tcnative_linux_x86_642806798427875031792.so: libssl.so.1.0.0: cannot open shared object file: No such file or directory
{noformat}

I figured out that I have to manually download and install libssl1.0.0_1.0.2n-1ubuntu5.7_amd64.deb package on the benchmarking machine which solved the problem. This solution was found [here|https://askubuntu.com/a/1331642];;;","31/Aug/21 13:48;chesnay;FYI this is also what we do in Flink: https://github.com/apache/flink/blob/981d7bc670d08618277ea53c9045f3699b10cea4/tools/azure-pipelines/jobs-template.yml#L254;;;","31/Aug/21 14:31;pnowojski;Thanks [~chesnay], good to know.

I've also enabled {{foe}} (fail on error) flag when running benchmarks on jenkins. In the future in case of such problems, benchmarks will be failing instead of being silently ignored. ;;;","01/Sep/21 06:17;pnowojski;Small improvement in error reporting merged as e050a48 into apache:master now;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnalignedCheckpointITCase hangs on Azure,FLINK-23466,13391269,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,dwysakowicz,dwysakowicz,22/Jul/21 07:18,15/Dec/21 01:44,13/Jul/23 08:12,16/Nov/21 08:37,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.3,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20813&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=16016

The problem is the buffer listener will be removed from the listener queue when notified and then it will be added to the listener queue again if it needs more buffers. However, if some buffers are recycled meanwhile, the buffer listener will not be notified of the available buffers. For example:

    1. Thread 1 calls LocalBufferPool#recycle().
    2. Thread 1 reaches LocalBufferPool#fireBufferAvailableNotification() and listener.notifyBufferAvailable() is invoked, but Thread 1 sleeps before acquiring the lock to registeredListeners.add(listener).
    3. Thread 2 is being woken up as a result of notifyBufferAvailable() call. It takes the buffer, but it needs more buffers.
    4. Other threads, return all buffers, including this one that has been recycled. None are taken. Are all in the LocalBufferPool.
    5. Thread 1 wakes up, and continues fireBufferAvailableNotification() invocation.
    6. Thread 1 re-adds listener that's waiting for more buffer registeredListeners.add(listener).
    7. Thread 1 exits loop LocalBufferPool#recycle(MemorySegment, int) inside, as the original memory segment has been used.

At the end we have a state where all buffers are in the LocalBufferPool, so no new recycle() calls will happen, but there is still one listener waiting for a buffer (despite buffers being available).",,akalashnikov,dwysakowicz,kevin.cyj,pnowojski,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24919,FLINK-16641,,,,FLINK-24035,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 16 08:37:29 UTC 2021,,,,,,,,,,"0|z0t84o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/21 08:00;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22340&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=17086;;;","20/Aug/21 13:51;akalashnikov;Unfortunately, I failed with reproducing this problem locally, but here is some information from cluster log: 
One of subtasks can not by restored by some reason:

{noformat}

03:28:52,885 [ Checkpoint Timer] INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Failed to trigger checkpoint for job 776bcb49e2efd18ee411248fa66ed39d since Checkpoint triggering task 
keyed (15/20) of job 776bcb49e2efd18ee411248fa66ed39d is not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.

{noformat}

{noformat}
""keyed (15/20)#4"" #20483 prio=5 os_prio=0 tid=0x00007fbe200f1000 nid=0x383c waiting on condition [0x00007fbc416d4000] 
 java.lang.Thread.State: TIMED_WAITING (parking) 
 at sun.misc.Unsafe.park(Native Method) 
 - parking to wait for <0x0000000094200658> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) 
 at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215) 
 at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163) 
 at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:149) 
 at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:335) 
 at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324) 
 at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201) 
 at org.apache.flink.streaming.runtime.tasks.StreamTask.executeRestore(StreamTask.java:669) 
 at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$1116/826634555.run(Unknown Source) 
 at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:785) 
 at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:638) 
 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) 
 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:572) 
 at java.lang.Thread.run(Thread.java:748)
{noformat}

In one log I see that `restore` is stuck on requestBufferBlocking which may indicate taht we have problem with buffers availability(the only test with buffersPerChannel=0 fails) but unfortunatelly, I don't see the same log in the second failed build which may mean that we have different problem or more than one problem(or threaddump was taken in wrong moment):

{noformat}
""channel-state-unspilling-thread-1"" #17216 daemon prio=5 os_prio=0 tid=0x00007f6660005000 nid=0x739d in Object.wait() [0x00007f63ff1f0000] 
 java.lang.Thread.State: WAITING (on object monitor) 
 at java.lang.Object.wait(Native Method) 
 at java.lang.Object.wait(Object.java:502) 
 at org.apache.flink.runtime.io.network.partition.consumer.BufferManager.requestBufferBlocking(BufferManager.java:117) 
 - locked <0x00000000954d5fa8> (a org.apache.flink.runtime.io.network.partition.consumer.BufferManager$AvailableBufferQueue) 
 at org.apache.flink.runtime.io.network.partition.consumer.RecoveredInputChannel.requestBufferBlocking(RecoveredInputChannel.java:268) 
 at org.apache.flink.runtime.checkpoint.channel.InputChannelRecoveredStateHandler.getBuffer(RecoveredChannelStateHandler.java:90) 
 at org.apache.flink.runtime.checkpoint.channel.InputChannelRecoveredStateHandler.getBuffer(RecoveredChannelStateHandler.java:69) 
 at org.apache.flink.runtime.checkpoint.channel.ChannelStateChunkReader.readChunk(SequentialChannelStateReaderImpl.java:198) 
 at org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReaderImpl.readSequentially(SequentialChannelStateReaderImpl.java:107) 
 at org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReaderImpl.read(SequentialChannelStateReaderImpl.java:93) 
 at org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReaderImpl.readInputData(SequentialChannelStateReaderImpl.java:64) 
 at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$restoreGates$0(StreamTask.java:609) 
 at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$1081/1369458294.run(Unknown Source) 
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 
 at java.lang.Thread.run(Thread.java:748)
{noformat}

Again, I see that `buffersPerChannel=0` and I see(at least in one build) the problem with buffer avilability. So maybe [~kevin.cyj] have any ideas in top of your head why it happens? In fact, I still don't sure that it is the true reason. 
The second reason can be the problem with compliting StateConsumedFuture after the recovery but there is no any clue that point to this.;;;","23/Aug/21 09:40;kevin.cyj;[~akalashnikov] Thanks for the analysis. Previously, there was no such test for unaligned checkpoint that 'buffersPerChannel=0', I added them in FLINK-16428. Maybe 'buffersPerChannel=0' still does not work for unaligned checkpoint. If so, we may fix it or remove this test. I will take a look at this;;;","24/Aug/21 09:52;kevin.cyj;Update:

After running the test over one day. I think I finally reproduced the case. The case ""hung"" there for a very long time, but finally, it succeeded. The common stack among the three cases is:
{code:java}
""Source: source (16/20)#4"" #10439 prio=5 os_prio=31 tid=0x00007f93973a2000 nid=0x4c91b sleeping[0x000070000b228000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
        at java.lang.Thread.sleep(Native Method)
        at org.apache.flink.test.checkpointing.UnalignedCheckpointTestBase$LongSource$LongSourceReader.pollNext(UnalignedCheckpointTestBase.java:319)
        at org.apache.flink.streaming.api.operators.SourceOperator.pollNext(SourceOperator.java:365)
        at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:325)
        at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:489)
        at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$1091/178711115.runDefaultAction(Unknown Source)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:819)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:746)
        at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$1279/2130631630.run(Unknown Source)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:785)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:728)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:786)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:572)
        at java.lang.Thread.run(Thread.java:748)
{code}
I guess this maybe a purely test case issue. I will further analysis it to find out why the thread stuck there that long.;;;","25/Aug/21 07:34;kevin.cyj;Update:

I reproduced the case where the thread is stuck in request buffer blocking. This time the test can not pass even after several hours. I will investigate this case further.;;;","25/Aug/21 11:13;kevin.cyj;Update:

From the dumped heap, I can see that the buffer manager is waiting for buffer and has registered itself as buffer listener in the buffer pool. At the same time the local buffer pool is full of buffers, but the buffer manager is not notified of available buffer. I guess there may be some concurrent issues.;;;","26/Aug/21 02:50;kevin.cyj;After reading the code, I think two issues may caused the problem:
 # After calling the requestMemorySegmentFromGlobal method, the buffer listener is not notified. I will try to fix and test it.
 # When recycle buffer, we remove the buffer listener from the queue first and then add it back, if there is any buffer recycled meanwhile, the listener will not be notified.;;;","26/Aug/21 03:05;kevin.cyj;I will try to fix the issues mentioned above and see if it solves the problem.;;;","26/Aug/21 06:46;pnowojski;Thanks for the investigation and writing down your suspicions. If the problem is with `BufferManager`, the bug was previously unnoticed, because exclusive buffers were never returned to the buffer pool? That's why only `buffersPerChannel=0` is surfacing the problem?

In a retrospect, maybe we should have randomised the number of exclusive buffers in {{org.apache.flink.streaming.util.TestStreamEnvironment}} to provide test coverage for 0 exclusive buffers mode in the ITCases?;;;","26/Aug/21 07:28;kevin.cyj;>>> Thanks for the investigation and writing down your suspicions. If the problem is with `BufferManager`, the bug was previously unnoticed, because exclusive buffers were never returned to the buffer pool? That's why only `buffersPerChannel=0` is surfacing the problem?

I think you are right, if there are exclusive buffers, the floating buffers are optional.

 

>>> In a retrospect, maybe we should have randomised the number of exclusive buffers in {{org.apache.flink.streaming.util.TestStreamEnvironment}} to provide test coverage for 0 exclusive buffers mode in the ITCases?

I think it is a good idea.;;;","26/Aug/21 09:31;pnowojski;I've bumped the priority to the release blocker, as I think we can hit this problem even with the default configuration options if there are just simply not enough buffers in the buffer pool. Before FLINK-16641 Flink would fail if there are no exclusive buffers for the input channels. No that's allowed, but can lead to this deadlock.;;;","27/Aug/21 06:47;kevin.cyj;I have opened a PR which fix the issue of recycle logic and tested the fix locally, the problem did not produce. For the requestMemorySegmentFromGlobal logic, it seems not easy to fix. Previously, I was thinking that we can unify the buffer lister to the availability listener. But I am afraid that it may cause performance issue when the number of input channels is large. Another way is to refactor the code and split the LocalBufferPool into two parts (two pool implementations), one contains the buffer listener logic and serves the input gates, the other contains the availability logic and serves the result partition. The logic of LocalBufferPool is pretty complicated. I think it is better to do some simplification. [~pnowojski] What do you think? Any suggestions?;;;","27/Aug/21 13:50;kevin.cyj;Previously, the buffer listener will be removed from the listener queue when notified and then it will be added to the listener queue again if it needs more buffers. However, if some buffers are recycled meanwhile, the buffer listener will not be notified of the available buffers. For example:

    

    1. Thread 1 calls LocalBufferPool#recycle().

    2. Thread 1 reaches LocalBufferPool#fireBufferAvailableNotification() and listener.notifyBufferAvailable() is invoked, but Thread 1 sleeps before acquiring the lock to registeredListeners.add(listener).

    3. Thread 2 is being woken up as a result of notifyBufferAvailable() call. It takes the buffer, but it needs more buffers.

    4. Other threads, return all buffers, including this one that has been recycled. None are taken. Are all in the LocalBufferPool.

    5. Thread 1 wakes up, and continues fireBufferAvailableNotification() invocation.

    6. Thread 1 re-adds listener that's waiting for more buffer registeredListeners.add(listener).

    7. Thread 1 exits loop LocalBufferPool#recycle(MemorySegment, int) inside, as the original memory segment has been used.

    

    At the end we have a state where all buffers are in the LocalBufferPool, so no new recycle() calls will happen, but there is still one listener waiting for a buffer (despite buffers being available).

    

    This change fixes the issue by letting the buffer listener request multiple buffers one after another without having to enqueue BufferListener to the registeredListener queue.;;;","30/Aug/21 09:09;pnowojski;Merged to master as 48a384dffc7
Merged to release-1.14 as 0067d35cc0f;;;","11/Nov/21 08:23;trohrmann;This problem seems to still occur: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26304&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=13067

Maybe it is a new problem that manifests with the same symptoms.;;;","15/Nov/21 18:12;trohrmann;Can this also be a problem affecting {{1.15.0}} [~pnowojski]?;;;","16/Nov/21 05:58;kevin.cyj;Nov 10 16:13:03 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointITCase#execute[pipeline with mixed channels, p = 20, timeout = 0, buffersPerChannel = 1].

From the log, we can see this case hangs. I guess this seems a new issue which is different from the one reported in this ticket. From the stack, it seems there is something wrong with the checkpoint coordinator, the following thread locked 0x0000000087db4fb8:
{code:java}
2021-11-10T17:14:21.0899474Z Nov 10 17:14:21 ""jobmanager-io-thread-2"" #12984 daemon prio=5 os_prio=0 tid=0x00007f12e000b800 nid=0x3fb6 runnable [0x00007f0fcd6d4000]
2021-11-10T17:14:21.0899924Z Nov 10 17:14:21    java.lang.Thread.State: RUNNABLE
2021-11-10T17:14:21.0900300Z Nov 10 17:14:21 	at java.util.HashMap$TreeNode.balanceDeletion(HashMap.java:2338)
2021-11-10T17:14:21.0900745Z Nov 10 17:14:21 	at java.util.HashMap$TreeNode.removeTreeNode(HashMap.java:2112)
2021-11-10T17:14:21.0901146Z Nov 10 17:14:21 	at java.util.HashMap.removeNode(HashMap.java:840)
2021-11-10T17:14:21.0901577Z Nov 10 17:14:21 	at java.util.LinkedHashMap.afterNodeInsertion(LinkedHashMap.java:301)
2021-11-10T17:14:21.0902002Z Nov 10 17:14:21 	at java.util.HashMap.putVal(HashMap.java:664)
2021-11-10T17:14:21.0902531Z Nov 10 17:14:21 	at java.util.HashMap.putMapEntries(HashMap.java:515)
2021-11-10T17:14:21.0902931Z Nov 10 17:14:21 	at java.util.HashMap.putAll(HashMap.java:785)
2021-11-10T17:14:21.0903429Z Nov 10 17:14:21 	at org.apache.flink.runtime.checkpoint.ExecutionAttemptMappingProvider.getVertex(ExecutionAttemptMappingProvider.java:60)
2021-11-10T17:14:21.0904060Z Nov 10 17:14:21 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.reportStats(CheckpointCoordinator.java:1867)
2021-11-10T17:14:21.0904686Z Nov 10 17:14:21 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:1152)
2021-11-10T17:14:21.0905372Z Nov 10 17:14:21 	- locked <0x0000000087db4fb8> (a java.lang.Object)
2021-11-10T17:14:21.0905895Z Nov 10 17:14:21 	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$acknowledgeCheckpoint$1(ExecutionGraphHandler.java:89)
2021-11-10T17:14:21.0906493Z Nov 10 17:14:21 	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler$$Lambda$1368/705813936.accept(Unknown Source)
2021-11-10T17:14:21.0907086Z Nov 10 17:14:21 	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119)
2021-11-10T17:14:21.0907698Z Nov 10 17:14:21 	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler$$Lambda$1369/1447418658.run(Unknown Source)
2021-11-10T17:14:21.0908210Z Nov 10 17:14:21 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2021-11-10T17:14:21.0908735Z Nov 10 17:14:21 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2021-11-10T17:14:21.0909333Z Nov 10 17:14:21 	at java.lang.Thread.run(Thread.java:748) {code}
But other thread is waiting for the lock. I am not familiar with these logics and not sure if this is in the right state. Could anyone who is familiar with these logics take a look?

 

BTW, concurrent access of HashMap may cause infinite loop，I see in the stack that there are multiple threads are accessing HashMap, though I am not sure if they are the same instance.;;;","16/Nov/21 08:37;pnowojski;I've extracted the newly reported issue to FLINK-24919;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Benchmarks aren't compiling,FLINK-23464,13391256,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,22/Jul/21 06:23,22/Jul/21 06:41,13/Jul/23 08:12,22/Jul/21 06:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Benchmarks,,,,,0,,,,,"{code}
19:07:09  [INFO] Compiling 1 Scala source and 64 Java sources to /home/jenkins/workspace/flink-master-benchmarks/flink-benchmarks/target/classes ...
19:07:13  [ERROR] /home/jenkins/workspace/flink-master-benchmarks/flink-benchmarks/src/main/java/org/apache/flink/benchmark/StreamGraphUtils.java:26:43:  error: cannot find symbol
19:07:13  [WARNING] javac exited with exit code 1
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23402,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 22 06:41:37 UTC 2021,,,,,,,,,,"0|z0t81s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/21 06:41;chesnay;benchmark-master: a8f7c71ecb244ef6937deb10a2555637b14da3be;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace <div> tags with ShortCodes,FLINK-23463,13391237,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,guoyangze,guoyangze,22/Jul/21 03:08,23/Sep/21 18:54,13/Jul/23 08:12,04/Aug/21 10:44,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Documentation,,,,,0,pull-request-available,,,,"In FLINK-22922, we migrate Flink website to hugo. At the moment, most of the div tag in user doc is no long take effect. We need to replace them with the ShortCodes.",,guoyangze,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 04 10:44:38 UTC 2021,,,,,,,,,,"0|z0t7xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/21 03:09;guoyangze;Can someone assign this to me?;;;","04/Aug/21 10:44;chesnay;master: b7906f6630a3dc831eac1f845dd0037b2f660eab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Translate the abfs documentation to chinese,FLINK-23462,13391204,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Liebing,psrinivasulu,psrinivasulu,21/Jul/21 21:20,09/Sep/21 08:09,13/Jul/23 08:12,09/Sep/21 08:09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,chinese-translation,Documentation,,,,1,pull-request-available,,,,Translate the documentation changes that were made in this PR to chinese https://github.com/apache/flink/pull/16559/ ,,Liebing,psrinivasulu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18562,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 09 08:09:18 UTC 2021,,,,,,,,,,"0|z0t7q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/21 06:14;Liebing;Hi, [~jark]. I'm willing to translate this document, Could you assign it to me? Thanks.;;;","11/Aug/21 09:36;arvid;This is now good to go. Thanks [~Liebing]!;;;","28/Aug/21 02:19;Liebing;Hi [~arvid]. I see that the corresponding English documents have been merged. Can you deal with this issue? ;;;","09/Sep/21 08:09;arvid;Merged into master as 19804811351c5a66503bf2c6d3b626d1fd1f8c65 and merged into 1.14 as 8e982b788562a5f7138c5386aadf70ecc2d99f08.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Properties map is not set in DebeziumAvroFormatFactory,FLINK-23450,13391119,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,twalthr,twalthr,21/Jul/21 11:58,15/Dec/21 01:40,13/Jul/23 08:12,03/Aug/21 08:02,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.0,,,Table SQL / Ecosystem,,,,,0,pull-request-available,,,,FLINK-21229 did not set the properties map correctly in DebeziumAvroFormatFactory.,,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 03 08:02:39 UTC 2021,,,,,,,,,,"0|z0t77c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/21 08:02;twalthr;Fixed in 1.14: 78db0e7c50235d7f5ae83225ac11192f5b121f7a

Fixed in 1.13: 55981a0ec8576da7776ef2d0fb803d7bcd905036

Fixed in 1.12: 33ad09863a81711ff79b040e12eabb051470ba40;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connection leak in XaFacadePoolingImpl,FLINK-23437,13390921,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,maver1ck,maver1ck,20/Jul/21 12:15,28/Aug/21 13:10,13/Jul/23 08:12,05/Aug/21 14:11,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Connectors / JDBC,,,,,0,pull-request-available,,,,"Hi,

I'm using JDBC XA connector to put data into Oracle database.

I'm facing issue with too many concurrent connection to database.

I changed this method to return XaFacadeImpl instead of XaFacadePoolingImpl and problem was solved.
{code:java}
static XaFacade fromXaDataSourceSupplier(
        Supplier<XADataSource> dataSourceSupplier, Integer timeoutSec) {
    return new XaFacadePoolingImpl(() -> new XaFacadeImpl(dataSourceSupplier, timeoutSec));
}
{code}
 ",,maver1ck,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22889,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 05 14:10:30 UTC 2021,,,,,,,,,,"0|z0t5zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/21 12:20;maver1ck;And the log:
{code:java}
10:44:21.338 [Source: Custom Source -> Sink: Output stream (1/1)#0] WARN  org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction - nothing to commit up to checkpoint: 80
10:44:21.373 [Source: Custom Source -> Sink: Output stream (1/1)#0] DEBUG org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction - snapshot state, checkpointId=81
10:44:21.374 [Source: Custom Source -> Sink: Output stream (1/1)#0] DEBUG org.apache.flink.connector.jdbc.xa.XaFacadeImpl - NETWORKS: end, xid=Optional[201:099b749c5e88f0e650d968606dacd9ac000000005100000000000000:619cadea]
10:44:21.374 [Source: Custom Source -> Sink: Output stream (1/1)#0] DEBUG org.apache.flink.connector.jdbc.xa.XaFacadeImpl - NETWORKS: prepare, xid=Optional[201:099b749c5e88f0e650d968606dacd9ac000000005100000000000000:619cadea]
10:44:21.374 [Source: Custom Source -> Sink: Output stream (1/1)#0] INFO  org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction - empty XA transaction (skip), xid: 201:099b749c5e88f0e650d968606dacd9ac000000005100000000000000:619cadea, checkpoint 81
10:44:21.418 [Source: Custom Source -> Sink: Output stream (1/1)#0] WARN  org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Sink: Output stream (1/1)#0 (8353219eab3eaabd015a6e6b00a06b92) switched from RUNNING to FAILED with failure cause: java.lang.Exception: Could not perform checkpoint 81 for operator Source: Custom Source -> Sink: Output stream (1/1)#0.
        at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpoint(StreamTask.java:1000)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$triggerCheckpointAsync$7(StreamTask.java:960)
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
        at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:344)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:330)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:202)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:681)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:636)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:647)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:620)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 81 for operator Source: Custom Source -> Sink: Output stream (1/1)#0. Failure reason: Checkpoint was declined.
        at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:264)
        at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:169)
        at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:371)
        at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointStreamOperator(SubtaskCheckpointCoordinatorImpl.java:706)
        at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.buildOperatorSnapshotFutures(SubtaskCheckpointCoordinatorImpl.java:627)
        at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:590)
        at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:312)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$8(StreamTask.java:1086)
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:1070)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpoint(StreamTask.java:988)
        ... 13 more
Caused by: java.sql.SQLRecoverableException: IO Error: Got minus one from a read call
        at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:874)
        at oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:807)
        at oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:77)
        at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:769)
        at oracle.jdbc.pool.OracleDataSource.getPhysicalConnection(OracleDataSource.java:450)
        at oracle.jdbc.xa.client.OracleXADataSource.getPooledConnection(OracleXADataSource.java:550)
        at oracle.jdbc.xa.client.OracleXADataSource.getPooledConnection(OracleXADataSource.java:270)
        at oracle.jdbc.xa.client.OracleXADataSource.getXAConnection(OracleXADataSource.java:159)
        at oracle.jdbc.xa.client.OracleXADataSource.getXAConnection(OracleXADataSource.java:116)
        at org.apache.flink.connector.jdbc.xa.XaFacadeImpl.open(XaFacadeImpl.java:98)
        at org.apache.flink.connector.jdbc.xa.XaFacadePoolingImpl.start(XaFacadePoolingImpl.java:80)
        at org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction.beginTx(JdbcXaSinkFunction.java:338)
        at org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction.snapshotState(JdbcXaSinkFunction.java:275)
        at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.trySnapshotFunctionState(StreamingFunctionUtils.java:118)
        at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.snapshotFunctionState(StreamingFunctionUtils.java:99)
        at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.snapshotState(AbstractUdfStreamOperator.java:89)
        at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:218)
        ... 23 more
Caused by: oracle.net.ns.NetException: Got minus one from a read call
        at oracle.net.ns.NSProtocolNIO.doSocketRead(NSProtocolNIO.java:562)
        at oracle.net.ns.NIOPacket.readNIOPacket(NIOPacket.java:408)
        at oracle.net.ns.NSProtocolNIO.negotiateConnection(NSProtocolNIO.java:132)
        at oracle.net.ns.NSProtocol.connect(NSProtocol.java:364)
        at oracle.jdbc.driver.T4CConnection.connect(T4CConnection.java:1625)
        at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:606)
        ... 39 more
{code}
 ;;;","20/Jul/21 14:17;maver1ck;Sometimes I'm getting different error. Same reason: exceeded number of connections.
{code:java}
16:09:39.988 [Source: Custom Source -> Sink: Output stream (1/1)#0] WARN  org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction - nothing to commit up to checkpoint: 80
16:09:40.025 [Source: Custom Source -> Sink: Output stream (1/1)#0] DEBUG org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction - snapshot state, checkpointId=81
16:09:40.025 [Source: Custom Source -> Sink: Output stream (1/1)#0] DEBUG org.apache.flink.connector.jdbc.xa.XaFacadeImpl - end, xid=Optional[201:4b568b2fceb593e236dabae63f027e06000000005100000000000000:976b2337]
16:09:40.025 [Source: Custom Source -> Sink: Output stream (1/1)#0] DEBUG org.apache.flink.connector.jdbc.xa.XaFacadeImpl - prepare, xid=Optional[201:4b568b2fceb593e236dabae63f027e06000000005100000000000000:976b2337]
16:09:40.025 [Source: Custom Source -> Sink: Output stream (1/1)#0] INFO  org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction - empty XA transaction (skip), xid: 201:4b568b2fceb593e236dabae63f027e06000000005100000000000000:976b2337, checkpoint 81
java.sql.SQLException: Listener refused the connection with the following error:
ORA-12516, TNS:listener could not find available handler with matching protocol stack        at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:874)
        at oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:807)
        at oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:77)
        at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:769)
        at oracle.jdbc.pool.OracleDataSource.getPhysicalConnection(OracleDataSource.java:450)
        at oracle.jdbc.xa.client.OracleXADataSource.getPooledConnection(OracleXADataSource.java:550)
        at oracle.jdbc.xa.client.OracleXADataSource.getPooledConnection(OracleXADataSource.java:270)
        at oracle.jdbc.xa.client.OracleXADataSource.getXAConnection(OracleXADataSource.java:159)
        at oracle.jdbc.xa.client.OracleXADataSource.getXAConnection(OracleXADataSource.java:116)
        at org.apache.flink.connector.jdbc.xa.XaFacadeImpl.open(XaFacadeImpl.java:98)
        at org.apache.flink.connector.jdbc.xa.XaFacadePoolingImpl.start(XaFacadePoolingImpl.java:80)
        at org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction.beginTx(JdbcXaSinkFunction.java:339)
        at org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction.snapshotState(JdbcXaSinkFunction.java:275)
        at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.trySnapshotFunctionState(StreamingFunctionUtils.java:118)
        at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.snapshotFunctionState(StreamingFunctionUtils.java:99)
        at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.snapshotState(AbstractUdfStreamOperator.java:89)
        at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:218)
        at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:169)
        at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:371)
        at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointStreamOperator(SubtaskCheckpointCoordinatorImpl.java:706)
        at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.buildOperatorSnapshotFutures(SubtaskCheckpointCoordinatorImpl.java:627)
        at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:590)
        at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:312)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$8(StreamTask.java:1089)
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:1073)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointAsyncInMailbox(StreamTask.java:991)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$triggerCheckpointAsync$7(StreamTask.java:955)
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
        at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:344)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:330)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:202)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:681)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:636)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:647)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:620)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: oracle.net.ns.NetException: Listener refused the connection with the following error:
ORA-12516, TNS:listener could not find available handler with matching protocol stack        at oracle.net.ns.NSProtocolNIO.negotiateConnection(NSProtocolNIO.java:289)
        at oracle.net.ns.NSProtocol.connect(NSProtocol.java:364)
        at oracle.jdbc.driver.T4CConnection.connect(T4CConnection.java:1625)
        at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:606)
        ... 39 more
{code};;;","20/Jul/21 14:19;maver1ck;I think we should allow user to choose if they want to use PoolingImpl.;;;","20/Jul/21 14:20;roman;Flink creates a new connection for a XID which can exceed DoP and probably connection limit (for Oracle it should be disabled I think, but there is no such option currently).

What are the the degree of parallelism (for sink), connection limit, and the actual number of connections if you know it?;;;","20/Jul/21 14:22;roman;> I think we should allow user to choose if they want to use PoolingImpl.

I agree, I hope it can be added to 1.14 and the next 1.13 version (likely not 1.13.2).;;;","20/Jul/21 14:33;maver1ck;On testing env I was able to generate more than 100 connections with parallelism 16. (with connection limit 100)

Now I'm testing version without PoolingImpl;;;","22/Jul/21 07:44;maver1ck;I can confirm that removing PoolingImpl fixed my problem.;;;","22/Jul/21 22:41;roman;Thanks [~maver1ck] for confirming  it.

I'll try to publish a PR this week (unless someone else would like to pick it).;;;","05/Aug/21 14:10;roman;The option added and merged into master as 0b4d17e7894fa0f3624f182e2222edff447cf5a8 and into 1.13 as 898419ebedc0a4c8005347c848e2b004187d0672.

Let's track the remaining issues with pooling in FLINK-22889.
I'm going to resolve this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent rowtypes occur in IncrementalAggregateRule,FLINK-23434,13390825,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,20/Jul/21 04:39,23/Sep/21 18:08,13/Jul/23 08:12,11/Aug/21 04:00,1.13.0,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"add the following test in IncrementalAggregateTest, and will get the following exception

{code:java}
  @Test
  def testSumCountWithSingleDistinctAndRetraction(): Unit = {
    val sqlQuery =
      s""""""
         |SELECT
         |  b, SUM(b1), COUNT(DISTINCT b1), COUNT(1)
         |FROM(
         |   SELECT
         |     a, COUNT(b) as b, MAX(b) as b1
         |   FROM MyTable
         |   GROUP BY a
         |) GROUP BY b
       """""".stripMargin
    util.verifyRelPlan(sqlQuery, ExplainDetail.CHANGELOG_MODE)
  }
{code}

{code:java}
java.lang.IllegalStateException
	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:177)
	at org.apache.flink.table.planner.plan.rules.physical.stream.IncrementalAggregateRule.onMatch(IncrementalAggregateRule.scala:127)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:271)
	at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:74)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
{code}


The reason is the global agg on incremental agg does not handle retraction message in IncrementalAggregateRule when the query has one distinct agg function and count star agg function
.
",,dwysakowicz,godfreyhe,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23479,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 11 04:00:53 UTC 2021,,,,,,,,,,"0|z0t5e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/21 08:27;godfreyhe;Fixed in 1.13.3: c8ad0ec1e5ad21da6b7663944d99f7e413aae0c9
Fixed in 1.14.0: a26887e1e257c6e9e14ad09d9aebe6416e69a4e1;;;","23/Jul/21 15:52;dwysakowicz;Reverted because of test instabilities:
* master
** b62382e634015733754b05c2ad351d9748ced2fb
* 1.13
** ea2bddc8e82ab4b9797f40534b1e5637eef4c5b5;;;","11/Aug/21 04:00;godfreyhe;Fixed in 1.13.3: d1ea26d6467d6a9c7294284ff7cbf8efbb233ddf
Fixed in 1.14.0: 6965c29bd723949395274f2c22a145636b69c424;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[DOCS] The 'docs/content.zh/docs/ops/state/checkpoints' page did not add a newline, which invalidated the Markdown header",FLINK-23432,13390800,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hapihu,hapihu,hapihu,20/Jul/21 02:01,28/Aug/21 12:21,13/Jul/23 08:12,26/Jul/21 01:10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Documentation,,,,,0,pull-request-available,,,,"The page url ：[https://ci.apache.org/projects/flink/flink-docs-master/zh/docs/ops/state/checkpoints/]

markdown： docs/content.zh/docs/ops/state/checkpoints.md
 * A blank line should be added between lines 69 and 70.

{code:java}
L67 <div class=""alert alert-warning"">
L68   <strong>注意:</strong> Checkpoint 目录不是公共 API 的一部分，因此可能在未来的 Release 中进行改变。
L69 </div>
L70 #### 通过配置文件全局配置
{code}
 
 * The modification is shown below

{code:java}
<div class=""alert alert-warning"">
  <strong>注意:</strong> Checkpoint 目录不是公共 API 的一部分，因此可能在未来的 Release 中进行改变。
</div>

#### 通过配置文件全局配置
{code}
 

 

 ",,hapihu,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/21 02:01;hapihu;image-20210720015506660.png;https://issues.apache.org/jira/secure/attachment/13030857/image-20210720015506660.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 26 01:10:37 UTC 2021,,,,,,,,,,"0|z0t58g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/21 15:28;hapihu;Hi [~jark]
Excuse me for taking up your time.
I created pull request for this issue.
Would you like to review it for me.
If there is any problem, please inform me in time.
I active support the modification.
Thank you very much!;;;","26/Jul/21 01:10;jark;Fixed in master: e26c28a4123f91f0b2b20e20a08bf33511a00352;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
State Processor API failed with FileNotFoundException when working with state files on Cloud Storage,FLINK-23429,13390732,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qinjunjerry,qinjunjerry,qinjunjerry,19/Jul/21 15:04,23/Sep/21 18:08,13/Jul/23 08:12,21/Jul/21 14:11,1.12.4,1.13.1,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,API / State Processor,,,,,0,pull-request-available,,,,"For example, 
{code:java}
Caused by: java.io.FileNotFoundException: /savepoints/savepoint-18cf55-d90c1b6b1d12/c965e4fd-9647-4f25-b4cd-5ce0485759fd (No such file or directory)
	at java.io.FileInputStream.open0(Native Method) ~[?:?]
	at java.io.FileInputStream.open(FileInputStream.java:219) ~[?:?]
	at java.io.FileInputStream.<init>(FileInputStream.java:157) ~[?:?]
	at org.apache.flink.core.fs.local.LocalDataInputStream.<init>(LocalDataInputStream.java:50) ~[flink-dist_2.12-1.12.2.jar:1.12.2]
	at org.apache.flink.core.fs.local.LocalFileSystem.open(LocalFileSystem.java:134) ~[flink-dist_2.12-1.12.2.jar:1.12.2]
	at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.open(SafetyNetWrapperFileSystem.java:87) ~[flink-dist_2.12-1.12.2.jar:1.12.2]
	at org.apache.flink.state.api.output.FileCopyFunction.writeRecord(FileCopyFunction.java:61) ~[?:?]
	at org.apache.flink.state.api.output.FileCopyFunction.writeRecord(FileCopyFunction.java:34) ~[?:?]
	at org.apache.flink.runtime.operators.DataSinkTask.invoke(DataSinkTask.java:235) ~[flink-dist_2.12-1.12.2.jar:1.12.2]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755) ~[flink-dist_2.12-1.12.2.jar:1.12.2]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570) ~[flink-dist_2.12-1.12.2.jar:1.12.2]
	at java.lang.Thread.run(Thread.java:834) ~[?:?]
{code}

However, the actual files to be copied do exist in the source savepoint.
",,qinjunjerry,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 21 14:11:01 UTC 2021,,,,,,,,,,"0|z0t4tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/21 14:11;sjwiesman;fixed in master: f9286a54ded7c34d7bf49146a93ee86cc701ac9b

release-1.13: dd2945d73849a3bedf4f390a5ea8541f9815ce6e

release-1.12: 909a6a116db1fd65e8a8332f5d0e095e42df342f;;;","21/Jul/21 14:11;sjwiesman;fixed in master: f9286a54ded7c34d7bf49146a93ee86cc701ac9b

release-1.13: dd2945d73849a3bedf4f390a5ea8541f9815ce6e

release-1.12: 909a6a116db1fd65e8a8332f5d0e095e42df342f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sql stream mode lag function java.lang.NullPointerException,FLINK-23420,13390589,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,TsReaper,xiechenling,xiechenling,19/Jul/21 02:28,20/Aug/21 03:29,13/Jul/23 08:12,20/Aug/21 03:29,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"flink 1.13.1  BlinkPlanner  StreamingMode  EXACTLY_ONCE

log
{code:java}
2021-07-15 21:07:46,328 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1 (type=CHECKPOINT) @ 1626354466304 for job fd3c2294afe74778cb6ce3bd5d42f0c0.
2021-07-15 21:07:46,774 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - OverAggregate(partitionBy=[targetId], orderBy=[lastDt ASC], window=[ RANG BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], select=[displayId, mmsi, latitude, longitude, course, heading, speed, len, minLen, maxLen, wid, id, province, nationality, lastTm, status, vesselName, sClass, targetId, lastDt, $20, LAG(displayId) AS w0$o0, LAG(mmsi) AS w0$o1, LAG($20) AS w0$o2, LAG(latitude) AS w0$o3, LAG(longitude) AS w0$o4, LAG(course) AS w0$o5, LAG(heading) AS w0$o6, LAG(speed) AS w0$o7, LAG(len) AS w0$o8, LAG(minLen) AS w0$o9, LAG(maxLen) AS w0$o10, LAG(wid) AS w0$o11, LAG(id) AS w0$o12, LAG(province) AS w0$o13, LAG(nationality) AS w0$o14, LAG(lastTm) AS w0$o15, LAG(status) AS w0$o16, LAG(vesselName) AS w0$o17, LAG(sClass) AS w0$o18, LAG(targetId) AS w0$o19, LAG(lastDt) AS w0$o20]) -> Calc(select=[displayId, mmsi, $20 AS state, latitude, longitude, course, heading, speed, len, minLen, maxLen, wid, id, province, nationality, lastTm, status, vesselName, sClass, targetId, lastDt, w0$o0 AS previous_displayId, w0$o1 AS previous_mmsi, w0$o2 AS previous_state, w0$o3 AS previous_latitude, w0$o4 AS previous_longitude, w0$o5 AS previous_course, w0$o6 AS previous_heading, w0$o7 AS previous_speed, w0$o8 AS previous_len, w0$o9 AS previous_minLen, w0$o10 AS previous_maxLen, w0$o11 AS previous_wid, w0$o12 AS previous_id, w0$o13 AS previous_province, w0$o14 AS previous_nationality, w0$o15 AS previous_lastTm, w0$o16 AS previous_status, w0$o17 AS previous_vesselName, w0$o18 AS previous_sClass, w0$o19 AS previous_targetId, CAST(w0$o20) AS previous_lastDt], where=[(w0$o1 <> mmsi)]) -> TableToDataSteam(type=ROW<`displayId` INT, `mmsi` INT, `state` TINYINT, `latitude` DOUBLE, `longitude` DOUBLE, `course` FLOAT, `heading` FLOAT, `speed` FLOAT, `len` INT, `minLen` INT, `maxLen` INT, `wid` INT, `id` STRING, `province` STRING, `nationality` STRING, `lastTm` BIGINT, `status` STRING, `vesselName` STRING, `sClass` STRING, `targetId` STRING, `lastDt` TIMESTAMP(3), `previous_displayId` INT, `previous_mmsi` INT, `previous_state` TINYINT, `previous_latitude` DOUBLE, `previous_longitude` DOUBLE, `previous_course` FLOAT, `previous_heading` FLOAT, `previous_speed` FLOAT, `previous_len` INT, `previous_minLen` INT, `previous_maxLen` INT, `previous_wid` INT, `previous_id` STRING, `previous_province` STRING, `previous_nationality` STRING, `previous_lastTm` BIGINT, `previous_status` STRING, `previous_vesselName` STRING, `previous_sClass` STRING, `previous_targetId` STRING, `previous_lastDt` TIMESTAMP(3)> NOT NULL, rowtime=false) (3/3) (34f17a50932ba7852cff00dabecae88e) switched from RUNNING to FAILED on container_1625646226467_0291_01_000005 @ hadoop-15 (dataPort=38082).
java.lang.NullPointerException: null
	at org.apache.flink.api.common.typeutils.base.IntSerializer.serialize(IntSerializer.java:67) ~[hlx_bigdata_flink.jar:?]
	at org.apache.flink.api.common.typeutils.base.IntSerializer.serialize(IntSerializer.java:30) ~[hlx_bigdata_flink.jar:?]
	at org.apache.flink.table.runtime.typeutils.LinkedListSerializer.serialize(LinkedListSerializer.java:114) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.runtime.typeutils.LinkedListSerializer.serialize(LinkedListSerializer.java:39) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.util.InstantiationUtil.serializeToByteArray(InstantiationUtil.java:558) ~[hlx_bigdata_flink.jar:?]
	at org.apache.flink.table.data.binary.BinaryRawValueData.materialize(BinaryRawValueData.java:113) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.data.binary.LazyBinaryFormat.ensureMaterialized(LazyBinaryFormat.java:126) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.runtime.typeutils.RawValueDataSerializer.copy(RawValueDataSerializer.java:60) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.runtime.typeutils.RawValueDataSerializer.copy(RawValueDataSerializer.java:36) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copyRowData(RowDataSerializer.java:170) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:131) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:48) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copyRowData(RowDataSerializer.java:170) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:131) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:48) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.runtime.state.heap.CopyOnWriteStateMap.get(CopyOnWriteStateMap.java:287) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.runtime.state.heap.StateTable.get(StateTable.java:267) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.runtime.state.heap.StateTable.get(StateTable.java:141) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.runtime.state.heap.HeapValueState.value(HeapValueState.java:72) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.table.runtime.operators.over.AbstractRowTimeUnboundedPrecedingOver.onTimer(AbstractRowTimeUnboundedPrecedingOver.java:213) ~[flink-table-blink_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.invokeUserFunction(KeyedProcessOperator.java:91) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.onEventTime(KeyedProcessOperator.java:70) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.advanceWatermark(InternalTimerServiceImpl.java:302) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.streaming.api.operators.InternalTimeServiceManagerImpl.advanceWatermark(InternalTimeServiceManagerImpl.java:180) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark(AbstractStreamOperator.java:626) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitWatermark(OneInputStreamTask.java:211) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.streaming.runtime.streamstatus.StatusWatermarkValve.findAndOutputNewMinWatermarkAcrossAlignedChannels(StatusWatermarkValve.java:196) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.streaming.runtime.streamstatus.StatusWatermarkValve.inputWatermark(StatusWatermarkValve.java:105) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:136) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:423) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:204) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:681) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:636) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:647) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:620) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_251]
2021-07-15 21:07:46,804 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - Calculating tasks to restart to recover the failed task 244ba2b64f33b53ee2f9b4b8d233e03c_2.
{code}
table sql
{code:java}
|CREATE TABLE unionTargetJs (
|  `displayId` INT,
|  `mmsi` INT,
|  `state` TINYINT,
|  `latitude` DOUBLE,
|  `longitude` DOUBLE,
|  `course` FLOAT,
|  `heading` FLOAT,
|  `speed` FLOAT,
|  `len` INT,
|  `minLen` INT,
|  `maxLen` INT,
|  `wid` INT,
|  `id` STRING,
|  `province` STRING,
|  `nationality` STRING,
|  `lastTm` BIGINT,
|  `status` STRING,
|  `vesselName` STRING,
|  `sClass` STRING,
|  `targetId` AS TargetIdFunction(`displayId`, `province`, `id`),
|  `lastDt` AS TO_TIMESTAMP(FROM_UNIXTIME(lastTm / 1000, 'yyyy-MM-dd HH:mm:ss')),
|  WATERMARK FOR `lastDt` AS `lastDt` - INTERVAL '1' SECOND
|) WITH (
|  'connector' = 'kafka',
|  'topic' = 'unionTargetJs',
|  'properties.bootstrap.servers' = '$kafkaAddress',
|  'properties.group.id' = 'aisChangeOrClose',
|  'scan.startup.mode' = 'group-offsets',
|  'format' = 'json',
|  'properties.auto.offset.reset' = 'latest'
|)
{code}
select sql
{code:java}
|SELECT
|  `displayId`,
|  `mmsi`,
|  `state`,
|  `latitude`,
|  `longitude`,
|  `course`,
|  `heading`,
|  `speed`,
|  `len`,
|  `minLen`,
|  `maxLen`,
|  `wid`,
|  `id`,
|  `province`,
|  `nationality`,
|  `lastTm`,
|  `status`,
|  `vesselName`,
|  `sClass`,
|  `targetId`,
|  `lastDt`,
| lag( displayId ) OVER w AS `previous_displayId`,
|  lag( mmsi ) OVER w AS `previous_mmsi`,
|  lag( state ) OVER w AS `previous_state`,
|  lag( latitude ) OVER w AS `previous_latitude`,
|  lag( longitude ) OVER w AS `previous_longitude`,
|  lag( course ) OVER w AS `previous_course`,
|  lag( heading ) OVER w AS `previous_heading`,
|  lag( speed ) OVER w AS `previous_speed`,
|  lag( len ) OVER w AS `previous_len`,
|  lag( minLen ) OVER w AS `previous_minLen`,
|  lag( maxLen ) OVER w AS `previous_maxLen`,
|  lag( wid ) OVER w AS `previous_wid`,
|  lag( id ) OVER w AS `previous_id`,
|  lag( province ) OVER w AS `previous_province`,
|  lag( nationality ) OVER w AS `previous_nationality`,
|  lag( lastTm ) OVER w AS `previous_lastTm`,
|  lag( status ) OVER w AS `previous_status`,
|  lag( vesselName ) OVER w AS `previous_vesselName`,
|  lag( sClass ) OVER w AS `previous_sClass`,
|  lag( targetId ) OVER w AS `previous_targetId`,
| CAST(lag( lastDt ) OVER w AS TIMESTAMP(3)) AS `previous_lastDt`
|FROM
|  unionTargetJs
|WHERE
|  province <> 'CeShi'
|  AND state = 1
| AND status <> 'DELETED'
|WINDOW w AS (
|   PARTITION BY targetId
|   ORDER BY lastDt)
{code}
 udf
{code:java}
class TargetIdFunction extends ScalarFunction {
  def eval(displayId: Integer, province: String, id: String): String = {
    displayId + ProvinceUtil.getProvinceEnumValue(province) + id
  }
}



public class ProvinceUtil {
    private static final Map<String, String> PROVINCE_MAP = new HashMap<>();

    static {
        PROVINCE_MAP.put(""CeShi"", ProvinceEnum.CS.getValue());

        PROVINCE_MAP.put(""HaiNan"", ProvinceEnum.HI.getValue());
        PROVINCE_MAP.put(""ShanDong"", ProvinceEnum.SD.getValue());
        PROVINCE_MAP.put(""FuJian"", ProvinceEnum.FJ.getValue());
        PROVINCE_MAP.put(""HeBei"", ProvinceEnum.HE.getValue());
        PROVINCE_MAP.put(""GuangDong"", ProvinceEnum.GD.getValue());
        PROVINCE_MAP.put(""GuangXi"", ProvinceEnum.GX.getValue());
        PROVINCE_MAP.put(""ZheJiang"", ProvinceEnum.ZJ.getValue());
    }

    public static String getProvinceEnumValue(String key) {
        return PROVINCE_MAP.get(key);
    }
}{code}
 
  ",,jark,lzljs3620320,TsReaper,xiechenling,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23529,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 17 08:25:02 UTC 2021,,,,,,,,,,"0|z0t3xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/21 03:12;jark;It seems you hard-code to use the internal {{LinkedListSerializer}} in your UDF {{TargetIdFunction}}. The {{LinkedListSerializer}} looks like have a bug when serializing nullable elements.  cc [~lzljs3620320];;;","19/Jul/21 03:55;lzljs3620320;LinkedListSerializer is OK, the problem is in LagAggFunction.
{code:java}
new LinkedListSerializer<>(serializer)
{code}
We should wrap serializer by NullableSerializer to deal with nullable values.;;;","19/Jul/21 04:22;jark;[~lzljs3620320], usually, it's the responsibility of collection serializer to handle nullable elements, see {{MapSerializer}} and {{RowSerializer}}.
However, we should be careful with the state-compatibility when fixing this problem. ;;;","19/Jul/21 05:17;lzljs3620320;[~jark] Thanks for the reminding about state-compatibility. Yes, maybe this is why ListSerializer and LinkedListSerializer not handle nullable elements.;;;","28/Jul/21 11:21;TsReaper;Hi all!

Actually \{{LinkedListSerializer}} is a serializer in the table module, while other serializers in table module (such as \{{MapDataSerializer}} and \{{ArrayDataSerializer}}) are all aware of null values. So I think we just need to let \{{LinkedListSerializer}} support null values.

I'd like to take this issue. Please assign it to me.;;;","28/Jul/21 11:29;lzljs3620320;We can use TypeSerializerSnapshot to deal with state-compatibility.;;;","17/Aug/21 08:25;lzljs3620320;master: d5f7b959599d1e8a4df594bcd014010bda27339f
release-1.13: dd80a1a5b2cddb3ceab6a01a2ef1a4924e6d4178;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointBarrierTrackerTest.testNextFirstCheckpointBarrierOvertakesCancellationBarrier fails on Azure,FLINK-23419,13390369,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,dwysakowicz,dwysakowicz,18/Jul/21 08:33,23/Sep/21 18:07,13/Jul/23 08:12,21/Jul/21 09:48,1.12.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20620&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=0dbaca5d-7c38-52e6-f4fe-2fb69ccb3ada&l=9236

{code}
[INFO] Results:
[INFO] 
[ERROR] Failures: 
[ERROR]   CheckpointBarrierTrackerTest.testNextFirstCheckpointBarrierOvertakesCancellationBarrier:366 
Expected: a value less than <30L>
     but: <30L> was equal to <30L>
{code}
",,akalashnikov,dwysakowicz,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 21 09:51:43 UTC 2021,,,,,,,,,,"0|z0t2kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/21 08:34;dwysakowicz;cc [~akalashnikov];;;","19/Jul/21 06:36;akalashnikov;[~dwysakowicz], let's maybe just ignore this test in this branch. It happened because this branch doesn't have 'Clock' yet so it is too difficult to write tests which should rely on the time.;;;","21/Jul/21 09:51;ym;[{{600ce81}}|https://github.com/apache/flink/commit/600ce81fb006b4437e560e9f7dc81fc7bb2f790a] into apache:release-1.12;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'Run kubernetes application HA test' fail on Azure,FLINK-23418,13390368,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangyang0918,dwysakowicz,dwysakowicz,18/Jul/21 08:25,28/Aug/21 12:24,13/Jul/23 08:12,28/Jul/21 08:58,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Deployment / Kubernetes,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20589&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=16ca2cca-2f63-5cce-12d2-d519b930a729&l=3747

{code}
Jul 16 23:58:49 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@172.17.0.3:6123/user/rpc/jobmanager_2#2101744934]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.RemoteFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
Jul 16 23:58:49 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Jul 16 23:58:49 	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292]

{code}",,dwysakowicz,liyu,trohrmann,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 30 06:15:01 UTC 2021,,,,,,,,,,"0|z0t2ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/21 09:46;trohrmann;[~fly_in_gis] could you take a look at this instability?;;;","21/Jul/21 04:01;wangyang0918;I will have a look on this failed instance.;;;","23/Jul/21 03:24;wangyang0918;The test failed because of timeout(30s) waiting for log ""Restoring job 00000000000000000000000000000000 from Checkpoint"". I did not find any exceptions, potential bugs for Kubernetes HA and recovery process. It just seems that the JobManager started and recover slowly. I will increase the timeout to 120s to make the {{test_kubernetes_application_ha.sh}} more stable.;;;","28/Jul/21 08:58;wangyang0918;Merged via

master: b296738c7add3309a0c04f5956368a3e52d470df

1.13: 4efc2c193bc002776346cc2b2d3a71377291c79c;;;","30/Jul/21 06:15;liyu;Move the fix version from 1.13.2 to 1.13.3 since the current 1.13.2 RC3 doesn't include this JIRA and might pass the voting and got released. Please feel free to move it back if the current RC got canceled.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MiniClusterITCase.testHandleBatchJobsWhenNotEnoughSlot fails on Azure,FLINK-23417,13390367,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,dwysakowicz,dwysakowicz,18/Jul/21 08:20,28/Aug/21 12:22,13/Jul/23 08:12,26/Jul/21 14:57,1.12.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20588&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392&l=8065

{code}
[ERROR] Tests run: 17, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 14.001 s <<< FAILURE! - in org.apache.flink.runtime.minicluster.MiniClusterITCase
[ERROR] testHandleBatchJobsWhenNotEnoughSlot(org.apache.flink.runtime.minicluster.MiniClusterITCase)  Time elapsed: 0.524 s  <<< FAILURE!
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.flink.runtime.minicluster.MiniClusterITCase.testHandleBatchJobsWhenNotEnoughSlot(MiniClusterITCase.java:140)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

{code}",,dwysakowicz,Nicolaus Weidner,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 26 14:57:59 UTC 2021,,,,,,,,,,"0|z0t2kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/21 13:05;trohrmann;I think the problem is a race condition between the startupPeriod of the {{ResourceManager}} and the slot request. The test assumes that the slot request times out but if the startup period has passed earlier (in fact it is the same timeout value), then the request is failed with a different exception.;;;","21/Jul/21 14:42;Nicolaus Weidner;[~trohrmann] I am not sure about your analysis - I tried to get the different exception by setting the slot request timeout way higher than the startupPeriod, but it didn't have any effect. See the comments on the PR.

 

edit: This was lack of understanding on my part, looking good!;;;","26/Jul/21 14:57;chesnay;release-1.12: f07bfa6427a3fc47abc3ab254381e78c6bfbd139;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClusterUncaughtExceptionHandlerITCase.testExitDueToUncaughtException fails on azure,FLINK-23406,13389958,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,xtsong,xtsong,16/Jul/21 05:58,23/Sep/21 18:04,13/Jul/23 08:12,19/Jul/21 10:21,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20523&view=logs&j=d8d26c26-7ec2-5ed2-772e-7a1a1eb8317c&t=be5fb08e-1ad7-563c-4f1a-a97ad4ce4865&l=7837

{code}
Jul 15 21:31:22 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 4.401 s <<< FAILURE! - in org.apache.flink.runtime.entrypoint.ClusterUncaughtExceptionHandlerITCase
Jul 15 21:31:22 [ERROR] testExitDueToUncaughtException(org.apache.flink.runtime.entrypoint.ClusterUncaughtExceptionHandlerITCase)  Time elapsed: 4.338 s  <<< FAILURE!
Jul 15 21:31:22 java.lang.AssertionError: 
Jul 15 21:31:22 
Jul 15 21:31:22 Expected: is <239>
Jul 15 21:31:22      but: was <1>
Jul 15 21:31:22 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Jul 15 21:31:22 	at org.junit.Assert.assertThat(Assert.java:964)
Jul 15 21:31:22 	at org.junit.Assert.assertThat(Assert.java:930)
Jul 15 21:31:22 	at org.apache.flink.runtime.entrypoint.ClusterUncaughtExceptionHandlerITCase.testExitDueToUncaughtException(ClusterUncaughtExceptionHandlerITCase.java:64)
Jul 15 21:31:22 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 15 21:31:22 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 15 21:31:22 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 15 21:31:22 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 15 21:31:22 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jul 15 21:31:22 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jul 15 21:31:22 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jul 15 21:31:22 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jul 15 21:31:22 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Jul 15 21:31:22 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jul 15 21:31:22 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Jul 15 21:31:22 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 15 21:31:22 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jul 15 21:31:22 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jul 15 21:31:22 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Jul 15 21:31:22 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Jul 15 21:31:22 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Jul 15 21:31:22 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Jul 15 21:31:22 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Jul 15 21:31:22 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Jul 15 21:31:22 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Jul 15 21:31:22 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 15 21:31:22 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Jul 15 21:31:22 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Jul 15 21:31:22 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Jul 15 21:31:22 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Jul 15 21:31:22 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Jul 15 21:31:22 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jul 15 21:31:22 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jul 15 21:31:22 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jul 15 21:31:22 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,dwysakowicz,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 19 10:21:57 UTC 2021,,,,,,,,,,"0|z0t01k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jul/21 10:13;trohrmann;I suspect that the {{t.join(1000L)}} is too short to succeed always on the CI infrastructure. I propose to replace it with {{t.join()}}. Moreover, I suggest to also include the logs of the test process in case of a test failure.;;;","16/Jul/21 10:35;trohrmann;The problem is actually that {{t.join()}} will keep the {{ClusterEntrypoint.lock}} and therefore, we cannot shut down the {{ClusterEntrypoint}}. A better solution is to return a {{DispatcherResourceManagerComponentFactory}} from the {{ClusterEntrypoint.createDispatcherResourceManagerComponentFactory}} method.;;;","18/Jul/21 08:16;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20557&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=8057;;;","19/Jul/21 10:21;trohrmann;Fixed via c034b7ecfab008123e9730d547b460f42ec625a8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
State backend benchmarks are failing,FLINK-23398,13389804,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunta,pnowojski,pnowojski,15/Jul/21 09:33,16/Jul/21 02:58,13/Jul/23 08:12,16/Jul/21 02:58,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Benchmarks,Runtime / State Backends,,,,0,pull-request-available,,,,"[http://codespeed.dak8s.net:8080/job/flink-statebackend-benchmark/]

failed due to

 
{code:java}
[INFO] --- exec-maven-plugin:1.6.0:exec (default-cli) @ benchmark ---
Error: A JNI error has occurred, please check your installation and try again
Exception in thread ""main"" java.lang.SecurityException: Invalid signature file digest for Manifest main attributes
	at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:330)
	at sun.security.util.SignatureFileVerifier.process(SignatureFileVerifier.java:263)
	at java.util.jar.JarVerifier.processEntry(JarVerifier.java:318)
	at java.util.jar.JarVerifier.update(JarVerifier.java:230)
	at java.util.jar.JarFile.initializeVerifier(JarFile.java:383)
	at java.util.jar.JarFile.ensureInitialization(JarFile.java:617)
	at java.util.jar.JavaUtilJarAccessImpl.ensureInitialization(JavaUtilJarAccessImpl.java:69)
	at sun.misc.URLClassPath$JarLoader$2.getManifest(URLClassPath.java:991)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:451)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:495)
[ERROR] Command execution failed.
org.apache.commons.exec.ExecuteException: Process exited with an error: 1 (Exit value: 1)
	at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:404)
	at org.apache.commons.exec.DefaultExecutor.execute(DefaultExecutor.java:166)
	at org.codehaus.mojo.exec.ExecMojo.executeCommandLine(ExecMojo.java:804)
	at org.codehaus.mojo.exec.ExecMojo.executeCommandLine(ExecMojo.java:751)
	at org.codehaus.mojo.exec.ExecMojo.execute(ExecMojo.java:313)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:154)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:146)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:81)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:309)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:194)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:107)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:993)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:345)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:191)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356){code}
 ",,pnowojski,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2021-07-15 09:33:23.0,,,,,,,,,,"0|z0sz3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Benchmarks do not support logging,FLINK-23393,13389770,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,15/Jul/21 07:18,04/Aug/21 10:49,13/Jul/23 08:12,26/Jul/21 12:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Benchmarks,,,,,0,pull-request-available,,,,"The logging setup in the benchmarks isn't working, which makes debugging difficult.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16461,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 26 12:51:55 UTC 2021,,,,,,,,,,"0|z0syvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/21 12:51;chesnay;benchmark-master: 8e9bec8970eb113af1aeecb03dbb641417ff926a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix nullability of COALESCE,FLINK-23385,13389640,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,slinkydeveloper,maver1ck,maver1ck,14/Jul/21 14:21,23/Sep/21 08:50,13/Jul/23 08:12,23/Sep/21 08:50,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"EDIT: Simpler case:
{code:java}
SELECT COALESCE(REGEXP_EXTRACT('22','[A-Z]+'),'-');
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.TableException: Column 'EXPR$0' is NOT NULL, however, a null value is being written into it. You can set job configuration 'table.exec.sink.not-null-enforcer'='drop' to suppress this exception and drop such records silently.
{code}
When using REGEXP_EXTRACT on NOT NULL column I'm getting following exception
{code:java}
select COALESCE(REGEXP_EXTRACT(test, '[A-Z]+'), '-') from test limit 10

[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.TableException: Column 'EXPR$0' is NOT NULL, however, a null value is being written into it. You can set job configuration 'table.exec.sink.not-null-enforcer'='drop' to suppress this exception and drop such records silently.
{code}
I think the reason is that nullability of result is wrongly calculated.
 Example:
{code:java}
create table test (
     test STRING NOT NULL
) WITH (
    'connector' = 'datagen'
);

explain select COALESCE(REGEXP_EXTRACT(test, '[A-Z]+'), '-') from test

== Abstract Syntax Tree ==
LogicalProject(EXPR$0=[REGEXP_EXTRACT($0, _UTF-16LE'[A-Z]+')])
+- LogicalTableScan(table=[[default_catalog, default_database, test]])== Optimized Physical Plan ==
Calc(select=[REGEXP_EXTRACT(test, _UTF-16LE'[A-Z]+') AS EXPR$0])
+- TableSourceScan(table=[[default_catalog, default_database, test]], fields=[test])== Optimized Execution Plan ==
Calc(select=[REGEXP_EXTRACT(test, _UTF-16LE'[A-Z]+') AS EXPR$0])
+- TableSourceScan(table=[[default_catalog, default_database, test]], fields=[test]){code}
As you can see Flink is removing COALESCE from query which is wrong.

 

Same for view (null = false):
{code:java}
create view v as select COALESCE(REGEXP_EXTRACT(test, '[A-Z]+'), '-') from test

describe v;
+--------+--------+-------+-----+--------+-----------+
|   name |   type |  null | key | extras | watermark |
+--------+--------+-------+-----+--------+-----------+
| EXPR$0 | STRING | false |     |        |           |
+--------+--------+-------+-----+--------+-----------+
{code}",,airblader,dwysakowicz,godfreyhe,jark,libenchao,maver1ck,twalthr,wenlong.lwl,yanghua,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 23 08:50:59 UTC 2021,,,,,,,,,,"0|z0sy2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/21 14:39;maver1ck;[~yanghua] as the author maybe you can help ? ;;;","14/Jul/21 15:07;maver1ck;Simpler case added.;;;","14/Jul/21 15:19;yanghua;[~maver1ck] Sorry, I am no longer interested in the Flink project.;;;","14/Jul/21 16:06;twalthr;{{COALESCE}} might be the issue here. Did you try {{IFNULL}}?;;;","14/Jul/21 16:53;maver1ck;IFNULL is working.;;;","15/Jul/21 06:20;twalthr;I renamed the issue. There are a couple of functions that have an invalid type inference.;;;","16/Jul/21 11:53;wenlong.lwl;hi, [~twalthr] I think the root cause of the issue is the return type of REGEXP_EXTRACT should be force nullable instead of  depending on input type, introduce by FLINK-13783;;;","23/Sep/21 08:50;dwysakowicz;Fixed in f074642a0d3c51dc82b2837510719f8d51c9d872;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkEnvironmentContext#setUp is called twice,FLINK-23383,13389622,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,chesnay,chesnay,14/Jul/21 12:46,14/Jul/21 16:42,13/Jul/23 08:12,14/Jul/21 16:42,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Benchmarks,,,,,0,pull-request-available,,,,"Several benchmarks subclass the FlinkEnvironmentContext and override #setUp, like this:
{code}
@Setup
public void setUp() throws Exception {
    super.setUp();
    // do more stuff
}
{code}
Because this method is also annotated with {{@Setup}}, both the overridden and super version will be called separately, and since the overridden once _also_ calls into super, then the super version is called twice.

This is not a problem right now, but it could result in leaks if we allocate a resource in super.setUp, because @TearDown would only be called once.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 14 16:42:08 UTC 2021,,,,,,,,,,"0|z0sxyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/21 16:42;chesnay;benchmark-master: 5ccb6b2003e37a3574996e281e38dfdb0f4b101a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the wrong mapping of state cache in PyFlink,FLINK-23368,13389348,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,13/Jul/21 08:00,23/Sep/21 18:02,13/Jul/23 08:12,13/Jul/21 11:38,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,API / Python,,,,,0,pull-request-available,,,,"The details and demo are discussed in
https://lists.apache.org/x/thread.html/r69780c67e0f1f8522df15f4420997842e9f4faceedf019d99901b1ef@%3Cuser.flink.apache.org%3E
",,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 13 11:38:38 UTC 2021,,,,,,,,,,"0|z0swaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/21 11:38;hxbks2ks;Merged into master via 08e9343b0be8e1d438fe9883f5cc407c5ae9e88e
Merged into release-1.13 via c1f30eefe82841f2269e6cba300eb342b1193741;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClientTest.testSimpleRequests fails due to timeout on azure,FLINK-23362,13389272,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,xtsong,xtsong,13/Jul/21 02:09,15/Dec/21 01:40,13/Jul/23 08:12,17/Aug/21 18:05,1.12.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.0,,,Runtime / Queryable State,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20347&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=14440

{code}
[ERROR] Tests run: 6, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 22.994 s <<< FAILURE! - in org.apache.flink.queryablestate.network.ClientTest
[ERROR] testSimpleRequests(org.apache.flink.queryablestate.network.ClientTest)  Time elapsed: 20.055 s  <<< FAILURE!
java.lang.AssertionError: Receive timed out
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:712)
	at org.apache.flink.queryablestate.network.ClientTest.testSimpleRequests(ClientTest.java:177)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 17 18:05:23 UTC 2021,,,,,,,,,,"0|z0svu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/21 18:05;chesnay;master: 650d463f5133b91551a5cbe96963e3fc678aebf0
1.13: 36368e61f52e74363fd6760db343f2d436476501 
1.12: 097c77fd3873841823ac3f173c0826881fa8e629;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the number of available slots in testResourceCanBeAllocatedForDifferentJobAfterFree,FLINK-23359,13389074,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,guoyangze,guoyangze,12/Jul/21 09:38,30/Nov/21 20:37,13/Jul/23 08:12,13/Jul/21 02:15,1.13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Tests,,,,,0,pull-request-available,,,,"In AbstractFineGrainedSlotManagerITCase#testResourceCanBeAllocatedForDifferentJobAfterFree, we need only 1 default slot exist in the cluster. However, we currently register a TaskManager with 2 default slots. ",,guoyangze,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 13 02:15:08 UTC 2021,,,,,,,,,,"0|z0sum0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/21 09:44;guoyangze;[~xintongsong] Can you assign this to me?;;;","12/Jul/21 09:51;trohrmann;I've assigned you [~guoyangze].;;;","13/Jul/21 02:15;xtsong;Fixed via
- master (1.14): 2e374954b9bfa69e30624dfb27ff3762749da725
- release-1.13: 4d865341e16f668899e4295e9d85cc5258145e24;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBase delegation token expired after 7 days,FLINK-23356,13389039,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gsomogyi,gsomogyi,gsomogyi,12/Jul/21 08:06,23/Sep/21 18:02,13/Jul/23 08:12,13/Jul/21 07:50,1.13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / HBase,,,,,0,pull-request-available,,,,"FLINK-6376 has solved the problem for HDFS but HBase still has the issue.
The root cause of the issue is that HBase delegation token expires after 7 days and Flink is not re-obtaining any kind of token at the moment.",,gsomogyi,gyfora,mbalassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 13 07:50:13 UTC 2021,,,,,,,,,,"0|z0sue8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/21 08:06;gsomogyi;I'm going to file a PR soon.;;;","12/Jul/21 08:52;mbalassi;Thanks, [~gsomogyi] - please proceed.;;;","13/Jul/21 07:50;gyfora;Fixed for master via 657df14677a8f4e500efdc79627f095e32e8c4b4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FileReadingWatermarkITCase.testWatermarkEmissionWithChaining fails due to ""too few watermarks emitted"" on azure",FLINK-23351,13388932,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fpaul,xtsong,xtsong,12/Jul/21 03:37,01/Sep/21 09:44,13/Jul/23 08:12,01/Sep/21 05:56,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,API / DataStream,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20267&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=81f2da51-a161-54c7-5b84-6001fed26530&l=10500

{code}
Jul 10 22:53:45 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 21.065 s <<< FAILURE! - in org.apache.flink.test.streaming.api.FileReadingWatermarkITCase
Jul 10 22:53:45 [ERROR] testWatermarkEmissionWithChaining(org.apache.flink.test.streaming.api.FileReadingWatermarkITCase)  Time elapsed: 20.25 s  <<< FAILURE!
Jul 10 22:53:45 java.lang.AssertionError: too few watermarks emitted in 3057 ms expected:<305.0> but was:<124.0>
Jul 10 22:53:45 	at org.junit.Assert.fail(Assert.java:89)
Jul 10 22:53:45 	at org.junit.Assert.failNotEquals(Assert.java:835)
Jul 10 22:53:45 	at org.junit.Assert.assertEquals(Assert.java:555)
Jul 10 22:53:45 	at org.apache.flink.test.streaming.api.FileReadingWatermarkITCase.testWatermarkEmissionWithChaining(FileReadingWatermarkITCase.java:79)
Jul 10 22:53:45 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 10 22:53:45 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 10 22:53:45 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 10 22:53:45 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 10 22:53:45 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jul 10 22:53:45 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jul 10 22:53:45 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jul 10 22:53:45 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jul 10 22:53:45 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Jul 10 22:53:45 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 10 22:53:45 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jul 10 22:53:45 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jul 10 22:53:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Jul 10 22:53:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Jul 10 22:53:45 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Jul 10 22:53:45 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Jul 10 22:53:45 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Jul 10 22:53:45 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Jul 10 22:53:45 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Jul 10 22:53:45 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 10 22:53:45 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Jul 10 22:53:45 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Jul 10 22:53:45 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Jul 10 22:53:45 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Jul 10 22:53:45 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Jul 10 22:53:45 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jul 10 22:53:45 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jul 10 22:53:45 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jul 10 22:53:45 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,dwysakowicz,roman,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 01 05:56:35 UTC 2021,,,,,,,,,,"0|z0stqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/21 08:27;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20610&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=4935;;;","18/Jul/21 08:27;dwysakowicz;cc [~roman_khachatryan];;;","27/Jul/21 06:23;roman;https://dev.azure.com/khachatryanroman/flink/_build/results?buildId=1001&view=logs&j=cc649950-03e9-5fae-8326-2f1ad744b536&t=51cab6ca-669f-5dc0-221d-1e4f7dc4fc85&l=10609;;;","30/Jul/21 07:34;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21129&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5257;;;","03/Aug/21 02:03;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21330&view=logs&j=f2b08047-82c3-520f-51ee-a30fd6254285&t=3810d23d-4df2-586c-103c-ec14ede6af00&l=11708;;;","10/Aug/21 01:44;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21790&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5135;;;","12/Aug/21 03:23;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21932&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=5124;;;","16/Aug/21 03:33;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22179&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=5118;;;","18/Aug/21 04:02;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22401&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10740;;;","31/Aug/21 17:53;arvid;Merged into master as 0b52bfdbf94c4c355fc50cb681a717b51e9ef442.;;;","01/Sep/21 05:56;arvid;Merged into 1.14 as e66b92be2a5abaa6dc7b1510b21b00a9447754f1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate to the next version of Python `requests` when released,FLINK-23345,13388874,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dianfu,potiuk,potiuk,11/Jul/21 11:54,15/Dec/21 01:40,13/Jul/23 08:12,10/Sep/21 02:31,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.10.4,1.11.7,1.12.8,1.13.3,1.14.0,API / Python,,,,,0,pull-request-available,,,,"Hello Maintainers, 

I am a PMC member of Apache Airflow, and I wanted to give you a bit of heads-up with rather important migration to the upcoming version of `requests` library in your Python release. 

Since you are using `requests` library in your project (at least indirectly via apache-beam), you are affected.

As discussed at length in https://issues.apache.org/jira/browse/LEGAL-572 we found out that the 'chardet` library used by `requests` library was a mandatory dependency to requests and since it has LGPL licence, we should not release any Apache Software with it. 

Since then (and since in Airflow we rely on requests heavily) we have been working with the requests maintainers and ""charset-normalizer"" maintainer to make it possible to replace `chardet` with MIT-licensed `charset-normalizer` instead so that requests library can be used in Python releases by Apache projects.

This was a bumpy road but finally the PR by [~ash] has been merged: [https://github.com/psf/requests/pull/5797] and we hope soon a new version of requests library will be released. 

This is just a heads-up. I will let you know when it is released, but I have a kind requests as well - I might ask the maintainers to release a release candidate of requests and maybe you could help to test it before it is released, that would be some re-assurance for the maintainers of requests who are very concerned about stability of their releases.

Let me know if you need any more information and whether you would like to help in testing the candidate when it is out.",,dianfu,potiuk,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24260,,,BEAM-12598,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 10 02:31:19 UTC 2021,,,,,,,,,,"0|z0stdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/21 15:38;potiuk;Requests 2.26.0 released without the mandatory `chardet` dependency:

[https://pypi.org/project/requests/2.26.0/]

 
 * Instead of ??chardet??, use the MIT-licensed ??charset_normalizer?? for Python3 to remove license ambiguity for projects bundling requests. If ??chardet?? is already installed on your machine it will be used instead of ??charset_normalizer?? to keep backwards compatibility. (#5797)

You can also install ??chardet?? while installing requests by specifying ??[use_chardet_on_py3]?? extra as follows:
{quote}{{`shell pip install ""requests[use_chardet_on_py3]"" `}}
{quote}
Python2 still depends upon the ??chardet?? module.;;;","04/Aug/21 10:51;chesnay;Marked this as a blocker for 1.14. Technically previous versions are affected as well but I can't gauge how feasible it is to either bump beam or otherwise bump the transitive requests dependency.;;;","04/Aug/21 14:05;potiuk;Related issue in Beam's JIRA: https://issues.apache.org/jira/projects/BEAM/issues/BEAM-12598;;;","09/Sep/21 12:47;dianfu;[~potiuk] Thanks a lot for sharing this information. Actually `requests` was not used in PyFlink (also not used in codepaths dependent in apache-beam) and it was just a transitive dependency from apache-beam. Since PyFlink only supports Python 3 and Requests 2.26.0 (without the mandatory `chardet` dependency) has been released for Python 3, personally I think there is nothing need to do at Flink side. I'm closing this ticket. Thanks again for letting us know about this.;;;","09/Sep/21 12:52;chesnay;Imo, whether it is used or not by Flink isn't important.

We ship Beam with it's transitive dependencies, and that contains an LGPL dependency.
At the very least we should keep this ticket open to bump our Beam dependency once they have gotten around to resolving the issue on their end.;;;","09/Sep/21 12:53;chesnay;If it is truly not used, is there some way to just exclude requests?;;;","09/Sep/21 12:58;dianfu;We can limit the version of requests, e.g. requests >= 2.26.0. Then there is no need to bump Beam dependency.;;;","09/Sep/21 13:00;chesnay;That's also fine.;;;","09/Sep/21 13:56;potiuk;Yeah. Limiting to >= 2.26.0 should be fine!;;;","10/Sep/21 02:31;dianfu;Fixed in:
- master via 0cecdcae4e20edf6a513d5be5ebf3ad7d59b9f4f
- release-1.14 via 3621b00358fecacbc63e4919525343560adf5ebe
- release-1.13 via 53f79eaa7a6d65e92ae4258d334d35d573e43c41
- release-1.12 via 07a3f6633fc88b96856dc94be664e0dca6bb991d
- release-1.11 via aae0f918f3188abaf348513511be49b5b286a01c
- release-1.10 via 3d95391ca025266debd4149c33c005901a90b66e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RMQSourceITCase.testStopWithSavepoint fails on azure due to timeout,FLINK-23322,13388514,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fpaul,xtsong,xtsong,09/Jul/21 02:07,15/Dec/21 01:40,13/Jul/23 08:12,29/Jul/21 15:31,1.12.4,1.13.1,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.0,,,Connectors/ RabbitMQ,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20196&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=13696

{code}
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 41.237 s <<< FAILURE! - in org.apache.flink.streaming.connectors.rabbitmq.RMQSourceITCase
[ERROR] testStopWithSavepoint(org.apache.flink.streaming.connectors.rabbitmq.RMQSourceITCase)  Time elapsed: 7.609 s  <<< ERROR!
java.util.concurrent.TimeoutException
	at com.rabbitmq.utility.BlockingCell.get(BlockingCell.java:77)
	at com.rabbitmq.utility.BlockingCell.uninterruptibleGet(BlockingCell.java:120)
	at com.rabbitmq.utility.BlockingValueOrException.uninterruptibleGetValue(BlockingValueOrException.java:36)
	at com.rabbitmq.client.impl.AMQChannel$BlockingRpcContinuation.getReply(AMQChannel.java:502)
	at com.rabbitmq.client.impl.AMQConnection.start(AMQConnection.java:326)
	at com.rabbitmq.client.impl.recovery.RecoveryAwareAMQConnectionFactory.newConnection(RecoveryAwareAMQConnectionFactory.java:64)
	at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.init(AutorecoveringConnection.java:156)
	at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:1130)
	at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:1087)
	at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:1045)
	at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:1207)
	at org.apache.flink.streaming.connectors.rabbitmq.RMQSourceITCase.getRMQConnection(RMQSourceITCase.java:133)
	at org.apache.flink.streaming.connectors.rabbitmq.RMQSourceITCase.setUp(RMQSourceITCase.java:82)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,cmick,fpaul,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22698,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 29 15:31:11 UTC 2021,,,,,,,,,,"0|z0sr5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jul/21 02:08;xtsong;cc [~arvid] [~cmick];;;","09/Jul/21 12:31;cmick;Most probably the RMQ container did not start on time, causing the connection init in the test to fail.

[~fpaul] since you're the assignee - do you need any help on that matter?;;;","09/Jul/21 12:45;fpaul;I briefly checked the logs found the following line. 
{code:java}
// 22:07:26,427 [AMQP Connection 172.17.0.1:59470] WARN  com.rabbitmq.client.impl.ForgivingExceptionHandler           [] - An unexpected connection driver error occured (Exception message: Socket closed)

{code}
When reading through the RMQ connection code it seems that the default connection timeout should be 60 seconds which in general seems very fair. I enabled additional logging in the PR to see the output of the container. Currently, I do not have an immediate idea what is causing this problem otherwise.

 

WDYT [~cmick]?;;;","09/Jul/21 12:57;cmick;I think it's 10 seconds by default in this case (com.rabbitmq.client.ConnectionFactory.DEFAULT_HANDSHAKE_TIMEOUT). I'm able to get the same error locally when I set the handshakeTimeout ({color:#000000}factory{color}.setHandshakeTimeout) to some really low values (below 50ms in my case). Still 10s should be more than enough. I will try to look at more closely today.;;;","09/Jul/21 21:44;cmick;I've had a closer look and found out in com.rabbitmq.client.impl.AMQConnection.java#L326 (causing trouble here) that the timeout for a single RPC is actually set to half of the handshakeTimeout:

{code:java}
(AMQP.Connection.Start) connStartBlocker.getReply(handshakeTimeout/2).getMethod();
{code}

In result the timeout is actually 5 seconds (not 10 as I thought initially). Some more docs here: https://www.rabbitmq.com/networking.html#handshake-timeout

So probably it will not hurt to increase the default value from 10 seconds to 20 or 30 seconds. Just need to add {{factory.setHandshakeTimeout(20000);}} in RMQSourceITCase.getRMQConnection.

Of course, adding logging is a great idea. Without it, it is hard to say what had happened.

;;;","19/Jul/21 07:55;fpaul;[~cmick] thanks for your suggestion I updated the PR to increase the handshake timeout can you take a look?;;;","29/Jul/21 15:31;fpaul;master: 1562a211697aa200bbaf8f61511d44d6e4ee3c24

release-1.13: 8a7e53af959522082fc666b9e4cb7a3727c00524

release-1.12: 4e14b3f6bea21da0a24b62dbfda9c1b9459a11a9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AkkaRpcActorTest#testOnStopFutureCompletionDirectlyTerminatesAkkaRpcActor fails on azure,FLINK-23318,13388414,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,dwysakowicz,dwysakowicz,08/Jul/21 14:17,13/Jul/21 03:51,13/Jul/23 08:12,12/Jul/21 07:52,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20163&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=6023

{code}
Jul 08 11:03:13 java.lang.AssertionError: 
Jul 08 11:03:13 
Jul 08 11:03:13 Expected: is <false>
Jul 08 11:03:13      but: was <true>
Jul 08 11:03:13 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Jul 08 11:03:13 	at org.junit.Assert.assertThat(Assert.java:964)
Jul 08 11:03:13 	at org.junit.Assert.assertThat(Assert.java:930)
Jul 08 11:03:13 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.testOnStopFutureCompletionDirectlyTerminatesAkkaRpcActor(AkkaRpcActorTest.java:375)
Jul 08 11:03:13 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 08 11:03:13 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 08 11:03:13 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 08 11:03:13 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 08 11:03:13 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jul 08 11:03:13 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jul 08 11:03:13 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jul 08 11:03:13 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jul 08 11:03:13 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jul 08 11:03:13 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 08 11:03:13 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jul 08 11:03:13 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Jul 08 11:03:13 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Jul 08 11:03:13 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Jul 08 11:03:13 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Jul 08 11:03:13 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Jul 08 11:03:13 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Jul 08 11:03:13 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Jul 08 11:03:13 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Jul 08 11:03:13 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jul 08 11:03:13 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jul 08 11:03:13 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jul 08 11:03:13 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Jul 08 11:03:13 
Jul 08 11:03:13 [INFO] Running org.apache.flink.runtime.rpc.akka.TimeoutCallStackTest
Jul 08 11:03:13 [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.107 s - in org.apache.flink.runtime.rpc.akka.TimeoutCallStackTest
Jul 08 11:03:13 [INFO] Running org.apache.flink.runtime.rpc.akka.AkkaRpcActorOversizedResponseMessageTest
Jul 08 11:03:13 [INFO] Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.823 s - in org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest
Jul 08 11:03:14 [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.344 s - in org.apache.flink.runtime.rpc.akka.AkkaRpcActorOversizedResponseMessageTest
Jul 08 11:03:14 [INFO] 
Jul 08 11:03:14 [INFO] Results:
Jul 08 11:03:14 [INFO] 
Jul 08 11:03:14 [ERROR] Failures: 
Jul 08 11:03:14 [ERROR]   AkkaRpcActorTest.testOnStopFutureCompletionDirectlyTerminatesAkkaRpcActor:375 
Jul 08 11:03:14 Expected: is <false>
Jul 08 11:03:14      but: was <true>

{code}",,dwysakowicz,nkruber,trohrmann,TsReaper,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23341,FLINK-23366,,,,,,,,,,,,,,FLINK-23202,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 12 07:52:07 UTC 2021,,,,,,,,,,"0|z0sqjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jul/21 02:59;TsReaper;After binary searching through the commits I found that this issue is caused by c47d8b92499fe25405c337be68907098309fd6c9 by [~trohrmann] .

For developers blocked by this issue, you can rebase your pull requests on commit 09cd685d6afcffb0d5488a1dd6a9b3b742a5661d (which is one commit behind the above commit).;;;","09/Jul/21 03:26;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20195&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392&l=7388;;;","09/Jul/21 03:26;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20195&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107&l=6024;;;","09/Jul/21 03:30;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20195&view=logs&j=d8d26c26-7ec2-5ed2-772e-7a1a1eb8317c&t=be5fb08e-1ad7-563c-4f1a-a97ad4ce4865&l=5993;;;","09/Jul/21 03:30;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20195&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=19336553-69ec-5b03-471a-791a483cced6&l=6159;;;","09/Jul/21 03:30;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20195&view=logs&j=3b6ec2fd-a816-5e75-c775-06fb87cb6670&t=2aff8966-346f-518f-e6ce-de64002a5034&l=6874;;;","09/Jul/21 03:31;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20195&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=0dbaca5d-7c38-52e6-f4fe-2fb69ccb3ada&l=7356;;;","09/Jul/21 13:08;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20225&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=6157;;;","12/Jul/21 01:58;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20254&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0&l=7650;;;","12/Jul/21 01:58;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20254&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=6109;;;","12/Jul/21 01:58;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20254&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392&l=7390;;;","12/Jul/21 01:59;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20254&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107&l=6022;;;","12/Jul/21 01:59;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20254&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=19336553-69ec-5b03-471a-791a483cced6&l=6159;;;","12/Jul/21 01:59;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20254&view=logs&j=f2b08047-82c3-520f-51ee-a30fd6254285&t=601c4a98-b94a-54db-dc38-ae78131029dc&l=6873;;;","12/Jul/21 02:00;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20254&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=7650;;;","12/Jul/21 02:54;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20267&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=6107

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20267&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392&l=7388

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20267&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107&l=6024

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20267&view=logs&j=d8d26c26-7ec2-5ed2-772e-7a1a1eb8317c&t=be5fb08e-1ad7-563c-4f1a-a97ad4ce4865&l=5989

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20267&view=logs&j=f2b08047-82c3-520f-51ee-a30fd6254285&t=601c4a98-b94a-54db-dc38-ae78131029dc&l=6976

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20285&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392&l=7390

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20285&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107&l=6024

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20285&view=logs&j=6bfdaf55-0c08-5e3f-a2d2-2a0285fd41cf&t=fd9796c3-9ce8-5619-781c-42f873e126a6&l=5988

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20285&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=19336553-69ec-5b03-471a-791a483cced6&l=6159

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20285&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=81f2da51-a161-54c7-5b84-6001fed26530&l=6164

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20285&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=7650;;;","12/Jul/21 07:52;trohrmann;Fixed via 72150ab72ac7a56695850c357b5608383add7146;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reintroduce temporal table function documentation,FLINK-23313,13388378,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,leonard,twalthr,twalthr,08/Jul/21 11:52,30/Sep/21 00:44,13/Jul/23 08:12,29/Sep/21 21:31,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,1.15.0,,,,Documentation,Table SQL / API,,,,0,pull-request-available,,,,"FLIP-132 introduced the new {{FOR SYSTEM_TIME AS OF}} and dropped the main documentation for temporal table function. This causes a lot of confusion for users.

First, because processing time joins are not supported yet.

Second, because a primary key might not always be present in the current pipeline when using Table API.

We have not deprecated `createTemporalTableFunction` and the documentation in https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/table/tableapi/#join-with-temporal-table is not enough.",,jark,leonard,martijnvisser,sjwiesman,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 29 21:30:53 UTC 2021,,,,,,,,,,"0|z0sqbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/21 11:52;twalthr;CC [~Leonard Xu];;;","08/Jul/21 12:12;leonard;Thanks [~twalthr] for open this ticket, I keep the temporal table function in FLINK-19082, but the documentation lost  latter. I agree that we should reIntroduce it until we support  processing time temporal join.

Could you assign this ticket to me ? ;;;","08/Sep/21 12:24;martijnvisser;[~Leonard Xu] Should I still assign this ticket to you?;;;","08/Sep/21 12:52;leonard;[~MartijnVisser] Sure, please assign this to me.;;;","29/Sep/21 21:30;sjwiesman;resolved in release-1.14: fc8e269e9415a75fe9d1c5634eef2caaf059e1bc

master: a328e3b2a264a914d9fef4a94f91eecc6623a08c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkRelMdUniqueKeys causes exception when used with new Schema,FLINK-23306,13388323,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,twalthr,twalthr,08/Jul/21 08:17,23/Sep/21 18:00,13/Jul/23 08:12,09/Jul/21 07:33,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,FlinkRelMdUniqueKeys should not use the deprecated `TableSchema`. It causes exceptions when e.g. {{sourceWatermark()}} is used in schema.,,jark,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 09 07:33:41 UTC 2021,,,,,,,,,,"0|z0spzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jul/21 07:33;twalthr;Fixed in 1.14.0: f37cf259d62784f24685b5199be3ff816de45247
Fixed in 1.13.3: 51e4ee24a48b475ce6e19b6c06c43c12d5dce42d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[DOCS]Some links on page docs/content/docs/dev/table/concepts/dynamic_tables is failed and 404 is returned,FLINK-23292,13388108,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hapihu,hapihu,hapihu,07/Jul/21 07:39,23/Sep/21 18:01,13/Jul/23 08:12,12/Jul/21 03:43,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Documentation,,,,,0,pull-request-available,,,," 

The original is as follows：
{code:java}
 [time attributes](time_attributes.html) 
{code}
When i click the link [time attributes| https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/table/concepts/dynamic_tables/time_attributes.html], 404 is returned.

 

The English page is：[https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/table/concepts/dynamic_tables]
{noformat}

//The markdown file: docs/content/docs/dev/table/concepts/dynamic_tables.md
//1、 line 121 
[time attributes](time_attributes.html) 

This link should be modified to [time attributes]({{< ref ""docs/dev/table/concepts/time_attributes"" >}}) 

//2、 line 158  
[Query Configuration](query_configuration.html)  

This link should be modified to  [Query Configuration]({{< ref ""docs/dev/table/config"" >}})

//3、line 180  
[TableSources and TableSinks](../sourceSinks.html#define-a-tablesink)

This link should be modified to [TableSources and TableSinks]({{< ref ""docs/dev/table/sourcesSinks"">}}#dynamic-table-sink){noformat}
The Chinese page is：[https://ci.apache.org/projects/flink/flink-docs-master/zh/docs/dev/table/concepts/dynamic_tables/]
{noformat}

//The markdown file: docs/content.zh/docs/dev/table/concepts/dynamic_tables.md

//1、 line 120 
[时间属性](time_attributes.html)

This link should be modified to [time attributes]({{< ref ""docs/dev/table/concepts/time_attributes"" >}}) 

//2、 line 159  
[查询配置](query_configuration.html)  

This link should be modified to  [查询配置]({{< ref ""docs/dev/table/config"" >}})

//3、line 182  
[TableSources and TableSinks](../sourceSinks.html#define-a-tablesink)

This link should be modified to [TableSources and TableSinks]({{< ref ""docs/dev/table/sourcesSinks"">}}#dynamic-table-sink){noformat}
 

 

 

 

 ",,hapihu,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/21 07:38;hapihu;image-20210707153836711.png;https://issues.apache.org/jira/secure/attachment/13030214/image-20210707153836711.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 12 03:43:45 UTC 2021,,,,,,,,,,"0|z0sonk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/21 18:12;hapihu;Hi [~jark]，
Excuse me for taking up your time.
I created pull request for this issue.
Would you like to review it for me.
If there is any problem, please inform me in time. 
I will actively revise it in time.
Thank you very much!

 ;;;","12/Jul/21 03:43;jark;Fixed in master: 63f9c09d653c89a23c1cb9b930b56ab6255f0821;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"cast '(LZ *3' as boolean, get a null",FLINK-23290,13388094,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,xiaojin.wy,xiaojin.wy,07/Jul/21 06:54,15/Jul/21 07:56,13/Jul/23 08:12,15/Jul/21 07:56,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"{code:sql}
CREATE TABLE database5_t0(
c0 VARCHAR , c1 BIGINT 
) WITH (
 'connector' = 'filesystem',
 'path' = 'hdfs:///tmp/database5_t0.csv',
 'format' = 'csv'
);
INSERT OVERWRITE database5_t0(c0, c1) VALUES('(LZ *3', 2135917226)
SELECT database5_t0.c0 AS ref0 FROM database5_t0 WHERE CAST (database5_t0.c0 AS BOOLEAN)
{code}

After excuting that, you will get the error:

{code}
Caused by: java.lang.NullPointerException
	at BatchExecCalc$20.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:101)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:319)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:414)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:104)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:62)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:269)
{code}",,libenchao,TsReaper,xiaojin.wy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 07 07:39:56 UTC 2021,,,,,,,,,,"0|z0sokg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/21 07:39;TsReaper;Hi! I've looked into this issue and found that {{CalcCodeGenerator}} forgot to check if the expression is null when generating code for conditions.

I'm taking this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BinarySection should add null check in constructor method,FLINK-23289,13388070,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,Terry1897,Terry1897,07/Jul/21 03:38,10/Sep/21 10:58,13/Jul/23 08:12,17/Aug/21 08:22,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"{{BinarySection}} currently does not check if {{MemorySegment[]}} is null in its constructor. This might cause {{NullPointerException}} somewhere else and makes it harder to debug (as we don't know who sets the null value into {{BinarySection}}).

{code:java}
Caused by: java.lang.NullPointerException
    at org.apache.flink.table.data.binary.BinarySegmentUtils.inFirstSegment(BinarySegmentUtils.java:411)
    at org.apache.flink.table.data.binary.BinarySegmentUtils.copyToBytes(BinarySegmentUtils.java:132)
    at org.apache.flink.table.data.binary.BinarySegmentUtils.copyToBytes(BinarySegmentUtils.java:118)
    at org.apache.flink.table.data.binary.BinaryStringData.copy(BinaryStringData.java:360)
    at org.apache.flink.table.runtime.typeutils.StringDataSerializer.copy(StringDataSerializer.java:59)
    at org.apache.flink.table.runtime.typeutils.StringDataSerializer.copy(StringDataSerializer.java:37)
    at org.apache.flink.table.runtime.typeutils.ArrayDataSerializer.copyGenericArray(ArrayDataSerializer.java:128)
    at org.apache.flink.table.runtime.typeutils.ArrayDataSerializer.copy(ArrayDataSerializer.java:86)
    at org.apache.flink.table.runtime.typeutils.ArrayDataSerializer.copy(ArrayDataSerializer.java:47)
    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copyRowData(RowDataSerializer.java:170)
    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:131)
    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:48)
    at org.apache.flink.table.runtime.operators.join.lookup.AsyncLookupJoinWithCalcRunner$CalcCollectionCollector.collect(AsyncLookupJoinWithCalcRunner.java:152)
    at org.apache.flink.table.runtime.operators.join.lookup.AsyncLookupJoinWithCalcRunner$CalcCollectionCollector.collect(AsyncLookupJoinWithCalcRunner.java:142)
{code}
",,jark,lzljs3620320,Terry1897,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24251,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 17 08:22:06 UTC 2021,,,,,,,,,,"0|z0sof4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/21 05:42;lzljs3620320;+1;;;","09/Jul/21 05:43;TsReaper;I've added some description after discussing with [~Terry1897] offline. I'd like to take this issue.;;;","16/Jul/21 06:43;lzljs3620320;[~TsReaper] Assigned to u;;;","17/Aug/21 08:22;lzljs3620320;master: f9eaab54be84c1712756502c7214c953aa151c30;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The link on page docs/dev/datastream/event-time/generating_atermarks/ is failed and 404 is returned,FLINK-23285,13387978,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hapihu,hapihu,hapihu,06/Jul/21 16:56,23/Sep/21 18:01,13/Jul/23 08:12,12/Jul/21 03:44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Documentation,,,,,0,pull-request-available,,,,"The page url ：[https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/datastream/event-time/generating_watermarks/#watermark-strategies-and-the-kafka-connector]

 

1. When i click the link [Apache Kafka]([https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/datastream/event-time/generating_watermarks/connectors/kafka.html]), 404 is returned.The correct jump address is [https://ci.apache.org/projects/flink/flink-docs-master/docs/connectors/datastream/kafka/]

2. When i click the link [ascending timestamps watermark generator]([https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/datastream/event-time/generating_watermarks/event_timestamp_extractors.html#assigners-with-ascending-timestamps]), 404 is returned.The correct jump address is [https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/datastream/event-time/built_in/#monotonously-increasing-timestamps].

3. The link shown below returns 404.
{code:java}
//
//1. line414 
[Apache Kafka](connectors/kafka.html)
//2. line427
[ascending timestamps watermark
generator](event_timestamp_extractors.html#assigners-with-ascending-timestamps)

{code}
4.  The correct link address is shown below.
{code:java}
//1.
[Apache Kafka]({{< ref ""docs/connectors/datastream/kafka"" >}}) 

//2. 
[ascending timestamps watermark
generator]({{< ref ""docs/dev/datastream/event-time/built_in"">}}#monotonously-increasing-timestamps)
{code}",,hapihu,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 12 03:44:51 UTC 2021,,,,,,,,,,"0|z0snv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/21 18:13;hapihu;Hi [~jark]，
Excuse me for taking up your time.
I created pull request for this issue.
Would you like to review it for me.
If there is any problem, please inform me in time.
I will actively revise it in time.
Thank you very much!;;;","12/Jul/21 03:44;jark;Fixed in master: ac16b45ade627afd37ded95d390b5cab415c92c9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
All records are processed in the close stage in ContinuousFileReaderOperatorBenchmark,FLINK-23284,13387961,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,06/Jul/21 14:52,07/Jul/21 07:59,13/Jul/23 08:12,07/Jul/21 07:59,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Benchmarks,,,,,0,pull-request-available,,,,The {{TARGET_COUNT_REACHED_LATCH}} is not correctly reset after the warmup iterations and thus subsequent runs process all records in the {{CLOSE}} stage of the {{ContinuousFileReaderOperator}} testing something different than anticipated.,,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 07 07:59:30 UTC 2021,,,,,,,,,,"0|z0snrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/21 07:59;dwysakowicz;Fixed in f40b69e6341118720ff941b91e98729baf7e0224;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GroupWindowITCase.testWindowAggregateOnUpsertSource fails on azure,FLINK-23283,13387954,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jingzhang,roman,roman,06/Jul/21 14:37,23/Sep/21 17:59,13/Jul/23 08:12,07/Jul/21 11:53,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19978&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=a6e0f756-5bb9-5ea8-a468-5f60db442a29&l=7086

{code}
2021-07-06T10:13:10.6091443Z Jul 06 10:13:10 [ERROR] Tests run: 48, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 14.945 s <<< FAILURE! - in org.apache.flink.table.planner.runtime.stream.sql.GroupWindowITCase
2021-07-06T10:13:10.6093667Z Jul 06 10:13:10 [ERROR] testWindowAggregateOnUpsertSource[StateBackend=HEAP, UseTimestampLtz = false](org.apache.flin*k.table.planner.runtime.stream.sql.GroupWindowITCase)  Time elapsed: 0.329 s  <<< FAILURE!
2021-07-06T10:13:10.6096965Z Jul 06 10:13:10 java.lang.AssertionError: expected:<List(Euro,1,118,1970-01-01T00:00:15,1970-01-01T00:00:20, US Dolla r,1,102,1970-01-01T00:00,1970-01-01T00:00:05, Yen,1,1,1970-01-01T00:00,1970-01-01T00:00:05)> but was:<List(Euro,1,118,1970-01-01T00:00:15,1970-01- 01T00:00:20, US Dollar,1,104,1970-01-01T00:00,1970-01-01T00:00:05, Yen,1,1,1970-01-01T00:00,1970-01-01T00:00:05)>
2021-07-06T10:13:10.6098985Z Jul 06 10:13:10    at org.junit.Assert.fail(Assert.java:89)
2021-07-06T10:13:10.6099695Z Jul 06 10:13:10    at org.junit.Assert.failNotEquals(Assert.java:835)
2021-07-06T10:13:10.6100489Z Jul 06 10:13:10    at org.junit.Assert.assertEquals(Assert.java:120)
2021-07-06T10:13:10.6101292Z Jul 06 10:13:10    at org.junit.Assert.assertEquals(Assert.java:146)
2021-07-06T10:13:10.6102395Z Jul 06 10:13:10    at org.apache.flink.table.planner.runtime.stream.sql.GroupWindowITCase.testWindowAggregateOnUpsert*Source(GroupWindowITCase.scala:421)
{code}

Fails locally 3 times out of 1000",,dwysakowicz,godfreyhe,jingzhang,roman,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 07 11:53:45 UTC 2021,,,,,,,,,,"0|z0snps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/21 14:54;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19995&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=a6e0f756-5bb9-5ea8-a468-5f60db442a29&l=7334;;;","07/Jul/21 02:26;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20034&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=63589e47-958f-5d34-4609-30c2a008e9d1&l=9130;;;","07/Jul/21 02:28;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20034&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=c7cad7d8-80fd-5b5c-ba7f-247c7452c419&l=9441;;;","07/Jul/21 02:31;xtsong;[~qingru zhang], do you mind take a look at this?;;;","07/Jul/21 04:04;jingzhang;[~xintongsong] Sure, please assign to me.;;;","07/Jul/21 04:45;xtsong;Thank you [~qingru zhang], you are assigned.;;;","07/Jul/21 06:20;jingzhang;[~xintongsong] The unstable case is caused by whether the last two records dropped as late data is nonderterministic. It depends on the watermark on current task which is relates to parallelism and what data is processed on each concurrent task.

And

`GroupWindowITCase#testWindowAggregateOnUpsertSourcePushdownWatermark` has similar problems, I would set parallelism to 1 for those two cases to avoid unstable failure.;;;","07/Jul/21 11:53;godfreyhe;Fixed in 1.14.0: 4e9fa339dfd921d2cbc4c169bf8b894635d1c528;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python ExplainDetails does not have JSON_EXECUTION_PLAN option,FLINK-23280,13387925,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,nicholasjiang,mans2singh,mans2singh,06/Jul/21 12:00,23/Sep/21 17:59,13/Jul/23 08:12,07/Jul/21 12:47,1.13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,API / Python,Table SQL / API,,,,0,explain,pull-request-available,python,table-api,Add missing JSON_EXECUTION_PLAN option to python ExplainDetails class (https://github.com/apache/flink/blob/master/flink-python/pyflink/table/explain_detail.py),,hxbks2ks,mans2singh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 07 12:47:56 UTC 2021,,,,,,,,,,"0|z0snjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/21 12:47;hxbks2ks;Merged into master via da7c685fadf4cacd765fd9a4b888bbcd3d4ced67
Merged into release-1.13 via 79019bc1196fb65a04f7533d6abc3776a4dcb652;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Changelog backend creates ""raw materialized"" savepoint but expects ""normal"" snapshot on recovery",FLINK-23278,13387895,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,06/Jul/21 09:58,23/Sep/21 18:00,13/Jul/23 08:12,08/Jul/21 18:32,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"Savepoint consist of ""raw"" keyed handles from the underlying backend:
{code}
    public SavepointResources<K> savepoint() throws Exception {
        return keyedStateBackend.savepoint();
    }
{code}

On recovery, ChangelogStateBackendHandles are expected.

This fails e.g. SavepointWriterITCase if enabled.",,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23279,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 08 18:32:42 UTC 2021,,,,,,,,,,"0|z0snco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/21 18:32;roman;Merged into master as 4e2ce7b3fe6ce96edae22520b8b7a6c59cb18452.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changelog backend doesn't apply TTL after recovery,FLINK-23277,13387894,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,06/Jul/21 09:53,23/Sep/21 18:01,13/Jul/23 08:12,09/Jul/21 12:09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"Upon recovery, changelog backend requests states to apply changes.
TTL config is not available at this moment, so states are created regardless of TTL config.
One solution is to serialize TTL config along with metadata (in changelog).

Note: values are already serialized as TTL values and serializers as TTL seralizers

{code}
Caused by: java.lang.ClassCastException: org.apache.flink.runtime.state.ttl.TtlValue cannot be cast to org.apache.flink.table.data.RowData
   at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:129)
   at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43)
   at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83)
   at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:228)
   at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
   at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
   at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66)
   at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:428)
   at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:204)
   at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:691)
   at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:646)
   at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:657)
   at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:630)
   at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
   at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
   at java.lang.Thread.run(Thread.java:748)
{code}

(doesn't affect test stability as changelog backend is currently disabled in tests)",,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23279,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 09 12:09:49 UTC 2021,,,,,,,,,,"0|z0sncg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jul/21 12:09;roman;Merged into master as a3405df7655ca437ec13d76f7b4aea6837e28d2f..f2eb6559ba87d1de0c6a5e18ba15bc55dae135f8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changelog backend not always updates delegating functions,FLINK-23276,13387888,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,06/Jul/21 09:43,08/Jul/21 07:18,13/Jul/23 08:12,08/Jul/21 07:18,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"(currently disabled in tests, so doesn't affect build stability)

E.g. org.apache.flink.table.planner.runtime.stream.table.GroupWindowITCase, or other tests from the same package.

{code}
 2021-07-03T20:30:28.0384912Z Jul 03 20:30:28 Caused by: java.lang.NullPointerException
*2021-07-03T20:30:28.0385566Z Jul 03 20:30:28    at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:59)
*2021-07-03T20:30:28.0386330Z Jul 03 20:30:28    at org.apache.flink.state.changelog.restore.FunctionDelegationHelper$DelegatingReduceFunction.redu*ce(FunctionDelegationHelper.java:138)
*2021-07-03T20:30:28.0387147Z Jul 03 20:30:28    at org.apache.flink.contrib.streaming.state.RocksDBReducingState.add(RocksDBReducingState.java:95)
*2021-07-03T20:30:28.0387892Z Jul 03 20:30:28    at org.apache.flink.state.changelog.ChangelogReducingState.add(ChangelogReducingState.java:82)
*2021-07-03T20:30:28.0388677Z Jul 03 20:30:28    at org.apache.flink.table.runtime.operators.window.triggers.ElementTriggers$CountElement.onElement*(ElementTriggers.java:124)
*2021-07-03T20:30:28.0389503Z Jul 03 20:30:28    at org.apache.flink.table.runtime.operators.window.WindowOperator$TriggerContext.onElement(WindowO*perator.java:572)
*2021-07-03T20:30:28.0390296Z Jul 03 20:30:28    at org.apache.flink.table.runtime.operators.window.WindowOperator.processElement(WindowOperator.ja*va:379)
*2021-07-03T20:30:28.0391107Z Jul 03 20:30:28    at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(*OneInputStreamTask.java:228)
*2021-07-03T20:30:28.0391936Z Jul 03 20:30:28    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStr*eamTaskNetworkInput.java:134)
*2021-07-03T20:30:28.0392776Z Jul 03 20:30:28    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTas*kNetworkInput.java:105)
*2021-07-03T20:30:28.0393929Z Jul 03 20:30:28    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProces*sor.java:66)
*2021-07-03T20:30:28.0394611Z Jul 03 20:30:28    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:428)
*2021-07-03T20:30:28.0395288Z Jul 03 20:30:28    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcess*or.java:204)
*2021-07-03T20:30:28.0395966Z Jul 03 20:30:28    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:691)
*2021-07-03T20:30:28.0396892Z Jul 03 20:30:28    at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:646)
*2021-07-03T20:30:28.0397601Z Jul 03 20:30:28    at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:657)
*2021-07-03T20:30:28.0398398Z Jul 03 20:30:28    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:630)
*2021-07-03T20:30:28.0399051Z Jul 03 20:30:28    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
*2021-07-03T20:30:28.0399669Z Jul 03 20:30:28    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
 2021-07-03T20:30:28.0400238Z Jul 03 20:30:28    at java.lang.Thread.run(Thread.java:748)

{code}

The reason is missing functionDelegationHelper.addOrUpdate() call in  
ChangelogKeyedStateBackend.getPartitionedState().",,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23279,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 08 07:18:51 UTC 2021,,,,,,,,,,"0|z0snb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/21 07:18;roman;Merged as 95ea0da61f26afbe9d3499791e1a95895d714dff.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Impove description of Regular Joins section,FLINK-23270,13387858,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liufangliang,liufangliang,liufangliang,06/Jul/21 07:46,23/Sep/21 17:57,13/Jul/23 08:12,06/Jul/21 13:57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Documentation,Table SQL / API,,,,0,pull-request-available,,,,"[https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/table/sql/queries/joins/]

Regular Joins description

Regular joins are the most generic type of join in which any new record, or changes to either side of the join, are visible and affect the entirety of the join result. For example, if there is a new record on the left side, it will be joined with all the previous and future records on the right side. 

But, example 
{code:java}
SELECT * FROM Orders
INNER JOIN Product
ON Orders.productId = Product.id
{code}
Is the following better?
{code:java}
SELECT * FROM Orders 
INNER JOIN Product
{code}
[~jark] , looking forward to your replay.

 ",,jark,liufangliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 06 13:57:39 UTC 2021,,,,,,,,,,"0|z0sn4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/21 08:13;jark;Why remove the JOIN ON condition? ;;;","06/Jul/21 08:41;liufangliang;[~jark]

it is same as  INNER Equi-JOIN when adding JOIN ON condition. Returns a simple Cartesian product restricted by the join condition.Does not meet the description ""For example, if there is a new record on the left side, it will be joined with all the previous and future records on the right side."" in Regular Joins section.

In addition, this example is the same as  INNER Equi-JOIN section.;;;","06/Jul/21 08:52;jark;I think what we should improve is the description to mention: ""if there is a new record on the left side, it will be joined with all the previous and future records on the right side when the product id equals. "";;;","06/Jul/21 08:53;jark;It's not recommended to use JOIN without JOIN ON condition in streaming mode.;;;","06/Jul/21 09:12;liufangliang;[~jark], i agress with you, can you assign this to me ?;;;","06/Jul/21 13:57;jark;Fixed in 
 - master: 8e65421f4f7a9c3a32f82bcbc5d6790c69ca5010
 - release-1.13: ca7955a691c6dbf4102b499ed73e452a30650620;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[DOCS]The link on page docs/dev/table/sql/queries/match_recognize/ is failed and 404 is returned,FLINK-23268,13387839,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hapihu,hapihu,hapihu,06/Jul/21 05:47,12/Jul/21 03:45,13/Jul/23 08:12,12/Jul/21 03:45,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Documentation,Table SQL / API,,,,0,pull-request-available,,,,"Some link information on the page is incorrectly written, resulting in a 404 page

The page url ：[https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/table/sql/queries/match_recognize/]

The markdown file：[https://github.com/apache/flink/blob/master/docs/content/docs/dev/table/sql/queries/match_recognize.md]

When i click on this the link `[append table](dynamic_tables.html#update-and-append-queries)`, I get a 404 page。

The corresponding address is ：[https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/table/sql/queries/match_recognize/dynamic_tables.html#update-and-append-queries]

Refer to document [https://github.com/apache/flink/blob/master/docs/content.zh/docs/dev/table/sql/queries/match_recognize.md] for the correct link information

 

The link shown below returns 404
{code:java}
//1 
 [append table](dynamic_tables.html#update-and-append-queries)
//2 
 [processing time or event time](time_attributes.html)
//3
 [time attributes](time_attributes.html)
//4
 <a href=""time_attributes.html"">rowtime attribute</a>
//5
 <a href=""time_attributes.html#processing-time"">proctime attribute</a>
//6
 [state retention time](query_configuration.html#idle-state-retention-time)
{code}",,hapihu,jark,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jul/21 05:47;hapihu;image-20210706134442433.png;https://issues.apache.org/jira/secure/attachment/13030127/image-20210706134442433.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 12 03:45:52 UTC 2021,,,,,,,,,,"0|z0sn08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/21 03:14;yunta;[~hapihu], already assigned this ticket to you.;;;","11/Jul/21 18:14;hapihu;Hi [~yunta],
Excuse me for taking up your time.
I created pull request for this issue.
Would you like to review it for me.
If there is any problem, please inform me in time.
I will actively revise it in time.
Thank you very much!;;;","12/Jul/21 03:45;jark;Fixed in master: 2fe17acef83cca24a021e3ee0392f9bf57b4bbd6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileReadingWatermarkITCase.testWatermarkEmissionWithChaining fails on azure,FLINK-23262,13387815,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,xtsong,xtsong,06/Jul/21 01:59,23/Sep/21 18:01,13/Jul/23 08:12,09/Jul/21 07:55,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,API / DataStream,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19942&view=logs&j=219e462f-e75e-506c-3671-5017d866ccf6&t=4c5dc768-5c82-5ab0-660d-086cb90b76a0&l=5584

{code}
Jul 05 22:19:00 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 4.334 s <<< FAILURE! - in org.apache.flink.test.streaming.api.FileReadingWatermarkITCase
Jul 05 22:19:00 [ERROR] testWatermarkEmissionWithChaining(org.apache.flink.test.streaming.api.FileReadingWatermarkITCase)  Time elapsed: 4.16 s  <<< FAILURE!
Jul 05 22:19:00 java.lang.AssertionError: too few watermarks emitted: 4
Jul 05 22:19:00 	at org.junit.Assert.fail(Assert.java:89)
Jul 05 22:19:00 	at org.junit.Assert.assertTrue(Assert.java:42)
Jul 05 22:19:00 	at org.apache.flink.test.streaming.api.FileReadingWatermarkITCase.testWatermarkEmissionWithChaining(FileReadingWatermarkITCase.java:65)
Jul 05 22:19:00 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 05 22:19:00 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 05 22:19:00 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 05 22:19:00 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 05 22:19:00 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jul 05 22:19:00 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jul 05 22:19:00 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jul 05 22:19:00 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jul 05 22:19:00 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 05 22:19:00 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jul 05 22:19:00 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jul 05 22:19:00 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Jul 05 22:19:00 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Jul 05 22:19:00 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Jul 05 22:19:00 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Jul 05 22:19:00 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Jul 05 22:19:00 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Jul 05 22:19:00 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Jul 05 22:19:00 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 05 22:19:00 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Jul 05 22:19:00 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Jul 05 22:19:00 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Jul 05 22:19:00 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Jul 05 22:19:00 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Jul 05 22:19:00 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jul 05 22:19:00 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jul 05 22:19:00 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jul 05 22:19:00 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,dwysakowicz,roman,xtsong,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23331,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 09 07:55:39 UTC 2021,,,,,,,,,,"0|z0smuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/21 09:05;zhuzh;another instance:
https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/19956/logs/104;;;","07/Jul/21 02:14;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20029&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=10332;;;","07/Jul/21 02:24;xtsong;[~roman_khachatryan], do you mind take a look at this? ;;;","07/Jul/21 10:02;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20070&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=4916;;;","07/Jul/21 10:25;roman;Sure, I'll take a look [~xintongsong] .;;;","08/Jul/21 06:57;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20131&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=4861;;;","08/Jul/21 07:08;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20132&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=10513;;;","08/Jul/21 07:36;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20136&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=4916;;;","08/Jul/21 10:55;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20141&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=10430;;;","09/Jul/21 05:37;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20201&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=4820;;;","09/Jul/21 06:26;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20202&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=4820;;;","09/Jul/21 07:55;dwysakowicz;Fixed in b8ff60d9ea3ce22546b50d23dc0ad6c96b034191;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[DOCS]The link on page docs/libs/gelly/overview is failed and 404 is returned,FLINK-23260,13387774,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hapihu,hapihu,hapihu,05/Jul/21 17:46,23/Sep/21 17:56,13/Jul/23 08:12,06/Jul/21 06:42,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Documentation,,,,,0,pull-request-available,,,,"[https://ci.apache.org/projects/flink/flink-docs-master/docs/libs/gelly/overview/]

The link shown below returns 404

```txt
 * [Graph API](graph_api.html)
 * [Iterative Graph Processing](iterative_graph_processing.html)
 * [Library Methods](library_methods.html)
 * [Graph Algorithms](graph_algorithms.html)
 * [Graph Generators](graph_generators.html)
 * [Bipartite Graphs](bipartite_graph.html)

 

[JaccardIndex](./library_methods.html#jaccard-index)

 

[graph metrics](./library_methods.html#metric)

 

[library generator](./graph_generators.html#rmat-graph)

```",,hapihu,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/21 17:47;hapihu;image-20210706014654506.png;https://issues.apache.org/jira/secure/attachment/13030116/image-20210706014654506.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 06 06:42:42 UTC 2021,,,,,,,,,,"0|z0smls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/21 06:42;jark;Fixed in
- master: aedfec0ba2bada7efd12843f81588d2fc42b143b
- release-1.13: 9526541f51e0cb472fbad350a52b840f88c31004
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[DOCS]The 'window' link on page docs/dev/datastream/operators/overview is failed and 404 is returned,FLINK-23259,13387768,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hapihu,hapihu,hapihu,05/Jul/21 17:12,23/Sep/21 17:56,13/Jul/23 08:12,06/Jul/21 06:44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Documentation,,,,,0,pull-request-available,,,," 

[https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/datastream/operators/overview/#window]

 

The 'window' link this page is failed and 404 is returned。

The original is as follows：

```txt
 See [windows](windows.html) for a complete description of windows.
 ```

See [windows]([https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/datastream/operators/overview/windows.html]) for a complete description of windows.

 

 

 

The modification is done as follows

```txt
 [windows](windows.html) --> [windows]({{< ref ""docs/dev/datastream/operators/windows"" >}})
 ```
  

 

 

 ",,hapihu,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/21 17:14;hapihu;image-20210706004743893.png;https://issues.apache.org/jira/secure/attachment/13030114/image-20210706004743893.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 06 06:44:47 UTC 2021,,,,,,,,,,"0|z0smkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/21 17:32;hapihu;Hi [~jark] 
I created pull request for this issue .
Please help to review it
If there is any problem, please inform me in time. 
I active support the modification.
Thank you.;;;","06/Jul/21 06:44;jark;Fixed in 
- master: dfb4394c8e0e8b266c43dd4af89ddba1321e4e1b
- release-1.13: 8ac80279d7d0d19af0d0bf2b7129ec7304c628eb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
docker-build.sh outdated,FLINK-23257,13387766,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,05/Jul/21 16:51,06/Jul/21 15:00,13/Jul/23 08:12,06/Jul/21 15:00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Project Website,,,,,0,pull-request-available,,,,The script doesn't work because it tries to use an outdated ruby version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 06 15:00:21 UTC 2021,,,,,,,,,,"0|z0smk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/21 15:00;chesnay;asf-site: f72e7de3ee6612d0ae801a01656e58a6b18cb0e9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SinkWriter is not closed when failing,FLINK-23248,13387669,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fpaul,fpaul,fpaul,05/Jul/21 08:55,23/Sep/21 17:56,13/Jul/23 08:12,06/Jul/21 08:16,1.12.4,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,,,,Runtime / Task,,,,,0,pull-request-available,,,,"Currently the SinkWriter is only closed when the operator finishes in `AbstractSinkWriterOperator#close()` but we also must close the SinkWrite on `AbstractSinkWriterOperator#dispose()` to release possible acquired resources when failing

 

 ",,fpaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 06 08:15:51 UTC 2021,,,,,,,,,,"0|z0slyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/21 07:54;fpaul;In Flink 1.14.0 the issue is resolved by https://issues.apache.org/jira/browse/FLINK-22972 ;;;","06/Jul/21 08:15;arvid;Merged into 1.13 as 9d77656381f55d05a3eb74c6c6c6de873de2b7c7, into 1.12 as 701d20e896112b64d3a2dad9bb7e2e976bc88cb1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResumeCheckpointManuallyITCase.testExternalizedFSCheckpointsWithLocalRecoveryZookeeper fails on azure,FLINK-23240,13387637,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xtsong,xtsong,xtsong,05/Jul/21 05:31,20/Feb/22 08:58,13/Jul/23 08:12,20/Feb/22 08:58,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.4,1.15.0,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19872&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=d13f554f-d4b9-50f8-30ee-d49c6fb0b3cc&l=10186

{code}
Jul 04 22:17:29 [ERROR] Tests run: 12, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 91.407 s <<< FAILURE! - in org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase
Jul 04 22:17:29 [ERROR] testExternalizedFSCheckpointsWithLocalRecoveryZookeeper(org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase)  Time elapsed: 31.356 s  <<< ERROR!
Jul 04 22:17:29 java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Invocation of public abstract java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.cancelJob(org.apache.flink.api.common.JobID,org.apache.flink.api.common.time.Time) timed out.
Jul 04 22:17:29 	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
Jul 04 22:17:29 	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
Jul 04 22:17:29 	at org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.runJobAndGetExternalizedCheckpoint(ResumeCheckpointManuallyITCase.java:303)
Jul 04 22:17:29 	at org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.testExternalizedCheckpoints(ResumeCheckpointManuallyITCase.java:275)
Jul 04 22:17:29 	at org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.testExternalizedFSCheckpointsWithLocalRecoveryZookeeper(ResumeCheckpointManuallyITCase.java:215)
Jul 04 22:17:29 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 04 22:17:29 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 04 22:17:29 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 04 22:17:29 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
Jul 04 22:17:29 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jul 04 22:17:29 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jul 04 22:17:29 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jul 04 22:17:29 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jul 04 22:17:29 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jul 04 22:17:29 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Jul 04 22:17:29 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 04 22:17:29 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jul 04 22:17:29 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jul 04 22:17:29 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Jul 04 22:17:29 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Jul 04 22:17:29 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Jul 04 22:17:29 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Jul 04 22:17:29 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Jul 04 22:17:29 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Jul 04 22:17:29 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Jul 04 22:17:29 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Jul 04 22:17:29 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Jul 04 22:17:29 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 04 22:17:29 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Jul 04 22:17:29 	at org.junit.runners.Suite.runChild(Suite.java:128)
Jul 04 22:17:29 	at org.junit.runners.Suite.runChild(Suite.java:27)
Jul 04 22:17:29 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Jul 04 22:17:29 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Jul 04 22:17:29 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Jul 04 22:17:29 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Jul 04 22:17:29 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Jul 04 22:17:29 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 04 22:17:29 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Jul 04 22:17:29 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
Jul 04 22:17:29 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
Jul 04 22:17:29 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
Jul 04 22:17:29 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
Jul 04 22:17:29 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
Jul 04 22:17:29 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
Jul 04 22:17:29 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jul 04 22:17:29 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jul 04 22:17:29 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jul 04 22:17:29 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Jul 04 22:17:29 Caused by: java.util.concurrent.TimeoutException: Invocation of public abstract java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.cancelJob(org.apache.flink.api.common.JobID,org.apache.flink.api.common.time.Time) timed out.
Jul 04 22:17:29 	at com.sun.proxy.$Proxy30.cancelJob(Unknown Source)
Jul 04 22:17:29 	at org.apache.flink.runtime.minicluster.MiniCluster.lambda$cancelJob$7(MiniCluster.java:716)
Jul 04 22:17:29 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:680)
Jul 04 22:17:29 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
Jul 04 22:17:29 	at java.base/java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:2094)
Jul 04 22:17:29 	at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:758)
Jul 04 22:17:29 	at org.apache.flink.runtime.minicluster.MiniCluster.cancelJob(MiniCluster.java:715)
Jul 04 22:17:29 	at org.apache.flink.client.program.MiniClusterClient.cancel(MiniClusterClient.java:83)
Jul 04 22:17:29 	... 46 more
Jul 04 22:17:29 Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/dispatcher_2#-1806874751]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
Jul 04 22:17:29 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
Jul 04 22:17:29 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
Jul 04 22:17:29 	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)
Jul 04 22:17:29 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)

Jul 04 22:17:29 	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
Jul 04 22:17:29 	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
Jul 04 22:17:29 	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
Jul 04 22:17:29 	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
Jul 04 22:17:29 	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)
Jul 04 22:17:29 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)
Jul 04 22:17:29 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
Jul 04 22:17:29 	at java.base/java.lang.Thread.run(Thread.java:834)
{code}",,pnowojski,roman,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21667,,,,,,,,FLINK-25893,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 18 09:29:22 UTC 2022,,,,,,,,,,"0|z0slrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/22 17:46;roman;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30857&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=24001
This is 1.15. The test didn't time out but hang up, and then the whole build timed out.

{code}
16:29:42,407 [    pool-15-thread-1] WARN  org.apache.flink.runtime.minicluster.MiniCluster             [] - Error in MiniCluster. Shutting the MiniCluster down.
org.apache.flink.util.FlinkException: Unexpected termination of ResourceManagerService.
        at org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent.lambda$handleUnexpectedResourceManagerTermination$0(DispatcherResourceManagerComponent.java:104) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) ~[?:1.8.0_292]
        at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.util.concurrent.FutureUtils.lambda$forwardTo$24(FutureUtils.java:1372) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:792) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2153) ~[?:1.8.0_292]
        at org.apache.flink.util.concurrent.FutureUtils.forward(FutureUtils.java:1342) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl.closeAsync(ResourceManagerServiceImpl.java:165) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl.lambda$revokeLeadership$2(ResourceManagerServiceImpl.java:221) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
{code}

cc: [~trohrmann];;;","08/Feb/22 08:37;trohrmann;The problem seems to be that the {{ResourceManagerServiceImpl}} closes itself if it loses leadership. Therefore, the {{MiniCluster}} shuts down and the test hangs.

It looks to me as if the {{ResourceManagerServiceImpl}} can only survive a single leadership session unless the property {{flink.tests.enable-rm-multi-leader-session}} is set ([code|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManagerServiceImpl.java#L220-L222]). If this is correct, then we might have a cluster instability because every RM leadership loss would trigger an unexpected termination of the {{ResourceManager}} which will terminate the process. [~xtsong] have I understood the code correctly?;;;","08/Feb/22 08:38;trohrmann;I am promoting this issue to be a blocker until we have invalidated my suspicion.;;;","08/Feb/22 08:39;trohrmann;cc [~dmvk];;;","08/Feb/22 14:00;xtsong;[~trohrmann],

I think you have understand the code correctly.

Moreover, despite the original RM interface (and some test cases, thus the `enable-rm-multi-leader-session` property) looks like it was designed to live through multiple leader sessions, I have an impression that we had never really supported that. I cannot recall what was the problem before FLINK-21667. According to the discussion in this [PR|https://github.com/apache/flink/pull/15524], we agreed to narrow down the scope of FLINK-21667 to only solve the problem that non-leading RM may accidentally change the resources, and decided to support multiple leader sessions in one process later if needed.

I think at least the Yarn deployment cannot support multiple leader sessions. Maybe we can revisit this for other deployment modes, and do not terminate the process where multiple leader session is feasible.

From my side, I would not consider this as a release blocker. Because normally RM lost leadership only when 1) there's a problem with the leading master process or 2) when the HA services is unstable/unavailable.
- For 1), termination of process is desired.
- For 2), the job fails anyway. The regression only exists when the HA services has been down for long enough to trigger the leadership lost, and soon come back online that is faster than the process being restarted.;;;","08/Feb/22 16:59;trohrmann;It might be a problem for standalone setups where the JobManager process won't be automatically restarted. Moreover, it might lead to unnecessary restarts that might be confusing to people.;;;","18/Feb/22 09:29;xtsong;Fixed via
- master (1.15): 7a6b64adc438ff26a1cdc809df126ab3d30c7c3d
- release-1.14: 2465c850f48497cdb19edd10240ce3e8c8fe71de;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveTableSinkITCase hangs on azure,FLINK-23239,13387634,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,xtsong,xtsong,05/Jul/21 05:26,23/Sep/21 18:10,13/Jul/23 08:12,30/Jul/21 06:52,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Connectors / Hive,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19872&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=420bd9ec-164e-562e-8947-0dacde3cec91&l=23845

{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007fac9000b800 nid=0x619b waiting on condition [0x00007fac98621000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sleepBeforeRetry(CollectResultFetcher.java:237)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:113)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
	at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:370)
	at org.apache.flink.connectors.hive.HiveTableSinkITCase.fetchRows(HiveTableSinkITCase.java:384)
	at org.apache.flink.connectors.hive.HiveTableSinkITCase.testStreamingSinkWithTimestampLtzWatermark(HiveTableSinkITCase.java:360)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,dwysakowicz,leonard,lirui,liyu,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 30 06:52:42 UTC 2021,,,,,,,,,,"0|z0slqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/21 07:04;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20925&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=27047;;;","26/Jul/21 13:29;lirui;I examined the test log and guess I've found the cause. We write two records into each partition of {{source_table}}, and each of these two records can trigger partition commit in {{sink_table}}. So if a CP is done in between of these two records, the partition will be committed with only one record. And if the reading job checks {{sink_table}} for new partitions before the 2nd record is written, it only finds the 1st record but believes the partition has all the data. Therefore we'll hang at the verification code waiting for the 2nd record, which is now a late event.;;;","29/Jul/21 13:51;lirui;Fixed in master: 65f3197cf055630ec35dc660e7ca99364eb50dde
Fixed in release-1.13: e4d62ba2b8a716afdd32819b9fbe650ce1faa4dd;;;","30/Jul/21 06:36;liyu;[~lirui] Thanks for the efforts. Any reason we still haven't marked the status of the issue as resolved? Waiting for some verification? Thanks.;;;","30/Jul/21 06:52;lirui;[~liyu] Thanks for the reminder. I'm closing this one.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SinkITCase.writerAndCommitterAndGlobalCommitterExecuteInStreamingMode fails on azure,FLINK-23235,13387628,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,maguowei,xtsong,xtsong,05/Jul/21 04:48,23/Sep/21 18:02,13/Jul/23 08:12,13/Jul/21 02:59,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,API / DataStream,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19867&view=logs&j=02c4e775-43bf-5625-d1cc-542b5209e072&t=e5961b24-88d9-5c77-efd3-955422674c25&l=9972

{code}
Jul 03 23:57:29 [ERROR] Tests run: 6, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 5.53 s <<< FAILURE! - in org.apache.flink.test.streaming.runtime.SinkITCase
Jul 03 23:57:29 [ERROR] writerAndCommitterAndGlobalCommitterExecuteInStreamingMode(org.apache.flink.test.streaming.runtime.SinkITCase)  Time elapsed: 0.68 s  <<< FAILURE!
Jul 03 23:57:29 java.lang.AssertionError: 
Jul 03 23:57:29 
Jul 03 23:57:29 Expected: iterable over [""(895,null,-9223372036854775808)"", ""(895,null,-9223372036854775808)"", ""(127,null,-9223372036854775808)"", ""(127,null,-9223372036854775808)"", ""(148,null,-9223372036854775808)"", ""(148,null,-9223372036854775808)"", ""(161,null,-9223372036854775808)"", ""(161,null,-9223372036854775808)"", ""(148,null,-9223372036854775808)"", ""(148,null,-9223372036854775808)"", ""(662,null,-9223372036854775808)"", ""(662,null,-9223372036854775808)"", ""(822,null,-9223372036854775808)"", ""(822,null,-9223372036854775808)"", ""(491,null,-9223372036854775808)"", ""(491,null,-9223372036854775808)"", ""(275,null,-9223372036854775808)"", ""(275,null,-9223372036854775808)"", ""(122,null,-9223372036854775808)"", ""(122,null,-9223372036854775808)"", ""(850,null,-9223372036854775808)"", ""(850,null,-9223372036854775808)"", ""(630,null,-9223372036854775808)"", ""(630,null,-9223372036854775808)"", ""(682,null,-9223372036854775808)"", ""(682,null,-9223372036854775808)"", ""(765,null,-9223372036854775808)"", ""(765,null,-9223372036854775808)"", ""(434,null,-9223372036854775808)"", ""(434,null,-9223372036854775808)"", ""(970,null,-9223372036854775808)"", ""(970,null,-9223372036854775808)"", ""(714,null,-9223372036854775808)"", ""(714,null,-9223372036854775808)"", ""(795,null,-9223372036854775808)"", ""(795,null,-9223372036854775808)"", ""(288,null,-9223372036854775808)"", ""(288,null,-9223372036854775808)"", ""(422,null,-9223372036854775808)"", ""(422,null,-9223372036854775808)""] in any order
Jul 03 23:57:29      but: Not matched: ""end of input""
Jul 03 23:57:29 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Jul 03 23:57:29 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
Jul 03 23:57:29 	at org.apache.flink.test.streaming.runtime.SinkITCase.writerAndCommitterAndGlobalCommitterExecuteInStreamingMode(SinkITCase.java:139)
Jul 03 23:57:29 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 03 23:57:29 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 03 23:57:29 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 03 23:57:29 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 03 23:57:29 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Jul 03 23:57:29 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jul 03 23:57:29 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Jul 03 23:57:29 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jul 03 23:57:29 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Jul 03 23:57:29 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Jul 03 23:57:29 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jul 03 23:57:29 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
Jul 03 23:57:29 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Jul 03 23:57:29 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
Jul 03 23:57:29 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
Jul 03 23:57:29 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
Jul 03 23:57:29 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Jul 03 23:57:29 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Jul 03 23:57:29 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Jul 03 23:57:29 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Jul 03 23:57:29 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Jul 03 23:57:29 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
Jul 03 23:57:29 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
Jul 03 23:57:29 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Jul 03 23:57:29 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
Jul 03 23:57:29 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Jul 03 23:57:29 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Jul 03 23:57:29 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Jul 03 23:57:29 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Jul 03 23:57:29 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jul 03 23:57:29 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jul 03 23:57:29 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jul 03 23:57:29 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,kevin.cyj,maguowei,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20950,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/21 09:03;maguowei;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13030261/screenshot-1.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 12 09:03:01 UTC 2021,,,,,,,,,,"0|z0slpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/21 04:49;xtsong;cc [~kevin.cyj]
Could this be related to FLINK-20010?;;;","06/Jul/21 02:28;kevin.cyj;The error seems to be similar. But FLINK-20010 has been confirmed to be fixed and not reported for a long time. I guess this instability can be a new Introduced one.;;;","08/Jul/21 09:07;maguowei;Investigated the reason for this problem: At present, an operator may still complete the checkpoint after receiving ""EndOfInput"". Leading to GlobalCommit will submit an ""end of input"".Specifically, when Checkpoint4 is successful, the source node has sent the end-of-input message
 !screenshot-1.png! 
Because at present, it is not certain that an operator can complete a checkpoint after receiving ""EndOfInput"". Therefore, in Streaming mode, I will tell the test sink not to output the string ""end of input"" in endOfInput.

In the future, this will be restored when Final-CP becomes a certainty.
;;;","09/Jul/21 04:33;maguowei;I thought about it again. In fact, we only need to exclude the ""end of input"" contained in GLOBAL_COMMIT_QUEUE. Such modification will be simpler than modifying TestSink. After the Final Checkpoint is over in the future, it will be very convenient to resume the detection of ""end of input"".;;;","12/Jul/21 09:03;maguowei;Fix in release-1.13: 92398d42498e61554566ab2dcfd8a5fa8f2a64a2
Fix in master: 2268baf211f1b367e56c8f8d7cd8ee8dee355cab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OperatorEventSendingCheckpointITCase.testOperatorEventLostWithReaderFailure fails on azure,FLINK-23233,13387617,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gaoyunhaii,xtsong,xtsong,05/Jul/21 03:20,23/Sep/21 18:02,13/Jul/23 08:12,13/Jul/21 12:34,1.12.3,1.13.1,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,Runtime / Checkpointing,,,,,1,pull-request-available,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19857&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107&l=9382

{code}
Jul 03 01:37:31 [ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 21.415 s <<< FAILURE! - in org.apache.flink.runtime.operators.coordination.OperatorEventSendingCheckpointITCase
Jul 03 01:37:31 [ERROR] testOperatorEventLostWithReaderFailure(org.apache.flink.runtime.operators.coordination.OperatorEventSendingCheckpointITCase)  Time elapsed: 3.623 s  <<< FAILURE!
Jul 03 01:37:31 java.lang.AssertionError: expected:<[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]> but was:<[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]>
Jul 03 01:37:31 	at org.junit.Assert.fail(Assert.java:88)
Jul 03 01:37:31 	at org.junit.Assert.failNotEquals(Assert.java:834)
Jul 03 01:37:31 	at org.junit.Assert.assertEquals(Assert.java:118)
Jul 03 01:37:31 	at org.junit.Assert.assertEquals(Assert.java:144)
Jul 03 01:37:31 	at org.apache.flink.runtime.operators.coordination.OperatorEventSendingCheckpointITCase.runTest(OperatorEventSendingCheckpointITCase.java:254)
Jul 03 01:37:31 	at org.apache.flink.runtime.operators.coordination.OperatorEventSendingCheckpointITCase.testOperatorEventLostWithReaderFailure(OperatorEventSendingCheckpointITCase.java:143)
Jul 03 01:37:31 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 03 01:37:31 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 03 01:37:31 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 03 01:37:31 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 03 01:37:31 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Jul 03 01:37:31 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jul 03 01:37:31 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Jul 03 01:37:31 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jul 03 01:37:31 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jul 03 01:37:31 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
Jul 03 01:37:31 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Jul 03 01:37:31 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
Jul 03 01:37:31 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
Jul 03 01:37:31 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
{code}",,gaoyunhaii,kevin.cyj,liyu,medb,pnowojski,sewen,trohrmann,xtsong,ym,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23046,,,,,,,FLINK-21996,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 13 12:34:25 UTC 2021,,,,,,,,,,"0|z0slmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/21 03:34;xtsong;cc [~gaoyunhaii]
Could you take a look? Most importantly, is this a correctness issue that should block release 1.13.2?;;;","05/Jul/21 03:35;gaoyunhaii;Ok, I'll have a look~;;;","06/Jul/21 17:10;gaoyunhaii;It seems the issue is caused by this part:
{code:java}
01:37:29,362 [           Thread-49] INFO  org.apache.flink.streaming.api.operators.collect.CollectSinkFunction [] - Invalid request. Received version = 8a0b400e-0aa9-4223-89dc-f7638a67a845, offset = 0, while expected version = 99a5868b-4923-46ec-ad60-58443bdba519, offset = 0
01:37:29,764 [    Checkpoint Timer] WARN  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Failed to trigger checkpoint 5 for job a4214b0065b42350a8831b7ce4a32ad2. (1 consecutive failed attempts so far)
org.apache.flink.util.FlinkException: Failing OperatorCoordinator checkpoint because some OperatorEvents before this checkpoint barrier were not received by the target tasks.
    at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.lambda$completeCheckpointOnceEventsAreDone$4(OperatorCoordinatorHolder.java:344) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:?]
    at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_282]
    at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_282]
    at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_282]
    at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_282]
    at org.apache.flink.runtime.concurrent.FutureUtils$WaitingConjunctFuture.handleCompletedFuture(FutureUtils.java:905) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_282]
    at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_282]
    at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_282]
    at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_282]
    at org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$23(FutureUtils.java:1356) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_282]
    at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_282]
    at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_282]
    at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_282]
    at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1255) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    at org.apache.flink.runtime.concurrent.DirectExecutorService.execute(DirectExecutorService.java:217) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    at org.apache.flink.runtime.concurrent.FutureUtils.lambda$orTimeout$15(FutureUtils.java:582) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_282]
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_282]
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_282]
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_282]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_282]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_282]
    at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
01:37:29,766 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 6 (type=CHECKPOINT) @ 1625276249765 for job a4214b0065b42350a8831b7ce4a32ad2.
01:37:29,768 [jobmanager-future-thread-1] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 6 for job a4214b0065b42350a8831b7ce4a32ad2 (942 bytes in 3 ms).
01:37:29,768 [SourceCoordinator-Source: numbers -> Map -> Sink: Data stream collect sink] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 6 as completed for source Source: numbers -> Map -> Sink: Data stream collect sink.
01:37:29,769 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 7 (type=CHECKPOINT) @ 1625276249768 for job a4214b0065b42350a8831b7ce4a32ad2.
01:37:29,770 [jobmanager-future-thread-2] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 7 for job a4214b0065b42350a8831b7ce4a32ad2 (942 bytes in 2 ms).
01:37:29,770 [SourceCoordinator-Source: numbers -> Map -> Sink: Data stream collect sink] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 7 as completed for source Source: numbers -> Map -> Sink: Data stream collect sink.
01:37:29,771 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 8 (type=CHECKPOINT) @ 1625276249770 for job a4214b0065b42350a8831b7ce4a32ad2.
01:37:29,772 [jobmanager-future-thread-1] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 8 for job a4214b0065b42350a8831b7ce4a32ad2 (942 bytes in 2 ms). 
01:37:29,773 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: numbers -> Map -> Sink: Data stream collect sink (1/1) (f37c51a50966c297173f7852774e54f2) switched from RUNNING to FAILED on 0057f3d7-309e-4ba2-8b12-99d2269c5aa7 @ localhost (dataPort=-1).
org.apache.flink.util.FlinkException: An OperatorEvent from an OperatorCoordinator to a task was lost. Triggering task failover to ensure consistency. Event: 'AddSplitEvents[[[B@6c71f390]]', targetTask: Source: numbers -> Map -> Sink: Data stream collect sink (1/1) - execution #1
    at org.apache.flink.runtime.operators.coordination.SubtaskGatewayImpl.lambda$sendEvent$0(SubtaskGatewayImpl.java:81) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:?]
    at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_282]
    at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_282]
    at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) ~[?:1.8.0_282]
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]

{code}
Currently for the OperatorCoordinator, each event send action is bound with a future, and two actions are relying on the result of the future:
 # When taking checkpoints, it would check if all the previous events are sent successfully, if not, it would  fail the checkpoint. But during this process, it would cleanup the pending previous events.
 # For each event, if the sending failed, it would trigger a failover for the subtask.

The in this case:
 # The job has only one task (source -> map -> sink (1/1)), the source has 3 splits, namely 1 - 34, 35 - 67, 68 - 100.
 # With some prior execution, AddSplit[1-34] is emitted and processed. Then now only two splits remains.
 # Then as the above log shows, the next AddSplit[68-100] is emitted before checkpoint 5, and the test deliberately make the event sending failed due to timeout.
 # The stage related to checkpoint executed first, abort the checkpoint 5. In this process it removes the pending event, namely now only 1 split remains.
 # Now new checkpoints are triggered. Since now there is no failed pending events, the checkpoint would complete, and only the remaining split 35-67 is snapshotted.
 # Then after some time the stage related to subtask failover executed, trigger the failover.
 # However, after the failover, it would recover from the checkpoint 8, which contains only the split 35-67. Then AddSplit[35-67] is emitted and executed. Thus the split 68-100 is missed.

The failure could be reproduced locally by simulates the above case as in [https://github.com/gaoyunhaii/flink/commits/fix_oc_cp] .

 ;;;","07/Jul/21 07:07;yunta;[~sewen], as the original developer of this feature, would you please also take a look at this problem? Currently we think this might be a blocker for releasing 1.13.2, what do you think?;;;","07/Jul/21 07:39;gaoyunhaii;To fix this issue, perhaps we should change the mechanism of the tracking incomplete event sending futures in OperatorCoordinatorHolder:

# All the future would be tracked before actually send.
# If one future complete successfully, remove it from the pending set.
# When doing checkpoints, only check the pending futures but do not cleanup.
# After we have failed subtask for one future, we remove it from the set.

Perhaps it might be something like [https://github.com/gaoyunhaii/flink/commit/c52ddd8c345f9a25dbb558db07499b8a3d29c4ec] ? ;;;","07/Jul/21 09:14;trohrmann;I think this a very serious bug which warrants making it a release blocker. The effect is that you might lose input splits w/o noticing if I understood the analysis correctly. This is equal to losing data.

I think we need to fix this immediately.;;;","07/Jul/21 09:16;trohrmann;Are we sure that this only affects 1.14 and 1.13? I think FLINK-21996 was also merged to 1.12. Hence, I would assume that 1.12 is also affected.;;;","07/Jul/21 09:33;xtsong;[~gaoyunhaii], how long do you expect it takes to fix this problem? If not too long, I'm leaning towards canceling the 1.13.2/1.12.5 RCs and fix this immediately.;;;","07/Jul/21 11:28;gaoyunhaii;Yes, this should cause losing data, and I check it indeed also exists in 1.12.x.

I think I would be able to open the PR by tomorrow~;;;","09/Jul/21 03:31;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20195&view=logs&j=219e462f-e75e-506c-3671-5017d866ccf6&t=4c5dc768-5c82-5ab0-660d-086cb90b76a0&l=5134;;;","09/Jul/21 16:41;sewen;Thanks, [~gaoyunhaii] for debugging this. Your analysis of the cause is correct.

I commented on the Pull Request regarding the suggested solution.

Regarding the Bugfix Release criticality: This condition here can only happen in conjunction with actual RPC loss, plus a very fast checkpoint interval. So this bug should be super rare. The original RPC loss fix was already fixing a rare issue, this here should be even rarer.
Of course we should fix it asap, but I would expect that this will be hard to observe in practice, outside tests with very specific setups. So if this fix takes a bit, we may not want to block other more critical fixes on this.;;;","13/Jul/21 12:34;trohrmann;Fixed via

1.14.0: c874338c6b19d8939f21a4781661759bea5d8449
1.13.2: 1fa52e1a93d025bc8482987ddac04c94142fc6cc
1.12.5: 2c3f8f64f653a3ab604148698b5ae9cb3e343887;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink tox check fails on azure,FLINK-23232,13387615,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,xtsong,xtsong,05/Jul/21 02:36,17/Aug/21 05:47,13/Jul/23 08:12,17/Aug/21 05:20,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,API / Python,,,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19855&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3&l=23069

{code}
Jul 03 00:07:56 ============tox checks... [FAILED]============
Jul 03 00:07:56 Process exited with EXIT CODE: 1.
Jul 03 00:07:56 Trying to KILL watchdog (3140).
/__w/2/s/tools/ci/watchdog.sh: line 100:  3140 Terminated              watchdog
Jul 03 00:07:56 Searching for .dump, .dumpstream and related files in '/__w/2/s'
The STDIO streams did not close within 10 seconds of the exit event from process '/bin/bash'. This may indicate a child process inherited the STDIO streams and has not yet exited.
{code}",,dian.fu,dianfu,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 17 05:47:58 UTC 2021,,,,,,,,,,"0|z0slmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/21 02:38;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19855&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=23341

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19855&view=logs&j=bdd9ea51-4de2-506a-d4d9-f3930e4d2355&t=98717c4f-b888-5636-bb1e-db7aca25755e&l=22591

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19855&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=f5211ead-5e53-5af8-f827-4dbf08df26bb&l=23012

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19855&view=logs&j=ff2e2ea5-07e3-5521-7b04-a4fc3ad765e9&t=613f538c-bcef-59e6-f9cd-9714bec9fb97&l=23947

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19855&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=d59eb898-29f7-5a99-91a7-b2dfc3e8a653&l=23370

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19855&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=45a89cfc-9ff2-5909-6443-6c732efcf06b&l=23071;;;","05/Jul/21 02:39;xtsong;cc [~dian.fu] [~hxbks2ks];;;","05/Jul/21 03:07;dian.fu;[~xintongsong] Thanks, I will take a look ~.;;;","05/Jul/21 03:40;dian.fu;Fixed in master via 284f4842f997146adf6fbd3e9981ca5074f74c77;;;","05/Jul/21 03:47;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19865&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3&l=23382

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19865&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=23134

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19865&view=logs&j=bdd9ea51-4de2-506a-d4d9-f3930e4d2355&t=98717c4f-b888-5636-bb1e-db7aca25755e&l=22924

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19865&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=f5211ead-5e53-5af8-f827-4dbf08df26bb&l=23319

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19865&view=logs&j=ff2e2ea5-07e3-5521-7b04-a4fc3ad765e9&t=613f538c-bcef-59e6-f9cd-9714bec9fb97&l=23948;;;","17/Aug/21 04:56;xtsong;Failing again on master
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22329&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=24199;;;","17/Aug/21 05:04;dianfu;[~xtsong] Not sure why you think it's relates to this JIRA? Per my understanding, the new failed instance is the same as FLINK-23790 and should already be fixed few hours ago. ;;;","17/Aug/21 05:06;dianfu;I guess you judge it according to ""============tox checks... [FAILED]============"". If so, actually all the failed Python tests will print that line and we should find out the real failed test case.;;;","17/Aug/21 05:11;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22329&view=logs&j=bdd9ea51-4de2-506a-d4d9-f3930e4d2355&t=dd50312f-73b5-56b5-c172-4d81d03e2ef1&l=24039;;;","17/Aug/21 05:19;xtsong;[~dian.fu],
I guess you're right. Sorry for the false alarm.;;;","17/Aug/21 05:47;dianfu;No worries and thanks a lot for the work! (y);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The JM's pod was failed when the high-availability.storageDir which using the S3,FLINK-23231,13387613,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,levi-015,levi-015,05/Jul/21 02:26,09/Jul/21 00:53,13/Jul/23 08:12,07/Jul/21 08:20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"Hi Team,

Recently, I'm using S3 as Flink 1.13.1's HA storage directory as below. But I meet an error when deploying the JM and TM to k8s.  the steps as below. Could you please have a check?  Thank you!

 1. Fink-conf.yaml 
 blob.server.port: 6124
 high-availability: zookeeper
 high-availability.zookeeper.quorum: zetcd:2181
 high-availability.zookeeper.path.root: /flink
 high-availability.cluster-id: /flink
 *high-availability.storageDir: s3://dsw-dia-test/recovery*
 high-availability.jobmanager.port: 50010
 high-availability.zookeeper.client.acl: open
 state.backend: filesystem
 state.checkpoints.dir: s3://dsw-dia-test/checkpoints
 state.backend.fs.checkpointdir: s3://dsw-dia-test/checkpoints
 s3.path.style.access:true
 fs.allowed-fallback-filesystems: s3
 s3.endpoint: s3.us-south.cloud-object-storage.appdomain.cloud
 s3.access-key: ***************
 s3.secret-key:*************
  
 I'm already building the new image for Flink 1.13 where it's used for the JM and TM's deployments. 
  
 2. The Docker file is 
  
 FROM flink:1.13.1-scala_2.11

#Add the hadoop jar file to folder /opt/flink/lib/
 WORKDIR $FLINK_HOME
 COPY flink-hadoop-compatibility_2.12-1.13.1.jar flink-shaded-hadoop-3-uber-3.1.1.7.1.1.0-565-9.0.jar $FLINK_HOME/lib/
 RUN chown -R flink:flink .;

#Entropy injection for S3 file systems
 RUN mkdir $FLINK_HOME/plugins/s3-fs-hadoop \
 &&mkdir$FLINK_HOME/plugins/s3-fs-presto

COPY flink-s3-fs-hadoop-1.13.1.jar $FLINK_HOME/plugins/s3-fs-hadoop
 COPY flink-s3-fs-presto-1.13.1.jar $FLINK_HOME/plugins/s3-fs-presto
 RUN chown -R flink:flink .;
  
 3. Have checked the jars inside TM pod 
  
 the plugins for file system
 drwxr-xr-x 1 flink flink 4096 Jul 4 11:35 s3-fs-hadoop
 drwxr-xr-x 1 flink flink 4096 Jul 4 11:35 s3-fs-presto

flink@63f9c0076cfc:~/plugins/s3-fs-hadoop$ ls -lrt
 total 19796
 -rw-r--r-- 1 flink flink 20269950 Jul 2 02:25 flink-s3-fs-hadoop-1.13.1.jar

flink@63f9c0076cfc:~/plugins/s3-fs-presto$ ls -lrt
 total 32692
 -rw-r--r-- 1 flink flink 33474159 May 25 12:20 flink-s3-fs-presto-1.13.1.jar

 
 4. The error for JM as attachments JM_log.txt.

5.  pod Status

kubectl get pods
 NAME READY STATUS RESTARTS AGE
 flink-jobmanager-9f9664f87-4dwg5 0/1 CrashLoopBackOff 6 8m43s
 flink-taskmanager-d7f8b4f87-swl2q 0/1 CrashLoopBackOff 6 8m33s
 flink-taskmanager-d7f8b4f87-vs52c 0/1 Error 3 21m
 zetcd-6458cd87f8-67rxn 2/2 Running 0 3d17h
  
 If you need any more information, please let me know. 
 Thanks In Advance!
  ","[^JM_log.txt]",levi-015,wangyang0918,yuwang0917@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/21 02:23;levi-015;JM_log.txt;https://issues.apache.org/jira/secure/attachment/13028966/JM_log.txt","05/Jul/21 02:32;levi-015;bucket_info.png;https://issues.apache.org/jira/secure/attachment/13028967/bucket_info.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 09 00:53:47 UTC 2021,,,,,,,,,,"0|z0slm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/21 02:39;levi-015;BTW, I'm have checked the bucket did not have any file under recovery folder. 
s3://dsw-dia-test/recovery !bucket_info.png!
 ;;;","07/Jul/21 04:48;wangyang0918;Could you please also verify that the s3 plugins are located in the JobManager pod? I am not aware of any bugs about Flink FileSystem plugins mechanism. Moreover, the full JobManager logs with debug level will help a lot for debugging the root cause.

 

In the future, I would like to suggest you to ask the similar questions in the ML so that they could be replied timely.;;;","07/Jul/21 08:20;levi-015;Hi [~fly_in_gis]

I'm have confirmed that it's has s3 plugins. It's resolved after this parameter  'imagePullPolicy:Always' was added in JM yaml file.

root@flink-jobmanager-7859cfd9b7-pzt6z:/opt/flink/plugins# ls -lrt
total 44
-rwxr-xr-x 1 flink flink 654 Jan 29 16:03 README.txt
drwxrwxr-x 1 flink flink 4096 May 25 12:17 metrics-statsd
drwxrwxr-x 1 flink flink 4096 May 25 12:17 metrics-slf4j
drwxrwxr-x 1 flink flink 4096 May 25 12:17 metrics-prometheus
drwxrwxr-x 1 flink flink 4096 May 25 12:17 metrics-jmx
drwxrwxr-x 1 flink flink 4096 May 25 12:17 metrics-influx
drwxrwxr-x 1 flink flink 4096 May 25 12:17 metrics-graphite
drwxrwxr-x 1 flink flink 4096 May 25 12:17 metrics-datadog
drwxrwxr-x 1 flink flink 4096 May 25 12:17 external-resource-gpu
drwxr-xr-x 1 flink flink 4096 Jul 5 09:56 s3-fs-hadoop
drwxr-xr-x 1 flink flink 4096 Jul 5 09:56 s3-fs-presto

 

root@flink-jobmanager-7859cfd9b7-pzt6z:/opt/flink/plugins/s3-fs-hadoop# ls -lrt
total 19796
-rw-r--r-- 1 flink flink 20269950 Jul 2 02:25 flink-s3-fs-hadoop-1.13.1.jar

root@flink-jobmanager-7859cfd9b7-pzt6z:/opt/flink/plugins/s3-fs-presto# ls -lrt
total 32692
-rw-r--r-- 1 flink flink 33474159 May 25 12:20 flink-s3-fs-presto-1.13.1.jar

 

BTW, How can i access to ML as you mentioned? Could you please provide the link to us?  Thank you so much!

 ;;;","08/Jul/21 03:21;wangyang0918;[~levi-015] You could find the mail list here[1] and then subscribe.

[1]. https://flink.apache.org/community.html#mailing-lists;;;","09/Jul/21 00:53;levi-015;Thank you [~fly_in_gis];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot compile Flink on MacOS with M1 chip,FLINK-23230,13387573,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,rmetzger,osaman88,osaman88,04/Jul/21 14:46,03/Jan/22 15:22,13/Jul/23 08:12,03/Jan/22 15:22,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Build System,,,,,0,pull-request-available,,,,"Flink doesn't currently compile on MacOS with M1 silicon.

This is true for all recent versions (1.13.X) as well as master.

Some of the problems have potentially easy fixes, such as installing node separately or updating the relevant pom.xml to use a newer version of node. I am getting some errors about deprecated features being used which are not supported by newer node, but on the surface they seem easy to resolve. 

I've had less success with complex dependencies such as protobuf.

My long term objective is to use and contribute to Flink. If I can get some help with the above issues, I am willing to make the modifications, submit the changes as a pull request, and shepherd them to release. If compilation on MacOS/M1 is not a priority, I can look for a virtual machine solution instead. Feedback appreciated. 

 

Thanks

 

Osama",,ana4,ganeshraju,JasonLee,martijnvisser,maver1ck,osaman88,Paul Lin,qinjunjerry,rmetzger,trohrmann,trushev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25188,,,,FLINK-25505,,,,,,,,,,,FLINK-23110,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 03 12:41:54 UTC 2022,,,,,,,,,,"0|z0sld4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/21 06:06;airblader;Updating node isn't as easy of a change either. The node version is tied to the Angular version and with it a lot of dependencies in turn. We have an umbrella issue for modernizing Flink UI which we should be able to address around Mid-July though which would take care of that.;;;","05/Jul/21 14:18;trohrmann;Thanks for reporting the issue [~osaman88]. Can you attach some more details about what exactly is not working and how it is failing. Could you try disabling building the web-ui via {{-Dskip-webui-build}} when running {{mvn}}?;;;","05/Jul/21 19:05;osaman88;Thank you both for quick responses.

 

So far I've run into the following errors.

1)

runtime-web stage fails due to absence of
[{color:#000000}https://nodejs.org/dist/v10.9.0/node-v10.9.0-darwin-arm64.tar.gz{color}]
Node version 16.0.0 and later have such a file
but this causes potential harmful side-effects when upgraded, such as:
{color:#000000}[{color}*{color:#ca3323}ERROR{color}*] (node:78328) [DEP0111] DeprecationWarning: Access to process.binding('http_parser') is deprecated.
 
2)
Stage{color:#000000}Flink : Formats : Parquet{color}
{color:#000000}fails due to absence of{color}
[{color:#000000}https://repo.maven.apache.org/maven2/com/google/protobuf/protoc/3.5.1/protoc-3.5.1-osx-aarch_64.exe{color}]
In version {color:#000000}3.17.3, osx-aarch_64 exists, but attempting to upgrade protobuf became messy very quickly. 
{color}
 
A few other stages fail when I force compilation maven to never fail, but I haven't dug into whether they are related or not. I suspect attempting to modify protobuf versions caused domino failures. These are the other stages that fail:
 
{color:#000000}[{color}*{color:#5620f4}INFO{color}*] Flink : Connectors : Hive .......................... *{color:#ca3323}FAILURE{color}* [ 36.684 s]
 
{color:#000000}[{color}*{color:#5620f4}INFO{color}*] Flink : Formats : SQL Parquet ...................... *{color:#ca3323}FAILURE{color}* [  0.054 s]
 
{color:#000000}[{color}*{color:#5620f4}INFO{color}*] Flink : Python ..................................... *{color:#ca3323}FAILURE{color}* [ 20.272 s]
 
{color:#000000}[{color}*{color:#5620f4}INFO{color}*] Flink : Dist ....................................... *{color:#ca3323}FAILURE{color}* [ 15.850 s]
 
 
I wasn't sure if any of this is of interest or not, hence I filed the issue. If of interest, I can attempt to narrow down the failures and provide more details on signature and causes.
 
Thanks
 
Osama;;;","07/Jul/21 08:59;trohrmann;I see. Some of the dependencies (NodeJS and Protobuf) don't have support for ARM in the versions we are using. I think the solution would be to upgrade these dependencies. However, as Ingo said, for the web UI this might not be super trivial. For the web UI update, we have FLINK-23110.

For upgrading Protobuf, I've created a sub-task here.;;;","09/Jul/21 00:57;osaman88;Thank you [~trohrmann].

Let me provide update on what I did, and I'll let you decide what to do with this issue as I'm able to make forward progress. I didn't want to block on these. 

For node, I installed the expected version 10.9.0 manually on my machine. Performance with Rosetta 2 (MacOS's x86/aarch64 emulation) is very good. So that's good enough for now.

For protobuf within format/Parquet, I installed an x86 version in my local maven repository, registered it with Maven, and again Rosetta 2 is able to run it with no issues.

The final issue, fyi,  was in flink-python. This uses protoc-jar, which only supports very limited versions of protoc. I installed protoc x86 locally, modified flink-python/pom.xml to include
<properties>
<protocCommand>protoc</protocCommand>
</properties>
and now it is using my locally installed protoc and working fine.

With all this, I was able to compile Flink all the way, run an example locally, and verify that the web dashboard is also working well.

Thanks

Osama;;;","09/Jul/21 10:40;osaman88;BTW the protoc-jar team is aware of the issue with Apple M1 chips, and they have an issue that tracks a possible solution. There is a PR pending that hasn't been accepted yet:

[https://github.com/os72/protoc-jar/issues/93]

Thanks

Osama;;;","09/Jul/21 13:18;trohrmann;Great. Thanks for updating this ticket [~osaman88].;;;","06/Sep/21 13:49;martijnvisser;[~osaman88] Could you test master again to see if it now works? ;;;","15/Nov/21 11:39;maver1ck;For the web UI upgrade of frontend-maven-plugin to 1.12.0 fixed issue.

I found also failing test :)
{code:java}
[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Failures: 
[ERROR]   CPUResourceTest.toHumanReadableString:32 
Expected: is ""0.00 cores""
     but: was ""0,00 cores""
[INFO] 
[ERROR] Tests run: 4615, Failures: 1, Errors: 0, Skipped: 657 {code}
[~osaman88]  did you run test after compilation ?;;;","15/Nov/21 11:49;martijnvisser; [~maver1ck] Can you make a PR for that plugin update?;;;","15/Nov/21 12:03;maver1ck;Done.
[https://github.com/apache/flink/pull/17795]

EDIT: Plugin version 1.11.0 support both M1 and maven 3.2.5;;;","19/Nov/21 09:29;maver1ck;[~MartijnVisser] 

Current status with current master and my PR is like this:

 
{code:java}
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  06:32 min
[INFO] Finished at: 2021-11-19T09:31:23+01:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.xolstice.maven.plugins:protobuf-maven-plugin:0.5.1:test-compile (default) on project flink-parquet_2.12: Missing:
[ERROR] ----------
[ERROR] 1) com.google.protobuf:protoc:exe:osx-aarch_64:3.5.1
[ERROR] 
[ERROR]   Try downloading the file manually from the project website.
[ERROR] 
[ERROR]   Then, install it using the command: 
[ERROR]       mvn install:install-file -DgroupId=com.google.protobuf -DartifactId=protoc -Dversion=3.5.1 -Dclassifier=osx-aarch_64 -Dpackaging=exe -Dfile=/path/to/file
[ERROR] 
[ERROR]   Alternatively, if you host your own repository you can deploy the file there: 
[ERROR]       mvn deploy:deploy-file -DgroupId=com.google.protobuf -DartifactId=protoc -Dversion=3.5.1 -Dclassifier=osx-aarch_64 -Dpackaging=exe -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]
[ERROR] 
[ERROR]   Path to dependency: 
[ERROR]   	1) org.apache.flink:flink-parquet_2.12:jar:1.15-SNAPSHOT
[ERROR]   	2) com.google.protobuf:protoc:exe:osx-aarch_64:3.5.1
[ERROR] 
[ERROR] ----------
[ERROR] 1 required artifact is missing.
[ERROR] 
[ERROR] for artifact: 
[ERROR]   org.apache.flink:flink-parquet_2.12:jar:1.15-SNAPSHOT
[ERROR] 
[ERROR] from the specified remote repositories:
[ERROR]   apache.snapshots (https://repository.apache.org/snapshots, releases=false, snapshots=true),
[ERROR]   central (https://repo.maven.apache.org/maven2, releases=true, snapshots=false)
[ERROR]  {code}
What can we do next ?

 

PS. Is there a chance somebody merge my PR ?
I added subtask for that: https://issues.apache.org/jira/browse/FLINK-24964
[https://github.com/apache/flink/pull/17795];;;","27/Nov/21 12:59;rmetzger;I've managed to get Flink compiling on my M1 MBP, with a few small pom changes to some protoc-related stuff.
I'm not sure if the changes are acceptable, but I'll open a PR soon to discuss.

mvn clean install time was ~11 minutes (2 minutes for the frontend), which is quite fast compared to the 19 minutes I needed on my 8 Core Intel i9 MBP from 2019.;;;","29/Nov/21 15:05;martijnvisser;[~rmetzger] Did you use a M1 optimized JDK? https://alhuelamo.com/posts/2021/11/native-apple-silicon-jdks/;;;","30/Nov/21 19:44;rmetzger;Yes, I used zulu jdk8 for apple silicon. I was also hoping for a little bit more wonders too. The real wonder is probably that I could not hear any fans spinning (which is not true for the intel one at all), and that the compile time on battery is probably also 11 minutes ;) ;;;","06/Dec/21 09:37;rmetzger;Another problem I'm facing is this:

{code}
[INFO] --- frontend-maven-plugin:1.11.0:npm (npm install) @ flink-runtime-web ---
[INFO] Running 'npm ci --cache-max=0 --no-save' in /Users/robert/Projects/flink/flink-runtime-web/web-dashboard
[INFO] npm WARN prepare removing existing node_modules/ before installation
[INFO] npm ERR! code ENOTEMPTY
[INFO] npm ERR! syscall rmdir
[INFO] npm ERR! path /Users/robert/Projects/flink/flink-runtime-web/web-dashboard/node_modules/@antv
[INFO] npm ERR! errno -66
[INFO] npm ERR! ENOTEMPTY: directory not empty, rmdir '/Users/robert/Projects/flink/flink-runtime-web/web-dashboard/node_modules/@antv'
[INFO]
[INFO] npm ERR! A complete log of this run can be found in:
[INFO] npm ERR!     /Users/robert/.npm/_logs/2021-12-06T09_23_29_806Z-debug.log
{code}

[~maver1ck] are you seeing this too (looks like you can't run a clean build twice w/o deleting the node_modules)?
;;;","16/Dec/21 08:11;maver1ck;[~rmetzger] 
I was able to do 2 consecutive builds (-DskipTests) without errors on master with only one manual step (mvn install:install-file of protoc x86 executable);;;","16/Dec/21 08:17;rmetzger;Allright, thanks for checking for me. Then maybe its a local issue. I'll look into it.;;;","16/Dec/21 08:21;maver1ck;Sorry for delay but I get my own M1 Macbook yesterday. 
Now I'm checking compilation WITH tests.

Result:
{code:java}
[INFO] Results:
[INFO]
[ERROR] Errors:
[ERROR]   NetworkBufferPoolTest.testIsAvailableOrNotAfterRequestAndRecycleMultiSegments » TestTimedOut
[ERROR]   SystemResourcesCounterTest.testObtainAnyMetrics:35 » UnsatisfiedLink /private/...
[INFO]
[ERROR] Tests run: 5922, Failures: 0, Errors: 2, Skipped: 30{code}
 ;;;","03/Jan/22 12:41;rmetzger;Merged to master in https://github.com/apache/flink/commit/e548f21f241717b6ac60ae5e311a1b377b32c9a8.

I'll file separate tickets about making the tests pass on M1.

EDIT: FLINK-25505;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
There is some mismatches between the current translation and the latest English version on the Flink-Architecture page,FLINK-23225,13387497,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,hapihu,hapihu,hapihu,03/Jul/21 15:55,23/Sep/21 17:56,13/Jul/23 08:12,06/Jul/21 04:03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Documentation,,,,,0,pull-request-available,,,,"There is some mismatches between the current translation and the latest English version on the Flink-Architecture page

 

 

The original English text is as follows([https://ci.apache.org/projects/flink/flink-docs-master/docs/concepts/flink-architecture/#flink-application-execution])：


 ​
{code:java}
A Flink Application is any user program that spawns one or multiple Flink
 jobs from its ``main()`` method. The execution of these jobs can happen in a
 local JVM (``LocalEnvironment``) or on a remote setup of clusters with multiple
 machines (``RemoteEnvironment``). For each program, the `ExecutionEnvironment`
 provides methods to control the job execution (e.g. setting the parallelism) and to interact with
 the outside world (see [Anatomy of a Flink Program](< ref ""docs/dev/datastream/overview"" >#anatomy-of-a-flink-program)).
 ​
 The jobs of a Flink Application can either be submitted to a long-running
 [Flink Session Cluster](< ref ""docs/concepts/glossary"" >#flink-session-cluster), a dedicated [Flink Job
 Cluster](< ref ""docs/concepts/glossary"" >#flink-job-cluster), or a
 [Flink Application Cluster](< ref ""docs/concepts/glossary"" >#flink-application-cluster). The difference between these options is mainly related to the cluster’s lifecycle and to resource
 isolation guarantees.
 
{code}
 

 


 *The Chinese translation is as follows*
{code:java}
 Flink 应用程序 是从其 ``main()`` 方法产生的一个或多个 Flink 作业的任何用户程序。这些作业的执行可以在本地 JVM（`LocalEnvironment``）中进行，或具有多台机器的集群的远程设置（``RemoteEnvironment``）中进行。对于每个程序，[``ExecutionEnvironment``]({{ site.javadocs_baseurl }}/api/java/) 提供了一些方法来控制作业执行（例如设置并行度）并与外界交互（请参考 [Flink 程序剖析](< ref ""docs/dev/datastream/overview"" >#anatomy-of-a-flink-program) ）。
 ​
 Flink 应用程序的作业可以被提交到长期运行的 [Flink Session 集群](< ref ""docs/concepts/glossary"" >#flink-session-cluster)、专用的 [Flink Job 集群](< ref ""docs/concepts/glossary"" >#flink-job-cluster) 或 [Flink Application 集群](< ref ""docs/concepts/glossary"" >#flink-application-cluster)。这些选项之间的差异主要与集群的生命周期和资源隔离保证有关。
{code}

  

The incorrect translation is as follows:
{code:java}
//1
 JVM（`LocalEnvironment``） should be   JVM（`LocalEnvironment`）
 ​
//2
 [``ExecutionEnvironment``]({{ site.javadocs_baseurl }}/api/java/) should be   `ExecutionEnvironment`

{code}",,hapihu,jark,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jul/21 15:55;hapihu;image-20210703235017401.png;https://issues.apache.org/jira/secure/attachment/13028952/image-20210703235017401.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 06 04:03:20 UTC 2021,,,,,,,,,,"0|z0skwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/21 07:38;hapihu;Hi [~jark]，

 

I created pull request for this translation .
Please check.
If there is any problem, please inform me in time. 
I active support the modification.
Thank you.;;;","06/Jul/21 04:03;jark;Fixed in master: bf7bd78f8a6065b8de1d94fdb97fc094cdf0ac3d
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
When flushAlways is enabled the subpartition may lose notification of data availability,FLINK-23223,13387271,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gaoyunhaii,gaoyunhaii,gaoyunhaii,02/Jul/21 11:45,23/Sep/21 18:01,13/Jul/23 08:12,12/Jul/21 09:55,1.11.3,1.12.5,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,Runtime / Network,,,,,0,pull-request-available,,,,"When the flushAways is enabled (namely set buffer timeout to 0), there might be cases like:
 # The subpartition emit an event which blocks the channel
 # The subpartition produce more records. However, this records would not be notified since isBlocked = true.
 # When the downstream tasks resume the subpartition later, the subpartition would only mark isBlocked to false. For local input channels although it tries to add the channel if isAvailable = true, but this check would not pass since flushRequest = false. 

One case for this issue is https://issues.apache.org/jira/browse/FLINK-22085 which uses LocalInputChannel.",,gaoyunhaii,kevin.cyj,pnowojski,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22085,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 12 09:55:38 UTC 2021,,,,,,,,,,"0|z0sjj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jul/21 09:37;pnowojski;Merged to master as 8319bf44b15

[~gaoyunhaii], to which versions should we back port this fix? I presume at the very least 1.12 and 1.13? ;;;","09/Jul/21 13:53;gaoyunhaii;Very thanks for the review! I'll back port the fix~;;;","12/Jul/21 09:55;pnowojski;merged commit 65ca515 into apache:release-1.13
merged commit 6292777 into apache:release-1.12;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The check on alignmentDurationNanos seems to be too strict,FLINK-23201,13386977,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,qinjunjerry,qinjunjerry,01/Jul/21 07:33,23/Sep/21 18:04,13/Jul/23 08:12,15/Jul/21 17:53,1.12.2,1.13.1,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,Runtime / Metrics,,,,,0,pull-request-available,,,,"The check on alignmentDurationNanos seems to be too strict at the line:
https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointMetrics.java#L74

This caused a job to fail when doing stop-with-savepoint. But doing savepoint only without stop does not seem to be impacted by this.",,dwysakowicz,pnowojski,qinjunjerry,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23302,,,,,,,,BEAM-10955,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 15 17:53:26 UTC 2021,,,,,,,,,,"0|z0shps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/21 17:53;dwysakowicz;Fixed in:
* master
** b35701a35c724668849c262e6f9b43e478969065..a6a9b192eac492884e7aafda6b42dcc1298d378d
* 1.13
** 7c6b0265e954da45386042bd2ead0102aabe2d1d..b6d89fe5d5ed6daefe75657e9c6bf75dfadb07bb
* 1.12
** 444ed318b83d8513c830b8b9ad3576b45e35d5a9..09c5d539521e36f0399c3660f2c676d53c494a73;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
An old interface method is used in this section of [Passing Options Factory to RocksDB].,FLINK-23198,13386943,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,yanchenyun,yanchenyun,yanchenyun,01/Jul/21 03:35,08/Oct/21 11:27,13/Jul/23 08:12,17/Aug/21 09:43,1.12.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Documentation,,,,,0,pull-request-available,,,,"[https://ci.apache.org/projects/flink/flink-docs-release-1.12/ops/state/state_backends.html]

 

!image-2021-07-01-11-30-57-676.png!

In version 1.12 of Flink, this method has been replaced by the following one：

!image-2021-07-01-11-32-25-200.png!

 ",,yanchenyun,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23211,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/21 03:31;yanchenyun;image-2021-07-01-11-30-57-676.png;https://issues.apache.org/jira/secure/attachment/13027485/image-2021-07-01-11-30-57-676.png","01/Jul/21 03:32;yanchenyun;image-2021-07-01-11-32-25-200.png;https://issues.apache.org/jira/secure/attachment/13027484/image-2021-07-01-11-32-25-200.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 17 09:43:21 UTC 2021,,,,,,,,,,"0|z0shi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/21 11:06;yunta;[~yanchenyun], I noticed that this documentation error existed both on flink-1.12, flink-1.13 and master branch. Would you please create two PRs, one is target for release-1.12 and one is target for master branch (I'll pick the latter change to release-1.13 branch)? If you're willing to do so, I could assign this ticket to you.;;;","02/Jul/21 01:26;yanchenyun;[~yunta], Thank you for your reply! I have create two PRs, another Jira link is https://issues.apache.org/jira/browse/FLINK-23211.;;;","02/Jul/21 03:01;yunta;[~yanchenyun] You don't need to create another ticket. In general, one ticket could follow one problem at different versions.
BTW, I did not see any PR related to this ticket. If you create the PR with correct ticket number, the correct link would be appended in this ticket.;;;","05/Jul/21 09:25;yanchenyun;[~yunta], i have create two PRs, one is target for release-1.12 and one is target for master branch. Two links have been appended in this ticket. This is my first chance to learn the whole contribution process. Can I ask you some questions about state tuning by send you email tangyun@apache.org? Thank you so much!;;;","05/Jul/21 10:01;yunta;[~yanchenyun], thanks for your update!
You could go to flink mailing list (user@flink.apache.org , for more info you could refer to [community info|https://flink.apache.org/community.html#mailing-lists]) to ask the questions about state tuning.;;;","05/Aug/21 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","17/Aug/21 09:43;yunta;merged
master: 0abc23de1f8ab521bf9ac4a21b6b1bce7da6fadc
release-1.13: 097f717b28cfdc2ba77ccffbd0b3da850eebdb1c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobMasterITCase fail on azure due to BindException,FLINK-23196,13386930,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,xtsong,xtsong,01/Jul/21 02:30,23/Sep/21 17:55,13/Jul/23 08:12,05/Jul/21 03:26,1.13.1,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19753&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4251

{code}
Jul 01 00:00:27 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 4.272 s <<< FAILURE! - in org.apache.flink.runtime.jobmaster.JobMasterITCase
Jul 01 00:00:27 [ERROR] testRejectionOfEmptyJobGraphs(org.apache.flink.runtime.jobmaster.JobMasterITCase)  Time elapsed: 3.009 s  <<< ERROR!
Jul 01 00:00:27 org.apache.flink.util.FlinkException: Could not create the DispatcherResourceManagerComponent.
Jul 01 00:00:27 	at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:275)
Jul 01 00:00:27 	at org.apache.flink.runtime.minicluster.MiniCluster.createDispatcherResourceManagerComponents(MiniCluster.java:470)
Jul 01 00:00:27 	at org.apache.flink.runtime.minicluster.MiniCluster.setupDispatcherResourceManagerComponents(MiniCluster.java:429)
Jul 01 00:00:27 	at org.apache.flink.runtime.minicluster.MiniCluster.start(MiniCluster.java:373)
Jul 01 00:00:27 	at org.apache.flink.runtime.jobmaster.JobMasterITCase.testRejectionOfEmptyJobGraphs(JobMasterITCase.java:56)
Jul 01 00:00:27 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 01 00:00:27 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 01 00:00:27 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 01 00:00:27 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 01 00:00:27 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jul 01 00:00:27 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jul 01 00:00:27 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jul 01 00:00:27 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jul 01 00:00:27 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jul 01 00:00:27 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Jul 01 00:00:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 01 00:00:27 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jul 01 00:00:27 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jul 01 00:00:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Jul 01 00:00:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Jul 01 00:00:27 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Jul 01 00:00:27 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Jul 01 00:00:27 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Jul 01 00:00:27 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Jul 01 00:00:27 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Jul 01 00:00:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 01 00:00:27 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Jul 01 00:00:27 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Jul 01 00:00:27 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Jul 01 00:00:27 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Jul 01 00:00:27 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Jul 01 00:00:27 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jul 01 00:00:27 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jul 01 00:00:27 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jul 01 00:00:27 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Jul 01 00:00:27 Caused by: java.net.BindException: Could not start rest endpoint on any port in port range 8081
Jul 01 00:00:27 	at org.apache.flink.runtime.rest.RestServerEndpoint.start(RestServerEndpoint.java:234)
Jul 01 00:00:27 	at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:172)
Jul 01 00:00:27 	... 34 more
{code}",,Thesharing,xtsong,Zakelly,zhangyy91,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 05 03:26:26 UTC 2021,,,,,,,,,,"0|z0shfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/21 02:31;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19753&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107&l=9800;;;","01/Jul/21 06:55;Thesharing;https://dev.azure.com/thesharing/Flink/_build/results?buildId=105&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=634cd701-c189-5dff-24cb-606ed884db87&l=3780;;;","01/Jul/21 07:57;zhangyy91;[~xintongsong] I met the same problem recently. The reason may be concurrent MiniCluster with the default port in different concurrent tests and the port conflict causes the error.

I explicitly set the rest and rpc port to 0 in JobMasterITCase and passed the tests as is in MiniClusterResource where both rest and RPC port are set to 0 to avoid clashes with concurrent MiniClusters.

There are already some tests where the port is set to 0 when using MiniCluster directly.

But there are other cases that use MiniCluster without setting the port to 0 and may cause this issue when running concurrently. Maybe we can find all these cases and correct them.

 

See:

MiniClusterResource

[https://github.com/apache/flink/blob/a40abc7f834888a5f42efeefa662ad6ad5d7c222/flink-runtime/src/test/java/org/apache/flink/runtime/testutils/MiniClusterResource.java#L185]

 My Fix:

https://github.com/zhangyy91/flink/blob/170d40507599618f471e46b3b2843fb83234100f/flink-tests/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterITCase.java#L53;;;","02/Jul/21 01:38;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19796&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4251;;;","02/Jul/21 01:39;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19796&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107&l=9800;;;","02/Jul/21 03:18;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19798&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4063;;;","02/Jul/21 03:19;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19798&view=logs&j=59c257d0-c525-593b-261d-e96a86f1926b&t=b93980e3-753f-5433-6a19-13747adae66a&l=9340;;;","02/Jul/21 03:19;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19798&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107&l=9340;;;","02/Jul/21 06:20;chesnay;master: d47a15930851ccfc53dfb3a173182e6235f25889;;;","05/Jul/21 02:58;xtsong;Reopen to port to 1.13.

Instances on 1.13:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19857&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4063

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19857&view=logs&j=59c257d0-c525-593b-261d-e96a86f1926b&t=b93980e3-753f-5433-6a19-13747adae66a&l=9340

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19857&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107&l=9340;;;","05/Jul/21 03:26;xtsong;1.13: 51c02e434629722ed622c72b9132f973ce51d3a8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unsupported function definition: IFNULL. Only user defined functions are supported as inline functions,FLINK-23188,13386629,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,airblader,xiaojin.wy,xiaojin.wy,30/Jun/21 02:26,23/Sep/21 18:03,13/Jul/23 08:12,15/Jul/21 10:13,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"CREATE TABLE database0_t0(
c0 FLOAT
) WITH (
  'connector' = 'filesystem',
  'path' = 'hdfs:///tmp/database0_t0.csv',
  'format' = 'csv'
);
INSERT OVERWRITE database0_t0(c0) VALUES(0.40445197);

SELECT database0_t0.c0 AS ref0 FROM database0_t0 WHERE ((IFNULL(database0_t0.c1, database0_t0.c1)) IS NULL);

The errors:
""<Exception on server side: org.apache.flink.table.api.TableException: Unsupported function definition: IFNULL. Only user defined functions are supported as inline functions.  at org.apache.flink.table.planner.functions.bridging.BridgingUtils.lambda$createInlineFunctionName$0(BridgingUtils.java:81)  at java.util.Optional.orElseThrow(Optional.java:290)  at org.apache.flink.table.planner.functions.bridging.BridgingUtils.createInlineFunctionName(BridgingUtils.java:78)  at org.apache.flink.table.planner.functions.bridging.BridgingUtils.createName(BridgingUtils.java:58)  at org.apache.flink.table.planner.functions.bridging.BridgingSqlFunction.<init>(BridgingSqlFunction.java:76)  at org.apache.flink.table.planner.functions.bridging.BridgingSqlFunction.of(BridgingSqlFunction.java:116)  at org.apache.flink.table.planner.expressions.converter.FunctionDefinitionConvertRule.convert(FunctionDefinitionConvertRule.java:65)  at org.apache.flink.table.planner.expressions.converter.ExpressionConverter.visit(ExpressionConverter.java:97)  at org.apache.flink.table.planner.expressions.converter.ExpressionConverter.visit(ExpressionConverter.java:71)  at org.apache.flink.table.expressions.CallExpression.accept(CallExpression.java:134)  at org.apache.flink.table.planner.expressions.converter.ExpressionConverter$1.toRexNode(ExpressionConverter.java:247)  at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)  at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374)  at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)  at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)  at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)  at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)  at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)  at org.apache.flink.table.planner.expressions.converter.ExpressionConverter.toRexNodes(ExpressionConverter.java:240)  at org.apache.flink.table.planner.expressions.converter.DirectConvertRule.lambda$convert$0(DirectConvertRule.java:220)  at java.util.Optional.map(Optional.java:215)  at org.apache.flink.table.planner.expressions.converter.DirectConvertRule.convert(DirectConvertRule.java:217)  at org.apache.flink.table.planner.expressions.converter.ExpressionConverter.visit(ExpressionConverter.java:97)  at org.apache.flink.table.planner.expressions.converter.ExpressionConverter.visit(ExpressionConverter.java:71)  at org.apache.flink.table.expressions.CallExpression.accept(CallExpression.java:134)  at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoSourceScanRuleBase.lambda$convertExpressionToRexNode$0(PushFilterIntoSourceScanRuleBase.java:73)  at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)  at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374)  at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)  at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)  at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)  at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)  at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)  at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoSourceScanRuleBase.convertExpressionToRexNode(PushFilterIntoSourceScanRuleBase.java:73)  at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoSourceScanRuleBase.resolveFiltersAndCreateTableSourceTable(PushFilterIntoSourceScanRuleBase.java:116)  at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.pushFilterIntoScan(PushFilterIntoTableSourceScanRule.java:95)  at org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoTableSourceScanRule.onMatch(PushFilterIntoTableSourceScanRule.java:70)  at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)  at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)  at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)  at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)  at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)  at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)  at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)  at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)  at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)  at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:63)  at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:60)  at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)  at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)  at scala.collection.Iterator$class.foreach(Iterator.scala:891)  at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)  at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)  at scala.collection.AbstractIterable.foreach(Iterable.scala:54)  at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)  at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)  at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:60)  at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:55)  at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)  at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)  at scala.collection.immutable.Range.foreach(Range.scala:160)  at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)  at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)  at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:55)  at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)  at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)  at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)  at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)  at scala.collection.Iterator$class.foreach(Iterator.scala:891)  at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)  at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)  at scala.collection.AbstractIterable.foreach(Iterable.scala:54)  at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)  at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)  at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)  at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:87)  at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58)  at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)  at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:46)  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)  at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:46)  at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:93)  at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:310)  at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:172)  at com.ververica.flink.table.gateway.operation.SelectOperation.lambda$executeQueryInternal$0(SelectOperation.java:183)  at com.ververica.flink.table.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:130)  at com.ververica.flink.table.gateway.operation.SelectOperation.executeQueryInternal(SelectOperation.java:182)  at com.ververica.flink.table.gateway.operation.SelectOperation.execute(SelectOperation.java:82)  at com.ververica.flink.table.gateway.operation.executor.OneByOneOperationExecutor.execute(OneByOneOperationExecutor.java:57)  at com.ververica.flink.table.gateway.rest.session.Session.lambda$runStatement$1(Session.java:115)  at com.ververica.flink.table.gateway.utils.EnvironmentUtil.lambda$wrapWithHadoopUsernameIfNeeded$0(EnvironmentUtil.java:57)  at com.ververica.flink.table.gateway.utils.EnvironmentUtil.wrapWithHadoopUsernameIfNeeded(EnvironmentUtil.java:65)  at com.ververica.flink.table.gateway.utils.EnvironmentUtil.wrapWithHadoopUsernameIfNeeded(EnvironmentUtil.java:56)  at com.ververica.flink.table.gateway.rest.session.Session.runStatement(Session.java:114)  at com.ververica.flink.table.gateway.rest.handler.StatementExecuteHandler.handleRequest(StatementExecuteHandler.java:83)  at com.ververica.flink.table.gateway.rest.handler.AbstractRestHandler.respondToRequest(AbstractRestHandler.java:85)  at com.ververica.flink.table.gateway.rest.handler.AbstractHandler.channelRead0(AbstractHandler.java:184)  at com.ververica.flink.table.gateway.rest.handler.AbstractHandler.channelRead0(AbstractHandler.java:76)  at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)  at org.apache.flink.runtime.rest.handler.router.RouterHandler.routed(RouterHandler.java:115)  at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:94)  at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:55)  at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)  at org.apache.flink.shaded.netty4.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)  at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:208)  at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:69)  at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)  at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)  at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)  at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296)  at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)  at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)  at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)  at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)  at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)  at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)  at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)  at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)  at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)  at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)  at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)  at java.lang.Thread.run(Thread.java:834) End of exception on server side>""",,airblader,jark,libenchao,twalthr,xiaojin.wy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 15 10:13:29 UTC 2021,,,,,,,,,,"0|z0sfko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/21 07:25;airblader;Thanks for reporting this issue. It seems this is related to the filter being pushed into the source. It'd probably be easy enough to treat specialized functions separately in createInlineFunctionName, but I'm not sure if this would be fixing a problem or treating a symptom. I'll dig into this.;;;","30/Jun/21 12:54;airblader;Reproduction test case for `CalcITCase`:
{code:java}
@Test
def test23188(): Unit = {
  val tableId = TestValuesTableFactory.registerData(Seq())

  tEnv.executeSql(s""""""
                     |CREATE TABLE T (
                     |  f0 INT
                     |) WITH (
                     |  'connector' = 'values',
                     |  'data-id' = '$tableId',
                     |  'bounded' = 'true'
                     |)
     """""".stripMargin)

  tEnv.sqlQuery(""SELECT f0 AS ref0 FROM T WHERE ((IFNULL(f0, f0)) IS NULL)"").execute()
    .collect().toList
}
{code};;;","30/Jun/21 13:34;airblader;The following patch fixes it, but will have to discuss this first.
{code:java}
diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/bridging/BridgingUtils.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/bridging/BridgingUtils.java
index 541c998e0c..798a6d970e 100644
--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/bridging/BridgingUtils.java
+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/bridging/BridgingUtils.java
@@ -22,6 +22,7 @@ import org.apache.flink.table.api.TableException;
 import org.apache.flink.table.catalog.DataTypeFactory;
 import org.apache.flink.table.catalog.ObjectIdentifier;
 import org.apache.flink.table.functions.AggregateFunctionDefinition;
+import org.apache.flink.table.functions.BuiltInFunctionDefinition;
 import org.apache.flink.table.functions.FunctionDefinition;
 import org.apache.flink.table.functions.FunctionIdentifier;
 import org.apache.flink.table.functions.ScalarFunctionDefinition;
@@ -70,6 +71,10 @@ final class BridgingUtils {
     }
 
     private static String createInlineFunctionName(FunctionDefinition functionDefinition) {
+        if (functionDefinition instanceof BuiltInFunctionDefinition) {
+            return ((BuiltInFunctionDefinition) functionDefinition).getName();
+        }
+
         final Optional<UserDefinedFunction> userDefinedFunction =
                 extractUserDefinedFunction(functionDefinition);
 
@@ -101,6 +106,7 @@ final class BridgingUtils {
                     ((TableAggregateFunctionDefinition) functionDefinition)
                             .getTableAggregateFunction());
         }
+
         return Optional.empty();
     }
 

{code};;;","06/Jul/21 11:26;airblader;It seems that the functionIdentifier should in fact be set, and isn't because due to the filter pushdown there's a conversion step from RexNode to Expression which loses the functionIdentifier when later on it gets converted back to RexNode. The following fix is probably more appropriate, will look into it more:
{code:java}
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeExtractor.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeExtractor.scala
index 3d8b8a4f6e..3401c1a1be 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeExtractor.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/RexNodeExtractor.scala
@@ -532,7 +532,8 @@ class RexNodeToExpressionConverter(
     Try(functionCatalog.lookupFunction(identifier)) match {
       case Success(f: java.util.Optional[FunctionLookup.Result]) =>
         if (f.isPresent) {
-          Some(new CallExpression(f.get().getFunctionDefinition, operands, outputType))
+          Some(new CallExpression(f.get().getFunctionIdentifier, f.get().getFunctionDefinition,
+            operands, outputType))
         } else {
           None
         }

{code};;;","14/Jul/21 15:07;twalthr;Fixed in 1.14.0: 7c64b245278704764254bb4287ec256f60704dfc

I'm currently investigating if we can safely merge the fix also to 1.13.;;;","15/Jul/21 10:13;twalthr;Fixed in 1.13.3: 1db1112fa0e5e5e51be00e2b0bdceab76d5ce3e3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CompileException Assignment conversion not possible from type ""int"" to type ""short""",FLINK-23184,13386516,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,xiaojin.wy,xiaojin.wy,29/Jun/21 12:06,23/Sep/21 17:59,13/Jul/23 08:12,12/Jul/21 08:27,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"{code:sql}
CREATE TABLE MySink (
  `a` SMALLINT
) WITH (
  'connector' = 'filesystem',
  'format' = 'testcsv',
  'path' = '$resultPath'
)

CREATE TABLE database8_t0 (
  `c0` SMALLINT
) WITH (
  'connector' = 'filesystem',
  'format' = 'testcsv',
  'path' = '$resultPath11'
)

CREATE TABLE database8_t1 (
  `c0` SMALLINT,
  `c1` TINYINT
) WITH (
  'connector' = 'filesystem',
  'format' = 'testcsv',
  'path' = '$resultPath22'
)

INSERT OVERWRITE database8_t0(c0) VALUES(cast(22424 as SMALLINT))
INSERT OVERWRITE database8_t1(c0, c1) VALUES(cast(-17443 as SMALLINT), cast(97 as TINYINT))
insert into MySink
SELECT database8_t0.c0 AS ref0 FROM database8_t0, database8_t1 WHERE CAST ((- (database8_t0.c0)) AS BOOLEAN)
{code}

After running that , you will get the errors:
{code}
2021-06-29 19:39:27
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:207)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:188)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:677)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:440)
	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.RuntimeException: Could not instantiate generated class 'BatchExecCalc$4536'
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:66)
	at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:43)
	at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:80)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperator(OperatorChain.java:626)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:600)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:540)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:171)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeRestore(StreamTask.java:547)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:646)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:536)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:759)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
	at java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:78)
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:77)
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:64)
	... 12 more
Caused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:76)
	... 14 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:106)
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:76)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
	at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
	... 17 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 53, Column 26: Assignment conversion not possible from type ""int"" to type ""short""
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
	at org.codehaus.janino.UnitCompiler.assignmentConversion(UnitCompiler.java:11062)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3790)
	at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3754)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3734)
	at org.codehaus.janino.Java$Assignment.accept(Java.java:4477)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)
	at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553)
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$Block.accept(Java.java:2779)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476)
	at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:103)
	... 23 more
{code}",,leonard,libenchao,lzljs3620320,TsReaper,xiaojin.wy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 12 08:27:47 UTC 2021,,,,,,,,,,"0|z0sevk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/21 08:22;TsReaper;I've looked into this issue and found that this is a bug in code generation.

In Java the following code cannot compile:

{code:java}
short a = 1;
short b = -a;
{code}

As {{-a}} results in an {{int}} value.

I'm taking this issue.;;;","12/Jul/21 08:27;lzljs3620320;Fixed via:

master: 0ba87eac4b5b40b36d4fccbb6674f92a7dbddfc6

1.13: 447452d0cdbec9a6d97e96dcb89977e52c27750e

1.12: 86cc6a19b5b57e773e54a3a3961d24c7ef9a578e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lost ACKs for redelivered messages in RMQSource ,FLINK-23183,13386513,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,cmick,cmick,cmick,29/Jun/21 11:58,15/Dec/21 01:40,13/Jul/23 08:12,04/Aug/21 08:20,1.12.4,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.0,,,Connectors/ RabbitMQ,,,,,0,pull-request-available,,,,"As described in the FLINK-20244, the redelivered messages are not acknowledged properly (only applicable when autoAck is disabled). When used with a prefetch count in the consumer it may even lead to stop the source to consume any more messages.


 The solution (proposed in FLINK-20244) should resolve the issue. All successfully consumed RMQ messages should be acknowledged, regardless of whether the message is ignored or processed further in the pipeline. The {{sessionIds.add(deliveryTag)}} ([RMQSource.java#L423|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-rabbitmq/src/main/java/org/apache/flink/streaming/connectors/rabbitmq/RMQSource.java#L423]) should be called before checking if the message has already been processed.",,cmick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20244,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 04 08:19:50 UTC 2021,,,,,,,,,,"0|z0seuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/21 18:14;cmick;Hi [~fpaul], I've prepared a PR that fixes the issue: [https://github.com/apache/flink/pull/16472]

The proposed fix is similar to the one discussed, but instead of {{basic.ack}},  {{basic.reject}} is sent on redelivered but not processed messages. It seems to be more appropriate here (though the difference is just semantics, not the actual behavior). As the RabbitMQ documentation ([https://www.rabbitmq.com/confirms.html#acknowledgement-modes]) states:
{noformat}
Positive acknowledgements simply instruct RabbitMQ to record a message as delivered and can be discarded. Negative acknowledgements with basic.reject have the same effect. The difference is primarily in the semantics: positive acknowledgements assume a message was successfully processed while their negative counterpart suggests that a delivery wasn't processed but still should be deleted.{noformat}
So, when a message that was previously received, processed, and successfully check-pointed, is redelivered again (e.g. due to previous acknowledgment process failure), it will be acknowledged immediately using {{basic.reject}} method.;;;","04/Aug/21 08:19;arvid;Merged into master as 900af3e43eb34f8899a4509a8f6c8e43d72f1cc8, into 1.13 as 0ecb730e569de59b2e60a96405d8b138634b29b2, and into 1.12 as 7e17c1d19da0e2330c508928cd3f1fc1e9494d03. Thank you very much!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connection leak in RMQSource ,FLINK-23182,13386502,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,cmick,cmick,cmick,29/Jun/21 11:27,17/Nov/21 15:21,13/Jul/23 08:12,06/Jul/21 16:46,1.12.4,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,Connectors/ RabbitMQ,,,,,0,pull-request-available,,,,"The RabbitMQ connection is not closed properly in the RMQSource connector in case of failures. This leads to a connection leak (we loose handles to still opened connections) that will last until the Flink TaskManager is either stopped or crashes.

The issue is caused by improper resource releasing in open and close methods of RMQSource:
 - [https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-rabbitmq/src/main/java/org/apache/flink/streaming/connectors/rabbitmq/RMQSource.java#L260] - here the connection is opened, but not closed in case of failure (e.g. caused by invalid queue configuration)
 - [https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-rabbitmq/src/main/java/org/apache/flink/streaming/connectors/rabbitmq/RMQSource.java#L282] - here the connection might not closed properly if stopping the consumer causes a failure first

In both cases, the solution is relatively simple - make sure that the connection#close is always called if it should be (failing to close one resource should not prevent other close methods from being called). In open we probably can silently close allocated resources (as the process did not succeed eventually anyway). In close, we should either throw the first caught exception or the last one, and log all the others as warnings.",,cmick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20244,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 06 16:46:02 UTC 2021,,,,,,,,,,"0|z0sesg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/21 07:47;cmick;Hi [~fpaul], I've created a PR fixing the issue described. Would you be able to review that one?;;;","06/Jul/21 16:46;arvid;Merged into master as b662ed19406c8f552c916b6840c8d62fad64b77c, into 1.13 as c830bde767c8080509f24fc688350cff0eb9f3ed, and into 1.12 as 4d017d3b361e066ad9114ebcd7e4011ef5fb6752.

Thank you very much for the contribution!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Links to Task Failure Recovery page on Configuration page are broken,FLINK-23172,13386395,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Thesharing,Thesharing,Thesharing,29/Jun/21 03:25,26/Aug/21 02:37,13/Jul/23 08:12,26/Aug/21 02:37,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Documentation,,,,,0,pull-request-available,,,,"The links to [Task Failure Recovery|https://ci.apache.org/projects/flink/flink-docs-master/docs/ops/state/task_failure_recovery/] page inside [Fault Tolerance|https://ci.apache.org/projects/flink/flink-docs-master/docs/deployment/config/#fault-tolerance] section and [Advanced Fault Tolerance Options|https://ci.apache.org/projects/flink/flink-docs-master/docs/deployment/config/#advanced-fault-tolerance-options] section on the [Configuration|https://ci.apache.org/projects/flink/flink-docs-master/docs/deployment/config/#fault-tolerance/] page are broken.

Let's take an example. In the description of {{restart-strategy}}, currently the link of {{fixed-delay}} refers to [https://ci.apache.org/projects/flink/flink-docs-master/docs/deployment/dev/task_failure_recovery.html#fixed-delay-restart-strategy], which doesn't exist and would head to 404 error. The correct link is [https://ci.apache.org/projects/flink/flink-docs-master/docs/ops/state/task_failure_recovery/#fixed-delay-restart-strategy].

The links are located in {{RestartStrategyOptions.java}} and {{JobManagerOptions.java}}.",,Thesharing,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 26 02:37:10 UTC 2021,,,,,,,,,,"0|z0se4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/21 02:54;zhuzh;Thanks for reporting this problem! [~Thesharing]
I have assigned you this ticket.;;;","04/Aug/21 03:32;zhuzh;Fixed via 
5183b2af9d467708725bd1454a671bc7689159a5
46bf6d68ee97684949ba3ad38dc18ff7c800092a;;;","05/Aug/21 11:42;Thesharing;We found that current solution doesn't really fix the broken links. The links to each options in Task Failure Recovery page is still broken. Also the links are broken in 1.13, too.;;;","23/Aug/21 20:27;chesnay;master: 349f6f5fc7842256cce024a78707dacd9cc40ecd;;;","26/Aug/21 02:37;zhuzh;release-1.13: 1fb2c19fbd24c0037b436b3f261db63fc4056df5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZipUtils doesn't handle properly for softlinks inside the zip file,FLINK-23166,13386203,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,28/Jun/21 07:50,23/Sep/21 17:52,13/Jul/23 08:12,28/Jun/21 23:29,1.10.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.11.4,1.12.5,1.13.2,1.14.0,,API / Python,,,,,0,pull-request-available,,,,"When extracting softlinks inside the zip file, we should restore the softlinks. However, currently we just create a file which content is the target of the softlink.",,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 28 23:29:12 UTC 2021,,,,,,,,,,"0|z0scy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/21 23:29;dian.fu;Fixed in
- master via c44dd62c3b75a1aba9b909fbeef15256043d5808
- release-1.13 via 1ffdc082b6d6368cd66e15ea6b4c591ea99a9c37
- release-1.12 via 169c9674ee71577c583ca4ac26704f315ee84c17
- release-1.11 via d280b2241111650caa03ff671e6e73835c51ea29;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobMasterTest.testMultipleStartsWork unstable on azure,FLINK-23164,13386143,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,xtsong,xtsong,28/Jun/21 02:16,12/Jul/21 07:55,13/Jul/23 08:12,12/Jul/21 07:55,1.11.3,1.12.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.11.4,1.12.5,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19578&view=logs&j=3b6ec2fd-a816-5e75-c775-06fb87cb6670&t=2aff8966-346f-518f-e6ce-de64002a5034&l=6143

{code}
[ERROR] Tests run: 28, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.067 s <<< FAILURE! - in org.apache.flink.runtime.jobmaster.JobMasterTest
[ERROR] testMultipleStartsWork(org.apache.flink.runtime.jobmaster.JobMasterTest)  Time elapsed: 0.054 s  <<< ERROR!
java.util.concurrent.CompletionException: org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcException: Discard message, because the rpc endpoint akka://flink/user/rpc/jobmanager_17 has not been started yet.
	at java.util.concurrent.CompletableFuture.reportJoin(CompletableFuture.java:375)
	at java.util.concurrent.CompletableFuture.join(CompletableFuture.java:1947)
	at org.apache.flink.runtime.jobmaster.JobMasterTest.testMultipleStartsWork(JobMasterTest.java:2314)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcException: Discard message, because the rpc endpoint akka://flink/user/rpc/jobmanager_17 has not been started yet.
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:170)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:517)
	at akka.actor.Actor.aroundReceive$(Actor.scala:515)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}",,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 12 07:55:32 UTC 2021,,,,,,,,,,"0|z0scko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jul/21 09:32;trohrmann;The problem is that we don't wait for the job suspension before restarting the {{JobMaster}}.;;;","12/Jul/21 07:55;trohrmann;Fixed via

1.11.4: ab4e3e3ca10ed4901a8f3b130ff5066ef95785bb
1.12.5: e599aa2a5394897d7a61571ffcfebdbb96a3d152;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Benchmark are not compiling,FLINK-23153,13385771,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Thesharing,Thesharing,Thesharing,25/Jun/21 06:05,20/Aug/21 02:43,13/Jul/23 08:12,25/Jun/21 06:41,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Benchmarks,,,,,0,pull-request-available,,,,"In FLINK-23085, FutureUtils is moved from flink-runtime to flink-core. The reference in flink-benchmark should also be changed. The reference is located at: org/apache/flink/benchmark/operators/RecordSource.java.

The travis CI is broken at this moment: https://travis-ci.com/github/apache/flink-benchmarks/builds/230813827#L2026",,dwysakowicz,Thesharing,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23085,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 25 06:41:22 UTC 2021,,,,,,,,,,"0|z0saa0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jun/21 06:26;zhuzh;Thanks for reporting this issue [~Thesharing]. I have assigned you the ticket.;;;","25/Jun/21 06:41;dwysakowicz;Fixed in b21b5ba92a88fef5bb4d96bb7ae78fafd9a0a590;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KinesisTableApiITCase.testTableApiSourceAndSink fails on azure,FLINK-23151,13385760,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,xtsong,xtsong,25/Jun/21 03:23,25/Jun/21 03:38,13/Jul/23 08:12,25/Jun/21 03:38,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,,,,,Connectors / Kinesis,,,,,0,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19502&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=28179

{code}
Jun 25 00:59:29 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 22.764 s <<< FAILURE! - in org.apache.flink.streaming.kinesis.test.KinesisTableApiITCase
Jun 25 00:59:29 [ERROR] testTableApiSourceAndSink(org.apache.flink.streaming.kinesis.test.KinesisTableApiITCase)  Time elapsed: 22.027 s  <<< FAILURE!
Jun 25 00:59:29 java.lang.AssertionError: expected:<[org.apache.flink.streaming.kinesis.test.model.Order@bed, org.apache.flink.streaming.kinesis.test.model.Order@c11, org.apache.flink.streaming.kinesis.test.model.Order@c35]> but was:<[]>
Jun 25 00:59:29 	at org.junit.Assert.fail(Assert.java:88)
Jun 25 00:59:29 	at org.junit.Assert.failNotEquals(Assert.java:834)
Jun 25 00:59:29 	at org.junit.Assert.assertEquals(Assert.java:118)
Jun 25 00:59:29 	at org.junit.Assert.assertEquals(Assert.java:144)
Jun 25 00:59:29 	at org.apache.flink.streaming.kinesis.test.KinesisTableApiITCase.testTableApiSourceAndSink(KinesisTableApiITCase.java:111)
Jun 25 00:59:29 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jun 25 00:59:29 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jun 25 00:59:29 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jun 25 00:59:29 	at java.lang.reflect.Method.invoke(Method.java:498)
Jun 25 00:59:29 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Jun 25 00:59:29 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jun 25 00:59:29 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Jun 25 00:59:29 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jun 25 00:59:29 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Jun 25 00:59:29 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jun 25 00:59:29 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
Jun 25 00:59:29 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Jun 25 00:59:29 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
Jun 25 00:59:29 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
Jun 25 00:59:29 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
Jun 25 00:59:29 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Jun 25 00:59:29 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Jun 25 00:59:29 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Jun 25 00:59:29 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Jun 25 00:59:29 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Jun 25 00:59:29 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
Jun 25 00:59:29 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
Jun 25 00:59:29 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
Jun 25 00:59:29 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
Jun 25 00:59:29 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Jun 25 00:59:29 	at java.lang.Thread.run(Thread.java:748)
{code}",,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23009,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 25 03:37:17 UTC 2021,,,,,,,,,,"0|z0sa7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jun/21 03:26;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19502&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=a68b8d89-50e9-5977-4500-f4fde4f57f9b&l=27563

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19502&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=16ca2cca-2f63-5cce-12d2-d519b930a729&l=27748

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19502&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=3425d8ba-5f03-540a-c64b-51b8481bf7d6&l=27425

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19502&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=7f606211-1454-543c-70ab-c7a028a1ce8c&l=28031

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19502&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=0b23652f-b18b-5b6e-6eb6-a11070364610&l=18007

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19502&view=logs&j=4dd4dbdd-1802-5eb7-a518-6acd9d24d0fc&t=8d6b4dd3-4ca1-5611-1743-57a7d76b395a&l=16712;;;","25/Jun/21 03:37;xtsong;The test case starts to constantly fail on the 1.13 branch since FLINK-23009 is merged.
Reverted in 7d71b0b0d771e456460be9984f3d95b4d1500aa2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UpdatableTopNFunction output wrong order in the same unique key,FLINK-23142,13385605,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,24/Jun/21 11:31,23/Sep/21 17:54,13/Jul/23 08:12,01/Jul/21 03:24,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"See {{UpdatableTopNFunctionTest.testSortKeyChangesWhenOutputRankNumber}} 
{code:java}
expectedOutput.add(updateBeforeRecord(""book"", 3L, 16, 1L));
expectedOutput.add(updateAfterRecord(""book"", 2L, 11, 1L));
expectedOutput.add(updateBeforeRecord(""book"", 2L, 19, 2L));
expectedOutput.add(updateAfterRecord(""book"", 3L, 16, 2L));
{code}
It should collect the third record and then collect the second record.

This wrong order will lead to the wrong implementation of the downstream, because the records expected by the downstream has unique key, but the disorder here will lead to the data being deleted by mistake.",,libenchao,lzljs3620320,lzy3261944,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 01 03:24:09 UTC 2021,,,,,,,,,,"0|z0s994:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/21 03:24;lzljs3620320;Fixed via:

master: a40abc7f834888a5f42efeefa662ad6ad5d7c222;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL Error while applying rule AggregateReduceGroupingRule,FLINK-23135,13385568,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,godfreyhe,zhengjiewen,zhengjiewen,24/Jun/21 07:28,29/Jun/21 03:08,13/Jul/23 08:12,29/Jun/21 03:08,1.12.3,1.12.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,,,,,Table SQL / Planner,,,,,0,,,,,"When I updated version from 1.12.1 to 1.12.4, the follow SQL was cannot run.
{code:sql}
//代码占位符
String retailSql = ""SELECT\n"" +
        ""    customer_id,\n"" +
        ""    ware_virtual_category,\n"" +
        ""    min(pay_datetime) as pay_datetime\n"" +
        "" FROM "" +
        ""   `kudu`.`default_database`.`impala::cube_kudu.dwd_order_retail_order_pay` \n"" +
        "" WHERE "" +
        ""   pay_date = TO_TIMESTAMP('"" + partitionTime + ""')"" +
        "" AND "" +
        ""   freight_flag in (0)  "" + 
        "" AND   "" +
        ""   order_pay_type <> '3' "" + 
        "" GROUP BY \n"" +
        ""    customer_id,"" +
        ""    ware_virtual_category"";{code}
 

the error message is follow:
{code:java}
//代码占位符
Exception in thread ""main"" java.lang.RuntimeException: Error while applying rule AggregateReduceGroupingRule, args [rel#833:FlinkLogicalAggregate.LOGICAL.any.[](input=RelSubset#832,group={0, 1},pay_datetime=MIN($2))]Exception in thread ""main"" java.lang.RuntimeException: Error while applying rule AggregateReduceGroupingRule, args [rel#833:FlinkLogicalAggregate.LOGICAL.any.[](input=RelSubset#832,group={0, 1},pay_datetime=MIN($2))] at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:256) at org.apache.calcite.plan.volcano.IterativeRuleDriver.drive(IterativeRuleDriver.java:58) at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:510) at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:312) at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:64) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:86) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:57) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:45) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:45) at scala.collection.immutable.List.foreach(List.scala:392) at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:45) at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77) at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:287) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:160) at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1329) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:676) at org.apache.flink.table.api.internal.StatementSetImpl.execute(StatementSetImpl.java:98) at com.kad.cube.dws.day.DwsAllWareCategoryCustomerPayTimeDay.sinkToKudu(DwsAllWareCategoryCustomerPayTimeDay.java:54) at com.kad.cube.dws.day.DwsAllWareCategoryCustomerPayTimeDay.main(DwsAllWareCategoryCustomerPayTimeDay.java:45)Caused by: java.lang.IllegalArgumentException at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:122) at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getUniqueGroups(FlinkRelMetadataQuery.java:197) at org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueGroups.getUniqueGroups(FlinkRelMdUniqueGroups.scala:411) at GeneratedMetadataHandler_UniqueGroups.getUniqueGroups_$(Unknown Source) at GeneratedMetadataHandler_UniqueGroups.getUniqueGroups(Unknown Source) at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getUniqueGroups(FlinkRelMetadataQuery.java:196) at org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueGroups.getUniqueGroupsOfProject(FlinkRelMdUniqueGroups.scala:131) at org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueGroups.getUniqueGroups(FlinkRelMdUniqueGroups.scala:92) at GeneratedMetadataHandler_UniqueGroups.getUniqueGroups_$(Unknown Source) at GeneratedMetadataHandler_UniqueGroups.getUniqueGroups(Unknown Source) at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getUniqueGroups(FlinkRelMetadataQuery.java:196) at org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueGroups.getUniqueGroups(FlinkRelMdUniqueGroups.scala:411) at GeneratedMetadataHandler_UniqueGroups.getUniqueGroups_$(Unknown Source) at GeneratedMetadataHandler_UniqueGroups.getUniqueGroups(Unknown Source) at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getUniqueGroups(FlinkRelMetadataQuery.java:196) at org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueGroups.getUniqueGroupsOfProject(FlinkRelMdUniqueGroups.scala:131) at org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueGroups.getUniqueGroups(FlinkRelMdUniqueGroups.scala:92) at GeneratedMetadataHandler_UniqueGroups.getUniqueGroups_$(Unknown Source) at GeneratedMetadataHandler_UniqueGroups.getUniqueGroups(Unknown Source) at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getUniqueGroups(FlinkRelMetadataQuery.java:196) at org.apache.flink.table.planner.plan.metadata.FlinkRelMdUniqueGroups.getUniqueGroups(FlinkRelMdUniqueGroups.scala:411) at GeneratedMetadataHandler_UniqueGroups.getUniqueGroups_$(Unknown Source) at GeneratedMetadataHandler_UniqueGroups.getUniqueGroups(Unknown Source) at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getUniqueGroups(FlinkRelMetadataQuery.java:196) at org.apache.flink.table.planner.plan.rules.logical.AggregateReduceGroupingRule.onMatch(AggregateReduceGroupingRule.scala:56) at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:229) ... 29 more{code}
!image-2021-06-24-18-04-03-473.png!

  !image-2021-06-24-18-20-54-752.png!

rel#208:FlinkLogicalTableSourceScan.LOGICAL.any.[](table=[kudu, default_database, impala::cube_kudu.dwd_order_retail_order_pay, filter=[equals(pay_date, 2021-06-23T00:00)], project=[customer_id, order_source_id, order_sale_channel_id, pay_datetime, order_pay_type]],fields=customer_id, order_source_id, order_sale_channel_id, pay_datetime, order_pay_type)

 

Compare to version 1.12.1 :

!image-2021-06-24-18-13-16-056.png!",,godfreyhe,hackergin,jark,libenchao,lzljs3620320,martijnvisser,zhengjiewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21710,FLINK-22157,,,,,,,,,,,,,,,"24/Jun/21 10:04;zhengjiewen;image-2021-06-24-18-04-03-473.png;https://issues.apache.org/jira/secure/attachment/13027231/image-2021-06-24-18-04-03-473.png","24/Jun/21 10:13;zhengjiewen;image-2021-06-24-18-13-16-056.png;https://issues.apache.org/jira/secure/attachment/13027232/image-2021-06-24-18-13-16-056.png","24/Jun/21 10:20;zhengjiewen;image-2021-06-24-18-20-54-752.png;https://issues.apache.org/jira/secure/attachment/13027233/image-2021-06-24-18-20-54-752.png","24/Jun/21 07:56;zhengjiewen;yarn.txt;https://issues.apache.org/jira/secure/attachment/13027223/yarn.txt",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 29 03:07:22 UTC 2021,,,,,,,,,,"0|z0s90w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/21 11:46;godfreyhe;Notes: AggregateReduceGroupingRule is only used for batch sql now.;;;","25/Jun/21 01:33;zhengjiewen;thanks [~godfreyhe] [~jark] Can you help look up this problem? ;;;","25/Jun/21 02:28;zhengjiewen;I try to run this sql in version 1.12.2 and 1.12.3. 
The version 1.12.2 is success and 1.12.3 is fail.
The error log is the same as 1.12.4 above;;;","25/Jun/21 02:30;godfreyhe;I will fix this ASAP;;;","27/Jun/21 04:21;godfreyhe;[~zhengjiewen] could you provide the ddl statements, I can not reproduce the error.;;;","27/Jun/21 08:20;zhengjiewen;[~godfreyhe]  Hi he, this is the DDL contains partial fields.
{code:sql}
CREATE TABLE cube_kudu.dwd_order_retail_order_pay (
	pay_date TIMESTAMP comment '支付日期',			-- yyyy-MM-dd 00:00:00
	id STRING COMMENT '订单详情明细ID',
	customer_id STRING COMMENT '客户ID',
	ware_id STRING COMMENT '商品ID',
	order_id STRING COMMENT '订单ID',
order_pay_type STRING COMMENT '支付方式',
	pay_amount decimal(18,4) COMMENT '支付金额',
	pay_datetime TIMESTAMP COMMENT '支付时间',
	ware_virtual_category STRING COMMENT '虚拟分类',
       freight_flag TINYINT COMMENT '运费记录标识', 
	PRIMARY key (pay_date, id)
)
PARTITION BY HASH(pay_date) PARTITIONS 8
STORED AS KUDU;
{code}
;;;","27/Jun/21 14:43;godfreyhe;[~zhengjiewen] This issue is introduced in FLINK-21710 in 1.12.3. It has been fixed by  FLINK-22157 in 1.13. So the solution is we cherry pick FLINK-22157 to 1.12.5.;;;","28/Jun/21 07:01;martijnvisser;[~godfreyhe] should we also make [~lzljs3620320] (as he's release manager for 1.12.5) aware that this fix is coming? ;;;","28/Jun/21 07:33;lzljs3620320;I see that FLINK-22157 has already been cherry-picked to release-1.12. The fix version of FLINK-22157 should include 1.12.5? [~godfreyhe]

Should we close this one because FLINK-22157 has been fixed?;;;","28/Jun/21 07:38;martijnvisser;I believe this one is not yet fixed because https://github.com/apache/flink/pull/16305 is not merged yet ;;;","29/Jun/21 03:07;godfreyhe;Fixed in 1.12.5: d1b8c5fd54e1d104387ef6acb94b9a9698378ed3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The dependencies are not handled properly when mixing use of Python Table API and Python DataStream API,FLINK-23133,13385556,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,24/Jun/21 06:28,27/Jun/21 07:24,13/Jul/23 08:12,25/Jun/21 02:01,1.12.0,1.13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,API / Python,,,,,0,pull-request-available,,,,"The reason is that when converting from DataStream to Table, the dependencies should be handled and set correctly for the existing DataStream operators.",,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 25 02:01:28 UTC 2021,,,,,,,,,,"0|z0s8y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jun/21 02:01;dian.fu;Fixed in:
 - master via cb9ee853e1808b391e84990c4d0bbe2218a40cb6
 - release-1.13 via 4695fa5fb7f90d843b4dae7b2dfb5c9ff807b701
 - release-1.12 via 91d9004a4b41f3f7c499d410c1dd09299dd44b25;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When cancelling any running job of multiple jobs in an application cluster, JobManager shuts down",FLINK-23129,13385491,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,23/Jun/21 20:02,25/Jun/21 09:42,13/Jul/23 08:12,25/Jun/21 09:42,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"I have a jar with two jobs, both executeAsync() from the same main method. I execute the main method in an Application Mode cluster. When I cancel one of the two jobs, both jobs will stop executing.

I would expect that the JobManager shuts down once all jobs submitted from an application are finished.

If this is a known limitation, we should document it.

{code}
2021-06-23 21:29:53,123 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job first job (18181be02da272387354d093519b2359) switched from state RUNNING to CANCELLING.
2021-06-23 21:29:53,124 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source -> Sink: Unnamed (1/1) (5a69b1c19f8da23975f6961898ab50a2) switched from RUNNING to CANCELING.
2021-06-23 21:29:53,141 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source -> Sink: Unnamed (1/1) (5a69b1c19f8da23975f6961898ab50a2) switched from CANCELING to CANCELED.
2021-06-23 21:29:53,144 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job 18181be02da272387354d093519b2359
2021-06-23 21:29:53,145 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job first job (18181be02da272387354d093519b2359) switched from state CANCELLING to CANCELED.
2021-06-23 21:29:53,145 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job 18181be02da272387354d093519b2359.
2021-06-23 21:29:53,147 INFO  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down
2021-06-23 21:29:53,150 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job 18181be02da272387354d093519b2359 reached terminal state CANCELED.
2021-06-23 21:29:53,152 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job first job(18181be02da272387354d093519b2359).
2021-06-23 21:29:53,155 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [c35b64879d6b02d383c825ea735ebba0].
2021-06-23 21:29:53,159 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job 18181be02da272387354d093519b2359
2021-06-23 21:29:53,159 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection 281b3fcf7ad0a6f7763fa90b8a5b9adb: Stopping JobMaster for job first job(18181be02da272387354d093519b2359)..
2021-06-23 21:29:53,160 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@localhost:6123/user/rpc/jobmanager_2 for job 18181be02da272387354d093519b2359 from the resource manager.
2021-06-23 21:29:53,225 INFO  org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap [] - Application CANCELED:
java.util.concurrent.CompletionException: org.apache.flink.client.deployment.application.UnsuccessfulExecutionException: Application Status: CANCELED
	at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$unwrapJobResultException$4(ApplicationDispatcherBootstrap.java:304) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) ~[?:1.8.0_252]
	at org.apache.flink.client.deployment.application.JobStatusPollingUtils.lambda$null$2(JobStatusPollingUtils.java:101) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) ~[?:1.8.0_252]
	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:237) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) [?:1.8.0_252]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) [?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) [?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) [?:1.8.0_252]
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1081) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.OnComplete.internal(Future.scala:264) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.OnComplete.internal(Future.scala:261) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
Caused by: org.apache.flink.client.deployment.application.UnsuccessfulExecutionException: Application Status: CANCELED
	at org.apache.flink.client.deployment.application.UnsuccessfulExecutionException.fromJobResult(UnsuccessfulExecutionException.java:71) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	... 42 more
Caused by: org.apache.flink.runtime.client.JobCancellationException: Job was cancelled.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.client.deployment.application.UnsuccessfulExecutionException.fromJobResult(UnsuccessfulExecutionException.java:60) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	... 42 more
2021-06-23 21:29:53,238 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting StandaloneApplicationClusterEntryPoint down with application status CANCELED. Diagnostics null.
2021-06-23 21:29:53,239 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Shutting down rest endpoint.
2021-06-23 21:29:53,257 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Removing cache directory /var/folders/js/yfk_y2450q7559kygttykwk00000gn/T/flink-web-a0d034d2-da2b-4d72-9ece-ec00c9ae032b/flink-web-ui
2021-06-23 21:29:53,307 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - http://localhost:8081 lost leadership
2021-06-23 21:29:53,307 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Shut down complete.
2021-06-23 21:29:53,307 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Shut down cluster because application is in CANCELED, diagnostics null.
2021-06-23 21:29:53,307 INFO  org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent [] - Closing components.
2021-06-23 21:29:53,308 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Stopping SessionDispatcherLeaderProcess.
2021-06-23 21:29:53,308 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopping dispatcher akka.tcp://flink@localhost:6123/user/rpc/dispatcher_0.
2021-06-23 21:29:53,308 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopping all currently running jobs of dispatcher akka.tcp://flink@localhost:6123/user/rpc/dispatcher_0.
2021-06-23 21:29:53,308 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Stopping resource manager service.
2021-06-23 21:29:53,308 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job second job(e4ff65c30754648cf114232c07ef903e).
2021-06-23 21:29:53,309 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Closing the slot manager.
2021-06-23 21:29:53,309 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job e4ff65c30754648cf114232c07ef903e reached terminal state SUSPENDED.
2021-06-23 21:29:53,309 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Suspending the slot manager.
2021-06-23 21:29:53,309 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is not running. Ignore revoking leadership.
2021-06-23 21:29:53,309 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job second job (e4ff65c30754648cf114232c07ef903e) switched from state RUNNING to SUSPENDED.
org.apache.flink.util.FlinkException: Scheduler is being stopped.
	at org.apache.flink.runtime.scheduler.SchedulerBase.closeAsync(SchedulerBase.java:604) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.stopScheduling(JobMaster.java:962) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.stopJobExecution(JobMaster.java:926) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:398) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2021-06-23 21:29:53,311 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source -> Sink: Unnamed (1/1) (b08fac5184817c72f73a0b3fff0afbd3) switched from RUNNING to CANCELING.
2021-06-23 21:29:53,312 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source -> Sink: Unnamed (1/1) (b08fac5184817c72f73a0b3fff0afbd3) switched from CANCELING to CANCELED.
2021-06-23 21:29:53,313 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b08fac5184817c72f73a0b3fff0afbd3.
2021-06-23 21:29:53,314 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job e4ff65c30754648cf114232c07ef903e.
2021-06-23 21:29:53,314 INFO  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down
2021-06-23 21:29:53,314 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job e4ff65c30754648cf114232c07ef903e has been suspended.
2021-06-23 21:29:53,314 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [30b64fc00bc2c8e83e80567e4f984ae9].
2021-06-23 21:29:53,315 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection 281b3fcf7ad0a6f7763fa90b8a5b9adb: Stopping JobMaster for job second job(e4ff65c30754648cf114232c07ef903e)..
2021-06-23 21:29:53,318 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopped dispatcher akka.tcp://flink@localhost:6123/user/rpc/dispatcher_0.
2021-06-23 21:29:53,323 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:61498
2021-06-23 21:29:53,323 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
2021-06-23 21:29:53,326 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
2021-06-23 21:29:53,331 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Shutting down remote daemon.
2021-06-23 21:29:53,331 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Shutting down remote daemon.
2021-06-23 21:29:53,332 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remote daemon shut down; proceeding with flushing remote transports.
2021-06-23 21:29:53,332 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remote daemon shut down; proceeding with flushing remote transports.
2021-06-23 21:29:53,348 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remoting shut down.
2021-06-23 21:29:53,348 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remoting shut down.
2021-06-23 21:29:53,359 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
2021-06-23 21:29:53,366 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.
2021-06-23 21:29:53,366 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Terminating cluster entrypoint process StandaloneApplicationClusterEntryPoint with exit code 0.
{code}",,rmetzger,Thesharing,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 25 09:42:06 UTC 2021,,,,,,,,,,"0|z0s8js:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/21 13:08;rmetzger;I had an offline discussion with [~trohrmann] about this, and we agreed to just update the docs with this limitation for now.;;;","25/Jun/21 02:02;wangyang0918;I am +1 for only updating the documentation. IIUC, it is an expected behavior when job is canceled or failed. If the job finished successfully, then it should work well.

I remember some of our users are submitting multiple job to a same application in the following way.
{code:java}
// do some preparatory work if necessary
if (xxxx) {
  env.execute(""job1"");
}
// start the streaming job
env.executeAsync(""job2"");{code};;;","25/Jun/21 09:42;rmetzger;Merged to master in https://github.com/apache/flink/commit/18a95ca87dd76ec3d2a7eb19fe72f980d72af90f
merged to release-1.13 in https://github.com/apache/flink/commit/6634b3fc4ef7305513cef067b15edc3d4fa61e7f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the issue that the InternalRow as arguments in Python UDAF,FLINK-23121,13385379,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,23/Jun/21 10:02,24/Jun/21 11:41,13/Jul/23 08:12,24/Jun/21 11:41,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,,,,,API / Python,,,,,0,pull-request-available,,,,"The problem is reported from
https://stackoverflow.com/questions/68026832/pyflink-udaf-internalrow-vs-row

In release-1.14, we have reconstructed the coders and fixed this problem. So this problem only appeared in 1.13
",,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 24 11:41:49 UTC 2021,,,,,,,,,,"0|z0s7v4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/21 11:41;dian.fu;Fixed in release-1.13 via 0db977dee2cf0e74ec2d42ceff7b7a0f519ad673;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ByteArrayWrapperSerializer.serialize should use writeInt to serialize the length,FLINK-23120,13385361,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dian.fu,dian.fu,dian.fu,23/Jun/21 09:00,23/Sep/21 17:52,13/Jul/23 08:12,27/Jun/21 02:21,1.12.0,1.13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,API / Python,,,,,0,pull-request-available,,,,,,dian.fu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jun 27 02:21:10 UTC 2021,,,,,,,,,,"0|z0s7r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/21 02:21;dian.fu;Fixed in
- master via 2a7e7d7e92f0610502e59f6104391a83dc8b5692
- release-1.13 via 862f14e411f4d5e8d7bc21a1dc003aeb507c8889
- release-1.12 via d77c51d449e8d25611b0d84fe5d438949f360384;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the issue that the exception that General Python UDAF is unsupported is not thrown in Compile Stage.,FLINK-23119,13385351,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,23/Jun/21 08:19,23/Sep/21 17:28,13/Jul/23 08:12,24/Jun/21 03:50,1.12.4,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,,,,API / Python,,,,,0,pull-request-available,,,,,,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 24 12:15:29 UTC 2021,,,,,,,,,,"0|z0s7ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/21 03:50;dian.fu;Fixed in
- master via 669fbf29e34b75fbb4944b9ec36564fa1bd3ac47
- release-1.13 via f63bd0b0ab0a40a8b875dbda31e7c2953e34cac6;;;","24/Jun/21 12:15;hxbks2ks;Merged into release-1.12 via 1619bbbbcf1f58bb57605e9dafd1bc34d424cb26;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-statebackend-changelog does not build with scala 2.12,FLINK-23104,13385291,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,xtsong,xtsong,23/Jun/21 02:35,24/Jun/21 02:10,13/Jul/23 08:12,24/Jun/21 02:10,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Build System,Runtime / State Backends,,,,0,pull-request-available,,,,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19335&view=logs&j=ed6509f5-1153-558c-557a-5ee0afbcdf24&t=241b1e5e-1a8e-5e6a-469a-a9b8cad87065&l=4868,,xtsong,Zakelly,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22678,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 24 02:10:13 UTC 2021,,,,,,,,,,"0|z0s7bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/21 02:38;xtsong;Seems related to FLINK-22678.
cc [~Zakelly];;;","23/Jun/21 03:10;Zakelly;[~xintongsong] Yeah, my fault. I'll fix it. Thanks;;;","24/Jun/21 02:10;xtsong;Fixed via:
- master (1.14): 402ddf1bdf1321938ea8f473f189d06fac5c4b4c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Accessing FlameGraphs while not being enabled returns an exception,FLINK-23102,13385201,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,paul8263,nkruber,nkruber,22/Jun/21 15:39,28/Aug/21 12:22,13/Jul/23 08:12,26/Jul/21 08:58,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Runtime / Web Frontend,,,,,0,pull-request-available,usability,,,"Trying to retrieve the FlameGraph in a job that doesn't have it enabled returns this ugly exception:

!image-2021-06-22-17-36-47-730.png!

Instead, it could mention that this feature is not enabled and describe how to enable it.",,nkruber,paul8263,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23101,,FLINK-22527,,,,,,,,,,,,,,,FLINK-23101,,,,,,,,,,,,"22/Jun/21 15:36;nkruber;image-2021-06-22-17-36-47-730.png;https://issues.apache.org/jira/secure/attachment/13027177/image-2021-06-22-17-36-47-730.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 03 07:44:20 UTC 2021,,,,,,,,,,"0|z0s6rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jul/21 07:37;paul8263;Hi [~NicoK]，

After examining the code, I discovered that if you did not enable rest.flamegraph.enabled option, the web endpoint for flame graph would be skipped. As shown below:

{code:java}
        if (clusterConfiguration.get(RestOptions.ENABLE_FLAMEGRAPH)) {
            final JobVertexFlameGraphHandler jobVertexFlameGraphHandler =
                    new JobVertexFlameGraphHandler(
                            leaderRetriever,
                            timeout,
                            responseHeaders,
                            executionGraphCache,
                            executor,
                            initializeThreadInfoTracker(executor));
            handlers.add(
                    Tuple2.of(
                            jobVertexFlameGraphHandler.getMessageHeaders(),
                            jobVertexFlameGraphHandler));
        }
{code}

I plan to change it so that it would send a empty flame graph instead of a missing file error.

Could you pls assign this to me?
;;;","26/Jul/21 08:58;chesnay;master: 2e21321f9c9d9aada7e4ad8ca90d915c34f58015;;;","03/Aug/21 07:44;nkruber;release-1.13: 39ad632a60896c94fb796e46c1bacfea32ea68d8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flame Graphs initial view says it is 18800 days in the past,FLINK-23101,13385198,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,paul8263,nkruber,nkruber,22/Jun/21 15:36,28/Aug/21 13:07,13/Jul/23 08:12,03/Aug/21 07:49,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Runtime / Web Frontend,,,,,0,pull-request-available,usability,,,"When you look at the Flame Graphs for a task for the first time, it will show an empty space and say that the measurement was ~18800 days in the past (see the attached image).

 

This should rather be something more useful like ""no measurement yet"" or so...",,afedulov,airblader,nkruber,paul8263,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23102,,,,,,,,,,,,,,FLINK-23102,,,,,,,,,,,,,,,,"22/Jun/21 15:34;nkruber;image.png;https://issues.apache.org/jira/secure/attachment/13027175/image.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 06 09:04:03 UTC 2021,,,,,,,,,,"0|z0s6qw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/21 01:48;paul8263;Hi [~afedulov] [~NicoK],
By fixing FLINK-23102, I realized this issue is caused by the wrong display of an empty flame graph. If you were not available, could you please consider let me do this work?;;;","06/Jul/21 09:04;afedulov;Hi [~paul8263],
feel free to address this issue, I am not working on this at the moment.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'Queryable state (rocksdb) with TM restart end-to-end test' fails on azure,FLINK-23097,13385188,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,dwysakowicz,dwysakowicz,22/Jun/21 14:26,15/Dec/21 01:40,13/Jul/23 08:12,23/Aug/21 09:22,1.12.4,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.0,,,Runtime / Queryable State,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19310&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=12333

{code}
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
Jun 22 14:04:55 MapState has 22 entries
Jun 22 14:04:56 TaskManager 422719 killed.
Jun 22 14:04:56 Number of running task managers 1 is not yet 0.
Jun 22 14:05:00 Number of running task managers 1 is not yet 0.
Jun 22 14:05:04 Number of running task managers 1 is not yet 0.
Jun 22 14:05:08 Number of running task managers has reached 0.
Jun 22 14:05:08 Latest snapshot count was 42
Jun 22 14:05:09 Starting taskexecutor daemon on host fv-az68-17.
Jun 22 14:05:09 Number of running task managers 0 is not yet 1.
Jun 22 14:05:13 Number of running task managers has reached 1.
Jun 22 14:05:15 Job (5b515e0f9168e338d1645bf2e9f92820) is running.
Jun 22 14:05:15 Starting to wait for completion of 18 checkpoints
Jun 22 14:05:15 13/18 completed checkpoints
Jun 22 14:05:17 13/18 completed checkpoints
Jun 22 14:05:19 17/18 completed checkpoints
Jun 22 14:05:21 17/18 completed checkpoints
SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
Jun 22 14:05:24 after: 40
Jun 22 14:05:24 An error occurred
Jun 22 14:05:24 [FAIL] Test script contains errors.
Jun 22 14:05:24 Checking of logs skipped.
Jun 22 14:05:24 
Jun 22 14:05:24 [FAIL] 'Queryable state (rocksdb) with TM restart end-to-end test' failed after 0 minutes and 50 seconds! Test exited with exit code 1
Jun 22 14:05:24 
14:05:24 ##[group]Environment Information

{code}",,dwysakowicz,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 23 09:22:12 UTC 2021,,,,,,,,,,"0|z0s6oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/21 12:10;trohrmann;I am wondering whether we shouldn't simply drop queryable state and the related tests given that it is no longer actively maintained.;;;","01/Jul/21 06:07;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19761&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=10769;;;","07/Jul/21 10:01;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20070&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=10822;;;","07/Jul/21 10:02;dwysakowicz;I wouldn't be opposed to that [~trohrmann]. I think it still requires quite a lot of effort to be production-ready.;;;","23/Jul/21 05:33;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20851&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=11938;;;","20/Aug/21 10:39;trohrmann;I think the problem is that we write out uncommitted information to STDOUT that is later used by the qs client to figure out whether the correct state has been retrieved.;;;","23/Aug/21 09:22;trohrmann;Fixed via 

1.14.0: e10af1f7b164605dea7695e4418b5e61e4bd92df
1.13.3: 7522bd71cca80dcfd69f64e20020c174073d3ff3
1.12.6: 86aa3aef779c8b73eeb2c9a406bd701f6ae43f93;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveParser could not attach the sessionstate of hive,FLINK-23096,13385171,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tinny,tinny,tinny,22/Jun/21 13:08,23/Sep/21 17:28,13/Jul/23 08:12,24/Jun/21 03:34,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"My sql code is as follows：
{code:java}
//代码占位符
CREATE CATALOG myhive WITH (
    'type' = 'hive',
    'default-database' = 'default',
    'hive-conf-dir' = '/home/service/upload-job-file/1624269463008'
);

use catalog hive;

set 'table.sql-dialect' = 'hive';

create view if not exists view_test as
select
  cast(goods_id as string) as goods_id,
  cast(depot_id as string) as depot_id,
  cast(product_id as string) as product_id,
  cast(tenant_code as string) as tenant_code
from edw.dim_yezi_whse_goods_base_info/*+ OPTIONS('streaming-source.consume-start-offset'='dayno=20210621') */;
{code}
and the exception is as follows:
{code:java}
//代码占位符
org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Conf non-local session path expected to be non-null
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372)
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
        at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)
        at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:812)
        at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:246)
        at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1054)
        at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1132)
        at org.apache.flink.client.cli.CliFrontend$$Lambda$68/330382173.call(Unknown Source)
        at org.apache.flink.runtime.security.contexts.HadoopSecurityContext$$Lambda$69/680712932.run(Unknown Source)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
        at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1132)
Caused by: java.lang.NullPointerException: Conf non-local session path expected to be non-null
        at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:208)
        at org.apache.hadoop.hive.ql.session.SessionState.getHDFSSessionPath(SessionState.java:669)
        at org.apache.flink.table.planner.delegation.hive.HiveParser.clearSessionState(HiveParser.java:376)
        at org.apache.flink.table.planner.delegation.hive.HiveParser.parse(HiveParser.java:219)
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:724)
        at com.shizhengchao.io.FlinkSqlStreamingPlatform.callFlinkSql(FlinkSqlStreamingPlatform.java:157)
        at com.shizhengchao.io.FlinkSqlStreamingPlatform.callCommand(FlinkSqlStreamingPlatform.java:129)
        at com.shizhengchao.io.FlinkSqlStreamingPlatform.run(FlinkSqlStreamingPlatform.java:91)
        at com.shizhengchao.io.FlinkSqlStreamingPlatform.main(FlinkSqlStreamingPlatform.java:66)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
        ... 13 common frames omitted
{code}
My guess is that sessionstate is not set to threadlocal：
{code:java}
//代码占位符
// @see org.apache.hadoop.hive.ql.session.SessionState.setCurrentSessionState
public static void setCurrentSessionState(SessionState startSs) {
  tss.get().attach(startSs);
}
{code}
 ",,leexu,leonard,lirui,luoyuxia,tinny,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 04 05:49:41 UTC 2021,,,,,,,,,,"0|z0s6kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/21 13:19;tinny;This is the location of the code exception I found:

 
{code:java}
//代码占位符

// @see org.apache.flink.table.planner.delegation.hive.HiveParser
private void clearSessionState(HiveConf hiveConf) {
    SessionState sessionState = SessionState.get();
    if (sessionState != null) {
        try {
            sessionState.close();
            List<Path> toDelete = new ArrayList<>();
            toDelete.add(SessionState.getHDFSSessionPath(hiveConf));
            toDelete.add(SessionState.getLocalSessionPath(hiveConf));
            for (Path path : toDelete) {
                FileSystem fs = path.getFileSystem(hiveConf);
                fs.delete(path, true);
            }
        } catch (IOException e) {
            LOG.warn(""Error closing SessionState"", e);
        }
    }
}

// @see org.apache.hadoop.hive.ql.session.SessionState
public static Path getHDFSSessionPath(Configuration conf) {
  SessionState ss = SessionState.get();
  if (ss == null) {
    String sessionPathString = conf.get(HDFS_SESSION_PATH_KEY);
    Preconditions.checkNotNull(sessionPathString,
        ""Conf non-local session path expected to be non-null"");
    return new Path(sessionPathString);
  }
  Preconditions.checkNotNull(ss.hdfsSessionPath,
      ""Non-local session path expected to be non-null"");
  return ss.hdfsSessionPath;
}
{code}
 

 ;;;","23/Jun/21 03:06;tinny;[~Leonard Xu] [~lirui]
{code:java}
//代码占位符
private void clearSessionState(HiveConf hiveConf) {
    SessionState sessionState = SessionState.get();
    if (sessionState != null) {
        try {
            sessionState.close();
            List<Path> toDelete = new ArrayList<>();
            toDelete.add(SessionState.getHDFSSessionPath(hiveConf));
            toDelete.add(SessionState.getLocalSessionPath(hiveConf));
            for (Path path : toDelete) {
                FileSystem fs = path.getFileSystem(hiveConf);
                fs.delete(path, true);
            }
        } catch (IOException e) {
            LOG.warn(""Error closing SessionState"", e);
        }
    }

public static Path getHDFSSessionPath(Configuration conf) {
    SessionState ss = SessionState.get();
    if (ss == null) {
      String sessionPathString = conf.get(HDFS_SESSION_PATH_KEY);
      Preconditions.checkNotNull(sessionPathString,
          ""Conf non-local session path expected to be non-null"");
      return new Path(sessionPathString);
    }
    Preconditions.checkNotNull(ss.hdfsSessionPath,
        ""Non-local session path expected to be non-null"");
    return ss.hdfsSessionPath;
  }
}{code}

beacuse the session is closed，so SessionState.get() is null.;;;","23/Jun/21 08:43;tinny;The  *clearSessionState* method catches IOException, which causes the root cause to be overwritten。When I caught the Exception, I got the root cause：
{code:java}
Caused by: org.apache.hadoop.security.AccessControlException: Permission denied: user=service, access=WRITE, inode=""/tmp"":hdfs:supergroup:drwxr-xr-x
        at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkFsPermission(DefaultAuthorizationProvider.java:280)
       ...
        at org.apache.hadoop.hive.ql.exec.Utilities.createDirsWithPermission(Utilities.java:3678)
        at org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:597)
        at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:554)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:508)
{code};;;","24/Jun/21 01:46;luoyuxia;Hi [~tinny]

As the exception shows, 
java.lang.NullPointerException: Conf non-local session path expected to be non-null
I think the reason is missing the configuration '_hive.hdfs.session.path'.

And for the last exception you show, 
Caused by: org.apache.hadoop.security.AccessControlException: Permission denied: user=service, access=WRITE, inode=""/tmp"":hdfs:supergroup:drwxr-xr-x
I think the reason is you have no right to access(delete) the file.;;;","24/Jun/21 03:34;lirui;Fixed in master: 36a05834f0aa0e949138a52e1bb1aa7116767b1a
Fixed in release-1.13: ea3d085f6b19b6dfb36462650344229c9a3ca70b;;;","24/Jun/21 03:35;lirui;This is actually a different issue from FLINK-16688, un-linking it.;;;","03/Aug/21 03:12;leexu;LocalSessionPath is FileSystem local path;

path.getFileSystem(hiveConf) is HdfsFileSystem,so delete error.

I think , use LocalFileSystem to delete LocalSessionPath.
{code:java}
//代码占位符
org.apache.hadoop.hive.ql.session.SessionState

private static void createPath(HiveConf conf, Path path, String permission, boolean isLocal, boolean isCleanUp) throws IOException {
    FsPermission fsPermission = new FsPermission(permission);
    Object fs;
    if (isLocal) {
        fs = FileSystem.getLocal(conf);
    } else {
        fs = path.getFileSystem(conf);
    }

    if (!((FileSystem)fs).exists(path)) {
        ((FileSystem)fs).mkdirs(path, fsPermission);
        String dirType = isLocal ? ""local"" : ""HDFS"";
        LOG.info(""Created "" + dirType + "" directory: "" + path.toString());
    }

    if (isCleanUp) {
        ((FileSystem)fs).deleteOnExit(path);
    }

}

org.apache.flink.table.planner.delegation.hive.HiveParser

private void clearSessionState(HiveConf hiveConf) {
    SessionState sessionState = SessionState.get();
    if (sessionState != null) {
        try {
            sessionState.close();
            List<Path> toDelete = new ArrayList<>();
            toDelete.add(SessionState.getHDFSSessionPath(hiveConf));
            toDelete.add(SessionState.getLocalSessionPath(hiveConf));
            for (Path path : toDelete) {
                FileSystem fs = path.getFileSystem(hiveConf);
                fs.delete(path, true);
            }
        } catch (IOException e) {
            LOG.warn(""Error closing SessionState"", e);
        }
    }
}
{code}
 ;;;","04/Aug/21 01:39;luoyuxia;[~leexu]  Actual, path.getFileSystem with return HdfsFileSystem or LocalFileSystem according to what the path is. It'll get LocalFileSystem for LocalSessionPath, there should be error when delete the LocalSessionPath.;;;","04/Aug/21 05:49;leexu;{code:java}
//代码占位符
java.lang.IllegalArgumentException: Pathname /C:/Users/merit/AppData/Local/Temp/merit/b6b954c0-78b8-458b-be35-191dcd94d535 from C:/Users/merit/AppData/Local/Temp/merit/b6b954c0-78b8-458b-be35-191dcd94d535 is not a valid DFS filename.java.lang.IllegalArgumentException: Pathname /C:/Users/merit/AppData/Local/Temp/merit/b6b954c0-78b8-458b-be35-191dcd94d535 from C:/Users/merit/AppData/Local/Temp/merit/b6b954c0-78b8-458b-be35-191dcd94d535 is not a valid DFS filename. at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:196) at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:105) at org.apache.hadoop.hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:638) at org.apache.hadoop.hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:634) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:634) at org.apache.flink.table.planner.delegation.hive.HiveParser.clearSessionState(HiveParser.java:229) at org.apache.flink.table.planner.delegation.hive.HiveParser.parse(HiveParser.java:108) at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:724)
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Built-in UDAFs could not be mixed use with Python UDAF in group window,FLINK-23092,13385118,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,22/Jun/21 09:10,23/Sep/21 17:27,13/Jul/23 08:12,23/Jun/21 03:30,1.13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,,,,,API / Python,,,,,0,pull-request-available,,,,,,dian.fu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 23 03:30:14 UTC 2021,,,,,,,,,,"0|z0s694:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/21 03:30;hxbks2ks;Merged into master via b5d6a4349605048776c7765b1522ece73895b483
Merged into release-1.13 via 4140455083a0bb0c9240ceb1d2aac6d2024c097d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scheduler Benchmarks not compiling,FLINK-23078,13385079,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Thesharing,pnowojski,pnowojski,22/Jun/21 06:11,29/Jun/21 13:56,13/Jul/23 08:12,29/Jun/21 12:32,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Benchmarks,Runtime / Coordination,,,,0,pull-request-available,,,,"{code:java}
07:46:50  [ERROR] /home/jenkins/workspace/flink-master-benchmarks/flink-benchmarks/src/main/java/org/apache/flink/scheduler/benchmark/SchedulerBenchmarkBase.java:21:44:  error: cannot find symbol
{code}

CC [~chesnay] [~Thesharing]",,pnowojski,Thesharing,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22988,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 29 12:32:30 UTC 2021,,,,,,,,,,"0|z0s60g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/21 06:21;pnowojski;-I'm fixing the benchmark build-, but it looks like we are still referring in the flink-benchmarks repository to some internal flink classes. At the first glance it looks like this reference to {{TestingUtils}} could be removed, because each individual benchmark is defining it's own {{tearDown}} method that either already is doing, or could be doing the shutdown? [~Thesharing] [~chesnay] could you take a look at this?;;;","22/Jun/21 09:57;Thesharing;Thank you for reminding me, [~pnowojski]. I noticed that [~chesnay] has removed {{TestingUtil.defaultExecutor().shutdownNow()}} from the teardown of SchedulerBenchmarkBase. In fact, {{TestingUtil.defaultExecutor()}} is introduced in {{DefaultSchedulerBuilder}}, which is widely used in tests related to Scheduler. If {{TestingUtil.defaultExecutor()}} is not shutdown, JMH may need to wait until the timeout. This will slow down the execution of the scheduler benchmark. I think maybe it's better to preserve this teardown function.;;;","22/Jun/21 10:20;pnowojski;In that case [~Thesharing], can not we move the {{shutdownNow()}} to the {{flink-runtime}} package to {{org.apache.flink.runtime.scheduler.benchmark.e2e.SchedulerBenchmarkBase#teardown}} ? That would make {{flink-benchmarks}} repo free of the {{TestingUtil}} dependency.;;;","22/Jun/21 16:57;Thesharing;I agree. I'll do it at once.;;;","23/Jun/21 08:28;Thesharing;Hi [~pnowojski], I've came up with a pull request that wraps `TestingUtil.defaultExecutor().shutdownNow()` into `SchedulerBenchmarkUtils`. I've tested it locally and it works well. The PR: https://github.com/apache/flink/pull/16250;;;","29/Jun/21 12:32;zhuzh;Fixed via
flink:
439dbfa48122df164780f55da2cb05f64669a247
49fafcaa62cd34a468b6efc14bf49c53f860d7ce

flink-benchmarks:
e44f22bfa314c08b5a15cea932b84a848a6975ec
1331a9d73255b277d3c37bf9f222bfd0c968393b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DispatcherTest.testWaitingForJobMasterLeadership fails on azure,FLINK-23076,13385068,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,xtsong,xtsong,22/Jun/21 03:49,12/Jul/21 07:57,13/Jul/23 08:12,12/Jul/21 07:57,1.12.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19265&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=19336553-69ec-5b03-471a-791a483cced6&l=6511

{code}
[ERROR] Failures: 
[ERROR]   DispatcherTest.testWaitingForJobMasterLeadership:672 
Expected: is <RUNNING>
     but: was <INITIALIZING>
{code}",,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 12 07:57:02 UTC 2021,,,,,,,,,,"0|z0s5y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/21 07:57;trohrmann;Fixed via da53f93b53755b3b3b0caef1c46b63cd3015d14e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
There is a class conflict between flink-connector-hive and flink-parquet,FLINK-23074,13385058,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ana4,ana4,ana4,22/Jun/21 02:55,17/Mar/23 08:12,13/Jul/23 08:12,06/Jul/21 12:15,1.12.4,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,Connectors / Hive,,,,,0,pull-request-available,,,,"flink-connector-hive 2.3.6 include parquet-hadoop 1.8.1 version but flink-parquet include 1.11.1.
org.apache.parquet.hadoop.example.GroupWriteSupport
 is different.",,ana4,lirui,luoyuxia,yuchuanchen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/21 09:55;luoyuxia;E8C394D1-F970-4825-82CD-3EFA74C65B27.png;https://issues.apache.org/jira/secure/attachment/13027196/E8C394D1-F970-4825-82CD-3EFA74C65B27.png","23/Jun/21 09:26;ana4;image-2021-06-23-17-26-32-559.png;https://issues.apache.org/jira/secure/attachment/13027194/image-2021-06-23-17-26-32-559.png","01/Jul/21 10:23;ana4;image-2021-07-01-18-23-47-105.png;https://issues.apache.org/jira/secure/attachment/13027521/image-2021-07-01-18-23-47-105.png","01/Jul/21 10:40;ana4;image-2021-07-01-18-40-00-991.png;https://issues.apache.org/jira/secure/attachment/13027524/image-2021-07-01-18-40-00-991.png","01/Jul/21 10:40;ana4;image-2021-07-01-18-40-31-729.png;https://issues.apache.org/jira/secure/attachment/13027523/image-2021-07-01-18-40-31-729.png","24/Jun/21 02:48;ana4;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13027217/screenshot-1.png","30/Jun/21 13:02;ana4;screenshot-3.png;https://issues.apache.org/jira/secure/attachment/13027446/screenshot-3.png","01/Jul/21 09:56;ana4;screenshot-4.png;https://issues.apache.org/jira/secure/attachment/13027516/screenshot-4.png","19/Oct/22 02:31;yuchuanchen;screenshot-5.png;https://issues.apache.org/jira/secure/attachment/13051161/screenshot-5.png","19/Oct/22 02:33;yuchuanchen;screenshot-6.png;https://issues.apache.org/jira/secure/attachment/13051162/screenshot-6.png",,10.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 17 08:12:13 UTC 2023,,,,,,,,,,"0|z0s5vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/21 02:58;ana4;cc [~lzljs3620320]
cc [~lirui];;;","22/Jun/21 03:08;ana4;Could we add  a <relocation> parquet in connector-hive-2.3.6 pom.
{code:xml}
<relocations>
  <relocation>
    <pattern>org.apache.parquet</pattern>						 
    <shadedPattern>org.apache.flink.hive.shaded.parquet</shadedPattern>
  </relocation>
</relocations>
{code}

The same applies to other versions.

;;;","22/Jun/21 08:12;ana4;If we cloud relocate org.apache.parquet. Please assign this ticket to me. I wanna fix it.;;;","23/Jun/21 08:50;luoyuxia;[~ana4] 

Hi, I see the relocation has been done in module flink-connector-hive_2.11, so the flink-parquet will be shaded while building.  And as the other versions' hive connector just refer it as a dependency,  there's no need to do relocation for these hive connector, there won't be any conflict.;;;","23/Jun/21 09:22;ana4;cc [~luoyuxia] flink-connector-hive 2.3.6 depend hive-exec. hive-exec shaded parquet-hadoop 1.8.1 . So there is a conflict between flink-connector-hive 2.3.6 and flink-parquet.

 !image-2021-06-23-17-26-32-559.png! ;;;","23/Jun/21 09:55;luoyuxia;[~ana4] 

I'm not sure about what the confict is. According to the solution you proposed, I guess you mean we need to shade the 'org.apache.parquet' included in hive-exec to avoid flink-parquet and hive-exec refer to same 'org.apache.parquet' .

 

But after I build the module flink-connector-hive 2.3.6,  the 'org.apache.parquet' included in hive-exec and flink-parquet actually belong to different  package as shown below. 

!E8C394D1-F970-4825-82CD-3EFA74C65B27.png|width=268,height=297!

'org.apache.parquet' has been shaded to 'org.apache.flink.hive.shaded.parque' in flink-connector-hive_2.11, and then when  flink-connector-hive 2.3.6 including hive-exec, it's no need to shade 'org.apache.parquet'  again.

 

 ;;;","30/Jun/21 13:04;ana4;[~luoyuxia] When I use HDFS read or write parquet and use flink hive connector as table catalog, it will conflict. 
They don't load at the same job.

 !screenshot-3.png! ;;;","30/Jun/21 14:45;luoyuxia;[~ana4] Hi, would you like to explain what the conflict is?;;;","01/Jul/21 10:03;ana4;[~luoyuxia]
I custom a HDFS connector, it use ExampleParquetWriter class included in flink-parquet.
 !screenshot-4.png! 
when call ExampleParquetWriter.Builder#build and already loaded hive-connector-2.3.6, it will throw the following error.

{code:java}
Caused by: java.lang.NoSuchMethodError: org.apache.parquet.hadoop.example.GroupWriteSupport.<init>(Lorg/apache/parquet/schema/MessageType;Ljava/util/Map;)V
	at org.apache.parquet.hadoop.example.ExampleParquetWriter$Builder.getWriteSupport(ExampleParquetWriter.java:114)
	at org.apache.parquet.hadoop.ParquetWriter$Builder.build(ParquetWriter.java:489)
{code}

So I want to shade hive-connector all version. Because hive is often used as the Flink catalog.



GroupWriteSupport in hive-connector 
 !image-2021-07-01-18-23-47-105.png! 

GroupWriteSupport in flink-parquet
 !image-2021-07-01-18-40-00-991.png! 
 !image-2021-07-01-18-40-31-729.png! 
;;;","05/Jul/21 02:21;luoyuxia;[~ana4] Now, I understand what you mean. I'm not sure whether we need to shade 'org.apache.parquet' again as we have done once when include flink-parquet. 

 [~lirui], could you have a look about it?;;;","05/Jul/21 02:47;lirui;I think we can relocate parquet in sql-connector-hive jars, but the shaded pattern should be different from {{org.apache.flink.hive.shaded.parquet}} otherwise it overrides the classes in connector-hive jar.;;;","05/Jul/21 11:45;ana4;[~lirui] Could we name 'org.apache.flink.hive.relocated.parquet'. It is different from org.apache.flink.hive.shaded.parquet.;;;","06/Jul/21 02:46;lirui;How about just {{org.apache.hive.shaded.parquet}}, as we're relocating parquet classes in hive-exec?;;;","06/Jul/21 02:59;ana4;I agree;;;","06/Jul/21 12:15;lirui;Fixed in master: 78bafc55363d11e33143f1769248b85eaec2ee5a;;;","06/Jul/21 12:16;lirui;[~ana4] If you want this fix in release 1.13 or 1.12, please open PRs for those branches respectively.;;;","08/Jul/21 08:37;ana4;[~lirui] I has fixed in release 1.13 and 1.12. please review it. thanks.;;;","09/Jul/21 04:18;lirui;Fixed in release-1.13: 8359fa8a9149392d964f52e7492b4dc24d74bb15
Fixed in release-1.12: 09ac6e50da595b080878d1f18815d2a9fb251f13;;;","19/Oct/22 02:37;yuchuanchen;I found that  this commit(ce7481ae20cf6b4a32568df1d774467247acd08b)  committed by [~twalthr] had changed to org.apache.flink.hive.shaded.parquet. Could some one confirm this? And was this change expected?
 !screenshot-5.png! 
 !screenshot-6.png! ;;;","20/Oct/22 01:49;luoyuxia;Not very sure why this changes. [~yuchuanchen] Do you encounter the same problem reported in this jira with this change?;;;","21/Oct/22 02:49;yuchuanchen;[~luoyuxia]  After this change, the version of org.apache.flink.hive.shaded.parquet has been changed to the parqeut-hadoop version used by FLINK(for example, parquet-hadoop-1.12.2 in flink-1.15). I found this issue when i met zstd-jni not found problem(since parquet-hadoop-1.12 using zstd-jni-1.4.9-1 and we did not include zstd-jni-1.4.9-1 in our classpath).

Now i have included zstd-jni-1.4.9-1 in our classpath and everything is ok now. But i am still wondering the reason  of this change. Maybe relates to some vulnerabilities?;;;","17/Mar/23 08:12;luoyuxia;[~yuchuanchen] No. It's just make  relocation pattern consistent  in whole flink.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix space handling in Row CSV timestamp parser,FLINK-23073,13384993,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sjwiesman,sjwiesman,sjwiesman,21/Jun/21 18:36,23/Sep/21 17:27,13/Jul/23 08:12,23/Jun/21 15:08,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,FLINK-21947 Added support for TIMESTAMP_LTZ in the CSV format by replacing java.sql.Timestamp.valueOf with java.time.LocalDateTime.parse. Timestamp.valueOf internally calls `trim()` on the string before parsing while LocalDateTime.parse does not. This caused a breaking change where the CSV format can no longer parse timestamps of CSV's with spaces after the delimiter. We should manually re-add the call to trim to revert the behavior.,,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 23 15:08:25 UTC 2021,,,,,,,,,,"0|z0s5hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/21 15:08;sjwiesman;fixed in master: 41ce9ccbf42537a854087b6ba33a61092a04538f

fixed in release-1.13: 61f1f0f4f09875b5d8f4c2db956aa520facc4b2c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-elasticsearch-6 not work,FLINK-23058,13384828,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,yandufeng,yandufeng,21/Jun/21 09:27,22/Jun/21 05:49,13/Jul/23 08:12,22/Jun/21 05:49,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / ElasticSearch,,,,,0,bug,,,,"i have two questions.

1. when i add elasticserach host and port，i random write host, but not report error. for example

List<HttpHost> httpHosts = new ArrayList<>();
 httpHosts.add(new HttpHost(""sdfsdfsf"", 9200, ""http""));

2. when i write conrrect elasticsearch host and port, but no response, also not create index in elasticsearch

 

!企业微信截图_16242678783453.png!",,yandufeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/21 09:28;yandufeng;code.txt;https://issues.apache.org/jira/secure/attachment/13027105/code.txt","21/Jun/21 09:33;yandufeng;企业微信截图_16242678783453.png;https://issues.apache.org/jira/secure/attachment/13027106/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_16242678783453.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 22 05:49:04 UTC 2021,,,,,,,,,,"0|z0s4go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/21 05:49;yandufeng;it works, i forget to add elasticsearch's package

<dependency>
 <groupId>org.elasticsearch.client</groupId>
 <artifactId>elasticsearch-rest-high-level-client</artifactId>
 <version>6.8.16</version>
</dependency>;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-console.sh doesn't do variable expansion for FLINK_ENV_JAVA_OPTS like flink-daemon.sh,FLINK-23057,13384817,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zor_X_L,Zor_X_L,Zor_X_L,21/Jun/21 09:03,24/Jun/21 13:18,13/Jul/23 08:12,24/Jun/21 13:18,1.12.4,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Client / Job Submission,,,,,0,pull-request-available,,,,"In flink-deamon.sh:

 
{code:java}
...

# Evaluate user options for local variable expansion
FLINK_ENV_JAVA_OPTS=$(eval echo ${FLINK_ENV_JAVA_OPTS})

echo ""Starting $DAEMON daemon on host $HOSTNAME.""
""$JAVA_RUN"" $JVM_ARGS ${FLINK_ENV_JAVA_OPTS} ""${log_setting[@]}"" -classpath ""`manglePathList ""$FLINK_TM_CLASSPATH:$INTERNAL_HADOOP_CLASSPATHS""`"" ${CLASS_TO_RUN} ""${ARGS[@]}"" > ""$out"" 200<&- 2>&1 < /dev/null &

...
{code}
There is a ""$(eval echo ...)"" line, so variables like ${FLINK_LOG_PREFIX} in FLINK_ENV_JAVA_OPTS can be expanded, as described in [https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/ops/debugging/application_profiling/]

but flink-console.sh doesn't have the line, and as kubernetes-jobmanager.sh and kubernetes-taskmanager.sh all depend on flink-console.sh, so in native kubernetes application mode, variable expansion of FLINK_ENV_JAVA_OPTS is not working.

Add that line to flink-console.sh sovles the problem, patch: [https://github.com/Zor-X-L/flink/commit/e09ba4400b4183e2486c3ef73b986473b7dbea9a]

 ",,rmetzger,wangyang0918,Zor_X_L,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 24 13:18:14 UTC 2021,,,,,,,,,,"0|z0s4e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/21 02:54;wangyang0918;I think the changes make sense to me. Would you like to provide a PR for this ticket?;;;","22/Jun/21 07:16;Zor_X_L;[~fly_in_gis] PR created;;;","24/Jun/21 13:18;rmetzger;Resolved in master https://github.com/apache/flink/commit/4b62f214202d311b816e12ef60c82c640e12c4a0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnalignedCheckpointCompatibilityITCase hangs on azure,FLINK-23049,13384760,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,xtsong,xtsong,21/Jun/21 02:53,05/Oct/21 14:16,13/Jul/23 08:12,05/Oct/21 14:16,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.15.0,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19175&view=logs&j=219e462f-e75e-506c-3671-5017d866ccf6&t=4c5dc768-5c82-5ab0-660d-086cb90b76a0&l=6390

{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007f521000b800 nid=0x3735 waiting on condition [0x00007f5217938000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.flink.test.checkpointing.UnalignedCheckpointCompatibilityITCase.waitForChild(UnalignedCheckpointCompatibilityITCase.java:188)
	at org.apache.flink.test.checkpointing.UnalignedCheckpointCompatibilityITCase.runAndTakeExternalCheckpoint(UnalignedCheckpointCompatibilityITCase.java:166)
	at org.apache.flink.test.checkpointing.UnalignedCheckpointCompatibilityITCase.test(UnalignedCheckpointCompatibilityITCase.java:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,akalashnikov,dwysakowicz,pnowojski,roman,wind_ljy,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 05 14:16:05 UTC 2021,,,,,,,,,,"0|z0s41k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/21 13:07;akalashnikov;Unfortunately, there is no full log left so it is not possible to check for the possible exceptions. But maybe [~roman_khachatryan], do you have any idea why `waitForChild(checkpointDir, (dir, name) -> name.equals(""_metadata""))` inside of `UnalignedCheckpointCompatibilityITCase#runAndTakeExternalCheckpoint` can stuck forever? Also, as I can see this fail happened only once a couple of months ago, so maybe we already made the fix for this problem?;;;","27/Aug/21 13:44;roman;That function can stuck apparently when no checkpoint is produced.

From the log, I see that the source task was not stopped and was waiting for the cancellation.

Cancellation was expected from stop-with-savepoint (UnalignedCheckpointCompatibilityITCase.runAndTakeExternalCheckpoint).

 

So probably something happened while stopping job with savepoint; it was modified and had some bug fixes recently, so probably it's not relevant anymore.;;;","27/Aug/21 13:44;pnowojski;Isn't this related some somehow to FLINK-23647?;;;","27/Aug/21 13:47;roman;It seems unrelated to FLINK-23647: there, an error occurs when a checkpoint is aborted (not hanging up). ;;;","27/Aug/21 13:56;pnowojski;As the test has changed and the issue waiting for _metadata file was almost definitely a test code issue, let's close it as cannot reproduce. If it happens again we can investigate once more.

Thanks for taking a look into this one [~akalashnikov];;;","21/Sep/21 12:04;dwysakowicz;happened on 1.13, should we reopen [~pnowojski]? https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24337&view=logs&j=21408240-6569-5a01-c099-3adfe83ce651&t=b2761bb8-3852-5a0d-bc43-6a1d327b63cb&l=14152;;;","05/Oct/21 14:16;dwysakowicz;Fixed in 1.15 in e7d1bfbbb0bbf57007cbd0028fba3db065e249e9

For older versions we plan to see how often it happens, so far it is not too prominent.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraConnectorITCase.testCassandraBatchTupleFormat fails on azure,FLINK-23047,13384758,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,echauchot,xtsong,xtsong,21/Jun/21 02:41,24/Jan/22 08:34,13/Jul/23 08:12,13/Jan/22 11:28,1.12.4,1.13.2,1.14.0,1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.6,1.14.3,1.15.0,,,Connectors / Cassandra,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19176&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=13995

{code}
[ERROR] Tests run: 17, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 157.28 s <<< FAILURE! - in org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase
[ERROR] testCassandraBatchTupleFormat(org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase)  Time elapsed: 12.052 s  <<< ERROR!
com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /127.0.0.1:9042 (com.datastax.driver.core.exceptions.OperationTimedOutException: [/127.0.0.1] Timed out waiting for server response))
	at com.datastax.driver.core.exceptions.NoHostAvailableException.copy(NoHostAvailableException.java:84)
	at com.datastax.driver.core.exceptions.NoHostAvailableException.copy(NoHostAvailableException.java:37)
	at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37)
	at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:245)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:63)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:39)
	at org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.createTable(CassandraConnectorITCase.java:234)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /127.0.0.1:9042 (com.datastax.driver.core.exceptions.OperationTimedOutException: [/127.0.0.1] Timed out waiting for server response))
	at com.datastax.driver.core.RequestHandler.reportNoMoreHosts(RequestHandler.java:218)
	at com.datastax.driver.core.RequestHandler.access$1000(RequestHandler.java:43)
	at com.datastax.driver.core.RequestHandler$SpeculativeExecution.sendRequest(RequestHandler.java:284)
	at com.datastax.driver.core.RequestHandler$SpeculativeExecution$1.run(RequestHandler.java:406)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}",,dmvk,dwysakowicz,echauchot,gaoyunhaii,mapohl,martijnvisser,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,FLINK-25415,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25147,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 13 13:14:36 UTC 2022,,,,,,,,,,"0|z0s414:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/21 02:25;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21791&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=13994;;;","12/Aug/21 03:17;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21934&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=5d6e4255-0ea8-5e2a-f52c-c881b7872361&l=15090;;;","12/Aug/21 11:52;trohrmann;{{CassandraConnectorITCase.createTable}} fails with a similar exception. https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21962&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=14549;;;","23/Aug/21 02:27;xtsong;{{CassandraConnectorITCase.testDataPersistenceUponMissedNotify}} on 1.12:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22540&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=13647;;;","31/Aug/21 03:38;xtsong;CassandraConnectorITCase.testScalingUp
CassandraConnectorITCase.testIdealCircumstances
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23142&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=60581941-0138-53c0-39fe-86d62be5f407&l=13846;;;","02/Sep/21 01:46;xtsong;CassandraConnectorITCase.testScalingDown
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23344&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51&l=14083;;;","07/Sep/21 06:57;mapohl;CassandraConnectorITCase.createTable
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23606&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=14252;;;","08/Sep/21 12:59;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23759&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=14035;;;","10/Sep/21 02:21;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23877&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51&l=14431;;;","13/Sep/21 02:21;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23956&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=13647;;;","30/Nov/21 05:56;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27229&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11032];;;","01/Dec/21 15:25;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27357&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11433;;;","02/Dec/21 09:43;echauchot;Generally speaking, this error comes with load. I've had it already on another ASF project with embedded Cassandra and I fixed it with retries.
 ;;;","02/Dec/21 09:53;echauchot;[~chesnay] can you assign me this ticket ?;;;","02/Dec/21 09:54;trohrmann;So you mean that we are querying Cassandra too often?;;;","02/Dec/21 09:59;chesnay;The embedded cassandra cluster has always being a massive resource hog. I remember way back when it was added that it would freeze my entire machine If I ran it locally.

Maybe we should look into upgrading our cassandra dependencies first?;;;","02/Dec/21 10:41;echauchot;[~trohrmann] no I mean that the ITest servers might be overloaded some times.;;;","02/Dec/21 10:43;echauchot;[~chesnay] There are alternatives as discussed [here|https://issues.apache.org/jira/browse/FLINK-22775?focusedCommentId=17446552&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17446552]. Achilles project works fine in Apache Beam tests and is ASF V2 licenced. Of course upgrade is to be done as well;;;","02/Dec/21 10:53;echauchot;Do you guys want that we migrate the Cassandra daemon in the tests to Achilles ? It is true that there are a lot of flakiness in this test suite;;;","02/Dec/21 11:36;martijnvisser;[~echauchot] I'm not familiar with Achilles, but given that we're using Testcontainers more and more, would that also be an option? https://www.testcontainers.org/modules/databases/cassandra/;;;","02/Dec/21 12:25;chesnay;I'd prefer testcontainers as well.;;;","02/Dec/21 13:28;echauchot;[~MartijnVisser] yes I proposed testContainers in the other ticket [here|https://issues.apache.org/jira/browse/FLINK-22775?focusedCommentId=17446552&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17446552] so it is definitely an option.  I opened FLINK-25147 for migration to testContainers. That being said, testContainers relies on the official cassandra docker image with the following conf: XMx 1024M and JMX disabled. It may not consume less resources but at least it could improve isolation compared to the current situation and also it has the advantage of not adding an unknown tool. If you want I can tackle the migration.

[~chesnay] can you please:
 * assign me to FLINK-25147
 *  unassign me from FLINK-23047
 *  unassign me from FLINK-24844
 *  for FLINK-22775 leave the assignment as I need to monitor if lowering replica expectancy fixed the flakiness.
 * When testContainers PR is merged we'll see if the flakiness tickets above can be closed.;;;","02/Dec/21 13:32;chesnay;Done.;;;","02/Dec/21 13:42;echauchot;[~chesnay] thx;;;","13/Dec/21 15:59;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28017&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11106];;;","14/Dec/21 09:14;echauchot;OOM really  ? [~gaoyunhaii] thx for pointing out, I guess [migrating to test containers|https://issues.apache.org/jira/browse/FLINK-25147] will help;;;","21/Dec/21 19:01;martijnvisser;As can be seen in the umbrella ticket https://issues.apache.org/jira/browse/FLINK-25147 that Cassandra tests are now using the testcontainers. We're going to monitor the Cassandra tests until the new year and if no new occurences are popping up, we are closing this (and the other subtasks) as resolved;;;","22/Dec/21 08:23;dmvk;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28447&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d;;;","22/Dec/21 08:23;dmvk;Can we disable the test until the issue is resolved?;;;","22/Dec/21 08:43;martijnvisser;[~echauchot] WDYT?;;;","22/Dec/21 09:48;echauchot;Thanks David for pointing out, it's nice to work with you again ! That is exactly the monitoring I was referring to. [~MartijnVisser] We are in the case I mentioned that migration to testContainers is not enough to erase all flakiness. So, I agree, we could disable this test. In the meanTime, I can introduce retrials on connection because this error regularly happens no matter the Cassandra backend we use: I saw that using [Achilles test backend|https://github.com/doanduyhai/Achilles], cassandra daemon and testContainers. So I opened [this ticket|https://issues.apache.org/jira/browse/FLINK-25415];;;","23/Dec/21 08:55;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28502&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=41463ccd-0694-5d4d-220d-8f771e7d098b&l=12056;;;","23/Dec/21 08:56;trohrmann;+1 for disabling this test and re-enabling it after being fixed. [~echauchot] do you want to open a PR for it (with backports for 1.14 and 1.13)?;;;","23/Dec/21 12:57;echauchot;Hi Till, sure! PR ongoing but I did not have time to finish before leaving. I'm on a train I'm off until monday. I'll submit on monday;;;","25/Dec/21 03:16;gaoyunhaii;Very thanks [~echauchot] for taking care this issue! 

Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28551&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=14352;;;","25/Dec/21 03:26;gaoyunhaii;1.13: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28552&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=b6c4efed-9c7d-55ea-03a9-9bd7d5b08e4c&l=13475;;;","25/Dec/21 03:29;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28553&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11924;;;","25/Dec/21 03:34;gaoyunhaii;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28553&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=41463ccd-0694-5d4d-220d-8f771e7d098b&l=11527;;;","27/Dec/21 08:50;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28601&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51;;;","27/Dec/21 09:05;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28602&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=41463ccd-0694-5d4d-220d-8f771e7d098b;;;","27/Dec/21 09:18;trohrmann;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28603&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=12943;;;","27/Dec/21 10:24;echauchot;I disabled this flaky test and backported to 1.13 and 1.14:
https://github.com/apache/flink/pull/18206
https://github.com/apache/flink/pull/18205
https://github.com/apache/flink/pull/18204

I'm preparing a PR to [enable retrials|https://issues.apache.org/jira/browse/FLINK-25415] to fix the flakiness issues;;;","27/Dec/21 10:29;trohrmann;Test disabled via

master: 509530bc79c8b9acce0bc86805956d624fc3f0fa
release-1.14: 457c42fe36b32c8436f6310a825b7f8ec1e8a2e0
release-1.13: 4f81fc60eabe772b2c05f5eff9fea6b01e1e0d28

Bumping the priority of this ticket to blocker in order to not forget about this disabled test.;;;","27/Dec/21 18:49;echauchot;[~trohrmann]I just submitted [the PR for retrials|https://github.com/apache/flink/pull/18211] as all the failures above are linked to NoHostAvailableException. But I sumitted the PR as part of the retrial ticket (FLINK-25415) and not the current ticket because it is more relevant. AFAICT there were several cassandra flakiness:
- the NoHostAvailableException: should be corrected with retrial above: FLINK-25415 (https://github.com/apache/flink/pull/18211)
- the wait for replica timeout: was closed by one of my previous PRs: FLINK-22775 (https://github.com/apache/flink/pull/17849)
- the table already exists exception: might be closed by the migration to testContainers FLINK-25147 (https://github.com/apache/flink/pull/18115) + keyspace drop FLINK-25415 (https://github.com/apache/flink/pull/18211)
;;;","28/Dec/21 10:31;trohrmann;Thanks for the PRs [~echauchot]. Could you maybe link the PRs for the individual points you mentioned. I don't know what the ""above PR"" is, because it refers to keyspace drop whereas one of the PRs adds retries. Due to this, I don't really know when this ticket can be closed.;;;","28/Dec/21 17:22;echauchot;[~trohrmann] sure ! I updated my comment above to add the links to the PRs;;;","29/Dec/21 15:05;trohrmann;I think all the PRs are now merged. Do you think that we can now close this ticket [~echauchot]?;;;","29/Dec/21 17:10;echauchot;[~trohrmann] Yes all the PRs are merged (thanks for that) but I'd prefer to monitor flakiness for a week on all the tickets that depend upon [cassandra test container & retrials (FLINK-25147)|https://issues.apache.org/jira/browse/FLINK-25147] , especially _NoHostAvailableException_ and _table already exists_ error.  Then if no flakiness we could close all of them;;;","30/Dec/21 08:51;martijnvisser;[~echauchot] I think that's fine, I have reduced the priority to a Critical one and I propose that we close it after this week if no issues are occurring anymore.;;;","30/Dec/21 09:13;trohrmann;Sounds like a good plan [~echauchot] :-) Thanks for working so much on hardening the Cassandra tests. This is super helpful for the community!;;;","30/Dec/21 11:22;echauchot;My pleasure ! Thanks guys for the reviews / feedback /merge ...;;;","03/Jan/22 10:31;echauchot;[~trohrmann] [~MartijnVisser] I just checked the status of ci connectors tests on master and so far there was no failure since the last PR was merged on Thursday.  Let's wait until Friday to have to a full business week with less holidays and more load to see if all this tickets were indeed fixed.;;;","03/Jan/22 10:36;martijnvisser;[~echauchot] Sounds good!;;;","13/Jan/22 09:22;echauchot;Hi guys, FYI, I'm checking now the history of failures and I'll will give the status for deciding on closing this ticket. Stay tuned !;;;","13/Jan/22 09:41;echauchot;I see no failure since my last Cassandra PR on retrials (https://github.com/apache/flink/pull/18211) was merged on dec 29th, see report on https://dev.azure.com/apache-flink/apache-flink/_test/analytics?definitionId=1&contextType=build

Please also remember that this PR was also backported to 1.13 and 1.14

[~trohrmann] I think you can close all the cassandra failure tickets that are linked in https://issues.apache.org/jira/browse/FLINK-25147, meaning:
- https://issues.apache.org/jira/browse/FLINK-23047
- https://issues.apache.org/jira/browse/FLINK-24844
- https://issues.apache.org/jira/browse/FLINK-25165
- https://issues.apache.org/jira/browse/FLINK-22739

Cassandra flakiness seems to be over ! :);;;","13/Jan/22 11:28;trohrmann;Fixed via FLINK-25147.;;;","13/Jan/22 11:28;trohrmann;Great news [~echauchot]! Thanks a lot for your help with this issue :-);;;","13/Jan/22 13:14;echauchot;I'm always glad to help !;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RunnablesTest.testExecutorService_uncaughtExceptionHandler fails on azure,FLINK-23045,13384752,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,xtsong,xtsong,21/Jun/21 02:13,23/Sep/21 17:29,13/Jul/23 08:12,25/Jun/21 15:50,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19152&view=logs&j=3b6ec2fd-a816-5e75-c775-06fb87cb6670&t=2aff8966-346f-518f-e6ce-de64002a5034&l=6902
{code}
Jun 18 21:25:48 [ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.402 s <<< FAILURE! - in org.apache.flink.runtime.util.RunnablesTest
Jun 18 21:25:48 [ERROR] testExecutorService_uncaughtExceptionHandler(org.apache.flink.runtime.util.RunnablesTest)  Time elapsed: 0.121 s  <<< FAILURE!
Jun 18 21:25:48 java.lang.AssertionError: Expected handler to be called.
Jun 18 21:25:48 	at org.junit.Assert.fail(Assert.java:89)
Jun 18 21:25:48 	at org.junit.Assert.assertTrue(Assert.java:42)
Jun 18 21:25:48 	at org.apache.flink.runtime.util.RunnablesTest.testExecutorService_uncaughtExceptionHandler(RunnablesTest.java:56)
Jun 18 21:25:48 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jun 18 21:25:48 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jun 18 21:25:48 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jun 18 21:25:48 	at java.lang.reflect.Method.invoke(Method.java:498)
Jun 18 21:25:48 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jun 18 21:25:48 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jun 18 21:25:48 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jun 18 21:25:48 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jun 18 21:25:48 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jun 18 21:25:48 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jun 18 21:25:48 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jun 18 21:25:48 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Jun 18 21:25:48 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Jun 18 21:25:48 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Jun 18 21:25:48 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Jun 18 21:25:48 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Jun 18 21:25:48 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Jun 18 21:25:48 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Jun 18 21:25:48 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jun 18 21:25:48 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Jun 18 21:25:48 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Jun 18 21:25:48 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Jun 18 21:25:48 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Jun 18 21:25:48 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Jun 18 21:25:48 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jun 18 21:25:48 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jun 18 21:25:48 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jun 18 21:25:48 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}",,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 25 15:50:45 UTC 2021,,,,,,,,,,"0|z0s3zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/21 14:23;trohrmann;The test instability looks a bit as if the timeout of 100ms for the count down latch to be decremented is too short for our testing infrastructure. I will remove the timeouts since they are not necessary.;;;","25/Jun/21 15:50;trohrmann;Fixed via

1.14.0: 4d5f4df5361294197cf2b48e000550b0c54adbb0
1.13.2: d9121d701d6a1b1e950ed6d41f5c91d5321477e3
1.12.5: 872a45b7eb60416960d67d34cbacb2f6f5a492dc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Chinese docs Fraud Detection with the DataStream API page picture display issue,FLINK-23042,13384691,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Rollsbean,Rollsbean,Rollsbean,20/Jun/21 04:51,28/Aug/21 12:19,13/Jul/23 08:12,21/Jun/21 05:34,1.13.1,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.1,1.14.0,,,,Documentation,,,,,0,docs,documentation-update,pull-request-available,zh_CN,"Flink chinese docs: [基于 DataStream API 实现欺诈测|[https://ci.apache.org/projects/flink/flink-docs-release-1.13/zh/docs/try-flink/datastream/|https://ci.apache.org/projects/flink/flink-docs-release-1.13/zh/docs/try-flink/datastream/](https://ci.apache.org/projects/flink/flink-docs-release-1.13/zh/docs/try-flink/datastream/)]] picture `fraud-transactions.svg` can't display becase the link is incorrect.

 

*Screenshot* as below:

 

!image-2021-06-20-12-45-36-749.png!

 

*Reason*: the src should be a http link not a path.

!image-2021-06-20-12-48-22-229.png!

 ",,Rollsbean,xtsong,,,,,,,,,,,,,,,,,,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/21 04:45;Rollsbean;image-2021-06-20-12-45-36-749.png;https://issues.apache.org/jira/secure/attachment/13027072/image-2021-06-20-12-45-36-749.png","20/Jun/21 04:48;Rollsbean;image-2021-06-20-12-48-22-229.png;https://issues.apache.org/jira/secure/attachment/13027071/image-2021-06-20-12-48-22-229.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,,Mon Jun 21 05:34:49 UTC 2021,,,,,,,,,,"0|z0s3m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/21 04:55;Rollsbean;Can anyone([~xtsong] [~NicoK] ) assign this issue to me as my first Flink contribution?;;;","21/Jun/21 01:46;xtsong;Hi [~Rollsbean],
Welcome to the Apache Flink community, and thanks for reporting and volunteering to fix this issue. You are assigned. Please go ahead.
FYI, here's the guidelines for new contributors, if you have not already read them.
https://flink.apache.org/contributing/how-to-contribute.html;;;","21/Jun/21 02:50;Rollsbean;Hi [~xintongsong], 

 

Both branch *release-1.13* and *master* have issue, can you tell me which branch I based on?

Also I checked Flink docs How to Contribute am it said :
 * Make sure your change has been rebased to the latest commits in your base branch.

But still confused.

 

Thanks,

Kevin;;;","21/Jun/21 02:59;xtsong;Usually, you only need to open PRs on the master branch. Once the PR is merged, the committer should backport it to other branches if needed.;;;","21/Jun/21 03:45;Rollsbean;Thanks a lot [~xintongsong], PR created, number: #16212, from: RollsBean:FLINK-23042;;;","21/Jun/21 05:34;xtsong;Subsumed by #16162;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken link in Documentation Style Guide,FLINK-23038,13384543,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,daisyt,daisyt,daisyt,18/Jun/21 13:13,18/Jun/21 21:11,13/Jul/23 08:12,18/Jun/21 19:34,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Project Website,,,,,0,pull-request-available,,,,The link to the Flink Glossary is broken here: https://flink.apache.org/contributing/docs-style.html,,daisyt,sjwiesman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 18 19:34:26 UTC 2021,,,,,,,,,,"0|z0s2pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jun/21 19:34;sjwiesman;fixed in: ca9f1ca660c5ca79269dfd3f7a4e041aa0da663f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in JobDetailsDeserializer during the reading old version of ExecutionState,FLINK-23034,13384532,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,akalashnikov,akalashnikov,18/Jun/21 12:18,23/Sep/21 17:26,13/Jul/23 08:12,21/Jun/21 10:18,1.13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"There is no compatibility for ExecutionState:
{noformat}
java.lang.NullPointerException
        at org.apache.flink.runtime.messages.webmonitor.JobDetails$JobDetailsDeserializer.deserialize(JobDetails.java:308)
        at org.apache.flink.runtime.messages.webmonitor.JobDetails$JobDetailsDeserializer.deserialize(JobDetails.java:278)
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:322)
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4593)
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3479)
        at org.apache.flink.runtime.messages.webmonitor.JobDetailsTest.testJobDetailsCompatibleUnmarshalling(JobDetailsTest.java:82)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
        at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
        at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
        at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
        at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
        at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
        at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
        at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:221)
        at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)

{noformat}",,akalashnikov,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17012,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 21 10:18:33 UTC 2021,,,,,,,,,,"0|z0s2mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/21 10:18;chesnay;master: adc9e4125ff97b637ef38ae4071fa17cd5dcfa91
1.13: 36e7ffb597bd830acbf1c9ccc3754d6232138120 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PartitionRequestClientFactory#createPartitionRequestClient should throw when network failure,FLINK-23030,13384468,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jinxing6042@126.com,jinxing6042@126.com,jinxing6042@126.com,18/Jun/21 07:27,23/Sep/21 17:28,13/Jul/23 08:12,21/Jun/21 15:14,1.12.4,1.13.1,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,Runtime / Network,,,,,0,pull-request-available,stale-blocker,,,"In current _PartitionRequestClientFactory#createPartitionRequestClient_, _ChannelFuture#await()_ is invoked, thus to build a connection to remote synchronously.

But with the doc of _io.netty.util.concurrent.Future_ [1] and its implementation _io.netty.channel.DefaultChannelPromise_ [2], _ChannelFuture#await()_ never throws when completed with failure. I guess what Flink needs is _ChannelFuture#sync()._

[1]  [https://netty.io/4.1/api/io/netty/util/concurrent/class-use/Future.html]

[2] [https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/DefaultChannelPromise.java]

      https://github.com/netty/netty/blob/4.1/common/src/main/java/io/netty/util/concurrent/DefaultPromise.java",,aitozi,jinxing6042@126.com,pnowojski,Thesharing,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24133,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 21 15:14:38 UTC 2021,,,,,,,,,,"0|z0s28w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jun/21 22:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Jun/21 15:14;pnowojski;Thanks for reporting and fixing the bug!

Merged to master f46cfecf027
Merged to release-1.13 d0d40718c1e
Merged to release-1.12 38bf63b2fbe;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sink-buffer-max-rows and sink-buffer-flush-interval options produce a lot of duplicates,FLINK-23025,13384385,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fsk119,joemoe,joemoe,17/Jun/21 15:31,24/Jun/21 09:23,13/Jul/23 08:12,24/Jun/21 09:23,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Connectors / Kafka,Table SQL / Ecosystem,,,,0,pull-request-available,,,,"Using the [sink-buffer-flush-max-rows|https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/connectors/table/upsert-kafka/#sink-buffer-flush-interval] and [sink-buffer-flush-interval|https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/connectors/table/upsert-kafka/#sink-buffer-flush-interval] options for a kafka sink produces a lot of duplicate key/values in the target kafka topic. Maybe the {{BufferedUpsertSinkFunction}} should clone the buffered key/value RowData objects, but it doesn’t. Seems like in [line 134|https://github.com/apache/flink/blob/60c7d9e77a6e9d82e0feb33f0d8bc263dddf2fd9/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/BufferedUpsertSinkFunction.java#L133-L137] the condition should be negated or the ternary operator results swapped:
{code:java}
this.valueCopier =
 getRuntimeContext().getExecutionConfig().isObjectReuseEnabled()
 ? Function.identity()
 : typeSerializer::copy;{code}

(in the jdbc sink the same logic is done but the ternary operator results swapped)

 ",,fsk119,jark,joemoe,knaufk,leonard,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 24 02:28:23 UTC 2021,,,,,,,,,,"0|z0s1qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jun/21 06:04;jark;Thanks [~joemoe] for reporting this. Yes, I think this is a bug. ;;;","18/Jun/21 06:05;jark;[~fsk119], do you have time to have a look?;;;","24/Jun/21 02:28;jark;Fixed in 
 - master: 6defc99cabb733a4480f2664dbf10a1bb3cdc4e3
 - release-1.13: 28e1f7de58aa534efb533b5de42d2a78f4c4dd96;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RPC result TaskManagerInfoWithSlots not serializable,FLINK-23024,13384374,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,arvid,arvid,17/Jun/21 14:24,30/Nov/21 20:37,13/Jul/23 08:12,18/Jun/21 10:20,1.13.1,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Runtime / REST,,,,,0,pull-request-available,,,,"A user reported the following stacktrace while accessing web UI.


{noformat}
Unhandled exception.
org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcException: Failed
to serialize the result for RPC call : requestTaskManagerDetailsInfo.
at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.serializeRemoteResultAndVerifySize(AkkaRpcActor.java:404)
~[flink-dist_2.11-1.13.1.jar:1.13.1] at
org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$sendAsyncResponse$0(AkkaRpcActor.java:360)
~[flink-dist_2.11-1.13.1.jar:1.13.1] at
java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
~[?:1.8.0_251] at
java.util.concurrent.CompletableFuture.uniHandleStage(CompletableFuture.java:848)
~[?:1.8.0_251] at
java.util.concurrent.CompletableFuture.handle(CompletableFuture.java:2168)
~[?:1.8.0_251] at
org.apache.flink.runtime.rpc.akka.AkkaRpcActor.sendAsyncResponse(AkkaRpcActor.java:352)
~[flink-dist_2.11-1.13.1.jar:1.13.1] at
org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:319)
~[flink-dist_2.11-1.13.1.jar:1.13.1] at
org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
~[flink-dist_2.11-1.13.1.jar:1.13.1] at
org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
~[flink-dist_2.11-1.13.1.jar:1.13.1] at
org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
~[flink-dist_2.11-1.13.1.jar:1.13.1] at
akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
~[flink-dist_2.11-1.13.1.jar:1.13.1] at
akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
~[flink-dist_2.11-1.13.1.jar:1.13.1] at
scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
~[flink-dist_2.11-1.13.1.jar:1.13.1] at
akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
~[flink-dist_2.11-1.13.1.jar:1.13.1] at
scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
~[flink-dist_2.11-1.13.1.jar:1.13.1] at
scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
~[flink-dist_2.11-1.13.1.jar:1.13.1] at
scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
~[flink-dist_2.11-1.13.1.jar:1.13.1] at
akka.actor.Actor$class.aroundReceive(Actor.scala:517)
~[flink-dist_2.11-1.13.1.jar:1.13.1] at
akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
~[flink-dist_2.11-1.13.1.jar:1.13.1] at
akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
[flink-dist_2.11-1.13.1.jar:1.13.1] at
akka.actor.ActorCell.invoke(ActorCell.scala:561)
[flink-dist_2.11-1.13.1.jar:1.13.1] at
akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
[flink-dist_2.11-1.13.1.jar:1.13.1] at
akka.dispatch.Mailbox.run(Mailbox.scala:225)
[flink-dist_2.11-1.13.1.jar:1.13.1] at
akka.dispatch.Mailbox.exec(Mailbox.scala:235)
[flink-dist_2.11-1.13.1.jar:1.13.1] at
akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
[flink-dist_2.11-1.13.1.jar:1.13.1] at
akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
[flink-dist_2.11-1.13.1.jar:1.13.1] at
akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
[flink-dist_2.11-1.13.1.jar:1.13.1] at
akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[flink-dist_2.11-1.13.1.jar:1.13.1] Caused by:
java.io.NotSerializableException:
org.apache.flink.runtime.resourcemanager.TaskManagerInfoWithSlots at
java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184)
~[?:1.8.0_251] at
java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
~[?:1.8.0_251] at
org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:624)
~[flink-dist_2.11-1.13.1.jar:1.13.1] at
org.apache.flink.runtime.rpc.akka.AkkaRpcSerializedValue.valueOf(AkkaRpcSerializedValue.java:66)
~[flink-dist_2.11-1.13.1.jar:1.13.1] at
org.apache.flink.runtime.rpc.akka.AkkaRpcActor.serializeRemoteResultAndVerifySize(AkkaRpcActor.java:387)
~[flink-dist_2.11-1.13.1.jar:1.13.1] ... 27 more
{noformat}
",,guoyangze,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21794,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 18 10:20:57 UTC 2021,,,,,,,,,,"0|z0s1o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/21 14:25;arvid;CC [~guoyangze];;;","18/Jun/21 10:20;trohrmann;Fixed via

master: 1a3c797ea3f20efff9246d30c76fbdd6a45e9030
1.13.2: 9f90d4b31f1e2316b0e01a10cb0d727ab647677e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
State factories should handle extended state descriptors,FLINK-23018,13384301,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,17/Jun/21 08:21,28/Aug/21 12:19,13/Jul/23 08:12,21/Jun/21 02:56,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"Currently, {{TtlStateFactory}} and other state factories can only handle fixed type of state descriptors. As {{ValueStateDescriptor}} is not a final class and user could still extend it, however, {{TtlStateFactory}} cannot recognize the extending class.

 {{TtlStateFactory}} should use {{StateDescriptor#Type}} to check what kind of state is.",,wind_ljy,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 21 02:56:06 UTC 2021,,,,,,,,,,"0|z0s17s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/21 02:56;yunta;Merged
master: 73c103b6b117fe3996eedfb9d04e926f00c70996
release-1.13: b4f1a41b1c7564f4966567fbaf6e47c390874700;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HELP in sql-client still shows the removed SOURCE functionality,FLINK-23017,13384299,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,nicholasjiang,DanielLenz,DanielLenz,17/Jun/21 08:12,23/Sep/21 17:54,13/Jul/23 08:12,30/Jun/21 08:33,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"The sql-client still shows the SOURCE command in HELP, even though the command itself doesn't exist anymore and using it causes an error.

 
{code:java}
/opt/flink# echo ""select 'hello';"" > test.sql
/opt/flink# sql-client.sh

Flink SQL> select 'hello';
-- works as intended
[INFO] Result retrieval cancelled.

Flink SQL> source test.sql;
[ERROR] Could not execute SQL statement. Reason: org.apache.calcite.runtime.CalciteException: Non-query expression encountered in illegal context
{code}
 

 I'd be happy to create a PR and remove the relevant lines from the HELP.",,DanielLenz,jark,nicholasjiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 30 08:33:02 UTC 2021,,,,,,,,,,"0|z0s17c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jun/21 04:26;nicholasjiang;[~jark], I would like to take this ticket and could you please assign this issue to me?;;;","30/Jun/21 08:33;jark;Fixed in master: 27bda7349abfda222db6414c6b73b50739951ae3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-27 sources are generating non-deterministic results when using event time,FLINK-23011,13384131,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dwysakowicz,pnowojski,pnowojski,16/Jun/21 10:10,24/Jun/21 10:15,13/Jul/23 08:12,22/Jun/21 16:30,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,API / DataStream,,,,,0,,,,,"FLIP-27 sources currently start in the {{StreamStatus.IDLE}} state and they switch to {{ACTIVE}} only after emitting first {{Watermark}}. Until this happens, downstream operators are ignoring {{IDLE}} inputs from calculating the input (min) watermark. 

An extreme example to what problem this leads to, are completely bogus results if for example one FLIP-27 source subtask is slower than others for some reason:
{code:java}
env.getConfig().setAutoWatermarkInterval(2000);
env.setParallelism(2);
env.setRestartStrategy(RestartStrategies.fixedDelayRestart(Integer.MAX_VALUE, 10));

DataStream<Long> eventStream =
        env.fromSource(
                        new NumberSequenceSource(0, Long.MAX_VALUE),
                        WatermarkStrategy.<Long>forMonotonousTimestamps()
                                .withTimestampAssigner(new LongTimestampAssigner()),
                        ""NumberSequenceSource"")
                .map(
                        new RichMapFunction<Long, Long>() {
                            @Override
                            public Long map(Long value) throws Exception {
                                if (getRuntimeContext().getIndexOfThisSubtask() == 0) {
                                    Thread.sleep(1);
                                }
                                return 1L;
                            }
                        });

eventStream.windowAll(TumblingEventTimeWindows.of(Time.seconds(1))).sum(0).print();

(...)
private static class LongTimestampAssigner implements SerializableTimestampAssigner<Long> {
    private long counter = 0;

    @Override
    public long extractTimestamp(Long record, long recordTimeStamp) {
        return counter++;
    }
}
{code}
In such case, after 2 seconds ({{setAutoWatermarkInterval}}) the not throttled subtask (subTaskId == 1) generates very high watermarks. The other source subtask (subTaskId == 0) emits very low watermarks. If the non throttled watermark reaches the downstream {{WindowOperator}} first, while the other input channel is still idle, it will take those high watermarks as combined input watermark for the the whole {{WindowOperator}}. When the input channel from the throttled source subtask finally receives it's {{ACTIVE}} status and a much lower watermark, that's already too late.

Actual output of the example program:
{noformat}
1596
2000
1000
1000
1000
1000
1000
1000
(...)
{noformat}
while the expected output should be always ""2000"" (2000 records fitting in every 1 second global window)
{noformat}
2000
2000
2000
2000
(...)
{noformat}.
",,dwysakowicz,libenchao,liyu,pnowojski,sewen,trohrmann,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22926,,,,FLINK-22890,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 24 10:15:35 UTC 2021,,,,,,,,,,"0|z0s068:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/21 10:17;pnowojski;Even after fixing FLINK-22926 the problem will persist, as there still would be a race condition between registering splits/switching to ACTIVE and emitting watermarks between different subtasks.

Hotfix might be to force FLIP-27 source to start ACTIVE (as the legacy sources are doing). However this doesn't work if there will be more then one split assigned to single {{SourceReader}}, and there can be a delay between assigning first and second split.

The problem is that if source doesn't know about some splits, because they haven't been yet assigned (or even discovered), it can not know what watermarks would result from those unknown splits. This gets more visible if you think about some source, where discovering splits takes long time, for example some {{FileSource}}.

I think the proper solution should be something like {{SplitEnumerator}} emitting it's own watermarks, that would be capping/combined with the watermarks emitted from the sources.;;;","16/Jun/21 11:22;pnowojski;Let me elaborate on {{FileSource}} example. Let's say we have a bucketed file source where, split equals to a single file, with buckets (directories?) created per each hour. New files/buckets can be appearing as you go. 

If bucket for [12:00, 13:00) is committed, {{SplitEnumerator}} could emit capped watermark for 13:00, and the already assigned splits will be bumping the current watermarks until this cap, as the splits are being read. Then next bucket [13:00, 14:00) is created, {{SourceReaders}} could even start reading files from this bucket before it’s fully committed, but the watermark cap is bumped only when this next bucket is committed.

It's also important in this case to automatically switch {{SourceReader}} to idle if they don't have assigned splits. As for whatever the reason, for some buckets there can be fewer splits than parallel instances of the {{SourceReader}}s. In this case you need idleness to make progress.;;;","16/Jun/21 11:30;pnowojski;[~AHeise] suggested that as a hotfix we could block emitting watermarks/switching to {{IDLE}} until all splits are assigned ({{SourceReader#notifyNoMoreSplits}}). This would be a very special case of the more general solution that I was proposing with {{SplitEnumerator}} capping the watermarks. This hotfix should work well in the cases where number of splits is determined/known from the beginning.;;;","22/Jun/21 06:30;dwysakowicz;After some investigation of FLINK-22926 I realized I accidently changed the behaviour for situation when there are no splits assigned. In 1.13 and older if there were no {{PartialWatermarks}} in {{WatermarkOutputMultiplexer}} we were not emitting the {{StreamStatus.IDLE}}.

I tried reverting that behaviour in: https://github.com/apache/flink/pull/16221/commits/1110cf3a53f6ccb7d09f18347ccb92f8cb346b8a
I emit {{StreamStatus.IDLE}} only if there are no splits, but the {{Watermark}} has progressed. If it is at its initial value and there are no splits assigned yet, we do not emit {{StreamStatus}}. Effectively that implements the workaround suggested by [~AHeise].;;;","22/Jun/21 07:15;pnowojski;Ok, that's a good news. As I understand it means this issue doesn't affect any previous release, just the master branch?;;;","22/Jun/21 07:42;dwysakowicz;Yes, that's correct. In past releases the FLIP-27 were ACTIVE if there were no splits.;;;","22/Jun/21 16:30;dwysakowicz;Fixed in e0614e75ddec48129c30a256a4bcb70a3f8951c2;;;","23/Jun/21 09:52;sewen;Some thoughts on the whole Idleness business and the cases described here:

*(1) About Idleness*

Idleness inherently circumvents the core event-time mechanism. The temporary unavailability of events (data) in a partition means we cannot make any statement about event time progress (which is data-driven).

However, real-world setups have the need to handle such situations (absence of data). But the behavior for that situation is very dependent on the specific data characteristics of the stream, and can really only ever be defined by a user.

Therefore, I think we should have those two core rules for Idleness:

    (a) Idleness should always be a user-defined thing, coming from the configured watermark strategies. The system should never trigger idleness by itself.

    (b) Idleness should always refer to the situation where a source has work (splits assigned) but no data (empty partition). We should not use it in a situation where a source has no work, because in such situations there is no way to define idleness relative to stream activity (ensure that idleness is only triggered when we are sure to have fallen behind by at least X time).

This means that the FLIP-27 readers should start in stat ""active"" and never switch to ""idle"" by themselves, but only when the overall watermark status (merged from all partition-specific statuses) is idle.

*(2) About Global Watermarks*

There is currently a lack of expressiveness in the watermark system. We cannot handle the cases well where splits are in the backlog, not assigned to any reader. That is a problem most prominently for the file source at the moment, but also foreseeable for other sources.

The problem is that Readers would signal watermark make progress based on their local view, lacking the knowledge that more splits are in the backlog.

I think that Idleness is not the right mechanism to deal with this. What we need here is a global ""watermark holdback"" that is handled by the coordinator. Meaning the coordinator signals the readers how far they may advance their watermarks at most (based on the split backlog).

For long backlogs of splits, this means that watermark alignment is inherently off and state is going to be large. But that is inherent in the nature of the way such a source generates data. We can fix that in two ways, though:

  - Either change the source to assign all splits immediately and read them concurrently and throttle progress on individual splits based on watermark alignment (I think this will be not super efficient)

  - Or advance the functionality in mixed batch/streaming execution: As long as there is a backlog of splits, execute in batch, then switch to streaming once all initial splits were processed and the source is in discovery mode for new splits. The batch execution is not sensitive to state size, which is why we don't need to worry about the drawbacks of the global watermark holdback.;;;","23/Jun/21 10:12;arvid;Thanks for sharing your thoughts.

I agree with 1(a) but I'm torn on (b).

If we don't use idleness in (b), we would not advance watermarks in cases where you have fewer partitions than readers. That can happen permanently (fewer kafka partitions than readers) or temporarily (kinesis currently has fewer shards but that can change later). So I think we should also use idleness here but it would be nice to differentiate the cause of idleness. 

I agree that we should never switch idle, active in the general framework and leave it to the source implementation. The only exception is readers that received `noMoreSplits` and drained all their splits; but that is effectively covered with END_OF_INPUT and subsequent closing of the reader.

For 2). I think we see the same fundamental design issue with the current idleness that is decided per reader.

Piotr and I were doing a bit of brainstorming and were thinking that a complete alternative to the current solution would be to always generate watermarks on the sources and in the case of idleness let the enumerator generate the watermarks. In certain systems, it may be easier to calculate a lower bound and propagate it to all readers.

That would also allow the enumerator to not advance the watermark when it knows that splits are still in the backlog and cover situation where partitions are dynamically split/merged. I think this pretty much aligns with your idea.

For me, the big question is if we need then idleness in the current form at all. If the enumerator of all sources have a meaningful way to advance the watermark and all source tasks emit these watermarks, we could get rid of StreamStatus and simplify the whole logic. I'm just not sure if we can assume that all sources could implement such a behavior. I'm especially struggling with continuous FileSource but it feels like watermarks are not terrible useful here anyways.;;;","23/Jun/21 13:36;sewen;*About 1(b):*

In my understanding, that is covered by the following mechanism, which we already have: When the reader gets the {{noMoreSplits}} event and has finished all currently assigned splits, it emits a LONG_MAX watermark, thus fully and permanently unblocking the downstream eventtime progress. Having fewer partitions than readers (e.g. in Kafka) means that some readers get the end-of-partition event directly and not ever getting any partition. That would be fine.

This admittedly isn't working well with Kafka partition discovery, in which case there would never be a {{noMoreSplitsEvent}}. Then again, partition discovery breaks a lot of things (like key partition and order guarantees).

I guess to make this case work, we need something like coordinator-triggered idleness, that is the only thing I can see working well. The Kafka Source's split enumerator would see that it didn't assign splits to a reader, and when being in split discovery mode, it would send a ""go idle"" event to that reader. The reader alone cannot make that decision.

*About 2*:

I kind of think about it in the exact opposite way. I agree that idleness is not per reader, but it is per split (partition), and the main purpose of it is to cover the case where a long-living split (like a Kafka or Kinesis partition/segment) have no data temporarily.
To my understanding, the Kafka and Kinesis case are the most common ones that we have to handle well.
I don't see how that can be covered be delegating the watermark assignment back to the enumerator.

We could solve this without idleness by always communicating the watermark back from the reader to the enumerator, merging it there, and then rebroadcasting it from there. But that seems pretty involved and inefficient in the common case. Maybe I am overlooking something there.

What I agree with is that we shouldn't be thinking about Idleness in the source design, that was what I was trying to motivate in the previous comment. Idleness is purely in the space of a partition-local Watermark Generator and only handled between WatermarkGenerators and downstream operators. The source framework never gets involved with idelness at all.;;;","23/Jun/21 14:33;arvid;Re 1b) I agree that we already have it for static assignments. My main concern is that all more recent streaming storages seem to go into dynamic partitions (Kinesis, Pulsar, Pravega). Similarly, work-stealing is a long-term goal of FLIP-27. So having a solution that just works for Kafka with static assignment for now feels unsatisfying to me.

Furthermore, with HybridSources, the issue is more pronounced. Think of a job that reads from iceberg and switches to Kafka, now some iceberg splits may be larger and take longer to ingest. At that time, the idle readers have no iceberg splits but can't close either. So we would have no watermarks until the switch to Kafka actually happens.

Re 2) I like the idea of thinking in splits that would certainly also help with the unrelated event time alignment. As stated above, I don't agree on applying it only to long-living split. What happens when a storage system simply closes unused partitions after some time and reopens them when they are used finally?

Watermark assignment would not happen through enumerator; just watermark generation. A reader goes idle and that tells the enumerator that it needs to generate suitable lower bounds. The enumerator periodically queries the source system for a low watermark and then sends it to ALL idle readers which in turn emit it. We could do the same just on reader level but you could easily DDoS the source system for a high degree of parallelism.;;;","23/Jun/21 16:51;sewen;The dynamic partitioning case is certainly good to keep in mind. I was thinking about that in a similar way as for the file source:
  - It means that we don't yet know all splits
  - Splits get discovered over time
  - But we can deduce (from metadata) the event time ranges of splits.

A Kinesis / Pulasr / Pravega source could also use the global watermark holdback: It would set the global watermark to the lowest value for which all overlapping splits (segments) are already assigned. So instead of emitting LONG_MAX, the empty reader would emit the watermark corresponding to the global holdback.

As a general mechanism, this would have kind of this shape:
  - When an enumerator starts, the global watermark holdback is LONG_MIN
  - Sources that eagerly discover and assign their splits (like Kafka w/o partition discovery) would set the global holdback to LONG_MAX immediately after assigning splits and sending the noMoreSplits.
  - Sources like the FileSource (or Kinesis/Pulasr/Pravega) would eagerly assign the first set of splits, then set the global watermark holdback the start of the earliest remaining split. With each next split assignment, it would advance the global watermark holdback to the start of the lowest remaining split.

I think this is a bit nicer than going with idleness, because idleness semantically advances the watermark (locally) and the pulls it back, hoping that everything is still alright (that's why it is something I would only let users do explicitly, not the system).
In contrast this approach advances the watermark step by step, never too far (even locally). I think that makes it somewhat easier to understand and well defined. ;;;","24/Jun/21 06:20;arvid;I think we a have a large agreement here. I'd open a FLIP but I still would like some clarification before that:
 * Does that mean, we can completely remove StreamStatus and related mechanisms? Effectively, watermark aggregation is really as simple as it gets: you take the min of all non-finished inputs. If all inputs are finished, watermark is LONG_MAX.
 * For multiple sources: completely drained sources emit LONG_MAX and finish, so they don't participate at all in watermark assignment.
 * For multiple sources: if a source is completely idle but not yet finished (bursty payloads), we may need to invent a watermark out of thin air. I think the best course in your described setup is to have idleness in the watermark generator on the enumerator that periodically advances the global watermark holdback. This is again approximate and we may be able to avoid it for sources with built-in watermarks in the first place but this is also explicitly user-defined and probably cannot be avoided.

From an architecture's point of view, we would shift idleness and implicit watermark generation completely into the source enumerator and hide it from the downstream tasks. That gives source implementers much more freedom on the cost of having more complexity. We can solve the latter by providing building blocks and documentation.;;;","24/Jun/21 10:15;sewen;I am not convinced that we should remove idleness. Idleness effectively says to temporarily advance a reader at the speed of other readers, due to a lack of better information. To provide a similar replacement, we would need to communicate watermarks from readers back to coordinators and back to other readers. That sounds a lot more complex.

I would like to understand why there is such a strong desire to remove idleness. My understanding is that the idleness mechanism in the operators is pretty isolated/lightweight and doesn't add much complexity. Any added mechanism in the readers and coordinators that tries to invent a reader's watermark based on looking at other readers would easily be more complicates and less scalable and responsive.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HivePartitionFetcherContextBase::getComparablePartitionValueList can return partitions that don't exist,FLINK-23010,13384114,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,16/Jun/21 08:59,25/Jun/21 09:53,13/Jul/23 08:12,25/Jun/21 09:53,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,Connectors / Hive,,,,,0,pull-request-available,,,,"When consume order is {{CREATE_TIME}}, HivePartitionFetcherContextBase lists folders under table location to find partitions. This is wrong because HMS is the single source of truth to decide whether a partition exists for a hive table. Returning a non-existing partition will lead to unnecessary job FO when {{HiveContinuousPartitionFetcher}} fails to get the partition from HMS.",,leonard,lirui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 25 09:53:59 UTC 2021,,,,,,,,,,"0|z0s02g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jun/21 09:53;lirui;Fixed in:
* master: 53034eacc498d79f08ae17d8c941b4931e619351
* release-1.13: 86581e90fb59289c4ad4be24495a9895d1099176
* release-1.12: 3a7552a3db8d4437a39f128b0a89470e9e2e7750;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resource leak in RocksIncrementalSnapshotStrategy,FLINK-23003,13384029,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yanfei Lei,Yanfei Lei,Yanfei Lei,16/Jun/21 02:16,28/Aug/21 12:18,13/Jul/23 08:12,17/Jun/21 15:33,1.12.4,1.13.1,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Runtime / Checkpointing,Runtime / State Backends,,,,0,pull-request-available,,,,"We found that `RocksDBStateUploader` in `RocksIncrementalSnapshotStrategy` is not closed correctly after being used. It would lead to a resource leak.

`RocksDBStateUploader` inherits `RocksDBStateDataTransfer`, and `RocksDBStateDataTransfer` holds an `ExecutorService`. `RocksDBStateUploader` uses the `ExecutorService` to upload files to DFS asynchronously.

When `RocksDBKeyedStateBackend` is cleaned up, all resources held by the backend should be closed, but now `RocksIncrementalSnapshotStrategy` lacks a close() function.

And we encountered an example caused by this problem. When we benchmarked the performance of incremental rescaling, we observed that the forked VM of JMH can't exit normally.
{code:java}
[INFO]
[INFO] --- exec-maven-plugin:1.6.0:exec (default-cli) @ benchmark ---
# JMH version: 1.19
# VM version: JDK 1.8.0_281, VM 25.281-b09
# VM invoker: /home/leiyanfei.lyf/jdk1.8.0_281/jre/bin/java
# VM options: -Djava.rmi.server.hostname=127.0.0.1 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.ssl
# Warmup: 10 iterations, 1 s each
# Measurement: 10 iterations, 1 s each
# Timeout: 10 min per iteration
# Threads: 1 thread, will synchronize iterations
# Benchmark mode: Average time, time/op
# Benchmark: org.apache.flink.state.RocksIncrementalCheckpointScaleUpBenchmark.ScalingUp
# Parameters: (numberElements = 100, parallelism1 = 2, parallelism2 = 3)# Run progress: 0.00% complete, ETA 00:02:00
# Fork: 1 of 3
# Warmup Iteration   1: 244.717 ms/op
# Warmup Iteration   2: 104.749 ms/op
# Warmup Iteration   3: 104.182 ms/op
...
Iteration   1: 96.600 ms/op
Iteration   2: 108.463 ms/op
Iteration   3: 93.657 ms/op
...<JMH had finished, but forked VM did not exit, are there stray running threads? Waiting 24 seconds more...>Non-finished threads:
...
Thread[pool-15-thread-4,5,main]
  at sun.misc.Unsafe.park(Native Method)
  at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
  at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
  at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
  at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)
<shutdown timeout of 30 seconds expired, forcing forked VM to exit>{code}
 
The root cause of this example is that the `{{RocksDBStateUploader}}` in `{{RocksIncrementalSnapshotStrategy`}} is not closed normally when `{{RocksDBKeyedStateBackend`}} is disposed.

 

The solution to this problem is quite straightforward, `{{RocksDBStateUploader`}} in `{{RocksIncrementalSnapshotStrategy}}` can be closed when cleaning up `{{RocksDBKeyedStateBackend}}`.",Flink: 1.14-SNAPSHOT,roman,wind_ljy,Yanfei Lei,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22886,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 17 15:32:53 UTC 2021,,,,,,,,,,"0|z0rzjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/21 02:40;wind_ljy;[~Yanfei Lei] This looks like a duplicate of https://issues.apache.org/jira/browse/FLINK-22886. The issue is already in-progress and I'm sure there would be a PR in two days.;;;","16/Jun/21 02:51;ym;cc [~yunta] [~roman_khachatryan]

would you please take a review?;;;","16/Jun/21 08:42;roman;Sure, reviewed, thanks for pulling me in.;;;","17/Jun/21 07:23;roman;Merged into master as 8be1058a60565587b465a2237136dbbbb4c168f3.

It should also be ported to 1.13 according to the [Update Policy|https://flink.apache.org/downloads.html#update-policy-for-old-releases].

[~Yanfei Lei] would you like to open a PR to backport it to 1.13?;;;","17/Jun/21 07:51;Yanfei Lei;Sure, opened.  [https://github.com/apache/flink/pull/16176]

[~roman_khachatryan] would you please take a review again?;;;","17/Jun/21 15:32;roman;Sure, reviewd and merged into 1.13 as 84578222cc84537c2f014573706bf070d3da49da.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-avro-glue-schema-registry lacks scala suffix,FLINK-23001,13383966,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sewen,chesnay,chesnay,15/Jun/21 15:48,23/Jun/21 13:05,13/Jul/23 08:12,23/Jun/21 13:05,1.13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,The dependency on flink-streaming-java implies a need for a scala suffix.,,sewen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22987,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 23 13:05:44 UTC 2021,,,,,,,,,,"0|z0rz5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jun/21 22:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Jun/21 07:04;chesnay;The module has been temporarily excluded from the scala suffix check in FLINK-22987.;;;","23/Jun/21 08:50;sewen;The {{flink-streaming-java}} dependency isn't needed at all. Removing it means the module doesn't need any Scala suffix any more.;;;","23/Jun/21 13:05;sewen;Fixed in 1.14.0 (master) via
  - 49856728fbfafbf4e07e0e5271b0f4bf8a6d1471
  - 10146366bec7feca85acedb23184b99517059bc6

Fixed in 1.13.2 (release-1.13) via
  - 51880ee4d17e9042f9d81059c7e83b6f6b53b948
  - 728bb3309b74cf6457c5d0b962018a45e470ed05;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompactFileWriter won't emit EndCheckpoint with Long.MAX_VALUE checkpointId,FLINK-22993,13383867,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,15/Jun/21 06:53,23/Sep/21 17:26,13/Jul/23 08:12,21/Jun/21 06:26,1.13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Connectors / FileSystem,Connectors / Hive,,,,0,pull-request-available,,,,"CompactFileWriter won't emit EndCheckpoint with Long.MAX_VALUE checkpointId even though the inputs end. 

It will cause data lose. For example, after completing checkpoint 5, CompactFileWriter write some file, then the inputs end, but it won't emit EndCheckpoint  with Long.MAX_VALUE checkpointId, so the downstream operator won't do the compaction for the files which means these files are invisible.",,luoyuxia,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22073,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 21 06:26:47 UTC 2021,,,,,,,,,,"0|z0ryjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/21 06:54;luoyuxia;I'd like to fix it.;;;","15/Jun/21 07:06;luoyuxia;[~lzljs3620320] Look forward your review :).;;;","21/Jun/21 06:26;lzljs3620320;Fixed via:

master: 046d9b683e04c77671fe96e0a5fb20b31ed4436e

release-1.13: d8157b960e6d8d9d4903daf49b8d76d91f27dac9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala suffix check isn't working,FLINK-22987,13383855,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,15/Jun/21 06:07,23/Sep/21 17:26,13/Jul/23 08:12,21/Jun/21 07:07,1.12.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,Build System / CI,,,,,0,pull-request-available,,,,"The scala suffix check (tools/ci/verify_scala_suffixes.sh) isn't working because:
* /dev/tty isn't available
* the module names introduced in FLINK-18607 broke the parsing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18607,,,,FLINK-23001,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 21 07:03:34 UTC 2021,,,,,,,,,,"0|z0rygw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/21 07:03;chesnay;master: 11eb30e81fc3380d0131341a543a34236201c573
1.13: fdabee0d3720f6d1690a7df7fb9329d266b49add
1.12: 2b26df9343640a91cc310d8b2818cf15323716f7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException when cast string literal to date or time,FLINK-22985,13383835,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,simen,simen,15/Jun/21 02:59,28/Aug/21 12:24,13/Jul/23 08:12,28/Jul/21 11:24,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"sql:
{code:java}
CREATE TABLE source_table
(
    id               INT,
    score            INT,
    address          STRING,
    create_time      TIME,
    create_date      DATE,
    create_timestamp TIMESTAMP
) WITH (
      'connector' = 'datagen'
      );

CREATE TABLE console_table
(
    create_time      TIME,
    create_date      DATE,
    create_timestamp TIMESTAMP
) WITH (
      'connector' = 'print'
      );

INSERT INTO console_table
SELECT CASE
           WHEN A.create_time IS NULL
               OR A.create_time = '' THEN CURRENT_TIME
           ELSE A.create_time
           END
           AS create_time,
       CASE
           WHEN A.create_date IS NULL
               OR A.create_date = '' THEN CURRENT_DATE
           ELSE A.create_date
           END
           AS create_date,
       CASE
           WHEN A.create_timestamp IS NULL
               OR A.create_timestamp = '' THEN CURRENT_TIMESTAMP
           ELSE A.create_timestamp
           END
           AS create_timestamp
FROM source_table A;
{code}
exception:
{code:java}
java.lang.RuntimeException: Could not instantiate generated class 'StreamExecCalc$23'java.lang.RuntimeException: Could not instantiate generated class 'StreamExecCalc$23' at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:66) at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40) at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:80) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperator(OperatorChain.java:652) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:626) at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:566) at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:181) at org.apache.flink.streaming.runtime.tasks.StreamTask.executeRestore(StreamTask.java:548) at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:647) at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:537) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:759) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.GeneratedConstructorAccessor27.newInstance(Unknown Source) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:64) ... 12 moreCaused by: java.lang.NullPointerException at StreamExecCalc$23.<init>(Unknown Source) ... 16 more
{code}",,hackergin,jark,libenchao,lzljs3620320,simen,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 28 11:24:22 UTC 2021,,,,,,,,,,"0|z0rycg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/21 03:01;simen;[~jark] hello jark , Is my grammar wrong？;;;","15/Jun/21 03:13;jark;[~simen] looks like a bug in code generation. Could you help to have a look [~lzljs3620320]?;;;","15/Jun/21 03:46;lzljs3620320;Thanks [~simen] for the reporting, There is bug in {{ScalarOperatorGens.generateCastStringLiteralToDateTime}} , it does not consider cast return null.;;;","09/Jul/21 07:34;TsReaper;Hi [~simen], thanks for raising this problem. As mentioned by [~lzljs3620320] this is indeed a bug and I'll take this issue.

However I have one doubt about this SQL code. As {{create_time = ''}} is always false, what would you like to express by this SQL?;;;","12/Jul/21 02:52;lzljs3620320;Thanks [~TsReaper];;;","12/Jul/21 03:22;TsReaper;After researching on the behavior of some databases, I found that postgre throws an exception when compiling, notifying the user that their given string has an invalid time format. [~lzljs3620320] and I both agree that this is the most suitable behavior so I'm going to fix this issue by throwing a proper exception.;;;","28/Jul/21 11:24;lzljs3620320;master: 2b7e60922987814f2c1166d64625e21983261f8b
release-1.13: 87211c63d888a6df029122b45ef4428a1c637d4e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsupportedOperationException when using Python UDF to generate watermark,FLINK-22984,13383795,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Juntao Hu,maver1ck,maver1ck,14/Jun/21 19:54,22/Apr/22 14:40,13/Jul/23 08:12,22/Apr/22 14:40,1.13.0,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.7,1.14.5,1.15.1,1.16.0,,API / Python,,,,,0,auto-deprioritized-critical,auto-deprioritized-major,pull-request-available,,"Hi,

I'm trying to use output of Python UDF (parse_data) to set watermark for the table
{code:java}
CREATE TABLE test (
    data BYTES,
    ts as parse_data(data).ts,
    WATERMARK for ts as ts
) WITH (
   'connector' = 'kafka',
   'topic' = 'test',
   'properties.bootstrap.servers' = 'localhost:9092',
   'properties.group.id' = 'flink',
   'scan.startup.mode' = 'earliest-offset',
   'format' = 'raw'
){code}
Then running SELECT on this table gives me exception
{code:java}
Py4JJavaError: An error occurred while calling o311.hasNext.
: java.lang.RuntimeException: Failed to fetch next result
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
	at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:370)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.IOException: Failed to fetch job execution result
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:177)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:120)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
	... 13 more
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2022)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:175)
	... 15 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:680)
	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
	at java.base/java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:2094)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:134)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:174)
	... 15 more
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:207)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:197)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:188)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:677)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:435)
	at jdk.internal.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.RuntimeException: Generated WatermarkGenerator fails to generate for row: +I([2, 10, ..., 23]).
	at org.apache.flink.table.planner.plan.abilities.source.WatermarkPushDownSpec$DefaultWatermarkGeneratorSupplier$DefaultWatermarkGenerator.onEvent(WatermarkPushDownSpec.java:172)
	at org.apache.flink.table.planner.plan.abilities.source.WatermarkPushDownSpec$DefaultWatermarkGeneratorSupplier$DefaultWatermarkGenerator.onEvent(WatermarkPushDownSpec.java:150)
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionStateWithWatermarkGenerator.onEvent(KafkaTopicPartitionStateWithWatermarkGenerator.java:82)
	at org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.emitRecordsWithTimestamps(AbstractFetcher.java:368)
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher.partitionConsumerRecordsHandler(KafkaFetcher.java:183)
	at org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher.runFetchLoop(KafkaFetcher.java:142)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.run(FlinkKafkaConsumerBase.java:826)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:66)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:269)
Caused by: java.lang.UnsupportedOperationException: This method is a placeholder and should not be called.
	at org.apache.flink.table.functions.python.PythonScalarFunction.eval(PythonScalarFunction.java:69)
	at WatermarkGenerator$14.currentWatermark(Unknown Source)
	at org.apache.flink.table.planner.plan.abilities.source.WatermarkPushDownSpec$DefaultWatermarkGeneratorSupplier$DefaultWatermarkGenerator.onEvent(WatermarkPushDownSpec.java:166)
	... 9 more
{code}",,dian.fu,dianfu,hxbks2ks,Juntao Hu,maver1ck,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 22 14:40:53 UTC 2022,,,,,,,,,,"0|z0ry3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/21 07:33;trohrmann;Thanks for reporting this issue [~maver1ck]. Could you also post how {{parse_data}} looks like? cc [~dian.fu];;;","15/Jun/21 08:02;maver1ck; 
{code:java}
from fastavro import parse_schema
import json
from fastavro.read import schemaless_reader

with open('schema.avsc', 'r') as f:
    schema_dict = json.load(f)
    schema = parse_schema(schema_dict)

@udf(result_type=DataTypes.ROW([DataTypes.FIELD(""id"", DataTypes.STRING()), DataTypes.FIELD('eventType', DataTypes.STRING()), DataTypes.FIELD('ts', DataTypes.TIMESTAMP(3))]))
def parse_data(record):
    parsed_record = schemaless_reader(BytesIO(record), schema)
    return Row(parsed_record['id'], parsed_record['eventType'], parsed_record['timestamp'])
table_env.create_temporary_function(""parse_data"", parse_data)
{code}
 ;;;","16/Jun/21 11:31;dian.fu;Thanks [~maver1ck] for reporting this issue. This is a good catch and we should support it.;;;","23/Jun/21 22:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","01/Jul/21 22:38;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","03/Aug/21 09:36;maver1ck;[~dian.fu] 
Any chance to have this fixed in 1.14 ?;;;","03/Aug/21 09:43;dianfu;[~maver1ck] There are only less than two weeks before the feature freeze of 1.14 and so I'm afraid that we could not support this in 1.14. 

PS: The features planned to supported in 1.14 could be seen in [https://cwiki.apache.org/confluence/display/FLINK/1.14+Release] ;;;","03/Aug/21 09:51;maver1ck;I don't think this is new feature.  It's rather a bug.

 ;;;","21/Oct/21 10:44;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Oct/21 10:56;maver1ck;[~dianfu] 
Is there a chance for a fix in this issue ?;;;","22/Oct/21 01:19;dianfu;[~maver1ck] Yes, I think we could make it in 1.15.0.;;;","29/Oct/21 22:39;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","22/Apr/22 03:09;Juntao Hu;It's a bug in optimizing sql plan, rather than a feature request to support Python UDF in computed column (this is already done by existing rules). PR is available.;;;","22/Apr/22 14:40;dianfu;Fixed in:
- master via 7ce5a7c6e1eab6823094a94bc0bca30d0ee618f1
- release-1.15 via 703b10ca5d004e8e79059e814fcf8503f84e2da8
- release-1.14 via 0806ad5a154e37d09b53ce56d59cec8dc11209da
- release-1.13 via 79a86f35fb321cb5f8dd40442db8c6bafb00153c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.ClassCastException when using Python UDF,FLINK-22982,13383723,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,maver1ck,maver1ck,14/Jun/21 13:08,23/Sep/21 17:27,13/Jul/23 08:12,23/Jun/21 06:42,1.13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,API / Python,,,,,0,pull-request-available,stale-critical,,,"Hi,

I'm trying to use Python UDF with logical condition as argument.

 
{code:java}
log = logging.getLogger()

@udf(result_type=DataTypes.BOOLEAN())
def trace(message, condition):
    if condition:
        log.warn(message)
    return condition
table_env.create_temporary_function('trace', trace)

table_env.execute_sql(""""""
CREATE TABLE datagen (
    n int
) WITH (
    'connector' = 'datagen',
    'number-of-rows' = '10'
)
"""""")

result = table_env.sql_query(""""""
SELECT * 
FROM datagen
WHERE trace(n, n < 0)
"""""")
for r in result.execute().collect():
    print(r){code}
 

As a result I'm getting exception:
{code:java}
Py4JJavaError: An error occurred while calling o135.execute.
: java.lang.ClassCastException: class org.apache.calcite.rex.RexInputRef cannot be cast to class org.apache.calcite.rex.RexCall (org.apache.calcite.rex.RexInputRef and org.apache.calcite.rex.RexCall are in unnamed module of loader 'app')
	at org.apache.flink.table.planner.plan.rules.logical.PythonMapMergeRule.matches(PythonMapMergeRule.java:70)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:538)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
{code}
 ",,dian.fu,hxbks2ks,maver1ck,RocMarshal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 23 06:42:59 UTC 2021,,,,,,,,,,"0|z0rxnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/21 13:29;maver1ck;I have same exception when using different SQL function inside UDF.
I tried CONCAT and CASE
 ;;;","14/Jun/21 15:17;maver1ck;When using java UDF everything is working fine.;;;","15/Jun/21 02:00;dian.fu;cc [~hxbks2ks];;;","22/Jun/21 22:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","23/Jun/21 06:42;hxbks2ks;Merged into master via cc3f85eb4cd3e5031a84321e62d01b3009a00577
Merged into releases-1.13 via 1f22ccecf3964b5bdb89d0ad334a6bf667fadde2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileExecutionGraphInfoStoreTest hangs on azure,FLINK-22980,13383673,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fpaul,dwysakowicz,dwysakowicz,14/Jun/21 07:24,17/Nov/21 15:21,13/Jul/23 08:12,18/Jun/21 13:56,1.13.1,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,,dwysakowicz,rmetzger,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22986,,,,,,,FLINK-22908,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 18 13:56:03 UTC 2021,,,,,,,,,,"0|z0rxcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/21 07:25;dwysakowicz;cc [~fpaul];;;","14/Jun/21 07:31;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18942&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=9398
;;;","14/Jun/21 07:35;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18945&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=9434;;;","14/Jun/21 07:36;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18947&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=9402;;;","14/Jun/21 07:39;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18951&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=9058;;;","14/Jun/21 07:41;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18953&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=9410;;;","16/Jun/21 02:18;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19003&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=9060;;;","16/Jun/21 03:16;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19005&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8862;;;","16/Jun/21 03:22;xtsong;An observation that might be related: For all the reported instances, there's a {{WebMonitorEndpointTest}} port binding failure before the {{FileExecutionGraphInfoStoreTest}} hangs.
{code:java}
Jun 15 23:09:25 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.768 s <<< FAILURE! - in org.apache.flink.runtime.webmonitor.WebMonitorEndpointTest
Jun 15 23:09:25 [ERROR] cleansUpExpiredExecutionGraphs(org.apache.flink.runtime.webmonitor.WebMonitorEndpointTest)  Time elapsed: 0.766 s  <<< ERROR!
Jun 15 23:09:25 java.net.BindException: Could not start rest endpoint on any port in port range 8081
Jun 15 23:09:25 	at org.apache.flink.runtime.rest.RestServerEndpoint.start(RestServerEndpoint.java:234)
Jun 15 23:09:25 	at org.apache.flink.runtime.webmonitor.WebMonitorEndpointTest.cleansUpExpiredExecutionGraphs(WebMonitorEndpointTest.java:71)
Jun 15 23:09:25 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jun 15 23:09:25 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jun 15 23:09:25 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jun 15 23:09:25 	at java.lang.reflect.Method.invoke(Method.java:498)
Jun 15 23:09:25 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Jun 15 23:09:25 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jun 15 23:09:25 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Jun 15 23:09:25 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jun 15 23:09:25 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jun 15 23:09:25 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
Jun 15 23:09:25 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Jun 15 23:09:25 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
Jun 15 23:09:25 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
Jun 15 23:09:25 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
Jun 15 23:09:25 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Jun 15 23:09:25 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Jun 15 23:09:25 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Jun 15 23:09:25 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Jun 15 23:09:25 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Jun 15 23:09:25 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
Jun 15 23:09:25 	at org.junit.runners.Suite.runChild(Suite.java:128)
Jun 15 23:09:25 	at org.junit.runners.Suite.runChild(Suite.java:27)
Jun 15 23:09:25 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Jun 15 23:09:25 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Jun 15 23:09:25 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Jun 15 23:09:25 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Jun 15 23:09:25 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Jun 15 23:09:25 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
Jun 15 23:09:25 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
Jun 15 23:09:25 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
Jun 15 23:09:25 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
Jun 15 23:09:25 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
Jun 15 23:09:25 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
Jun 15 23:09:25 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
Jun 15 23:09:25 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jun 15 23:09:25 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jun 15 23:09:25 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jun 15 23:09:25 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code};;;","17/Jun/21 01:51;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19035&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=9049;;;","17/Jun/21 04:00;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19037&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=9407;;;","18/Jun/21 02:03;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19084&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=9415;;;","18/Jun/21 02:51;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19086&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=9027;;;","18/Jun/21 13:56;rmetzger;Merged to master in https://github.com/apache/flink/commit/465fc66949c010af740bb242125ce15116ac0aeb
merged to release-1.13 in https://github.com/apache/flink/commit/1183fb5f7d258bb24d829efa229e3eb598050faa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The documentation for `TO_TIMESTAMP` UDF has an incorrect description,FLINK-22970,13383314,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,tonywei,tonywei,tonywei,11/Jun/21 05:15,23/Sep/21 17:52,13/Jul/23 08:12,28/Jun/21 02:19,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Documentation,Table SQL / API,,,,0,pull-request-available,,,,"According to this ML discussion [http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/confused-about-TO-TIMESTAMP-document-description-td44352.html]

The description for `TO_TIMESTAMP` udf is not correct. It will use UTC+0 timezone instead of session timezone. We should fix this documentation.",,jark,leonard,tonywei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 28 02:19:58 UTC 2021,,,,,,,,,,"0|z0rv4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jun/21 06:21;leonard;[~tonywei] Thanks for the report. Yes, the system built-in function  *TO_TIMESTAMP* doesn't use any time zone information.;;;","17/Jun/21 02:52;tonywei;Hi [~Leonard Xu]
Could you assign this jira to me and help to merge this PR if it looks good to you? Thank you.;;;","28/Jun/21 02:19;jark;Fixed in 
- master: aef798d7c85b397a96bd20b2ecec9ab105177551;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CodeGenException: Unsupported cast for nested Decimals,FLINK-22967,13383262,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,maver1ck,maver1ck,10/Jun/21 18:42,18/Jun/21 09:05,13/Jul/23 08:12,18/Jun/21 09:05,1.12.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.1,,,,,Table SQL / Runtime,,,,,0,,,,,"This query is failing in SQL Client
{code:java}
Flink SQL> CREATE TABLE abc (
>     test ROW<a ARRAY<ROW<id STRING, amount DECIMAL(18, 4) >>>
> ) WITH ( 'connector' = 'datagen');
[INFO] Table has been created.

Flink SQL> select * from abc;
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.planner.codegen.CodeGenException: Unsupported cast from 'ROW<`a` ARRAY<ROW<`id` STRING, `amount` DECIMAL(18, 4)>>>' to 'ROW<`a` ARRAY<ROW<`id` STRING, `amount` DECIMAL(38, 18)>>>'.

{code}
 ",,fsk119,godfreyhe,maver1ck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 18 09:04:54 UTC 2021,,,,,,,,,,"0|z0rut4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jun/21 09:04;maver1ck;I'm closing this as fixed in 1.13.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in StateAssignmentOperation when rescaling,FLINK-22966,13383246,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,roman,roman,10/Jun/21 17:08,23/Sep/21 17:52,13/Jul/23 08:12,28/Jun/21 11:15,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"[Reported|http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/NPE-when-restoring-from-savepoint-in-Flink-1-13-1-application-td44345.html] on user ML.

From the code, it looks like if an operator doesn't have at least one subtask with state , then some variables in StateAssignmentOperation.reAssignSubKeyedStates can be null:
{code}
subManagedKeyedState.isEmpty() && subRawKeyedState.isEmpty()
{code}

{code}
    2021-06-09 13:08:59,849 WARN  org.apache.flink.client.deployment.application.DetachedApplicationRunner [] - Could not execute application:
    org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Failed to execute job '<censored>'.
            at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.client.deployment.application.DetachedApplicationRunner.tryExecuteJobs(DetachedApplicationRunner.java:84) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.client.deployment.application.DetachedApplicationRunner.run(DetachedApplicationRunner.java:70) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.lambda$handleRequest$0(JarRunHandler.java:102) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700) [?:?]
            at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
            at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
            at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) [?:?]
            at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
            at java.lang.Thread.run(Thread.java:834) [?:?]
    Caused by: org.apache.flink.util.FlinkException: Failed to execute job '<censored>'.
            at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1970) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:135) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:76) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1834) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.scala:801) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at <censored> ~[?:?]
            at <censored> ~[?:?]
            at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
            at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]
            at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
            at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]
            at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            ... 12 more
    Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
            at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:316) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedFunction$2(FunctionUtils.java:75) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642) ~[?:?]
            at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478) ~[?:?]
            ... 1 more
    Caused by: org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
            at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.lambda$new$0(DefaultJobMasterServiceProcess.java:97) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) ~[?:?]
            at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) ~[?:?]
            at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) ~[?:?]
            at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1705) ~[?:?]
            ... 6 more
    Caused by: java.util.concurrent.CompletionException: java.lang.NullPointerException
            at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:314) ~[?:?]
            at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:319) ~[?:?]
            at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1702) ~[?:?]
            ... 6 more
    Caused by: java.lang.NullPointerException
            at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.reAssignSubKeyedStates(StateAssignmentOperation.java:300) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.lambda$reDistributeKeyedStates$0(StateAssignmentOperation.java:260) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at java.util.HashMap.forEach(HashMap.java:1336) ~[?:?]
            at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.reDistributeKeyedStates(StateAssignmentOperation.java:252) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.assignAttemptState(StateAssignmentOperation.java:196) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.assignStates(StateAssignmentOperation.java:139) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreLatestCheckpointedStateInternal(CheckpointCoordinator.java:1562) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1642) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.tryRestoreExecutionGraphFromSavepoint(DefaultExecutionGraphFactory.java:163) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:138) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:342) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:190) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:120) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:132) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:110) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:340) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:317) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:107) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:95) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112) ~[flink-dist_2.12-1.13.1.jar:1.13.1]
            ... 7 more
    2021-06-09 13:08:59,852 ERROR org.apache.flink.runtime.webmonitor.handlers.JarRunHandler   [] - Exception occurred in REST handler: Could not execute application.
{code}
",,dwysakowicz,pnowojski,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 28 11:15:40 UTC 2021,,,,,,,,,,"0|z0rupk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/21 11:15;dwysakowicz;Fixed in:
* master
** 649b5c9e6c077f268c339ba28eb3efe71c44164e
* 1.13.2
** 045e68dd412b05b123e7d2b278b4eb9f9afe07ec;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connector-base exposes dependency to flink-core.,FLINK-22964,13383212,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,arvid,arvid,arvid,10/Jun/21 14:29,23/Sep/21 17:54,13/Jul/23 08:12,30/Jun/21 07:46,1.12.4,1.13.1,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,Connectors / Common,,,,,0,classloading,pull-request-available,,,"Connectors get shaded into the user jar and as such should contain no unnecessary dependencies to flink. However, connector-base is exposing `flink-core` which then by default gets shaded into the user jar. Except for 6MB of extra size, the dependency also causes class loading issues, when `classloader.parent-first-patterns` does not include `o.a.f`.

Fix is to make `flink-core` provided in `connector-base`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20098,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 30 07:44:15 UTC 2021,,,,,,,,,,"0|z0rui0:",9223372036854775807,Connectors do not transitively hold a reference to `flink-core` anymore. That means that a fat jar with a connector does not include `flink-core` with this fix.,,,,,,,,,,,,,,,,,,,"30/Jun/21 07:44;arvid;Merged into master as 96dba32541d0f756858a5bcdda730e6817b68600, into 1.13 asfdc09154d5f4bb4c618da0e0f224ded4a807916f, into 1.12 as ca999b28ccaff2cb82638ada8a4157444c9a8efb.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The description of taskmanager.memory.task.heap.size in the official document is incorrect,FLINK-22963,13383209,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jasonlee1017,jasonlee1017,jasonlee1017,10/Jun/21 14:21,28/Aug/21 12:17,13/Jul/23 08:12,15/Jun/21 07:27,1.12.4,1.13.1,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,Documentation,,,,,0,documentation,pull-request-available,starter,,"When I studied the memory model of TaskManager, I found that there is a problem in the official document, which is the description of taskmanager.memory.task.heap.size is incorrect.

According to the official memory model, I think the correct description should be that task Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for tasks. If not specified, it will be derived as Total Flink Memory minus Framework Heap Memory, Framework Off-Heap Heap Memory, Task Off-Heap Memory, Managed Memory and Network Memory.

However, in the official document, the Framework Off-Heap Heap Memory should be subtracted.",,jasonlee1017,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 15 02:09:06 UTC 2021,,,,,,,,,,"0|z0ruhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/21 15:44;jasonlee1017;The current link to the description of taskmanager-memory-task-heap-size is: https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html#taskmanager-memory-task- heap-size. And I think the description of TASK_HEAP_MEMORY in the org.apache.flink.configuration.TaskManagerOptions class is also wrong

 ;;;","11/Jun/21 02:25;jasonlee1017;Hi Xintong


Thank you for your confirmation and changes to the issue.


When I checked the source code in multiple versions, I found that the description of TASK_HEAP_MEMORY in the org.apache.flink.configuration.TaskManagerOptions class is also wrong. For example, Flink 1.10. 


So in addition to the version you specified, do other affected versions need to be repaired? so it can Modify the source code and documentation. If possible, I can fix this issue for the community.


Best,
Jason
JasonLee
jasonlee1781@163.com
签名由网易邮箱大师定制


在2021年06月11日 09:53，Xintong Song (Jira)<jira@apache.org> 写道：

[ https://issues.apache.org/jira/browse/FLINK-22963?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Xintong Song 更新了 FLINK-22963:
-----------------------------
影响版本:     (原值: 1.13.0)
(原值: 1.12.0)
(原值: 1.10.0)
1.14.0
1.13.1
1.12.4

The description of taskmanager.memory.task.heap.size in the official document is incorrect
------------------------------------------------------------------------------------------

关键字: FLINK-22963
URL: https://issues.apache.org/jira/browse/FLINK-22963
项目: Flink
问题类型: 故障
模块: Documentation
影响版本: 1.14.0, 1.13.1, 1.12.4
报告人: JasonLee
优先级: 重要
标签: documentation

When I studied the memory model of TaskManager, I found that there is a problem in the official document, which is the description of taskmanager.memory.task.heap.size is incorrect.
According to the official memory model, I think the correct description should be that task Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for tasks. If not specified, it will be derived as Total Flink Memory minus Framework Heap Memory, Framework Off-Heap Heap Memory, Task Off-Heap Memory, Managed Memory and Network Memory.
However, in the official document, the Framework Off-Heap Heap Memory should be subtracted.



--
这条信息是由Atlassian Jira发送的
(v8.3.4#803005)
;;;","11/Jun/21 02:57;xtsong;[~jasonlee1017], thanks for reporting this issue and volunteering to fix it. I've assigned you to the ticket.

According to the [release update policy|https://flink.apache.org/downloads.html#update-policy-for-old-releases], the community provides bugfixes only for the latest release and the one before it. That means this should be fixed for 1.12, 1.13 and of course the upcoming 1.14 releases.

Usually, the contributors only need to open PRs against the master branch and the committers will port the changes to the old release branches if needed. For this ticket, we probably need an additional PR against 1.12, because we have switched our document framework from Ruby to Hugo since1.13.;;;","11/Jun/21 07:37;jasonlee1017;[~xintongsong]

Hi Xitong, I have completed the modification of this issue and submitted a Pull Request. For me, I am very excited to participate in the community contribution for the first time. If there is any problem, please notify me of the modification.

I will continue to learn and contribute to the community. I will also continue to follow the community's progress in Flink's cloud-native direction.;;;","11/Jun/21 09:24;xtsong;[~jasonlee1017],

Welcome to the community, and thanks for your contribution. You're doing great with your first contribution. :)

FYI, there are some guidelines for new contributors, if you have not already read them.
https://flink.apache.org/contributing/how-to-contribute.html;;;","15/Jun/21 02:09;xtsong;Fixed via:
- master (1.14): c2df2382f70adffd374b74b7955808cf916a1c30
- release-1.13: 1df0bbac32877b2cfddf16a3f31d73e76f0daae9
- release-1.12: 7b4d6b8ac06b970667fcea474145dfc22c826c89
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect calculation of alignment timeout for LocalInputChannel,FLINK-22961,13383156,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,akalashnikov,akalashnikov,10/Jun/21 10:37,23/Sep/21 17:27,13/Jul/23 08:12,23/Jun/21 06:49,1.13.1,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"Right now, the calculation of alignment timeout happens inside of SingleCheckpointBarrierHandler(org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.ControllerImpl#isTimedOut) and it based on  firstBarrierArrivalTime.  the  firstBarrierArrivalTime recalculated only when barrier announcement was received but if we receive the first checkpoint barrier from the LocalInputChannel which doesn't support announcement, the calculation of alignment timeout will be based on the firstBarrierArrivalTime from the previous checkpoint which is wrong.",,akalashnikov,dwysakowicz,Thesharing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 23 06:49:45 UTC 2021,,,,,,,,,,"0|z0ru5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/21 06:49;dwysakowicz;Fixed in:
* master
** 833983cd5717de0e448fa3c152be826eedc2eeae
* 1.13.2
** 644ba4dcea7e37b83ec3aab2f53245a6b1aca3ae;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't support consuming update and delete changes when use table function that does not contain table field,FLINK-22954,13383082,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wenlong.lwl,hehuiyuan,hehuiyuan,10/Jun/21 03:20,15/Dec/21 01:44,13/Jul/23 08:12,09/Oct/21 12:03,1.12.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.3,1.15.0,,,Table SQL / Planner,,,,,0,pull-request-available,stale-assigned,,,"
{code:java}
Exception in thread ""main"" org.apache.flink.table.api.TableException: Table sink 'default_catalog.default_database.kafkaTableSink' doesn't support consuming update and delete changes which is produced by node Join(joinType=[LeftOuterJoin], where=[true], select=[name, word], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])Exception in thread ""main"" org.apache.flink.table.api.TableException: Table sink 'default_catalog.default_database.kafkaTableSink' doesn't support consuming update and delete changes which is produced by node Join(joinType=[LeftOuterJoin], where=[true], select=[name, word], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.createNewNode(FlinkChangelogModeInferenceProgram.scala:382) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.visit(FlinkChangelogModeInferenceProgram.scala:265) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.org$apache$flink$table$planner$plan$optimize$program$FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor$$visitChild(FlinkChangelogModeInferenceProgram.scala:341) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor$$anonfun$3.apply(FlinkChangelogModeInferenceProgram.scala:330) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor$$anonfun$3.apply(FlinkChangelogModeInferenceProgram.scala:329) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.immutable.Range.foreach(Range.scala:160) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.visitChildren(FlinkChangelogModeInferenceProgram.scala:329) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.visit(FlinkChangelogModeInferenceProgram.scala:279) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.org$apache$flink$table$planner$plan$optimize$program$FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor$$visitChild(FlinkChangelogModeInferenceProgram.scala:341) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor$$anonfun$3.apply(FlinkChangelogModeInferenceProgram.scala:330) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor$$anonfun$3.apply(FlinkChangelogModeInferenceProgram.scala:329) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.immutable.Range.foreach(Range.scala:160) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.visitChildren(FlinkChangelogModeInferenceProgram.scala:329) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.visit(FlinkChangelogModeInferenceProgram.scala:125) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.optimize(FlinkChangelogModeInferenceProgram.scala:50) at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.optimize(FlinkChangelogModeInferenceProgram.scala:39) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:63) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:60) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:60) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:55) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.immutable.Range.foreach(Range.scala:160) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:55) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:163) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:79) at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77) at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:287) at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:100) at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:42) at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainInternal(TableEnvironmentImpl.java:630) at org.apache.flink.table.api.internal.StatementSetImpl.explain(StatementSetImpl.java:92){code}
 

UDF code:
{code:java}
@FunctionHint(output = @DataTypeHint(""ROW<word INT>""))
public class GenerateSeriesUdf extends TableFunction<Row> {

    public void eval(int from, int to) {
        for (int i = from; i < to; i++) {
            Row row = new Row(1);
            row.setField(0, i);
            collect(row);
        }
    }

    @Override
    public TypeInformation<Row> getResultType() {
        return Types.ROW(Types.INT());
    }
}

{code}
 

`JOIN` is ok,  `LEFT JOIN` has error. 

 

 
{code:java}
CREATE TABLE kafkaTableSource (
    name string,
    age int,
    sex string,
    address string,
    pt as PROCTIME()
) WITH (
    'connector' = 'kafka',
    'topic' = 'hehuiyuan1',
    'scan.startup.mode' = 'latest-offset',
    'properties.bootstrap.servers' = 'localhost:9092',
    'properties.client.id' = 'test-consumer-group',
    'properties.group.id' = 'test-consumer-group',
    'format' = 'json'
);

CREATE TABLE kafkaTableSink (
    name string,
    sname string,
    sno string,
    sclass string,
    address string
) WITH (
    'connector' = 'kafka',
    'topic' = 'hehuiyuan2',
    'properties.bootstrap.servers' = 'localhost:9092',
    'format' = 'json'
);
INSERT INTO kafkaTableSink
SELECT name, name, name, name, word
FROM kafkaTableSource
LEFT JOIN LATERAL TABLE(GENERATE_SERIES(1,5)) AS T(word) ON TRUE;
{code}
 

 
{code:java}
// UDF is constant , two inut
optimize result:
Sink(table=[default_catalog.default_database.kafkaTableSink], fields=[name, name0, name1, name2, word])
+- Calc(select=[name, name AS name0, name AS name1, name AS name2, word])
   +- Join(joinType=[LeftOuterJoin], where=[true], select=[name, word], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])
      :- Exchange(distribution=[single])
      :  +- Calc(select=[name])
      :     +- TableSourceScan(table=[[default_catalog, default_database, kafkaTableSource]], fields=[name, age, sex, address])
      +- Exchange(distribution=[single])
         +- Correlate(invocation=[GENERATE_SERIES(1, 5)], correlate=[table(GENERATE_SERIES(1,5))], select=[word], rowType=[RecordType:peek_no_expand(INTEGER word)], joinType=[INNER])
            +- Values(tuples=[[{  }]])

// UDF that use table field , one inut
optimize result:
 Sink(table=[default_catalog.default_database.kafkaTableSink], fields=[name, name0, name1, name2, province])
+- Calc(select=[name, name AS name0, name AS name1, name AS name2, word AS province])
   +- Correlate(invocation=[JSON_TUPLE($cor0.address, _UTF-16LE'province')], correlate=[table(JSON_TUPLE($cor0.address,_UTF-16LE'province'))], select=[name,age,sex,address,pt,word], rowType=[RecordType(VARCHAR(2147483647) name, INTEGER age, VARCHAR(2147483647) sex, VARCHAR(2147483647) address, TIME ATTRIBUTE(PROCTIME) pt, VARCHAR(2147483647) word)], joinType=[LEFT])
      +- Calc(select=[name, age, sex, address, PROCTIME() AS pt])
         +- TableSourceScan(table=[[default_catalog, default_database, kafkaTableSource]], fields=[name, age, sex, address])
{code}
 ",,chengyunzhang,godfreyhe,hehuiyuan,jark,libenchao,lzljs3620320,wenlong.lwl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 28 10:27:59 UTC 2021,,,,,,,,,,"0|z0rtp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/21 03:37;lzljs3620320;What is the kafkaTableSource? Can you show the DDL?;;;","10/Jun/21 05:38;jark;Currently, this behavior is as expected. Becuase if the UDTF is a constant, the query will be optimized into two-input {{StreamExecJoin}} instead of {{StreamExecCorrelate}}. {{StreamExecJoin}} with LEFT_JOIN will generate updating result, that's why the query is failed. 

I think maybe we shouldn't apply {{StreamPhysicalConstantTableFunctionScanRule}} when outer joins, because it will change ChangelogMode semantic. What do you think [~godfreyhe]?;;;","10/Jun/21 06:33;godfreyhe;[~jark] If we do not apply StreamPhysicalConstantTableFunctionScanRule, we can not get physical plan because there is no physical implementation of FlinkLogicalTableFunctionScan. Or we should optimize the above query into StreamExecCorrelate;;;","10/Jun/21 06:51;jark;Another way can be refactoring the join operator to build the right side first before processing left side. However, this depends on whether {{InputSelectable}} and {{BoundedMultiInput}} can work on streaming operators. ;;;","18/Jun/21 04:37;wenlong.lwl;[~jark][~godfreyhe], I think ConstantTableFunctionScanRule is not the right way to  process the ConstantTableFunction, when the function is not deterministic, the function should be called by every record,  How about adding a new rule to convert LogicalJoin(XX, TableFunctionScan) to LogicalCorrelate(XXX, TableFunctionScan) ?;;;","18/Jun/21 06:52;godfreyhe;[~wenlong.lwl] Currently, ConstantTableFunctionScanRule works only when the function is deterministic. See {{RexUtil.isConstant}};;;","18/Jun/21 07:57;wenlong.lwl;[~godfreyhe]  thanks for correct me, you are right. so when the function is not deterministic, it is still not supported now?  I have tried to add the rewrite rule, with this pr planner can work well on join constant table function: https://github.com/apache/flink/pull/16192

I am thinking about that maybe we should remove the limitation of RexUtil.isConstant on ConstantTableFunctionScanRule, because ConstantTableFunctionScanRule only works on: select * from lateral table(XXX), in this case it is ok to support non-deterministic function. what do you think;;;","29/Jun/21 03:55;godfreyhe;[~wenlong.lwl] I think we should also make sure there is not input reference in the RexCall of TableFunctionScan (do not care about whether the function is deterministic).;;;","13/Aug/21 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","09/Oct/21 12:03;godfreyhe;Fixed in 1.15.0: f83e60387a17bd927004891fe6dcc35dfddf0488
Fixed in 1.14.1: 91cb0005dea33a96d445ff9ea08fa7668fd5513f
Fixed in 1.13.3: 4250543ab483ccc6f8fb0f16788dece42ce6a4d0;;;","28/Oct/21 10:27;hehuiyuan;Hi [~jark] , why inner join is not  changelogMode semantic ,  left\right  join is changelogMode semantic? 

 
{code:java}
case join: StreamExecJoin =>
  // join support all changes in input
  val children = visitChildren(rel, ModifyKindSetTrait.ALL_CHANGES)
  val leftKindSet = getModifyKindSet(children.head)
  val rightKindSet = getModifyKindSet(children.last)
  val innerOrSemi = join.flinkJoinType == FlinkJoinType.INNER ||
      join.flinkJoinType == FlinkJoinType.SEMI
  val providedTrait = if (innerOrSemi) {
    // forward left and right modify operations
    new ModifyKindSetTrait(leftKindSet.union(rightKindSet))
  } else {
    // otherwise, it may produce any kinds of changes
    ModifyKindSetTrait.ALL_CHANGES
  }
  createNewNode(join, children, providedTrait, requiredTrait, requester)
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
docs_404_check fail on azure due to ruby version not available,FLINK-22952,13383077,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,xtsong,xtsong,10/Jun/21 02:37,28/Aug/21 12:16,13/Jul/23 08:12,11/Jun/21 05:47,1.12.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,Test Infrastructure,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18852&view=logs&j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&t=404fcc1b-71ae-54f6-61c8-430a6aeff2b5

{code}
Starting: UseRubyVersion
==============================================================================
Task         : Use Ruby version
Description  : Use the specified version of Ruby from the tool cache, optionally adding it to the PATH
Version      : 0.186.0
Author       : Microsoft Corporation
Help         : https://docs.microsoft.com/azure/devops/pipelines/tasks/tool/use-ruby-version
==============================================================================
##[error]Version spec = 2.4 for architecture %25s did not match any version in Agent.ToolsDirectory.
Available versions: /opt/hostedtoolcache
2.5.9,2.6.7,2.7.3,3.0.1
If this is a Microsoft-hosted agent, check that this image supports side-by-side versions of Ruby at https://aka.ms/hosted-agent-software.
If this is a self-hosted agent, see how to configure side-by-side Ruby versions at https://go.microsoft.com/fwlink/?linkid=2005989.
Finishing: UseRubyVersion
{code}",,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 11 03:04:13 UTC 2021,,,,,,,,,,"0|z0rto0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/21 02:38;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18802&view=logs&j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&t=404fcc1b-71ae-54f6-61c8-430a6aeff2b5;;;","10/Jun/21 02:39;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18851&view=logs&j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&t=404fcc1b-71ae-54f6-61c8-430a6aeff2b5;;;","10/Jun/21 02:43;xtsong;Seems to me there's a recent update from Azure that Ruby 2.4 is no longer supported?
[~rmetzger], could you help take a look into this?;;;","10/Jun/21 03:58;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18853&view=logs&j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&t=404fcc1b-71ae-54f6-61c8-430a6aeff2b5;;;","10/Jun/21 13:58;chesnay;master:
084f8460020a6188e2c36048b6de51ff2e4c7538
2976323b08e1430951191214eb7b2eb3b1a474f6

1.13:
f1c85ffc4416b84cfa638b84f2613db047195bfb
dc44739c53cac1d8177e077742a761bbfb39cb59

1.12:
aa3cd4ba3ad92d3a5b086ef7d6830852ff9f6c7b;;;","11/Jun/21 02:03;xtsong;Happens again on master, after the PR is merged.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18890&view=logs&j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&t=404fcc1b-71ae-54f6-61c8-430a6aeff2b5

Seems the PR removed Ruby usage only for the ci tasks. The cron tasks are overlooked.;;;","11/Jun/21 03:04;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18892&view=logs&j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala example for toDataStream does not compile,FLINK-22948,13382978,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,alpinegizmo,alpinegizmo,09/Jun/21 14:39,17/Jun/21 11:21,13/Jul/23 08:12,17/Jun/21 11:21,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Documentation,Table SQL / API,,,,0,,,,,"The scala example at [https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/table/data_stream_api/#examples-for-todatastream] does not compile – {{User.class}} should be {{classOf[User]}}.

It would also be better to show the table DDL as

{{tableEnv.executeSql(}}
{{  """"""}}
{{  CREATE TABLE GeneratedTable (}}
{{    name STRING,}}
{{    score INT,}}
{{    event_time TIMESTAMP_LTZ(3),}}
{{    WATERMARK FOR event_time AS event_time - INTERVAL '10' SECOND}}
{{  )}}
{{  WITH ('connector'='datagen')}}
{{  """"""}}
{{)}}",,alpinegizmo,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 17 11:21:09 UTC 2021,,,,,,,,,,"0|z0rt20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/21 11:21;twalthr;Fixed in 1.14.0: e23337aace8f02fb8b940480a4ce3c2f054f323e
Fixed in 1.13.2: bf304e49c4e6ea7b8d73cfb7709e87a2e4c8b52a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Network buffer deadlock introduced by unaligned checkpoint,FLINK-22946,13382938,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,guokuai.huang,guokuai.huang,guokuai.huang,09/Jun/21 11:54,28/Aug/21 12:18,13/Jul/23 08:12,18/Jun/21 13:00,1.11.3,1.12.4,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,1.11.4,1.12.5,1.13.2,1.14.0,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"We recently encountered deadlock when using unaligned checkpoint. Below are two thread stacks that cause deadlock:
{code:java}
""Channel state writer Join(xxxxxx) (34/256)#1"": at org.apache.flink.runtime.io.network.partition.consumer.BufferManager.notifyBufferAvailable(BufferManager.java:296) - waiting to lock <0x00000007296dfa90> (a org.apache.flink.runtime.io.network.partition.consumer.BufferManager$AvailableBufferQueue) at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.fireBufferAvailableNotification(LocalBufferPool.java:507) at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.recycle(LocalBufferPool.java:494) at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.recycle(LocalBufferPool.java:460) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.deallocate(NetworkBuffer.java:182) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.handleRelease(AbstractReferenceCountedByteBuf.java:110) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:100) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:156) at org.apache.flink.runtime.io.network.partition.consumer.BufferManager$AvailableBufferQueue.addExclusiveBuffer(BufferManager.java:399) at org.apache.flink.runtime.io.network.partition.consumer.BufferManager.recycle(BufferManager.java:200) - locked <0x00000007296bc450> (a org.apache.flink.runtime.io.network.partition.consumer.BufferManager$AvailableBufferQueue) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.deallocate(NetworkBuffer.java:182) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.handleRelease(AbstractReferenceCountedByteBuf.java:110) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:100) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:156) at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.write(ChannelStateCheckpointWriter.java:173) at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.writeInput(ChannelStateCheckpointWriter.java:131) at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequest.lambda$write$0(ChannelStateWriteRequest.java:63) at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequest$$Lambda$785/722492780.accept(Unknown Source) at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequest.lambda$buildWriteRequest$2(ChannelStateWriteRequest.java:93) at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequest$$Lambda$786/1360749026.accept(Unknown Source) at org.apache.flink.runtime.checkpoint.channel.CheckpointInProgressRequest.execute(ChannelStateWriteRequest.java:212) at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatchInternal(ChannelStateWriteRequestDispatcherImpl.java:82) at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatch(ChannelStateWriteRequestDispatcherImpl.java:59) at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.loop(ChannelStateWriteRequestExecutorImpl.java:96) at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.run(ChannelStateWriteRequestExecutorImpl.java:75) at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl$$Lambda$253/502209879.run(Unknown Source) at java.lang.Thread.run(Thread.java:745){code}
{code:java}
""Join(xxxxxx) (34/256)#1"": at org.apache.flink.runtime.io.network.partition.consumer.BufferManager.notifyBufferAvailable(BufferManager.java:296) - waiting to lock <0x00000007296bc450> (a org.apache.flink.runtime.io.network.partition.consumer.BufferManager$AvailableBufferQueue) at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.fireBufferAvailableNotification(LocalBufferPool.java:507) at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.recycle(LocalBufferPool.java:494) at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.recycle(LocalBufferPool.java:460) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.deallocate(NetworkBuffer.java:182) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.handleRelease(AbstractReferenceCountedByteBuf.java:110) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:100) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:156) at org.apache.flink.runtime.io.network.partition.consumer.BufferManager$AvailableBufferQueue.addExclusiveBuffer(BufferManager.java:399) at org.apache.flink.runtime.io.network.partition.consumer.BufferManager.recycle(BufferManager.java:200) - locked <0x00000007296dfa90> (a org.apache.flink.runtime.io.network.partition.consumer.BufferManager$AvailableBufferQueue) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.deallocate(NetworkBuffer.java:182) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.handleRelease(AbstractReferenceCountedByteBuf.java:110) at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:100) at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:156) at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.getNextRecord(SpillingAdaptiveSpanningRecordDeserializer.java:95) at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:95) at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66) at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processInput(StreamTwoInputProcessor.java:96) at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:423) at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$226/1801850008.runDefaultAction(Unknown Source) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:204) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:681) at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:636) at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$577/1653738667.run(Unknown Source) at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:647) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:620) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) at java.lang.Thread.run(Thread.java:745){code}
The root cause of this problem is that unaligned checkpoint makes it possible that *under the same input gate,* *multiple input channels may recycle network buffer at the same time.*

Previously, network buffer recycling would only occur serially between input channels under the same input gate, because each sub-task is process Input data serially, and an input gate belongs to only one sub-task. When unaligned checkpoint is enabled, each input channel will take a snapshot of the input channel when it receives the checkpoint barrier, and the network buffer may be recycled in the process.

Unfortunately, *the current network buffer recycling mechanism does not take into account the situation where multiple input channels perform network buffer recycling at the same time.* The following code block is from org.apache.flink.runtime.io.network.partition.consumer.BufferManager$AvailableBufferQueue that causes deadlock when multiple input channels under same input gate perform network buffer recycling at the same time.

!Screen Shot 2021-06-09 at 7.02.04 PM.png!

The solution to this problem is quite straightforward. Here are two possible solutions:
 *1. Case by case solution.* Note that input channel A (locked A) gave the released network buffer to input channel B (waiting to lock B), and input channel B (locked B) gave the released network buffer to input channel A (waiting to lock A) ), so when an input channel releases the network buffer, first check whether it is also waiting for the network buffer, and if it is, directly allocate it to itself, which can avoid the situation that different input channels exchange network buffers.
 2. *A straightforward solution.* Considering that the input channel occupies the lock during recycle to remove the network buffer from the bufferQueue, the subsequent operations do not need to hold this lock. Therefore, we only need to place Buffer::recycleBuffer outside the bufferQueue lock to avoid deadlock.",,guokuai.huang,kevin.cyj,Ming Li,pnowojski,roman,Thesharing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/21 10:40;guokuai.huang;Screen Shot 2021-06-09 at 6.39.47 PM.png;https://issues.apache.org/jira/secure/attachment/13026601/Screen+Shot+2021-06-09+at+6.39.47+PM.png","09/Jun/21 11:02;guokuai.huang;Screen Shot 2021-06-09 at 7.02.04 PM.png;https://issues.apache.org/jira/secure/attachment/13026600/Screen+Shot+2021-06-09+at+7.02.04+PM.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 18 13:00:51 UTC 2021,,,,,,,,,,"0|z0rst4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jun/21 09:27;guokuai.huang;[~zjwang] [~roman_khachatryan] [~arvid heise] [~ym] 
Would you please help to see this problem? Thanks.;;;","12/Jun/21 22:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","14/Jun/21 11:33;pnowojski;Hi [~guokuai.huang], thanks for reporting this issue. I think indeed you are right.

On the outputs there are multiple threads that simultaneously recycling the buffers, but on the output side, there is no {{BufferManager}} to cause this problem, right?

{quote}
1. Case by case solution. Note that input channel A (locked A) gave the released network buffer to input channel B (waiting to lock B), and input channel B (locked B) gave the released network buffer to input channel A (waiting to lock A) ), so when an input channel releases the network buffer, first check whether it is also waiting for the network buffer, and if it is, directly allocate it to itself, which can avoid the situation that different input channels exchange network buffers.
{quote}
I'm not sure if I understand this solution, as {{BufferManager}}s are already checking in BufferManager.AvailableBufferQueue#addExclusiveBuffer:
{code:java}
if (getAvailableBufferSize() > numRequiredBuffers) {
{code}
Is it a problem that there is some lingering registered buffer availability listener? Can you elaborate more about this idea of yours?

About the proposed solution #2. I think you are right, we could also recycle the buffers outside of the {{bufferQueue}} lock in {{BufferManager#recycle}}, instead of inside {{BufferManager.AvailableBufferQueue#addExclusiveBuffer}} as it is now? ;;;","15/Jun/21 08:37;guokuai.huang;Hi [~pnowojski], thanks for your reply. 

1. For your concern about the output side. In credit-based flow control mode, floating network buffers are shared among input channels. That is, in our case, the problem of swapping floating network buffers between two input channels. On the output side, there is no floating network buffer mechanism, so there will be no such problem. Please correct me if I am wrong.

2. For the questions about the first solution. BufferManager::numRequireBuffers is adjust dynamically based on backlog (real-time output buffers in the subpartition) feedback. So when the following condition is true, it does not mean that the manager's listener has been triggered. 
{code:java}
if (getAvailableBufferSize() > numRequiredBuffers) {}
{code}
Because it may register the listener first, and then reduce the value of numRequireBuffers based on backlog changes. At this moment, Channel A and Channel B don’t really need more network buffer. They are calling the other’s notifyBufferAvailable to quickly determine that the other side does not need a buffer, and then remove the other’s listener. *To be precise, the first solution is to remove its outdated listener when releasing floating buffer.*

 Of course, I personally prefer the second solution, which is more general and can avoid similar deadlocks.;;;","15/Jun/21 08:54;pnowojski;Thanks for the explanations.

Second solution also sounds better to me. We used to have similar problems in other parts of the code, and usually minimising the amount of things done under the lock was our preferred option.

[~guokuai.huang], would you like to prepare a fix for this/should I assign this ticket to you?;;;","15/Jun/21 09:17;guokuai.huang;Of course, please assign this ticket to me. [~pnowojski];;;","18/Jun/21 13:00;pnowojski;Merged to master as 903de19442b
Merged to release-1.13 as 59322aef23c
Merged to release-1.12 as 949db6fdf7b
Merged to release-1.11 as aa7baa5bd9d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StackOverflowException can happen when a large scale job is CANCELING/FAILING,FLINK-22945,13382927,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pltbkd,zhuzh,zhuzh,09/Jun/21 11:15,28/Aug/21 13:06,13/Jul/23 08:12,30/Jun/21 11:12,1.12.4,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"The pending requests in ExecutionSlotAllocator are not cleared when a job transitions to CANCELING or FAILING, while all vertices will be canceled and assigned slot will be returned. The returned slot is possible to be used to fulfill the pending request of a CANCELED vertex and the assignment will fail immediately and the slot will be returned and used to fulfilled another vertex in a recursive way. StackOverflow can happen in this way when there are many vertices, and fatal error can happen and lead to JM will crash. A sample call stack is attached below.
To fix this problem, we should clear the pending requests in ExecutionSlotAllocator when a job is CANCELING or FAILING. Besides that, I think it's better to also improve the call stack of slot assignment to avoid similar StackOverflowException to occur.


...
	at org.apache.flink.runtime.scheduler.SharedSlot.returnLogicalSlot(SharedSlot.java:234) ~[flink-dist_2.11-1.13-vvr-4.0-SNAPSHOT.jar:1.13-vvr-4.0-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.lambda$returnSlotToOwner$0(SingleLogicalSlot.java:203) ~[flink-dist_2.11-1.13-vvr-4.0-SNAPSHOT.jar:1.13-vvr-4.0-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:705) ~[?:1.8.0_102]
	at java.util.concurrent.CompletableFuture.uniRunStage(CompletableFuture.java:717) ~[?:1.8.0_102]
	at java.util.concurrent.CompletableFuture.thenRun(CompletableFuture.java:2010) ~[?:1.8.0_102]
	at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.returnSlotToOwner(SingleLogicalSlot.java:200) ~[flink-dist_2.11-1.13-vvr-4.0-SNAPSHOT.jar:1.13-vvr-4.0-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.releaseSlot(SingleLogicalSlot.java:130) ~[flink-dist_2.11-1.13-vvr-4.0-SNAPSHOT.jar:1.13-vvr-4.0-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.releaseSlotIfPresent(DefaultScheduler.java:533) ~[flink-dist_2.11-1.13-vvr-4.0-SNAPSHOT.jar:1.13-vvr-4.0-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$8(DefaultScheduler.java:512) ~[flink-dist_2.11-1.13-vvr-4.0-SNAPSHOT.jar:1.13-vvr-4.0-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ~[?:1.8.0_102]
	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ~[?:1.8.0_102]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_102]
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) ~[?:1.8.0_102]
	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge$PendingRequest.fulfill(DeclarativeSlotPoolBridge.java:552) ~[flink-dist_2.11-1.13-vvr-4.0-SNAPSHOT.jar:1.13-vvr-4.0-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge$PendingRequestSlotMatching.fulfillPendingRequest(DeclarativeSlotPoolBridge.java:587) ~[flink-dist_2.11-1.13-vvr-4.0-SNAPSHOT.jar:1.13-vvr-4.0-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.newSlotsAreAvailable(DeclarativeSlotPoolBridge.java:171) ~[flink-dist_2.11-1.13-vvr-4.0-SNAPSHOT.jar:1.13-vvr-4.0-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.lambda$freeReservedSlot$0(DefaultDeclarativeSlotPool.java:316) ~[flink-dist_2.11-1.13-vvr-4.0-SNAPSHOT.jar:1.13-vvr-4.0-SNAPSHOT]
	at java.util.Optional.ifPresent(Optional.java:159) ~[?:1.8.0_102]
	at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.freeReservedSlot(DefaultDeclarativeSlotPool.java:313) ~[flink-dist_2.11-1.13-vvr-4.0-SNAPSHOT.jar:1.13-vvr-4.0-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.releaseSlot(DeclarativeSlotPoolBridge.java:335) ~[flink-dist_2.11-1.13-vvr-4.0-SNAPSHOT.jar:1.13-vvr-4.0-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotProviderImpl.cancelSlotRequest(PhysicalSlotProviderImpl.java:112) ~[flink-dist_2.11-1.13-vvr-4.0-SNAPSHOT.jar:1.13-vvr-4.0-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator.releaseSharedSlot(SlotSharingExecutionSlotAllocator.java:242) ~[flink-dist_2.11-1.13-vvr-4.0-SNAPSHOT.jar:1.13-vvr-4.0-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.SharedSlot.releaseExternally(SharedSlot.java:281) ~[flink-dist_2.11-1.13-vvr-4.0-SNAPSHOT.jar:1.13-vvr-4.0-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.SharedSlot.removeLogicalSlotRequest(SharedSlot.java:242) ~[flink-dist_2.11-1.13-vvr-4.0-SNAPSHOT.jar:1.13-vvr-4.0-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.SharedSlot.returnLogicalSlot(SharedSlot.java:234) ~[flink-dist_2.11-1.13-vvr-4.0-SNAPSHOT.jar:1.13-vvr-4.0-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.lambda$returnSlotToOwner$0(SingleLogicalSlot.java:203) ~[flink-dist_2.11-1.13-vvr-4.0-SNAPSHOT.jar:1.13-vvr-4.0-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:705) ~[?:1.8.0_102]
	at java.util.concurrent.CompletableFuture.uniRunStage(CompletableFuture.java:717) ~[?:1.8.0_102]
	at java.util.concurrent.CompletableFuture.thenRun(CompletableFuture.java:2010) ~[?:1.8.0_102]
	at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.returnSlotToOwner(SingleLogicalSlot.java:200) ~[flink-dist_2.11-1.13-vvr-4.0-SNAPSHOT.jar:1.13-vvr-4.0-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.releaseSlot(SingleLogicalSlot.java:130) ~[flink-dist_2.11-1.13-vvr-4.0-SNAPSHOT.jar:1.13-vvr-4.0-SNAPSHOT]
...",,jinxing6042@126.com,maguowei,pltbkd,Thesharing,trohrmann,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 30 11:12:30 UTC 2021,,,,,,,,,,"0|z0rsqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/21 07:17;pltbkd;Hi [~zhuzh],

I'd like to work on this, would you please assign the ticket to me?

By the way, I've tried to make resource assignment or new slots notification asynchronous to break the recursive loop in call stack of slot assignment, but found that the status maintenance of slots would be more complex and may lead to unexpected issues. I suggest not to modify the call stack but only avoid entering the recursive loop. What do you think about it?;;;","28/Jun/21 07:33;zhuzh;[~pltbkd] I have assign you the ticket. Feel free to open a fix for it.
As discussed offline, if breaking the recursive loop via async call, slot status may change before the async call is executed, which may introduce quite some complexity to avoid slot leak. Given that the recursive loop would not happen if we correctly cleans up the pending requests, I agree to just fix the entry to the recursive loop by clearing pending requests in ExecutionSlotAllocator when a job is CANCELING or FAILING.;;;","30/Jun/21 11:12;zhuzh;Fixed via:
master: 5badc356abdcbb3d5cae1fe3f00f1ec18f414d98
1.13: 2d229fc6521b4fc924a4a66347d71b72a1455f77
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDBStateBackendWindowITCase fails with savepoint timeout,FLINK-22932,13382782,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,roman,roman,08/Jun/21 19:49,15/Dec/21 01:40,13/Jul/23 08:12,24/Aug/21 10:35,1.12.4,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.3,1.14.0,,,Runtime / Coordination,,,,,0,auto-deprioritized-critical,pull-request-available,test-stability,,"Initially [reported|https://issues.apache.org/jira/browse/FLINK-22067?focusedCommentId=17358306&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17358306] in FLINK-22067

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18709&view=logs&j=a8bc9173-2af6-5ba8-775c-12063b4f1d54&t=46a16c18-c679-5905-432b-9be5d8e27bc6&l=10183

Savepoint is triggered but is not completed in time.


{noformat}
2021-06-06T22:27:46.4845045Z Jun 06 22:27:46 java.lang.RuntimeException: Failed to take savepoint
2021-06-06T22:27:46.4846088Z Jun 06 22:27:46 	at org.apache.flink.state.api.utils.SavepointTestBase.takeSavepoint(SavepointTestBase.java:71)
2021-06-06T22:27:46.4847049Z Jun 06 22:27:46 	at org.apache.flink.state.api.utils.SavepointTestBase.takeSavepoint(SavepointTestBase.java:46)
2021-06-06T22:27:46.4848262Z Jun 06 22:27:46 	at org.apache.flink.state.api.SavepointWindowReaderITCase.testApplyEvictorWindowStateReader(SavepointWindowReaderITCase.java:350)
2021-06-06T22:27:46.4854133Z Jun 06 22:27:46 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-06-06T22:27:46.4855430Z Jun 06 22:27:46 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-06-06T22:27:46.4856528Z Jun 06 22:27:46 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-06-06T22:27:46.4857487Z Jun 06 22:27:46 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-06-06T22:27:46.4858685Z Jun 06 22:27:46 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-06-06T22:27:46.4859773Z Jun 06 22:27:46 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-06-06T22:27:46.4860964Z Jun 06 22:27:46 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-06-06T22:27:46.4862306Z Jun 06 22:27:46 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-06-06T22:27:46.4863756Z Jun 06 22:27:46 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-06-06T22:27:46.4864993Z Jun 06 22:27:46 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-06-06T22:27:46.4866179Z Jun 06 22:27:46 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-06-06T22:27:46.4867272Z Jun 06 22:27:46 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-06-06T22:27:46.4868255Z Jun 06 22:27:46 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-06-06T22:27:46.4869045Z Jun 06 22:27:46 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-06-06T22:27:46.4869902Z Jun 06 22:27:46 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-06-06T22:27:46.4871038Z Jun 06 22:27:46 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-06-06T22:27:46.4871756Z Jun 06 22:27:46 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-06-06T22:27:46.4872502Z Jun 06 22:27:46 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-06-06T22:27:46.4873389Z Jun 06 22:27:46 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-06-06T22:27:46.4874150Z Jun 06 22:27:46 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-06-06T22:27:46.4874914Z Jun 06 22:27:46 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-06-06T22:27:46.4875661Z Jun 06 22:27:46 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-06-06T22:27:46.4876382Z Jun 06 22:27:46 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-06-06T22:27:46.4877018Z Jun 06 22:27:46 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-06-06T22:27:46.4877661Z Jun 06 22:27:46 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-06-06T22:27:46.4878522Z Jun 06 22:27:46 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-06-06T22:27:46.4879506Z Jun 06 22:27:46 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-06-06T22:27:46.4880246Z Jun 06 22:27:46 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-06-06T22:27:46.4881025Z Jun 06 22:27:46 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-06-06T22:27:46.4881839Z Jun 06 22:27:46 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-06-06T22:27:46.4882650Z Jun 06 22:27:46 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-06-06T22:27:46.4883596Z Jun 06 22:27:46 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-06-06T22:27:46.4884971Z Jun 06 22:27:46 Caused by: java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.triggerSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.
2021-06-06T22:27:46.4886218Z Jun 06 22:27:46 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2021-06-06T22:27:46.4887018Z Jun 06 22:27:46 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2021-06-06T22:27:46.4887787Z Jun 06 22:27:46 	at org.apache.flink.state.api.utils.SavepointTestBase.takeSavepoint(SavepointTestBase.java:69)
2021-06-06T22:27:46.4888521Z Jun 06 22:27:46 	... 34 more
2021-06-06T22:27:46.4889560Z Jun 06 22:27:46 Caused by: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.triggerSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.
2021-06-06T22:27:46.4890708Z Jun 06 22:27:46 	at com.sun.proxy.$Proxy32.triggerSavepoint(Unknown Source)
2021-06-06T22:27:46.4891470Z Jun 06 22:27:46 	at org.apache.flink.runtime.minicluster.MiniCluster.lambda$triggerSavepoint$8(MiniCluster.java:716)
2021-06-06T22:27:46.4892292Z Jun 06 22:27:46 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2021-06-06T22:27:46.4893139Z Jun 06 22:27:46 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2021-06-06T22:27:46.4894022Z Jun 06 22:27:46 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2021-06-06T22:27:46.4894810Z Jun 06 22:27:46 	at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:751)
2021-06-06T22:27:46.4895876Z Jun 06 22:27:46 	at org.apache.flink.runtime.minicluster.MiniCluster.triggerSavepoint(MiniCluster.java:714)
2021-06-06T22:27:46.4896736Z Jun 06 22:27:46 	at org.apache.flink.client.program.MiniClusterClient.triggerSavepoint(MiniClusterClient.java:101)
2021-06-06T22:27:46.4897610Z Jun 06 22:27:46 	at org.apache.flink.state.api.utils.SavepointTestBase.triggerSavepoint(SavepointTestBase.java:93)
2021-06-06T22:27:46.4898651Z Jun 06 22:27:46 	at org.apache.flink.state.api.utils.SavepointTestBase.lambda$takeSavepoint$0(SavepointTestBase.java:68)
2021-06-06T22:27:46.4899492Z Jun 06 22:27:46 	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966)
2021-06-06T22:27:46.4900311Z Jun 06 22:27:46 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
2021-06-06T22:27:46.4901105Z Jun 06 22:27:46 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-06-06T22:27:46.4901882Z Jun 06 22:27:46 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1646)
2021-06-06T22:27:46.4902703Z Jun 06 22:27:46 	at java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1632)
2021-06-06T22:27:46.4903544Z Jun 06 22:27:46 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2021-06-06T22:27:46.4904457Z Jun 06 22:27:46 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2021-06-06T22:27:46.4905221Z Jun 06 22:27:46 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2021-06-06T22:27:46.4905948Z Jun 06 22:27:46 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2021-06-06T22:27:46.4908488Z Jun 06 22:27:46 Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/dispatcher_2#1085446192]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
2021-06-06T22:27:46.4909806Z Jun 06 22:27:46 	at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:635)
2021-06-06T22:27:46.4910572Z Jun 06 22:27:46 	at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:650)
2021-06-06T22:27:46.4911233Z Jun 06 22:27:46 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
2021-06-06T22:27:46.4911980Z Jun 06 22:27:46 	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:870)
2021-06-06T22:27:46.4912770Z Jun 06 22:27:46 	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:109)
2021-06-06T22:27:46.4913636Z Jun 06 22:27:46 	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:103)
2021-06-06T22:27:46.4914406Z Jun 06 22:27:46 	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:868)
2021-06-06T22:27:46.4915259Z Jun 06 22:27:46 	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
2021-06-06T22:27:46.4916164Z Jun 06 22:27:46 	at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:279)
2021-06-06T22:27:46.4917078Z Jun 06 22:27:46 	at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:283)
2021-06-06T22:27:46.4917924Z Jun 06 22:27:46 	at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:235)
2021-06-06T22:27:46.4918737Z Jun 06 22:27:46 	at java.lang.Thread.run(Thread.java:748)
{noformat}
",,roman,trohrmann,wind_ljy,xtsong,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22571,FLINK-22568,,,,,,,,,,,,,,FLINK-22067,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 24 10:35:02 UTC 2021,,,,,,,,,,"0|z0rrug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/21 22:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","24/Jun/21 22:38;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","05/Jul/21 05:17;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19873&view=logs&j=b2f046ab-ae17-5406-acdc-240be7e870e4&t=93e5ae06-d194-513d-ba8d-150ef6da1d7c&l=8510;;;","12/Aug/21 03:16;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21934&view=logs&j=56781494-ebb0-5eae-f732-b9c397ec6ede&t=6568c985-5fcc-5b89-1ebd-0385b8088b14&l=8791;;;","17/Aug/21 05:39;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22331&view=logs&j=56781494-ebb0-5eae-f732-b9c397ec6ede&t=6568c985-5fcc-5b89-1ebd-0385b8088b14&l=9099;;;","20/Aug/21 14:58;trohrmann;It looks as if the savepoint cannot be finished in time. The logs show a gap of 10s in the timestamps. Hence, I assume that this is caused by our CI infrastructure.

{code}
23:43:19,979 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Triggering savepoint for job 5351586c56778c282abf1993bd738a0d.
23:43:19,989 [jobmanager-future-thread-5] INFO  org.apache.flink.core.fs.FileSystem                          [] - Hadoop is not in the classpath/dependencies. The extended set of supported File Systems via Hadoop is not available.
23:43:20,032 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1 (type=SAVEPOINT) @ 1629157399981 for job 5351586c56778c282abf1993bd738a0d.
23:43:20,070 [Window(TumblingEventTimeWindows(10), EventTimeTrigger, NoOpEvictor, NoOpWindowFunction) -> Sink: Unnamed (3/4)#0] INFO  org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Successfully loaded RocksDB native library
23:43:20,076 [Window(TumblingEventTimeWindows(10), EventTimeTrigger, NoOpEvictor, NoOpWindowFunction) -> Sink: Unnamed (1/4)#0] INFO  org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Getting managed memory shared cache for RocksDB.
23:43:20,076 [Window(TumblingEventTimeWindows(10), EventTimeTrigger, NoOpEvictor, NoOpWindowFunction) -> Sink: Unnamed (4/4)#0] INFO  org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Getting managed memory shared cache for RocksDB.
23:43:20,076 [Window(TumblingEventTimeWindows(10), EventTimeTrigger, NoOpEvictor, NoOpWindowFunction) -> Sink: Unnamed (3/4)#0] INFO  org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Getting managed memory shared cache for RocksDB.
23:43:20,077 [Window(TumblingEventTimeWindows(10), EventTimeTrigger, NoOpEvictor, NoOpWindowFunction) -> Sink: Unnamed (2/4)#0] INFO  org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Getting managed memory shared cache for RocksDB.
23:43:20,081 [Window(TumblingEventTimeWindows(10), EventTimeTrigger, NoOpEvictor, NoOpWindowFunction) -> Sink: Unnamed (1/4)#0] INFO  org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Obtained shared RocksDB cache of size 20971520 bytes
23:43:20,081 [Window(TumblingEventTimeWindows(10), EventTimeTrigger, NoOpEvictor, NoOpWindowFunction) -> Sink: Unnamed (3/4)#0] INFO  org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Obtained shared RocksDB cache of size 20971520 bytes
23:43:20,081 [Window(TumblingEventTimeWindows(10), EventTimeTrigger, NoOpEvictor, NoOpWindowFunction) -> Sink: Unnamed (2/4)#0] INFO  org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Obtained shared RocksDB cache of size 20971520 bytes
23:43:20,081 [Window(TumblingEventTimeWindows(10), EventTimeTrigger, NoOpEvictor, NoOpWindowFunction) -> Sink: Unnamed (4/4)#0] INFO  org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Obtained shared RocksDB cache of size 20971520 bytes
23:43:29,992 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Flink Streaming Job (5351586c56778c282abf1993bd738a0d) switched from state RUNNING to CANCELLING.
{code}

Maybe we should think about increasing the {{akka.ask.timeout}} for the test runs or in general.;;;","20/Aug/21 15:16;trohrmann;However, we can maybe change that the {{MiniCluster}} uses a higher rpc timeout if nothing has been configured. That way we simulate a bit how the {{RestClusterClient}} and the REST handler handle timeouts. They use the {{WebOptions.TIMEOUT}} value that defaults to 10 minutes.;;;","24/Aug/21 10:35;trohrmann;Fixed via

1.14.0: 07f659b969ba47304a635e25d84a5cf2b9dd6d6f
1.13.3: 69eb23970bd43e35c8ffd09031613f6f8752060f
1.12.6: fc9da62894ca1e9d3187a653d4149da040005cb4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception on JobClient.get_job_status().result(),FLINK-22927,13382721,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,maver1ck,maver1ck,08/Jun/21 13:40,23/Sep/21 17:28,13/Jul/23 08:12,24/Jun/21 03:04,1.12.4,1.13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,API / Python,,,,,0,pull-request-available,,,,"Following code finish with exception
{code:java}
table_env.execute_sql(""""""
    CREATE TABLE IF NOT EXISTS datagen (
        id INT,
        data STRING
    ) WITH (
        'connector' = 'datagen'
    )
table_env.execute_sql(""""""
    CREATE TABLE IF NOT EXISTS print (
        id INT,
        data STRING
    ) WITH (
        'connector' = 'print'
    )
""""""){code}
{code:java}
table_result = table_env.execute_sql(""INSERT INTO print SELECT * FROM datagen"")
table_result.get_job_client().get_job_status().result()

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
ValueError: JavaObject id=o125 is not a valid JobStatus

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-16-ee785b26d044> in <module>
----> 1 result.get_job_client().get_job_status().result()

/usr/local/lib/python3.8/dist-packages/pyflink/common/completable_future.py in result(self)
     76             return self._j_completable_future.get()
     77         else:
---> 78             return self._py_class(self._j_completable_future.get())
     79 
     80     def exception(self):

/usr/lib/python3.8/enum.py in __call__(cls, value, names, module, qualname, type, start)
    307         """"""
    308         if names is None:  # simple value lookup
--> 309             return cls.__new__(cls, value)
    310         # otherwise, functional API: we're creating a new Enum type
    311         return cls._create_(value, names, module=module, qualname=qualname, type=type, start=start)

/usr/lib/python3.8/enum.py in __new__(cls, value)
    598                         )
    599             exc.__context__ = ve_exc
--> 600             raise exc
    601 
    602     def _generate_next_value_(name, start, count, last_values):

/usr/lib/python3.8/enum.py in __new__(cls, value)
    582         try:
    583             exc = None
--> 584             result = cls._missing_(value)
    585         except Exception as e:
    586             exc = e

/usr/lib/python3.8/enum.py in _missing_(cls, value)
    611     @classmethod
    612     def _missing_(cls, value):
--> 613         raise ValueError(""%r is not a valid %s"" % (value, cls.__name__))
    614 
    615     def __repr__(self):

ValueError: JavaObject id=o125 is not a valid JobStatus
{code}",,dian.fu,hxbks2ks,maver1ck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 24 03:04:48 UTC 2021,,,,,,,,,,"0|z0rrgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/21 12:17;hxbks2ks;Thanks a lot for the report. I will fix it as soon as possible. Besides, you can directly use `table_result.wait()` to get result.;;;","10/Jun/21 12:30;maver1ck;I'm using Flink from jupyter.

table_result.wait() is blocking call and stoping cell with this formula is not killing Flink job.

So I  need to use job_client.;;;","10/Jun/21 12:40;hxbks2ks;If you don’t want to block, I think you don't need to add additional logic after `execute_insert`. Do you want to judge the status of the job by TableResult and you have some additional logic depending that status?;;;","24/Jun/21 03:04;hxbks2ks;Merged into master via f998ec73158657ac1ea7fc09a670dc39b925b6e2
Merged into release-1.13 via d755fa945632c135a6179a837d776d21ec28f63c
Merged into release-1.12 via cdec22eec857e24508dc67099c5d4fef70a3200b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Guava version conflict in flink-format module,FLINK-22920,13382673,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sonice_lj,sujun1020,sujun1020,08/Jun/21 09:37,21/Jun/22 11:31,13/Jul/23 08:12,17/Jun/22 11:21,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.16.0,,,,,Connectors / ORC,,,,,0,pull-request-available,,,,"In the Flink-ORC and Flink-Parquet modules, The hadoop-common dependency contains the 11.0.2 version of guava, which conflicts with the 29.0-jre version required by the flink-table-planner-blink module. We should exclude guava from the hadoop-common dependency. Otherwise, running the unit test through the IDE throws a NoClassDefFoundError",,jark,lzljs3620320,martijnvisser,sonice_lj,sujun1020,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25746,,,,,,FLINK-28173,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 21 11:31:12 UTC 2022,,,,,,,,,,"0|z0rr6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/21 09:40;sujun1020;[~lzljs3620320] This issue appeared in the latest code of the master branch, Please confirm this issue;;;","15/Jun/22 04:31;sonice_lj;[~lzljs3620320] Problem still exists in the master branch. And FLINK-25746 reported too. 

Can you assign this ticket to me ? ;;;","15/Jun/22 05:41;lzljs3620320;This is an error under IDE, guava actually have been shaded, just IDE does not support shade;;;","15/Jun/22 06:31;sonice_lj;[~lzljs3620320] Yes, guava is shaded actually. But I think the real reason is the guava version conflict between calcite and hadoop-xxx. And hadoop-xxx is marked as provided scope, so mvn shell can pass.;;;","15/Jun/22 06:47;lzljs3620320;[~sonice_lj] Assigned to u~;;;","17/Jun/22 11:21;jark;Fixed in master: fd0d1298d4f447afd42de80124a853855f20fccb;;;","21/Jun/22 11:31;martijnvisser;[~sonice_lj] This PR causes FLINK-28173 - Can you look into that test stability, else we'll have to revert this commit. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileExecutionGraphInfoStoreTest.testPutSuspendedJobOnClusterShutdown should wait until job is running,FLINK-22908,13382588,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fpaul,xtsong,xtsong,08/Jun/21 02:07,17/Nov/21 15:21,13/Jul/23 08:12,11/Jun/21 10:07,1.12.4,1.13.1,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18754&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392&l=7744


{code}
Jun 08 00:03:01 [ERROR] Tests run: 10, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 4.21 s <<< FAILURE! - in org.apache.flink.runtime.dispatcher.FileExecutionGraphInfoStoreTest
Jun 08 00:03:01 [ERROR] testPutSuspendedJobOnClusterShutdown(org.apache.flink.runtime.dispatcher.FileExecutionGraphInfoStoreTest)  Time elapsed: 2.763 s  <<< ERROR!
Jun 08 00:03:01 org.apache.flink.util.FlinkException: Could not close resource.
Jun 08 00:03:01 	at org.apache.flink.util.AutoCloseableAsync.close(AutoCloseableAsync.java:39)
Jun 08 00:03:01 	at org.apache.flink.runtime.dispatcher.FileExecutionGraphInfoStoreTest.testPutSuspendedJobOnClusterShutdown(FileExecutionGraphInfoStoreTest.java:349)
Jun 08 00:03:01 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jun 08 00:03:01 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jun 08 00:03:01 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jun 08 00:03:01 	at java.lang.reflect.Method.invoke(Method.java:498)
Jun 08 00:03:01 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jun 08 00:03:01 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jun 08 00:03:01 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jun 08 00:03:01 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jun 08 00:03:01 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jun 08 00:03:01 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Jun 08 00:03:01 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jun 08 00:03:01 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jun 08 00:03:01 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jun 08 00:03:01 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Jun 08 00:03:01 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Jun 08 00:03:01 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Jun 08 00:03:01 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Jun 08 00:03:01 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Jun 08 00:03:01 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Jun 08 00:03:01 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Jun 08 00:03:01 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Jun 08 00:03:01 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Jun 08 00:03:01 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jun 08 00:03:01 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Jun 08 00:03:01 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Jun 08 00:03:01 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Jun 08 00:03:01 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Jun 08 00:03:01 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Jun 08 00:03:01 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jun 08 00:03:01 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jun 08 00:03:01 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jun 08 00:03:01 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Jun 08 00:03:01 Caused by: org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcException: Discard message, because the rpc endpoint akka://flink/user/rpc/resourcemanager_2 has not been started yet.
Jun 08 00:03:01 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:170)
Jun 08 00:03:01 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
Jun 08 00:03:01 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
Jun 08 00:03:01 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
Jun 08 00:03:01 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
Jun 08 00:03:01 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
Jun 08 00:03:01 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
Jun 08 00:03:01 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
Jun 08 00:03:01 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
Jun 08 00:03:01 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
Jun 08 00:03:01 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
Jun 08 00:03:01 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
Jun 08 00:03:01 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
Jun 08 00:03:01 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
Jun 08 00:03:01 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
Jun 08 00:03:01 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
Jun 08 00:03:01 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
Jun 08 00:03:01 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
Jun 08 00:03:01 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

{code}
",,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22980,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 11 07:03:44 UTC 2021,,,,,,,,,,"0|z0rqns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/21 07:37;trohrmann;[~fpaul] could you take a look at this problem?;;;","10/Jun/21 02:45;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18851&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392&l=7354;;;","11/Jun/21 02:04;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18890&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=7c61167f-30b3-5893-cc38-a9e3d057e392&l=7354;;;","11/Jun/21 07:03;chesnay;master: f52b2d4d78a3b6facc2af5cc589b3fcbeaa12826
1.13: e107e29fa930d4e08e25882f1b52b0d987741694
1.12: 11c20305b58484285cef0c4e98b30e27c70d29a1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Code of method xxx of class ""StreamExecCalc$1248"" grows beyond 64 KB",FLINK-22903,13382480,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,xlang,xlang,07/Jun/21 13:17,22/Jul/21 07:22,13/Jul/23 08:12,22/Jul/21 07:22,1.13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Runtime,,,,,0,,,,,"when I select a table from kafka, the logs is under below:
java.lang.RuntimeException: Could not instantiate generated class 'StreamExecCalc$1248'
    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:66)
    at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40)
    at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:80)
    at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperator(OperatorChain.java:652)
    at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:626)
    at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:566)
    at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:181)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.executeRestore(StreamTask.java:548)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:647)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:537)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:759)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:76)
    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:77)
    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:64)
    ... 12 more
Caused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203)
    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937)
    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74)
    ... 14 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:89)
    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74)
    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)
    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
    at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
    ... 17 more
Caused by: org.codehaus.janino.InternalCompilerException: Compiling ""StreamExecCalc$1248"": Code of method ""split$1245$(LStreamExecCalc$1248;)V"" of class ""StreamExecCalc$1248"" grows beyond 64 KB
    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
    at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86)
    ... 23 more
Caused by: org.codehaus.janino.InternalCompilerException: Code of method ""split$1245$(LStreamExecCalc$1248;)V"" of class ""StreamExecCalc$1248"" grows beyond 64 KB
    at org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:1048)
    at org.codehaus.janino.CodeContext.write(CodeContext.java:957)
    at org.codehaus.janino.CodeContext.writeBranch(CodeContext.java:1077)
    at org.codehaus.janino.UnitCompiler.writeBranch(UnitCompiler.java:12301)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2466)
    at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495)
    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487)
    at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2468)
    at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495)
    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487)
    at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2468)
    at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495)
    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487)
    at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553)
    at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493)
    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487)
    at org.codehaus.janino.Java$Block.accept(Java.java:2779)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2486)
    at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495)
    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487)
    at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553)
    at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493)
    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487)
    at org.codehaus.janino.Java$Block.accept(Java.java:2779)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2468)
    at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495)
    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487)
    at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
    at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
    at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
    ... 30 more","my sql query is under below:

insert into `jz_lbs_data_cnt` 
 select case when substr(geohash,1,8) in ('wecntxr1','wecntxr3','wecntxr9','wecntxrc','wecntz21','wecntz23','wecntxr4','wecntxr6','wecntxrd','wecntxrf','wecntz24','wecntz26','wecntz2d','wecntxr7','wecntxre','wecntxrg','wecntz25','wecntz27','wecntz2e','wecntxrk','wecntxrs','wecntxru','wecntz2h','wecntz2k','wecntz2s','wecntz2u','wecntxrm','wecntxrt','wecntxrv','wecntz2j','wecntz2m','wecntz2t','wecntz2v','wecntz3j','wecntxrq','wecntxrw','wecntxry','wecntz2n','wecntz2q','wecntz2w','wecntz2y','wecntz3n','wecntxrx','wecntxrz','wecntz2p','wecntz2r','wecntz2x','wecntz2z','wecntz3p','wecntz3r','wecntxx8','wecntxxb','wecntz80','wecntz82','wecntz88','wecntz8b','wecntz90','wecntz92','wecntxxc','wecntz81','wecntz83','wecntz89','wecntz8c','wecntz91','wecntz93','wecntz84','wecntz86','wecntz8d','wecntz8f','wecntz94','wecntz96','wecntz9d','wecntz87','wecntz8e','wecntz8g','wecntz95','wecntz97','wecntz9e','wecntz8s','wecntz8u','wecntz9h','wecntz9k','wecntz9s','wecntz9u','wecntz8v','wecntz9j','wecntz9m','wecntz9t','wecntz9v','wecntz8y','wecntz9n','wecntz9q','wecntz9w','wecntz9p','wecntz9r','wecntzc2') then '202008240001' 
 when substr(geohash,1,8) in ('wecnwrvh','wecnwrvk','wecnwrvs','wecnwrvu','wecnwryh','wecnwryk','wecnwrum','wecnwrut','wecnwruv','wecnwrvj','wecnwrvm','wecnwrvt','wecnwrvv','wecnwryj','wecnwrym','wecnwryt','wecnwrun','wecnwruq','wecnwruw','wecnwruy','wecnwrvn','wecnwrvq','wecnwrvw','wecnwrvy','wecnwryn','wecnwryq','wecnwryw','wecnwrgz','wecnwrup','wecnwrur','wecnwrux','wecnwruz','wecnwrvp','wecnwrvr','wecnwrvx','wecnwrvz','wecnwryp','wecnwryr','wecnwryx','wecny258','wecny25b','wecny2h0','wecny2h2','wecny2h8','wecny2hb','wecny2j0','wecny2j2','wecny2j8','wecny2jb','wecny2n0','wecny2n2','wecny2n8','wecny259','wecny25c','wecny2h1','wecny2h3','wecny2h9','wecny2hc','wecny2j1','wecny2j3','wecny2j9','wecny2jc','wecny2n1','wecny25d','wecny25f','wecny2h4','wecny2h6','wecny2hd','wecny2hf','wecny2j4','wecny2j6','wecny2jd','wecny2jf','wecny257','wecny25e','wecny25g','wecny2h5','wecny2h7','wecny2he','wecny2hg','wecny2j5','wecny2j7','wecny2je','wecny25k','wecny25s','wecny25u','wecny2hh','wecny2hk','wecny2hs','wecny2hu','wecny2jh','wecny25m','wecny25t','wecny25v','wecny2hj','wecny2hm','wecny2ht','wecny2hv','wecny25q','wecny25w','wecny25y','wecny2hn','wecny2hq','wecny2hw','wecny25x','wecny25z','wecny2hp') then '202105180001' else '其他' end as tag
 ,TUMBLE_START(eventTime, INTERVAL '10' MINUTE) as window_start
 ,TUMBLE_END(eventTime, INTERVAL '10' MINUTE) as window_end
 ,count(distinct gid) as cnt
 from chengjw.gt_lbs_type11
 where
 -- area in (0,81000000) and
 substr(geohash,1,8) in ('wecntxr1','wecntxr3','wecntxr9','wecntxrc','wecntz21','wecntz23','wecntxr4','wecntxr6','wecntxrd','wecntxrf','wecntz24','wecntz26','wecntz2d','wecntxr7','wecntxre','wecntxrg','wecntz25','wecntz27','wecntz2e','wecntxrk','wecntxrs','wecntxru','wecntz2h','wecntz2k','wecntz2s','wecntz2u','wecntxrm','wecntxrt','wecntxrv','wecntz2j','wecntz2m','wecntz2t','wecntz2v','wecntz3j','wecntxrq','wecntxrw','wecntxry','wecntz2n','wecntz2q','wecntz2w','wecntz2y','wecntz3n','wecntxrx','wecntxrz','wecntz2p','wecntz2r','wecntz2x','wecntz2z','wecntz3p','wecntz3r','wecntxx8','wecntxxb','wecntz80','wecntz82','wecntz88','wecntz8b','wecntz90','wecntz92','wecntxxc','wecntz81','wecntz83','wecntz89','wecntz8c','wecntz91','wecntz93','wecntz84','wecntz86','wecntz8d','wecntz8f','wecntz94','wecntz96','wecntz9d','wecntz87','wecntz8e','wecntz8g','wecntz95','wecntz97','wecntz9e','wecntz8s','wecntz8u','wecntz9h','wecntz9k','wecntz9s','wecntz9u','wecntz8v','wecntz9j','wecntz9m','wecntz9t','wecntz9v','wecntz8y','wecntz9n','wecntz9q','wecntz9w','wecntz9p','wecntz9r','wecntzc2','wecnwrvh','wecnwrvk','wecnwrvs','wecnwrvu','wecnwryh','wecnwryk','wecnwrum','wecnwrut','wecnwruv','wecnwrvj','wecnwrvm','wecnwrvt','wecnwrvv','wecnwryj','wecnwrym','wecnwryt','wecnwrun','wecnwruq','wecnwruw','wecnwruy','wecnwrvn','wecnwrvq','wecnwrvw','wecnwrvy','wecnwryn','wecnwryq','wecnwryw','wecnwrgz','wecnwrup','wecnwrur','wecnwrux','wecnwruz','wecnwrvp','wecnwrvr','wecnwrvx','wecnwrvz','wecnwryp','wecnwryr','wecnwryx','wecny258','wecny25b','wecny2h0','wecny2h2','wecny2h8','wecny2hb','wecny2j0','wecny2j2','wecny2j8','wecny2jb','wecny2n0','wecny2n2','wecny2n8','wecny259','wecny25c','wecny2h1','wecny2h3','wecny2h9','wecny2hc','wecny2j1','wecny2j3','wecny2j9','wecny2jc','wecny2n1','wecny25d','wecny25f','wecny2h4','wecny2h6','wecny2hd','wecny2hf','wecny2j4','wecny2j6','wecny2jd','wecny2jf','wecny257','wecny25e','wecny25g','wecny2h5','wecny2h7','wecny2he','wecny2hg','wecny2j5','wecny2j7','wecny2je','wecny25k','wecny25s','wecny25u','wecny2hh','wecny2hk','wecny2hs','wecny2hu','wecny2jh','wecny25m','wecny25t','wecny25v','wecny2hj','wecny2hm','wecny2ht','wecny2hv','wecny25q','wecny25w','wecny25y','wecny2hn','wecny2hq','wecny2hw','wecny25x','wecny25z','wecny2hp')
 group by case when substr(geohash,1,8) in ('wecntxr1','wecntxr3','wecntxr9','wecntxrc','wecntz21','wecntz23','wecntxr4','wecntxr6','wecntxrd','wecntxrf','wecntz24','wecntz26','wecntz2d','wecntxr7','wecntxre','wecntxrg','wecntz25','wecntz27','wecntz2e','wecntxrk','wecntxrs','wecntxru','wecntz2h','wecntz2k','wecntz2s','wecntz2u','wecntxrm','wecntxrt','wecntxrv','wecntz2j','wecntz2m','wecntz2t','wecntz2v','wecntz3j','wecntxrq','wecntxrw','wecntxry','wecntz2n','wecntz2q','wecntz2w','wecntz2y','wecntz3n','wecntxrx','wecntxrz','wecntz2p','wecntz2r','wecntz2x','wecntz2z','wecntz3p','wecntz3r','wecntxx8','wecntxxb','wecntz80','wecntz82','wecntz88','wecntz8b','wecntz90','wecntz92','wecntxxc','wecntz81','wecntz83','wecntz89','wecntz8c','wecntz91','wecntz93','wecntz84','wecntz86','wecntz8d','wecntz8f','wecntz94','wecntz96','wecntz9d','wecntz87','wecntz8e','wecntz8g','wecntz95','wecntz97','wecntz9e','wecntz8s','wecntz8u','wecntz9h','wecntz9k','wecntz9s','wecntz9u','wecntz8v','wecntz9j','wecntz9m','wecntz9t','wecntz9v','wecntz8y','wecntz9n','wecntz9q','wecntz9w','wecntz9p','wecntz9r','wecntzc2') then '202008240001' 
 when substr(geohash,1,8) in ('wecnwrvh','wecnwrvk','wecnwrvs','wecnwrvu','wecnwryh','wecnwryk','wecnwrum','wecnwrut','wecnwruv','wecnwrvj','wecnwrvm','wecnwrvt','wecnwrvv','wecnwryj','wecnwrym','wecnwryt','wecnwrun','wecnwruq','wecnwruw','wecnwruy','wecnwrvn','wecnwrvq','wecnwrvw','wecnwrvy','wecnwryn','wecnwryq','wecnwryw','wecnwrgz','wecnwrup','wecnwrur','wecnwrux','wecnwruz','wecnwrvp','wecnwrvr','wecnwrvx','wecnwrvz','wecnwryp','wecnwryr','wecnwryx','wecny258','wecny25b','wecny2h0','wecny2h2','wecny2h8','wecny2hb','wecny2j0','wecny2j2','wecny2j8','wecny2jb','wecny2n0','wecny2n2','wecny2n8','wecny259','wecny25c','wecny2h1','wecny2h3','wecny2h9','wecny2hc','wecny2j1','wecny2j3','wecny2j9','wecny2jc','wecny2n1','wecny25d','wecny25f','wecny2h4','wecny2h6','wecny2hd','wecny2hf','wecny2j4','wecny2j6','wecny2jd','wecny2jf','wecny257','wecny25e','wecny25g','wecny2h5','wecny2h7','wecny2he','wecny2hg','wecny2j5','wecny2j7','wecny2je','wecny25k','wecny25s','wecny25u','wecny2hh','wecny2hk','wecny2hs','wecny2hu','wecny2jh','wecny25m','wecny25t','wecny25v','wecny2hj','wecny2hm','wecny2ht','wecny2hv','wecny25q','wecny25w','wecny25y','wecny2hn','wecny2hq','wecny2hw','wecny25x','wecny25z','wecny2hp') then '202105180001' else '其他' end 
 ,TUMBLE(eventTime, INTERVAL '10' MINUTE) 
 ;",lzljs3620320,xlang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23007,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 22 07:22:15 UTC 2021,,,,,,,,,,"0|z0rpzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/21 01:48;lzljs3620320;Thanks for the reporting, [~xlang] Can you show the SQL? ;;;","22/Jul/21 07:22;lzljs3620320;This should be resolved in FLINK-23007, you can check latest master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveParallelismInference limit return wrong parallelism,FLINK-22898,13382402,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,lzljs3620320,lzljs3620320,07/Jun/21 08:02,14/Dec/21 09:05,13/Jul/23 08:12,15/Jul/21 12:10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,Table SQL / Ecosystem,,,,,0,auto-deprioritized-critical,pull-request-available,,,,,lirui,luoyuxia,lzljs3620320,tartarus,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 15 12:07:27 UTC 2021,,,,,,,,,,"0|z0rpig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/21 08:05;lzljs3620320;When no source infer, should return -1 instead of 1.;;;","14/Jun/21 22:41;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","22/Jun/21 22:40;flink-jira-bot;This issue was labeled ""stale-critical"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","28/Jun/21 08:32;luoyuxia;[~lzljs3620320]

when no souce infer, can we just return value 'table.exec.resource.default-parallelism' which may be not -1 when user set the option ?;;;","29/Jun/21 02:35;lzljs3620320;Yes.;;;","06/Jul/21 06:53;lirui;Fixed in master: a765da5dc9bc7f56971d1714abc8bce4796f9610

[~luoyuxia] Please open PRs for release-1.13 and release-1.12. A simple cherry-pick doesn't compile.;;;","15/Jul/21 12:07;lirui;Fixed in release-1.13: 004d3b58e4f4637efb09e3c7c84f01c3bd64ad9f
Fixed in release-1.12: 344f6e42cdcdcc4d9d98fe414016138cdd748bf3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Example typo in ""Table Concepts & Common API"" page",FLINK-22895,13382370,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sujun1020,sujun1020,sujun1020,07/Jun/21 03:05,23/Sep/21 17:23,13/Jul/23 08:12,07/Jun/21 09:49,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Documentation,Table SQL / API,,,,0,pull-request-available,,,,"The variable is misspelled, the setting should be changed to settings.

The document url is: [https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/table/common/#create-a-tableenvironment]

 

!image-2021-06-07-11-04-30-998.png!",,jark,sujun1020,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/21 03:04;sujun1020;image-2021-06-07-11-04-30-998.png;https://issues.apache.org/jira/secure/attachment/13026467/image-2021-06-07-11-04-30-998.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 01 11:06:57 UTC 2021,,,,,,,,,,"0|z0rpbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/21 03:08;sujun1020;[~jark]  If the community wants to fix it, you can assign it to me;;;","07/Jun/21 09:49;jark;Fixed in master: 11301c254269c1a3b3d7e1a34922393e3707e28a;;;","01/Sep/21 11:06;chesnay;1.13: b7f71ca75ee209fceef8199699387aa5c5bf6487 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Window Top-N should allow n=1,FLINK-22894,13382352,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingzhang,alpinegizmo,alpinegizmo,06/Jun/21 18:35,23/Sep/21 17:25,13/Jul/23 08:12,09/Jun/21 04:22,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"I tried to reimplement the Hourly Tips exercise from the DataStream training using Flink SQL. The objective of this exercise is to find the one taxi driver who earned the most in tips during each hour, and report that driver's driverId and the sum of their tips. 

This can be expressed as a window top-n query, where n=1, as in

{{FROM (}}
{{  SELECT *, ROW_NUMBER() OVER }}{{(PARTITION BY window_start, window_end ORDER BY sumOfTips DESC) as rownum}}
{{  FROM ( }}
{{    SELECT driverId, window_start, window_end, sum(tip) as sumOfTips}}
{{    FROM TABLE( }}
{{      TUMBLE(TABLE fares, DESCRIPTOR(startTime), INTERVAL '1' HOUR))}}
{{    GROUP BY driverId, window_start, window_end}}
{{  )}}
{{) WHERE rownum = 1;}}

 

This fails because the {{WindowRankOperatorBuilder}} insists on {{rankEnd > 1. }}So, in other words, while it is possible to report the top 2 drivers, or the driver in 2nd place, it's not possible to report only the top driver.

This appears to be an off-by-one error in the range checking.

 

 ",,alpinegizmo,jingzhang,leonard,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 09 04:22:07 UTC 2021,,,,,,,,,,"0|z0rp7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/21 02:39;jingzhang;[~alpinegizmo] Thanks for reporting the bug. I would like to fix it soon.;;;","07/Jun/21 02:50;lzljs3620320;[~alpinegizmo] Thanks! [~qingru zhang] Assigned to u~;;;","09/Jun/21 04:22;lzljs3620320;Fixed via:

master: fcc4b89b7568cd955b139c8ebc5aa8f46e136a53

release-1.13: 160cfa41e406bda18e0cb257c5628039d0900e75;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Leader retrieval fails with NoNodeException,FLINK-22893,13382351,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,dwysakowicz,dwysakowicz,06/Jun/21 18:22,05/Apr/23 15:25,13/Jul/23 08:12,26/Jul/21 07:36,1.11.1,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"The NodeCache used by the LeaderElection-/-RetrievalDrivers ensures that parents to the observed node exists by regularly issuing mkdir calls. This operation can fail if concurrently the HA data is being cleaned up, which results in curator throwing an unhandled exception which crashes the TM.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18700&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=2c7d57b9-7341-5a87-c9af-2cf7cc1a37dc&l=4382",,akalashnikov,dwysakowicz,frank wang,mapohl,pnowojski,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23323,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 26 07:36:47 UTC 2021,,,,,,,,,,"0|z0rp74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/21 13:59;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=445&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=634cd701-c189-5dff-24cb-606ed884db87&l=4775;;;","15/Jul/21 07:53;akalashnikov;This test fails because the job could not start  due to NoNode for zookeeper:
{code:java}
11:31:41,156 [ Curator-Framework-0] ERROR org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl [] - Background exception was not retry-able or retry gave up
org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /flink/default/jobs/0cfb90b498a0da0d4b882dc0fc6ace19/leader
        at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:114) ~[flink-shaded-zookeeper-3-3.4.14-13.0.jar:3.4.14-13.0]
        at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:54) ~[flink-shaded-zookeeper-3-3.4.14-13.0.jar:3.4.14-13.0]
        at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:792) ~[flink-shaded-zookeeper-3-3.4.14-13.0.jar:3.4.14-13.0]
        at org.apache.flink.shaded.curator4.org.apache.curator.utils.ZKPaths.mkdirs(ZKPaths.java:308) ~[flink-shaded-zookeeper-3-3.4.14-13.0.jar:3.4.14-13.0]
        at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CreateBuilderImpl$9.performBackgroundOperation(CreateBuilderImpl.java:801) ~[flink-shaded-zookeeper-3-3.4.14-13.0.jar:3.4.14-13.0]
        at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.OperationAndData.callPerformBackgroundOperation(OperationAndData.java:84) ~[flink-shaded-zookeeper-3-3.4.14-13.0.jar:3.4.14-13.0]
        at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:965) [flink-shaded-zookeeper-3-3.4.14-13.0.jar:3.4.14-13.0]
        at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:943) [flink-shaded-zookeeper-3-3.4.14-13.0.jar:3.4.14-13.0]
        at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:66) [flink-shaded-zookeeper-3-3.4.14-13.0.jar:3.4.14-13.0]
        at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:346) [flink-shaded-zookeeper-3-3.4.14-13.0.jar:3.4.14-13.0]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_282]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_282]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_282]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_282]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_282]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
{code};;;","19/Jul/21 09:31;chesnay;A similar issue was reporter half a year ago on the mailing lists for 1.11.1: http://apache-flink.147419.n8.nabble.com/flink-yarn-ha-tt10715.html#none;;;","19/Jul/21 10:05;chesnay;I have assigned myself to this issue because we believe this to be an issue in our Zookeeper code.;;;","26/Jul/21 07:36;chesnay;master: 6d0e2cf6884e9c47ea5e55d3b2523e2a1570e290;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FineGrainedSlotManagerDefaultResourceAllocationStrategyITCase fails on azure,FLINK-22891,13382349,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,dwysakowicz,dwysakowicz,06/Jun/21 18:15,30/Nov/21 20:37,13/Jul/23 08:12,06/Aug/21 07:36,1.13.1,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18700&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=05b74a19-4ee4-5036-c46f-ada307df6cf0&l=8660

{code}
Jun 05 21:16:00 [ERROR] Tests run: 11, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 6.24 s <<< FAILURE! - in org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerDefaultResourceAllocationStrategyITCase
Jun 05 21:16:00 [ERROR] testResourceCanBeAllocatedForDifferentJobWithDeclarationBeforeSlotFree(org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerDefaultResourceAllocationStrategyITCase)  Time elapsed: 5.015 s  <<< ERROR!
Jun 05 21:16:00 java.util.concurrent.TimeoutException
Jun 05 21:16:00 	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1784)
Jun 05 21:16:00 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
Jun 05 21:16:00 	at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTestBase.assertFutureCompleteAndReturn(FineGrainedSlotManagerTestBase.java:121)
Jun 05 21:16:00 	at org.apache.flink.runtime.resourcemanager.slotmanager.AbstractFineGrainedSlotManagerITCase$4.lambda$new$4(AbstractFineGrainedSlotManagerITCase.java:374)
Jun 05 21:16:00 	at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTestBase$Context.runTest(FineGrainedSlotManagerTestBase.java:212)
Jun 05 21:16:00 	at org.apache.flink.runtime.resourcemanager.slotmanager.AbstractFineGrainedSlotManagerITCase$4.<init>(AbstractFineGrainedSlotManagerITCase.java:310)
Jun 05 21:16:00 	at org.apache.flink.runtime.resourcemanager.slotmanager.AbstractFineGrainedSlotManagerITCase.testResourceCanBeAllocatedForDifferentJobAfterFree(AbstractFineGrainedSlotManagerITCase.java:308)
Jun 05 21:16:00 	at org.apache.flink.runtime.resourcemanager.slotmanager.AbstractFineGrainedSlotManagerITCase.testResourceCanBeAllocatedForDifferentJobWithDeclarationBeforeSlotFree(AbstractFineGrainedSlotManagerITCase.java:262)
Jun 05 21:16:00 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jun 05 21:16:00 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jun 05 21:16:00 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jun 05 21:16:00 	at java.lang.reflect.Method.invoke(Method.java:498)
Jun 05 21:16:00 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jun 05 21:16:00 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jun 05 21:16:00 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jun 05 21:16:00 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jun 05 21:16:00 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jun 05 21:16:00 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Jun 05 21:16:00 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jun 05 21:16:00 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jun 05 21:16:00 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jun 05 21:16:00 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Jun 05 21:16:00 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Jun 05 21:16:00 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Jun 05 21:16:00 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Jun 05 21:16:00 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Jun 05 21:16:00 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Jun 05 21:16:00 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Jun 05 21:16:00 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jun 05 21:16:00 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Jun 05 21:16:00 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Jun 05 21:16:00 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Jun 05 21:16:00 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Jun 05 21:16:00 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Jun 05 21:16:00 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jun 05 21:16:00 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jun 05 21:16:00 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jun 05 21:16:00 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Jun 05 21:16:00 

{code}",,akalashnikov,dwysakowicz,guoyangze,trohrmann,wangyang0918,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 06 07:36:30 UTC 2021,,,,,,,,,,"0|z0rp6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/21 02:53;guoyangze;I've run this test over 10,000 times in my local env. My gut feeling is that it just be scheduled to an overloaded machine. Let's see how frequent this issue is.;;;","08/Jun/21 07:38;trohrmann;If we cannot reproduce the problem, then let's close it as cannot reproduce [~guoyangze].;;;","08/Jun/21 07:42;guoyangze;Please reopen the ticket if the issue occurs again.;;;","11/Jun/21 15:57;akalashnikov;[~guoyangze], I still observe this problem on the fresh changes - [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18912&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=6980]

 

I believe we should keep at least one such ticket open to collect the problems. The guess about the overloaded machines is not so bad, maybe it makes sense to increase corresponded timeout specifically for this test, at least we will be sure that it is not any deadlock or the missing completion.;;;","17/Jun/21 03:43;xtsong;New instance on the 1.13 branch.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19037&view=logs&j=d8d26c26-7ec2-5ed2-772e-7a1a1eb8317c&t=be5fb08e-1ad7-563c-4f1a-a97ad4ce4865&l=6837;;;","17/Jun/21 07:12;guoyangze;I'll take another look into it.;;;","25/Jun/21 03:04;guoyangze;Hi, there. Sorry that I still failed to reproduce that issue and find some clues. However, I find that sometimes the {{FineGrainedSlotManagerDefaultResourceAllocationStrategyITCase}} can run for over 7s. It might make sense to increase the timeout to 10s for the moment. WDYT?;;;","28/Jun/21 02:18;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19579&view=logs&j=6bfdaf55-0c08-5e3f-a2d2-2a0285fd41cf&t=fd9796c3-9ce8-5619-781c-42f873e126a6&l=6921;;;","12/Jul/21 08:46;trohrmann;cc [~guoyangze].;;;","12/Jul/21 09:41;guoyangze;I still cannot reproduce the issue. However, there is indeed a bug in this test which is filed in FLINK-23359. I'll keep an eye on this ticket after the fix of FLINK-23359.;;;","28/Jul/21 06:41;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21053&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=8172;;;","28/Jul/21 09:21;guoyangze;The log shows there is something wrong with the scheduling behavior. I'll take another look.;;;","30/Jul/21 02:26;guoyangze;After a deeper investigation, I think the root cause is that the return of {{ScheduledFuture#isDone}} can be a false negative. And thus we missing a schedule for the {{checkResourceRequirements}}.
The core logic is located in FutureTask#run.

{code:java}
public void run() {
    if (state != NEW ||
        !UNSAFE.compareAndSwapObject(this, runnerOffset,
                                        null, Thread.currentThread()))
        return;
    try {
        Callable<V> c = callable;
        if (c != null && state == NEW) {
            V result;
            boolean ran;
            try {
                result = c.call();
                ran = true;
            } catch (Throwable ex) {
                result = null;
                ran = false;
                setException(ex);
            }
            if (ran)
                set(result);
        }
    } finally {
        // runner must be non-null until state is settled to
        // prevent concurrent calls to run()
        runner = null;
        // state must be re-read after nulling runner to prevent
        // leaked interrupts
        int s = state;
        if (s >= INTERRUPTING)
            handlePossibleCancellationInterrupt(s);
    }
}
{code}

The {{ScheduledFuture#isDone}} will return true after the execution of {{set(result)}}. Howeveer, if we call the {{isDone}} between {{set(result)}} and {{result = c.call()}}, it can get an intermediate state and do not schedule another {{checkResourceRequirements}} as expected.

One possible solution is to replace the {{ScheduledFuture}} with a {{CompletableFuture}} and complete it at the end of {{checkResourceRequirements}}.;;;","06/Aug/21 07:36;xtsong;Fixed via
- master (1.14): f367a226acf941bc41c6d49c9f8fa02e53c687bf
- release-1.13: 568c649cb1e1aab53e5849c2c2ea7630bcc4da75;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Few tests fail in HiveTableSinkITCase,FLINK-22890,13382348,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,06/Jun/21 18:14,22/Feb/22 07:06,13/Jul/23 08:12,01/Jul/21 09:07,1.13.1,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Connectors / Hive,Table SQL / Ecosystem,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18692&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=420bd9ec-164e-562e-8947-0dacde3cec91&l=23189

{code}
Jun 05 01:22:13 [ERROR] Errors: 
Jun 05 01:22:13 [ERROR]   HiveTableSinkITCase.testBatchAppend:138 » Validation Could not execute CREATE ...
Jun 05 01:22:13 [ERROR]   HiveTableSinkITCase.testDefaultSerPartStreamingWrite:156->testStreamingWrite:494 » Validation
Jun 05 01:22:13 [ERROR]   HiveTableSinkITCase.testHiveTableSinkWithParallelismInStreaming:100->testHiveTableSinkWithParallelismBase:108 » Validation
Jun 05 01:22:13 [ERROR]   HiveTableSinkITCase.testPartStreamingMrWrite:179->testStreamingWrite:423 » Validation
Jun 05 01:22:13 [ERROR]   HiveTableSinkITCase.testStreamingSinkWithTimestampLtzWatermark:360->fetchRows:384 » TestTimedOut
{code}",,dwysakowicz,fpaul,jingzhang,leonard,lirui,xtsong,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23079,FLINK-22735,,,,,,,,,,FLINK-22416,FLINK-22926,FLINK-23011,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 01 09:07:25 UTC 2021,,,,,,,,,,"0|z0rp6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/21 18:14;dwysakowicz;cc [~lirui];;;","07/Jun/21 02:41;lirui;The root cause seems to be  {{testStreamingSinkWithTimestampLtzWatermark}} times out and fails to clean up the test DB. I'll take a look into that.;;;","07/Jun/21 16:12;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18730&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf&l=23595;;;","08/Jun/21 05:42;lirui;I have pushed a fix to:
* master: a2a637e91ea97871bf856cf2ac61948b9bad5cee
* release-1.13: a1dbcc9b939bbf0b2c53ecf7580882abec62c0d6

It solves the issue in my local test. We can keep this open for a while to verify the effect.;;;","16/Jun/21 06:42;lirui;Fixed in release-1.12: 654e2a637a09568670c5b8f538647321a2800ddb;;;","21/Jun/21 02:36;xtsong;Failed again on master, with the fixing commit included.
cc [~lirui]

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19167&view=logs&j=8fd975ef-f478-511d-4997-6f15fe8a1fd3&t=ac0fa443-5d45-5a6b-3597-0310ecc1d2ab&l=25531;;;","21/Jun/21 07:23;dwysakowicz;The test crashes also JVM:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19065&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf&l=22862;;;","21/Jun/21 11:56;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19208&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf&l=22545;;;","21/Jun/21 11:57;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19209&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf&l=22543;;;","21/Jun/21 12:55;lirui;[~dwysakowicz] I did some debugging and I think the recent failures are related to FLINK-22881. When the watermark assigner operator tries to advance watermark, {{ChainingOutput::emitWatermark}} method somehow believes the announced status is idle and doesn't propagate watermark to downstream operators.

In {{testStreamingSinkWithTimestampLtzWatermark}}, Hive's partition committer relies on watermark to commit new partitions. Since watermark doesn't advance, the partition was never committed and therefore the reader can't consume the data written to the partition.;;;","21/Jun/21 13:10;lirui;I meant it's related to this [commit|https://github.com/apache/flink/commit/2c260b53a37dfcc040c8d5cfbca36d010f9a3cef] .;;;","21/Jun/21 13:22;dwysakowicz;I'll check it. Do you know if the same problem occurs in 1.13 and earlier versions? The commit you refer to, actually reverts the 1.14 branch to the behaviour from older versions.;;;","21/Jun/21 14:44;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19218&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf&l=22738;;;","22/Jun/21 03:22;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19238&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf&l=22515;;;","22/Jun/21 03:23;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19248&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf&l=22515;;;","22/Jun/21 03:24;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19247&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf&l=22538;;;","22/Jun/21 03:26;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19258&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf&l=22538;;;","22/Jun/21 03:28;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19262&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf&l=22515;;;","22/Jun/21 03:29;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19264&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf&l=22498;;;","22/Jun/21 03:34;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19264&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=420bd9ec-164e-562e-8947-0dacde3cec91&l=22517;;;","22/Jun/21 03:34;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19264&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354&l=23171;;;","22/Jun/21 03:34;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19264&view=logs&j=8fd975ef-f478-511d-4997-6f15fe8a1fd3&t=ac0fa443-5d45-5a6b-3597-0310ecc1d2ab&l=25554;;;","22/Jun/21 03:35;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19264&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=9c1ddabe-d186-5a2c-5fcc-f3cafb3ec699&l=22894;;;","22/Jun/21 03:36;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19267&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf&l=22896;;;","22/Jun/21 03:59;lirui;bq. I'll check it. Do you know if the same problem occurs in 1.13 and earlier versions? The commit you refer to, actually reverts the 1.14 branch to the behaviour from older versions.
None that I know, and the test still works on 1.13 branch.;;;","22/Jun/21 07:33;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19274&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf&l=22539;;;","22/Jun/21 07:33;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19276&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf&l=22539;;;","22/Jun/21 09:36;jingzhang;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19234&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf];;;","22/Jun/21 09:36;jingzhang;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19268&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf] ;;;","22/Jun/21 09:36;jingzhang;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19273&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf];;;","22/Jun/21 11:29;fpaul;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19239&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf;;;","24/Jun/21 07:28;yunta;Since this ticket is a blocker for relaseing 1.13.2, will you plan to create a fix PR recently [~dwysakowicz]?;;;","24/Jun/21 07:42;dwysakowicz;I will downgrade the issue as I fixed the recent failures already. It was a blocker for 1.14 only. I don't want to close the ticket yet as the previous failures were unrelated to FLINK-22881.;;;","01/Jul/21 09:07;dwysakowicz;Closing as fixed. The failure from 21.07 has been fixed, the previous ones seem to be fixed as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JdbcExactlyOnceSinkE2eTest.testInsert hangs on azure,FLINK-22889,13382347,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,dwysakowicz,dwysakowicz,06/Jun/21 18:08,23/Sep/21 18:53,13/Jul/23 08:12,14/Sep/21 07:59,1.13.1,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Connectors / JDBC,,,,,0,pull-request-available,test-stability,,,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18690&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51&l=16658,,dmvk,dwysakowicz,rmetzger,roman,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22462,,,,,FLINK-23862,,,,,,,,FLINK-23437,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 14 07:59:04 UTC 2021,,,,,,,,,,"0|z0rp68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/21 18:09;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18690&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=5d6e4255-0ea8-5e2a-f52c-c881b7872361&l=21175;;;","06/Jun/21 18:10;dwysakowicz;different stacktrace: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18690&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=b6c4efed-9c7d-55ea-03a9-9bd7d5b08e4c&l=19474;;;","06/Jun/21 18:20;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18700&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51&l=20781;;;","06/Jun/21 18:21;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18700&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=5d6e4255-0ea8-5e2a-f52c-c881b7872361&l=21490
;;;","06/Jun/21 18:23;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18700&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=43529380-51b4-5e90-5af4-2dccec0ef402&l=20033;;;","06/Jun/21 18:23;dwysakowicz;cc [~roman_khachatryan];;;","07/Jun/21 02:53;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18707&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=19861;;;","07/Jun/21 02:56;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18707&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=60581941-0138-53c0-39fe-86d62be5f407&l=16599;;;","07/Jun/21 02:56;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18707&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51&l=20829;;;","07/Jun/21 02:57;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18707&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=b6c4efed-9c7d-55ea-03a9-9bd7d5b08e4c&l=13502;;;","07/Jun/21 02:58;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18707&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=43529380-51b4-5e90-5af4-2dccec0ef402&l=19989;;;","07/Jun/21 16:11;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18724&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=20031;;;","07/Jun/21 16:14;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18737&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=20397;;;","08/Jun/21 01:54;xtsong;Test case is temporally disabled in ee432b2ebb4557699d4ad9fee71f15e361e44ac5.
Keep the ticket open until the problem is properly fixed in FLINK-22462 and the test is re-activated.
Downgrade to Critical.;;;","08/Jun/21 02:18;xtsong;Instances before the test is ignored:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18754&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51&l=20671

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18754&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=b6c4efed-9c7d-55ea-03a9-9bd7d5b08e4c&l=16482

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18754&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=43529380-51b4-5e90-5af4-2dccec0ef402&l=19914

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18756&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=b6c4efed-9c7d-55ea-03a9-9bd7d5b08e4c&l=17051;;;","09/Jun/21 02:44;xtsong;on 1.13

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18803&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=20268

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18803&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51&l=20572

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18803&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=5d6e4255-0ea8-5e2a-f52c-c881b7872361&l=22033

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18803&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=43529380-51b4-5e90-5af4-2dccec0ef402&l=20610;;;","09/Jun/21 10:49;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18817&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=19962;;;","09/Jun/21 10:56;roman;Test ignored in master since ee432b2ebb4557699d4ad9fee71f15e361e44ac5,
in 1.13 since 41a21454da1c1ddb38bcd7aaae058b2040257bd4.;;;","23/Jun/21 07:03;roman;Closing the issue, tests were fixed and unignored in FLINK-22462.;;;","24/Jun/21 07:40;dwysakowicz;Why was it closed as a duplicate? Which issue does it duplicate?;;;","24/Jun/21 07:40;dwysakowicz;1.13: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19436&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=16533;;;","24/Jun/21 09:20;roman;It duplicates  !https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype|width=16,height=16!  FLINK-22462  (I'll change the link type).;;;","05/Jul/21 03:52;xtsong;Reopen the ticket. The problem seems still exist.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19865&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51&l=14538;;;","06/Jul/21 03:06;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19944&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=b6c4efed-9c7d-55ea-03a9-9bd7d5b08e4c&l=15351;;;","06/Jul/21 03:06;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19944&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=b6c4efed-9c7d-55ea-03a9-9bd7d5b08e4c&l=15351;;;","06/Jul/21 11:41;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19966&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=16430;;;","06/Jul/21 20:42;roman;Couldn't reproduce the issue locally (1.13/14).

From the above logs it looks like it happens mosty in 1.13 and during the nightly builds.

No tasks are running when the program is killed (number of restarts is unlimited).

I've published a PR to log debug info: [https://github.com/apache/flink/pull/16401]

 ;;;","07/Jul/21 03:30;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20036&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=5d6e4255-0ea8-5e2a-f52c-c881b7872361&l=15741;;;","09/Jul/21 03:21;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20197&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51&l=15222;;;","12/Jul/21 09:41;roman;From the logs, I see that test with MySQL and PostreSQL fails in different configurations (UC disabled seems prevalent).
In the beginning, some task fails and watchdog goes off which kills the TM.
With one TM killed, JM is unable to deploy new tasks.
I'm opening several more PRs to improve logging, mostly to see why the task fails in the first place (https://github.com/apache/flink/pull/16456, https://github.com/apache/flink/pull/16457, https://github.com/apache/flink/pull/16458).
;;;","14/Jul/21 08:42;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20393&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20&l=16420;;;","14/Jul/21 11:40;roman;Thanks for posting the failures.

With more logging I see that:
 # all tasks started checkpoint 10 and issued XA_PREPARE in the sync phase of it - and blocked
 # which caused checkpoint timeout (1s) and job restart
 # which caused tasks cancellation by JM
 # tasks cancellation timed out and watch dogs got off after 1s
 # which killed the TMs and then JM wasn't abel to re-deploy the job

The tasks were blocked reading from socket, so they likely already got the connection:
{code:java}
18:13:21,556  - Task 'Source: Custom Source -> Map -> Sink: Unnamed (4/4)#18' did not react to cancelling signal - notifying TM; it is stuck for 1 seconds in method:
    java.net.SocketInputStream.socketRead0(Native Method)
    java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
    java.net.SocketInputStream.read(SocketInputStream.java:171)
    java.net.SocketInputStream.read(SocketInputStream.java:141)
    sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:457)
    sun.security.ssl.SSLSocketInputRecord.bytesInCompletePacket(SSLSocketInputRecord.java:68)
    sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1095)
    sun.security.ssl.SSLSocketImpl.access$200(SSLSocketImpl.java:72)
    sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:815)
    java.io.FilterInputStream.read(FilterInputStream.java:133)
    com.mysql.cj.protocol.FullReadInputStream.readFully(FullReadInputStream.java:64)
    com.mysql.cj.protocol.a.SimplePacketReader.readHeader(SimplePacketReader.java:63)
    com.mysql.cj.protocol.a.SimplePacketReader.readHeader(SimplePacketReader.java:45)
    com.mysql.cj.protocol.a.TimeTrackingPacketReader.readHeader(TimeTrackingPacketReader.java:52)
    com.mysql.cj.protocol.a.TimeTrackingPacketReader.readHeader(TimeTrackingPacketReader.java:41)
    com.mysql.cj.protocol.a.MultiPacketReader.readHeader(MultiPacketReader.java:54)
    com.mysql.cj.protocol.a.MultiPacketReader.readHeader(MultiPacketReader.java:44)
    com.mysql.cj.protocol.a.NativeProtocol.readMessage(NativeProtocol.java:532)
    com.mysql.cj.protocol.a.NativeProtocol.checkErrorMessage(NativeProtocol.java:702)
    com.mysql.cj.protocol.a.NativeProtocol.sendCommand(NativeProtocol.java:641)
    com.mysql.cj.protocol.a.NativeProtocol.sendQueryPacket(NativeProtocol.java:940)
    com.mysql.cj.protocol.a.NativeProtocol.sendQueryString(NativeProtocol.java:886)
    com.mysql.cj.NativeSession.execSQL(NativeSession.java:1073)
    com.mysql.cj.jdbc.StatementImpl.executeInternal(StatementImpl.java:724)
    com.mysql.cj.jdbc.StatementImpl.execute(StatementImpl.java:648)
    com.mysql.cj.jdbc.MysqlXAConnection.dispatchCommand(MysqlXAConnection.java:323)
    com.mysql.cj.jdbc.MysqlXAConnection.prepare(MysqlXAConnection.java:226)
    org.apache.flink.connector.jdbc.xa.XaFacadeImpl.lambda$endAndPrepare$3(XaFacadeImpl.java:176)
    org.apache.flink.connector.jdbc.xa.XaFacadeImpl$$Lambda$1040/95240942.call(Unknown Source)
    org.apache.flink.connector.jdbc.xa.XaFacadeImpl.execute(XaFacadeImpl.java:273)
    org.apache.flink.connector.jdbc.xa.XaFacadeImpl.endAndPrepare(XaFacadeImpl.java:176)
    org.apache.flink.connector.jdbc.xa.XaFacadePoolingImpl.endAndPrepare(XaFacadePoolingImpl.java:97)
    org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction.prepareCurrentTx(JdbcXaSinkFunction.java:290)
    org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction.snapshotState(JdbcXaSinkFunction.java:244)
{code}
This suggest they were either blocked by the previous transactions, deadlocked, or hit some bug in db like [https://bugs.mysql.com/bug.php?id=86819].
 I'll take a further look into it.
 ;;;","15/Jul/21 02:39;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20454&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=60581941-0138-53c0-39fe-86d62be5f407&l=14453;;;","15/Jul/21 02:40;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20454&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51&l=14595;;;","26/Jul/21 07:06;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20927&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=60581941-0138-53c0-39fe-86d62be5f407&l=15893;;;","27/Jul/21 13:40;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20965&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=16080;;;","02/Aug/21 14:28;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21270&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d;;;","02/Aug/21 16:42;roman;Thanks for reporting,

the latest run uses extended logging, but it shows two things in addition to the [above|https://issues.apache.org/jira/browse/FLINK-22889?focusedCommentId=17380514&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17380514]:
- all four transactions for which preparation was started were eventually prepared after 4..6 seconds (the last 3 almost immediately)
- no other transactions, no blocking on locks, no releasing connections was logged during this period

{code}
08:35:19,957 prepare, xid=Optional[201:d7bed71933463eee6d07fe3d24971987030000000000000000000000:978a7129]
08:35:19,958 prepare, xid=Optional[201:d7bed71933463eee6d07fe3d24971987000000000000000000000000:7365e172]
08:35:19,958 prepare, xid=Optional[201:d7bed71933463eee6d07fe3d24971987010000000000000000000000:d1ba0562]
08:35:19,958 prepare, xid=Optional[201:d7bed71933463eee6d07fe3d24971987020000000000000000000000:d8417ce2]

08:35:22,960 watchdog timeout (2s)
08:35:22,960 watchdog timeout (2s)
08:35:22,960 watchdog timeout (2s)
08:35:22,961 watchdog timeout (2s)

08:35:23,939 prepare succeeded, xid=Optional[201:d7bed71933463eee6d07fe3d24971987030000000000000000000000:978a7129]
08:35:25,940 prepare succeeded, xid=Optional[201:d7bed71933463eee6d07fe3d24971987000000000000000000000000:7365e172]
08:35:25,940 prepare succeeded, xid=Optional[201:d7bed71933463eee6d07fe3d24971987010000000000000000000000:d1ba0562]
08:35:25,940 prepare succeeded, xid=Optional[201:d7bed71933463eee6d07fe3d24971987020000000000000000000000:d8417ce2]

{code};;;","03/Aug/21 11:36;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21361&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=16430;;;","04/Aug/21 16:47;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21499&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=16342;;;","05/Aug/21 07:56;trohrmann;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21507&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=16124;;;","06/Aug/21 04:46;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21644&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=b6c4efed-9c7d-55ea-03a9-9bd7d5b08e4c&l=13367;;;","09/Aug/21 03:25;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21728&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=41463ccd-0694-5d4d-220d-8f771e7d098b&l=13900;;;","09/Aug/21 03:40;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21740&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=b6c4efed-9c7d-55ea-03a9-9bd7d5b08e4c&l=13435;;;","16/Aug/21 03:44;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22198&view=logs&j=c91190b6-40ae-57b2-5999-31b869b0a7c1&t=43529380-51b4-5e90-5af4-2dccec0ef402&l=13451;;;","16/Aug/21 05:00;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22223&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=15173;;;","17/Aug/21 04:00;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22252&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=14815;;;","17/Aug/21 05:13;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22329&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=15259;;;","18/Aug/21 04:04;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22415&view=logs&j=e1276d0f-df12-55ec-86b5-c0ad597d83c9&t=66648bdf-9af9-503d-c9a7-11f783a19935&l=14154;;;","18/Aug/21 10:54;roman;Seems like the recent spike in faliures is related to Postgress as opposed to Mysql previously.

Locally, I can see that the pool is of correct size but Postgress still keeps connections open even if no tasks are running.

That might mean that the pool is not closed properly in case of failover.

I'll look further into it.;;;","18/Aug/21 13:06;roman;I think there is a problem in StreamTask/Task causing the recent failures with Postgres; and created FLINK-23862 to track it (the original issue with MySQL seems different).;;;","19/Aug/21 03:44;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22463&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=14819;;;","20/Aug/21 03:25;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22511&view=logs&j=e1276d0f-df12-55ec-86b5-c0ad597d83c9&t=66648bdf-9af9-503d-c9a7-11f783a19935&l=14404;;;","23/Aug/21 02:31;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22577&view=logs&j=e1276d0f-df12-55ec-86b5-c0ad597d83c9&t=66648bdf-9af9-503d-c9a7-11f783a19935&l=14420;;;","23/Aug/21 02:48;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22605&view=logs&j=e1276d0f-df12-55ec-86b5-c0ad597d83c9&t=66648bdf-9af9-503d-c9a7-11f783a19935&l=14154;;;","23/Aug/21 02:51;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22605&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=4cf71635-d33f-53ff-7185-c5abb11ae3a0&l=16098;;;","23/Aug/21 06:21;dmvk;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22622&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d;;;","24/Aug/21 01:58;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22661&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=14575;;;","25/Aug/21 02:46;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22717&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=14933;;;","25/Aug/21 03:25;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22770&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=16260;;;","25/Aug/21 03:26;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22770&view=logs&j=3d12d40f-c62d-5ec4-6acc-0efe94cc3e89&t=4cf71635-d33f-53ff-7185-c5abb11ae3a0&l=15698;;;","26/Aug/21 03:22;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22855&view=logs&j=961f8f81-6b52-53df-09f6-7291a2e4af6a&t=f53023d8-92c3-5d78-ec7e-70c2bf37be20&l=16097;;;","26/Aug/21 03:23;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22855&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354&l=15523;;;","26/Aug/21 11:00;roman;The fix for FLINK-23862 merged (tests against Postgres should succeed).;;;","30/Aug/21 03:58;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23013&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=14941;;;","31/Aug/21 01:42;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23084&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=14943;;;","31/Aug/21 09:04;xtsong;Reached out to [~roman] offline.
We are downgrading this to Critical for the moment. As the Postgres issue being fixed this is expected to fail less frequently (due to MySQL issues). We can upgrade this again if more instances are reported.;;;","01/Sep/21 02:08;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23204&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=14641;;;","06/Sep/21 03:35;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23547&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=15522;;;","07/Sep/21 00:35;roman;In the latest logs it's clearly seen that MySQL ""XA PREPARE"" statements hang for >8s, for example:
{code:java}
 ---TRANSACTION 2139, ACTIVE 8 sec
 1 lock struct(s), heap size 1136, 0 row lock(s), undo log entries 7
 MySQL thread id 43, OS thread handle 140332354893568, query id 715 172.17.0.1 test starting
 XA PREPARE 0xb508d2a2741da30eb384acef037ce93c000000000000000000000000,0x6d33a077,0xc9 {code}
I don't see any obvious issues in the reported InnoDB state (in particular, there are no deadlocks between transactions or IO pressure).

This causes timeout in Flink in snapshotState and task cancellation.

However, after usually 10s the command succeeds (but the cluster is already shut down).

As the issue seems to be not related to Flink but rather MySQL I'm going increase timeouts on the Flink side from 2 to 15s so that test can proceed.;;;","08/Sep/21 15:36;roman;Timeouts increased in master by dea0d8fec1620abf47b93accc451633a5f92e06c,

in 1.14 by e859af62b83755c52275096b6be9fbc6e15e2596.;;;","13/Sep/21 02:10;xtsong;Instance on 1.13:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23943&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=b6c4efed-9c7d-55ea-03a9-9bd7d5b08e4c&l=13447;;;","14/Sep/21 07:59;roman;Merged into 1.13 as cf78dc6e6ad8c14ba4f98a777f931eced3f5018d;;;",,,,,,,,,,,,,,,,,,,,,,
Matches results may be wrong when using notNext as the last part of the pattern with Window,FLINK-22888,13382327,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,nicholasjiang,mayuehappy,mayuehappy,06/Jun/21 08:49,29/Mar/22 00:53,13/Jul/23 08:12,29/Mar/22 00:53,1.9.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Library / CEP,,,,,0,,,,,"the pattern is like 
Pattern.begin(""start"").where(records == ""a"")

            .notNext(""notNext"").where(records == ""b"")

            .withIn(5milliseconds).

If there is only one event *""a""* in 5 milliseconds. I think this *“a”* should be output as the correct result of the match next time in advanceTime.

But in the actual operation of CEP. This “a” will be treated as matching timeout data
{code:java}
// code placeholder
@Test
public void testNoNextWithWindow() throws Exception {
   StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
   env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

   // (Event, timestamp)
   DataStream<Event> input = env.fromElements(
      Tuple2.of(new Event(1, ""start"", 1.0), 5L),

      // last element for high final watermark
      Tuple2.of(new Event(5, ""final"", 5.0), 100L)
   ).assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Tuple2<Event, Long>>() {

      @Override
      public long extractTimestamp(Tuple2<Event, Long> element, long previousTimestamp) {
         return element.f1;
      }

      @Override
      public Watermark checkAndGetNextWatermark(Tuple2<Event, Long> lastElement, long extractedTimestamp) {
         return new Watermark(lastElement.f1 - 5);
      }

   }).map(new MapFunction<Tuple2<Event, Long>, Event>() {

      @Override
      public Event map(Tuple2<Event, Long> value) throws Exception {
         return value.f0;
      }
   });

   Pattern<Event, ?> pattern = Pattern.<Event>begin(""start"").where(new SimpleCondition<Event>() {
      @Override
      public boolean filter(Event value) throws Exception {
         return value.getName().equals(""start"");
      }
   }).notNext(""middle"").where(new SimpleCondition<Event>() {
      @Override
      public boolean filter(Event value) throws Exception {
         return value.getName().equals(""middle"");
      }
   }).within(Time.milliseconds(5L));

   DataStream<String> result = CEP.pattern(input, pattern).select(
      new PatternSelectFunction<Event, String>() {
         @Override
         public String select(Map<String, List<Event>> pattern) {
            StringBuilder builder = new StringBuilder();
            builder.append(pattern.get(""start"").get(0).getId());
            return builder.toString();
         }
      }
   );

   List<String> resultList = new ArrayList<>();

   DataStreamUtils.collect(result).forEachRemaining(resultList::add);

   resultList.sort(String::compareTo);

   assertEquals(Arrays.asList(""1""), resultList);
}
{code}
 ",,dianfu,leonard,libenchao,martijnvisser,mayuehappy,nicholasjiang,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 28 11:48:14 UTC 2022,,,,,,,,,,"0|z0rp1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/22 03:56;nicholasjiang;[~dianfu], could you please assign this ticket to me?;;;","11/Mar/22 03:59;dianfu;[~nicholasjiang]  Done~;;;","28/Mar/22 06:20;nicholasjiang;[~mayuehappy], after the offline discussion, the above description has something wrong that If there is a event ""a"" in 5 milliseconds and there are events which aren't event ""b"", event “a” could be output as the correct result of the match next time in advanceTime. Because the notNext is a node in NFA, which requires the next event to trigger. If there is no other event passed to the notNext node, the previous event doesn't meet the conditions of the Pattern. Therefore, the issue isn't a bug.

[~MartijnVisser], WDYT? If you have no question about this, I would like to close this issue.;;;","28/Mar/22 11:48;martijnvisser;[~nicholasjiang] I think this indeed not a bug, like you've explained it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thread leak in RocksDBStateUploader,FLINK-22886,13382322,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mayuehappy,wind_ljy,wind_ljy,06/Jun/21 05:45,28/Aug/21 12:19,13/Jul/23 08:12,22/Jun/21 18:27,1.11.3,1.12.4,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,Runtime / State Backends,,,,,0,critical,pull-request-available,,,"{{ExecutorService}} in {{RocksDBStateUploader}} is not shut down, which may leak thread when tasks fail.

BTW, we should name the thread group in {{ExecutorService}}, otherwise what we see in the stack, is a lot of threads named with pool-m-thread-n like this:

 

!image-2021-06-06-13-46-34-604.png!",,klion26,libenchao,mayuehappy,roman,stevenz3wu,wind_ljy,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23003,,,,,,,,,,,,"06/Jun/21 05:46;wind_ljy;image-2021-06-06-13-46-34-604.png;https://issues.apache.org/jira/secure/attachment/13026450/image-2021-06-06-13-46-34-604.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 22 18:27:00 UTC 2021,,,,,,,,,,"0|z0rp0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/21 02:58;yunta;Thanks for reporting this, would you like to fix this [~wind_ljy]?;;;","07/Jun/21 03:55;wind_ljy;[~yunta] Hi Yun, [~mayuehappy] is solving this issue in our inner version, you can assign this to [~mayuehappy]. ;;;","07/Jun/21 04:15;yunta;Glad to know ready solution.
[~mayuehappy], please go ahead to create PR.;;;","16/Jun/21 05:47;mayuehappy;[~yunta] Sorry for the late creation of pr. [https://github.com/apache/flink/pull/16171]  ;;;","16/Jun/21 08:49;roman;FYI there is a PR already in a duplicate ticket: [https://github.com/apache/flink/pull/16168]

Maybe we can pull the commit from 16171 with a unit test add it to 16168 and merge the resulting PR with two commits?;;;","16/Jun/21 10:45;mayuehappy;[~roman_khachatryan] thx, I will solve it soon;;;","16/Jun/21 16:44;mayuehappy;[~roman_khachatryan]  Follow your suggestion, i merge the resulting PR with two commits .  [https://github.com/apache/flink/pull/16171]  
would you please take a review?
 ;;;","21/Jun/21 15:06;roman;Merged into master as 3ebb34ec5e67f70f13898570d2f331fc9f32b4f7.

[~mayuehappy] would you like to publish a backport PR for 1.12? 

(alternatively, I can push the commit after CI, there shouldn't be any conflicts or comments);;;","22/Jun/21 03:06;mayuehappy;[~roman_khachatryan] ok ,There is a little difference between 1.12 and the master code, I will  publish it  soon;;;","22/Jun/21 06:40;mayuehappy;[https://github.com/apache/flink/pull/16234]   This commit includes bug fixes in [https://github.com/apache/flink/pull/16168]   and unit tests parts in  [https://github.com/apache/flink/pull/16171]  

[~roman_khachatryan] would you please take a review again?;;;","22/Jun/21 18:27;roman;Reviewed and merged into 1.12 as 20da2a10dd91d7dbc935a11dcf4964de8d7bef2b
and into 1.13 as 4519089d4a45f223ca3ffd2a88864d02a059f8cd.

Thanks a lot for providing the fix and the test!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Select view columns fail when store metadata with hive,FLINK-22884,13382267,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,ELLEX_SHEN,ELLEX_SHEN,05/Jun/21 06:56,14/Jul/21 08:16,13/Jul/23 08:12,06/Jul/21 04:01,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Connectors / Hive,,,,,0,pull-request-available,stale-assigned,,,"I am use hive for filnk metadata, so select view table will mismatch to hive table after create view, I founded is a bug in HiveCatalog.classs, all view table is default mark to hive table unexpected.

after store in hive metadata, view table without ""is_generic"" or ""connector"" properties.

bug is here:


 @VisibleForTesting
    public Table getHiveTable(ObjectPath tablePath) throws TableNotExistException {
        try {
            Table table = this.client.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());
            boolean isHiveTable;
            if (table.getParameters().containsKey(""is_generic"")) {
                isHiveTable = !Boolean.parseBoolean((String)table.getParameters().remove(""is_generic""));
            } else {
                isHiveTable = !table.getParameters().containsKey(""flink."" + FactoryUtil.CONNECTOR.key()) && !table.getParameters().containsKey(""flink.connector.type"");
            }

            if (isHiveTable) {
                table.getParameters().put(FactoryUtil.CONNECTOR.key(), ""hive"");
            }

            return table;
        } catch (NoSuchObjectException var4) {
            throw new TableNotExistException(this.getName(), tablePath);
        } catch (TException var5) {
            throw new CatalogException(String.format(""Failed to get table %s from Hive metastore"", tablePath.getFullName()), var5);
        }
    }",,ELLEX_SHEN,jark,lirui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 06 04:01:53 UTC 2021,,,,,,,,,,"0|z0roog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/21 02:11;jark;cc [~lirui];;;","07/Jun/21 02:53;lirui;Thanks [~ELLEX_SHEN] for reporting the issue. Let me know if you want to fix it.;;;","07/Jun/21 03:04;lirui;I think we don't have to differentiate whether a view is generic or not, as a view can reference both generic and non-generic tables.;;;","07/Jun/21 10:15;ELLEX_SHEN;Thanks for your reply.

For example, create a new Kafka table and a new Kafka view. Once the view is selected, an error will be reported: ""sqlvalidatorexception: column xxx not found in any table"", thinking that the view is mistakenly identified as a hive internal view. Therefore, I submitted the problem.

So I can't use hive as flink metadata storage. How can I solve this problem?;;;","07/Jun/21 10:17;ELLEX_SHEN;[~lirui] From the source code point of view, the current version after v1.13.0 has similar problems, but the version of v1.12.3 is normal, thank you.

;;;","09/Jun/21 04:46;ELLEX_SHEN;[~lirui][~jark] can you for help?;;;","10/Jun/21 03:50;lirui;[~ELLEX_SHEN] Thanks for providing an example to reproduce the issue. I'll have a try and propose a fix.;;;","10/Jun/21 12:30;ELLEX_SHEN;[~lirui] Thanks a lot.;;;","02/Jul/21 22:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 14 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it. If the ""warning_label"" label is not removed in 7 days, the issue will be automatically unassigned.
;;;","06/Jul/21 04:01;lirui;Fixed in master: 0b686824c33ad976cefb588c5a272a09de4b23ca
Fixed in release-1.13: 28b09e575bd174685a15d93345d0f87a37d6f002;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tasks are blocked while broadcasting stream status,FLINK-22881,13382184,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dwysakowicz,pnowojski,pnowojski,04/Jun/21 16:14,23/Sep/21 17:26,13/Jul/23 08:12,22/Jun/21 16:31,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Network,Runtime / Task,,,,0,pull-request-available,,,,"On a cluster I observed symptoms of tasks being blocked for long time, causing long delays with unaligned checkpointing. 99% of those cases were caused by `broadcastEmit` of the stream status

{noformat}
2021-06-04 14:41:44,049 ERROR org.apache.flink.runtime.io.network.buffer.LocalBufferPool   [] - Blocking wait [11059 ms] for an available buffer.
java.lang.Exception: Stracktracegenerator
        at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegmentBlocking(LocalBufferPool.java:323) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestBufferBuilderBlocking(LocalBufferPool.java:290) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewBufferBuilderFromPool(BufferWritingResultPartition.java:338) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewUnicastBufferBuilder(BufferWritingResultPartition.java:314) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.appendUnicastDataForNewRecord(BufferWritingResultPartition.java:246) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.emitRecord(BufferWritingResultPartition.java:142) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:104) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.broadcastEmit(ChannelSelectorRecordWriter.java:67) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.RecordWriterOutput.writeStreamStatus(RecordWriterOutput.java:136) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.streamstatus.AnnouncedStatus.ensureActive(AnnouncedStatus.java:65) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:103) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:90) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:44) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:38) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:101) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:82) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask$AsyncDataOutputToOutput.emitRecord(SourceOperatorStreamTask.java:182) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:110) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:101) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.api.connector.source.lib.util.IteratorSourceReader.pollNext(IteratorSourceReader.java:98) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:294) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:69) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:422) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:204) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:680) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:635) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:646) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:619) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
{noformat}

*I have seen this happening both in source and network tasks.* ~80% cases were in the source tasks

{{broadcastEmit}} can easily bypass our non blocking checks. There are two questions:
# why is the stream idling so much? It’s like almost every millisecond it’s broadcasting status
# should we optimise this? Broadcasting CBs and other events is not an issue, as those are events that do not request/require buffers",,dwysakowicz,liyu,pnowojski,wind_ljy,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22882,,,,,,,,FLINK-22904,,,FLINK-18934,,,,FLINK-22882,FLINK-22904,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 22 16:31:14 UTC 2021,,,,,,,,,,"0|z0ro60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jun/21 16:16;pnowojski;This might be related to the [recent idleness changes|https://github.com/apache/flink/commit/18a2a8ac70cb66389c6b56acedc40887f6c48667], as those changes are in the stack trace.

CC [~dwysakowicz];;;","04/Jun/21 16:33;dwysakowicz;Could you share the code of the source you're using. There is a really high chance that the source is abusing the StreamStatus contract and emits records in an IDLE state, causing going through a toggle cycle of IDLE/ACTIVE.;;;","04/Jun/21 19:36;pnowojski;This is a FLIP-27 source created using:

{code:java}
streamExecutionEnvironment.fromSequence(0, Long.MAX_VALUE);
{code}
;;;","21/Jun/21 16:00;dwysakowicz;Fixed in 2c260b53a37dfcc040c8d5cfbca36d010f9a3cef;;;","22/Jun/21 04:01;xtsong;Reopening the ticket.

Reverted in 8758b21b44d84412113df03255f2bad9d2fb6372

The commit has broken the master branch. HiveTableSinkITCase constantly fails after it is merged.;;;","22/Jun/21 16:31;dwysakowicz;Committed again in 6d3969ed686f04afc9714f7a1ec61a5004f19082 after fixing FLINK-23011;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQLClientSchemaRegistryITCase timeouts on azure,FLINK-22869,13382026,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,xtsong,xtsong,04/Jun/21 01:53,17/Jan/22 15:26,13/Jul/23 08:12,17/Jan/22 15:26,1.12.4,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.8,1.13.6,,,,Table SQL / Ecosystem,Tests,,,,0,auto-deprioritized-critical,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18652&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=16ca2cca-2f63-5cce-12d2-d519b930a729&l=27324

{code}
Jun 03 23:51:30 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 227.425 s <<< FAILURE! - in org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase
Jun 03 23:51:30 [ERROR] testReading(org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase)  Time elapsed: 194.931 s  <<< ERROR!
Jun 03 23:51:30 org.junit.runners.model.TestTimedOutException: test timed out after 120000 milliseconds
Jun 03 23:51:30 	at java.lang.Object.wait(Native Method)
Jun 03 23:51:30 	at java.lang.Thread.join(Thread.java:1252)
Jun 03 23:51:30 	at java.lang.Thread.join(Thread.java:1326)
Jun 03 23:51:30 	at org.apache.kafka.clients.admin.KafkaAdminClient.close(KafkaAdminClient.java:541)
Jun 03 23:51:30 	at org.apache.kafka.clients.admin.Admin.close(Admin.java:96)
Jun 03 23:51:30 	at org.apache.kafka.clients.admin.Admin.close(Admin.java:79)
Jun 03 23:51:30 	at org.apache.flink.tests.util.kafka.KafkaContainerClient.createTopic(KafkaContainerClient.java:71)
Jun 03 23:51:30 	at org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase.testReading(SQLClientSchemaRegistryITCase.java:102)
Jun 03 23:51:30 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jun 03 23:51:30 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jun 03 23:51:30 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jun 03 23:51:30 	at java.lang.reflect.Method.invoke(Method.java:498)
Jun 03 23:51:30 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Jun 03 23:51:30 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jun 03 23:51:30 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Jun 03 23:51:30 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jun 03 23:51:30 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
Jun 03 23:51:30 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
Jun 03 23:51:30 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Jun 03 23:51:30 	at java.lang.Thread.run(Thread.java:748)

{code}
",,gaoyunhaii,martijnvisser,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22971,FLINK-24765,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 17 15:26:33 UTC 2022,,,,,,,,,,"0|z0rn6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/21 01:52;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18671&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=27252;;;","15/Jun/21 02:33;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18957&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=3425d8ba-5f03-540a-c64b-51b8481bf7d6&l=26225;;;","15/Jun/21 02:49;xtsong;This is likely caused by FLINK-22971.;;;","20/Jul/21 01:35;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20708&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=1fdd9d50-31f7-5383-5578-49e27385b5f1&l=16471;;;","12/Aug/21 03:12;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21934&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=a68b8d89-50e9-5977-4500-f4fde4f57f9b&l=27986;;;","20/Aug/21 02:06;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22494&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=27432;;;","03/Sep/21 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","06/Sep/21 03:14;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23546&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=16ca2cca-2f63-5cce-12d2-d519b930a729&l=27814;;;","06/Sep/21 03:39;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23559&view=logs&j=4dd4dbdd-1802-5eb7-a518-6acd9d24d0fc&t=8d6b4dd3-4ca1-5611-1743-57a7d76b395a&l=16945
;;;","10/Sep/21 02:19;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23877&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=7f606211-1454-543c-70ab-c7a028a1ce8c&l=27463;;;","17/Sep/21 22:37;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","11/Oct/21 02:35;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24924&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=27618;;;","12/Oct/21 08:42;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24967&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=3425d8ba-5f03-540a-c64b-51b8481bf7d6&l=27652;;;","07/Nov/21 15:44;gaoyunhaii;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26084&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=27168];;;","08/Nov/21 07:45;trohrmann;cc [~twalthr] this test seems to fail quite regularly.;;;","08/Nov/21 13:28;martijnvisser;[~trohrmann] I briefly looked into the test but I suspect that this is a Kafka related error, not a SQL Client error. ;;;","08/Nov/21 15:25;martijnvisser;We should first get https://issues.apache.org/jira/browse/FLINK-24765 done to see if that already improves the situation and monitor the current situation. ;;;","17/Jan/22 15:26;martijnvisser;No issues reported since beginning of November, closing this ticket;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayIndexOutOfBoundsException may happen when building rescale edges,FLINK-22863,13381851,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Thesharing,Thesharing,Thesharing,03/Jun/21 07:29,23/Sep/21 17:22,13/Jul/23 08:12,04/Jun/21 09:30,1.13.0,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.2,1.14.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"For EdgeManagerBuildUtil introduced in FLINK-21326, we find that during the construction of rescale edges, it may throw ArrayIndexOutOfBoundsException like this:

!image-2021-06-03-15-06-09-301.png|width=938,height=200!

It is mainly caused by the precision of {{double}} in Java.

In EdgeManagerBuildUtil#connectPointwise, when upstream parallelism < downstream parallelism, we calculate the indices of downstream vertices that connect to each upstream partition like this:
{code:java}
int start = (int) (Math.ceil(partitionNum * factor)); 
int end = (int) (Math.ceil((partitionNum + 1) * factor));
{code}
The index range is [{{start}}, {{end}}). 

In some cases the value of {{end}} may exceed the downstream parallelism and throw the ArrayIndexOutOfBoundsException.

Let's take an example. The upstream parallelism is 7. The downstream parallelism is 29. For the last upstream partition (which {{partitionNum}} is 6), {{(partitionNum + 1) * factor}} is 29.00002, which is slightly larger than 29. This is caused by the precision of {{double}}. Then {{end}} = {{Math.ceil(29.00002)}}, which is 30. ArrayIndexOutOfBoundsException is thrown here.

To solve this issue, we need to add an extra check for the boundary condition like this:
{code:java}
int end = Math.min(targetCount, (int) (Math.ceil((partitionNum + 1) * factor)));
{code}
This affects release-1.13.0 and release-1.13.1.",,klion26,Thesharing,trohrmann,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/21 07:06;Thesharing;image-2021-06-03-15-06-09-301.png;https://issues.apache.org/jira/secure/attachment/13026328/image-2021-06-03-15-06-09-301.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 04 09:30:19 UTC 2021,,,,,,,,,,"0|z0rm40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/21 07:31;Thesharing;cc [~zhuzh], [~trohrmann];;;","03/Jun/21 08:01;zhuzh;Thanks for reporting this issue. [~Thesharing]
 It is indeed a problem that we should fix ASAP.
I have assign it to you and let's have a fix PR for it soon.;;;","03/Jun/21 10:58;trohrmann;Yes, let's fix it asap. Thanks for reporting it [~Thesharing].;;;","04/Jun/21 09:30;zhuzh;Fixed via
master: 739a12add50c90e020e4b9aaafc1cc45465fa937
release-1.13: 6f774c9db8c33c86ce81cccd68beb7a2e096c177;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TIMESTAMPADD + timestamp_ltz type throws CodeGenException,FLINK-22861,13381823,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,03/Jun/21 03:07,23/Sep/21 18:08,13/Jul/23 08:12,26/Jul/21 02:19,1.13.2,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.13.3,1.14.0,,,,Table SQL / Planner,Table SQL / Runtime,,,,0,auto-unassigned,pull-request-available,,,"Add the following test case to {{org.apache.flink.table.planner.runtime.batch.sql.CalcITCase}} to reproduce this issue.

{code:scala}
@Test
def myTest(): Unit = {
  checkResult(""SELECT TIMESTAMPADD(MINUTE, 10, CURRENT_TIMESTAMP)"", Seq())
}
{code}

The exception stack is
{code}
org.apache.flink.table.planner.codegen.CodeGenException: Incompatible types of expression and result type. Expression[GeneratedExpression(result$5,isNull$4,


isNull$4 = false || false;
result$5 = null;
if (!isNull$4) {
  
  result$5 = org.apache.flink.table.data.TimestampData.fromEpochMillis(((org.apache.flink.table.data.TimestampData) queryStartTimestamp).getMillisecond() + ((long) 600000L), ((org.apache.flink.table.data.TimestampData) queryStartTimestamp).getNanoOfMillisecond());
  
}
,TIMESTAMP_LTZ(3) NOT NULL,None)] type is [TIMESTAMP_LTZ(3) NOT NULL], result type is [TIMESTAMP(6) NOT NULL]

	at org.apache.flink.table.planner.codegen.ExprCodeGenerator$$anonfun$generateResultExpression$1.apply(ExprCodeGenerator.scala:311)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator$$anonfun$generateResultExpression$1.apply(ExprCodeGenerator.scala:299)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateResultExpression(ExprCodeGenerator.scala:299)
	at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateResultExpression(ExprCodeGenerator.scala:255)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.produceProjectionCode$1(CalcCodeGenerator.scala:142)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateProcessCode(CalcCodeGenerator.scala:167)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateCalcOperator(CalcCodeGenerator.scala:50)
	at org.apache.flink.table.planner.codegen.CalcCodeGenerator.generateCalcOperator(CalcCodeGenerator.scala)
	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:247)
	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:58)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
	at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$translateToPlan$1.apply(BatchPlanner.scala:80)
	at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$translateToPlan$1.apply(BatchPlanner.scala:79)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:79)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:165)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1657)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:797)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1258)
	at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:577)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:300)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:140)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:106)
	at org.apache.flink.table.planner.runtime.batch.sql.CalcITCase.myTest2(CalcITCase.scala:81)
{code}

This is because {{org.apache.calcite.sql.fun.SqlTimestampAddFunction#deduceType}} always returns a timestamp type for some time unit. So it seems that we should write our own timestamp add function to get the correct return type.",,godfreyhe,leonard,libenchao,lzljs3620320,Terry1897,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CALCITE-4698,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 26 02:19:26 UTC 2021,,,,,,,,,,"0|z0rlxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/21 06:53;leonard;I'll take this one;;;","17/Jun/21 22:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 14, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it. If the ""warning_label"" label is not removed in 7 days, the issue will be automatically unassigned.
;;;","25/Jun/21 22:37;flink-jira-bot;This issue was marked ""stale-assigned"" 7 days ago and has not received an update. I have automatically removed the current assignee from the issue so others in the community may pick it up. If you are still working on this ticket, please ask a committer to reassign you and provide an update about your current status.
;;;","26/Jul/21 02:19;lzljs3620320;master: fce9c1d8b3242ea2ec9605394d87eda0824b4bd0
release-1.13: 09f868827166da872cb8d683c3c959231e96ae40;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move our Azure pipelines away from Ubuntu 16.04 by September,FLINK-22856,13381697,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rmetzger,rmetzger,rmetzger,02/Jun/21 12:43,04/May/23 11:39,13/Jul/23 08:12,08/Jun/21 15:03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,Build System / Azure Pipelines,,,,,0,pull-request-available,,,,"Azure won't support Ubuntu 16.04 starting from October, hence we need to migrate to a newer ubuntu version.

We should do this at a time when the builds are relatively stable to be able to clearly identify issues relating to the version upgrade. Also, we shouldn't do this before a feature freeze ;) ",,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31999,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 08 09:52:20 UTC 2021,,,,,,,,,,"0|z0rl5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/21 17:58;rmetzger;Looks like the e2e tests are failing with
{code}
Jun 02 14:05:32 Preparing Dockeriles
Cloning into 'flink-docker'...
Traceback (most recent call last):
  File ""/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/python2_fileserver.py"", line 19, in <module>
    import SimpleHTTPServer
ModuleNotFoundError: No module named 'SimpleHTTPServer'
{code}
Otherwise, CI looks fine: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=9108&view=results;;;","03/Jun/21 06:38;chesnay;Probably {{python}} now defaults to python3, so let's just drop the python 2 codepath.;;;","08/Jun/21 09:52;rmetzger;Merged to master in https://github.com/apache/flink/commit/bab81cb60538046a59ddf9a841eef713465198fa.

1.12: https://github.com/apache/flink/commit/9831b46be02bc3819acd02f893cb88d3de8cb46d
1.13: https://github.com/apache/flink/commit/ed02b5deca5881ac5572228ddd1cd3e85eadfd32;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Source tasks (both old and new) are not reporting checkpoint start delay via CheckpointMetrics,FLINK-22833,13381463,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,pnowojski,pnowojski,01/Jun/21 14:26,23/Sep/21 17:23,13/Jul/23 08:12,09/Jun/21 13:56,1.12.4,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,Runtime / Metrics,,,,,0,pull-request-available,,,,"checkpointStartDelay is still not reported in the WebUI. Previous bugs FLINK-22814 and FLINK-18656 have fixed this issue only for the Task's metric, not for checkpoint metrics.",,pnowojski,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22814,FLINK-18656,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 09 13:56:23 UTC 2021,,,,,,,,,,"0|z0rjq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/21 13:56;pnowojski;Merged to master as e7696fcdb63^^^..e7696fcdb63
Merged to release-1.13 as  41a21454da1..a83f067722e
Merged to release-1.12 as 277965bbad6..e794574b1e6 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stopping Yarn session cluster will cause fatal error,FLINK-22820,13381396,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fpaul,wangyang0918,wangyang0918,01/Jun/21 07:43,17/Nov/21 15:21,13/Jul/23 08:12,03/Jun/21 07:30,1.12.4,1.13.1,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,Deployment / Kubernetes,Deployment / Mesos,Deployment / YARN,Runtime / Coordination,,0,pull-request-available,,,,"Stopping the Yarn session cluster via {{echo ""stop"" | ./bin/yarn-session.sh -id application_xxxx}} will have the following fatal error. A full jobmanager log is also attached.


{code:java}
2021-06-01 15:37:38,005 ERROR org.apache.flink.runtime.util.FatalExitExceptionHandler      [] - FATAL: Thread 'flink-akka.actor.default-dispatcher-4' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: The job State machine job(2fab9b478eb86deade69c613fe0ab58b) is not in a globally terminal state. Instead it is in state SUSPENDED.
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_102]
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_102]
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:824) ~[?:1.8.0_102]
	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ~[?:1.8.0_102]
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442) ~[?:1.8.0_102]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.13.1.jar:1.13.1]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.13.1.jar:1.13.1]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.13.1.jar:1.13.1]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.13.1.jar:1.13.1]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.13.1.jar:1.13.1]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.13.1.jar:1.13.1]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.13.1.jar:1.13.1]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.13.1.jar:1.13.1]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.13.1.jar:1.13.1]
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.13.1.jar:1.13.1]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.13.1.jar:1.13.1]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.13.1.jar:1.13.1]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.13.1.jar:1.13.1]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.13.1.jar:1.13.1]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.13.1.jar:1.13.1]
Caused by: java.lang.IllegalArgumentException: The job State machine job(2fab9b478eb86deade69c613fe0ab58b) is not in a globally terminal state. Instead it is in state SUSPENDED.
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.runtime.dispatcher.FileExecutionGraphInfoStore.put(FileExecutionGraphInfoStore.java:168) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.runtime.dispatcher.Dispatcher.archiveExecutionGraph(Dispatcher.java:845) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.runtime.dispatcher.Dispatcher.jobReachedTerminalState(Dispatcher.java:836) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.runtime.dispatcher.Dispatcher.handleJobManagerRunnerResult(Dispatcher.java:443) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$runJob$3(Dispatcher.java:415) ~[flink-dist_2.11-1.13.1.jar:1.13.1]
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ~[?:1.8.0_102]
	... 24 more
{code}
",,fpaul,klion26,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22434,,,,,,,,,,,,,,,,,,,,"01/Jun/21 07:43;wangyang0918;log.jm;https://issues.apache.org/jira/secure/attachment/13026228/log.jm",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 03 07:30:13 UTC 2021,,,,,,,,,,"0|z0rjbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/21 07:46;wangyang0918;It might be caused by FLINK-22434.

cc [~fpaul] [~trohrmann];;;","01/Jun/21 07:48;fpaul;[~fly_in_gis] thanks for the hint. You are right, it is caused by the linked change I am working on a fix currently. ;;;","03/Jun/21 07:30;chesnay;master: 1db4e560d1b46fac27a18bce9556fec646f063d9

1.13: f82ffc337f7c028adade17e55f9fda57e7b48863

1.12: 31f40993c6c31c6ec26976235feed2c0a43d4cab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"YARNFileReplicationITCase fails with ""The YARN application unexpectedly switched to state FAILED during deployment""",FLINK-22819,13381389,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,dwysakowicz,dwysakowicz,01/Jun/21 07:17,23/Sep/21 18:01,13/Jul/23 08:12,09/Jul/21 14:04,1.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.12.5,1.13.2,1.14.0,,,Deployment / YARN,,,,,0,pull-request-available,stale-major,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18467&view=logs&j=8fd975ef-f478-511d-4997-6f15fe8a1fd3&t=ac0fa443-5d45-5a6b-3597-0310ecc1d2ab&l=32007

{code}
May 31 23:14:22 org.apache.flink.client.deployment.ClusterDeploymentException: Could not deploy Yarn job cluster.
May 31 23:14:22 	at org.apache.flink.yarn.YarnClusterDescriptor.deployJobCluster(YarnClusterDescriptor.java:481)
May 31 23:14:22 	at org.apache.flink.yarn.YARNFileReplicationITCase.deployPerJob(YARNFileReplicationITCase.java:106)
May 31 23:14:22 	at org.apache.flink.yarn.YARNFileReplicationITCase.lambda$testPerJobModeWithDefaultFileReplication$1(YARNFileReplicationITCase.java:78)
May 31 23:14:22 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:287)
May 31 23:14:22 	at org.apache.flink.yarn.YARNFileReplicationITCase.testPerJobModeWithDefaultFileReplication(YARNFileReplicationITCase.java:78)
May 31 23:14:22 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
May 31 23:14:22 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
May 31 23:14:22 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
May 31 23:14:22 	at java.lang.reflect.Method.invoke(Method.java:498)
May 31 23:14:22 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
May 31 23:14:22 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
May 31 23:14:22 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
May 31 23:14:22 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
May 31 23:14:22 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
May 31 23:14:22 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
May 31 23:14:22 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
May 31 23:14:22 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
May 31 23:14:22 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
May 31 23:14:22 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
May 31 23:14:22 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
May 31 23:14:22 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
May 31 23:14:22 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
May 31 23:14:22 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
May 31 23:14:22 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
May 31 23:14:22 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
May 31 23:14:22 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
May 31 23:14:22 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
May 31 23:14:22 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
May 31 23:14:22 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
May 31 23:14:22 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
May 31 23:14:22 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
May 31 23:14:22 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
May 31 23:14:22 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
May 31 23:14:22 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
May 31 23:14:22 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
May 31 23:14:22 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
May 31 23:14:22 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
May 31 23:14:22 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
May 31 23:14:22 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
May 31 23:14:22 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
May 31 23:14:22 Caused by: org.apache.flink.yarn.YarnClusterDescriptor$YarnDeploymentException: The YARN application unexpectedly switched to state FAILED during deployment. 
May 31 23:14:22 Diagnostics from YARN: Application application_1622502732791_0001 failed 1 times (global limit =2; local limit is =1) due to ApplicationMaster for attempt appattempt_1622502732791_0001_000001 timed out. Failing the application.
May 31 23:14:22 If log aggregation is enabled on your cluster, use this command to further investigate the issue:
May 31 23:14:22 yarn logs -applicationId application_1622502732791_0001
May 31 23:14:22 	at org.apache.flink.yarn.YarnClusterDescriptor.startAppMaster(YarnClusterDescriptor.java:1201)
May 31 23:14:22 	at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:593)
May 31 23:14:22 	at org.apache.flink.yarn.YarnClusterDescriptor.deployJobCluster(YarnClusterDescriptor.java:474)
May 31 23:14:22 	... 39 more

{code}",,abacuixe,dmvk,dwysakowicz,epojemoyamunu,iihiedidapimy,iqowesa,mapohl,opeyoecru,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/21 13:49;mapohl;FLINK-22819-YARNFileReplicationITCase-testPerJobModeWithDefaultFileReplication.log;https://issues.apache.org/jira/secure/attachment/13026301/FLINK-22819-YARNFileReplicationITCase-testPerJobModeWithDefaultFileReplication.log",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 09 14:04:55 UTC 2021,,,,,,,,,,"0|z0rj9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/21 08:19;mapohl;I couldn't get anything specific during my initial investigation (I attached the test's logs). We don't get any additional YARN logs due to the failure happening during application deployment. There is a timeout during deployment as stated in the error messages.
{code}
23:13:00,816 [ContainersLauncher #0] INFO  org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor [] - launchContainer: [bash, /__w/3/s/flink-yarn-tests/target/flink-yarn-tests-per-job/flink-yarn-tests-per-job-localDir-nm-0_0/usercache/agent0
23:13:12,994 [        Ping Checker] INFO  org.apache.hadoop.yarn.util.AbstractLivelinessMonitor        [] - Expired:appattempt_1622502732791_0001_000001 Timed out after 20 secs
23:13:12,996 [AsyncDispatcher event handler] INFO  org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl [] - Updating application attempt appattempt_1622502732791_0001_000001 with final state: FAILED, and exit status: -1000
{code}

Comparing it to a successful test of the same build it appears that there is some time consumed (5 secs here) for authentication:
{code}
23:15:52,955 [ContainersLauncher #0] INFO  org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor [] - launchContainer: [bash, /__w/3/s/flink-yarn-tests/target/flink-yarn-tests-capacityscheduler/flink-yarn-tests-capacityscheduler-localDir-nm-
1_0/usercache/agent03_azpcontainer/appcache/application_1622502943279_0001/container_1622502943279_0001_01_000001/default_container_executor.sh]
23:15:57,954 [Socket Reader #1 for port 44617] INFO  SecurityLogger.org.apache.hadoop.ipc.Server                  [] - Auth successful for appattempt_1622502943279_0001_000001 (auth:SIMPLE)
23:15:57,977 [IPC Server handler 0 on 44617] INFO  org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService [] - AM registration appattempt_1622502943279_0001_000001
{code};;;","03/Jul/21 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","07/Jul/21 12:33;dmvk;The issues is caused by `yarn.am.liveness-monitor.expiry-interval-ms`, that was lowered for yarn test cases. We can think of two reasons, why this options is lowered.

A) In order to stress, that we're actually sending heartbeats from ApplicationMaster to YARN Resource Manager.
B) To make tests fail faster, if user-code that we're executing inside ApplicationMaster gets stuck for some reason.

`expiry-interval-ms` for AM works as follows (simplified):
1) We start a YARN mini cluster + HDFS (where we upload flink jars)
2) We create a new YARN application and allocated a container for ApplicationMaster. This is the *actual startTime for AM expiration check*.
3) User jars / entrypoints / configs get downloaded from HDFS into ApplicationMaster.
4) Entrypoint (shell script that starts `YarnJobClusterEntrypoint`) gets executed.
5) Inside java entrypoint, we construct a YARN Resource Manager Client (`AMRMClientAsync<?>`), which we use for operating YARN cluster. This client also *starts sending heartbeats* (it's a side-effect, no explicit action needs to be taken for sending heartbeats).

We can see there is a lot of work to be done between 2) and 5) and in resource limited environment, such as CI, 20s may not be enough for registering RM Client with Resource Manager.

Anyway, I don't think there is a need for this timeout anymore, because if we wouldn't start RM Client, tests would fail anyway and by this timeout, we unnecessarily stress YARN's internal implementation.

As for potentially long running tests, this should be already solved by timeouts in AZURE pipelines.

As a fix, I propose to remove this override and use 5m default instead. IMO this option is actually meant just as a safeguard, when AM gets into an ""unhandled"" state - network failures / unable to shutdown.;;;","09/Jul/21 14:04;trohrmann;Fixed via

1.14.0: 9b4fc49a44920b6d94b8e69a98f3f725e6b63c22
1.13.2: 3708cc4cf9382f18e2f8e48b91d7c7dac74d607b
1.12.5: e280421bc1009e2462551e76f81c3ec980cf3f77;;;","23/Sep/21 05:54;abacuixe;[url=http://slkjfdf.net/]Iluvey[/url] <a href=""http://slkjfdf.net/"">Ojoqepi</a> tte.lyva.issues.apache.org.pmh.xm http://slkjfdf.net/;;;jira-users","23/Sep/21 10:57;iqowesa;[url=http://slkjfdf.net/]Uyebde[/url] <a href=""http://slkjfdf.net/"">Afekuqoye</a> dec.anzn.issues.apache.org.sfc.qa http://slkjfdf.net/;;;jira-users","23/Sep/21 13:33;opeyoecru;[url=http://slkjfdf.net/]Eyixace[/url] <a href=""http://slkjfdf.net/"">Ugymebo</a> jpg.gxrx.issues.apache.org.tfv.bo http://slkjfdf.net/;;;jira-users","23/Sep/21 13:47;epojemoyamunu;[url=http://slkjfdf.net/]Iqunipa[/url] <a href=""http://slkjfdf.net/"">Amedebef</a> fvh.lngd.issues.apache.org.ezw.yy http://slkjfdf.net/;;;jira-users","23/Sep/21 14:00;iihiedidapimy;[url=http://slkjfdf.net/]Okffajvo[/url] <a href=""http://slkjfdf.net/"">Edmaxofo</a> hvb.ioql.issues.apache.org.vgz.pi http://slkjfdf.net/;;;jira-users",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IgnoreInFlightDataITCase fails on azure,FLINK-22818,13381388,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,akalashnikov,dwysakowicz,dwysakowicz,01/Jun/21 07:09,28/Aug/21 12:17,13/Jul/23 08:12,25/Jun/21 09:20,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14.0,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18465&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=81f2da51-a161-54c7-5b84-6001fed26530&l=9807

{code}
May 31 22:28:49 [ERROR] Failures: 
May 31 22:28:49 [ERROR]   IgnoreInFlightDataITCase.testIgnoreInFlightDataDuringRecovery:101 
May 31 22:28:49 Expected: a value less than <57464560L>
May 31 22:28:49      but: <57464560L> was equal to <57464560L>
{code}",,dwysakowicz,pnowojski,rmetzger,roman,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 25 09:20:45 UTC 2021,,,,,,,,,,"0|z0rj9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/21 07:10;dwysakowicz;cc [~akalashnikov];;;","03/Jun/21 11:13;rmetzger;https://dev.azure.com/rmetzger/Flink/_build/results?buildId=9109&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=634cd701-c189-5dff-24cb-606ed884db87;;;","04/Jun/21 01:47;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18651&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107&l=9259;;;","06/Jun/21 18:04;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18690&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4424;;;","06/Jun/21 18:08;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18690&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=81f2da51-a161-54c7-5b84-6001fed26530&l=9803;;;","07/Jun/21 06:10;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18712&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=9508;;;","15/Jun/21 02:56;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18957&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=d13f554f-d4b9-50f8-30ee-d49c6fb0b3cc&l=9449;;;","15/Jun/21 07:41;roman;Merged into master as 0cfc6451219cb42a77ff57f0aff0215b85d0f4eb.;;;","16/Jun/21 08:05;roman;2nd PR to reduce the number of buffers sent from source to map and prevent backpressure eventually causing deadlock merged into master as d8a556c4f229d942490bbfa50403efba4b6fe530.;;;","23/Jun/21 19:35;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19401&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=4462;;;","24/Jun/21 02:51;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19415&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=4462;;;","24/Jun/21 02:59;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19415&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4409;;;","24/Jun/21 12:13;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19460&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=4407;;;","24/Jun/21 14:55;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19472&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=4407;;;","25/Jun/21 02:08;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19500&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56&l=4302;;;","25/Jun/21 02:08;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19500&view=logs&j=119bbba7-f5e3-5e08-e72d-09f1529665de&t=7dc1f5a9-54e1-502e-8b02-c7df69073cfc&l=4129;;;","25/Jun/21 02:09;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19500&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=6dff16b1-bf54-58f3-23c6-76282f49a185&l=4217;;;","25/Jun/21 02:09;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19500&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=2c7d57b9-7341-5a87-c9af-2cf7cc1a37dc&l=4073;;;","25/Jun/21 06:24;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19504&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=9503;;;","25/Jun/21 09:20;dwysakowicz;Fixed in 81881992206333090ebe021114150e6554b8375f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
