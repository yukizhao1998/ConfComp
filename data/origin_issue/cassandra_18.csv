Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocked),Outward issue link (Blocker),Inward issue link (Child-Issue),Inward issue link (Completes),Inward issue link (Container),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Outward issue link (Incorporates),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Outward issue link (Regression),Inward issue link (Supercedes),Outward issue link (Supercedes),Outward issue link (Testing),Inward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Authors),Custom field (Authors),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Impacts),Custom field (Impacts),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Platform),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Review Date),Custom field (Reviewer),Custom field (Reviewer),Custom field (Reviewers),Custom field (Reviewers),Custom field (Reviewers),Custom field (Severity),Custom field (Severity),Custom field (Since Version),Custom field (Since Version),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
fix some operational holes in incremental repair,CASSANDRA-14939,13204999,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,17/Dec/18 21:53,09/Oct/20 20:04,13/Jul/23 08:36,25/Aug/20 22:30,4.0,,,,,,,,,,0,,,,"Incremental repair has a few operational rough spots that make it more difficult to fully automate and operate at scale than it should be.

* Visibility into whether pending repair data exists for a given token range.
* Ability to force promotion/demotion of data for completed sessions instead of waiting for compaction.
* Get the most recent repairedAt timestamp for a given token range.",,bdeggleston,cscotta,jasonstack,jeromatron,jwest,kohlisankalp,marcuse,mbyrd,stefan.miklosovic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,Normal,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 25 22:30:17 UTC 2020,,,,,,,,,,,"0|u002f4:",9223372036854775807,,,,,,,,,marcuse,,marcuse,,,Normal,,4.0-alpha1,,,https://github.com/apache/cassandra/commit/c34317526fc6dbe559beb36cf44e24278656bdf2,,,,,,,,,,,,,,"18/Dec/18 21:26;bdeggleston;/cc [~mbyrd], [~spodxx@gmail.com];;;","29/Jan/19 20:51;bdeggleston;|[trunk|https://github.com/bdeggleston/cassandra/tree/14939-trunk]|[dtests|https://github.com/bdeggleston/cassandra-dtest/tree/14939]|[circle|https://circleci.com/workflow-run/587857d3-8d7a-4710-906f-010f499d4910]|

This adds {{\-\-cleanup}}, {{\-\-summarize-pending}}, and {{\-\-summarize-repaired}} flags to {{nodetool repair_admin}}. Each of which also accept keyspace/table and range args. To support reporting the min/max repairedAt time for a given token range across restarts, I had to alter the logic for purging old sessions. Finalized sessions will only be deleted if there is a newer session covering it's ranges.;;;","31/Jan/19 09:52;spod;Can you please also add {{parentRepairSession}} to [this|https://github.com/apache/cassandra/blob/a05785d82c621c9cd04d8a064c38fd2012ef981c/src/java/org/apache/cassandra/repair/RepairSession.java#L268] log statement, so we can get a link between parent and child sessionIds in the log files?;;;","21/Feb/19 14:27;marcuse;in general, this looks very nice and will be a huge help

General stuff
 * We keep the LocalSessions around for 1 day in system.repairs - I guess it is possible to give incomplete information for summarizeRepaired after bounce for example? (I have no real solution other than keeping them around for a longer time)

ColumnFamilyStore;
 * test(s) for getPendingRepairStats
 * in getPendingRepairStats there is a potential NPE - the pending repair status can be removed between the isPendingRepair check and getting the pendingRepair UUID from the sstable
 * when we use {{runWithCompactionsDisabled}} we could potentially pass in the ranges we need to cancel compactions for (totally ok leaving this for a future ticket though)
 * potential NPE - {{sst.isPendingRepair}} call first, then {{sst.getPendingRepair}} in the {{runWithCompactionsDisabled}} call

CompactionStrategyManager;
 * in {{releaseRepairedData}} - we should probably make sure all the callables are cancelled if we catch that exception there, otherwise we might keep the sstables marked as compacting forever

PendingRepairManager;
 * I think strategies can be removed even though we have the read lock - {{Set<SSTableReader> sstables = get(sessionID).getSSTables();}} could NPE in that case

Range;
 * new {{Range.intersects(..)}} method should probably have a test
 * same with new {{subtract}} method - both get tested indirectly, but might be good with a few direct ones

LocalSessions;
 * in {{getPendingStats}} sessionIDs set is not used
 * in {{getPendingStats}} when checking {{if (!Iterables.any(ranges, r -> r.intersects(session.ranges)))}} session could be null - there is a null check right below which we could move up.

RepairedState;
 * This class could use a more comments

PendingStat;
 * In {{addSSTable}}, instead of checking {{Preconditions.checkArgument(sessionID != null);}} we should probably just skip the sstable as it means it has been moved out of pending

PendingStats;
 * seems to be a mismatch in the columns in {{to/fromComposite}}

SchemaArgsParser;
 * Untested

I changed {{RepairAdmin}} nodetool command to use subcommands to reduce some of the manual parameter verification;
 [https://github.com/krummas/cassandra/commits/blake/14939-trunk-nodetool];;;","11/Feb/20 09:12;stefan.miklosovic;Is there any chance this will appear in upcomming 4.x release? Is anybody working on this atm?;;;","12/May/20 21:36;kohlisankalp;I think we should do this as part of 4.0-beta as it is important to users who will use IR in 4.0. IR has major changes in 4.0 and we hope lot more users will use this in 4.0!!

 

cc [~jwest];;;","12/May/20 22:25;jwest;[~kohlisankalp] I don't personally have enough background to comment on this ticket. With fresh eyes I saw it as a potential candidate for a follow up release since it hasn't been worked on for over one year and looks to be a set of improvements on an existing feature. I defer to those involved with the ticket and/or the community if its better to start a conversation on the mailing list. ;;;","05/Aug/20 23:43;bdeggleston;[~marcuse] rebased on current trunk and addressed all review comments;;;","20/Aug/20 08:04;marcuse;+1

bq. in getPendingStats when checking if (!Iterables.any(ranges, r -> r.intersects(session.ranges))) session could be null - there is a null check right below which we could move up.
Probably better to just skip it if the session is null ({{LocalSessions#getPendingStats}}, line 278), feel free to fix on commit if you agree.;;;","25/Aug/20 22:30;bdeggleston;Committed to trunk as [c34317526fc6dbe559beb36cf44e24278656bdf2 |https://github.com/apache/cassandra/commit/c34317526fc6dbe559beb36cf44e24278656bdf2], thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Anticompaction should throw exceptions on errors, not just log them",CASSANDRA-14936,13204474,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,14/Dec/18 13:06,15/May/20 08:06,13/Jul/23 08:37,19/Dec/18 17:52,4.0,4.0-alpha1,,,,,Local/Compaction,,,,0,,,,"Anticompaction currently catches any exceptions and just logs them instead of rethrowing, this can cause us to overstream and leave sstables unrepaired.

This was made more likely to happen with CASSANDRA-14397 (before that anticompactions could not be stopped at all).",,bdeggleston,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,Correctness -> Recoverable Corruption / Loss,,,,,,,,Challenging,Adhoc Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Dec 19 17:52:33 UTC 2018,,,,,,,,,,,"0|s01iyw:",9223372036854775807,,,,,,,,,bdeggleston,,bdeggleston,,,Normal,,,,,,,,,,,,,,,,,,,"14/Dec/18 13:51;marcuse;patch: https://github.com/krummas/cassandra/commits/marcuse/14936
tests: https://circleci.com/workflow-run/bf69001c-756b-4c43-a754-652066dca07f

Note that this also makes anticompactions unstoppable again to make sure that starting a conflicting AC fails by timing out when trying to cancel compactions. CASSANDRA-14935 will improve this behaviour by failing immediately if any conflicting AC is active and make them stoppable again.;;;","14/Dec/18 14:03;marcuse;cancelling patch - we need to abort any non-finished transactions as well;;;","14/Dec/18 15:02;marcuse;nope, missed that the transaction is closed [here|https://github.com/apache/cassandra/blob/a41b861fa4d4acfbcce13dd62b1e8f48be22f8ed/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L751]

pushed a byteman test which makes sure we don't have any compacting sstables after a failed AC;;;","17/Dec/18 23:25;bdeggleston;This addresses this ticket fine, but it seems like there's some stuff related to CASSANDRA-14935 in here as well. Could you put that in a separate branch?;;;","18/Dec/18 07:28;marcuse;bq. there's some stuff related to CASSANDRA-14935 in here as well
Do you mean making anticompaction unstoppable? I added that to avoid cancelling ongoing anticompactions until we have CASSANDRA-14935 finished, it might be unnecessary since we should fix 14935 soon anyway, so I could remove those parts;;;","18/Dec/18 15:50;bdeggleston;Ah ok I guess that part is complete. +1 then;;;","19/Dec/18 14:55;marcuse;while fixing this up for commit I realised that we would be running the PendingAntiCompactionTests twice with the Byteman tests I added

tiny refactor: https://github.com/krummas/cassandra/commit/87c87a6a5554cfc82bc4fd10ea1d6dc5e67a20b2
https://circleci.com/workflow-run/7e383aaf-e96a-42d2-bbf9-866ced074df7

[~bdeggleston] could you just sanity check that before I commit?;;;","19/Dec/18 16:14;bdeggleston;looks good to me;;;","19/Dec/18 17:52;marcuse;and committed as {{0a79f9f5c970dcb8265814cd5dc361eb2d4bec6b}}, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MigrationManager attempts to pull schema from different major version nodes,CASSANDRA-14928,13203519,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,aweisberg,aweisberg,aweisberg,10/Dec/18 19:49,01/Aug/21 12:51,13/Jul/23 08:37,13/Dec/18 18:33,2.2.14,3.0.18,3.11.7,4.0,4.0-alpha1,,Legacy/Distributed Metadata,,,,0,pull-request-available,,,"MigrationManager will do the version check against nodes it hasn't connected to yet so it doesn't know their messaging service version. We should also check the version in gossip as an additional layer of protection.

This causes many of the upgrade tests to fail.",,aleksey,aweisberg,djoshi,githubbot,jeromatron,tommy_s,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aweisberg,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 13 18:33:55 UTC 2018,,,,,,,,,,,"0|s01d3s:",9223372036854775807,,,,,,,,,,,djoshi,,,Critical,,,,,,,,,,,,,,,,,,,"10/Dec/18 21:57;aweisberg;||Branch|CircleCI|
|[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...aweisberg:14928-2.2?expand=1]|[CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/14928-2%2E2]|
|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...aweisberg:14928-3.0?expand=1]|[CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/14928-3%2E0]|
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...aweisberg:14928-3.11?expand=1]|[CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/14928-3%2E11]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...aweisberg:14928-trunk?expand=1]|[CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/14928-trunk]|

;;;","11/Dec/18 22:46;djoshi;[~aweisberg] the changes look good. I wonder if it would be better to add a helper {{FBUtilities::getReleaseVersionMajor()}}. Also note that the {{FBUtilities::getReleaseVersionString()}} may return a string ""Unknown"" in which case you'll experience an exception. Ideally, we'd like to assert that the version is not ""unknown"".;;;","11/Dec/18 23:54;githubbot;Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra-dtest/pull/41#discussion_r240837053
  
    --- Diff: upgrade_tests/upgrade_manifest.py ---
    @@ -70,22 +89,19 @@ def clone_with_local_env_version(self):
             return self
     
     
    -indev_2_0_x = None  # None if release not likely
    -current_2_0_x = VersionMeta(name='current_2_0_x', family='2.0.x', variant='current', version='2.0.17', min_proto_v=1, max_proto_v=2, java_versions=(7,))
    -
     indev_2_1_x = VersionMeta(name='indev_2_1_x', family='2.1.x', variant='indev', version='github:apache/cassandra-2.1', min_proto_v=1, max_proto_v=3, java_versions=(7, 8))
    -current_2_1_x = VersionMeta(name='current_2_1_x', family='2.1.x', variant='current', version='2.1.17', min_proto_v=1, max_proto_v=3, java_versions=(7, 8))
    +current_2_1_x = VersionMeta(name='current_2_1_x', family='2.1.x', variant='current', version='2.1.20', min_proto_v=1, max_proto_v=3, java_versions=(7, 8))
     
     indev_2_2_x = VersionMeta(name='indev_2_2_x', family='2.2.x', variant='indev', version='github:apache/cassandra-2.2', min_proto_v=1, max_proto_v=4, java_versions=(7, 8))
    -current_2_2_x = VersionMeta(name='current_2_2_x', family='2.2.x', variant='current', version='2.2.9', min_proto_v=1, max_proto_v=4, java_versions=(7, 8))
    +current_2_2_x = VersionMeta(name='current_2_2_x', family='2.2.x', variant='current', version='2.2.13', min_proto_v=1, max_proto_v=4, java_versions=(7, 8))
     
    -indev_3_0_x = VersionMeta(name='indev_3_0_x', family='3.0.x', variant='indev', version='github:apache/cassandra-3.0', min_proto_v=3, max_proto_v=4, java_versions=(8,))
    -current_3_0_x = VersionMeta(name='current_3_0_x', family='3.0.x', variant='current', version='3.0.12', min_proto_v=3, max_proto_v=4, java_versions=(8,))
    +indev_3_0_x = VersionMeta(name='indev_3_0_x', family='3.0.x', variant='indev', version='github:aweisberg/cassandra-3.0', min_proto_v=3, max_proto_v=4, java_versions=(8,))
    --- End diff --
    
    Like I said in the JIRA I will update the branches on commit once https://issues.apache.org/jira/browse/CASSANDRA-14928 is merged.
;;;","12/Dec/18 21:54;djoshi;LGTM +1;;;","13/Dec/18 18:33;aweisberg;Committed as [505a03c77764351e1b649e8c7d73d0421e7bcc13|https://github.com/apache/cassandra/commit/505a03c77764351e1b649e8c7d73d0421e7bcc13]. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DecimalSerializer.toString() can be used as OOM attack ,CASSANDRA-14925,13203364,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jasonstack,jasonstack,jasonstack,10/Dec/18 07:05,30/Dec/20 11:31,13/Jul/23 08:37,19/Dec/20 18:41,3.0.24,3.11.10,4.0,4.0-rc1,,,Legacy/Core,,,,1,,,,"Currently, in {{DecimalSerializer.toString(value)}}, it uses {{BigDecimal.toPlainString()}} which generates huge string for large scale values.

 
{code:java}
BigDecimal d = new BigDecimal(""1e-"" + (Integer.MAX_VALUE - 6));
d.toPlainString(); // oom{code}
 

Propose to use {{BigDecimal.toString()}} when scale is larger than 100 which is configurable via {{-Dcassandra.decimal.maxscaleforstring}}

 
| patch | circle-ci |
|[trunk|https://github.com/jasonstack/cassandra/commits/decimal-tostring-trunk]|[unit|https://circleci.com/gh/jasonstack/cassandra/751?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link]|

The code should apply cleanly to 3.0+.
",,blambov,blerer,e.dimitrova,jasonstack,jeromatron,jlewandowski,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasonstack,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Dec 19 18:42:21 UTC 2020,,,,,,,,,,,"0|s01c60:",9223372036854775807,,,,,,,,,,,blerer,,,Critical,,3.0.0,,,https://github.com/apache/cassandra/commit/ea52f1d2fa032dd28bbe69a1914497e48a6eb99a,,,,,,,,,,,,,,"10/Dec/18 10:12;slebresne;Can't we just use {{BigDecimal.toString()}} all the time as save ourselves the trouble of adding yet one more runtime parameter that no user will probably ever modify?;;;","10/Dec/18 12:32;jasonstack;{quote}Can't we just use BigDecimal.toString() all the time as save ourselves the trouble of adding yet one more runtime parameter that no user will probably ever modify?{quote}

Make sense.. updated the patch to use `toString()` only.
|patch|circle-ci|
|[3.0|https://github.com/jasonstack/cassandra/commits/decimal-tostring-3.0]|[unit|https://circleci.com/gh/jasonstack/cassandra/747?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link]|
|[3.11|https://github.com/jasonstack/cassandra/commits/decimal-tostring-3.11]|[unit|https://circleci.com/gh/jasonstack/cassandra/752?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link]|
|[trunk|https://github.com/jasonstack/cassandra/commits/decimal-tostring-trunk]|[unit|https://circleci.com/gh/jasonstack/cassandra/751?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link]|

 

 ;;;","11/Dec/18 09:41;slebresne;The patch looks obviously technically ok, but there is of course the question of backward compatibility. I believe the most of usage of {{AbstractType#getString}} is for log messages, but there is a few other usages. {{sstabledump}} is one, though I'm not too worried about this here. There is a also a bunch of case where it's used for internal stuffs but ""I think"" this should be case where {{decimal}} is not used. But I have far from make a careful analysis of all the places where it is used, so I think we're fine but I'm not 100% sure.

Overall, not sure what to do about that previous comment. I do think we should fix this and I don't think the risk of someone running into backward compatibility troubles is very high here, but I wonder if we shouldn't stick to trunk as a compromise. Would welcome other opinions here for sure. Maybe worth a quick email on the mailing list to gather opinions? 

;;;","17/Dec/18 04:53;jasonstack;Updated the patch to Trunk only.;;;","06/Nov/20 09:12;jlewandowski;When is it going to be merged?;;;","16/Dec/20 10:33;blambov;To me this is a serious bug for any version of Cassandra, and should be applied to all supported versions ASAP. This is a real security risk for users and can be very easily exploited as a denial of service attack. I am raising the severity accordingly – this has been ignored too long already.

The risk of problems after the patch is not real: any of the considered formats is a valid full precision representation of the number and will be accepted correctly by transformations back to {{BigDecimal}} from {{String}}.;;;","16/Dec/20 14:51;jasonstack;I have rebased..

|patch|circle-ci|
|[3.0|https://github.com/jasonstack/cassandra/commits/decimal-tostring-3.0]|[unit|https://app.circleci.com/pipelines/github/jasonstack/cassandra/333/workflows/b3d324ef-d356-4831-a8fb-d80c5eb4a49d]|
|[3.11|https://github.com/jasonstack/cassandra/commits/decimal-tostring-3.11]|[unit|https://app.circleci.com/pipelines/github/jasonstack/cassandra/332/workflows/74399094-9b27-44d7-b5bd-d34cd46fa944]|
|[trunk|https://github.com/jasonstack/cassandra/commits/decimal-tostring-trunk]|[unit|https://app.circleci.com/pipelines/github/jasonstack/cassandra/331/workflows/ffa08b52-3716-4fce-9603-4be9ad0563e8]|
;;;","16/Dec/20 17:53;blerer;The patches looks good to me. Feel free to commit if the tests are green. ;;;","19/Dec/20 18:42;jasonstack;committed, thanks for the review;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
In JVM dtests need to clean up after instance shutdown,CASSANDRA-14922,13202340,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jolynch,jolynch,jolynch,04/Dec/18 18:29,27/Aug/20 15:14,13/Jul/23 08:37,09/Jan/19 14:07,4.0,4.0-alpha1,,,,,Test/dtest/java,,,,0,,,,"Currently the unit tests are failing on circleci ([example one|https://circleci.com/gh/jolynch/cassandra/300#tests/containers/1], [example two|https://circleci.com/gh/rustyrazorblade/cassandra/44#tests/containers/1]) because we use a small container (medium) for unit tests by default and the in JVM dtests are leaking a few hundred megabytes of memory per test right now. This is not a big deal because the dtest runs with the larger containers continue to function fine as well as local testing as the number of in JVM dtests is not yet high enough to cause a problem with more than 2GB of available heap. However we should fix the memory leak so that going forwards we can add more in JVM dtests without worry.

I've been working with [~ifesdjeen] to debug, and the issue appears to be unreleased Table/Keyspace metrics (screenshot showing the leak attached). I believe that we have a few potential issues that are leading to the leaks:

1. The [{{Instance::shutdown}}|https://github.com/apache/cassandra/blob/f22fec927de7ac291266660c2f34de5b8cc1c695/test/distributed/org/apache/cassandra/distributed/Instance.java#L328-L354] method is not successfully cleaning up all the metrics created by the {{CassandraMetricsRegistry}}
 2. The [{{TestCluster::close}}|https://github.com/apache/cassandra/blob/f22fec927de7ac291266660c2f34de5b8cc1c695/test/distributed/org/apache/cassandra/distributed/TestCluster.java#L283] method is not waiting for all the instances to finish shutting down and cleaning up before continuing on
3. I'm not sure if this is an issue assuming we clear all metrics, but [{{TableMetrics::release}}|https://github.com/apache/cassandra/blob/4ae229f5cd270c2b43475b3f752a7b228de260ea/src/java/org/apache/cassandra/metrics/TableMetrics.java#L951] does not release all the metric references (which could leak them)

I am working on a patch which shuts down everything and assures that we do not leak memory.",,benedict,ifesdjeen,jay.zhuang,jeromatron,jolynch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14946,,,,,,CASSANDRA-14974,,CASSANDRA-14969,,,,,,,,,,"08/Dec/18 02:35;jolynch;AllThreadsStopped.png;https://issues.apache.org/jira/secure/attachment/12951080/AllThreadsStopped.png","08/Dec/18 02:35;jolynch;ClassLoadersRetaining.png;https://issues.apache.org/jira/secure/attachment/12951079/ClassLoadersRetaining.png","23/Jan/19 22:37;jolynch;LeakedNativeMemory.png;https://issues.apache.org/jira/secure/attachment/12956048/LeakedNativeMemory.png","04/Dec/18 18:28;jolynch;Leaking_Metrics_On_Shutdown.png;https://issues.apache.org/jira/secure/attachment/12950585/Leaking_Metrics_On_Shutdown.png","12/Dec/18 00:11;jolynch;MainClassRetaining.png;https://issues.apache.org/jira/secure/attachment/12951436/MainClassRetaining.png","08/Jan/19 21:42;jolynch;MemoryReclaimedFix.png;https://issues.apache.org/jira/secure/attachment/12954229/MemoryReclaimedFix.png","13/Dec/18 03:46;jolynch;Metaspace_Actually_Collected.png;https://issues.apache.org/jira/secure/attachment/12951598/Metaspace_Actually_Collected.png","08/Dec/18 02:35;jolynch;OnlyThreeRootsLeft.png;https://issues.apache.org/jira/secure/attachment/12951078/OnlyThreeRootsLeft.png","30/Jan/19 14:49;ifesdjeen;Screen Shot 2019-01-30 at 15.46.35.png;https://issues.apache.org/jira/secure/attachment/12956909/Screen+Shot+2019-01-30+at+15.46.35.png","30/Jan/19 14:48;ifesdjeen;Screen Shot 2019-01-30 at 15.47.13.png;https://issues.apache.org/jira/secure/attachment/12956908/Screen+Shot+2019-01-30+at+15.47.13.png","12/Dec/18 06:41;jolynch;no_more_references.png;https://issues.apache.org/jira/secure/attachment/12951468/no_more_references.png",,,11.0,jolynch,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 04 13:31:43 UTC 2019,,,,,,,,,,,"0|s015w8:",9223372036854775807,4.0,,,,,,,,ifesdjeen,,ifesdjeen,,,Low,,,,,,,,,,,,,,,,,,,"04/Dec/18 19:41;ifesdjeen;[~jolynch] thank you for reporting it and investing time to fix it! Great investigation, looking forward to see the full patch!

{{close}} is not waiting for full instance shutdown (same as instance shutdown does not wait for executor shutdown) wasn't an oversight. It makes shutdown significantly faster and allows shutting down a cluster much quicker. 

I do agree we should fix leaks and make sure things shutdown properly and we definitely have to do what we can to make it happen. But I also think we should keep it performant. If there's no way and clusters churn quicker than we can shutdown them, we can make a switch. 

Another thing we might want to consider is reusing cluster instance for multiple tests as I think this will most frequently be the case. We can make it in a follow-up ticket, since it is, however helpful, also orthogonal. ;;;","04/Dec/18 23:23;jolynch;Definitely have to keep the shutdown fast, I think we just have a deadlock or something like that in the cleanups where we're not actually cleaning up. I tried adding in {{Keyspace::clear}} calls in the shutdown hook but that deadlocked on the single STPE in {{nonPeriodicTasks}}, still trying to figure out why we can't drop all the CFs and release all the metrics but making progress :);;;","08/Dec/18 02:35;jolynch;Alright, I think I'm narrowing in on this. I've managed to get all the threads to die over in [a branch|https://github.com/jolynch/cassandra/tree/CASSANDRA-14922] but we're still leaking all the static state through the {{InstanceClassLoader}} s. I think I've narrowed it down to just three remaining references (and I _think_ only one of them is a strong reference), details attached in the screenshots.

We basically just need to kill that last strong reference and I believe that the whole {{InstanceClassLoader}} should become collectible at that point (even with all the static state and self references to the classloaders should be ok since it'll be cut off at the root, I think).;;;","12/Dec/18 00:11;jolynch;Nailed the MessageSink leak and properly shuts down logback now, and we clean up ThreadLocal variables which afaict means that there are no additional threads that could hold references ... and at least now [my branch|https://github.com/jolynch/cassandra/tree/CASSANDRA-14922] can pass unit tests in [circleci|https://circleci.com/workflow-run/6fb24842-bbb8-4aac-b137-4007729bf39a] so that's good.

Unfortunately I think we're still leaking, according to my heap dump analysis the _main_ thread ends up with a strong reference to the {{InstanceClassLoaders}}, which is really odd since the InstanceClassLoader doesn't have any static state so I don't see how those don't go out of scope when the test methods finish.

[~ifesdjeen] tbh at this point I'm pretty stuck. My understanding is that the {{InstanceClassLoaders}} should go out of scope after each test method, which should allow everything to GC now that there are no more live references from threads to the {{InstanceClassLoaders}}, but that's not happening. I think it might be related to passing in the current threads classloader [here|https://github.com/jolynch/cassandra/blob/1ca6ad3c41f4456b674e13883e0df0091f638564/test/distributed/org/apache/cassandra/distributed/TestCluster.java#L241] but I'm not sure how to achieve what we need there without that.;;;","12/Dec/18 06:40;jolynch;I think I figured it out, it turns out that our version of {{jna.Native}} holds a static map called [{{options}}|https://github.com/java-native-access/jna/blob/4bcc6191c5467361b5c1f12fb5797354cc3aa897/src/com/sun/jna/Native.java#L104] which holds noncollectable references to the [{{InstanceClassLoader}}|https://github.com/java-native-access/jna/blob/4bcc6191c5467361b5c1f12fb5797354cc3aa897/src/com/sun/jna/Native.java#L1528]; I believe this leak is what the last two references were talking about. I tried cleanly unloading or unregistering those but that still doesn't remove the reference to the ClassLoader in the {{options}} map so I just hacked some reflection based solution together (similar to what I did for the {{ThreadLocal}} variables) in  [6573176|https://github.com/jolynch/cassandra/commit/6573176ef3ed9601ce7f02602c964d478c6a5741]. According to Yourkit there are now no more strong references to the {{InstanceClassLoader}} instances (attached).

 !no_more_references.png! 

We leak a lot less memory with my latest patches, but ... for some reason the class loaders aren't going away and are still retaining some heap, just a lot less ... There is still something missing, maybe something like {{CMSClassUnloadingEnabled}} or some such?;;;","12/Dec/18 13:34;benedict;[~jolynch]: nice work, somehow I hadn't spotted this thread until now.

Unfortunately I don't have anything useful to add, except a shot-in-the-dark that maybe a full GC is needed?  This is apparently the case for G1GC; not sure about other GCs, or which you are using.  It's probably not helpful, but you could also look at {{-XX:+TraceClassUnloading}}

Mostly just wanted to say thanks for taking the time to track all of this down.;;;","13/Dec/18 03:42;jolynch;I found a better workaround (in [bfd8c328|https://github.com/apache/cassandra/commit/bfd8c328f92e6bd2c82269a048f6b868179f484a]) for the jna {{Native}} Classloader leak is to just call the [register|https://github.com/java-native-access/jna/blob/365f6b343a92427e7890cae0c16df7b6c4c254d4/src/com/sun/jna/Native.java#L1446] API that doesn't add the {{InstanceClassLoader}} to a global static map in {{Native}}. This still loads the libraries and doesn't leak the ClassLoader into a global static map. If we're concerned about the changes to the {{NativeLibrary}} I can pursue other option. I also found another thread that leaks only during ant test runs which is fixed in [512d15b5f|https://github.com/apache/cassandra/commit/512d15b5f08bda1b0dc8a952ce950b5c4341f992].

Furthermore I believe I know why the JVM was still not releasing the classloaders even after they are no longer referenced: [SoftReferences|https://docs.oracle.com/javase/8/docs/api/java/lang/ref/SoftReference.html] are apparently just ... not collected. It looks like Metaspace just grows without bound until we run the machine OOM (not the JVM, the machine).

I tried the following after eliminating all strong references:
 1. No JVM tuning. This _did not_ work as the metaspace would grow without bound and eventually OOM the machine (not the JVM, the machine).
 2. {{-XX:SoftRefLRUPolicyMSPerMB=0}} and or {{-XX:MaxMetaspaceSize=256M}}. This *worked* and caused the ClassLoaders to be released. With MaxMetaspaceSize we also successfully limit the offheap Metaspace which is nice.
 3. {{-XX:MaxMetaspaceSize=256M}} and or {{-XX:MetaspaceSize=100M}}. This _did not_ work. Setting the Max size would cause an {{java.lang.OutOfMemoryError: Metaspace}}, setting just the size didn't do anything (machine would just OOM itself anyways)
 4. {{-XX:UseConcMarkSweepGC}} and or {{-XX:CMSClassUnloadingEnabled}}. This _did not_ work. Again the metaspace just grew without bound and ran out of memory.

I think that running all unit tests under #2 is a pretty reasonable thing to do (we shouldn't be leaking Metaspace imo). I've pursued that option in [86f982b1|https://github.com/apache/cassandra/commit/86f982b19ba23fc5efcd7421449af4a84d93711b] and it appears to work looking at my [profiler|https://issues.apache.org/jira/secure/attachment/12951598/Metaspace_Actually_Collected.png].

[~ifesdjeen]/[~benedict] Do you guys think it's acceptable to run all unit tests with the off-heap Metaspace limited to 256 megabytes and {{SoftRefLRUPolicyMSPerMB}} set to 0 to tell the GC to actually collect soft references? It's different than the status quo but I think we probably shouldn't leak metaspace (aka permgen from java7/6) and we probably shouldn't rely on soft references for things to work generally... I checked and both options are available in openjdk8+, but I'm not sure if there is a better way.

Other than the ThreadLocal reflection hacks I think the branch is almost ready to squash and merge, what do you think?;;;","13/Dec/18 12:35;benedict;Do you know where the soft references originate?  I wonder if there's anything we can do to simply eliminate them.

Short of that, I'm comfortable setting the SoftRefLRUPolicyMSPerMB - the default seems pretty strange (of 1s per MB), and probably does not seem to play well with Metaspace, since this probably doesn't contribute to the input to the SoftRefLRUPolicyMSPerMB.  So we could have Gigabytes of free space, and judge that we need the referent to have been unused for hours before we collect it.

It's been a while since I sperlunked in the JDK source, but it might be worth taking a look to find out exactly what number if provides, but honestly I don't really see a problem with setting the value to zero.  The worst impact of soft references being freed too-eagrerly is performance, it should never be functional.  We don't depend on them directly in C*, so it would only be via the JDK.  Worst case scenario, we need to investigate again at some later date an issue with the tests.

Nice catch on the Native API also.

Overall this looks like excellent work (Though I'll leave a full review to Alex, since he's marked himself reviewer);;;","08/Jan/19 12:36;ifesdjeen;The patch looks good, and I'd say [~jolynch] let's merge it, since tests have been failing for a while now, unless there's something else you wanted to include in the patch immediately. 

I've had a couple of minor suggestions. All of the issues are easier to see / reproducible with a very small heap, ~256Mb: 
  * Hints are leaking direct memory 
  * Threadlocals are leaked 
  * FastThreadLocalThread thread locals are leaked (sorry for a tongue-twister)

I've put together a small [demo|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-14922] just for demonstration purposes if you wanted to see the impact of suggested changes.;;;","08/Jan/19 21:42;jolynch;{quote}The patch looks good, and I'd say [~jolynch] let's merge it,
{quote}
Ok, yea I agree let's merge what we have so that the unit tests can pass on trunk again. I've put up a patch against trunk with what we have so far (including your changes from the demo branch which as far as I can tell remove the need for the ThreadLocal clearing).

||trunk||
|[024e6943|https://github.com/apache/cassandra/commit/024e69436e89bb79cdbf4e136a1f6d9c2747275d]|
|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14922.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14922]|

If I attach a profiler during an intellij ""run this test until it fails"" mode I can see that the memory is indeed getting cleaned up:

!MemoryReclaimedFix.png!;;;","08/Jan/19 21:49;benedict;Marking 'Ready to Commit' given [~ifesdjeen]'s comments.  I'll give it another quick once over then commit, so I can rebase CASSANDRA-14931 and CASSANDRA-14937.;;;","08/Jan/19 22:54;jolynch;[~benedict],

Awesome, can we wait for Alex to see the latest diff though with the reflection removed in favor of his proposed fast local thread pool cleanup method? I've changed the patch a bit since he last looked.

Regarding the backport, I am slightly concerned about the NativeLibrary changes being backported in their current form. From my reading of the JNA source code in version 4.2.2 in trunk we're just skipping the cache by using [NativeLibrary::getInstance|https://github.com/java-native-access/jna/blob/4bcc6191c5467361b5c1f12fb5797354cc3aa897/src/com/sun/jna/NativeLibrary.java#L341] directly and passing it to [Native::register(NativeLibrary)|https://github.com/java-native-access/jna/blob/4bcc6191c5467361b5c1f12fb5797354cc3aa897/src/com/sun/jna/Native.java#L1260] instead of having [Native::register(String)|https://github.com/java-native-access/jna/blob/4bcc6191c5467361b5c1f12fb5797354cc3aa897/src/com/sun/jna/Native.java#L1251] do that for us and cache the classloader along the way [here|https://github.com/java-native-access/jna/blob/4bcc6191c5467361b5c1f12fb5797354cc3aa897/src/com/sun/jna/NativeLibrary.java#L363]. But, if I'm wrong it's unlikely we'd know, as while our tests cover Linux pretty thoroughly, darwin/windows are less covered.

Also I forgot to respond to your question about SoftReferences here, did it on IRC but not here.
{quote}Do you know where the soft references originate? I wonder if there's anything we can do to simply eliminate them.
{quote}
I think the Soft references are coming from {{java.io.ObjectStreamClass$Caches.localDescs}}, but the object serder we're doing in {{InvokableInstance}} is a bit beyond my JVM skills I'm afraid. I don't know how we can prevent the object serializations from caching the class descriptions... Perhaps the JVM option is sufficient for now and if we don't like that going forward we can dive in more?;;;","09/Jan/19 01:11;benedict;bq.  can we wait for Alex to see the latest diff though... I've changed the patch a bit since he last looked.

Sure thing.  I'll start the rebase tomorrow in that case.  In that case, also, I've pushed my one nit from a quick look through [here|https://github.com/belliottsmith/cassandra/tree/14922] for Alex to look at, that I would have simply ninja'd in (with comment here, of course).  This is just using the {{HintsBuffer.free}} method instead of directly invoking {{DirectByteBuffer.cleaner().clean()}}.

bq. Regarding the backport, I am slightly concerned about the NativeLibrary changes being backported in their current form.

Thanks for highlighting this.  I'll be sure to take a close look at the behaviour on each version we backport to.  I expect there will be other places that need similar treatment to what you've done here, as well, so I need to double check anyway.

bq. I think the Soft references are coming from java.io.ObjectStreamClass$Caches.localDescs, but the object serder we're doing in InvokableInstance is a bit beyond my JVM skills I'm afraid.

No worries at all, thanks very much for reproducing this information here for posterity.  If we ever want to clean this up, it would probably be easiest to simply avoid ser/deser entirely (or use custom ser/deser), but your approach is a much more suitable compromise for now.  Thanks again also for all the investigative work to plug these gaps.;;;","09/Jan/19 01:30;jolynch;{quote}
Sure thing. I'll start the rebase tomorrow in that case. In that case, also, I've pushed my one nit from a quick look through here for Alex to look at, that I would have simply ninja'd in (with comment here, of course). This is just using the HintsBuffer.free method instead of directly invoking DirectByteBuffer.cleaner().clean().
{quote}
Ah cool, yea that appears to still work (and then we can leave the slab private in {{HintsBuffer}} as well.);;;","09/Jan/19 14:06;ifesdjeen;Committed to trunk with [d5005627b02b4e716947fa05a40473368017c0f9|https://github.com/apache/cassandra/commit/d5005627b02b4e716947fa05a40473368017c0f9].

[CI run|https://circleci.com/workflow-run/eed85c4b-3a55-46bb-bad8-93c4c1e820f9]

[~jolynch] thank you for the investigation & patch. 
[~benedict] thank you for input as well!;;;","09/Jan/19 19:44;benedict;In back porting this to 3.0, I wondered if it was worth opening a brief bit of discussion around alternative approaches to the {{ThreadLocal}} issue, for which the Netty approach does not anyway work in 3.0 (since we use regular {{ThreadLocal}}).

The fix introduced here assumes that we only access any cluster behaviour from the {{main}} thread.  An alternative approach would be to isolate all tests to a thread owned by the {{TestCluster}} (which we already have available to us, and we shutdown on {{close}}).

If this were the standard pattern for implementing any tests, we should not have an issue with the {{main}} thread retaining any references inside its {{ThreadLocal}}, but we also will create a pattern that extends to any tests written to utilise multi-threading, and hence not run against the {{main}} thread.

I've implemented this in my backport branch, and it is quite straight forward, however I am still chasing down other 3.0-era leaks (right now around our custom log4j integrations);;;","09/Jan/19 19:46;benedict;On a related topic, it might be nice to eventually migrate to assigning a {{ThreadGroup}} to all our threads, so we can reliably manage them en masse.;;;","11/Jan/19 16:21;benedict;I've pushed a branch [here|https://github.com/belliottsmith/cassandra/tree/14922-followup] that removes the cleanup of thread locals, and removes the cluster-wide executor, instead introducing a node-specific executor on which all invocations to that node happen.  This might introduce some slight penalty for evaluating tests, as we have multiple synchronisation points where control-flow is handed between threads, but it guarantees that all state remains isolated without ever burdening the user of the API to restrict their own program design.

We can elaborate on this later to make it ergonomic to use the executor, or perhaps to skip the executor, in order to provide efficiency where it matters.

I think it makes sense to classify this as part of the work for this ticket, and I will then backport it alongside the original patch for this ticket and the jvm-dtests, however it's primarily necessary for _future_ reasons:
 * 3.0 and earlier use {{ThreadLocal}}, not {{FastThreadLocal}}, so we need the earlier hackier reflection to fix on these
 * The current approach only cleans up the main thread - any other threads may themselves retain state indefinitely
 * Most importantly, when we begin stopping and starting nodes, and/or upgrading them, the cluster-wide executor service will retain thread local state from the stopped nodes, potentially indefinitely, probably leading to a steady leak of memory.

It seems easiest to introduce the future-proof approach now and port it to all the versions at once.

On a related note, there is CASSANDRA-14974, which simultaneously fixes a bug in our stdout capture, _and_ the impact this has to the cleanup of a {{TestCluster}} that occurs once this bug is fixed.  If somebody watching this ticket fancies reviewing that ticket, it would also be appreciated.;;;","14/Jan/19 10:11;ifesdjeen;[~benedict] thank you for the patch. It does look like an improvement. 

I think that using node-local executor will work better for scenarios different than ""fire up cluster, shut down the cluster"". 

Just to confirm my understanding is correct: we can remove thread-local works in this case since we're passing the right class loader, making references unreachable. Is that right? I did test it and tests do confirm that passing ""root"" class loader would cause heap problems again, but maybe you have some more insight on that.

+1 to commit it in the scope of this ticket as a follow-up. However, there are some test failures in distributed tests, seemingly caused by different exception wrapping in the new version, we should fix them before committing.;;;","14/Jan/19 13:15;benedict;Thanks for the review.  I'll push an update shortly, with the test failures handled, and a slight tweak to not misleadingly return a SerializableX when it is wrapped with an executor (since this cannot be serialized).

bq. we can remove thread-local works in this case since we're passing the right class loader, making references unreachable. Is that right?

By 'passing' do you mean to the thread factory?  In which case, no, that shouldn't have an impact (I pass it only because it seems to make sense, it should work still without doing so, and seems to if I try).  All we're really doing is ensuring that any thread that evaluates anything inside one of the classes loaded by the instance's classloader is shutdown when the node is shutdown, by passing the work to a thread on this executor.;;;","14/Jan/19 16:07;benedict;I've pushed an update with the suggested changes, and also reintroduced some sequencing to the executor shutdowns.;;;","15/Jan/19 13:40;ifesdjeen;+1 for the latest version, huge improvement!;;;","15/Jan/19 19:13;benedict;Thanks, committed follow-up as [00fff3ee6e6c0142529de621bcaeee5790a0c235|https://github.com/apache/cassandra/commit/00fff3ee6e6c0142529de621bcaeee5790a0c235];;;","23/Jan/19 22:38;jolynch;I'm getting trunk failures again as of e871903d, and after an [IRC discussion|https://wilderness.apache.org/channels/?f=cassandra-dev/2019-01-23] with Benedict it looks like we may be leaking:
 1. Off heap memory via some combination of the {{HintsBuffer}}, {{CommitLogs}} and the {{BufferPool}}
 2. File descriptors are potentially leaked and it's unclear that we clean those up

What is odd is that according to a profiler attached while running one of the dtests in a for loop, most of the leaked native memory is either pending finalization or unreachable from GC roots:

!LeakedNativeMemory.png!

Afaict both {{HintsBuffer}} and {{CommitLogs}} should be getting cleaned in the {{Instance::shutdown}} methods, although I don't think we clean the {{BufferPool}}.

Continuing to investigate this so that we can have green runs on trunk again.

*Edit:*

The test I'm running is applying this diff:
{noformat}
--- a/test/distributed/org/apache/cassandra/distributed/DistributedReadWritePathTest.java
+++ b/test/distributed/org/apache/cassandra/distributed/DistributedReadWritePathTest.java
@@ -27,6 +27,15 @@ import static org.apache.cassandra.net.MessagingService.Verb.READ_REPAIR;
 
 public class DistributedReadWritePathTest extends DistributedTestBase
 {
+    @Test
+    public void manyCoordinatedReads() throws Throwable
+    {
+        for (int i = 0; i < 20; i ++)
+        {
+            coordinatorRead();
+        }
+    }
+
     @Test
     public void coordinatorRead() throws Throwable
     {
{noformat}
Then I run the test suite and measure OS memory usage, for example if I re-wind trunk to the first patch (3dcde082) I see only 1.6GB allocated:
{noformat}
/usr/bin/time -f ""mem=%K RSS=%M elapsed=%E cpu.sys=%S .user=%U"" ant testclasslist -Dtest.classlistfile=/tmp/java_dtests_1_final.txt -Dtest.classlistprefix=distributed

testclasslist:
     [echo] Number of test runners: 1
[junit-timeout] Testsuite: org.apache.cassandra.distributed.DistributedReadWritePathTest
[junit-timeout] Testsuite: org.apache.cassandra.distributed.DistributedReadWritePathTest Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 103.473 sec
[junit-timeout] 
[junitreport] Processing /home/josephl/pg/cassandra/build/test/TESTS-TestSuites.xml to /tmp/null554485593
[junitreport] Loading stylesheet jar:file:/usr/share/ant/lib/ant-junit.jar!/org/apache/tools/ant/taskdefs/optional/junit/xsl/junit-frames.xsl
[junitreport] Transform time: 269ms
[junitreport] Deleting: /tmp/null554485593

BUILD SUCCESSFUL
Total time: 1 minute 47 seconds
mem=0 RSS=1606332 elapsed=1:47.37 cpu.sys=10.95 .user=159.99
{noformat}
But, if I use latest trunk (e871903d), I get 5GB:
{noformat}
testclasslist:
     [echo] Number of test runners: 1
    [mkdir] Created dir: /home/josephl/pg/cassandra/build/test/cassandra
    [mkdir] Created dir: /home/josephl/pg/cassandra/build/test/output
[junit-timeout] Testsuite: org.apache.cassandra.distributed.DistributedReadWritePathTest
[junit-timeout] Testsuite: org.apache.cassandra.distributed.DistributedReadWritePathTest Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 103.098 sec
[junit-timeout] 
[junitreport] Processing /home/josephl/pg/cassandra/build/test/TESTS-TestSuites.xml to /tmp/null126756458
[junitreport] Loading stylesheet jar:file:/usr/share/ant/lib/ant-junit.jar!/org/apache/tools/ant/taskdefs/optional/junit/xsl/junit-frames.xsl
[junitreport] Transform time: 330ms
[junitreport] Deleting: /tmp/null126756458

BUILD SUCCESSFUL
Total time: 2 minutes 15 seconds
mem=0 RSS=4962924 elapsed=2:15.93 cpu.sys=16.55 .user=284.28
{noformat}
Since the heap is 1GB, and we allocate about 256MB for the off-heap metaspace, 1.6GB is much closer to what we expect than 5GB.

So ,.. something about the new executor system may be contributing. Continuing to dig in.;;;","24/Jan/19 11:19;ifesdjeen;[~jolynch] to be honest I do not see OOMs so far. I'm running up to 100 3-node cluster instances and it works fine. Which JDK are you using? Could that be a difference?;;;","24/Jan/19 18:15;jolynch;[~ifesdjeen] they are no longer JVM level OOMs; I believe from the circleci ssh dmesg output this is a OS oomkill:
{noformat}
[32619.267150] Task in /docker/db2d2aa36ea5a82fa51c91a99d32de688d45610c74f496288015ad82845ebf40/390b809d1f78090fda35dcf8b56612302f71bee5632fd911b2ef94558023610b killed as a result of limit of /docker/db2d2aa36ea5a82fa51c91a99d32de688d45610c74f496288015ad82845ebf40
[32619.267155] memory: usage 4194304kB, limit 4194304kB, failcnt 28170
[32619.267156] memory+swap: usage 0kB, limit 9007199254740988kB, failcnt 0
[32619.267157] kmem: usage 0kB, limit 9007199254740988kB, failcnt 0
[32619.267158] Memory cgroup stats for /docker/db2d2aa36ea5a82fa51c91a99d32de688d45610c74f496288015ad82845ebf40: cache:16KB rss:9444KB rss_huge:0KB mapped_file:0KB dirty:0KB writeback:0KB inactive_anon:8KB active_anon:9452KB inactive_file:0KB active_file:0KB unevictable:0KB
[32619.267168] Memory cgroup stats for /docker/db2d2aa36ea5a82fa51c91a99d32de688d45610c74f496288015ad82845ebf40/390b809d1f78090fda35dcf8b56612302f71bee5632fd911b2ef94558023610b: cache:64956KB rss:4119888KB rss_huge:0KB mapped_file:17712KB dirty:32988KB writeback:0KB inactive_anon:60KB active_anon:4120272KB inactive_file:32116KB active_file:31896KB unevictable:0KB
[32619.267176] [ pid ]   uid  tgid total_vm      rss nr_ptes nr_pmds swapents oom_score_adj name
[32619.267245] [30017]     0 30017    31256     5620      22       4        0         -1000 circleci
[32619.267248] [31296] 166536 31296     1157      198       8       3        0           200 sh
[32619.267251] [31416] 166536 31416    31064     5075      22       4        0           200 circleci-agent
[32619.267253] [33206] 166536 33206     5012      745      15       3        0           200 bash
[32619.267351] [32251] 166536 32251  5984655   127267     432      11        0           200 java
[32619.267353] [32391] 166536 32391  8094234   904842    6190      34        0           200 java
[32619.267355] Memory cgroup out of memory: Kill process 32391 (java) score 1068 or sacrifice child
[32619.271649] Killed process 32391 (java) total-vm:32376936kB, anon-rss:3603092kB, file-rss:16276kB

# Try it manually
cassandra@390b809d1f78:/tmp/cassandra$ /usr/bin/time -f ""mem=%K RSS=%M elapsed=%E cpu.sys=%S .user=%U"" ant testclasslist -Dtest.classlistfile=/tmp/java_dtests_${CIRCLE_NODE_INDEX}_final.txt -Dtest.classlistprefix=distributed
...
testclasslist:
     [echo] Number of test runners: 1
[junit-timeout] Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF8
[junit-timeout] Testsuite: org.apache.cassandra.distributed.DistributedReadWritePathTest
[junit-timeout] Testsuite: org.apache.cassandra.distributed.DistributedReadWritePathTest
[junit-timeout] Testsuite: org.apache.cassandra.distributed.DistributedReadWritePathTest Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0 sec
[junit-timeout] 
[junit-timeout] Testcase: org.apache.cassandra.distributed.DistributedReadWritePathTest:readRepairTest: Caused an ERROR
[junit-timeout] Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
[junit-timeout] junit.framework.AssertionFailedError: Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
[junit-timeout]         at java.util.Vector.forEach(Vector.java:1275)
[junit-timeout]         at java.util.Vector.forEach(Vector.java:1275)
[junit-timeout]         at java.lang.Thread.run(Thread.java:748)
[junit-timeout] 
[junit-timeout] 
[junit-timeout] Test org.apache.cassandra.distributed.DistributedReadWritePathTest FAILED (crashed)
[junitreport] Processing /tmp/cassandra/build/test/TESTS-TestSuites.xml to /tmp/null1789186966
[junitreport] Loading stylesheet jar:file:/usr/share/ant/lib/ant-junit.jar!/org/apache/tools/ant/taskdefs/optional/junit/xsl/junit-frames.xsl
[junitreport] Transform time: 2223ms
[junitreport] Deleting: /tmp/null1789186966

BUILD FAILED
/tmp/cassandra/build.xml:1861: The following error occurred while executing this line:
/tmp/cassandra/build.xml:1751: Some test(s) failed.

Total time: 25 seconds
Command exited with non-zero status 1
mem=0 RSS=3667504 elapsed=0:26.40 cpu.sys=17.58 .user=52.25

# This was another oomkill
[33152.855977] Task in /docker/db2d2aa36ea5a82fa51c91a99d32de688d45610c74f496288015ad82845ebf40/390b809d1f78090fda35dcf8b56612302f71bee5632fd911b2ef94558023610b killed as a result of limit of /docker/db2d2aa36ea5a82fa51c91a99d32de688d45610c74f496288015ad82845ebf40 
[33152.855982] memory: usage 4194304kB, limit 4194304kB, failcnt 50983 
[33152.855984] memory+swap: usage 0kB, limit 9007199254740988kB, failcnt 0 
[33152.855985] kmem: usage 0kB, limit 9007199254740988kB, failcnt 0 
[33152.855986] Memory cgroup stats for /docker/db2d2aa36ea5a82fa51c91a99d32de688d45610c74f496288015ad82845ebf40: cache:16KB rss:9144KB rss_huge:0KB mapped_file:0KB dirty:0KB writeback:0KB inactive_anon:8KB active_anon:9152KB inactive_file:0KB active_file:0KB unevictable:0KB 
[33152.856001] Memory cgroup stats for /docker/db2d2aa36ea5a82fa51c91a99d32de688d45610c74f496288015ad82845ebf40/390b809d1f78090fda35dcf8b56612302f71bee5632fd911b2ef94558023610b: cache:2332KB rss:4182812KB rss_huge:0KB mapped_file:500KB dirty:944KB writeback:0KB inactive_anon
:52KB active_anon:4183236KB inactive_file:808KB active_file:632KB unevictable:0KB 
[33152.856012] [ pid ]   uid  tgid total_vm      rss nr_ptes nr_pmds swapents oom_score_adj name 
[33152.856086] [30017]     0 30017    31256     5515      23       4        0         -1000 circleci 
[33152.856090] [31296] 166536 31296     1157       16       8       3        0           200 sh 
[33152.856093] [31416] 166536 31416    31640     5323      24       4        0           200 circleci-agent 
[33152.856132] [  708] 166536   708     5070      143      15       3        0           200 bash 
[33152.856251] [28288] 166536 28288     1096       16       7       3        0           200 time 
[33152.856254] [28289] 166536 28289  5984655   125056     437      11        0           200 java 
[33152.856262] [28645] 166536 28645  8143453   914706    6291      35        0           200 java 
[33152.856310] Memory cgroup out of memory: Kill process 28645 (java) score 1078 or sacrifice child 
[33152.860694] Killed process 28645 (java) total-vm:32573812kB, anon-rss:3658488kB, file-rss:336kB{noformat}
For example on the latest trunk I see three test fails, the in-jvm dtests and two PagingTests: [7d138e20|https://circleci.com/gh/jolynch/cassandra/413#tests]. I'm also getting these failures in other bug fix branches e.g. [CASSANDRA-14096-trunk|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-trunk].

My java version running locally is 8u191:
{noformat}
$ java -version
java version ""1.8.0_191""
Java(TM) SE Runtime Environment (build 1.8.0_191-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode)
{noformat}
 ;;;","24/Jan/19 22:44;benedict;FWIW, I don't suspect anything.  I have had terminations due to file handle exhaustion locally, but I absolutely believe that we are likely to have some native memory leaks too.;;;","04/Feb/19 13:31;ifesdjeen;I've investigated this one a bit further, but to be honest could not find any proof of leaks. I've also tried to make shutdown process more synchronous to reduce amount of in-flight memory, but this hasn't helped. I did also check the possible native memory leak. You are right there native byte buffer instances hanging after instance shutdown, but all of them as far as I can tell were pending finalization (see !Screen Shot 2019-01-30 at 15.47.13.png!).

I've tried running tests in non-constrained environment in loop [here|https://circleci.com/gh/ifesdjeen/cassandra/1197], and it seems to be passing fine after 10x100 runs. 

I made a small follow-up patch (however, these improvements are minor and do not have any impact on the end result) [here|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:oom-improvements]. In this patch I also disable in-jvm dtests for resource constrained environment.

If this looks ok, I suggest we do that and merge [~benedict]'s multi-version patch and continue writing tests.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
thrift_test.py failures on 3.0 and 3.x branches,CASSANDRA-14921,13202318,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,samt,samt,04/Dec/18 17:35,16/Apr/19 09:29,13/Jul/23 08:37,07/Dec/18 15:55,3.0.18,3.11.4,,,,,Legacy/Testing,,,,0,,,,"{{putget_test::TestPutGet::test_wide_slice}} fails on CircleCI since the docker image was updated for CASSANDRA-14713. The reason for this is that the {{fastbinary}} extension used by {{TBinaryProtocolAccelerated}} is not compatible with thrift 0.10.0 (according to [this bug report against Pycassa|https://github.com/pycassa/pycassa/issues/245]). The offending binary is present in the filesystem of the [current docker image|https://hub.docker.com/r/spod/cassandra-testing-ubuntu18-java11/], but wasn't in [the previous image |https://hub.docker.com/r/kjellman/cassandra-test/], which meant that thrift would fallback to the standard protocol implementation (silently).

As this is the only test which uses {{TBinaryProtocolAccelerated}} it's easy enough to switch it to {{TBinaryProtocol}}, which also fixes things. We might want consider removing the binary next time the image is updated though (cc [~spodxx@gmail.com]).",,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Dec 07 15:55:57 UTC 2018,,,,,,,,,,,"0|s015rs:",9223372036854775807,,,,,,,,,,,spod,,,Normal,,,,,,,,,,,,,,,,,,,"04/Dec/18 17:40;samt;dtest branch: [https://github.com/beobal/cassandra-dtest/commits/fix_putget_test]

3.0 CI run using updated dtest: [https://circleci.com/workflow-run/cc252860-46a5-42b5-bf7a-fc8292bbbbac]

 ;;;","06/Dec/18 20:12;spod;Can't we just downgrade to 0.9.3 in dtest's requirements.txt? If there's a compatibility issue with our tests and 0.10, shouldn't we either use a different thrift version, or fix the issue on our side (which your patch does)? ;;;","07/Dec/18 10:03;samt;bq. Can't we just downgrade to 0.9.3 in dtest's requirements.txt?

I seem to recall that there is some other incompatibility between thrift 0.9 and python3, which was why the dependency got bumped in CASSANDRA-14134. I didn't make that change though, so I'll have to check. 

Fixing the issue on our side is fine with me, hence the patch. I just wanted to note the presence of the problematic binary so we can maybe remove next time we have to do some maintenance on the docker image.
 

 ;;;","07/Dec/18 10:05;samt;bq. yes, pycassa isn't python 3 compatible and we needed new thrift bindings to get python 3 support for the remianing thrift tests we still have. Jeff Jirsa did this work.

from: [this comment | https://issues.apache.org/jira/browse/CASSANDRA-14134?focusedCommentId=16314023&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16314023] on CASSANDRA-14134. ;;;","07/Dec/18 11:47;spod;+1 on the patch

 

Do you have the path of the binary that should be removed?;;;","07/Dec/18 15:55;samt;Thanks, committed as {{3d069f713ef2bd61a32863b29a6f160b74ecd89c}}

The binary that causes the issue is: {{/home/cassandra/env/lib/python3.6/site-packages/thrift/protocol/fastbinary.cpython-36m-x86_64-linux-gnu.so}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some comparisons used for verifying paging queries in dtests only test the column names and not values,CASSANDRA-14920,13202230,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aweisberg,samt,samt,04/Dec/18 10:31,02/Aug/19 03:10,13/Jul/23 08:37,15/Jan/19 21:35,,,,,,,Test/dtest/python,,,,0,,,,"The implementation of {{PageAssertionMixin::assertEqualIgnoreOrder}} introduced in CASSANDRA-14134 can't be used to compare expected and actual results when the row data is represented by a {{dict}}. The underlying {{list_to_hashed_dict}} function expected lists of values and when it encounters a dict, it constructs its normalized list using only the keys. So the actual result values may be completely wrong, but as long as the field names are the same the equality check will pass. This affects only {{paging_tests.py}} and {{upgrade_tests/paging_test.py}}, and looks like it maybe a leftover from an earlier dev iteration, as some tests in the affected fixtures are already using the alternative (and correct) {{assertions.py::assert_lists_equal_ignoring_order}}.
",,aweisberg,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14421,,,,,,,,,,,,,,,,,,,,,,,0.0,aweisberg,,,,,,,,,,,,Correctness -> Test Failure,,,,,,,,Normal,Unit Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 15 21:34:57 UTC 2019,,,,,,,,,,,"0|s01588:",9223372036854775807,,,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"15/Jan/19 21:34;aweisberg;I resolved this as part of 14421, I modifed https://github.com/apache/cassandra-dtest/commit/84598f11513f4c1dc0be4d7115a47b59940a649e#diff-96abe2232f1118eb0579d88abe504a9eL167 a bit and also pointed some tests at the other equality method that creates an order by sorting on a key.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regression in paging queries in mixed version clusters ,CASSANDRA-14919,13202226,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,samt,samt,samt,04/Dec/18 10:24,02/Aug/19 03:00,13/Jul/23 08:37,06/Dec/18 16:18,3.0.18,3.11.4,,,,,CQL/Interpreter,Messaging/Internode,Messaging/Thrift,,0,,,,"The changes to handling legacy bounds in CASSANDRA-14568/CASSANDRA-14749/CASSANDRA-14912 break paging queries where the coordinator is a legacy node and the replica is an upgraded node. 

The long-held assumption made by {{LegacyLayout::decodeBound}} that ""There can be more components than the clustering size only in the case this is the bound of a collection range tombstone."" is not true as serialized paged read commands may also include these type of bounds in their {{SliceQueryFilter}}. The additional checks the more recent tickets add cause such queries to error when processed by a 3.0 replica.",,aweisberg,benedict,cscotta,samt,tommy_s,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,Availability -> Unavailable,,,,,,,,Challenging,Unit Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 06 16:18:25 UTC 2018,,,,,,,,,,,"0|s0157c:",9223372036854775807,,,,,,,,,,,benedict,,,Critical,,3.0.0,,,,,,,,,,,,,,,,,"04/Dec/18 14:59;samt;This issue showed up during work on CASSANDRA-14421 to re-enable the upgrade dtests. Using [~aweisberg]'s dtest branch as a starting point, I've verified that the upgrade tests concerned with paging are now passing (after some additional tweaks - see CASSANDRA-14920). I've only run this locally though and haven't updated the CircleCI config to add those tests, leaving that to 14421.

The CircleCI runs below are running with the current dtest master branch:

||branch||CI||
|[14919-3.0|https://github.com/beobal/cassandra/tree/14919-3.0]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F14919-3.0]|
|[14919-3.11|https://github.com/beobal/cassandra/tree/14919-3.11]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F14919-3.11]|



 

 ;;;","04/Dec/18 15:53;aweisberg;Typo in the links to the code.;;;","04/Dec/18 16:41;samt;{quote}Typo in the links to the code{quote}

oops, actually a failure to push those 2 branches. Fixed now;;;","04/Dec/18 21:57;benedict;I'm largely +1 the changes, but I wonder if we should be inserting some assertions around our expectations here.

For instance, [here|https://github.com/beobal/cassandra/commit/b8bb748dca93deeae7c50319ab0634f599fedea9#diff-f53ee75d79fb30b497f6d1584d51d7d3R2429], [here|https://github.com/beobal/cassandra/commit/b8bb748dca93deeae7c50319ab0634f599fedea9#diff-861bc950c82973fb8f97f179e35be7f3R1586] and [here|https://github.com/beobal/cassandra/commit/b8bb748dca93deeae7c50319ab0634f599fedea9#diff-f53ee75d79fb30b497f6d1584d51d7d3R374] at least we pretty much require that there be no column name in the payload?  If this is somehow present, something looks likely to have gone awry, since we are discarding it and have nothing in place to suggest this is OK (unlike [here|https://github.com/beobal/cassandra/commit/b8bb748dca93deeae7c50319ab0634f599fedea9#diff-861bc950c82973fb8f97f179e35be7f3R1093])?

I'm also honestly not certain what the correct behaviour [here|https://github.com/beobal/cassandra/commit/b8bb748dca93deeae7c50319ab0634f599fedea9#diff-f53ee75d79fb30b497f6d1584d51d7d3R1245] is.  Presumably for thrift schemas, it is impossible to receive a collection name.  However I'm very out-of-touch with the compatibility mechanisms for accessing CQL via thrift.  Can we legitimately receive a collection RT?  I doubt it, but I cannot be sure.;;;","06/Dec/18 13:32;samt;{quote}For instance, [here|https://github.com/beobal/cassandra/commit/b8bb748dca93deeae7c50319ab0634f599fedea9#diff-f53ee75d79fb30b497f6d1584d51d7d3R2429], [here|https://github.com/beobal/cassandra/commit/b8bb748dca93deeae7c50319ab0634f599fedea9#diff-861bc950c82973fb8f97f179e35be7f3R1586] and [here|https://github.com/beobal/cassandra/commit/b8bb748dca93deeae7c50319ab0634f599fedea9#diff-f53ee75d79fb30b497f6d1584d51d7d3R374] at least we pretty much require that there be no column name in the payload? If this is somehow present, something looks likely to have gone awry, since we are discarding it and have nothing in place to suggest this is OK (unlike [here|https://github.com/beobal/cassandra/commit/b8bb748dca93deeae7c50319ab0634f599fedea9#diff-861bc950c82973fb8f97f179e35be7f3R1093])?  
{quote}

Actually, in these slice cases (as opposed to RTs), this is safe and equivalent to the pre-14568/14749 behaviour. Previously, the column name element from a query slice would be erroneously identified as a collection name, but this was not problematic as we only ever used the {{LegacyBound.slice}} field, which was correctly constructed as a result of the final element being popped from the list of components in {{decodeBound}}. The same thing holds now, the difference being that we no longer incorrectly assign the {{LegacyBound}} a collection name.

{quote}I'm also honestly not certain what the correct behaviour here is. Presumably for thrift schemas, it is impossible to receive a collection name. However I'm very out-of-touch with the compatibility mechanisms for accessing CQL via thrift. Can we legitimately receive a collection RT? I doubt it, but I cannot be sure.
{quote}
It's technically possible to receive a collection name here as they can always be hand rolled, but that's also fine. If a client were to send a deletion with a range slice that emulates a collection tombstone, we'd process it as normal and delete the collection. It's going down exactly the same path as non-thrift requests, so an invalid slice bound (e.g. like a bound ending with a missing or non-collection column name) would be rejected.  
;;;","06/Dec/18 13:52;benedict;bq. Actually, in these slice cases (as opposed to RTs), this is safe

But it isn't safe for the other end to have actually sent this, I think?  Since we ignore the value, we aren't going to be semantically equivalent to whatever was intended - either the other end has a bug, asking for something nonsensical, or we have a bug in that we treat it erroneously?

With paging we know it is fine because there are comments indicating we expect the column name, and that our treatment of ignoring it is correct, but in these cases I'm not sure it is logically possible to respond correctly to a request with this component?

I agree ignoring it would be equivalent to 3.0.0 behaviour, but it might help our future selves to document and enforce these expectations.

This isn't something I would block commit on, if you disagree though.

bq. It's technically possible to receive a collection name here

I agree it's technically possible, I am just unsure if our API intended to actually support this, and if it is misuse to use it (if we are to introduce assertions).  Looking at 2.2, though, we only refuse super columns because our new schema cannot support them.  So it's probably the case that this remains equivalent in behaviour to 2.2, which is all that really matters.

+1, however you decide to proceed.;;;","06/Dec/18 16:18;samt;I'm all for helping our future selves and I hope that the comments and link to this discussion will do that. I've stopped short of enforcing the expectations though, as what we have now fixes the breakage introduced by 14568/14749/14912 whilst retaining the fixes from those issues. It also preserves the behaviour of previous 3.0/3.x versions so I think this has the minimal risk of introducing new, unforeseen regressions. If it does transpire that there other cases where this existing behaviour is harmful, or this patch breaks something else we didn't consider, we can revisit then.

Committed to 3.0 in {{11043610e38281f650f289a7f9286d306f1369e3}} and merged to 3.11/trunk, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle ant-optional dependency,CASSANDRA-14915,13201256,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,29/Nov/18 06:43,15/May/20 08:05,13/Jul/23 08:37,20/Dec/18 12:48,3.0.18,3.11.4,4.0,4.0-alpha1,,,Build,,,,0,,,,"CASSANDRA-13117 added a JUnit task which dumps threads on unit test timeout, and it depends on a class in {{org.apache.tools.ant.taskdefs.optional}} which seems to not always be present depending on how {{ant}} was installed. It can cause this error when building;
{code:java}
Throws: cassandra-trunk/build.xml:1134: taskdef A class needed by class org.krummas.junit.JStackJUnitTask cannot be found:
org/apache/tools/ant/taskdefs/optional/junit/JUnitTask  using the classloader
AntClassLoader[/.../cassandra-trunk/lib/jstackjunit-0.0.1.jar]
{code}",,jmeredithco,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 20 12:48:32 UTC 2018,,,,,,,,,,,"0|s00za8:",9223372036854775807,,,,,,,,,jmeredithco,,jmeredithco,,,Normal,,,,,,,,,,,,,,,,,,,"29/Nov/18 07:46;marcuse;tested on Debian, Linux Mint and Fedora and with the ant 1.10 tarball, it fails only on Fedora, workaround for now is to {{yum install ant-optional}}, but working on a patch to not fail the build if it is not available.;;;","30/Nov/18 08:39;marcuse;patch: https://github.com/krummas/cassandra/commits/marcuse/junit
tests: https://circleci.com/workflow-run/85f433df-51f5-4b93-9c56-38788064041e

this will allow users to build cassandra without ant-optional

[~jmeredithco] do you have time to review? (you made some changes to build.xml lately so figured you would be a good candidate);;;","30/Nov/18 17:00;jmeredithco;I've confirmed the patch allows you to get beyond the initial error in the description on Fedora 26 and complete an {{ant build}}. However, it still requires ant-junit to be installed to get any of the test targets to complete (which are needed for {{jar}}/{{package}} targets).

Is this what you were hoping to achieve, or did you also expect the unit tests to be able to run with just {{ant}} and {{java-1.8.0-openjdk-devel}} installed?

An alternative would be to be more explicit about which dependencies are needed to build on Fedora in the docs.

 

 ;;;","30/Nov/18 17:40;marcuse;yeah we need to document that tests depend on ant-optional on fedora, I'll update the patch with that next week;;;","30/Nov/18 18:53;jmeredithco;I think that class is always packaged up in the {{ant-junit}} dependency, I grabbed the rpm and expanded it and checked the class is there.  I couldn't find an {{ant-optional}} package that included it under the standard repos.

{code}
[jmeredith@localhost x]$ jar tvf ./usr/share/java/ant/ant-junit.jar | grep JUnitTask
1746 Wed Mar 01 08:56:26 MST 2017 org/apache/tools/ant/taskdefs/optional/junit/JUnitTask$1.class
{code}

If you always need to install {{ant-junit}} on Fedora, does this patch still have value other than permitting the build task to succeed (and none of the test tasks)?;;;","30/Nov/18 20:06;jmeredithco;I worked out you meant ant-optional on the Debian derived ones - agreed we'll just need document requiring ant-junit on Fedora et al, and will test the patch against a distro with ant-optional available too.;;;","30/Nov/18 20:38;marcuse;bq. I worked out you meant ant-optional
ah sorry about that;;;","30/Nov/18 20:41;jmeredithco;Gave Ubuntu 16.04 a go - if you don't have {{ant-optionals}} installed the patch allows the {{ant build}} command to succeed, however the tests still won't run without {{ant-optionals}} installed as that contains the junit integration. 

Anyway, patch works as advertised (lowering the test timeout still triggers stack traces) - however I'm not sure why somebody would want to run without ant-junit available.

+1;;;","19/Dec/18 13:52;marcuse;3.0 branch: https://github.com/krummas/cassandra/commits/marcuse/14915-3.0
3.0 test run: https://circleci.com/workflow-run/6328c243-e81d-48cb-bb46-75920200d73e
3.11 branch: https://github.com/krummas/cassandra/commits/marcuse/14915-3.11
3.11 test run: https://circleci.com/workflow-run/eeed3f28-4230-48e7-8fc0-41db30df45da
trunk branch: https://github.com/krummas/cassandra/commits/marcuse/14915-trunk
trunk test run: https://circleci.com/workflow-run/6ae5b998-caa5-41ff-b172-f74c9a266950

note the documentation update on the trunk branch:
https://github.com/apache/cassandra/compare/trunk...krummas:marcuse/14915-trunk?expand=1#diff-1a8570f8bfa11221c5ea6eb81bf4f22eR55
I only added it to the trunk branch since we don't have any in-tree docs in the other branches, but users will most likely google the error and find this^;;;","19/Dec/18 15:11;jmeredithco;Patches all look good.  Does org.apache.cassandra.distributed.DistributedReadWritePathTest on trunk experience intermittent failures? I had a quick look but couldn't see much more than abnormal VM exit as the reason.;;;","19/Dec/18 15:25;marcuse;[~jmeredithco] yeah I think it might be because I ran on the low resource circle config - there is CASSANDRA-14922 which seems to be addressing those issues;;;","20/Dec/18 12:48;marcuse;and committed as {{23d722ee3a7b91bfe5ee6fe64d090ff247e80438}}, thanks!

of course I messed up the commit message and said ant-junit instead of ant-optional, the doc change should be correct though;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Forbid re-adding static columns as regular and vice versa,CASSANDRA-14913,13200641,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,26/Nov/18 18:12,15/May/20 08:07,13/Jul/23 08:37,27/Nov/18 14:01,4.0,4.0-alpha1,,,,,Cluster/Schema,,,,0,,,,"Re-adding a dropped column with an incompatible kind (dropped regular re-added as static, or dropped static re-added as regular) can ultimately result in corruption (see CASSANDRA-14843 for more context). In 3.x, unfortunately, we don’t persist enough context when dropping a column. In trunk, however, we do, and it’s trivial to forbid this operation, as we should.",,aleksey,benedict,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14948,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,Correctness -> Unrecoverable Corruption / Loss,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 27 14:09:40 UTC 2018,,,,,,,,,,,"0|s00vi0:",9223372036854775807,,,,,,,,,,,benedict,,,Normal,,,,,,,,,,,,,,,,,,,"26/Nov/18 18:15;aleksey;[Code|https://github.com/iamaleksey/cassandra/commits/14913], [CI|https://circleci.com/workflow-run/f88826a9-62c0-42bb-84ec-7e403ef80747].;;;","26/Nov/18 20:20;benedict;+1;;;","27/Nov/18 07:37;ifesdjeen;This is great, +1. 

Should we back port this and re-add multi-cell as single cell or vice versa to 3.0/3.11? These are pretty serious issues I'd say as they can lead to corruption.;;;","27/Nov/18 14:00;aleksey;Thanks, committed as [8aec742a1f86a825028f4a1267e111cd1a4aea40|https://github.com/apache/cassandra/commit/8aec742a1f86a825028f4a1267e111cd1a4aea40] to trunk. ;;;","27/Nov/18 14:08;aleksey;[~ifesdjeen] I wouldn't mind it, personally. The change would be more involved: would have to add an extra column ({{kind}}) to the schema table in 3.0 (that already exists in 4.0, making this change super trivial). Also extend {{DroppedColumn}} a little. But that doesn't sound prohibitively involved to me.

If you are interested in doing the backport, please do. Otherwise I'll do it eventually.

While at it, might also backport 4.0 prohibition on re-adding columns with a different type from the dropped one if nobody objects.;;;","27/Nov/18 14:09;benedict;{quote}While at it, might also backport 4.0 prohibition on re-adding columns with a different type from the dropped one if nobody objects.
{quote}
+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LegacyLayout errors on collection tombstones from dropped columns,CASSANDRA-14912,13200622,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,samt,samt,samt,26/Nov/18 17:04,02/Aug/19 02:44,13/Jul/23 08:37,28/Nov/18 16:15,3.0.18,3.11.4,,,,,Legacy/Local Write-Read Paths,,,,0,,,,"When reading legacy sstables in 3.0, the dropped column records in table metadata are not checked when a collection tombstone is encountered. This means that if a collection column was dropped and a new column with the same name but a non-collection type subsequently added prior to upgrading to 3.0, reading any sstables containing the collection data will error. This includes reads done by upgradesstables, which makes recovery from this situation without losing data impossible. Scrub will clean the affected tables, but any valid data will also be discarded.",,aleksey,benedict,jeromatron,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,Correctness -> Recoverable Corruption / Loss,,,,,,,,Challenging,Adhoc Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 28 16:15:57 UTC 2018,,,,,,,,,,,"0|s00ve0:",9223372036854775807,,,,,,,,,,,aleksey,,,Critical,,3.0.0,,,,,,,,,,,,,,,,,"27/Nov/18 12:43;samt;||branch||CI||
|[14912-3.0|https://github.com/beobal/cassandra/tree/14912-3.0]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F14912-3.0]|
|[14912-3.11|https://github.com/beobal/cassandra/tree/14912-3.11]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F14912-3.11]|

 ;;;","27/Nov/18 14:08;benedict;I think this is related to CASSANDRA-14749; basically I didn't properly fix it for a drop and re-add of different type.

Should a fix also go [here|https://github.com/apache/cassandra/blob/06c55f779ae68de98cce531e0b78be5716849003/src/java/org/apache/cassandra/db/LegacyLayout.java#L217]?  We could simply check that the column definition is a collection, and perhaps also (if we wanted to be super paranoid) that the type is compatible.

Checking the dropped time is also a great additional check, but it doesn't necessarily seem as robust, given clock drift.;;;","27/Nov/18 16:12;samt;Adding the check to {{LegacyLayout::decodeBound}} seems reasonable as an extra safety check, but it wouldn’t make a difference in this case as we’d then fetch the dropped column definition and proceed as before to construct the bound. We could also check that the dropped column def was a collection and throw if not, but that’s a slightly different bug IMO.

The dropped time check is what's really required otherwise we don't properly filter data from sstables written before the schema changes happened. Maybe I'm misunderstanding what you mean here about it not being terribly robust, but this is the mechanism by which we filter such atoms (or simple/complex columns) in both the legacy and non-legacy cases. I agree that it’s not perfect wrt to clock drift btw, just that it’s the only method we have AFAIK for handling dropped columns.;;;","27/Nov/18 16:13;aleksey;[~beobal] +1

[~benedict] Sam is about to reply, but I think we are actually good there.;;;","27/Nov/18 16:23;aleksey;bq. Adding the check to LegacyLayout::decodeBound seems reasonable as an extra safety check, but it wouldn’t make a difference in this case as we’d then fetch the dropped column definition and proceed as before to construct the bound. We could also check that the dropped column def was a collection and throw if not, but that’s a slightly different bug IMO.

Additionally, it can easily and legally be {{BytesType}}, as we used to not to bother to record dropped types in 2.1, so we cannot really reliably tell. But also we don't need to, as this should work out just fine either way.;;;","27/Nov/18 16:53;benedict;{quote}this is the mechanism by which we filter such atoms (or simple/complex columns) in both the legacy and non-legacy cases
{quote}
So, I agree that this bug probably also exists for our non-legacy handling of complex tombstones.

But our regular atom dropping is only expected to be a 'best effort' to prevent dropped data making it to the client.  This bug seems to be about preventing data being entirely unavailable because the {{ColumnDefinition}} is incorrect (i.e. not a complex column)?

So surely specifically checking the condition that causes the failure, i.e. that we're building a tombstone with a non-complex column, is the robust solution to preventing this unavailable data?

I agree that filtering it downstream based on dropped time is also beneficial, but in preventing this specific failure it seems to be either superfluous or insufficient in some cases?;;;","27/Nov/18 17:13;samt;bq. So surely specifically checking the condition that causes the failure, i.e. that we're building a tombstone with a non-complex column, is the robust solution to preventing this unavailable data?

Do you mean changing {{decodeBound}} to do something like return null or a special value in such cases and having its callers handle that? It can't throw in situations like this or else we're no better off than we are currently as the row containing the collection tombstone would still be unreadable.;;;","27/Nov/18 17:30;benedict;I _think_ the easiest fix for preventing the exception is to simply return a RT bound where the {{ColumnDefinition}} is definitely complex?

This might mean we return a RT that covers a non-existent column, but with your change we *should* filter that out.  But if we don't, that's fine, it's only a best effort, and it won't actually cause us any problems.

FWIW, I've tried this change to confirm it works [here|https://github.com/belliottsmith/cassandra/tree/14912-3.0-suggest].;;;","28/Nov/18 13:44;samt;Adding the first {{isComplex}} check (as per your branch) does help here because the dropped definition is complex. Adding the second {{isComplex}} check on that and constructing a fake definition if it isn't is probably worth it to deal with the rare case where a column has been dropped and re-added multiple times. Pushed a second commit with those additions & the test extended to cover them.

Although we *could* live without the {{isDroppedComplexDeletion}} check, I've left it in and extended the test to check both when it fails and succeeds. Though this does make the verification of results somewhat finicky, it's maybe useful in documenting the expected behaviour in these edge cases.;;;","28/Nov/18 13:50;benedict;+1;;;","28/Nov/18 16:15;samt;Thanks, committed to 3.0 in {{0a7fbee43f25b6ad3172825cd29bae455223ab33}} and merged to 3.11 and trunk (with {{-s ours}});;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't skip entire sstables when reading backwards with mixed clustering column order,CASSANDRA-14910,13200244,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,23/Nov/18 12:54,16/Apr/19 09:29,13/Jul/23 08:37,10/Dec/18 18:28,2.2.14,,,,,,Legacy/Local Write-Read Paths,,,,0,,,,"In 2.x, if a table has clustering columns in {{DESC}} order, any SSTable that doesn’t have any static rows in it will be skipped altogether when iterated in reverse.

This occurs due to the logic in {{ColumnSlice.compare()}} errorneusly sorting any empty {{ByteBuffer}} after non-empty values due to the way {{ReversedType}} operates. In case that empty {{ByteBuffer}} is coming from a static {{Composite}}, however, the logic breaks down. Static {{Composite}} components must *always* sort before any non-empty value, no matter the table’s comparator.

2.0, 2.1, and 2.2 are all affected. 3.0 isn’t, but only because we skip slice intersection logic entirely if static rows are present in the filter.

Introduced by CASSANDRA-8502.
",,aleksey,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Dec 10 18:28:35 UTC 2018,,,,,,,,,,,"0|s00t28:",9223372036854775807,2.1.20,2.2.13,,,,,,,,,ifesdjeen,,,Normal,,2.0.16,,,,,,,,,,,,,,,,,"23/Nov/18 13:00;aleksey;Code [here|https://github.com/iamaleksey/cassandra/commits/14910-2.2], CI [here|https://circleci.com/workflow-run/0c915d7b-e94a-4c47-b604-9f938e659b28].;;;","26/Nov/18 14:23;aleksey;As spotted by [~ifesdjeen] during review, fixing comparison logic with static slices exposes another breakage - specifically the scenario when a static row is present in the sstable. With reversed comparator, when trying to read just the static row, such sstables would now be skipped.

Pushed the fix to the same branch.;;;","27/Nov/18 10:49;ifesdjeen;The new version looks much better, the only thing is that current tests do not cover [this|https://github.com/apache/cassandra/compare/trunk...iamaleksey:14910-2.2#diff-82e58a7c5bec8818f2e88a982725690fR113] part (e.g. {{compare}} is now not called on static bounds since we're returning {{true}} before calling {{intersect}} which would invoke comparator), so while I'm not saying that the change is wrong, I'm not 100% sure if we should keep it unless we have a test or at least an example scenario where this one is required. Can you comment on where we need it?

UPDATE: I've ran some more tests and so far could only land into {{comparison != null}} clause but couldn't do it with static bounds.;;;","27/Nov/18 21:06;aleksey;Force-pushed an updated version that just avoids making any decisions regarding skipping sstables based on static slice bounds - start or end. Can't rely on them, can't trust them, they are as dodgy as our recording of min/max cell names if static rows are involved.;;;","03/Dec/18 18:08;ifesdjeen;Thank you for the patch, 

Latest version looks good! +1;;;","10/Dec/18 18:28;aleksey;Committed to 2.2 as [afa4563864889c78569e29466047b411cd866b38|https://github.com/apache/cassandra/commit/afa4563864889c78569e29466047b411cd866b38] and merged (the unit tests) upwards. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Netty IOExceptions caused by unclean client disconnects being logged at INFO instead of TRACE,CASSANDRA-14909,13200137,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,sumanth.pasupuleti,sumanth.pasupuleti,sumanth.pasupuleti,22/Nov/18 17:37,01/Aug/21 12:24,13/Jul/23 08:37,29/Nov/18 18:55,3.0.18,3.11.7,4.0,4.0-alpha1,,,Legacy/Streaming and Messaging,,,,1,,,,"Observed spam logs on 3.0.17 cluster with redundant Netty IOExceptions caused due to client-side disconnections.

{code:java}
INFO  [epollEventLoopGroup-2-28] 2018-11-20 23:23:04,386 Message.java:619 - Unexpected exception during request; channel = [id: 0x12995bc1, L:/xxx.xx.xxx.xxx:7104 - R:/xxx.xx.xxx.xxx:33754]
io.netty.channel.unix.Errors$NativeIoException: syscall:read(...)() failed: Connection reset by peer
	at io.netty.channel.unix.FileDescriptor.readAddress(...)(Unknown Source) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
{code}


{code:java}
INFO  [epollEventLoopGroup-2-23] 2018-11-20 13:16:33,263 Message.java:619 - Unexpected exception during request; channel = [id: 0x98bd7c0e, L:/xxx.xx.xxx.xxx:7104 - R:/xxx.xx.xx.xx:33350]
io.netty.channel.unix.Errors$NativeIoException: readAddress() failed: Connection timed out
	at io.netty.channel.unix.Errors.newIOException(Errors.java:117) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.unix.Errors.ioResult(Errors.java:138) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.unix.FileDescriptor.readAddress(FileDescriptor.java:175) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.epoll.AbstractEpollChannel.doReadBytes(AbstractEpollChannel.java:238) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:926) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:397) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:302) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) [netty-all-4.0.44.Final.jar:4.0.44.Final]
{code}

[CASSANDRA-7849|https://issues.apache.org/jira/browse/CASSANDRA-7849] addresses this for JAVA IO Exception like ""java.io.IOException: Connection reset by peer"", but not for Netty IOException since the exception message in Netty includes method name.",,aweisberg,jasobrown,sumanth.pasupuleti,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,sumanth.pasupuleti,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 29 18:55:00 UTC 2018,,,,,,,,,,,"0|s00seg:",9223372036854775807,3.0.17,4.0,,,,,,,jasobrown,,jasobrown,,,Low,,,,,,,,,,,,,,,,,,,"22/Nov/18 17:39;sumanth.pasupuleti;Code in question has not changed in trunk, so submitting a patch against trunk
https://github.com/apache/cassandra/pull/294;;;","29/Nov/18 14:35;jasobrown;[~sumanth.pasupuleti] added wrt use of the Java stream API on the PR;;;","29/Nov/18 18:55;jasobrown;+1

committed as sha {{e4d0ce6ba2d6088c7edf8475f02462e1606f606d}}. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"cassandra-stress does not work with frozen collections: list, set",CASSANDRA-14907,13199814,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jasonstack,jasonstack,jasonstack,21/Nov/18 11:51,15/May/20 08:03,13/Jul/23 08:37,10/Apr/19 11:56,3.0.19,3.11.5,4.0,4.0-alpha1,,,Tool/stress,,,,0,,,,"{code}
com.datastax.driver.core.exceptions.InvalidQueryException: Invalid operation (f_list = f_list + ?) for frozen collection column f_list
{code}
| patch | utest |
| [3.0|https://github.com/jasonstack/cassandra/commits/stress_frozen_collection_3.0] | [circle|https://circleci.com/gh/jasonstack/cassandra/745] |

This patch should apply cleanly.. {{Map}} is not supported yet..
",,jasonstack,jeromatron,mck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/19 10:57;mck;stress_14907.cql;https://issues.apache.org/jira/secure/attachment/12965442/stress_14907.cql","10/Apr/19 10:57;mck;stress_14907.yaml;https://issues.apache.org/jira/secure/attachment/12965443/stress_14907.yaml",,,,,,,,,,,,2.0,jasonstack,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 15 01:39:06 UTC 2019,,,,,,,,,,,"0|s00qfk:",9223372036854775807,,,,,,,,,,,mck,,,Low,,2.1 rc3,,,,,,,,,,,,,,,,,"10/Apr/19 10:57;mck;To test:
{code}
cassandra
cqlsh -f stress_14907.cql
cassandra-stress user profile=stress_14907.yaml ops\(insert=1\) no-warmup duration=1m
{code}
Fails with the exception from the description without the patch.

It would be awesome with a unit test for this, but the stress classes arn't so friendly to that atm.;;;","10/Apr/19 11:53;mck;Committed as 4a70a9a982f8b34b4ccc7744c074863aa1c5b63b;;;","15/Apr/19 01:39;jasonstack;thanks for the review!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If SizeEstimatesRecorder misses a 'onDropTable' notification, the size_estimates table will never be cleared for that table.",CASSANDRA-14905,13199553,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,Gerrrr,Gerrrr,Gerrrr,20/Nov/18 09:04,15/May/20 08:05,13/Jul/23 08:37,15/Jan/19 16:14,3.0.18,3.11.4,4.0,4.0-alpha1,,,Observability/Metrics,,,,0,,,,"if a node is down when a keyspace/table is dropped, it will receive the schema notification before the size estimates listener is registered, so the entries for the dropped keyspace/table will never be cleaned from the table. ",,aweisberg,Gerrrr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Dec/18 16:22;Gerrrr;14905-3.0-dtest.png;https://issues.apache.org/jira/secure/attachment/12952697/14905-3.0-dtest.png","21/Dec/18 16:22;Gerrrr;14905-3.0-testall.png;https://issues.apache.org/jira/secure/attachment/12952698/14905-3.0-testall.png","21/Dec/18 16:22;Gerrrr;14905-3.11-dtest.png;https://issues.apache.org/jira/secure/attachment/12952699/14905-3.11-dtest.png","21/Dec/18 16:22;Gerrrr;14905-3.11-testall.png;https://issues.apache.org/jira/secure/attachment/12952700/14905-3.11-testall.png","21/Dec/18 16:22;Gerrrr;14905-4.0-dtest.png;https://issues.apache.org/jira/secure/attachment/12952701/14905-4.0-dtest.png","21/Dec/18 16:22;Gerrrr;14905-4.0-testall.png;https://issues.apache.org/jira/secure/attachment/12952702/14905-4.0-testall.png",,,,,,,,6.0,Gerrrr,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 15 16:14:04 UTC 2019,,,,,,,,,,,"0|s00ovk:",9223372036854775807,,,,,,,,,aweisberg,,aweisberg,,,Low,,,,,,,,,,,,,,,,,,,"20/Nov/18 09:53;Gerrrr;Patches:
* [3.0|https://github.com/Gerrrr/cassandra/tree/14905-3.0]
* [3.11|https://github.com/Gerrrr/cassandra/tree/14905-3.11]
* [4.0|https://github.com/Gerrrr/cassandra/tree/14905-4.0]
* [dtest|https://github.com/Gerrrr/cassandra-dtest/tree/14905];;;","21/Dec/18 16:26;Gerrrr;CI:

||3.0||3.11||4.0||
|[dtest|https://issues.apache.org/jira/secure/attachment/12952697/14905-3.0-dtest.png]|[dtest|https://issues.apache.org/jira/secure/attachment/12952699/14905-3.11-dtest.png]|[dtest|https://issues.apache.org/jira/secure/attachment/12952701/14905-4.0-dtest.png]|
|[testall|https://issues.apache.org/jira/secure/attachment/12952698/14905-3.0-testall.png]|[testall|https://issues.apache.org/jira/secure/attachment/12952700/14905-3.11-testall.png]|[testall|https://issues.apache.org/jira/secure/attachment/12952702/14905-4.0-testall.png]|
;;;","31/Dec/18 18:52;aweisberg;LGTM. There is some whitespace churn in the 3.11 patch that isn't in the other patches. I would rather do a dedicated whitespace ticket and tackle more files then sneak it in here since we aren't touching that code.

Who is getting the author credit here? Multiple author credits?;;;","08/Jan/19 10:46;Gerrrr;Thanks for the review! It'll be great if both authors can get the credit. Otherwise please give it to Joel.;;;","15/Jan/19 16:14;aweisberg;Ran the dtests [here|https://circleci.com/gh/aweisberg/cassandra/2502#tests/containers/11] and [here|https://circleci.com/gh/aweisberg/cassandra/2503#tests/containers/32] as well as the utests.

https://circleci.com/gh/aweisberg/cassandra/2500
https://circleci.com/gh/aweisberg/cassandra/2498
https://circleci.com/gh/aweisberg/cassandra/2496

Committed as [ddbcff3363c5ad13bd8975e80b3f28ae8149a459|https://github.com/apache/cassandra/commit/ddbcff3363c5ad13bd8975e80b3f28ae8149a459] in Cassandra and [7a6d9002709628de2bc6af9d987a189b302e4472|https://github.com/apache/cassandra-dtest/commit/7a6d9002709628de2bc6af9d987a189b302e4472] in the dtests.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableloader doesn't understand listening for CQL connections on multiple ports,CASSANDRA-14904,13199468,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,icleasby,KurtG,KurtG,20/Nov/18 00:05,15/May/20 08:54,13/Jul/23 08:37,19/Mar/20 16:10,3.11.7,4.0,4.0-alpha4,,,,Local/Config,,,,0,,,,"sstableloader only searches the yaml for native_transport_port, so if native_transport_port_ssl is set and encryption is enabled sstableloader will fail to connect as it will use the non-SSL port for the connection.",,csplinter,e.dimitrova,icleasby,Jan Karlsson,jeromatron,KurtG,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,icleasby,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 19 16:10:06 UTC 2020,,,,,,,,,,,"0|s00ocw:",9223372036854775807,,,,,,,,,,,brandon.williams,,,Low,,3.11.6,,,https://github.com/apache/cassandra/commit/111f1da0075e1a7d6b1a5dec0f3a90d3ca427083,,,,,,,,,,,,,,"20/Nov/18 00:50;icleasby;Patches:
[trunk|https://github.com/apache/cassandra/compare/trunk...PenguinRage:OST-129-Update-sstabletableloader-to-understand-CQL-listening-on-multiple-ports]
 [3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...PenguinRage:OST-129-Update-sstabletableloader-to-understand-CQL-listening-on-multiple-ports-3.11];;;","10/Apr/19 12:48;Jan Karlsson;I scraped together some time to have a look. LGTM for the most part, but I have some thoughts.

I have been thinking of the use case where both native_transport_port and the native_transport_port_ssl are set.

1. With this patch, the behavior will be that we will always use the native_transport_port_ssl if both are set unless overridden by command line. I don't necessarily see a problem with that but it might not be very transparent behavior. 
2. No matter what we choose to do about this behavior, a test case that tests the case of both being set would be good to add.;;;","28/Jan/20 22:45;e.dimitrova;[~icleasby] [~Jan Karlsson] what is the status of this one? Any thoughts?;;;","19/Mar/20 16:07;brandon.williams;bq. With this patch, the behavior will be that we will always use the native_transport_port_ssl if both are set unless overridden by command line.

Actually if the non-ssl native_transport_port is set, it will always use that.  I don't see a problem with that either, if for some reason the port is inaccessible you'll find out quickly and be able to adjust the config.

bq. No matter what we choose to do about this behavior, a test case that tests the case of both being set would be good to add.

This actually tests both, native with the stock config, and ssl with the supplied one.

;;;","19/Mar/20 16:10;brandon.williams;I rebased this and fixed the test config for 3.11 so it passes correctly.  Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool cfstats prints index name twice,CASSANDRA-14903,13199459,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stefan.miklosovic,KurtG,KurtG,19/Nov/18 23:11,15/May/20 08:01,13/Jul/23 08:37,15/Apr/19 03:13,3.11.5,4.0,4.0-alpha1,,,,Tool/nodetool,,,,0,pull-request-available,,,"{code:java}
CREATE TABLE test.test (
id int PRIMARY KEY,
data text
);
CREATE INDEX test_data_idx ON test.test (data);

ccm node1 nodetool cfstats test

Total number of tables: 40
----------------
Keyspace : test
Read Count: 0
Read Latency: NaN ms
Write Count: 0
Write Latency: NaN ms
Pending Flushes: 0
Table (index): test.test_data_idxtest.test_data_idx
{code}",,icleasby,KurtG,mck,stefan.miklosovic,,,,,,,,,,,,,,,,,,,,,,"smiklosovic commented on pull request #310: CASSANDRA-14903 fixing nodetool cfstats printing index name twice
URL: https://github.com/apache/cassandra/pull/310
 
 
   Hi @michaelsembwever 
   
   I am not sure what output we want here. I just put together what Ian did for CASSANDRA-14903, it looks all consistent to me in sorted output too.
   
   Could you please review that and let me know?
   
   Thanks a lot!
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Apr/19 01:33;githubbot;600","michaelsembwever commented on pull request #310: CASSANDRA-14903 fixing nodetool cfstats printing index name twice
URL: https://github.com/apache/cassandra/pull/310
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Apr/19 02:44;githubbot;600","michaelsembwever commented on issue #310: CASSANDRA-14903 fixing nodetool cfstats printing index name twice
URL: https://github.com/apache/cassandra/pull/310#issuecomment-483089417
 
 
   you're correct smiklosovic . i didn't notice that keyspace always gets prepended to the name when in sorted mode.
   i will commit the patch as is.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Apr/19 02:44;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,CASSANDRA-5977,,,,,,,,,,,,,,,,,,,,,,,0.0,stefan.miklosovic,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 15 03:13:51 UTC 2019,,,,,,,,,,,"0|s00oaw:",9223372036854775807,,,,,,,,,,,mck,,,Low,,3.6,,,,,,,,,,,,.,,,,,"19/Nov/18 23:43;icleasby;One line patch for:

[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...PenguinRage:Nodetool-cfstats-prints-out-index-names-twice-3.11]

[trunk|https://github.com/apache/cassandra/compare/trunk...PenguinRage:Nodetool-cfstats-prints-out-index-names-twice] ;;;","04/Apr/19 03:56;mck;In trunk it doesn't fix the output when in sorted mode…
{code}
nodetool cfstats -s table_name  test
{code}
Before the patch
{noformat}
Total number of tables: 40
----------------
	Table (index): test.test_data_idxtest.test.test_data_idx
         …
{noformat}
After the patch
{noformat}
Total number of tables: 40
----------------
	Table (index): test.test.test_data_idx
        …
{noformat};;;","04/Apr/19 06:47;icleasby;[~mck] Hang on the output is including the keyspace {color:red}`test`{color} and table  name {color:red}`test`{color}  with idx of {color:red}`test_data_idx`{color}.;;;","04/Apr/19 21:59;mck;bq. Hang on the output is including the keyspace `test` and table name `test` with idx of `test_data_idx`.
Yes, it's a bit messy :-(

It's expanding the scope of the ticket, but it would be nice to get the display text correct in all modes in one commit.
I tested the yml and json output formats and they worked fine.;;;","04/Apr/19 22:38;stefan.miklosovic;Hi [~mck] no worries, I ll cover this (I work with [~icleasby] closely);;;","15/Apr/19 02:52;mck;bq. In trunk it doesn't fix the output when in sorted mode…

Back-tracking on this statement. It appears that the keyspace always gets prepended to the name in sorted mode.
The patch looks good as it was!;;;","15/Apr/19 03:13;mck;Committed as f0aef2c54dd23a3a603664b1bc17c2e058b68031;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DigestMismatchException log messages should be at TRACE,CASSANDRA-14900,13198928,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,Gerrrr,Gerrrr,Gerrrr,16/Nov/18 14:06,14/Jul/21 15:04,13/Jul/23 08:37,14/Jul/21 15:03,3.0.25,3.11.11,,,,,Legacy/Observability,Local/Config,,,0,,,,"DigestMismatchException log messages should probably be at TRACE. These log messages about normal digest mismatches that include scary stacktraces:

{noformat}
DEBUG [ReadRepairStage:40] 2017-10-24 19:45:50,349  ReadCallback.java:242 - Digest mismatch:
org.apache.cassandra.service.DigestMismatchException: Mismatch for key DecoratedKey(-786225366477494582, 31302e33322e37382e31332d6765744469736b5574696c50657263656e742d736463) (943070f62d72259e3c25be0c6f76e489 vs f4c7c7675c803e0028992e11e0bbc5a0)
        at org.apache.cassandra.service.DigestResolver.compareResponses(DigestResolver.java:92) ~[cassandra-all-3.11.0.1855.jar:3.11.0.1855]
        at org.apache.cassandra.service.ReadCallback$AsyncRepairRunner.run(ReadCallback.java:233) ~[cassandra-all-3.11.0.1855.jar:3.11.0.1855]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_121]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_121]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [cassandra-all-3.11.0.1855.jar:3.11.0.1855]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_121]
{noformat}",,e.dimitrova,easyoups,Gerrrr,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,Gerrrr,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 14 15:04:03 UTC 2021,,,,,,,,,,,"0|s00l1s:",9223372036854775807,,,,,,,,,,,brandon.williams,e.dimitrova,,Low,,NA,,,https://github.com/apache/cassandra/commit/3d59557aceaaaedd8eafc4783394330d55a7fb4e,,,,,,,,,run CI,,,,,"16/Nov/18 14:23;Gerrrr;||3.0 | [patch | https://github.com/Gerrrr/cassandra/tree/14900-3.0] | [test | https://gist.github.com/Gerrrr/484e2dd1081aa78504f7cbbf5774530b]|
||3.11| [patch | https://github.com/Gerrrr/cassandra/tree/14900-3.11] | [test | https://gist.github.com/Gerrrr/0bccf5ccd9fc66b62fde8174360e4635]|
;;;","23/Dec/18 12:20;ifesdjeen;In 4.0 we do not use exceptions for control-flow in this case anymore, and I do agree it's a good idea to avoid logging a stack trace here. But we do a [slightly different thing there|https://github.com/apache/cassandra/blob/e645b9172c5d50fc2af407de724e46121edfe109/src/java/org/apache/cassandra/service/reads/AbstractReadExecutor.java#L390], can you explain motivation for not keeping it the same across versions, since [we do have tracing in 3.0|https://github.com/Gerrrr/cassandra/blob/14900-3.0/src/java/org/apache/cassandra/tracing/Tracing.java]?;;;","26/Feb/20 16:16;Gerrrr;You are right! {{logger.trace}} was redundant there as {{traceState.trace}} was invoked right before it. I updated the patch, it only removes debug stack trace statement now.;;;","08/Oct/20 15:43;e.dimitrova;Hi [~Gerrrr] and [~ifesdjeen],

Any thoughts about this ticket?

I haven't looked at the patch but it looks like you both agreed on it?;;;","09/Oct/20 11:15;Gerrrr;Thanks for bringing it up [~e.dimitrova] !

I think the patch in its current shape is ready if we agree that it is a good idea not to print digest mismatch exceptions in debug log.;;;","07/Jul/21 22:11;e.dimitrova;I personally don't see a reason why this can be agreed in 4.0 but not good for 3.11 or 3.0.

Anyway, we need one more committer reviewer or maybe [~ifesdjeen] will have some time to look at it again. 

 ;;;","14/Jul/21 14:27;brandon.williams;+1;;;","14/Jul/21 15:04;brandon.williams;Committed, thank you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot perform slice reads in reverse direction against tables with clustering columns in mixed order,CASSANDRA-14899,13198891,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,16/Nov/18 12:14,16/Apr/19 09:29,13/Jul/23 08:37,20/Nov/18 15:31,2.2.14,,,,,,Legacy/CQL,,,,0,,,,"CASSANDRA-11196 accidentally broke reading from tables with mixed clustering column order in the opposite direction.

{{ReversedPrimaryKeyRestrictions::boundsAsComposites}} method attempts to reverse the list returned from {{PrimaryKeyRestrictionSet::boundsAsComposites}} and fails, as Guava’s {{Lists::transform}} method returns a {{List}} that doesn’t support {{set()}}.

Reproduction:

{code}
CREATE TABLE test.test (
    a int,
    b int,
    c int,
    PRIMARY KEY (a, b, c)
) WITH CLUSTERING ORDER BY (b ASC, c DESC);

SELECT * FROM test.test WHERE a = 0 AND (b, c) > (0, 0) ORDER BY b DESC, c ASC;

> ServerError: java.lang.UnsupportedOperationException
{code}

{code}
java.lang.UnsupportedOperationException: null
	at java.util.AbstractList.set(AbstractList.java:132) ~[na:1.8.0_181]
	at java.util.Collections.swap(Collections.java:497) ~[na:1.8.0_181]
	at java.util.Collections.reverse(Collections.java:378) ~[na:1.8.0_181]
	at org.apache.cassandra.cql3.restrictions.ReversedPrimaryKeyRestrictions.boundsAsComposites(ReversedPrimaryKeyRestrictions.java:63) ~[main/:na]
	at org.apache.cassandra.cql3.restrictions.StatementRestrictions.getClusteringColumnsBoundsAsComposites(StatementRestrictions.java:580) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.makeFilter(SelectStatement.java:418) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.getSliceCommands(SelectStatement.java:359) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.getPageableCommand(SelectStatement.java:191) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:172) ~[main/:na]
{code}",,aleksey,cscotta,ifesdjeen,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 20 15:31:20 UTC 2018,,,,,,,,,,,"0|s00ktk:",9223372036854775807,2.2.13,,,,,,,,,,ifesdjeen,,,Normal,,2.2.6,,,,,,,,,,,,,,,,,"16/Nov/18 12:20;aleksey;Code [here|https://github.com/iamaleksey/cassandra/commits/14899-2.2], CI [here|https://circleci.com/workflow-run/b6db8bff-2aa2-49e7-8a8e-d01079c85872].;;;","20/Nov/18 14:26;ifesdjeen;+1 with two minor optional nits for your consideration: 

  * {{ArrayList -> List}} [here|https://github.com/iamaleksey/cassandra/commit/29b7f064d69aff55123dcfc282c3c44ab733734f#diff-f2feb48017f94269b5fbb96ee38b5816R204]
  * you can short-circuit [here|https://github.com/iamaleksey/cassandra/commit/29b7f064d69aff55123dcfc282c3c44ab733734f#diff-f2feb48017f94269b5fbb96ee38b5816R199]

If you think these modifications are unnecessary, please feel free to commit as-is.;;;","20/Nov/18 15:31;aleksey;Committed as [cf6f7920f7742bb9a17a23ad37499d9213807d81|https://github.com/apache/cassandra/commit/cf6f7920f7742bb9a17a23ad37499d9213807d81] to 2.2, and merged upwards (just the unit tests bit). Cheers.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Key cache loading is very slow when there are many SSTables,CASSANDRA-14898,13198826,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,n.v.harikrishna,jolynch,jolynch,16/Nov/18 05:37,16/Mar/22 08:53,13/Jul/23 08:37,09/Dec/21 15:51,3.0.26,3.11.12,4.0.2,,,,Legacy/Local Write-Read Paths,,,,1,low-hanging-fruit,Performance,,"While dealing with a production issue today where some 3.0.17 nodes had close to ~8k sstables on disk due to excessive write pressure, we had a few nodes crash due to OOM and then they took close to 17 minutes to load the key cache and recover. This excessive key cache load significantly increased the duration of the outage (to mitigate we just removed the saved key cache files). For example here is one example taking 17 minutes to load 10k keys, or about 10 keys per second (which is ... very slow):
{noformat}
INFO  [pool-3-thread-1] 2018-11-15 21:50:21,885 AutoSavingCache.java:190 - reading saved cache /mnt/data/cassandra/saved_caches/KeyCache-d.db
INFO  [pool-3-thread-1] 2018-11-15 22:07:16,490 AutoSavingCache.java:166 - Completed loading (1014606 ms; 10103 keys) KeyCache cache
{noformat}
I've witnessed similar behavior in the past with large LCS clusters, and indeed it appears that any time the number of sstables is large, KeyCache loading takes a _really_ long time. Today I got a flame graph and I believe that I found the issue and I think it's reasonably easy to fix. From what I can tell the {{KeyCacheSerializer::deserialize}} [method |https://github.com/apache/cassandra/blob/06209037ea56b5a2a49615a99f1542d6ea1b2947/src/java/org/apache/cassandra/service/CacheService.java#L445] which is called for every key is linear in the number of sstables due to the [call|https://github.com/apache/cassandra/blob/06209037ea56b5a2a49615a99f1542d6ea1b2947/src/java/org/apache/cassandra/service/CacheService.java#L459] to {{ColumnFamilyStore::getSSTables}} which ends up calling {{View::select}} [here|https://github.com/apache/cassandra/blob/06209037ea56b5a2a49615a99f1542d6ea1b2947/src/java/org/apache/cassandra/db/lifecycle/View.java#L139]. The {{View::select}} call is linear in the number of sstables and causes a _lot_ of {{HashSet}} [resizing|https://github.com/apache/cassandra/blob/06209037ea56b5a2a49615a99f1542d6ea1b2947/src/java/org/apache/cassandra/db/lifecycle/View.java#L139] when the number of sstables is much greater than 16 (the default size of the backing {{HashMap}}).

As we see in the attached flamegraph we spend 50% of our CPU time in these {{getSSTable}} calls, of which 36% is spent adding sstables to the HashSet in {{View::select}} and 17% is spent just iterating the sstables in the first place. A full 16% of CPU time is spent _just resizing the HashMap_. Then another 4% is spend calling {{CacheService::findDesc}} which does [a linear search|https://github.com/apache/cassandra/blob/06209037ea56b5a2a49615a99f1542d6ea1b2947/src/java/org/apache/cassandra/service/CacheService.java#L475] for the sstable generation.

I believe that this affects at least Cassandra 3.0.17 and trunk, and could be pretty easily fixed by either caching the getSSTables call or at the very least pre-sizing the {{HashSet}} in {{View::select}} to be the size of the sstables map.","AWS i3.2xlarge, 4 physical cores (8 threads), 60GB of RAM, loading about 8MB of KeyCache with 10k keys in it.",aleksey,djoshi,dnk,jeromatron,jolynch,KurtG,marcuse,mck,n.v.harikrishna,sumanth.pasupuleti,vinaykumarcse,yakir.g,yifanc,,,,,,,,,,,,,"nvharikrishna opened a new pull request #1288:
URL: https://github.com/apache/cassandra/pull/1288


   Made changes to load key cache faster, added limit cache load time using config parameter and few test cases.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Oct/21 17:31;githubbot;600","nvharikrishna opened a new pull request #1290:
URL: https://github.com/apache/cassandra/pull/1290


   1. Made changes to load key cache faster, added limit cache load time using config parameter and few test cases.
   2. Updated jmh version as CacheLoaderBench not terminating after the test.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Oct/21 17:19;githubbot;600","krummas commented on a change in pull request #1290:
URL: https://github.com/apache/cassandra/pull/1290#discussion_r763002911



##########
File path: src/java/org/apache/cassandra/service/CacheService.java
##########
@@ -472,7 +482,23 @@ public void serialize(KeyCacheKey key, DataOutputPlus out, ColumnFamilyStore cfs
             int generation = input.readInt();
             input.readBoolean(); // backwards compatibility for ""promoted indexes"" boolean
             SSTableReader reader = null;
-            if (cfs == null || !cfs.isKeyCacheEnabled() || (reader = findDesc(generation, cfs.getSSTables(SSTableSet.CANONICAL))) == null)
+            if (!skipEntry)
+            {
+                Pair<String, String> qualifiedName = Pair.create(cfs.metadata.ksName, cfs.metadata.cfName);
+                Map<Integer, SSTableReader> generationToSSTableReader = cachedSSTableReaders.get(qualifiedName);
+                if (generationToSSTableReader == null)
+                {
+                    generationToSSTableReader = cfs.collect(SSTableSet.CANONICAL,

Review comment:
       could we just build the map here? Don't see a need for the new collect(...) method beyond this
   
   if we want to keep the method there should probably be tests for it as well

##########
File path: conf/cassandra.yaml
##########
@@ -286,7 +286,11 @@ counter_cache_save_period: 7200
 # If not set, the default directory is $CASSANDRA_HOME/data/saved_caches.
 # saved_caches_directory: /var/lib/cassandra/saved_caches
 
-# commitlog_sync may be either ""periodic"" or ""batch."" 
+# Max amount of time in seconds server has wait for cache to load while starting the Cassandra process.
+# Cache will not load (during startup) if this value is negative as there is no time left to load.
+# max_cache_load_time: 30

Review comment:
       maybe name this `max_cache_load_time_seconds` to make it clearer




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Dec/21 13:44;githubbot;600","jolynch commented on a change in pull request #1290:
URL: https://github.com/apache/cassandra/pull/1290#discussion_r763149754



##########
File path: src/java/org/apache/cassandra/db/lifecycle/View.java
##########
@@ -18,6 +18,9 @@
 package org.apache.cassandra.db.lifecycle;
 
 import java.util.*;
+import java.util.stream.Collector;
+import java.util.stream.Collectors;

Review comment:
       nit: unused import. If you remove the collect method I think you could delete all of these
   

##########
File path: src/java/org/apache/cassandra/cache/AutoSavingCache.java
##########
@@ -198,8 +198,9 @@ public int loadSaved()
                                               + "" does not match current schema version ""
                                               + Schema.instance.getVersion());
 
-                ArrayDeque<Future<Pair<K, V>>> futures = new ArrayDeque<Future<Pair<K, V>>>();
-                while (in.available() > 0)
+                ArrayDeque<Future<Pair<K, V>>> futures = new ArrayDeque<>();
+                long loadByNanos = start + TimeUnit.SECONDS.toNanos(DatabaseDescriptor.getMaxCacheLoadTime());

Review comment:
       :+1: Nice!

##########
File path: src/java/org/apache/cassandra/service/CacheService.java
##########
@@ -472,7 +482,23 @@ public void serialize(KeyCacheKey key, DataOutputPlus out, ColumnFamilyStore cfs
             int generation = input.readInt();
             input.readBoolean(); // backwards compatibility for ""promoted indexes"" boolean
             SSTableReader reader = null;
-            if (cfs == null || !cfs.isKeyCacheEnabled() || (reader = findDesc(generation, cfs.getSSTables(SSTableSet.CANONICAL))) == null)
+            if (!skipEntry)
+            {
+                Pair<String, String> qualifiedName = Pair.create(cfs.metadata.ksName, cfs.metadata.cfName);
+                Map<Integer, SSTableReader> generationToSSTableReader = cachedSSTableReaders.get(qualifiedName);
+                if (generationToSSTableReader == null)
+                {
+                    generationToSSTableReader = cfs.collect(SSTableSet.CANONICAL,

Review comment:
       Agreed, I think now that we're keeping state something similar to the original patch that just calls `cfs.getSSTables` would work. Then we can remove View::collect
   
   ```
   if (generationToSSTableReader == null)                                          
   {                                                                               
       generationToSSTableReader = new HashMap<>(cfs.getLiveSSTables().size());    
       for (SSTableReader ssTableReader : cfs.getSSTables(SSTableSet.CANONICAL))   
           generationToSSTableReader.put(ssTableReader.descriptor.generation, ssTableReader);
                                                                                   
       cachedSSTableReaders.putIfAbsent(cfs.metadata.ksAndCFName, generationToSSTableReader);
   }                                                                               
   reader = generationToSSTableReader.get(generation);
   ```
   
   If we do that we probably want to change the `CANONICAL` case to pre-size the map as well
   ```
   Set<SSTableReader> canonicalSSTables = new HashSet<>(sstables.size() + compacting.size());
   ```
   

##########
File path: test/unit/org/apache/cassandra/db/KeyCacheTest.java
##########
@@ -231,6 +258,111 @@ public void testKeyCache() throws ExecutionException, InterruptedException
         assertKeyCacheSize(noEarlyOpen ? 4 : 2, KEYSPACE1, COLUMN_FAMILY1);
     }
 
+    @Test
+    public void testKeyCacheLoadNegativeCacheLoadTime() throws Exception
+    {
+        DatabaseDescriptor.setMaxCacheLoadTime(-1);
+        String cf = COLUMN_FAMILY7;
+
+        createAndInvalidateCache(Collections.singletonList(Pair.create(KEYSPACE1, cf)), 100);
+
+        CacheService.instance.keyCache.loadSaved();
+
+        // Here max time to load cache is negative which means no time left to load cache. So the keyCache size should
+        // be zero after loadSaved().
+        assertKeyCacheSize(0, KEYSPACE1, cf);
+        assertEquals(0, CacheService.instance.keyCache.size());
+    }
+
+    @Test
+    public void testKeyCacheLoadTwoTablesTime() throws Exception
+    {
+        DatabaseDescriptor.setMaxCacheLoadTime(60);
+        String columnFamily1 = COLUMN_FAMILY8;
+        String columnFamily2 = COLUMN_FAMILY_K2_1;
+        int numberOfRows = 100;
+        List<Pair<String, String>> tables = new ArrayList<>(2);
+        tables.add(Pair.create(KEYSPACE1, columnFamily1));
+        tables.add(Pair.create(KEYSPACE2, columnFamily2));
+
+        createAndInvalidateCache(tables, numberOfRows);
+
+        CacheService.instance.keyCache.loadSaved();
+
+        // Here max time to load cache is negative which means no time left to load cache. So the keyCache size should
+        // be zero after load.
+        assertKeyCacheSize(numberOfRows, KEYSPACE1, columnFamily1);
+        assertKeyCacheSize(numberOfRows, KEYSPACE2, columnFamily2);
+        assertEquals(numberOfRows * tables.size(), CacheService.instance.keyCache.size());
+    }
+
+    @SuppressWarnings({ ""unchecked"", ""rawtypes"" })
+    @Test
+    public void testKeyCacheLoadCacheLoadTimeExceedingLimit() throws Exception
+    {
+        DatabaseDescriptor.setMaxCacheLoadTime(2);
+        int delayMillis = 1000;
+        int numberOfRows = 100;
+
+        String cf = COLUMN_FAMILY9;
+
+        createAndInvalidateCache(Collections.singletonList(Pair.create(KEYSPACE1, cf)), numberOfRows);
+
+        // Testing cache load. Here using custom built AutoSavingCache instance as simulating delay is not possible with
+        // 'CacheService.instance.keyCache'. 'AutoSavingCache.loadSaved()' is returning no.of entries loaded so we don't need
+        // to instantiate ICache.class.
+        CacheService.KeyCacheSerializer keyCacheSerializer = new CacheService.KeyCacheSerializer();
+        CacheService.KeyCacheSerializer keyCacheSerializerSpy = Mockito.spy(keyCacheSerializer);
+        AutoSavingCache autoSavingCache = new AutoSavingCache(mock(ICache.class),
+                                                              CacheService.CacheType.KEY_CACHE,
+                                                              keyCacheSerializerSpy);
+
+        doAnswer(new AnswersWithDelay(delayMillis, answer -> keyCacheSerializer.deserialize(answer.getArgument(0),
+                                                                                            answer.getArgument(1)) ))
+               .when(keyCacheSerializerSpy).deserialize(any(DataInputPlus.class), any(ColumnFamilyStore.class));
+
+        long maxExpectedKeyCache = Math.min(numberOfRows,
+                                            1 + TimeUnit.SECONDS.toMillis(DatabaseDescriptor.getMaxCacheLoadTime()) / delayMillis);
+
+        long keysLoaded = autoSavingCache.loadSaved();

Review comment:
       Since you've got a spy can you assert the cleanup method was called?
   ```
   Mockito.verify(keyCacheSerializerSpy, Mockito.times(1)).cleanupAfterDeserialize();
   ```

##########
File path: conf/cassandra.yaml
##########
@@ -286,7 +286,11 @@ counter_cache_save_period: 7200
 # If not set, the default directory is $CASSANDRA_HOME/data/saved_caches.
 # saved_caches_directory: /var/lib/cassandra/saved_caches
 
-# commitlog_sync may be either ""periodic"" or ""batch."" 
+# Max amount of time in seconds server has wait for cache to load while starting the Cassandra process.
+# Cache will not load (during startup) if this value is negative as there is no time left to load.
+# max_cache_load_time: 30

Review comment:
       I'm :+1: on the suggested name, or just call it a timeout `cache_load_timeout_seconds`?




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Dec/21 16:37;githubbot;600","jolynch commented on a change in pull request #1290:
URL: https://github.com/apache/cassandra/pull/1290#discussion_r765065981



##########
File path: test/unit/org/apache/cassandra/db/KeyCacheTest.java
##########
@@ -231,6 +258,111 @@ public void testKeyCache() throws ExecutionException, InterruptedException
         assertKeyCacheSize(noEarlyOpen ? 4 : 2, KEYSPACE1, COLUMN_FAMILY1);
     }
 
+    @Test
+    public void testKeyCacheLoadNegativeCacheLoadTime() throws Exception
+    {
+        DatabaseDescriptor.setMaxCacheLoadTime(-1);
+        String cf = COLUMN_FAMILY7;
+
+        createAndInvalidateCache(Collections.singletonList(Pair.create(KEYSPACE1, cf)), 100);
+
+        CacheService.instance.keyCache.loadSaved();
+
+        // Here max time to load cache is negative which means no time left to load cache. So the keyCache size should
+        // be zero after loadSaved().
+        assertKeyCacheSize(0, KEYSPACE1, cf);
+        assertEquals(0, CacheService.instance.keyCache.size());
+    }
+
+    @Test
+    public void testKeyCacheLoadTwoTablesTime() throws Exception
+    {
+        DatabaseDescriptor.setMaxCacheLoadTime(60);
+        String columnFamily1 = COLUMN_FAMILY8;
+        String columnFamily2 = COLUMN_FAMILY_K2_1;
+        int numberOfRows = 100;
+        List<Pair<String, String>> tables = new ArrayList<>(2);
+        tables.add(Pair.create(KEYSPACE1, columnFamily1));
+        tables.add(Pair.create(KEYSPACE2, columnFamily2));
+
+        createAndInvalidateCache(tables, numberOfRows);
+
+        CacheService.instance.keyCache.loadSaved();
+
+        // Here max time to load cache is negative which means no time left to load cache. So the keyCache size should
+        // be zero after load.
+        assertKeyCacheSize(numberOfRows, KEYSPACE1, columnFamily1);
+        assertKeyCacheSize(numberOfRows, KEYSPACE2, columnFamily2);
+        assertEquals(numberOfRows * tables.size(), CacheService.instance.keyCache.size());
+    }
+
+    @SuppressWarnings({ ""unchecked"", ""rawtypes"" })
+    @Test
+    public void testKeyCacheLoadCacheLoadTimeExceedingLimit() throws Exception
+    {
+        DatabaseDescriptor.setMaxCacheLoadTime(2);
+        int delayMillis = 1000;
+        int numberOfRows = 100;
+
+        String cf = COLUMN_FAMILY9;
+
+        createAndInvalidateCache(Collections.singletonList(Pair.create(KEYSPACE1, cf)), numberOfRows);
+
+        // Testing cache load. Here using custom built AutoSavingCache instance as simulating delay is not possible with
+        // 'CacheService.instance.keyCache'. 'AutoSavingCache.loadSaved()' is returning no.of entries loaded so we don't need
+        // to instantiate ICache.class.
+        CacheService.KeyCacheSerializer keyCacheSerializer = new CacheService.KeyCacheSerializer();
+        CacheService.KeyCacheSerializer keyCacheSerializerSpy = Mockito.spy(keyCacheSerializer);
+        AutoSavingCache autoSavingCache = new AutoSavingCache(mock(ICache.class),
+                                                              CacheService.CacheType.KEY_CACHE,
+                                                              keyCacheSerializerSpy);
+
+        doAnswer(new AnswersWithDelay(delayMillis, answer -> keyCacheSerializer.deserialize(answer.getArgument(0),
+                                                                                            answer.getArgument(1)) ))
+               .when(keyCacheSerializerSpy).deserialize(any(DataInputPlus.class), any(ColumnFamilyStore.class));
+
+        long maxExpectedKeyCache = Math.min(numberOfRows,
+                                            1 + TimeUnit.SECONDS.toMillis(DatabaseDescriptor.getMaxCacheLoadTime()) / delayMillis);
+
+        long keysLoaded = autoSavingCache.loadSaved();

Review comment:
       I can add it on commit too if you like, I just want to make sure that even in the error case we cleanup.

##########
File path: conf/cassandra.yaml
##########
@@ -286,7 +286,11 @@ counter_cache_save_period: 7200
 # If not set, the default directory is $CASSANDRA_HOME/data/saved_caches.
 # saved_caches_directory: /var/lib/cassandra/saved_caches
 
-# commitlog_sync may be either ""periodic"" or ""batch."" 
+# Max amount of time in seconds server has wait for cache to load while starting the Cassandra process.

Review comment:
       nit: copy edit
   
   ```
   Number of seconds the server will wait for each cache (row, key, etc ...) to load while starting
   the Cassandra process. Setting this to a negative value is equivalent to disabling all cache loading on
   startup while still having the cache during runtime.
   ```




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Dec/21 17:29;githubbot;600","nvharikrishna commented on a change in pull request #1290:
URL: https://github.com/apache/cassandra/pull/1290#discussion_r765072125



##########
File path: test/unit/org/apache/cassandra/db/KeyCacheTest.java
##########
@@ -231,6 +258,111 @@ public void testKeyCache() throws ExecutionException, InterruptedException
         assertKeyCacheSize(noEarlyOpen ? 4 : 2, KEYSPACE1, COLUMN_FAMILY1);
     }
 
+    @Test
+    public void testKeyCacheLoadNegativeCacheLoadTime() throws Exception
+    {
+        DatabaseDescriptor.setMaxCacheLoadTime(-1);
+        String cf = COLUMN_FAMILY7;
+
+        createAndInvalidateCache(Collections.singletonList(Pair.create(KEYSPACE1, cf)), 100);
+
+        CacheService.instance.keyCache.loadSaved();
+
+        // Here max time to load cache is negative which means no time left to load cache. So the keyCache size should
+        // be zero after loadSaved().
+        assertKeyCacheSize(0, KEYSPACE1, cf);
+        assertEquals(0, CacheService.instance.keyCache.size());
+    }
+
+    @Test
+    public void testKeyCacheLoadTwoTablesTime() throws Exception
+    {
+        DatabaseDescriptor.setMaxCacheLoadTime(60);
+        String columnFamily1 = COLUMN_FAMILY8;
+        String columnFamily2 = COLUMN_FAMILY_K2_1;
+        int numberOfRows = 100;
+        List<Pair<String, String>> tables = new ArrayList<>(2);
+        tables.add(Pair.create(KEYSPACE1, columnFamily1));
+        tables.add(Pair.create(KEYSPACE2, columnFamily2));
+
+        createAndInvalidateCache(tables, numberOfRows);
+
+        CacheService.instance.keyCache.loadSaved();
+
+        // Here max time to load cache is negative which means no time left to load cache. So the keyCache size should
+        // be zero after load.
+        assertKeyCacheSize(numberOfRows, KEYSPACE1, columnFamily1);
+        assertKeyCacheSize(numberOfRows, KEYSPACE2, columnFamily2);
+        assertEquals(numberOfRows * tables.size(), CacheService.instance.keyCache.size());
+    }
+
+    @SuppressWarnings({ ""unchecked"", ""rawtypes"" })
+    @Test
+    public void testKeyCacheLoadCacheLoadTimeExceedingLimit() throws Exception
+    {
+        DatabaseDescriptor.setMaxCacheLoadTime(2);
+        int delayMillis = 1000;
+        int numberOfRows = 100;
+
+        String cf = COLUMN_FAMILY9;
+
+        createAndInvalidateCache(Collections.singletonList(Pair.create(KEYSPACE1, cf)), numberOfRows);
+
+        // Testing cache load. Here using custom built AutoSavingCache instance as simulating delay is not possible with
+        // 'CacheService.instance.keyCache'. 'AutoSavingCache.loadSaved()' is returning no.of entries loaded so we don't need
+        // to instantiate ICache.class.
+        CacheService.KeyCacheSerializer keyCacheSerializer = new CacheService.KeyCacheSerializer();
+        CacheService.KeyCacheSerializer keyCacheSerializerSpy = Mockito.spy(keyCacheSerializer);
+        AutoSavingCache autoSavingCache = new AutoSavingCache(mock(ICache.class),
+                                                              CacheService.CacheType.KEY_CACHE,
+                                                              keyCacheSerializerSpy);
+
+        doAnswer(new AnswersWithDelay(delayMillis, answer -> keyCacheSerializer.deserialize(answer.getArgument(0),
+                                                                                            answer.getArgument(1)) ))
+               .when(keyCacheSerializerSpy).deserialize(any(DataInputPlus.class), any(ColumnFamilyStore.class));
+
+        long maxExpectedKeyCache = Math.min(numberOfRows,
+                                            1 + TimeUnit.SECONDS.toMillis(DatabaseDescriptor.getMaxCacheLoadTime()) / delayMillis);
+
+        long keysLoaded = autoSavingCache.loadSaved();

Review comment:
       done.

##########
File path: conf/cassandra.yaml
##########
@@ -286,7 +286,11 @@ counter_cache_save_period: 7200
 # If not set, the default directory is $CASSANDRA_HOME/data/saved_caches.
 # saved_caches_directory: /var/lib/cassandra/saved_caches
 
-# commitlog_sync may be either ""periodic"" or ""batch."" 
+# Max amount of time in seconds server has wait for cache to load while starting the Cassandra process.

Review comment:
       This one is sounding better. Updated.

##########
File path: conf/cassandra.yaml
##########
@@ -286,7 +286,11 @@ counter_cache_save_period: 7200
 # If not set, the default directory is $CASSANDRA_HOME/data/saved_caches.
 # saved_caches_directory: /var/lib/cassandra/saved_caches
 
-# commitlog_sync may be either ""periodic"" or ""batch."" 
+# Max amount of time in seconds server has wait for cache to load while starting the Cassandra process.
+# Cache will not load (during startup) if this value is negative as there is no time left to load.
+# max_cache_load_time: 30

Review comment:
       I am fine with both names. Picking the shorter one 'cache_load_timeout_seconds'.




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Dec/21 19:01;githubbot;600","smiklosovic closed pull request #1287:
URL: https://github.com/apache/cassandra/pull/1287


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 08:49;githubbot;600","smiklosovic closed pull request #1288:
URL: https://github.com/apache/cassandra/pull/1288


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 08:49;githubbot;600","smiklosovic closed pull request #1290:
URL: https://github.com/apache/cassandra/pull/1290


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 08:53;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,5400,,,0,5400,,,,,,,CASSANDRA-16380,,,,,,,,,,,,,,,,,,,,"16/Nov/18 05:37;jolynch;key_cache_load_slow.svg;https://issues.apache.org/jira/secure/attachment/12948454/key_cache_load_slow.svg",,,,,,,,,,,,,1.0,n.v.harikrishna,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 09 15:51:38 UTC 2021,,,,,,,,,,,"0|s00kf4:",9223372036854775807,3.0.x,,,,,,,,,,jolynch,marcuse,,Low,,3.0.0,,,https://github.com/apache/cassandra/commit/97b47c3b5f845097181130125752bd6efc1e1e47,,,,,,,,,"Pre-commit tests on 3.0, 3.11, 4.0 and trunk.

Included microbench shows reduction from ~140ms / to 20ms / load in keycache loading with 1000 sstables.",,,,,"16/Nov/18 06:07;djoshi;Interesting find. Have you tried pre-sizing the Map?;;;","16/Nov/18 06:31;jolynch;Yup, pre-sizing the map should help some (~25% faster most likely), but I'm currently looking into how hard it would be to just evaluate the {{getSSTables}} call once and pass in a cached {{generation}} -> {{SSTableReader}} map to each de-serialize call. I can either add a lazily instantiated {{Map<Integer SSTableReader> sstableGenerationToReader}} map to the {{KeyCacheSerializer}} static class or I can add it to the {{CacheSerializer}} interface (even though counter nor row would use it). I think the first one is maybe cleaner code but it does involve stateful serializers which is pretty meh as we have to somehow cleanup the HashMap.

I'm playing around a bit on how to test this change as well, the existing {{AutoSavingCacheTest}} unit test is somewhat minimal from what I can tell, and I think for something like this a microbenchmark that creates like 10k sstables and just dumps the cache and then loads it a bunch might be a better way to test it.;;;","20/Nov/18 16:10;jolynch;Started exploring caching the {{getSSTables}} call in [a 3.0 branch|https://github.com/apache/cassandra/compare/cassandra-3.0...jolynch:CASSANDRA-14898_30x]. The strategy I ended up using was to add an optional {{cleanupAfterDeserialize}} method to the {{CacheSerializer}} interface that cleans up any cached state after deserializations because I felt it was the least invasive change that allowed us to fix this problem. I also considered splitting the {{CacheSerializer}} interface into a {{CacheSerializer}} and {{CacheDeserializer}} class since that way the Deserializers wouldn't have to be static and could just throw away their state naturally at the end of a deserialization (since we only use them once afaik at startup) but I felt it was a more invasive change.

I also wrote a quick [microbenchmark|https://github.com/apache/cassandra/compare/cassandra-3.0...jolynch:CASSANDRA-14898_30x#diff-51c5d5cb6842a1aedbff457ba77dd424] that creates 1000 small sstables and then does a keycache load and as expected my branch is about two orders of magnitude faster. I'd expect it to reduce by the number of sstables which it did so that's good. Benchmark results:

status quo:
{noformat}
Benchmark                                                     Mode  Cnt    Score    Error  Units
CacheLoaderBench.keyCacheLoadTest                           sample   25  167.458 ± 24.430  ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p0.00    sample       125.436           ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p0.50    sample       164.364           ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p0.90    sample       216.583           ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p0.95    sample       231.106           ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p0.99    sample       236.454           ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p0.999   sample       236.454           ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p0.9999  sample       236.454           ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p1.00    sample       236.454           ms/op
{noformat}
Pre-allocating the Hashmap:
{noformat}
Benchmark                                                     Mode  Cnt    Score    Error  Units
CacheLoaderBench.keyCacheLoadTest                           sample   28  149.801 ± 11.483  ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p0.00    sample       114.426           ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p0.50    sample       147.980           ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p0.90    sample       168.401           ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p0.95    sample       181.941           ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p0.99    sample       190.317           ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p0.999   sample       190.317           ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p0.9999  sample       190.317           ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p1.00    sample       190.317           ms/op
{noformat}
Caching the entire {{getSSTables}} call and using a generation map for O(1) lookup:
{noformat}
Benchmark                                                     Mode   Cnt   Score   Error  Units
CacheLoaderBench.keyCacheLoadTest                           sample  1101   3.190 ± 0.108  ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p0.00    sample         1.876          ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p0.50    sample         2.863          ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p0.90    sample         4.615          ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p0.95    sample         5.161          ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p0.99    sample         6.750          ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p0.999   sample        14.178          ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p0.9999  sample        14.205          ms/op
CacheLoaderBench.keyCacheLoadTest:keyCacheLoadTest·p1.00    sample        14.205          ms/op
{noformat}
So this would seem to indicate that the production traces are correct and the {{O\(n\)}} traversal of the {{getSSTables}} is the dominating factor.;;;","15/Jan/21 16:15;n.v.harikrishna;[~jolynch] can I pick this ticket? I’m working on preparing a patch for this.;;;","15/Jan/21 16:26;jolynch;Of course! I would be happy to review and merge the patch as well.;;;","18/Jan/21 06:49;n.v.harikrishna;Changes are here: 

[https://github.com/nvharikrishna/cassandra/commit/c090ed640440b284686131ac73b3a6295cea27c0]

Circle CI: [https://app.circleci.com/pipelines/github/nvharikrishna/cassandra?branch=14898-keycache-performance-fix]

 

Tested locally in my laptop with 5000+ SSTables and these are the results observed:

Before:

 
{code:java}
Test 1:
INFO [pool-3-thread-1] 2021-01-07 15:40:24,767 AutoSavingCache.java:177 - Completed loading (148997 ms; 589886 keys) KeyCache cache
Test 2:
INFO [pool-3-thread-1] 2021-01-07 16:55:23,327 AutoSavingCache.java:177 - Completed loading (99441 ms; 396405 keys) KeyCache cache
{code}
 

After:
{code:java}
INFO [pool-3-thread-1] 2021-01-08 17:54:11,402 AutoSavingCache.java:177 - Completed loading (8502 ms; 401358 keys) KeyCache cache{code}
 

 ;;;","18/Jan/21 08:06;n.v.harikrishna;[~jolynch] I took your micro bench test and added it my branch: [https://github.com/nvharikrishna/cassandra/commits/14898-keycache-performance-fix] . Ran the test in my laptop without and with my changes and here are the results:

 

Without fix:
{noformat}
[java] Result ""org.apache.cassandra.test.microbench.CacheLoaderBench.keyCacheLoadTest"":
 [java] N = 33
 [java] mean = 125.885 ?(99.9%) 5.067 ms/op
 [java]
 [java] Histogram, ms/op:
 [java] [110.000, 115.000) = 0
 [java] [115.000, 120.000) = 2
 [java] [120.000, 125.000) = 19
 [java] [125.000, 130.000) = 8
 [java] [130.000, 135.000) = 2
 [java] [135.000, 140.000) = 0
 [java] [140.000, 145.000) = 1
 [java] [145.000, 150.000) = 0
 [java] [150.000, 155.000) = 0
 [java] [155.000, 160.000) = 0
 [java] [160.000, 165.000) = 1
 [java]
 [java] Percentiles, ms/op:
 [java] p(0.0000) = 118.358 ms/op
 [java] p(50.0000) = 123.863 ms/op
 [java] p(90.0000) = 131.990 ms/op
 [java] p(95.0000) = 148.347 ms/op
 [java] p(99.0000) = 163.578 ms/op
 [java] p(99.9000) = 163.578 ms/op
 [java] p(99.9900) = 163.578 ms/op
 [java] p(99.9990) = 163.578 ms/op
 [java] p(99.9999) = 163.578 ms/op
 [java] p(100.0000) = 163.578 ms/op{noformat}
 

With fix:
{noformat}
[java] Result ""org.apache.cassandra.test.microbench.CacheLoaderBench.keyCacheLoadTest"":
 [java] N = 277
 [java] mean = 14.093 ?(99.9%) 0.530 ms/op
 [java]
 [java] Histogram, ms/op:
 [java] [10.000, 11.250) = 0
 [java] [11.250, 12.500) = 118
 [java] [12.500, 13.750) = 50
 [java] [13.750, 15.000) = 27
 [java] [15.000, 16.250) = 16
 [java] [16.250, 17.500) = 20
 [java] [17.500, 18.750) = 23
 [java] [18.750, 20.000) = 16
 [java] [20.000, 21.250) = 6
 [java] [21.250, 22.500) = 1
 [java] [22.500, 23.750) = 0
 [java] [23.750, 25.000) = 0
 [java] [25.000, 26.250) = 0
 [java] [26.250, 27.500) = 0
 [java] [27.500, 28.750) = 0
 [java]
 [java] Percentiles, ms/op:
 [java] p(0.0000) = 11.289 ms/op
 [java] p(50.0000) = 13.238 ms/op
 [java] p(90.0000) = 18.468 ms/op
 [java] p(95.0000) = 19.218 ms/op
 [java] p(99.0000) = 20.588 ms/op
 [java] p(99.9000) = 22.053 ms/op
 [java] p(99.9900) = 22.053 ms/op
 [java] p(99.9990) = 22.053 ms/op
 [java] p(99.9999) = 22.053 ms/op
 [java] p(100.0000) = 22.053 ms/op{noformat}
 ;;;","18/Jan/21 15:47;jolynch;[~n.v.harikrishna] thank you for the patch! I just took a quick look at it (it's a holiday here but I will review more closely tomorrow), but is the gist of the patch that we skip copying the sstables into a new hash set (by just pushing the filter down to the View) but still perform a {{O( n )}} scan over those tables just within the View?

I think the API that's been introduced into View isn't problematic in that it seems generic and useful, but I'm curious is there a reason you preferred that to the generation cache approach in the initial proof of concept patch? I guess the tradeoff is making the CacheService implementations potentially stateful (introducing a new contract that AutoSavingCache will call a function at the end)?

 

 

 ;;;","19/Jan/21 18:03;n.v.harikrishna;[~jolynch] 
{quote}is the gist of the patch that we skip copying the sstables into a new hash set (by just pushing the filter down to the View) but still perform a {{O(  n  )}} scan over those tables just within the View?
{quote}
Yes, wanted to avoid copying lots of SSTables to hash set when we need only one. View's _find_ method don't do O( n ) scans all the times. In a normal case, find method stops at the fist entry (all SSTables generation would be same). I think it will require ( O( n ) scans for entry) in the case of upgrade and required generation SSTable is at end of the list. Added 60s for _get_ to avoid waiting forever.
{quote}I guess the tradeoff is making the CacheService implementations potentially stateful (introducing a new contract that AutoSavingCache will call a function at the end)?
{quote}
Thought of avoiding state across calls. _CacheService deserialize_ method is called for each entry. Things would have been easier if _deserialize_ method returns all keys instead of single key value pair (no need to maintain state across calls).;;;","03/Feb/21 07:30;n.v.harikrishna;[~jolynch] Taking a step back and re-looking at my patch. Give me sometime.;;;","06/Mar/21 11:32;n.v.harikrishna;[~jolynch] sorry, took long time to get back on this. I took your patch and tried it in my local env (with even more SStables: around 11k+) and it performed better than my patch. It loaded 400k+ keys with in a second where as my code changes took 8 to 9 seconds. It is worth maintaining state for this performance gain. I am fine with dropping my patch. Though I made a small change on top of your patch which is similar to my patch. Replaced find method with generic collect method so that copying SSTables into a new map can be avoided ([https://github.com/nvharikrishna/cassandra/commit/b26b5f17877b5d89698840e42a3c77a6629594f5]). Probably splitting the CacheSerializer is an alternative (which you mentioned earlier) if not this solution.

I have also tried to time bound the key and row cache loading by calling get method with specific time and cancelling it (may leave key cache entries in invalid state if not cancelled when compaction starts) if it couldn't finish. Cancelling it lead to serious problem. Cancelling the task interrupted the deserializer -> DataInputStream -> ChannelProxy which is throwing  _java.nio.channels.ClosedByInterruptException_ as _org.apache.cassandra.io.FSReadError_. FSReadError is treated as disk failure and instance is getting stopped as per disk failure policy. Pasting the stack trace here for reference. After looking at the performance improvement, I am not sure if it is worth digging down the path of modifying ChannelProxy to handle this case (not treating java.nio.channels.ClosedByInterruptException as error). So reverted the changes.
{code:java}
WARN [main] 2021-03-06 12:58:29,926 CassandraDaemon.java:346 - Cache did not load in given time. Cancelled loading of key and row cache. isCancelled: true
ERROR [pool-3-thread-1] 2021-03-06 12:58:29,928 DefaultFSErrorHandler.java:104 - Exiting forcefully due to file system exception on startup, disk failure policy ""stop""
org.apache.cassandra.io.FSReadError: java.nio.channels.ClosedByInterruptException
 at org.apache.cassandra.io.util.ChannelProxy.read(ChannelProxy.java:143)
 at org.apache.cassandra.io.util.SimpleChunkReader.readChunk(SimpleChunkReader.java:41)
 at org.apache.cassandra.io.util.ChecksummedRebufferer.rebuffer(ChecksummedRebufferer.java:45)
 at org.apache.cassandra.io.util.RandomAccessReader.reBufferAt(RandomAccessReader.java:65)
 at org.apache.cassandra.io.util.RandomAccessReader.reBuffer(RandomAccessReader.java:59)
 at org.apache.cassandra.io.util.RebufferingInputStream.read(RebufferingInputStream.java:90)
 at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
 at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
 at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
 at org.apache.cassandra.io.util.LengthAvailableInputStream.read(LengthAvailableInputStream.java:57)
 at java.io.DataInputStream.readFully(DataInputStream.java:195)
 at java.io.DataInputStream.readFully(DataInputStream.java:169)
 at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:433)
 at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:453)
 at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:227)
 at org.apache.cassandra.cache.AutoSavingCache$3.call(AutoSavingCache.java:168)
 at org.apache.cassandra.cache.AutoSavingCache$3.call(AutoSavingCache.java:164)
 at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:125)
 at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:57)
 at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:78)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.channels.ClosedByInterruptException: null
 at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
 at sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:740)
 at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:721)
 at org.apache.cassandra.io.util.ChannelProxy.read(ChannelProxy.java:139)
 ... 22 common frames omitted{code}

;;;","20/Sep/21 14:13;jolynch;[~n.v.harikrishna] Thank you for the patch! Would you like to get this into 3.0/3.11/4.0 and trunk? If so I think we need patches against each branch (the metadata variable names have changed slightly so I think separate patches might be easiest rather than one patch that we merge up); I am happy to do the minor changes on each branch if you like as well.

Review of patch:
* I think introducing a little bit of state here is wise, if we want to be extra careful do you think we could use [AutoCloseable|https://docs.oracle.com/javase/tutorial/essential/exceptions/tryResourceClose.html] somehow to ensure the state is cleaned up? Seems reasonable for {{close}} to just cleanup the state used during load or dump.
* Did you mean to remove RowIndexEntry.Serializer.skipForCache(input); on line 453 ?
* Do you think we could improve the tests in AutoSavingCacheTest to have a case where a few sstables are created across different ks/table and we want to confirm the key cache is loaded properly with more than one keyspace/table?
* Regarding the deadline approach that is an interesting idea and I like it, I'd recommend you implement it in the while loop in AutoSavingCache.loadSaved (the while in.available loop) instead of by cancelling the future at the high level. ;;;","21/Sep/21 17:51;n.v.harikrishna;[~jolynch] Thanks for the review!
{quote}Would you like to get this into 3.0/3.11/4.0 and trunk? 
{quote}
I will create three different patches
{quote}Did you mean to remove RowIndexEntry.Serializer.skipForCache(input); on line 453 ?
{quote}
No, looks like some mistake happened from my side before pushing the changes. Will revert it. Thanks for pointing it out.
{quote}If we want to be extra careful do you think we could use [AutoCloseable|https://docs.oracle.com/javase/tutorial/essential/exceptions/tryResourceClose.html] somehow to ensure the state is cleaned up?

Do you think we could improve the tests in AutoSavingCacheTest...

Regarding the deadline approach ... implement it in the while loop in AutoSavingCache.loadSaved ...
{quote}
I will make these changes and upload patches as soon as I can.

 ;;;","22/Oct/21 19:10;n.v.harikrishna;[~jolynch] I had updated the patch for trunk & here is the pr: [https://github.com/apache/cassandra/pull/1287]  & CI is here: https://app.circleci.com/pipelines/github/nvharikrishna/cassandra/41/workflows/7fb33758-2a6d-464a-bcc5-8b5709d1d997
 # I did not made changes to implement AutoCloseable as closing the resource generally gives the impression that resource it not usable any more but CacheSerializer later used for serialising too. Using normal method to clear up the map.
 # Added few tests to KeyCacheTest as it has more tests related to key cache.
 # Added limit on cache load time which is configurable through max_cache_load_time param (defaults to 30 sec).

Sorry for the delay. Would appreciate your review on the code changes. Please give me a day or two for making changes to other branches.

 

 ;;;","26/Oct/21 17:39;n.v.harikrishna;Here are the changes: 

|[trunk|https://github.com/apache/cassandra/pull/1287]|[CI|https://app.circleci.com/pipelines/github/nvharikrishna/cassandra/41/workflows/7fb33758-2a6d-464a-bcc5-8b5709d1d997]|

|[3.11|https://github.com/apache/cassandra/pull/1288]|[CI|https://app.circleci.com/pipelines/github/nvharikrishna/cassandra/42/workflows/e7cdd28d-9d2e-4445-9157-e2b68ca0ef47]|

|[3.0|https://github.com/apache/cassandra/pull/1290]|[CI|https://app.circleci.com/pipelines/github/nvharikrishna/cassandra/46/workflows/a2baf042-657d-44c2-8890-b52215f9f0cf]|

 ;;;","26/Oct/21 17:54;jolynch;Thank you for the patches, will review asap!;;;","06/Dec/21 13:41;marcuse;this lgtm, just a few minor comments on the 3.0 PR above (but the comments apply on all branches);;;","06/Dec/21 15:30;n.v.harikrishna;Thanks for the review [~marcuse] ! Taking a look.;;;","06/Dec/21 16:46;jolynch;Also lg2m, let's iron out the configuration name since that's a public interface but I think the rest of the feedback is straightforward.

in-jvm dtest flakes seem unrelated to me;;;","08/Dec/21 17:38;jolynch;+1 . Left a few minor suggestions on the 3.0 PR (apply to other branches as well).

When you're ready please rebase to a single commit for each branch with a commit that looks something like
{noformat}
$ git log -1
commit ...
Author: Venkata Harikrishna Nukala <n.v.harikrishna.apache@gmail.com>
Date: ...

Fix slow keycache load which blocks startup for tables with many sstables. 

Patch by Venkata Harikrishna Nukala; reviewed by Marcus Eriksson and Joseph Lynch for CASSANDRA-14898 {noformat}
Important part is your name is attributed as author (if you want it to be), and we note the patch author, reviewers and ticket in the last line.;;;","08/Dec/21 19:03;n.v.harikrishna;[~jolynch]  [~marcuse]  Addressed review comments, squashed the commits for each branch. ;;;","08/Dec/21 19:25;marcuse;+1
;;;","09/Dec/21 04:51;jolynch;Cherry picked the trunk branch back to 4.0 and applied a fixup or two to get [ae5954b1|https://github.com/jolynch/cassandra/commit/ae5954b1c513337484c43b575f09a0229464a33e].  I've kicked off circleci precommit runs for 3.0, 3.11, 4.0 and trunk. If those come back green will commit.;;;","09/Dec/21 15:02;jolynch;*trunk* - [Java 11 CI|https://app.circleci.com/pipelines/github/jolynch/cassandra/67/workflows/6a98bc38-3021-4763-906b-f84099157ba0], [Java 8 CI|https://app.circleci.com/pipelines/github/jolynch/cassandra/67/workflows/06b843ac-a0f9-4123-b14a-965dc8f22755]

4 dtest failures in:
 * (J8) test_bootstrap_with_reset_bootstrap_state - bootstrap_test.TestBootstrap
 * (J8) test_compression_cql_options - compression_test.TestCompression
 * (J11) test_multiple_repair - repair_tests.incremental_repair_test.TestIncRepair
 * (J11, marked flakey) test_bootstrap_with_reset_bootstrap_state - bootstrap_test.TestBootstrap

1 JVM dtest failure:
 * (J8): readWriteDuringBootstrapTest - org.apache.cassandra.distributed.test.ring.BootstrapTest

*4.0* - [Java 11 CI|https://app.circleci.com/pipelines/github/jolynch/cassandra/68/workflows/2fe689eb-1938-42fd-b5cb-abab32cf0d1f] [Java 8 CI|https://app.circleci.com/pipelines/github/jolynch/cassandra/68/workflows/7929a6c9-cc9c-4c5e-b271-cc276fc1d9a1]

1 JVM dtest failure:
 * (J8, J11) readWriteDuringBootstrapTest - org.apache.cassandra.distributed.test.ring.BootstrapTest

*3.11* [Java 8 CI|https://app.circleci.com/pipelines/github/jolynch/cassandra/65/workflows/6e86d386-4057-4d83-aab1-4d3731023f5d]

1 JVM dtest failure:
 * (J8) bootstrapTest - org.apache.cassandra.distributed.test.BootstrapTest

*3.0* [Java 8 CI|https://app.circleci.com/pipelines/github/jolynch/cassandra/64/workflows/5f7095bf-5031-465f-9d82-d574991e21ac]

1 JVM dtest failure:
 * (J8) bootstrapTest - org.apache.cassandra.distributed.test.BootstrapTest

Looking at the tests that failed I don't think any of these look related to the change. A local run of readWriteDuringBootstrapTest passes on 4.0 and bootstrapTest passed on 3.0, plus I'm running a trunk [CI|https://app.circleci.com/pipelines/github/jolynch/cassandra/69/workflows/58cdf28a-8e13-4920-94a2-870d0fe00790] run to see if readWriteDuringBootstrapTest flakes on trunk as well without the patch.

I believe this is ready to commit.;;;","09/Dec/21 15:51;jolynch;Committed to 3.0, 3.11, 4.0 and trunk. Thank you [~n.v.harikrishna] for your contribution!

 
trunk: [97b47c3b|https://github.com/apache/cassandra/commit/97b47c3b5f845097181130125752bd6efc1e1e47]
4.0: [e73d05bf|https://github.com/apache/cassandra/commit/e73d05bf858fa195ac2f2778027ef0d2ebcd3abd]
3.11: [1fce84f9|https://github.com/apache/cassandra/commit/1fce84f9833bd62227dbf8f5d063935457dbc18e]
3.0: [1911a887|https://github.com/apache/cassandra/commit/1911a887e8316d343c9bfe3aca3f9d143e9f4a61];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
In mixed 3.x/4 version clusters write tracing and repair history information without new columns,CASSANDRA-14897,13198748,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aweisberg,aweisberg,aweisberg,15/Nov/18 21:48,15/May/20 08:06,13/Jul/23 08:37,29/Nov/18 18:22,4.0,4.0-alpha1,,,,,Legacy/Distributed Metadata,,,,0,4.0-pre-rc-bugs,,,"In CASSANDRA-14841 I stopped it from writing to those tables so it wouldn't generate any errors. Aleksey pointed out I could write just the old columns. 

If a user manually adds the new columns to the old version nodes before upgrade they will be able to query this information across the cluster. This is a better situation then making it completely impossible for people to run repairs or perform tracing in mixed version clusters.

This would avoid breaking repair and tracing in mixed version clusters.

The missing columns can be added by following [these instructions|https://issues.apache.org/jira/browse/CASSANDRA-14841?focusedCommentId=16684959&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16684959]
For 3.0.15 and 3.11.1 and older versions:
{noformat}
cqlsh> ALTER TABLE system_distributed.repair_history ADD coordinator_port int;
cqlsh> ALTER TABLE system_distributed.repair_history ADD participants_v2 set<text>;
{noformat}
For 3.0.16 and 3.11.2 and newer:
{noformat}
cqlsh> INSERT INTO system_schema.columns (keyspace_name , table_name , column_name , clustering_order , column_name_bytes , kind , position , type ) VALUES ( 'system_distributed', 'repair_history', 'coordinator_port', 'none', 0x636f6f7264696e61746f725f706f7274, 'regular', -1, 'int');
cqlsh> INSERT INTO system_schema.columns (keyspace_name , table_name , column_name , clustering_order , column_name_bytes , kind , position , type ) VALUES ( 'system_distributed', 'repair_history', 'participants_v2', 'none', 0x7061727469636970616e74735f7632, 'regular', -1, 'set<text>');
cqlsh> exit
$ nodetool reloadlocalschema
{noformat}
Remember that the INSERT's and nodetool reloadschema must be done on the same node.

",,aweisberg,jasobrown,jeromatron,tommy_s,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14841,,,,,,,,,,,,,,,"15/Nov/18 21:50;aweisberg;14897.diff;https://issues.apache.org/jira/secure/attachment/12948396/14897.diff",,,,,,,,,,,,,1.0,aweisberg,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 29 18:18:34 UTC 2018,,,,,,,,,,,"0|s00jy0:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"28/Nov/18 23:46;aweisberg;[Trunk code|https://github.com/apache/cassandra/compare/trunk...aweisberg:14897-trunk?expand=1], [CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/14897-trunk];;;","29/Nov/18 14:01;jasobrown;+1 lgtm;;;","29/Nov/18 18:18;aweisberg;Committed as [1c8d0ad333c642405537150fed2cbb8623a8fe94|https://github.com/apache/cassandra/commit/1c8d0ad333c642405537150fed2cbb8623a8fe94]. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3.0 schema migration pulls from later version incompatible nodes,CASSANDRA-14896,13198747,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jasobrown,aweisberg,aweisberg,15/Nov/18 21:45,15/May/20 08:06,13/Jul/23 08:37,30/Nov/18 21:18,4.0,4.0-alpha1,,,,,Legacy/Core,Legacy/CQL,,,0,4.0-pre-rc-bugs,,,"I saw this in upgrade tests. The checks in 3.0 and 3.11 are slightly different and 3.0 in some scenarios it is pulling schema from a later version. This causes upgrade tests to have errors in the logs due to additional columns from configurable storage port.

{noformat}
Failed: Error details: 
Errors seen in logs for: node2
node2: ERROR [MessagingService-Incoming-/127.0.0.1] 2018-11-15 21:17:46,739 CassandraDaemon.java:207 - Exception in thread Thread[MessagingService-Incoming-/127.0.0.1,5,main]
java.lang.RuntimeException: Unknown column additional_write_policy during deserialization
	at org.apache.cassandra.db.Columns$Serializer.deserialize(Columns.java:433) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.db.SerializationHeader$Serializer.deserializeForMessaging(SerializationHeader.java:440) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.deserializeHeader(UnfilteredRowIteratorSerializer.java:190) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize30(PartitionUpdate.java:686) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize(PartitionUpdate.java:674) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:337) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:346) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.service.MigrationManager$MigrationsSerializer.deserialize(MigrationManager.java:641) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.service.MigrationManager$MigrationsSerializer.deserialize(MigrationManager.java:624) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:98) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:201) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:178) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:92) ~[apache-cassandra-3.0.17.jar:3.0.17]
{noformat}",,aweisberg,jasobrown,jeromatron,tommy_s,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 30 21:18:49 UTC 2018,,,,,,,,,,,"0|s00jxs:",9223372036854775807,,,,,,,,,aweisberg,,aweisberg,,,Critical,,,,,,,,,,,,,,,,,,,"28/Nov/18 19:38;aweisberg;4.0 is providing the wrong max version when connecting to other nodes which causes them to think that 4.0 nodes are older version nodes.;;;","28/Nov/18 22:56;jasobrown;[~aweisberg] is correct. On the third (and last) message of the internode messaging handshake, the node is [incorrectly sending back|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/async/OutboundHandshakeHandler.java#L180] the messaging version is received from the peer; it should be sending back it's own {{MessagingService.current_version}}.

Here's a one-line fix for sending the correct messaging version in {{ThirdHandshakeMessage}} as well as fixing the unit test that ensures the version being sent from {{OutboundHandshakeHandler}}
||14896||
|[branch|https://github.com/jasobrown/cassandra/tree/14896]|
|[utests & dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14896]|;;;","28/Nov/18 23:00;aweisberg;+1 to that fix. I've tested it and it worked for me.;;;","28/Nov/18 23:15;jasobrown;The only utest that failed was {{DistributedReadWritePathTest.writeWithSchemaDisagreement}}, which failed with ""Forked Java VM exited abnormally"". I ran locally and all was fine, so chalking it up to a testing fluke. Will commit shortly.;;;","28/Nov/18 23:26;jasobrown;Committed as sha \{{c5dee08dfb791ba28fecc8ca8b25a4a4d7e9cb07}};;;","29/Nov/18 14:53;tommy_s;I'm not sure why but this seams to course some problems when upgrading 3.x->4.0 with server encryption enabled. The 4.0 node can't connect to any 3.x node, if I build 4.0 without this patch I get the issue reported in CASSANDRA-14848 and the 4.0 node connects to one of the 3.x nodes. My patch for CASSANDRA-14848 does not help so I'm not sure what the problem is.;;;","29/Nov/18 16:40;aweisberg;[~tommy_s] unfortunate this didn't help. 

I will probably get to the SSL issue soon if [~jasobrown] doesn't just because the upgrade tests have SSL tests so I will hit it there.;;;","29/Nov/18 19:50;aweisberg;I +1ed the wrong fix :-(

The version in the constructor needs to be the peer's version. It's [this line here|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/async/HandshakeProtocol.java#L248] that needs to specify the current version.;;;","29/Nov/18 22:21;jasobrown;The problem with my first patch is that we need the peer's messaging version in order serialize the {{InetaddressAndPort}} correctly to the peer. We still need to write the local node's messaging version into the message, however.

Patch here:

||v2||
|[branch|https://github.com/jasobrown/cassandra/tree/14896-v2]|
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14896-v2]|
||;;;","29/Nov/18 22:29;aweisberg;The comment on writing the max version to the message should specify that it's the maximum supported version of this node. As opposed to the supported to the version it's using to connect.

Other then that +1;;;","30/Nov/18 21:18;jasobrown;Committed v2 patch as {{f3609995c09570d523527d9bd0fd69c2bc65d986}} with updated comments per [~aweisberg]'s recommendation.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTable loader exception when loading 3.0/3.11 compact tables into 4.0,CASSANDRA-14895,13198746,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,djoshi,aweisberg,aweisberg,15/Nov/18 21:39,15/May/20 08:04,13/Jul/23 08:37,15/Jan/19 16:59,4.0,4.0-alpha1,,,,,Legacy/Tools,,,,0,4.0-pre-rc-bugs,,,"While working on the upgrade tests I added 3.0/3.11 to current tests for loading old version sstables using sstable loader. [The tests for loading compact sstables fail.|addedhttps://github.com/apache/cassandra-dtest/blob/master/upgrade_tests/storage_engine_upgrade_test.py#L466]

It doesn't help to alter the table to drop compact storage and then run rebuild and cleanup before attempting to load into current.

Failed to list files in /var/folders/vx/2fcrbbx12g9bppxk7h41ww700000gn/T/dtest-4_4vb5jj/test/node1/data1_copy/ks/counter1-f4dc7fc0e91011e892e9c3e97b26557e
java.lang.RuntimeException: Unknown column value during deserialization
java.lang.RuntimeException: Failed to list files in /var/folders/vx/2fcrbbx12g9bppxk7h41ww700000gn/T/dtest-4_4vb5jj/test/node1/data1_copy/ks/counter1-f4dc7fc0e91011e892e9c3e97b26557e
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.list(LogAwareFileLister.java:77)
	at org.apache.cassandra.db.lifecycle.LifecycleTransaction.getFiles(LifecycleTransaction.java:561)
	at org.apache.cassandra.io.sstable.SSTableLoader.openSSTables(SSTableLoader.java:76)
	at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:166)
	at org.apache.cassandra.tools.BulkLoader.load(BulkLoader.java:83)
	at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:49)
Caused by: java.lang.RuntimeException: Unknown column value during deserialization
	at org.apache.cassandra.db.SerializationHeader$Component.toHeader(SerializationHeader.java:317)
	at org.apache.cassandra.io.sstable.format.SSTableReader.openForBatch(SSTableReader.java:440)
	at org.apache.cassandra.io.sstable.SSTableLoader.lambda$openSSTables$0(SSTableLoader.java:121)
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.lambda$innerList$2(LogAwareFileLister.java:99)
	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:174)
	at java.util.TreeMap$EntrySpliterator.forEachRemaining(TreeMap.java:2969)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.innerList(LogAwareFileLister.java:101)
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.list(LogAwareFileLister.java:73)
	... 5 more
Exception in thread ""main"" org.apache.cassandra.tools.BulkLoadException: java.lang.RuntimeException: Failed to list files in /var/folders/vx/2fcrbbx12g9bppxk7h41ww700000gn/T/dtest-4_4vb5jj/test/node1/data1_copy/ks/counter1-f4dc7fc0e91011e892e9c3e97b26557e
	at org.apache.cassandra.tools.BulkLoader.load(BulkLoader.java:96)
	at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:49)
Caused by: java.lang.RuntimeException: Failed to list files in /var/folders/vx/2fcrbbx12g9bppxk7h41ww700000gn/T/dtest-4_4vb5jj/test/node1/data1_copy/ks/counter1-f4dc7fc0e91011e892e9c3e97b26557e
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.list(LogAwareFileLister.java:77)
	at org.apache.cassandra.db.lifecycle.LifecycleTransaction.getFiles(LifecycleTransaction.java:561)
	at org.apache.cassandra.io.sstable.SSTableLoader.openSSTables(SSTableLoader.java:76)
	at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:166)
	at org.apache.cassandra.tools.BulkLoader.load(BulkLoader.java:83)
	... 1 more
Caused by: java.lang.RuntimeException: Unknown column value during deserialization
	at org.apache.cassandra.db.SerializationHeader$Component.toHeader(SerializationHeader.java:317)
	at org.apache.cassandra.io.sstable.format.SSTableReader.openForBatch(SSTableReader.java:440)
	at org.apache.cassandra.io.sstable.SSTableLoader.lambda$openSSTables$0(SSTableLoader.java:121)
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.lambda$innerList$2(LogAwareFileLister.java:99)
	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:174)
	at java.util.TreeMap$EntrySpliterator.forEachRemaining(TreeMap.java:2969)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.innerList(LogAwareFileLister.java:101)
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.list(LogAwareFileLister.java:73)
	... 5 more
",,aweisberg,cscotta,djoshi,jeromatron,tcooke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,djoshi,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 15 18:05:15 UTC 2019,,,,,,,,,,,"0|s00jxk:",9223372036854775807,,,,,,,,,aweisberg,,aweisberg,,,Critical,,,,,,,,,,,,,,,,,,,"02/Jan/19 18:32;aweisberg;So it doesn't get lost this is a test bug that is going to be fixed in CASSANDRA-14421;;;","10/Jan/19 22:22;aweisberg;This didn't get merged in with CASSANDRA-14421, I'll merge it now.;;;","10/Jan/19 22:22;aweisberg;https://github.com/aweisberg/cassandra-dtest/commit/c081a8cffd5a8a9e4a48284c2e13c8acd3a043eb;;;","15/Jan/19 02:59;aweisberg;Passes https://circleci.com/gh/aweisberg/cassandra/2494#tests/containers/;;;","15/Jan/19 06:49;djoshi;Thanks [~aweisberg]. Is there anything you need from me on this ticket?;;;","15/Jan/19 16:23;aweisberg;All good, going to commit this today. I just lost track of where I had run the tests last time so I am running them again. Took a while because it was both upgrade and dtests.;;;","15/Jan/19 16:59;aweisberg;Committed as [4c1479b5f457c3a8ed0302461ef79331cc13e798|https://github.com/apache/cassandra-dtest/commit/4c1479b5f457c3a8ed0302461ef79331cc13e798] to the dtests. Thanks!

Test results https://circleci.com/gh/aweisberg/cassandra/tree/14895-trunk;;;","15/Jan/19 18:05;djoshi;Thanks, [~aweisberg];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RangeTombstoneList doesn't properly clean up mergeable or superseded rts in some cases,CASSANDRA-14894,13198729,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,15/Nov/18 20:09,15/May/20 08:03,13/Jul/23 08:37,28/Nov/18 17:33,3.0.18,3.11.4,4.0,4.0-alpha1,,,Local/SSTable,,,,0,,,,"There are a few scenarios RangeTombstoneList doesn't handle correctly.

If there are 2 overlapping range tombstones with identical timestamps, they should be merged. Instead, they're stored as 2 rts with congruent bounds and identical timestamps.

If a range tombstone supersedes multiple sequential range tombstones, instead of removing them, they cause the superseding rt to be split into multiple rts with congruent bounds and identical timestamps.

When converted to an UnfilteredRowIterator, these become extra boundary markers with the same timestamp on each side. Logically these are noops, but they do cause digest mismatches which will cause unneeded read repairs, and repair overstreaming (since they're also included in flushed sstables).

Also, not sure if this is reachable in practice, but querying RTL with an empty slice that covers a range tombstone causes an rt to be returned with an empty slice. If reachable this might cause extra read repairs as well.",,bdeggleston,jay.zhuang,jeromatron,KurtG,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,Degradation -> Performance Bug/Regression,,,,,,,,Challenging,Fuzz Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 28 17:33:43 UTC 2018,,,,,,,,,,,"0|s00jts:",9223372036854775807,,,,,,,,,samt,,samt,,,Normal,,3.0.0,,,,,,,,,,,,,,,,,"26/Nov/18 23:58;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/14894-3.0]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14894-3.0]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/14894-3.11]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14894-3.11]|
|[trunk|https://github.com/bdeggleston/cassandra/tree/14894-trunk]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14894-trunk]|

I cheated and didn't actually fix RangeTombstoneList, but am just filtering them out in RowAndDeletionMergeIterator instead. Fixing RangeTombstoneList to correctly handle things was not at all trivial and would have risked introducing correctness bugs.;;;","28/Nov/18 14:46;samt;+1;;;","28/Nov/18 17:33;bdeggleston;thanks, committed as [f7630e4c3af3bbcf933f0708afaac7e3e7ef6101|https://github.com/apache/cassandra/commit/f7630e4c3af3bbcf933f0708afaac7e3e7ef6101];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible corruption in compressed files with uncompressed chunks,CASSANDRA-14892,13198559,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,blambov,blambov,blambov,15/Nov/18 09:02,15/May/20 08:06,13/Jul/23 08:37,29/Nov/18 14:29,4.0,4.0-alpha1,,,,,Legacy/Local Write-Read Paths,,,,0,,,,"When deciding to switch to writing a chunk uncompressed in a compressed file (see CASSANDRA-10520) and that chunk is smaller than the full chunk size (only currently happens in the last chunk of the file), it may very rarely happen that the chunk is:
- bigger than the compression limit when compressed
- smaller than the compression limit when left uncompressed

If this happens the writer will write it uncompressed, but the reader will treat it as compressed and fail when attempting to read it.

Such chunks should be padded with 0s to the minimum uncompressed size.",,aleksey,blambov,cscotta,jeromatron,snazy,stefania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,blambov,,,,,,,,,,,,Correctness -> Recoverable Corruption / Loss,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 29 14:28:35 UTC 2018,,,,,,,,,,,"0|s00is0:",9223372036854775807,,,,,,,,,snazy,,snazy,,,Normal,,,,,,,,,,,,,,,,,,,"29/Nov/18 09:29;blambov;As requested by reviewer off-line, added extra tests for writing and reading various input and compressed buffer sizes, as well as a unit test for {{ByteBufferUtil.writeZeroes}}.;;;","29/Nov/18 12:50;snazy;Not much to say here.

Just +1 :);;;","29/Nov/18 14:28;blambov;Got a clean dtest run at DataStax, and testall results match trunk. Committed as [9a7db292cc4e470cd913f5c850982a7d7300d6c8|https://github.com/apache/cassandra/commit/9a7db292cc4e470cd913f5c850982a7d7300d6c8].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix 14861 related test failures,CASSANDRA-14889,13198120,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,13/Nov/18 18:28,15/May/20 08:02,13/Jul/23 08:37,26/Nov/18 19:55,3.0.18,3.11.4,4.0,4.0-alpha1,,,Legacy/Testing,,,,0,,,,CASSANDRA-14861 broke a few tests unfortunately,,aleksey,bdeggleston,jay.zhuang,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14891,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 26 19:55:02 UTC 2018,,,,,,,,,,,"0|s00g2g:",9223372036854775807,,,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"26/Nov/18 19:55;bdeggleston;Turned out to be a simple one liner, so I just ninja'd as [60a8cfe115b78cee7e4d8024984fa1f8367685db |https://github.com/apache/cassandra/commit/60a8cfe115b78cee7e4d8024984fa1f8367685db];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Several mbeans are not unregistered when dropping a keyspace and table,CASSANDRA-14888,13198095,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,stillalex,aweisberg,aweisberg,13/Nov/18 16:42,16/Mar/22 13:38,13/Jul/23 08:37,23/Jun/20 17:35,4.0,4.0-beta1,,,,,Observability/Metrics,,,,0,patch-available,,,"CasCommit, CasPrepare, CasPropose, ReadRepairRequests, ShortReadProtectionRequests, AntiCompactionTime, BytesValidated, PartitionsValidated, RepairPrepareTime, RepairSyncTime, RepairedDataInconsistencies, ViewLockAcquireTime, ViewReadTime, WriteFailedIdealCL

Basically for 3 years people haven't known what they are doing because the entire thing is kind of obscure. Fix it and also add a dtest that detects if any mbeans are left behind after dropping a table and keyspace.",,abdulazizali,aweisberg,benedict,cnlwsu,cscotta,djoshi,jasonstack,jeromatron,maedhroz,shaurya10000,sickcate,stillalex,,,,,,,,,,,,,,"stillalex opened a new pull request #629:
URL: https://github.com/apache/cassandra/pull/629


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jun/20 01:00;githubbot;600","maedhroz commented on a change in pull request #629:
URL: https://github.com/apache/cassandra/pull/629#discussion_r439180287



##########
File path: test/unit/org/apache/cassandra/metrics/ViewWriteMetricsTest.java
##########
@@ -0,0 +1,85 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.metrics;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+import java.util.function.LongSupplier;
+
+import org.apache.cassandra.SchemaLoader;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.schema.Schema;
+import org.apache.cassandra.service.EmbeddedCassandraService;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import com.datastax.driver.core.Cluster;
+import com.datastax.driver.core.Session;
+
+public class ViewWriteMetricsTest extends SchemaLoader
+{
+    private static Session session;
+
+    @BeforeClass()
+    public static void setup() throws ConfigurationException, IOException
+    {
+        Schema.instance.clear();
+
+        EmbeddedCassandraService cassandra = new EmbeddedCassandraService();
+        cassandra.start();
+
+        Cluster cluster = Cluster.builder().addContactPoint(""127.0.0.1"").withPort(DatabaseDescriptor.getNativeTransportPort()).build();
+        session = cluster.connect();
+    }
+
+    @Test
+    public void testMetricsCleanupOnDrop()
+    {
+        String keyspace = ""junit"";
+        String table = ""viewwritemetrics"";
+        String view = ""viewwritemetrics_materialized_view"";
+
+        CassandraMetricsRegistry registry = CassandraMetricsRegistry.Metrics;
+        LongSupplier count = () -> registry.getNames().stream().filter(n -> n.contains(view)).count();
+
+        // no metrics before create
+        assertEquals(0, count.getAsLong());
+
+        // TODO the ViewWriteMetrics metrics are never created
+        session.execute(String.format(""CREATE KEYSPACE IF NOT EXISTS %s WITH replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };"", keyspace));
+        session.execute(String.format(""CREATE TABLE IF NOT EXISTS %s.%s (id int, val text, PRIMARY KEY(id, val));"", keyspace, table));
+        session.execute(String.format(""CREATE MATERIALIZED VIEW %s.%s AS SELECT id,val FROM %s.%s WHERE id IS NOT NULL AND val IS NOT NULL PRIMARY KEY (id,val);"", keyspace, view, keyspace, table));
+
+        // TODO needed?
+        //session.execute(String.format(""INSERT INTO %s.%s (id, val) VALUES (1, 'Apache Cassandra');"", keyspace, table));
+
+        // some metrics
+        assertTrue(count.getAsLong() > 0);
+
+        session.execute(String.format(""DROP MATERIALIZED VIEW %s.%s;"", keyspace, view));
+        session.execute(String.format(""DROP TABLE %s.%s;"", keyspace, table));

Review comment:
       You shouldn't have to drop the source table or the keyspace to get the MV's metrics removed.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jun/20 02:56;githubbot;600","maedhroz commented on a change in pull request #629:
URL: https://github.com/apache/cassandra/pull/629#discussion_r439180947



##########
File path: test/unit/org/apache/cassandra/metrics/ViewWriteMetricsTest.java
##########
@@ -0,0 +1,85 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.metrics;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+import java.util.function.LongSupplier;
+
+import org.apache.cassandra.SchemaLoader;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.schema.Schema;
+import org.apache.cassandra.service.EmbeddedCassandraService;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import com.datastax.driver.core.Cluster;
+import com.datastax.driver.core.Session;
+
+public class ViewWriteMetricsTest extends SchemaLoader
+{
+    private static Session session;
+
+    @BeforeClass()
+    public static void setup() throws ConfigurationException, IOException
+    {
+        Schema.instance.clear();
+
+        EmbeddedCassandraService cassandra = new EmbeddedCassandraService();
+        cassandra.start();
+
+        Cluster cluster = Cluster.builder().addContactPoint(""127.0.0.1"").withPort(DatabaseDescriptor.getNativeTransportPort()).build();
+        session = cluster.connect();
+    }
+
+    @Test
+    public void testMetricsCleanupOnDrop()
+    {
+        String keyspace = ""junit"";
+        String table = ""viewwritemetrics"";
+        String view = ""viewwritemetrics_materialized_view"";
+
+        CassandraMetricsRegistry registry = CassandraMetricsRegistry.Metrics;
+        LongSupplier count = () -> registry.getNames().stream().filter(n -> n.contains(view)).count();
+
+        // no metrics before create
+        assertEquals(0, count.getAsLong());
+
+        // TODO the ViewWriteMetrics metrics are never created
+        session.execute(String.format(""CREATE KEYSPACE IF NOT EXISTS %s WITH replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };"", keyspace));
+        session.execute(String.format(""CREATE TABLE IF NOT EXISTS %s.%s (id int, val text, PRIMARY KEY(id, val));"", keyspace, table));
+        session.execute(String.format(""CREATE MATERIALIZED VIEW %s.%s AS SELECT id,val FROM %s.%s WHERE id IS NOT NULL AND val IS NOT NULL PRIMARY KEY (id,val);"", keyspace, view, keyspace, table));
+
+        // TODO needed?
+        //session.execute(String.format(""INSERT INTO %s.%s (id, val) VALUES (1, 'Apache Cassandra');"", keyspace, table));

Review comment:
       Can confirm. We shouldn't need this.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jun/20 02:59;githubbot;600","maedhroz commented on a change in pull request #629:
URL: https://github.com/apache/cassandra/pull/629#discussion_r439181389



##########
File path: test/unit/org/apache/cassandra/metrics/ViewWriteMetricsTest.java
##########
@@ -0,0 +1,85 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.metrics;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+import java.util.function.LongSupplier;
+
+import org.apache.cassandra.SchemaLoader;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.schema.Schema;
+import org.apache.cassandra.service.EmbeddedCassandraService;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import com.datastax.driver.core.Cluster;
+import com.datastax.driver.core.Session;
+
+public class ViewWriteMetricsTest extends SchemaLoader
+{
+    private static Session session;
+
+    @BeforeClass()
+    public static void setup() throws ConfigurationException, IOException
+    {
+        Schema.instance.clear();
+
+        EmbeddedCassandraService cassandra = new EmbeddedCassandraService();
+        cassandra.start();
+
+        Cluster cluster = Cluster.builder().addContactPoint(""127.0.0.1"").withPort(DatabaseDescriptor.getNativeTransportPort()).build();
+        session = cluster.connect();
+    }
+
+    @Test
+    public void testMetricsCleanupOnDrop()
+    {
+        String keyspace = ""junit"";
+        String table = ""viewwritemetrics"";
+        String view = ""viewwritemetrics_materialized_view"";
+
+        CassandraMetricsRegistry registry = CassandraMetricsRegistry.Metrics;
+        LongSupplier count = () -> registry.getNames().stream().filter(n -> n.contains(view)).count();
+
+        // no metrics before create
+        assertEquals(0, count.getAsLong());
+
+        // TODO the ViewWriteMetrics metrics are never created

Review comment:
       @stillalex There are two kinds of metrics for MVs. The first represents the conditionally registered metrics in `TableMetrics`, for example `viewReadTime`. Then you have `ViewWriteMetrics` which is created on the `StorageProxy` and spans tables. (I think I mentioned the latter a bit too carelessly in one of my earlier comments on the Jira. Apologies.)
   
   In any case, all I think we need to do is drop this test at the end of `TableMetricsTest` and we're golden. That will ensure that even MVs-specific metrics in `TableMetrics` are removed. Does that make sense?




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jun/20 03:01;githubbot;600","maedhroz commented on a change in pull request #629:
URL: https://github.com/apache/cassandra/pull/629#discussion_r439182343



##########
File path: src/java/org/apache/cassandra/metrics/KeyspaceMetrics.java
##########
@@ -166,186 +168,84 @@ public KeyspaceMetrics(final Keyspace ks)
     {
         factory = new KeyspaceMetricNameFactory(ks);
         keyspace = ks;
-        memtableColumnsCount = createKeyspaceGauge(""MemtableColumnsCount"", new MetricValue()
-        {
-            public Long getValue(TableMetrics metric)
-            {
-                return metric.memtableColumnsCount.getValue();
-            }
-        });
-        memtableLiveDataSize = createKeyspaceGauge(""MemtableLiveDataSize"", new MetricValue()
-        {
-            public Long getValue(TableMetrics metric)
-            {
-                return metric.memtableLiveDataSize.getValue();
-            }
-        });
-        memtableOnHeapDataSize = createKeyspaceGauge(""MemtableOnHeapDataSize"", new MetricValue()
-        {
-            public Long getValue(TableMetrics metric)
-            {
-                return metric.memtableOnHeapSize.getValue();
-            }
-        });
-        memtableOffHeapDataSize = createKeyspaceGauge(""MemtableOffHeapDataSize"", new MetricValue()
-        {
-            public Long getValue(TableMetrics metric)
-            {
-                return metric.memtableOffHeapSize.getValue();
-            }
-        });
-        allMemtablesLiveDataSize = createKeyspaceGauge(""AllMemtablesLiveDataSize"", new MetricValue()
-        {
-            public Long getValue(TableMetrics metric)
-            {
-                return metric.allMemtablesLiveDataSize.getValue();
-            }
-        });
-        allMemtablesOnHeapDataSize = createKeyspaceGauge(""AllMemtablesOnHeapDataSize"", new MetricValue()
-        {
-            public Long getValue(TableMetrics metric)
-            {
-                return metric.allMemtablesOnHeapSize.getValue();
-            }
-        });
-        allMemtablesOffHeapDataSize = createKeyspaceGauge(""AllMemtablesOffHeapDataSize"", new MetricValue()
-        {
-            public Long getValue(TableMetrics metric)
-            {
-                return metric.allMemtablesOffHeapSize.getValue();
-            }
-        });
-        memtableSwitchCount = createKeyspaceGauge(""MemtableSwitchCount"", new MetricValue()
-        {
-            public Long getValue(TableMetrics metric)
-            {
-                return metric.memtableSwitchCount.getCount();
-            }
-        });
-        pendingCompactions = createKeyspaceGauge(""PendingCompactions"", new MetricValue()
-        {
-            public Long getValue(TableMetrics metric)
-            {
-                return (long) metric.pendingCompactions.getValue();
-            }
-        });
-        pendingFlushes = createKeyspaceGauge(""PendingFlushes"", new MetricValue()
-        {
-            public Long getValue(TableMetrics metric)
-            {
-                return (long) metric.pendingFlushes.getCount();
-            }
-        });
-        liveDiskSpaceUsed = createKeyspaceGauge(""LiveDiskSpaceUsed"", new MetricValue()
-        {
-            public Long getValue(TableMetrics metric)
-            {
-                return metric.liveDiskSpaceUsed.getCount();
-            }
-        });
-        totalDiskSpaceUsed = createKeyspaceGauge(""TotalDiskSpaceUsed"", new MetricValue()
-        {
-            public Long getValue(TableMetrics metric)
-            {
-                return metric.totalDiskSpaceUsed.getCount();
-            }
-        });
-        bloomFilterDiskSpaceUsed = createKeyspaceGauge(""BloomFilterDiskSpaceUsed"", new MetricValue()
-        {
-            public Long getValue(TableMetrics metric)
-            {
-                return metric.bloomFilterDiskSpaceUsed.getValue();
-            }
-        });
-        bloomFilterOffHeapMemoryUsed = createKeyspaceGauge(""BloomFilterOffHeapMemoryUsed"", new MetricValue()
-        {
-            public Long getValue(TableMetrics metric)
-            {
-                return metric.bloomFilterOffHeapMemoryUsed.getValue();
-            }
-        });
-        indexSummaryOffHeapMemoryUsed = createKeyspaceGauge(""IndexSummaryOffHeapMemoryUsed"", new MetricValue()
-        {
-            public Long getValue(TableMetrics metric)
-            {
-                return metric.indexSummaryOffHeapMemoryUsed.getValue();
-            }
-        });
-        compressionMetadataOffHeapMemoryUsed = createKeyspaceGauge(""CompressionMetadataOffHeapMemoryUsed"", new MetricValue()
-        {
-            public Long getValue(TableMetrics metric)
-            {
-                return metric.compressionMetadataOffHeapMemoryUsed.getValue();
-            }
-        });
+        memtableColumnsCount = createKeyspaceGauge(""MemtableColumnsCount"",
+                metric -> metric.memtableColumnsCount.getValue());
+        memtableLiveDataSize = createKeyspaceGauge(""MemtableLiveDataSize"",
+                metric -> metric.memtableLiveDataSize.getValue());
+        memtableOnHeapDataSize = createKeyspaceGauge(""MemtableOnHeapDataSize"",
+                metric -> metric.memtableOnHeapSize.getValue());
+        memtableOffHeapDataSize = createKeyspaceGauge(""MemtableOffHeapDataSize"",
+                metric -> metric.memtableOffHeapSize.getValue());
+        allMemtablesLiveDataSize = createKeyspaceGauge(""AllMemtablesLiveDataSize"",
+                metric -> metric.allMemtablesLiveDataSize.getValue());
+        allMemtablesOnHeapDataSize = createKeyspaceGauge(""AllMemtablesOnHeapDataSize"",
+                metric -> metric.allMemtablesOnHeapSize.getValue());
+        allMemtablesOffHeapDataSize = createKeyspaceGauge(""AllMemtablesOffHeapDataSize"",
+                metric -> metric.allMemtablesOffHeapSize.getValue());
+        memtableSwitchCount = createKeyspaceGauge(""MemtableSwitchCount"",
+                metric -> metric.memtableSwitchCount.getCount());
+        pendingCompactions = createKeyspaceGauge(""PendingCompactions"", metric -> metric.pendingCompactions.getValue());
+        pendingFlushes = createKeyspaceGauge(""PendingFlushes"", metric -> metric.pendingFlushes.getCount());
+        liveDiskSpaceUsed = createKeyspaceGauge(""LiveDiskSpaceUsed"", metric -> metric.liveDiskSpaceUsed.getCount());
+        totalDiskSpaceUsed = createKeyspaceGauge(""TotalDiskSpaceUsed"", metric -> metric.totalDiskSpaceUsed.getCount());
+        bloomFilterDiskSpaceUsed = createKeyspaceGauge(""BloomFilterDiskSpaceUsed"",
+                metric -> metric.bloomFilterDiskSpaceUsed.getValue());
+        bloomFilterOffHeapMemoryUsed = createKeyspaceGauge(""BloomFilterOffHeapMemoryUsed"",
+                metric -> metric.bloomFilterOffHeapMemoryUsed.getValue());
+        indexSummaryOffHeapMemoryUsed = createKeyspaceGauge(""IndexSummaryOffHeapMemoryUsed"",
+                metric -> metric.indexSummaryOffHeapMemoryUsed.getValue());
+        compressionMetadataOffHeapMemoryUsed = createKeyspaceGauge(""CompressionMetadataOffHeapMemoryUsed"",
+                metric -> metric.compressionMetadataOffHeapMemoryUsed.getValue());
+
         // latency metrics for TableMetrics to update
-        readLatency = new LatencyMetrics(factory, ""Read"");
-        writeLatency = new LatencyMetrics(factory, ""Write"");
-        rangeLatency = new LatencyMetrics(factory, ""Range"");
+        readLatency = createLatencyMetrics(""Read"");
+        writeLatency = createLatencyMetrics(""Write"");
+        rangeLatency = createLatencyMetrics(""Range"");
+
         // create histograms for TableMetrics to replicate updates to
-        sstablesPerReadHistogram = Metrics.histogram(factory.createMetricName(""SSTablesPerReadHistogram""), true);
-        tombstoneScannedHistogram = Metrics.histogram(factory.createMetricName(""TombstoneScannedHistogram""), false);
-        liveScannedHistogram = Metrics.histogram(factory.createMetricName(""LiveScannedHistogram""), false);
-        colUpdateTimeDeltaHistogram = Metrics.histogram(factory.createMetricName(""ColUpdateTimeDeltaHistogram""), false);
-        viewLockAcquireTime =  Metrics.timer(factory.createMetricName(""ViewLockAcquireTime""));
-        viewReadTime = Metrics.timer(factory.createMetricName(""ViewReadTime""));
-        // add manually since histograms do not use createKeyspaceGauge method
-        allMetrics.addAll(Lists.newArrayList(""SSTablesPerReadHistogram"", ""TombstoneScannedHistogram"", ""LiveScannedHistogram""));
+        sstablesPerReadHistogram = createKeyspaceHistogram(""SSTablesPerReadHistogram"", true);
+        tombstoneScannedHistogram = createKeyspaceHistogram(""TombstoneScannedHistogram"", false);
+        liveScannedHistogram = createKeyspaceHistogram(""LiveScannedHistogram"", false);
+        colUpdateTimeDeltaHistogram = createKeyspaceHistogram(""ColUpdateTimeDeltaHistogram"", false);
+        viewLockAcquireTime = createKeyspaceTimer(""ViewLockAcquireTime"");
+        viewReadTime = createKeyspaceTimer(""ViewReadTime"");
 
-        casPrepare = new LatencyMetrics(factory, ""CasPrepare"");
-        casPropose = new LatencyMetrics(factory, ""CasPropose"");
-        casCommit = new LatencyMetrics(factory, ""CasCommit"");
-        writeFailedIdealCL = Metrics.counter(factory.createMetricName(""WriteFailedIdealCL""));
-        idealCLWriteLatency = new LatencyMetrics(factory, ""IdealCLWrite"");
+        casPrepare = createLatencyMetrics(""CasPrepare"");
+        casPropose = createLatencyMetrics(""CasPropose"");
+        casCommit = createLatencyMetrics(""CasCommit"");
+        writeFailedIdealCL = createKeyspaceCounter(""WriteFailedIdealCL"");
+        idealCLWriteLatency = createLatencyMetrics(""IdealCLWrite"");
 
         speculativeRetries = createKeyspaceCounter(""SpeculativeRetries"", metric -> metric.speculativeRetries.getCount());
         speculativeFailedRetries = createKeyspaceCounter(""SpeculativeFailedRetries"", metric -> metric.speculativeFailedRetries.getCount());
         speculativeInsufficientReplicas = createKeyspaceCounter(""SpeculativeInsufficientReplicas"", metric -> metric.speculativeInsufficientReplicas.getCount());
         additionalWrites = createKeyspaceCounter(""AdditionalWrites"", metric -> metric.additionalWrites.getCount());
         repairsStarted = createKeyspaceCounter(""RepairJobsStarted"", metric -> metric.repairsStarted.getCount());
         repairsCompleted = createKeyspaceCounter(""RepairJobsCompleted"", metric -> metric.repairsCompleted.getCount());
-        repairTime = Metrics.timer(factory.createMetricName(""RepairTime""));
-        repairPrepareTime = Metrics.timer(factory.createMetricName(""RepairPrepareTime""));
-        anticompactionTime = Metrics.timer(factory.createMetricName(""AntiCompactionTime""));
-        validationTime = Metrics.timer(factory.createMetricName(""ValidationTime""));
-        repairSyncTime = Metrics.timer(factory.createMetricName(""RepairSyncTime""));
-        partitionsValidated = Metrics.histogram(factory.createMetricName(""PartitionsValidated""), false);
-        bytesValidated = Metrics.histogram(factory.createMetricName(""BytesValidated""), false);
+        repairTime =createKeyspaceTimer(""RepairTime"");
+        repairPrepareTime = createKeyspaceTimer(""RepairPrepareTime"");
+        anticompactionTime = createKeyspaceTimer(""AntiCompactionTime"");
+        validationTime = createKeyspaceTimer(""ValidationTime"");
+        repairSyncTime = createKeyspaceTimer(""RepairSyncTime"");
+        partitionsValidated = createKeyspaceHistogram(""PartitionsValidated"", false);
+        bytesValidated = createKeyspaceHistogram(""BytesValidated"", false);
 
-        confirmedRepairedInconsistencies = Metrics.meter(factory.createMetricName(""RepairedDataInconsistenciesConfirmed""));
-        unconfirmedRepairedInconsistencies = Metrics.meter(factory.createMetricName(""RepairedDataInconsistenciesUnconfirmed""));
+        confirmedRepairedInconsistencies = createKeyspaceMeter(""RepairedDataInconsistenciesConfirmed"");
+        unconfirmedRepairedInconsistencies = createKeyspaceMeter(""RepairedDataInconsistenciesUnconfirmed"");
 
-        repairedDataTrackingOverreadRows = Metrics.histogram(factory.createMetricName(""RepairedOverreadRows""), false);
-        repairedDataTrackingOverreadTime = Metrics.timer(factory.createMetricName(""RepairedOverreadTime""));
+        repairedDataTrackingOverreadRows = createKeyspaceHistogram(""RepairedOverreadRows"", false);
+        repairedDataTrackingOverreadTime = createKeyspaceTimer(""RepairedOverreadTime"");
     }
 
     /**
      * Release all associated metrics.
      */
     public void release()
     {
-        for(String name : allMetrics)
+        for(ReleasableMetric metric : allMetrics)

Review comment:
       ```suggestion
           for (ReleasableMetric metric : allMetrics)
   ```




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jun/20 03:05;githubbot;600","maedhroz commented on a change in pull request #629:
URL: https://github.com/apache/cassandra/pull/629#discussion_r439182440



##########
File path: src/java/org/apache/cassandra/metrics/TableMetrics.java
##########
@@ -981,28 +971,10 @@ public void updateSSTableIterated(int count)
      */
     public void release()
     {
-        for(Map.Entry<String, String> entry : all.entrySet())
-        {
-            CassandraMetricsRegistry.MetricName name = factory.createMetricName(entry.getKey());
-            CassandraMetricsRegistry.MetricName alias = aliasFactory.createMetricName(entry.getValue());
-            final Metric metric = Metrics.getMetrics().get(name.getMetricName());
-            if (metric != null)
-            {   // Metric will be null if it's a view metric we are releasing. Views have null for ViewLockAcquireTime and ViewLockReadTime
-                allTableMetrics.get(entry.getKey()).remove(metric);
-                Metrics.remove(name, alias);
-            }
+        for(ReleasableMetric entry : all)

Review comment:
       ```suggestion
           for (ReleasableMetric entry : all)
   ```




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jun/20 03:06;githubbot;600","maedhroz commented on a change in pull request #629:
URL: https://github.com/apache/cassandra/pull/629#discussion_r439181389



##########
File path: test/unit/org/apache/cassandra/metrics/ViewWriteMetricsTest.java
##########
@@ -0,0 +1,85 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.metrics;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+import java.util.function.LongSupplier;
+
+import org.apache.cassandra.SchemaLoader;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.schema.Schema;
+import org.apache.cassandra.service.EmbeddedCassandraService;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import com.datastax.driver.core.Cluster;
+import com.datastax.driver.core.Session;
+
+public class ViewWriteMetricsTest extends SchemaLoader
+{
+    private static Session session;
+
+    @BeforeClass()
+    public static void setup() throws ConfigurationException, IOException
+    {
+        Schema.instance.clear();
+
+        EmbeddedCassandraService cassandra = new EmbeddedCassandraService();
+        cassandra.start();
+
+        Cluster cluster = Cluster.builder().addContactPoint(""127.0.0.1"").withPort(DatabaseDescriptor.getNativeTransportPort()).build();
+        session = cluster.connect();
+    }
+
+    @Test
+    public void testMetricsCleanupOnDrop()
+    {
+        String keyspace = ""junit"";
+        String table = ""viewwritemetrics"";
+        String view = ""viewwritemetrics_materialized_view"";
+
+        CassandraMetricsRegistry registry = CassandraMetricsRegistry.Metrics;
+        LongSupplier count = () -> registry.getNames().stream().filter(n -> n.contains(view)).count();
+
+        // no metrics before create
+        assertEquals(0, count.getAsLong());
+
+        // TODO the ViewWriteMetrics metrics are never created

Review comment:
       @stillalex There are two kinds of metrics for MVs. The first represents the conditionally registered metrics in `TableMetrics`, for example `viewReadTime`. Then you have `ViewWriteMetrics` which is created on the `StorageProxy` and spans tables. (I think I mentioned the latter a bit too carelessly in one of my earlier comments on the Jira. Apologies.)
   
   In any case, all I think we need to do is drop this test at the end of `TableMetricsTest` (maybe renaming to `testViewMetricsCleanupOnDrop()`)and we're golden. That will ensure that even MVs-specific metrics in `TableMetrics` are removed. Does that make sense?




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jun/20 03:07;githubbot;600","stillalex commented on a change in pull request #629:
URL: https://github.com/apache/cassandra/pull/629#discussion_r439183924



##########
File path: test/unit/org/apache/cassandra/metrics/ViewWriteMetricsTest.java
##########
@@ -0,0 +1,85 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.metrics;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+import java.util.function.LongSupplier;
+
+import org.apache.cassandra.SchemaLoader;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.schema.Schema;
+import org.apache.cassandra.service.EmbeddedCassandraService;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import com.datastax.driver.core.Cluster;
+import com.datastax.driver.core.Session;
+
+public class ViewWriteMetricsTest extends SchemaLoader
+{
+    private static Session session;
+
+    @BeforeClass()
+    public static void setup() throws ConfigurationException, IOException
+    {
+        Schema.instance.clear();
+
+        EmbeddedCassandraService cassandra = new EmbeddedCassandraService();
+        cassandra.start();
+
+        Cluster cluster = Cluster.builder().addContactPoint(""127.0.0.1"").withPort(DatabaseDescriptor.getNativeTransportPort()).build();
+        session = cluster.connect();
+    }
+
+    @Test
+    public void testMetricsCleanupOnDrop()
+    {
+        String keyspace = ""junit"";
+        String table = ""viewwritemetrics"";
+        String view = ""viewwritemetrics_materialized_view"";
+
+        CassandraMetricsRegistry registry = CassandraMetricsRegistry.Metrics;
+        LongSupplier count = () -> registry.getNames().stream().filter(n -> n.contains(view)).count();
+
+        // no metrics before create
+        assertEquals(0, count.getAsLong());
+
+        // TODO the ViewWriteMetrics metrics are never created
+        session.execute(String.format(""CREATE KEYSPACE IF NOT EXISTS %s WITH replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };"", keyspace));
+        session.execute(String.format(""CREATE TABLE IF NOT EXISTS %s.%s (id int, val text, PRIMARY KEY(id, val));"", keyspace, table));
+        session.execute(String.format(""CREATE MATERIALIZED VIEW %s.%s AS SELECT id,val FROM %s.%s WHERE id IS NOT NULL AND val IS NOT NULL PRIMARY KEY (id,val);"", keyspace, view, keyspace, table));
+
+        // TODO needed?
+        //session.execute(String.format(""INSERT INTO %s.%s (id, val) VALUES (1, 'Apache Cassandra');"", keyspace, table));
+
+        // some metrics
+        assertTrue(count.getAsLong() > 0);
+
+        session.execute(String.format(""DROP MATERIALIZED VIEW %s.%s;"", keyspace, view));
+        session.execute(String.format(""DROP TABLE %s.%s;"", keyspace, table));

Review comment:
       this was not part of the test, it was the cleanup




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jun/20 03:13;githubbot;600","stillalex commented on a change in pull request #629:
URL: https://github.com/apache/cassandra/pull/629#discussion_r439184219



##########
File path: test/unit/org/apache/cassandra/metrics/ViewWriteMetricsTest.java
##########
@@ -0,0 +1,85 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.metrics;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+import java.util.function.LongSupplier;
+
+import org.apache.cassandra.SchemaLoader;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.schema.Schema;
+import org.apache.cassandra.service.EmbeddedCassandraService;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import com.datastax.driver.core.Cluster;
+import com.datastax.driver.core.Session;
+
+public class ViewWriteMetricsTest extends SchemaLoader
+{
+    private static Session session;
+
+    @BeforeClass()
+    public static void setup() throws ConfigurationException, IOException
+    {
+        Schema.instance.clear();
+
+        EmbeddedCassandraService cassandra = new EmbeddedCassandraService();
+        cassandra.start();
+
+        Cluster cluster = Cluster.builder().addContactPoint(""127.0.0.1"").withPort(DatabaseDescriptor.getNativeTransportPort()).build();
+        session = cluster.connect();
+    }
+
+    @Test
+    public void testMetricsCleanupOnDrop()
+    {
+        String keyspace = ""junit"";
+        String table = ""viewwritemetrics"";
+        String view = ""viewwritemetrics_materialized_view"";
+
+        CassandraMetricsRegistry registry = CassandraMetricsRegistry.Metrics;
+        LongSupplier count = () -> registry.getNames().stream().filter(n -> n.contains(view)).count();
+
+        // no metrics before create
+        assertEquals(0, count.getAsLong());
+
+        // TODO the ViewWriteMetrics metrics are never created

Review comment:
       I'll move the test over, sure. the trouble is I'm trying to verify (manually) that those metrics are actually created, otherwise that part of the code is not covered, so the test is not actually testing anything :)




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jun/20 03:14;githubbot;600","stillalex commented on a change in pull request #629:
URL: https://github.com/apache/cassandra/pull/629#discussion_r439184726



##########
File path: src/java/org/apache/cassandra/metrics/TableMetrics.java
##########
@@ -981,28 +971,10 @@ public void updateSSTableIterated(int count)
      */
     public void release()
     {
-        for(Map.Entry<String, String> entry : all.entrySet())
-        {
-            CassandraMetricsRegistry.MetricName name = factory.createMetricName(entry.getKey());
-            CassandraMetricsRegistry.MetricName alias = aliasFactory.createMetricName(entry.getValue());
-            final Metric metric = Metrics.getMetrics().get(name.getMetricName());
-            if (metric != null)
-            {   // Metric will be null if it's a view metric we are releasing. Views have null for ViewLockAcquireTime and ViewLockReadTime
-                allTableMetrics.get(entry.getKey()).remove(metric);
-                Metrics.remove(name, alias);
-            }
+        for(ReleasableMetric entry : all)

Review comment:
       same for KeyspaceMetrics




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jun/20 03:16;githubbot;600","maedhroz commented on a change in pull request #629:
URL: https://github.com/apache/cassandra/pull/629#discussion_r439188732



##########
File path: test/unit/org/apache/cassandra/metrics/ViewWriteMetricsTest.java
##########
@@ -0,0 +1,85 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.metrics;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+import java.util.function.LongSupplier;
+
+import org.apache.cassandra.SchemaLoader;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.schema.Schema;
+import org.apache.cassandra.service.EmbeddedCassandraService;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import com.datastax.driver.core.Cluster;
+import com.datastax.driver.core.Session;
+
+public class ViewWriteMetricsTest extends SchemaLoader
+{
+    private static Session session;
+
+    @BeforeClass()
+    public static void setup() throws ConfigurationException, IOException
+    {
+        Schema.instance.clear();
+
+        EmbeddedCassandraService cassandra = new EmbeddedCassandraService();
+        cassandra.start();
+
+        Cluster cluster = Cluster.builder().addContactPoint(""127.0.0.1"").withPort(DatabaseDescriptor.getNativeTransportPort()).build();
+        session = cluster.connect();
+    }
+
+    @Test
+    public void testMetricsCleanupOnDrop()
+    {
+        String keyspace = ""junit"";
+        String table = ""viewwritemetrics"";
+        String view = ""viewwritemetrics_materialized_view"";
+
+        CassandraMetricsRegistry registry = CassandraMetricsRegistry.Metrics;
+        LongSupplier count = () -> registry.getNames().stream().filter(n -> n.contains(view)).count();
+
+        // no metrics before create
+        assertEquals(0, count.getAsLong());
+
+        // TODO the ViewWriteMetrics metrics are never created

Review comment:
       An MV actually has fewer metrics than a normal table, due to `ViewLockAcquireTime` and `ViewReadTime` being present only for the latter. This test is still valuable, as it makes sure removal still works when those aren't present (i.e. the conditional logic in `releaseMetric()`).
   
   Either way, I don't think we need to worry about `ViewWriteMetrics`.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jun/20 03:35;githubbot;600","stillalex commented on pull request #629:
URL: https://github.com/apache/cassandra/pull/629#issuecomment-643046865


   @maedhroz updated the PR based on your suggestions, let me know if you want to dig deeper into the MV tests, or if this is sufficient.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jun/20 03:37;githubbot;600","maedhroz commented on a change in pull request #629:
URL: https://github.com/apache/cassandra/pull/629#discussion_r439190544



##########
File path: test/unit/org/apache/cassandra/metrics/ViewWriteMetricsTest.java
##########
@@ -0,0 +1,85 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.metrics;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+import java.util.function.LongSupplier;
+
+import org.apache.cassandra.SchemaLoader;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.exceptions.ConfigurationException;
+import org.apache.cassandra.schema.Schema;
+import org.apache.cassandra.service.EmbeddedCassandraService;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import com.datastax.driver.core.Cluster;
+import com.datastax.driver.core.Session;
+
+public class ViewWriteMetricsTest extends SchemaLoader
+{
+    private static Session session;
+
+    @BeforeClass()
+    public static void setup() throws ConfigurationException, IOException
+    {
+        Schema.instance.clear();
+
+        EmbeddedCassandraService cassandra = new EmbeddedCassandraService();
+        cassandra.start();
+
+        Cluster cluster = Cluster.builder().addContactPoint(""127.0.0.1"").withPort(DatabaseDescriptor.getNativeTransportPort()).build();
+        session = cluster.connect();
+    }
+
+    @Test
+    public void testMetricsCleanupOnDrop()
+    {
+        String keyspace = ""junit"";
+        String table = ""viewwritemetrics"";
+        String view = ""viewwritemetrics_materialized_view"";
+
+        CassandraMetricsRegistry registry = CassandraMetricsRegistry.Metrics;
+        LongSupplier count = () -> registry.getNames().stream().filter(n -> n.contains(view)).count();
+
+        // no metrics before create
+        assertEquals(0, count.getAsLong());
+
+        // TODO the ViewWriteMetrics metrics are never created

Review comment:
       If you want to verify here that `ViewLockAcquireTime` and `ViewReadTime` are created only the base table and not an MV, you could filter the names on each of them, etc.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jun/20 03:43;githubbot;600","stillalex commented on pull request #629:
URL: https://github.com/apache/cassandra/pull/629#issuecomment-643335193


   @maedhroz very good suggestion! I changed the tests to include the metrics in the error message. 


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jun/20 15:28;githubbot;600","maedhroz commented on pull request #629:
URL: https://github.com/apache/cassandra/pull/629#issuecomment-645422789


   @stillalex I think we're probably good to commit this, although squashing first might make it easier for the committer ;)


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jun/20 14:50;githubbot;600","smiklosovic closed pull request #629:
URL: https://github.com/apache/cassandra/pull/629


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 13:38;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,9600,,,0,9600,,,,,,,,,,,CASSANDRA-14819,,,,CASSANDRA-14238,CASSANDRA-15782,,,,,,,,,,,"26/Jul/19 13:08;stillalex;CASSANDRA-14888.patch;https://issues.apache.org/jira/secure/attachment/12975980/CASSANDRA-14888.patch","30/Jun/20 08:45;ExtendedDictionary.java;https://issues.apache.org/jira/secure/attachment/13006729/ExtendedDictionary.java",,,,,,,,,,,,2.0,stillalex,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 30 14:52:42 UTC 2020,,,,,,,,,,,"0|s00fww:",9223372036854775807,,,,,,,,,,,clohfink,djoshi,maedhroz,Critical,,3.0.0,,,https://github.com/apache/cassandra/commit/1731e2fe84c1da2a6f4b0d1f73b8fd76c88b3acd,,,,,,,,,"Review and tests
CircleCI: https://circleci.com/gh/dineshjoshi/cassandra/2534",,,,,"15/Apr/19 17:25;stillalex;There are a few more mbeans that don't unregister, not just the ones listed here.
Proposed patch [here|https://github.com/apache/cassandra/compare/trunk...stillalex:CASSANDRA-14888], I also tried adding a new test (java, not dtest) based on an existing one, if I got something wrong please let me know. I went for the simplest test there is, I'm open to suggestions if there is a need to improve it right away.
;;;","26/Jul/19 13:12;stillalex;Updated the patch to include the table parts as well (with a test), making CASSANDRA-14819 superfluous.;;;","26/Jul/19 13:17;benedict;Thanks!  [~djoshi3]: would you like to review this?;;;","26/Jul/19 15:02;djoshi;Yes, I’ll be happy to review it.;;;","29/Jul/19 05:49;djoshi;[~stillalex], thank you for the patch. I have imported it to a branch in my repo. https://github.com/apache/cassandra/compare/trunk...dineshjoshi:14888-trunk?expand=1 I will review it shortly and provide you feedback.;;;","18/Nov/19 01:49;stillalex;[~djoshi] any feedback?;;;","18/Nov/19 15:43;cnlwsu;You should to add a unit test to cover MVs as well as they have some conditionally registered metrics.

There are utility methods to create the metrics and automatically deregister them on cleanup. All metrics with issues just skipped that and created the metrics manually. Really this does fix the issue, but by doing more manual cleanup.

While this does fix the problem, I think we should change these metrics to register appropriately (which also may provide keyspace metrics) or clean up that mechanism up a bit to be easier (maybe using annotations, reflection or something?). We should try to enforce the registering and automatic cleanup or make it easier and more obvious instead of setting a further precedence to do it manually.

In the past the ""wall of removeMetric"" calls in {{release}} was never kept in sync and while the junit should work, this actually isn't the first time a unit test like this has been made (3rd to my knowledge) and there are some ""flakey"" scenarios with dropping tables, and the test actually doesn't capture everything (although we can improve that).

This is the 4th (that I remember at least) time this issue came up, I think we should look at this in a bigger picture sense. That said if you are not interested or don't have bandwidth we could just patch this up a little here as it has definite value and have a follow up ticket to try to prevent the issue in future.;;;","23/Nov/19 02:28;stillalex;[~cnlwsu] thanks for the great feedback! I am interested in providing a more in-depth solution to this, I'd just need a bit of guidance. Let me take another stab at it based on your observations and we can continue from there.;;;","26/Nov/19 16:52;stillalex;[~cnlwsu] I took another look and while the ""wall of removes"" is very fragile, it seems to be the way to go, or at least I couldn't find a cleaner example in Cassandra of how to approach this. One idea that I had was to have the factory keep track of registered & perform release when needed (looking at KeyspaceMetrics: basically move allMetrics into KeyspaceMetricNameFactory and add a #release method for cleanup).
Again, I would love a pointer to prior art here, so I get a better understanding of expectations.

Also, regarding the tests, could you expand a bit on what is missing? I could improve this bit so it covers MVs, not sure about the flakey scenarios.;;;","27/Nov/19 06:19;djoshi;Hi [~stillalex], my apologies for the delay and thanks [~cnlwsu] for the great feedback. I'll try to review over this week.;;;","17/Feb/20 20:19;djoshi;[~stillalex] I'm revisiting this ticket. I think we need to go further to ensure that future metrics are released properly without excessive manual intervention. We should follow a pattern where we use a factory to register all kinds of releasable metrics which doesn't seem to be the case right now. Once we have that we can iterate over them in the release method much like what we do for Metric instances and call release on them.;;;","22/Mar/20 00:12;djoshi;[~stillalex] and I chatted about the approach. For the time being we're going to take the minimally invasive approach and try to eliminate the manual release of all metrics in {{TableMetrics}}.;;;","23/Mar/20 00:54;stillalex;thanks [~djoshi] for the discussion. I agree with the minimal approach, but there was a fair amount of shuffle, mainly contained inside the TableMetrics class.
I pushed a new version of the patch, let me know what you think. [0]
Changes:
* TableMetrics: I tried to aggregate all registered metrics into the 'all' set, which now contains 'release' wrappers for each registered metric. it was a bit of a stretch, so if there's a way to make the code cleaner I'm open to further changing it.
* KeyspaceMetrics: this was a much simpler setup, most changes are trivial refactoring of functions. release happens automatically thanks to the factory keeping track of registered metrics.
* added some tests covering metrics unregistering for tables and keyspaces

[0] https://github.com/apache/cassandra/compare/trunk...stillalex:CASSANDRA-14888;;;","09/Jun/20 00:20;maedhroz;[~stillalex] The only thing that immediately jumps out to me about the newer patch is that we might want to stay consistent with the new [helper methods|https://github.com/apache/cassandra/pull/493/files#diff-40a2ae61c758454cc6166903250edbdfR953] for metric creation added in CASSANDRA-8272. (i.e. It might also [make sense|https://github.com/apache/cassandra/compare/trunk...stillalex:CASSANDRA-14888#diff-40a2ae61c758454cc6166903250edbdfR876] to have a version of {{createTableTimer()}} that doesn't take a keyspace metric, just an alias...);;;","09/Jun/20 15:13;maedhroz;Finished looking at the tests, and had a couple minor comments:

1.) {{TableMetricsTest#testMetricsCleanupOnDrop()}} already uses {{recreateTable()}}, so you might be able to get away with using a descriptive static name for the table.
2.) We might not need {{OrderedJUnit4ClassRunner}} in {{KeyspaceMetricsTest}}.
3.) {{KeyspaceMetricsTest#registerUnregister()}} could perhaps be renamed {{testMetricsCleanupOnDrop()}} to follow the pattern from {{TableMetricsTest}}.
4.) Echoing [~cnlwsu]'s earlier suggestion, a brief test around un-registering MV metrics (ex. in {{ViewWriteMetrics}}) would be a good idea. (The number of metrics there is small, so I don't expect we'd need to add any more complex machinery for that. The unit test would catch leaks going forward...)

Otherwise, this patch looks like a nice step in the right direction.;;;","10/Jun/20 02:49;stillalex;thanks [~maedhroz] for the review. I have rebased & updated the patch. [0]
the only changes still missing are the tests for ViewWriteMetrics, which I will add soon.
please take a look and let me know if it looks good so far.


[0] https://github.com/apache/cassandra/compare/trunk...stillalex:CASSANDRA-14888;;;","10/Jun/20 17:39;maedhroz;[~stillalex] Two minor things:

1.) I think there's a [minor typo|https://github.com/apache/cassandra/compare/trunk...stillalex:CASSANDRA-14888#diff-8f3132f0fa1e03f77fd29bc9b8163fe3R65] in {{KeyspaceMetricsTest#testMetricsCleanupOnDrop()}}.

2.) You probably don't need the JavaDoc comment for {{ReleasableMetric#release()}}, unless there's something more substantial we need to say there.

Just heads up...there also may have been an issue (which is now resolved) w/ compiling trunk at the time you rebased.

Do you mind creating a quick PR so we can discuss the patch inline?;;;","10/Jun/20 19:33;maedhroz;[~stillalex] There is one real problem I think we need to address. In {{KeyspaceMetricNameFactory#release()}}, we call {{createMetricName()}}, which adds names to {{allMetrics}}, _while we're iterating over_ {{allMetrics}}. You'll either need to store {{MetricName}} instances in {{allMetrics}}, so you can avoid the call to {{createMetricName()}}, or go back to something more like the approach in {{TableMetrics}}, keeping the metric lifecycle separate from the factory itself. (I lean toward the {{TableMetrics}} approach, given the helper methods might make it a little easier to add things going forward, and it doesn't fragment the responsibilities of the factory, but I could be wrong there.)

By the way, when we get to a good place (maybe after the MV test is ready) I'll cherry-pick your commit into my fork and run it through CircleCI. (The free tier hardware is a bit lacking...);;;","12/Jun/20 01:04;stillalex;[~maedhroz] created a PR https://github.com/apache/cassandra/pull/629

bq. Just heads up...there also may have been an issue (which is now resolved) w/ compiling trunk at the time you rebased.
that explains it :)

I rebased the PR, followed your suggestions on the Keyspace metrics it's now very close to the Table metrics version.
Re. MV test I added a test but it's not complete, it doesn't seem to create those specific metrics, I'm not sure if I missed something.;;;","12/Jun/20 03:04;maedhroz;[~stillalex] I dropped a few comments around the MV metrics test in the PR. Otherwise, this is getting really close.;;;","12/Jun/20 03:31;stillalex;thanks for the review so far.
the last pending issue I guess is related to the MV test itself. as suggested I moved the test over but I still don't see any of the metrics I was looking for (from ViewWriteMetrics): ViewReplicasAttempted, ViewReplicasSuccess, ViewWriteLatency, ViewPendingMutations.

This is what is created during the current test,(and all of this gets correctly cleaned):
{code}
org.apache.cassandra.metrics.Table.AdditionalWriteLatencyNanos.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.AdditionalWrites.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.AllMemtablesHeapSize.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.AllMemtablesLiveDataSize.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.AllMemtablesOffHeapSize.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.AnticompactionTime.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.BloomFilterDiskSpaceUsed.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.BloomFilterFalsePositives.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.BloomFilterFalseRatio.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.BloomFilterOffHeapMemoryUsed.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.BytesAnticompacted.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.BytesFlushed.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.BytesMutatedAnticompaction.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.BytesPendingRepair.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.BytesRepaired.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.BytesUnrepaired.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.BytesValidated.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.CasCommitLatency.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.CasCommitTotalLatency.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.CasPrepareLatency.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.CasPrepareTotalLatency.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.CasProposeLatency.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.CasProposeTotalLatency.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.ColUpdateTimeDeltaHistogram.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.CompactionBytesWritten.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.CompressionMetadataOffHeapMemoryUsed.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.CompressionRatio.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.CoordinatorReadLatency.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.CoordinatorScanLatency.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.CoordinatorWriteLatency.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.DroppedMutations.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.EstimatedColumnCountHistogram.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.EstimatedPartitionCount.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.EstimatedPartitionSizeHistogram.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.IndexSummaryOffHeapMemoryUsed.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.KeyCacheHitRate.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.LiveDiskSpaceUsed.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.LiveSSTableCount.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.LiveScannedHistogram.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.MaxPartitionSize.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.MeanPartitionSize.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.MemtableColumnsCount.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.MemtableLiveDataSize.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.MemtableOffHeapSize.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.MemtableOnHeapSize.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.MemtableSwitchCount.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.MinPartitionSize.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.MutatedAnticompactionGauge.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.OldVersionSSTableCount.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.PartitionsValidated.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.PendingCompactions.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.PendingFlushes.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.PercentRepaired.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.RangeLatency.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.RangeTotalLatency.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.ReadLatency.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.ReadRepairRequests.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.ReadTotalLatency.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.RecentBloomFilterFalsePositives.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.RecentBloomFilterFalseRatio.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.RepairJobsCompleted.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.RepairJobsStarted.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.RepairedDataInconsistenciesConfirmed.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.RepairedDataInconsistenciesUnconfirmed.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.RepairedDataTrackingOverreadRows.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.RepairedDataTrackingOverreadTime.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.ReplicaSideFilteringProtectionRequests.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.RowCacheHit.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.RowCacheHitOutOfRange.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.RowCacheMiss.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.SSTablesPerReadHistogram.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.ShortReadProtectionRequests.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.SnapshotsSize.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.SpeculativeFailedRetries.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.SpeculativeInsufficientReplicas.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.SpeculativeRetries.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.SpeculativeSampleLatencyNanos.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.SyncTime.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.TombstoneFailures.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.TombstoneScannedHistogram.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.TombstoneWarnings.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.TotalDiskSpaceUsed.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.UnleveledSSTables.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.ValidationTime.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.WaitingOnFreeMemtableSpace.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.WriteLatency.junit.tablemetricstest_materialized_view_cleanup, org.apache.cassandra.metrics.Table.WriteTotalLatency.junit.tablemetricstest_materialized_view_cleanup
{code};;;","12/Jun/20 03:38;stillalex;Updated the PR, will leave the final decision re. MV tests to you [~maedhroz]. 
Let me know if the PR needs extra work in any way.;;;","12/Jun/20 03:55;maedhroz;+1;;;","12/Jun/20 04:16;maedhroz;[~stillalex] Kicked off a [CI run|https://app.circleci.com/pipelines/github/maedhroz/cassandra/5/workflows/357e99c0-fa27-4027-8b01-3909eac71e4c]...;;;","12/Jun/20 04:17;maedhroz;bq. but I still don't see any of the metrics I was looking for

Right, those metrics (from {{ViewWriteMetrics}}) belong to {{StorageProxy}}, and they would only show up if you searched for {{ViewWrite}}, not the view table name.;;;","12/Jun/20 15:31;stillalex;thanks [~maedhroz] let me know how the CI results look like.
just in case it got missed in the wall of updates, this PR also includes CASSANDRA-14819.;;;","12/Jun/20 22:49;maedhroz;[~stillalex] The unit and in-JVM dtests look pretty good so far. I'm sorting out some hardware issues w/ the Python dtests, but I'll let you know when those resolve.;;;","16/Jun/20 20:58;djoshi;I kicked off a [test run|https://circleci.com/workflow-run/de5f7cdb-06b6-4869-9d19-81a145e79f3f]. However, I see a couple failures. Could you both please ensure that they are indeed unrelated / flaky? ;;;","16/Jun/20 21:22;maedhroz;The one failure in the unit tests appears to be CASSANDRA-14238 resurrected, which we've already noted.

In the dtests, the {{TestPushedNotifications}} failures have already been noted in CASSANDRA-15877.

There also appears to be some good evidence that {{test_simple_repair_order_preserving - repair_tests.repair_test.TestRepair}} is [flaky|https://issues.apache.org/jira/browse/CASSANDRA-15170?focusedCommentId=16909454&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16909454].;;;","16/Jun/20 21:25;maedhroz;...and {{hintedhandoff_test.TestHintedHandoffConfig}} appears to be covered by CASSANDRA-15865.;;;","17/Jun/20 03:15;maedhroz;[~djoshi] After the known issues above, the only other failure appears to be [compression_test.TestCompression|https://app.circleci.com/pipelines/github/dineshjoshi/cassandra/48/workflows/23de1e8d-108e-4138-8ea6-a650965920a5/jobs/2550], but CASSANDRA-15782 should have addressed that. Not exactly sure what's going on there, given that fix was committed to {{cassandra-dtests}} at the [beginning of May|https://github.com/apache/cassandra-dtest/commit/da7fcefb16d16af8924cda35c0a6a63ad553694f].;;;","17/Jun/20 05:46;maedhroz;I've made a note in CASSANDRA-15782, but I think it's pretty safe to say this patch isn't the cause of any of the regressions we're seeing. I'd say we're ready to commit.;;;","18/Jun/20 00:34;djoshi;+1;;;","23/Jun/20 17:26;djoshi;committed to [trunk|https://github.com/apache/cassandra/commit/1731e2fe84c1da2a6f4b0d1f73b8fd76c88b3acd]. Thank you for the contribution [~stillalex] and reviews [~maedhroz], [~cnlwsu]. [~stillalex] if you have backports to 3.0 and 3.11, I'll be happy to take a look.;;;","30/Jun/20 14:52;maedhroz;Hi [~Ryangdotson]. Does the file you attached ({{ExtendedDictionary.java}}) relate to this issue? It doesn't look like it, but just making sure...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a new tool to dump audit logs,CASSANDRA-14885,13197551,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vinaykumarcse,vinaykumarcse,vinaykumarcse,10/Nov/18 02:41,15/May/20 08:00,13/Jul/23 08:37,23/Jan/19 13:39,4.0,4.0-alpha1,,,,,Legacy/Tools,,,,0,,,,"As part of CASSANDRA-12151, AuditLogging feature uses [fqltool|https://github.com/apache/cassandra/blob/trunk/tools/bin/fqltool] to dump audit log file contents in human-readable text format from binary logging format ([BinLog| https://issues.apache.org/jira/browse/CASSANDRA-13983]).

The goal of this ticket is to create a separate tool to dump audit logs instead of relying fqltool and let fqltool be full query log specific.

",,jeromatron,marcuse,vinaykumarcse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14772,,,,,,,,,,,,,,,0.0,vinaykumarcse,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jan 23 13:39:01 UTC 2019,,,,,,,,,,,"0|s00cko:",9223372036854775807,,,,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"22/Jan/19 13:31;marcuse;looks good, just a few minor comments;
* the script should probably move to {{tools/bin/...}}
* in the test you could use {{Files.createTempDirectory(""foo"")}} instead of the {{createTempDir}} method
* also make sure you delete that directory on exit (not just the files in it);;;","23/Jan/19 09:24;vinaykumarcse;Thanks for the review [~krummas]. I have rebased on latest trunk and fixed your review comments. I have also added a batch file for {{auditlogviewer}} tool.

 
||Branch||utests||dtests||
|[patch|https://github.com/vinaykumarchella/cassandra/commits/trunk_CASSANDRA-14885]|[Circle CI|https://circleci.com/gh/vinaykumarchella/cassandra/340]|[Circle CI|https://circleci.com/gh/vinaykumarchella/cassandra/339]|

 ;;;","23/Jan/19 13:39;marcuse;and committed as {{7d138e20ea987d44fffbc47de4674b253b7431ff}} - thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition when setting bootstrap flags,CASSANDRA-14878,13197232,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,e.dimitrova,sbtourist,sbtourist,08/Nov/18 20:04,13/Jun/20 14:35,13/Jul/23 08:37,13/Jan/20 21:20,3.0.20,3.11.6,4.0,4.0-alpha3,,,Local/Startup and Shutdown,,,,0,,,,"{{StorageService#bootstrap()}} is supposed to wait for bootstrap to finish, but Guava calls the future listeners [after|https://github.com/google/guava/blob/ec2dedebfa359991cbcc8750dc62003be63ec6d3/guava/src/com/google/common/util/concurrent/AbstractFuture.java#L890] unparking its waiters, which causes a race on when the {{bootstrapFinished()}} will be executed, making it non-deterministic.",,brandon.williams,e.dimitrova,jaid,jeromatron,sbtourist,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15874,,,,,,,,,,,,,,,,,,"13/Jan/20 18:05;e.dimitrova;Screen Shot 2020-01-13 at 1.03.19 PM.png;https://issues.apache.org/jira/secure/attachment/12990755/Screen+Shot+2020-01-13+at+1.03.19+PM.png","13/Jan/20 18:05;e.dimitrova;Screen Shot 2020-01-13 at 1.03.41 PM.png;https://issues.apache.org/jira/secure/attachment/12990756/Screen+Shot+2020-01-13+at+1.03.41+PM.png","13/Jan/20 18:05;e.dimitrova;Screen Shot 2020-01-13 at 1.04.05 PM.png;https://issues.apache.org/jira/secure/attachment/12990757/Screen+Shot+2020-01-13+at+1.04.05+PM.png","13/Jan/20 18:05;e.dimitrova;Screen Shot 2020-01-13 at 1.04.29 PM.png;https://issues.apache.org/jira/secure/attachment/12990758/Screen+Shot+2020-01-13+at+1.04.29+PM.png","13/Jan/20 18:05;e.dimitrova;Screen Shot 2020-01-13 at 1.04.55 PM.png;https://issues.apache.org/jira/secure/attachment/12990759/Screen+Shot+2020-01-13+at+1.04.55+PM.png","13/Jan/20 18:05;e.dimitrova;Screen Shot 2020-01-13 at 12.41.41 PM.png;https://issues.apache.org/jira/secure/attachment/12990760/Screen+Shot+2020-01-13+at+12.41.41+PM.png","13/Jan/20 18:05;e.dimitrova;Screen Shot 2020-01-13 at 12.42.47 PM.png;https://issues.apache.org/jira/secure/attachment/12990761/Screen+Shot+2020-01-13+at+12.42.47+PM.png","13/Jan/20 18:05;e.dimitrova;Screen Shot 2020-01-13 at 12.43.28 PM.png;https://issues.apache.org/jira/secure/attachment/12990762/Screen+Shot+2020-01-13+at+12.43.28+PM.png","13/Jan/20 18:05;e.dimitrova;Screen Shot 2020-01-13 at 12.44.36 PM.png;https://issues.apache.org/jira/secure/attachment/12990763/Screen+Shot+2020-01-13+at+12.44.36+PM.png","13/Jan/20 18:05;e.dimitrova;Screen Shot 2020-01-13 at 12.48.24 PM.png;https://issues.apache.org/jira/secure/attachment/12990764/Screen+Shot+2020-01-13+at+12.48.24+PM.png","13/Jan/20 18:05;e.dimitrova;Screen Shot 2020-01-13 at 12.48.35 PM.png;https://issues.apache.org/jira/secure/attachment/12990765/Screen+Shot+2020-01-13+at+12.48.35+PM.png","13/Jan/20 18:05;e.dimitrova;Screen Shot 2020-01-13 at 12.48.49 PM.png;https://issues.apache.org/jira/secure/attachment/12990766/Screen+Shot+2020-01-13+at+12.48.49+PM.png",,12.0,e.dimitrova,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jan 13 21:20:20 UTC 2020,,,,,,,,,,,"0|s00als:",9223372036854775807,,,,,,,,,,,brandon.williams,,,Normal,,3.0 alpha 1,,,https://github.com/apache/cassandra/commit/301168b60331bbd01864265a02fce9b63b94b823,,,,,,,,,"||Branch||
|[trunk-14878|https://github.com/ekaterinadimitrova2/cassandra/tree/trunk-14878]|
|[cassandra-3.0-14878|https://github.com/ekaterinadimitrova2/cassandra/tree/cassandra-3.0-14878]|
|[cassandra-3.11-14878|https://github.com/ekaterinadimitrova2/cassandra/tree/cassandra-3.11-14878]|
|[DTest|https://github.com/ekaterinadimitrova2/cassandra-dtest/pulls]|",,,,,"09/Dec/19 18:07;e.dimitrova;New d-test added to test the patch. Pull request for the D-Test could be found [here|https://github.com/ekaterinadimitrova2/cassandra-dtest/pull/1] Patch implemented but there is a bug with the stress tool on versions 3.0 and 3.11 which prevents me from smooth running the d-tests.

Ongoing investigation of the issue.  

Checking whether this is related to this issue - CASSANDRA-11628  The patch is there but I still get the same errors.

Everything is fine with the trunk version. ;;;","02/Jan/20 12:47;e.dimitrova;Issue fixed. Related D-Test completed successfully. CI tests running for version 3.0. Upon successful completion, merge with the newer versions will follow.  ;;;","13/Jan/20 17:40;e.dimitrova;||Branch||
|[trunk-14878|https://github.com/ekaterinadimitrova2/cassandra/tree/trunk-14878]|
|[cassandra-3.0-14878| https://github.com/ekaterinadimitrova2/cassandra/tree/cassandra-3.0-14878]|
|[cassandra-3.11-14878| https://github.com/ekaterinadimitrova2/cassandra/tree/cassandra-3.11-14878]|
|[DTest| https://github.com/ekaterinadimitrova2/cassandra-dtest/pulls]|;;;","13/Jan/20 18:07;e.dimitrova;CI results screenshots added. Many failing dtests for version 11 but I did comparison with the OSS Jenkins latest builds and it looks like the same ones are failing even without the patch. ;;;","13/Jan/20 21:20;brandon.williams;+1, committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Snapshot name merges with keyspace name shown by nodetool listsnapshots for snapshots with long names,CASSANDRA-14876,13197138,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,Gerrrr,Gerrrr,Gerrrr,08/Nov/18 14:26,16/Apr/19 09:29,13/Jul/23 08:37,07/Apr/19 04:31,3.0.19,,,,,,Tool/nodetool,,,,0,,,,"If snapshot name is long enough, it will merge  keyspace name and the command output will be inconvenient to read for a {{nodetool}} user, e.g.

{noformat}
bin/nodetool listsnapshots
Snapshot Details:
Snapshot name       Keyspace name                Column family name           True size          Size on disk
1541670390886       system_distributed           parent_repair_history        0 bytes            13 bytes
1541670390886       system_distributed           repair_history               0 bytes            13 bytes
1541670390886       system_auth                  roles                        0 bytes            4.98 KB
1541670390886       system_auth                  role_members                 0 bytes            13 bytes
1541670390886       system_auth                  resource_role_permissons_index0 bytes            13 bytes
1541670390886       system_auth                  role_permissions             0 bytes            13 bytes
1541670390886       system_traces                sessions                     0 bytes            13 bytes
1541670390886       system_traces                events                       0 bytes            13 bytes
39_characters_long_name_2017-09-05-11-Usystem_distributed           parent_repair_history        0 bytes            13 bytes
39_characters_long_name_2017-09-05-11-Usystem_distributed           repair_history               0 bytes            13 bytes
39_characters_long_name_2017-09-05-11-Usystem_auth                  roles                        0 bytes            4.98 KB
39_characters_long_name_2017-09-05-11-Usystem_auth                  role_members                 0 bytes            13 bytes
39_characters_long_name_2017-09-05-11-Usystem_auth                  resource_role_permissons_index0 bytes            13 bytes
39_characters_long_name_2017-09-05-11-Usystem_auth                  role_permissions             0 bytes            13 bytes
39_characters_long_name_2017-09-05-11-Usystem_traces                sessions                     0 bytes            13 bytes
39_characters_long_name_2017-09-05-11-Usystem_traces                events                       0 bytes            13 bytes
41_characters_long_name_2017-09-05-11-UTCsystem_distributed           parent_repair_history        0 bytes            13 bytes
41_characters_long_name_2017-09-05-11-UTCsystem_distributed           repair_history               0 bytes            13 bytes
41_characters_long_name_2017-09-05-11-UTCsystem_auth                  roles                        0 bytes            4.98 KB
41_characters_long_name_2017-09-05-11-UTCsystem_auth                  role_members                 0 bytes            13 bytes
41_characters_long_name_2017-09-05-11-UTCsystem_auth                  resource_role_permissons_index0 bytes            13 bytes
41_characters_long_name_2017-09-05-11-UTCsystem_auth                  role_permissions             0 bytes            13 bytes
41_characters_long_name_2017-09-05-11-UTCsystem_traces                sessions                     0 bytes            13 bytes
41_characters_long_name_2017-09-05-11-UTCsystem_traces                events                       0 bytes            13 bytes
{noformat}",,Gerrrr,jeromatron,mck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-7950,,,,,,,,,,,,,,,,,,,,,,,0.0,Gerrrr,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Apr 07 04:31:35 UTC 2019,,,,,,,,,,,"0|s00a0w:",9223372036854775807,,,,,,,,,,,mck,,,Normal,,2.1 beta1,,,,,,,,,,,,,,,,,"08/Nov/18 14:29;Gerrrr;Patch: [3.0|https://github.com/Gerrrr/cassandra/tree/14876-3.0]

;;;","07/Apr/19 03:40;mck;LGTM.

Note, this takes the output width (of the header row) from 110 to 134 characters.
And, the same problem will occur if keyspace or table names are longer than 29 characters. A better approach may be to loop through the snapshots first to figure out the required widths for each column (which is how it works in 3.11 onwards, see CASSANDRA-7950 ).

 But, I reckon the snapshot name will be the column most likely be ""too long"", so this is an ok stopgap for 3.0;;;","07/Apr/19 04:31;mck;Committed to just 3.0 as 7a058c6b0345411caaffd436b75f23c7450cc85b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix missing rows when reading 2.1 SSTables with static columns in 3.0,CASSANDRA-14873,13196918,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,aleksey,aleksey,aleksey,07/Nov/18 19:41,02/Aug/19 02:50,13/Jul/23 08:37,13/Nov/18 15:34,3.0.18,3.11.4,,,,,Legacy/Local Write-Read Paths,,,,0,,,,"If a partition has a static row and is large enough to be indexed, then {{firstName}} of the first index block will be set to a static clustering. When deserializing the column index we then incorrectly deserialize the {{firstName}} as a regular, non-{{STATIC}} {{Clustering}} - a singleton array with an empty {{ByteBuffer}} to be exact. Depending on the clustering comparator, this can trip up binary search over {{IndexInfo}} list and cause an incorrect resultset to be returned.",,aleksey,bdeggleston,cscotta,jay.zhuang,jeromatron,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,Correctness -> Transient Incorrect Response,,,,,,,,Challenging,Fuzz Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 13 15:34:10 UTC 2018,,,,,,,,,,,"0|s008o0:",9223372036854775807,,,,,,,,,,,bdeggleston,,,Critical,,3.0.0,,,,,,,,,,,,,,,,,"12/Nov/18 19:36;aleksey;Code: [3.0|https://github.com/iamaleksey/cassandra/commits/14873-3.0], [3.11|https://github.com/iamaleksey/cassandra/commits/14873-3.11]. CI: [3.0|https://circleci.com/workflow-run/81d0530a-3d53-4831-ac9a-7051283caadf], [3.11|https://circleci.com/workflow-run/8a0efd5e-2a1e-455e-ba2c-b0bc1a2c29c6].;;;","13/Nov/18 01:14;bdeggleston;+1 on the fix, could you take a look at the failing dtests though? The only dtest I've seen fail recently in 3.0 and 3.11 is the HSHA one.;;;","13/Nov/18 15:34;aleksey;Thanks.

Had a look, yes. Just the usual flaky suspects.

Committed as [9eee7aa7874c17ab28d43ff58c97da889c87e397|https://github.com/apache/cassandra/commit/9eee7aa7874c17ab28d43ff58c97da889c87e397] to 3.0, merged fully to 3.11, and to trunk with {{-s ours}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update to version of python driver and update cqlsh to use driver metadata for virtual tables,CASSANDRA-14872,13196914,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,djoshi,andrew.tolbert,andrew.tolbert,07/Nov/18 19:23,15/May/20 08:53,13/Jul/23 08:37,20/Feb/20 23:43,4.0,4.0-alpha4,,,,,Tool/cqlsh,,,,0,,,,"When virtual tables were implemented ([CASSANDRA-7622]), cqlsh.py was updated to parse virtual keyspace metadata by making queries to the {{system_virtual_schema}} table and included a TODO:

{code:python}
# TODO remove after virtual tables are added to connection metadata
{code}

Since python driver 3.15.0 (released in August), the driver now parses virtual keyspace metadata.   It would be good to update the bundled python driver and simplify cqlsh code to utilize its capability to parse virtual tables.",,adutra,aholmber,andrew.tolbert,cnlwsu,dcapwell,djoshi,jeromatron,rustyrazorblade,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14512,,CASSANDRA-15578,,,,,,,,CASSANDRA-10190,CASSANDRA-15576,CASSANDRA-15578,,,,,,,,,,,,,,,,,,,,,0.0,djoshi,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 20 23:43:56 UTC 2020,,,,,,,,,,,"0|s008n4:",9223372036854775807,,,,,,,,,,,dcapwell,rustyrazorblade,,Normal,,4.0,,,https://github.com/apache/cassandra/commit/f7b3317379dc13aa62eb442741423a3300f0f8c6,,,,,,,,,Run cqlsh tests,,,,,"07/Oct/19 04:51;andrew.tolbert;Newer versions of the python driver also fix some issues around python3, so upgrading would also benefit [CASSANDRA-10190].;;;","09/Oct/19 14:34;cnlwsu;We need a new build of the driver for https://datastax-oss.atlassian.net/browse/PYTHON-1158 and also need at least 3.19 for https://datastax-oss.atlassian.net/browse/PYTHON-1118 or the virtual table stuff wont work even with new builds.;;;","19/Feb/20 23:31;djoshi;I have upgraded the driver to the latest 3.21 release and cleaned up some of the custom modifications we made. All cqlsh pass. Other tests also pass except 3 which are flakey. Here's the changes to [cassandra|https://github.com/apache/cassandra/compare/trunk...dineshjoshi:14872-trunk?expand=1], [cassandra-dtest|https://github.com/apache/cassandra-dtest/compare/master...dineshjoshi:14872-trunk?expand=1] and [CI|https://circleci.com/workflow-run/e6d00eb0-0c74-4078-89ec-4449256e387e].;;;","20/Feb/20 02:42;dcapwell;TR doesn't work

{code}
cqlsh> CREATE KEYSPACE bar WITH replication = {'class': 'NetworkTopologyStrategy', 'datacenter1': '3/1'};
cqlsh> DESCRIBE KEYSPACE bar;

'NoneType' object has no attribute 'export_for_schema'
{code};;;","20/Feb/20 02:46;dcapwell;vtables work

{code}
cqlsh> desc keyspaces;

system_virtual_schema  system_schema  system_views  system_distributed
bar                    system_auth    system        system_traces

cqlsh> describe system_views;

/*
Warning: Keyspace system_views is a virtual keyspace and cannot be recreated with CQL.
Structure, for reference:*/
// VIRTUAL KEYSPACE system_views;

/*
Warning: Table system_views.sstable_tasks is a virtual table and cannot be recreated with CQL.
Structure, for reference:
VIRTUAL TABLE system_views.sstable_tasks (
    keyspace_name text,
    table_name text,
    task_id uuid,
    kind text,
    progress bigint,
    total bigint,
    unit text,
    PRIMARY KEY (keyspace_name, table_name, task_id)
) WITH CLUSTERING ORDER BY (table_name ASC, task_id ASC)
    AND comment = 'current sstable tasks';
*/
...
cqlsh> use system_views;
cqlsh:system_views> desc tables;

sstable_tasks       clients                   coordinator_write_latency
disk_usage          local_write_latency       tombstones_per_read
thread_pools        internode_outbound        settings
local_scan_latency  coordinator_scan_latency  max_partition_size
internode_inbound   coordinator_read_latency  caches
local_read_latency  rows_per_read
cqlsh:system_views> desc rows_per_read;

/*
Warning: Table system_views.rows_per_read is a virtual table and cannot be recreated with CQL.
Structure, for reference:
VIRTUAL TABLE system_views.rows_per_read (
    keyspace_name text,
    table_name text,
    ""50th"" double,
    ""99th"" double,
    count bigint,
    max double,
    PRIMARY KEY ((keyspace_name, table_name))
) WITH comment = '';
*/
{code};;;","20/Feb/20 03:28;djoshi;[~dcapwell] thanks for testing. The latest `python-driver` doesn't seem to support DESCRIBE for transient replication enabled schemas. Originally I thought that the latest driver will fix the issue but it doesn't look like it. Lets not block on this. I'll reopen 15578 and work on when TR is added to the driver.

For the curious, the issue the way we describe TR enabled RF which breaks the parsing here: https://github.com/datastax/python-driver/blob/9297c74c9075b9eb732190f7f0bdf1b06493b808/cassandra/metadata.py#L503-L504;;;","20/Feb/20 06:58;andrew.tolbert;I remember trying transient replication with the java driver and it not working ([JAVA-2105|https://datastax-oss.atlassian.net/browse/JAVA-2105]), I suspect most/all client libraries that read schema need updates to parse it.;;;","20/Feb/20 07:11;andrew.tolbert;Changes look great!  Played around with it a little bit and it's working well.  I like that autocompletion works flawlessly for vtables :)

One thing that is interesting is that geomet is a required dependency.  Since it's apache licensed and seems pretty small that doesn't seem like much of a deal.  I think it could be made optional in the python driver (it's used for parsing DSE-specific geospatial types) so might be something worth pursuing on that end (but don't think it's needed as a prereq to this).;;;","20/Feb/20 07:37;djoshi;Thanks for checking it out Andy. I considered making geomet optional. It will need some driver changes and I am reluctant to check in a custom build of the driver. So this is an ok compromise for the time being. As an added benefit of leaving it in, users can use this cqlsh to connect to DSE and all features that the driver support will continue functioning including the geospatial types.;;;","20/Feb/20 07:44;andrew.tolbert;{quote}
It will need some driver changes and I am reluctant to check in a custom build of the driver. So this is an ok compromise for the time being
{quote}

+1 agree we should proceed with it included as it seems like a small self-contained dependency, and it is really nice to not use a custom build.;;;","20/Feb/20 20:21;rustyrazorblade;LGTM, +1.;;;","20/Feb/20 23:43;djoshi;Thanks every for the review and comments. Committed in [cassandra|https://github.com/apache/cassandra/commit/f7b3317379dc13aa62eb442741423a3300f0f8c6] and [cassandra-dtest|https://github.com/apache/cassandra-dtest/commit/18ccbb4d308c27b67a8d81a2c849dc27fc3e2b5c].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Severe concurrency issues in STCS,DTCS,TWCS,TMD.Topology,TypeParser",CASSANDRA-14871,13196766,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,snazy,snazy,snazy,07/Nov/18 08:37,15/May/20 07:59,13/Jul/23 08:37,01/Feb/19 11:35,3.0.18,3.11.4,4.0,4.0-alpha1,,,Legacy/Core,Local/Compaction,,,0,,,,"   There are a couple of places in the code base that do not respect that j.u.HashMap + related classes are not thread safe and some parts rely on internals of the implementation of HM, which can change.

We have observed failures like {{NullPointerException}} and  {{ConcurrentModificationException}} as well as wrong behavior.

Affected areas in the code base:
 * {{SizeTieredCompactionStrategy}}
 * {{DateTieredCompactionStrategy}}
 * {{TimeWindowCompactionStrategy}}
 * {{TokenMetadata.Topology}}
 * {{TypeParser}}
 * streaming / concurrent access to {{LifecycleTransaction}} (handled in CASSANDRA-14554)

While the patches for the compaction strategies + {{TypeParser}} are pretty straight forward, the patch for {{TokenMetadata.Topology}} requires it to be made immutable.",,aweisberg,bdeggleston,cscotta,djoshi,jeromatron,jkni,lizg,samt,snazy,tcooke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,snazy,,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,Challenging,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 01 11:35:43 UTC 2019,,,,,,,,,,,"0|s007qg:",9223372036854775807,,,,,,,,,bdeggleston,,bdeggleston,,,Critical,,3.0.0,,,,,,,,,,,,,,,,,"07/Nov/18 08:40;snazy;Patches:

|[3.0|https://github.com/snazy/cassandra/tree/14871-fix-concurrency-issues-3.0]|[3.11|https://github.com/snazy/cassandra/tree/14871-fix-concurrency-issues-3.11]|[trunk|https://github.com/snazy/cassandra/tree/14871-fix-concurrency-issues-trunk];;;","01/Dec/18 00:35;bdeggleston;I was taking a look at the 3.0 changes and had a few comments, I'll take a look at the other versions on Monday. Sorry if I'm stepping on your toes [~jkni], I'm also happy to take over the review if you're busy.

DateTieredCompactionStrategy
 * access to {{sstables}} is unguarded in {{getNextBackgroundSSTables}}

TokenMetadata
 * {{@VisibleForTesting}} annotation on topology field could be removed
 * Could you either use a read lock in {{TokenMetadata#getTopology}}, or rename it getTopologyUnsafe? I realize it’s not used in a way might have visibility issues at the moment, but it is silently deviating from the concurrency strategy used everywhere else for that field. It should either conform to the strategy, or have a name scary enough to make people read the method docs before using it. (I’d prefer just using a read lock for simplicity/consistency)
 * Since we’re now using copy on write for {{TokenMetadata.Topology}} changes and treating {{Topology}} as immutable, it would be clearer if the mutating methods were split out into a builder class and the Topology only exposed getter type methods. This might be overkill for 3.x, but it makes sense for trunk.;;;","03/Dec/18 16:11;jkni;No apology necessary, [~bdeggleston]. Thanks for taking a look! I'm busy enough that this hasn't made its way up my queue yet. If you have time/interest, please feel free to take over the review.;;;","04/Dec/18 23:37;bdeggleston;Thanks Joel, set myself as reviewer.

I spent some more time looking at the compaction strategy and TypeParser parts of the patch, and have some comments there as well.

[DateTiered/SizeTiered/TimeWindow]CompactionStrategy
 * access to sstables should be synchronized with the same granularity as the rest of the strategy state (ie: {{synchronized (this)}}). I don’t see any reason why we’d want to synchronize access to the sstable collection separately.

TypeParser
 * Again, if we’re using copy on write for the cache, we should make the cache immutable. Also we shouldn’t allocate a new map in the synchronized block unless we verify it’s still missing the key we’re interested in.

See previous comment for TokenMetadata;;;","05/Dec/18 21:36;bdeggleston;[~snazy] I've implemented my feedback here, let me know what you think.

|[3.0|https://github.com/bdeggleston/cassandra/tree/14871-3.0]|[3.11|https://github.com/bdeggleston/cassandra/tree/14871-3.11]|[trunk|https://github.com/bdeggleston/cassandra/tree/14871-trunk]|;;;","07/Dec/18 12:40;snazy;[~bdeggleston], thank you! Will take a look.;;;","07/Dec/18 12:56;snazy;Few comments:
 * I think [this lock|https://github.com/bdeggleston/cassandra/commit/7eeec9be03be6d326432fc715e9dce4b173acdf4#diff-b9ead760fa9628889810dd64e6507d9cR1279] has no ""real"" effect - mean, the method just returns the reference to {{topology}}. But we should make {{topology}} {{volatile}}. WDYT?
 * The builder-approach for {{Topology}} is nice!
 * Not sure how  [this|https://github.com/bdeggleston/cassandra/commit/0a8f3909098a233bca42d651b8242b288bb2c557#diff-052bdc412f1a356a3fb5409de51dceb5R108] could actually help. It's definitely fine as a safety net though. WDYT about replacing it with {{assert type != null : ""Parsing '"" + str + ""' yielded null, which is a bug"";}} right before the {{synchronized}}.
 * +1 on the other changes!

 ;;;","07/Dec/18 17:40;bdeggleston;thanks for taking a look Robert.

bq. I think this lock has no ""real"" effect - mean, the method just returns the reference to topology. But we should make topology volatile. WDYT?

Agreed, fixed.

bq. Not sure how  this could actually help. It's definitely fine as a safety net though. WDYT about replacing it with assert type != null : ""Parsing '"" + str + ""' yielded null, which is a bug""; right before the synchronized.

Right, it's just to quickly catch future bugs. I'd prefer to keep it as is if that's ok with you. Right now it's verifying the entire un-cached case up to and including the caching of the type. Regarding using {{assert}}, I avoid it outside of tests because asserts can be disabled, and {{Preconditions}} and {{Verify}} do a better job of communicating what you're checking (imo).;;;","09/Dec/18 10:14;lizg;sorry , wrong click , change status to ""ready to commit"" , i don't know how to rollback;;;","09/Dec/18 16:03;aweisberg;NP. Workflow -> Cancel Commit.;;;","10/Dec/18 12:55;snazy;I'm fine with using {{Verify}}, but it should include the string that yielded {{null}}. Like {{Verify.verify(type != null, ""Parsing %s yielded null, which is a bug"", str)}} and move that after the assignment of {{type}}. That way we don't need the {{cache.get(str)}} in the synchronized block, because {{type}} is then guaranteed to be non-null.

EDIT: So instead of
{code:java}
       synchronized (TypeParser.class)
       {
            if (!cache.containsKey(str))
            {
                ImmutableMap.Builder<String, AbstractType<?>> builder = ImmutableMap.builder();
                builder.putAll(cache).put(str, type);
                cache = builder.build();
            }
            AbstractType<?> rtype = cache.get(str);
            Verify.verify(rtype != null);
            return rtype;
        }
{code}
like this:
{code:java}
        if (!isEOS(str, i) && str.charAt(i) == '(')
            type = getAbstractType(name, new TypeParser(str, i));
        else
            type = getAbstractType(name);
        Verify.verify(type != null, ""Parsing %s yielded null, which is a bug"", str);

       // <comments>
       synchronized (TypeParser.class)
       {
            if (!cache.containsKey(str))
            {
                ImmutableMap.Builder<String, AbstractType<?>> builder = ImmutableMap.builder();
                builder.putAll(cache).put(str, type);
                cache = builder.build();
            }
            return type;
        }
{code}
;;;","12/Dec/18 23:23;bdeggleston;pushed up Verify fix;;;","14/Dec/18 12:47;snazy;Why the new {{Verify.verify()}}? Mean, it looks very paranoid like you don't trust the map-builder.;;;","14/Dec/18 16:02;bdeggleston;Mainly a check against future bugs. Like I said earlier, the benefit of putting the verify at the end there in the first place is to serve as a check against the entire method. I guess if it really bothers you, you can remove it. I don't think we should spend much more time discussing the specifics of verify statements though.;;;","07/Jan/19 22:48;bdeggleston;[~snazy] is this ready to commit?;;;","29/Jan/19 00:29;bdeggleston;[~snazy] ping;;;","29/Jan/19 09:37;snazy;Ah, sorry, missed your ping.

I've incorporated the changes into my branches and going to kick-off CI.

If you're ok, I'd go ahead, squash the changes and commit it.;;;","29/Jan/19 12:07;snazy;CI results [trunk|https://circleci.com/gh/snazy/cassandra/84] [3.11|https://circleci.com/gh/snazy/cassandra/83] [3.0|https://circleci.com/gh/snazy/cassandra/82];;;","31/Jan/19 00:01;bdeggleston;[~snazy] go for it;;;","01/Feb/19 11:35;snazy;Thanks for the review!

Committed as [bc18b4dd4e33020d0d58c3701077d0af5c39bce6|https://github.com/apache/cassandra/commit/bc18b4dd4e33020d0d58c3701077d0af5c39bce6] to [cassandra-3.0|https://github.com/apache/cassandra/tree/cassandra-3.0], [merged|https://github.com/apache/cassandra/commit/16c96c20dadfbda98d4b5daf7f6c169b691459b9] to [cassandra-3.11|https://github.com/apache/cassandra/tree/cassandra-3.11], [merged|https://github.com/apache/cassandra/commit/ef6c5f8ba9dc18e7a5bacfd5d8461ae5d9f12df4] to [trunk|https://github.com/apache/cassandra/tree/trunk].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The order of application of nodetool garbagecollect is broken,CASSANDRA-14870,13196765,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,blambov,blambov,blambov,07/Nov/18 08:22,15/May/20 08:03,13/Jul/23 08:37,13/Nov/18 11:10,3.11.4,4.0,4.0-alpha1,,,,Local/Compaction,,,,0,,,,"{{nodetool garbagecollect}} was intended to work from oldest sstable to newest, so that the collection in newer tables can purge tombstones over data that has been deleted.

However, {{SSTableReader.maxTimestampComparator}} currently sorts in the opposite order (the order changed in CASSANDRA-13776 and then back in CASSANDRA-14010), which makes the garbage collection unable to purge any tombstones.",,blambov,cscotta,jasonstack,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14099,,,,,,,,,,,,,,,,,,,,,,,,,0.0,blambov,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 13 11:07:54 UTC 2018,,,,,,,,,,,"0|s007q8:",9223372036854775807,,,,,,,,,jasonstack,,jasonstack,,,Normal,,,,,,,,,,,,,,,,,,,"07/Nov/18 12:36;blambov;Patch for 3.11 linked, applies with insignificant changes to trunk too. Incorporates CASSANDRA-14099 as that required the same change of order.
The LCS ordering test (by [~VincentWhite]) is copied from the CASSANDRA-14099 patch.;;;","08/Nov/18 08:34;blambov;testall and dtests are clean on DataStax CI servers.;;;","13/Nov/18 07:39;jasonstack;Sorry for delay.. Patch LGTM!;;;","13/Nov/18 11:07;blambov;Committed to 3.11 as [a03424ef95559c9df2bb7f86e1ac1edca1436058|https://github.com/apache/cassandra/commit/a03424ef95559c9df2bb7f86e1ac1edca1436058] and trunk as [b80f6c65fb0b97a8c79f6da027deac06a4af9801|https://github.com/apache/cassandra/commit/b80f6c65fb0b97a8c79f6da027deac06a4af9801].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Range.subtractContained produces incorrect results when used on full ring,CASSANDRA-14869,13196753,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,Gerrrr,Gerrrr,Gerrrr,07/Nov/18 07:10,01/Aug/21 12:24,13/Jul/23 08:37,19/Nov/18 08:03,3.0.18,3.11.7,4.0,4.0-alpha1,,,Legacy/Core,,,,0,,,,"Currently {{Range.subtractContained}} returns incorrect results if minuend range covers full ring and:
* subtrahend range wraps around. For example, {{(50, 50] - (10, 100]}} returns {{\{(50,10], (100,50]\}}} instead of {{(100,10]}}
* subtrahend range covers the full ring as well. For example {{(50, 50] - (0, 0]}} returns {{\{(0,50], (50,0]\}}} instead of {{\{\}}}",,aleksey,cscotta,Gerrrr,ifesdjeen,jeromatron,jjirsa,tcooke,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Nov/18 07:09;Gerrrr;range bug.jpg;https://issues.apache.org/jira/secure/attachment/12947191/range+bug.jpg",,,,,,,,,,,,,1.0,Gerrrr,,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,Challenging,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 19 08:02:40 UTC 2018,,,,,,,,,,,"0|s007nk:",9223372036854775807,,,,,,,,,,,ifesdjeen,,,Critical,,3.0.0,,,,,,,,,,,,,,,,,"07/Nov/18 07:14;Gerrrr;Patches:
* [3.0 | https://github.com/Gerrrr/cassandra/tree/14869-3.0]
* [3.11 | https://github.com/Gerrrr/cassandra/tree/14869-3.11]
* [4.0 | https://github.com/Gerrrr/cassandra/tree/14869-4.0];;;","12/Nov/18 09:50;ifesdjeen;Several remarks: 
  * I would extract [this check|https://github.com/Gerrrr/cassandra/commit/f92047ab378062e58d02d7f57e0694ba2e3c90a7#diff-b6aa8cb091f4de56555d650df9db6ca6R282] to {{isFull}} method.
  * if you're already using {{return}} there's no need for {{else if}}
  * Maybe add tests for {{subtract}} also not only {{subtractAll}}
  * Make sure full range subtraction is covered, like {{range(0,0).subtract(range(1,1))}} should be empty
  * Still mention of ArrayList [here|https://github.com/Gerrrr/cassandra/commit/f92047ab378062e58d02d7f57e0694ba2e3c90a7#diff-b6aa8cb091f4de56555d650df9db6ca6R277] 

In summary, there are two cases that are broken right now: subtracting one full range from another {{range(0,0).subtract(1,1)}} does not yield an empty range, and subtracting a non-wrapping range from wrapping one: {{range(0,0).subtract(-1, 1)}} is yielding two ranges which wrap incorrectly. Even though patch does fix both issues description does not fully elaborate on issue and added if cases might use a small elaboration comment (same as issue description).;;;","13/Nov/18 17:32;Gerrrr;Thanks for the review! I've fixed the patches according to your suggestions.

{quote}
Make sure full range subtraction is covered, like range(0,0).subtract(range(1,1)) should be empty
{quote}

I believe this case is covered by [that test|https://github.com/Gerrrr/cassandra/commit/9e4afefa1dd892a22197c7296ec1d9cd13ad5ae0#diff-5b7f5ad57ef5c28db2566ce381294b0dR404].



;;;","14/Nov/18 09:30;ifesdjeen;Did you run dtests results for this change across all branches?;;;","14/Nov/18 21:43;jjirsa;([~ifesdjeen], you could just push it into a branch and run it through circleci just to have a passing dtest link for posterity)


;;;","15/Nov/18 18:46;ifesdjeen;[~jjirsa] sure; I did trigger tests as I wrote the comment.

[~Gerrrr] looks like there's a failure in unit tests: https://circleci.com/gh/ifesdjeen/cassandra/782;;;","16/Nov/18 14:51;Gerrrr;[~ifesdjeen] Thanks for running the builds! I've tested all failing tests locally on {{cassandra-3.0}} and {{trunk}} right before my commit and they still fail in the same way.;;;","16/Nov/18 20:08;ifesdjeen;Builds: 

|[3.0|https://circleci.com/workflow-run/dec6ad52-121f-4bd6-92f9-95d52b041ec9]|[3.11|https://circleci.com/workflow-run/02340ed3-5b7f-4e8e-a51f-2192cdd93612]|[trunk|https://circleci.com/workflow-run/7de63da3-3977-4ace-aeef-be5430b9bc37]|;;;","19/Nov/18 08:02;ifesdjeen;[~Gerrrr] thank you for the patch!

Committed to [3.0|https://git1-us-west.apache.org/repos/asf?p=cassandra.git;a=commit;h=c6f822c2a07e0e7c8e4af72523fe62d181c71e56] and merged up to [3.11|https://git1-us-west.apache.org/repos/asf?p=cassandra.git;a=commit;h=78c7d57ebb28ac688cd287d7d8b8f483a99d0135] and [trunk|https://git1-us-west.apache.org/repos/asf?p=cassandra.git;a=commit;h=13108037177a30e103a84bca5dadb38d1c090453].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issue a CQL native protocol warning if SASI indexes are enabled on a table,CASSANDRA-14866,13196254,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,adelapena,adelapena,adelapena,05/Nov/18 13:52,27/Oct/20 12:23,13/Jul/23 08:37,19/Feb/19 16:17,3.11.5,4.0,4.0-alpha1,,,,Feature/SASI,,,,0,pull-request-available,,,"If someone enables SASI indexes then we should return a native protocol warning that will be printed by cqlsh saying that they are beta quality still and you need to be careful with using them in production.

This is motivated not only by [the existing bugs and limitations|https://issues.apache.org/jira/browse/CASSANDRA-12674?jql=project%20%3D%20CASSANDRA%20AND%20status%20%3D%20Open%20AND%20component%20%3D%20sasi] but for the fact that they haven't been extensively tested yet.",,adelapena,aleksey,eperott,jasonstack,jeromatron,jjirsa,jolynch,n.v.harikrishna,snazy,tcooke,,,,,,,,,,,,,,,,"adelapena commented on pull request #44: Adapt tests to materialized views disabled by default (CASSANDRA-14866)
URL: https://github.com/apache/cassandra-dtest/pull/44
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Feb/19 15:27;githubbot;600","adelapena commented on pull request #44: Adapt tests to materialized views disabled by default (CASSANDRA-14866)
URL: https://github.com/apache/cassandra-dtest/pull/44
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Feb/19 16:16;githubbot;600","adelapena commented on pull request #48: Fix wrong `enable_materialized_views` config property on version < 3.0 (CASSANDRA-14866)
URL: https://github.com/apache/cassandra-dtest/pull/48
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Mar/19 16:08;githubbot;600","michaelsembwever closed pull request #48:
URL: https://github.com/apache/cassandra-dtest/pull/48


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Oct/20 12:23;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,adelapena,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun May 19 08:54:24 UTC 2019,,,,,,,,,,,"0|s004l4:",9223372036854775807,,,,,,,,,snazy,,snazy,,,Normal,,,,,,,,,,,,,,,,,,,"05/Nov/18 18:28;adelapena;Patch: 
||[3.11|https://github.com/adelapena/cassandra/tree/CASSANDRA-14866-3.11]||[trunk|https://github.com/adelapena/cassandra/tree/CASSANDRA-14866-trunk]||;;;","05/Nov/18 19:44;snazy;+1;;;","06/Nov/18 05:17;jjirsa;Seems like an odd time for this, but other retroactively ""experimental"" features came with a way to toggle them off in the yaml rather than a per-user warning on CQL statements and discussion on the mailing list before they were committed.

Also, do we REALLY need to send the client warning on {{SELECT}} ? Seems like a big hammer there. ;;;","07/Nov/18 10:31;adelapena;[~jjirsa] We don't really need the client warning on {{SELECT}}, probably the warning on {{CREATE INDEX}} is enough. I'm not so sure about the usefulness of the {{cassandra.yaml}} flag, but indeed it would be more aligned with the MV behaviour.

Here is an alternative version of the patch warning only on {{CREATE INDEX}}, and with a new {{enable_sasi_indexes}} property in {{cassandra.yaml}} (enabled by default):
||[3.11|https://github.com/adelapena/cassandra/tree/CASSANDRA-14866-mv-3.11]||[trunk|https://github.com/adelapena/cassandra/tree/CASSANDRA-14866-mv-trunk]||

[Here|https://github.com/adelapena/cassandra-dtest/commit/f0eeb1140678f3fe48129441b6dde4b36f8901eb] are a couple of dtests for the warnings and the property. These are our very first SASI dtests so I have put them in a separate {{sasi_test.py}} file.

[~snazy] WDYT?;;;","19/Nov/18 19:46;adelapena;I'm adding a couple of unit tests for the client warning and the config flag, they are very similar to the dtests above.;;;","09/Jan/19 13:33;snazy;+1 on the latest changes (haven't noticed the update on this ticket).;;;","11/Jan/19 13:28;adelapena;[~snazy] I have just updated the patch set SASI as disabled by default on trunk. This has involved a few minor changes in unit tests. I think that with the new unit tests we don't require the extra dtests anymore. CI looks good to me.;;;","11/Jan/19 14:15;jjirsa;As already mentioned in a previous comment, this deserves a mailing list thread. That’s doubly true if you’re proposing changing the default
;;;","11/Jan/19 14:21;aleksey;It's one thing to issue a warning here - which, to be clear, I'm not against at all - but an altogether qualitatively different thing to disable the feature by default.

I would argue that the latter, in 2019 Cassandra universe, should require a discussion on the dev list first.;;;","14/Jan/19 19:23;adelapena;I have just sent [a mail|https://lists.apache.org/thread.html/280722c7f4b8f4a7a78b7413dc41b09f649a43d4fb0d760c44d08e4c@%3Cdev.cassandra.apache.org%3E] to dev list.;;;","04/Feb/19 15:18;adelapena;I've updated the patch to disable both {{enable_materialized_views}} and {{enable_sasi_indexes}} by default in trunk, as discussed in the mail list:
||[3.11|https://github.com/adelapena/cassandra/tree/CASSANDRA-14866-3.11]||[trunk|https://github.com/adelapena/cassandra/tree/CASSANDRA-14866-trunk]||[dtest|https://github.com/adelapena/cassandra-dtest/commits/CASSANDRA-14866]||

I've moved {{enable_materialized_views}}, {{enable_sasi_indexes}} and {{enable_transient_replication}} to an experimental features section in cassandra.yaml.

Each dtest creating materialized views is modified to activate the flag, I don't know if there is a better way to do this.;;;","18/Feb/19 20:28;snazy;+1 Thanks for the patch!;;;","19/Feb/19 16:16;adelapena;Thanks for reviewing.

Committed to 3.11 as [e6a61be8c857106d5d99a270b2d17de9f84c4d67|https://github.com/apache/cassandra/commit/e6a61be8c857106d5d99a270b2d17de9f84c4d67] and merged to trunk.

Dtests committed as [227ae5e7aca25c668340e467a35dfb0f7e1546fa|https://github.com/apache/cassandra-dtest/commit/227ae5e7aca25c668340e467a35dfb0f7e1546fa].;;;","06/Mar/19 05:47;jolynch;[~adelapena] I think that this may have broken 2.2 dtests in test teardown. Can you try running 2.2 dtests on circle and see if they pass for you?

For me I get a lot of ""test teardown failure"" because of the enable_materialized_views included in the yaml now:
{noformat}
test teardown failure
Unexpected error found in node logs (see stdout for full details). Errors: [ERROR [main] 2019-03-06 05:05:04,639 CassandraDaemon.java:670 - Exception encountered during startup
org.apache.cassandra.exceptions.ConfigurationException: Invalid yaml. Please remove properties [enable_materialized_views] from your cassandra.yaml
	at org.apache.cassandra.config.YamlConfigurationLoader$MissingPropertiesChecker.check(YamlConfigurationLoader.java:146) ~[main/:na]
	at org.apache.cassandra.config.YamlConfigurationLoader.loadConfig(YamlConfigurationLoader.java:113) ~[main/:na]
	at org.apache.cassandra.config.YamlConfigurationLoader.loadConfig(YamlConfigurationLoader.java:85) ~[main/:na]
	at org.apache.cassandra.config.DatabaseDescriptor.loadConfig(DatabaseDescriptor.java:151) ~[main/:na]
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:133) ~[main/:na]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:531) [main/:na]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:657) [main/:na], ERROR [main] 2019-03-06 05:05:04,639 CassandraDaemon.java:670 - Exception encountered during startup
org.apache.cassandra.exceptions.ConfigurationException: Invalid yaml. Please remove properties [enable_materialized_views] from your cassandra.yaml
	at org.apache.cassandra.config.YamlConfigurationLoader$MissingPropertiesChecker.check(YamlConfigurationLoader.java:146) ~[main/:na]
	at org.apache.cassandra.config.YamlConfigurationLoader.loadConfig(YamlConfigurationLoader.java:113) ~[main/:na]
	at org.apache.cassandra.config.YamlConfigurationLoader.loadConfig(YamlConfigurationLoader.java:85) ~[main/:na]
	at org.apache.cassandra.config.DatabaseDescriptor.loadConfig(DatabaseDescriptor.java:151) ~[main/:na]
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:133) ~[main/:na]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:531) [main/:na]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:657) [main/:na]]
{noformat}

For example, a run after your dtest change: [73 fails|https://circleci.com/gh/jolynch/cassandra/524#tests/containers/9]
Versus a run without your dtest change: [1 fail|https://circleci.com/gh/jolynch/cassandra/528#tests/containers/74];;;","18/Mar/19 16:16;adelapena;Right, the updated dtests fail with pre-3.0 versions due to the {{enable_materialized_views}} property.

[This PR|https://github.com/apache/cassandra-dtest/pull/48] should solve the problem, circleci results [here|https://circleci.com/gh/adelapena/cassandra/61#tests/containers/2].

[~snazy] could you please take a look?;;;","19/May/19 08:54;eperott;Would be nice to get this merged. But shouldn't the materialized-views setting be enabled for 3.0 as well?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix incorrect sorting of replicas in SimpleStrategy.calculateNaturalReplicas,CASSANDRA-14862,13195548,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jolynch,jolynch,jolynch,01/Nov/18 03:49,15/May/20 08:03,13/Jul/23 08:37,08/Nov/18 02:47,4.0,4.0-alpha1,,,,,Cluster/Membership,Consistency/Coordination,,,0,4.0-QA,,,"The sorting of natural replicas in {{SimpleStrategy.calculateNaturalReplicas}} committed as part of [e645b917|https://github.com/apache/cassandra/commit/e645b9172c5d50fc2af407de724e46121edfe109#diff-0e1563a70b49cd81e9e11b4ddad15cf2L68] for CASSANDRA-14726 has broken the {{TestTopology.test_size_estimates_multidc}} dtest ([example run|https://circleci.com/gh/jolynch/cassandra/245#tests/containers/48]) as the ""primary"" ranges have now changed. I'm actually surprised only a single dtest fails as I believe we've broken multi-dc {{SimpleStrategy}} reasonably badly.

In particular the {{SimpleStrategy.calculateNaturalReplicas}} method cannot sort the endpoints by datacenter first. It has to leave them in the order that it found them else change which replicas are considered ""primary"" replicas (which mostly impacts repair and size estimates and the such).

I have written a regression unit test for the SimpleStrategy and am running it through circleci now. Will post the patch shortly.",,aleksey,benedict,jolynch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jolynch,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 08 02:47:07 UTC 2018,,,,,,,,,,,"0|s0008g:",9223372036854775807,,,,,,,,,benedict,,benedict,,,Low,,,,,,,,,,,,,,,,,,,"01/Nov/18 03:59;jolynch;||trunk||
|[patch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14862]|
|dtests:[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14862.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14862]|

Patch is up, tests are running. [~benedict] do you think you can review?;;;","01/Nov/18 09:44;benedict;Can we not break this dependency?  It seems that simply selecting the endpoint with the minimum token in ({{getPrimaryRangesForEndpoint}} and {{getPrimaryRangeForEndpointWithinDC}}) for any returned by {{calculateNaturalReplicas}} would suffice?

I'm OK with also rolling back the change, as I'm not sure I even intended to leave it in (it's a very limited impact optimisation, that might yield dividends later). 

But, I would rather we also found these dependencies and broke them.  Failing that, we need to document them.  This is an unnecessarily brittle state of affairs.;;;","01/Nov/18 11:10;benedict;Ah, nevermind - that's not going to be easy to do.  I guess let's simply comment {{calculateNaturalEndpoints}} to specify that the output order is defined as the token order of the replicas, and that these two methods depend on this.

+1;;;","01/Nov/18 17:36;jolynch;Ok, I just pushed an updated commit ([0d9e09fc|https://github.com/apache/cassandra/commit/0d9e09fccc0acacc1b52e8d00dd0b8bfe86c81b0]) to my branch which clears up the {{calculateNaturalReplicas}} docstring and lists the implicit dependence. dtests are re-running.

;;;","02/Nov/18 18:52;jolynch;From Benedict on IRC:
{noformat}
> I might change the language slightly, as I don't think it's 100% clear they should be returned in the sequence they occur following the search token (like, 95% clear but you need to think about it), but the important point (that it's depended on) is plenty clear.
> I'd probably personally have written something like ""Calculate the natural endpoints for the given token. They are returned in the order they occur in the ring following the search token, as defined by the replication strategy"", but I don't think it's terribly important{noformat}

Based on this feedback I have pushed an amended commit to ([cafd44c8|https://github.com/jolynch/cassandra/commit/cafd44c8d9ae24c953a8d82746fc89bfe2465641].;;;","08/Nov/18 02:47;aleksey;Committed to trunk as [2adfa92044381aa9093104f3a105f3dbd7dda94c|https://github.com/apache/cassandra/commit/2adfa92044381aa9093104f3a105f3dbd7dda94c], thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstable min/max metadata can cause data loss,CASSANDRA-14861,13195507,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,bdeggleston,bdeggleston,bdeggleston,31/Oct/18 22:05,15/May/20 08:04,13/Jul/23 08:37,06/Nov/18 19:26,3.0.18,3.11.4,4.0,4.0-alpha1,,,Local/SSTable,,,,0,,,,"There’s a bug in the way we filter sstables in the read path that can cause sstables containing relevant range tombstones to be excluded from reads. This can cause data resurrection for an individual read, and if compaction timing is right, permanent resurrection via read repair. 

We track the min and max clustering values when writing an sstable so we can avoid reading from sstables that don’t contain the clustering values we’re looking for in a given read. The min max for each clustering column are updated for each row / RT marker we write. In the case of range tombstones markers though, we only update the min max for the clustering values they contain, which is almost never the full set of clustering values. This leaves a min/max that are above/below (respectively) the real ranges covered by the range tombstone contained in the sstable.

For instance, assume we’re writing an sstable for a table with 3 clustering values. The current min clustering is 5:6:7. We write an RT marker for a range tombstone that deletes any row with the value 4 in the first clustering value so the open marker is [4:]. This would make the new min clustering 4:6:7 when it should really be 4:. If we do a read for clustering values of 4:5 and lower, we’ll exclude this sstable and it’s range tombstone, resurrecting any data there that this tombstone would have deleted.",,aleksey,bdeggleston,benedict,cscotta,jay.zhuang,jeromatron,jjirsa,KurtG,mck,samt,tcooke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,Correctness -> Transient Incorrect Response,,,,,,,,Challenging,Fuzz Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 06 19:26:04 UTC 2018,,,,,,,,,,,"0|i3zv8n:",9223372036854775807,,,,,,,,,,,benedict,samt,,Critical,,3.0.0,,,,,,,,,,,,,,,,,"01/Nov/18 17:00;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/14861-3.0]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14861-3.0]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/14861-3.11]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14861-3.11]|
|[trunk|https://github.com/bdeggleston/cassandra/tree/14861-trunk]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/14861-trunk]|

This adds a minor sstable version to 3.x and changes 2 behaviors. First, when reading metadata for pre-md sstables, -only the first clustering value is loaded into the min/max values and the rest are discarded- min max values are discarded. When writing new sstables, the size of the min/max values written are limited by the length of the shortest RT clustering.

edit: min max values from legacy sstables need to be discarded, otherwise open ended RTs (ie: DELETE WHERE c < 100) would still have this problem.;;;","01/Nov/18 17:52;bdeggleston;[~benedict], [~beobal], [~iamaleksey] would one or more of you be interested in reviewing?;;;","01/Nov/18 21:12;benedict;Sure, happy to;;;","02/Nov/18 11:18;benedict;Really nice catch.  Unfortunately, I *think* this still leaves some incidences of this problem unhandled.  If I understand the patch correctly, and the way we do sstable filtering (it's been a while since I looked closely at it), we would still permit the following sequence of events to slip by:
 # RT[(a, b)..(x, x)]
 # Query for clustering (b, a)

 ;;;","02/Nov/18 15:10;samt;Benedict's understanding is correct, the issue is that when we check whether a slice intersects with the min/max values, each component is compared in isolation. We need to also remove the ""minor optimization"" from Slice::intersects or switch to min/max clusterings, rather than components.;;;","02/Nov/18 18:03;bdeggleston;Good catch, pushed up the fix for that;;;","06/Nov/18 18:16;samt;+1;;;","06/Nov/18 19:07;benedict;+1;;;","06/Nov/18 19:26;bdeggleston;committed as {{d60c78358b6f599a83f3c112bfd6ce72c1129c9f}}, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Message Flusher scheduling fell off the event loop, resulting in out of memory",CASSANDRA-14855,13194748,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sumanth.pasupuleti,sumanth.pasupuleti,sumanth.pasupuleti,29/Oct/18 05:49,16/Mar/22 12:07,13/Jul/23 08:37,06/Dec/18 15:59,3.0.18,,,,,,Messaging/Client,,,,0,pull-request-available,,,"We recently had a production issue where about 10 nodes in a 96 node cluster ran out of heap. 

From heap dump analysis, I believe there is enough evidence to indicate `queued` data member of the Flusher got too big, resulting in out of memory.
Below are specifics on what we found from the heap dump (relevant screenshots attached):
* non-empty ""queued"" data member of Flusher having retaining heap of 0.5GB, and multiple such instances.
* ""running"" data member of Flusher having ""true"" value
* Size of scheduledTasks on the eventloop was 0.

We suspect something (maybe an exception) caused the Flusher running state to continue to be true, but was not able to schedule itself with the event loop.
Could not find any ERROR in the system.log, except for following INFO logs around the incident time.


{code:java}
INFO [epollEventLoopGroup-2-4] 2018-xx-xx xx:xx:xx,592 Message.java:619 - Unexpected exception during request; channel = [id: 0x8d288811, L:/xxx.xx.xxx.xxx:7104 - R:/xxx.xx.x.xx:18886]
io.netty.channel.unix.Errors$NativeIoException: readAddress() failed: Connection timed out
 at io.netty.channel.unix.Errors.newIOException(Errors.java:117) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.unix.Errors.ioResult(Errors.java:138) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.unix.FileDescriptor.readAddress(FileDescriptor.java:175) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.epoll.AbstractEpollChannel.doReadBytes(AbstractEpollChannel.java:238) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:926) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:397) [netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:302) [netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) [netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) [netty-all-4.0.44.Final.jar:4.0.44.Final]
{code}

I would like to pursue the following proposals to fix this issue:
# ImmediateFlusher: Backport trunk's ImmediateFlusher ( [CASSANDRA-13651|https://issues.apache.org/jira/browse/CASSANDRA-13651] https://github.com/apache/cassandra/commit/96ef514917e5a4829dbe864104dbc08a7d0e0cec)  to 3.0.x and maybe to other versions as well, since ImmediateFlusher seems to be more robust than the existing Flusher as it does not depend on any running state/scheduling.
# Make ""queued"" data member of the Flusher bounded to avoid any potential of causing out of memory due to otherwise unbounded nature.


",,benedict,cscotta,jay.zhuang,jeromatron,sumanth.pasupuleti,tcooke,zznate,,,,,,,,,,,,,,,,,,,"Github user sumanth-pasupuleti commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/293#discussion_r235090149
  
    --- Diff: conf/cassandra.yaml ---
    @@ -1003,3 +1003,9 @@ windows_timer_interval: 1
     # An interval of 0 disables any wait time, which is the behavior of former Cassandra versions.
     #
     # otc_backlog_expiration_interval_ms: 200
    +
    +# Define use of immediate flusher for replies to TCP connections. This is an alternate simplified flusher that does not
    +# depend on any event loop scheduling. Details around why this has been backported from trunk: CASSANDRA-14855.
    +# Default is false.
    +# native_transport_use_immediate_flusher: false
    --- End diff --
    
    other more descriptive name I had in mind was ""native_transport_use_immediate_flusher_in_place_of_batches_flusher"", but felt it could be too long a name
;20/Nov/18 17:05;githubbot;600","Github user belliottsmith commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/293#discussion_r235729649
  
    --- Diff: conf/cassandra.yaml ---
    @@ -1003,3 +1003,9 @@ windows_timer_interval: 1
     # An interval of 0 disables any wait time, which is the behavior of former Cassandra versions.
     #
     # otc_backlog_expiration_interval_ms: 200
    +
    +# Define use of immediate flusher for replies to TCP connections. This is an alternate simplified flusher that does not
    +# depend on any event loop scheduling. Details around why this has been backported from trunk: CASSANDRA-14855.
    +# Default is false.
    +# native_transport_use_immediate_flusher: false
    --- End diff --
    
    perhaps native_transport_flush_messages_immediately ?
;22/Nov/18 13:46;githubbot;600","smiklosovic closed pull request #293:
URL: https://github.com/apache/cassandra/pull/293


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 12:07;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/18 05:48;sumanth.pasupuleti;blocked_thread_pool.png;https://issues.apache.org/jira/secure/attachment/12945991/blocked_thread_pool.png","29/Oct/18 05:48;sumanth.pasupuleti;cpu.png;https://issues.apache.org/jira/secure/attachment/12945990/cpu.png","29/Oct/18 05:48;sumanth.pasupuleti;eventloop_scheduledtasks.png;https://issues.apache.org/jira/secure/attachment/12945989/eventloop_scheduledtasks.png","29/Oct/18 05:48;sumanth.pasupuleti;flusher running state.png;https://issues.apache.org/jira/secure/attachment/12945988/flusher+running+state.png","29/Oct/18 05:48;sumanth.pasupuleti;heap.png;https://issues.apache.org/jira/secure/attachment/12945986/heap.png","29/Oct/18 05:48;sumanth.pasupuleti;heap_dump.png;https://issues.apache.org/jira/secure/attachment/12945987/heap_dump.png","29/Oct/18 05:48;sumanth.pasupuleti;read_latency.png;https://issues.apache.org/jira/secure/attachment/12945985/read_latency.png",,,,,,,7.0,sumanth.pasupuleti,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 06 16:46:41 UTC 2018,,,,,,,,,,,"0|i3zqkf:",9223372036854775807,,,,,,,,,,,benedict,,,Normal,,,,,,,,,,,,,,,,,,,"01/Nov/18 05:35;sumanth.pasupuleti;Backport of ImmediateFlusher to 3.0 https://github.com/sumanth-pasupuleti/cassandra/tree/3.0_backport_immediate_flusher

Passing UTs: https://circleci.com/gh/sumanth-pasupuleti/cassandra/139;;;","09/Nov/18 16:46;zznate;[~sumanth.pasupuleti] Thanks for the detailed analysis and patch. Was this the first time your team has seen this or has it potentially manifested in other clusters previously?;;;","09/Nov/18 21:26;sumanth.pasupuleti;[~zznate] Yes, this was the first time we saw this (and there was a second incident that followed with similar characteristics on the same cluster). This happened only on one cluster (this is our most read heavy 3.0 CQL cluster), and following are the characteristics:
* 3.0.17 C* version
* CQL
* Relatively high read traffic (~60k rps at peak at coordinator level)
* Has client side wire compression (LZ4) enabled
* Total outbound traffic of ~4Gbps across the cluster;;;","09/Nov/18 23:36;benedict;Thanks [~sumanth.pasupuleti] for the diagnosis and patch.

I haven't as yet reviewed it, but I think we need to consider whether changing the default is acceptable.  We have to weigh the chance of this bug versus the chance of detrimental performance impact to any existing users.  Unfortunately we cannot easily put a number to either risk (at least, not without significant effort).

In an ideal world, we would modify the existing flusher to avoid accumulating messages (introducing a bound on the number of waiting messages, or their size), but that might be overkill when we anticipate retiring the old flusher.  It shouldn't be too onerous, however.

I would personally find any of the options somewhat acceptable, including introducing this change, or doing so without changing the default property.  This would leave users exposed to this (presumably uncommon) bug, but with an easy option for fixing it by switching the config property.

It's not entirely clear to me which is the superior option, so input from anyone else would be welcome.;;;","19/Nov/18 23:56;sumanth.pasupuleti;Appreciate your thoughts [~benedict] . Trying to figure out a way forward since there have not been inputs from anyone else.

I also like the suggestion of keeping the existing flusher ON by default, and making ImmediateFlusher usage optional (through yaml property like native_transport_flush_immediate which is set to false by default) - I can work on a patch for that. Let me know.;;;","20/Nov/18 10:53;benedict;That seems like a reasonable path forward to me, yes.  I'll review the patch once you have it prepared.

I'd propose using a more descriptive name for the property, though, that's prefixed by 'native_transport';;;","20/Nov/18 17:10;sumanth.pasupuleti;[~benedict] Here is the updated patch for review - [https://github.com/apache/cassandra/pull/293/files], Passing UTs: [https://circleci.com/gh/sumanth-pasupuleti/cassandra/185];;;","06/Dec/18 15:59;benedict;Committed as [fff6eec2903ee85f648535dd051c9bc72631f524|https://github.com/apache/cassandra/commit/fff6eec2903ee85f648535dd051c9bc72631f524] to 3.0 and merged upwards.

[~sumanth.pasupuleti]: On reflection, the parameter names should ideally remain the same for both 3.0 and 3.11, so I have reverted to the names in Michael's original patch.  I have simply changed the default value to {{true}} until trunk.

I also removed the text from the cassandra.yaml; not all options are listed in the yaml explicitly, and it's probably not worth including this until it's truly legacy (i.e. in 4.0).  Anybody can fill in the parameter without the prompt if they need it.

Let me know if you disagree with any of this, but wanted to not delay any further the commit.;;;","06/Dec/18 16:46;sumanth.pasupuleti;I agree with you [~benedict]. Thanks for committing. I was going to update this JIRA with yet another finding we had in production a few days ago (waiting on collecting relevant screenshots of heap dumps, etc), on the same cluster, where CPU got hogged by ""something"", and messages were not getting flushed from ImmediateFlusher either (my theory is lack of CPU) resulting in OOM. As you were suggesting earlier in this JIRA, will look into adding a bound to the queue of items waiting to be flushed - this would apply to trunk as well, so will spin off a different JIRA for that.
In parallel, looking into automating taking of flamegraphs during such incident, so that we would have leads into what was actually hogging the CPU.

Let me know if you have any thoughts/suggestions around this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
some empty/invalid bounds aren't caught by SelectStatement,CASSANDRA-14849,13194237,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,25/Oct/18 20:10,15/May/20 08:03,13/Jul/23 08:37,13/Dec/18 22:23,4.0,4.0-alpha1,,,,,Legacy/CQL,Legacy/Local Write-Read Paths,,,0,,,,"Nonsensical clustering bounds like ""c >= 100 AND c < 100"" aren't converted to Slices.NONE like they should be. Although this seems to be completely benign, it is technically incorrect and complicates some testing since it can cause memtables and sstables to return different results for the same data for these bounds in some cases.",,aleksey,bdeggleston,tcooke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 13 22:23:45 UTC 2018,,,,,,,,,,,"0|i3znf3:",9223372036854775807,,,,,,,,,aleksey,,aleksey,,,Normal,,,,,,,,,,,,,,,,,,,"25/Oct/18 20:13;bdeggleston;[trunk|https://github.com/bdeggleston/cassandra/tree/14849-trunk]
[circle|https://circleci.com/workflow-run/8f1492ae-d04e-4880-a93b-b9ff891d855d];;;","29/Oct/18 14:02;aleksey;I think the patch is correct, but I have a few issues with it.

1. We are inlining a copy of {{ClusteringComparator.compare()}}, ish, into {{Slice.isEmpty()}}. If the former changes somehow, there is a risk of forgetting to apply the difference to the inlined version.
 2. There is duplication of work. After making the regular {{compare()}} call, we are going through the motions again in the common case.
 3. There are different returns with some nesting involved that makes it a bit trickier to follow than necessary.

I *think* essentially we are just lacking a {{cmp == 0 && one of the bounds is exclusive}} condition, and the whole method can be simplified quite a bit (relatively). Pushed an illustration/review branch with those issues handled [here|https://github.com/iamaleksey/cassandra/commits/14849-review].;;;","29/Oct/18 21:02;bdeggleston;Nice, that's much more succinct. Pushed up your changes, plus some expansion of the unit test.;;;","30/Oct/18 15:01;aleksey;+1;;;","13/Dec/18 22:23;bdeggleston;Committed to trunk as [a41b861fa4d4acfbcce13dd62b1e8f48be22f8ed|https://github.com/apache/cassandra/commit/a41b861fa4d4acfbcce13dd62b1e8f48be22f8ed], thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drop/add column name with different Kind can result in corruption,CASSANDRA-14843,13193916,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,benedict,benedict,benedict,24/Oct/18 16:44,02/Aug/19 02:44,13/Jul/23 08:37,29/Nov/18 14:56,3.0.18,,,,,,CQL/Semantics,Local/SSTable,,,0,,,,"While we have always imposed that the type of any column name remains consistent, we have not done the same for its kind.  If a column’s kind is changed via Drop/Add, the SerializationHeader’s column sets will be interpreted incorrectly, and may either lead to CorruptSSTableException or silent data corruption.

The problem occurs in SerializationHeader.Component.toHeader().  In this method, we lookup columns from the metadata only by name, ignoring the static status.  If a column of the same name but different kind to our dropped column exists in the schema, we will only add this column to the list of fields we expect.  So we will be missing a column from our set of expected static columns.  We will also add the regular variant to the set of regular columns present, which it may not be.

There are a number of ways this can play out:

1) There are no other static columns in the affected sstables.  In this case we will incorrectly treat the table as containing no static data, so we will not attempt to read any static row.  This leaves a static row to be read as the first regular row, and will throw the exception in the description.
1a) If for some reason the static row is absent - say, it has expired - then depending on lexicographical ordering, and if the sstable did not contain any instance of the regular variant, we may misattribute column data.  This will probably result in corruption exceptions, but if the columns were of the same type it could be undetectable.
2) There are other static columns on the table, and in the affected sstables.  In this case, the row will be correctly treated as a static row, but depending on the dropped column’s lexicographical position relative to these, it may result in column data being misattributed to other columns.  This *could* be undetectable corruption, if the column types were the same.
3) Either of these above scenario could be replayed with the static/regular relations replaced.

CASSANDRA-14591 would protect against 1a and 2.

Probably the safest and correct fix is to prevent adding a column back with a different kind to the original, however this would require storing more metadata about dropped columns.  This is probably viable for 4.0, but for 3.0 perhaps too invasive. 

It is quite easy to make this particular method safe against this particular corruption.  However it is hard to be certain there aren’t other places where assumptions were made about a name being of only one kind.  There are only a handful of places that *should* need to be of concern, specifically those places that invoke CFMetaData.getDroppedColumn, so we should be fairly confident that - if these call sites are safe - we are now insulated.  This might suffice for 3.0.  Thoughts?
",,aleksey,benedict,cscotta,KurtG,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14948,,,,,,,,,,,,,,,,,,,,,,,0.0,benedict,,,,,,,,,,,,Correctness -> Recoverable Corruption / Loss,,,,,,,,Challenging,Fuzz Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 29 14:56:33 UTC 2018,,,,,,,,,,,"0|i3zlfr:",9223372036854775807,,,,,,,,,,,samt,,,Normal,,3.0.0,,,,,,,,,,,,,,,,,"24/Oct/18 17:12;aleksey;Yep.

Too invasive for 3.0, but trivial to prevent re-addition of columns with a different kind in 4.0, as we do store the kind of dropped columns in 4.0 since CASSANDRA-9425.;;;","27/Nov/18 19:39;benedict;[3.0 patch|https://github.com/belliottsmith/cassandra/tree/14843], [CI|https://circleci.com/gh/belliottsmith/cassandra/tree/14843];;;","29/Nov/18 11:18;samt;+1 - there's an additional/unexpected dtest failure in {{putget_test.py::TestPutGet::test_wide_slice}}, but I've observed this failing on 3.0 yesterday, so I'm planning to investigate that separately.;;;","29/Nov/18 14:56;benedict;Thanks.  Committed as [4b1f40d5382638bf3913293b713d5d22b57c844d|https://github.com/apache/cassandra/commit/4b1f40d5382638bf3913293b713d5d22b57c844d] to 3.0 and up.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSL connection problems when upgrading to 4.0 when upgrading from 3.0.x,CASSANDRA-14842,13193828,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,tommy_s,tommy_s,tommy_s,24/Oct/18 10:49,15/May/20 08:41,13/Jul/23 08:37,11/Mar/20 16:39,4.0,4.0-alpha4,,,,,Messaging/Internode,,,,0,,,,"While testing to upgrade from 3.0.15 to 4.0 the old nodes fails to connect to the 4.0 node, I get this exception on the 4.0 node:

 
{noformat}
2018-10-22T11:57:44.366+0200 ERROR [MessagingService-NettyInbound-Thread-3-8] InboundHandshakeHandler.java:300 Failed to properly handshake with peer /10.216.193.246:58296. Closing the channel.
io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: SSLv2Hello is disabled
at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:459)
at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265)
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1434)
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:965)
at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:808)
at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:417)
at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:317)
at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884)
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
at java.lang.Thread.run(Thread.java:748)
Caused by: javax.net.ssl.SSLHandshakeException: SSLv2Hello is disabled
at sun.security.ssl.InputRecord.handleUnknownRecord(InputRecord.java:637)
at sun.security.ssl.InputRecord.read(InputRecord.java:527)
at sun.security.ssl.EngineInputRecord.read(EngineInputRecord.java:382)
at sun.security.ssl.SSLEngineImpl.readRecord(SSLEngineImpl.java:962)
at sun.security.ssl.SSLEngineImpl.readNetRecord(SSLEngineImpl.java:907)
at sun.security.ssl.SSLEngineImpl.unwrap(SSLEngineImpl.java:781)
at javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:624)
at io.netty.handler.ssl.SslHandler$SslEngineType$3.unwrap(SslHandler.java:294)
at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1275)
at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1177)
at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1221)
at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:489)
at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:428)
... 14 common frames omitted{noformat}
In the server encryption options on the 4.0 node I have both ""enabled and ""enable_legacy_ssl_storage_port"" set to true so it should accept incoming connections on the ""ssl_storage_port"".

 ",,cscotta,eperott,jeromatron,KurtG,n.v.harikrishna,tommy_s,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,tommy_s,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 06 11:42:01 UTC 2020,,,,,,,,,,,"0|i3zkw7:",9223372036854775807,4.0,,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"24/Oct/18 10:50;tommy_s;I this config on the 4.0 node:
{noformat}
storage_port: 12700
ssl_storage_port: 12701

server_encryption_options:
     # set to true for allowing secure incoming connections
     enabled: true
     # If enabled and optional are both set to true, encrypted and unencrypted connections are handled on the storage_port
     optional: false
     # if enabled, will open up an encrypted listening socket on ssl_storage_port. Should be used
     # during upgrade to 4.0; otherwise, set to false.
     enable_legacy_ssl_storage_port: true
     # on outbound connections, determine which type of peers to securely connect to. 'enabled' must be set to true.
     internode_encryption: all
     keystore: /usr/share/cassandra/.ssl/cil-intern/server/keystore.jks
     keystore_password: '*********'
     truststore: /usr/share/cassandra/.ssl/cil-intern/server/truststore.jks
     truststore_password: '**********'
     # More advanced defaults below:
     protocol: TLSv1.2
     # store_type: JKS
     cipher_suites: [TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA]
     # require_client_auth: false
     # require_endpoint_verification: false{noformat};;;","24/Oct/18 14:25;tommy_s;The issue when upgrading from 3.0.x still remains the same. I activated wire trace in {{NettyFactory.java}} to get some more logging.
{noformat}
2018-10-24T15:13:31.724+0200 [MessagingService-NettyInbound-Thread-3-3] INFO i.n.u.internal.logging.Slf4JLogger:101 info [id: 0x68a0cdd6, L:/10.216.193.242:12701 - R:/10.216.193.243:60911] REGISTERED
2018-10-24T15:13:31.725+0200 [MessagingService-NettyInbound-Thread-3-3] INFO i.n.u.internal.logging.Slf4JLogger:101 info [id: 0x68a0cdd6, L:/10.216.193.242:12701 - R:/10.216.193.243:60911] ACTIVE
2018-10-24T15:13:31.725+0200 [MessagingService-NettyInbound-Thread-3-3] INFO i.n.u.internal.logging.Slf4JLogger:101 info [id: 0x68a0cdd6, L:/10.216.193.242:12701 - R:/10.216.193.243:60911] USER_EVENT: SslHandshakeCompletionEvent(javax.net.ssl.SSLHandshakeException: SSLv2Hello is disabled)
2018-10-24T15:13:31.725+0200 [MessagingService-NettyInbound-Thread-3-3] INFO i.n.u.internal.logging.Slf4JLogger:121 info [id: 0x68a0cdd6, L:/10.216.193.242:12701 ! R:/10.216.193.243:60911] EXCEPTION: io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: SSLv2Hello is disabled
io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: SSLv2Hello is disabled
at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:459)
at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265)
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1434)
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:965)
at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:808)
at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:417)
at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:317)
at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884)
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
at java.lang.Thread.run(Thread.java:748)
Caused by: javax.net.ssl.SSLHandshakeException: SSLv2Hello is disabled
at sun.security.ssl.InputRecord.handleUnknownRecord(InputRecord.java:637)
at sun.security.ssl.InputRecord.read(InputRecord.java:527)
at sun.security.ssl.EngineInputRecord.read(EngineInputRecord.java:382)
at sun.security.ssl.SSLEngineImpl.readRecord(SSLEngineImpl.java:962)
at sun.security.ssl.SSLEngineImpl.readNetRecord(SSLEngineImpl.java:907)
at sun.security.ssl.SSLEngineImpl.unwrap(SSLEngineImpl.java:781)
at javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:624)
at io.netty.handler.ssl.SslHandler$SslEngineType$3.unwrap(SslHandler.java:294)
at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1275)
at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1177)
at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1221)
at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:489)
at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:428)
... 14 common frames omitted
2018-10-24T15:13:31.725+0200 [MessagingService-NettyInbound-Thread-3-3] ERROR o.a.c.n.a.InboundHandshakeHandler:300 exceptionCaught Failed to properly handshake with peer /10.216.193.243:60911. Closing the channel.
io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: SSLv2Hello is disabled
at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:459)
at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265)
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1434)
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:965)
at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:808)
at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:417)
at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:317)
at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884)
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
at java.lang.Thread.run(Thread.java:748)
Caused by: javax.net.ssl.SSLHandshakeException: SSLv2Hello is disabled
at sun.security.ssl.InputRecord.handleUnknownRecord(InputRecord.java:637)
at sun.security.ssl.InputRecord.read(InputRecord.java:527)
at sun.security.ssl.EngineInputRecord.read(EngineInputRecord.java:382)
at sun.security.ssl.SSLEngineImpl.readRecord(SSLEngineImpl.java:962)
at sun.security.ssl.SSLEngineImpl.readNetRecord(SSLEngineImpl.java:907)
at sun.security.ssl.SSLEngineImpl.unwrap(SSLEngineImpl.java:781)
at javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:624)
at io.netty.handler.ssl.SslHandler$SslEngineType$3.unwrap(SslHandler.java:294)
at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1275)
at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1177)
at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1221)
at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:489)
at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:428)
... 14 common frames omitted
2018-10-24T15:13:31.725+0200 [MessagingService-NettyInbound-Thread-3-3] INFO i.n.u.internal.logging.Slf4JLogger:101 info [id: 0x68a0cdd6, L:/10.216.193.242:12701 ! R:/10.216.193.243:60911] CLOSE
2018-10-24T15:13:31.725+0200 [MessagingService-NettyInbound-Thread-3-3] INFO i.n.u.internal.logging.Slf4JLogger:101 info [id: 0x68a0cdd6, L:/10.216.193.242:12701 ! R:/10.216.193.243:60911] READ COMPLETE
2018-10-24T15:13:31.725+0200 [MessagingService-NettyInbound-Thread-3-3] INFO i.n.u.internal.logging.Slf4JLogger:101 info [id: 0x68a0cdd6, L:/10.216.193.242:12701 ! R:/10.216.193.243:60911] USER_EVENT: SslCloseCompletionEvent(java.nio.channels.ClosedChannelException)
2018-10-24T15:13:31.726+0200 [MessagingService-NettyInbound-Thread-3-3] INFO i.n.u.internal.logging.Slf4JLogger:101 info [id: 0x68a0cdd6, L:/10.216.193.242:12701 ! R:/10.216.193.243:60911] INACTIVE
2018-10-24T15:13:31.726+0200 [MessagingService-NettyInbound-Thread-3-3] INFO i.n.u.internal.logging.Slf4JLogger:101 info [id: 0x68a0cdd6, L:/10.216.193.242:12701 ! R:/10.216.193.243:60911] CLOSE
2018-10-24T15:13:31.726+0200 [MessagingService-NettyInbound-Thread-3-3] INFO i.n.u.internal.logging.Slf4JLogger:101 info [id: 0x68a0cdd6, L:/10.216.193.242:12701 ! R:/10.216.193.243:60911] UNREGISTERED
{noformat}
I will see if I can enable the ssl debugging also.;;;","25/Oct/18 11:42;tommy_s;Could this be related to CASSANDRA-8265 ""Disable SSLv3 for POODLE""?;;;","26/Oct/18 13:58;tommy_s;I turned on ssl debugging. I'm no ssl expert in any way so I'm not sure how to read these logs but I think I found a few interesting points at least.

When a 3.0.x node successfully connects to another 3.0.x node it looks like this:
{noformat}
MessagingService-Outgoing-/10.216.193.246, WRITE: TLSv1.2 Handshake, length = 124
[write] MD5 and SHA1 hashes: len = 53
0000: 01 03 03 00 0C 00 00 00 20 00 C0 2B 00 C0 2F 00 ........ ..+../.
0010: 00 2F 00 00 FF 5B D3 05 0C 45 21 16 60 C9 F6 34 ./...[...E!.`..4
0020: 53 FD 7F 55 B7 CD DB 23 D5 0D D2 E5 07 29 0A 57 S..U...#.....).W
0030: 76 12 4B 1A 2A v.K.*
MessagingService-Outgoing-/10.216.193.246, WRITE: SSLv2 client hello message, length = 53
[Raw write]: length = 55
0000: 80 35 01 03 03 00 0C 00 00 00 20 00 C0 2B 00 C0 .5........ ..+..
0010: 2F 00 00 2F 00 00 FF 5B D3 05 0C 45 21 16 60 C9 /../...[...E!.`.
0020: F6 34 53 FD 7F 55 B7 CD DB 23 D5 0D D2 E5 07 29 .4S..U...#.....)
0030: 0A 57 76 12 4B 1A 2A .Wv.K.*
[Raw read]: length = 5
0000: 16 03 03 11 9B .....{noformat}
When a 4.0 node successfully connect to another 4.0 node I found this:
{noformat}
MessagingService-NettyOutbound-Thread-4-1, WRITE: TLSv1.2 Handshake, length = 124
[Raw write]: length = 129
0000: 16 03 03 00 7C 01 00 00 78 03 03 5B D3 0F 98 3F ........x..[...?
0010: B4 85 6E 5B 01 1F 73 7B 51 42 15 73 64 30 36 06 ..n[..s.QB.sd06.
0020: 69 0F B9 E2 C3 F6 80 92 CF 36 D2 00 00 06 C0 2B i........6.....+
0030: C0 2F 00 2F 01 00 00 49 00 0A 00 16 00 14 00 17 ././...I........
0040: 00 18 00 19 00 09 00 0A 00 0B 00 0C 00 0D 00 0E ................
0050: 00 16 00 0B 00 02 01 00 00 0D 00 1C 00 1A 06 03[Raw write]: length = 7
..[Raw write]: length = 7{noformat}
When a 3.0.x node fails to connect to a 4.0 node I got this:
{noformat}
MessagingService-Outgoing-/10.216.193.242, WRITE: TLSv1.2 Handshake, length = 124
[write] MD5 and SHA1 hashes: len = 53
0000: 01 03 03 00 0C 00 00 00 20 00 C0 2B 00 C0 2F 00 ........ ..+../.
0010: 00 2F 00 00 FF 5B D3 05 0D 29 40 68 39 E9 7F 3F ./...[...)@h9..?
0020: 39 FD 41 C9 98 4A 6F D2 99 46 AD A3 F9 56 36 8B 9.A..Jo..F...V6.
0030: 0F 42 87 D4 F5 .B...
MessagingService-Outgoing-/10.216.193.242, WRITE: SSLv2 client hello message, length = 53
[Raw write]: length = 55
0000: 80 35 01 03 03 00 0C 00 00 00 20 00 C0 2B 00 C0 .5........ ..+..
0010: 2F 00 00 2F 00 00 FF 5B D3 05 0D 29 40 68 39 E9 /../...[...)@h9.
0020: 7F 3F 39 FD 41 C9 98 4A 6F D2 99 46 AD A3 F9 56 .?9.A..Jo..F...V
0030: 36 8B 0F 42 87 D4 F5 6..B...
[Raw read]: length = 5
0000: 15 03 03 00 02 .....
[Raw read]: length = 2
0000: 02 0A ..
MessagingService-Outgoing-/10.216.193.242, READ: TLSv1.2 Alert, length = 2
MessagingService-Outgoing-/10.216.193.242, RECV TLSv1.2 ALERT: fatal, unexpected_message
MessagingService-Outgoing-/10.216.193.242, called closeSocket(){noformat}
Protocol and cipher_suites are configure the same in all nodes
{noformat}
protocol: TLSv1.2
cipher_suites: [TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA]{noformat};;;","31/Oct/18 12:19;tommy_s;To troubleshot this further I added a little custom logging. In {{NettyFactory.InboundInitializer.initChannel()}} I added:
{code:java}
logger.info(""inbound enabled {}"", Arrays.asList(sslHandler.engine().getEnabledProtocols()));
logger.info(""inbound supported {}"", Arrays.asList(sslHandler.engine().getSupportedProtocols()));
{code}
And in {{NettyFactory.OutboundInitializer.initChannel()}} I added:
{code:java}
logger.info(""outbound enabled {}"", Arrays.asList(sslHandler.engine().getEnabledProtocols()));
logger.info(""outbound supported {}"", Arrays.asList(sslHandler.engine().getSupportedProtocols()));
{code}
In the log I get:
{noformat}
2018-10-30T13:49:57.120+0100 [MessagingService-NettyOutbound-Thread-4-4] INFO o.a.c.n.a.NettyFactory$OutboundInitializer:375 initChannel outbound enabled [TLSv1, TLSv1.1, TLSv1.2]
2018-10-30T13:49:57.120+0100 [MessagingService-NettyOutbound-Thread-4-4] INFO o.a.c.n.a.NettyFactory$OutboundInitializer:376 initChannel outbound supported [SSLv2Hello, SSLv3, TLSv1, TLSv1.1, TLSv1.2]
2018-10-30T13:49:57.184+0100 [MessagingService-NettyInbound-Thread-3-3] INFO o.a.c.n.a.NettyFactory$InboundInitializer:297 initChannel inbound enabled [TLSv1, TLSv1.1, TLSv1.2]
2018-10-30T13:49:57.184+0100 [MessagingService-NettyInbound-Thread-3-3] INFO o.a.c.n.a.NettyFactory$InboundInitializer:298 initChannel inbound supported [SSLv2Hello, SSLv3, TLSv1, TLSv1.1, TLSv1.2]{noformat}
So SSLv2Hello is not enabled. After a bit of investigation it seams that the default in Netty is {{[TLSv1, TLSv1.1, TLSv1.2]}}, [Netty default protocol|https://github.com/netty/netty/blob/4.1/handler/src/main/java/io/netty/handler/ssl/JdkSslContext.java#L111].

Also I don't think the {{protocol}} parameter in the {{server_encryption_options}} is used in this context, regardless of what I set it to in the yaml file I get the Netty default as enabled protocols.

 ;;;","01/Nov/18 07:59;tommy_s;I changed {{SSLFactory.createNettySslContext()}} so {{SSLv2Hello}} is enabled on server sockets. This solved the connection problem but I run in to the problem I reported in CASSANDRA-14848 instead (but that was not unexpected).

I think we have to enable {{SSLv2Hello}} on server sockets. If there is some reason that we really don't won't to do that at least it must be enabled on the {{ssl_storage_port}} to allow upgrades from 3.0.x.;;;","09/Nov/18 14:37;tommy_s;I have created a patch that sets {{SSLv2Hello}} as enabled protocol on {{ssl_storage_port}} only.

Its available on my branch [cassandra-14842|https://github.com/tommystendahl/cassandra/tree/cassandra-14842].;;;","13/Nov/18 01:23;spod;Thanks for the detailed error report, Tommy. I might have a chance to look at this next week.;;;","04/Mar/20 15:52;brandon.williams;Tommy, is this still an issue after CASSANDRA-15066?  If so, can you rebase?;;;","06/Mar/20 11:42;tommy_s;[~brandon.williams] I tested to upgrade from 3.0.20 to 4.0-alpha3 with the same setup and its working now so I think we can consider this issue fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Don't write to system_distributed.repair_history, system_traces.sessions, system_traces.events in mixed version 3.X/4.0 clusters",CASSANDRA-14841,13193805,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aweisberg,tommy_s,tommy_s,24/Oct/18 09:36,15/May/20 07:59,13/Jul/23 08:37,01/Nov/18 16:13,4.0,4.0-alpha1,,,,,Legacy/Distributed Metadata,,,,0,,,,"When upgrading from 3.x to 4.0 I get exceptions in the old nodes once the first 4.0 node starts up. I have tested to upgrade from both 3.0.15 and 3.11.3 and get the same problem.

 
{noformat}
2018-10-22T11:12:05.060+0200 ERROR [MessagingService-Incoming-/10.216.193.244] CassandraDaemon.java:228 Exception in thread Thread[MessagingService-Incoming-/10.216.193.244,5,main]
java.lang.RuntimeException: Unknown column coordinator_port during deserialization
at org.apache.cassandra.db.Columns$Serializer.deserialize(Columns.java:452) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.db.filter.ColumnFilter$Serializer.deserialize(ColumnFilter.java:482) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:760) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:697) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.io.ForwardingVersionedSerializer.deserialize(ForwardingVersionedSerializer.java:50) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.net.MessageIn.read(MessageIn.java:123) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:192) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:180) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:94) ~[apache-cassandra-3.11.3.jar:3.11.3]{noformat}
I think it was introduced by CASSANDRA-7544.

 ",,aweisberg,djoshi,jjirsa,KurtG,tommy_s,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14897,,,CASSANDRA-14864,,,,,,,,,,,,,,,,,,,,,,,0.0,aweisberg,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 13 09:51:44 UTC 2018,,,,,,,,,,,"0|i3zkr3:",9223372036854775807,4.0,,,,,,,,djoshi,,djoshi,,,Normal,,,,,,,,,,,,,,,,,,,"24/Oct/18 09:37;tommy_s;Server encryption was disabled when I tested this.;;;","26/Oct/18 19:43;aweisberg;[~iamaleksey] in IRC
{noformat}
10:29 AM nope, that's safe, assuming you mean 'system'; 'system_schema' is also fine. things in system_distributed and system_traces is in the danger zone
10:29 AM ok, I see the problem
10:33 AM don't necessarily need new tables, but you can't just add a column to one of those, no
10:33 AM what I'm not seeing is what is doing the read
10:33 AM nothing in the code does it as far as I can see
10:33 AM code only writes
10:34 AM which would also be a problem if a writes comes from a 4.0 node in the mixed-mode
10:35 AM also do: read the comment for StorageService.maybeAddOrUpdateKeyspace()
10:35 AM and MigrationManager.forceAnnounceNewTable() that it calls
10:37 AM on startup 4.0 detects that the keyspace and the table exist, but the definitions are different, so it tries to force the migration with the new definition; that however will not be propagated to 3.x nodes because there is a schema barrier in place between different major-version nodes
10:38 AM that works as intended more or less, just annoying. although at least one problem is that the same 0 timestamp is reused, and for conflicting table params the new definition might never actually be applied depending on old/new value
10:39 AM that timestamp needs to be incremented on every major
10:39 AM I mentioned this before in one of the JIRAs
10:40 AM aweisberg: TL;DR is that reads and writes involving the new column in 4.0 will fail on 3.0 side. not much you can do about it here
10:40 AM could avoid using those columns until the whole cluster is on the latest major
{noformat}

We don't support cross version streaming so people can't run repair anyways. At least not unless they are fairly sophisticated about how they invoke repair and what order they bounce nodes. They also probably can't be using vnodes. Maybe it's fine if this generates an error until the cluster is upgraded?

[~tommy_s] is it possible you have an external process that is reading from the repair history table that is causing this error to appear?

WRT to tracing. Creating a new table and writing to both seems a little sketchy since it will double the overhead of tracing even when not upgrading. Upgrade would also need to do a migration of data from the old table into the new table, and we can only perform the migration once all nodes are upgraded. I suppose after the cluster is not mixed version we could stop writing to the old table.

I'm not sure if the fix here should be documentation in NEW.TXT or documentation + changes to avoid generating errors in mixed version clusters by not doing the writes.;;;","26/Oct/18 19:54;jjirsa;Would personally prefer not generating errors in mixed versions. A lot of people log or alert on errors, and we know that we CAN avoid this, so we SHOULD. We've done far more impactful workarounds in the past (see: 2.x -> 3.0 schema upgrades and 2.1 -> 2.2 system_auth changes), so going down that path seems like the right approach.;;;","26/Oct/18 20:21;tommy_s;[~aweisberg] We have an external repair scheduler that reads the repair history, I think I forgot to disable that before starting to upgrade. I didn't make the connection when I saw the exception but your probably right.;;;","29/Oct/18 19:32;aweisberg;[trunk code|https://github.com/apache/cassandra/compare/trunk...aweisberg:14841-trunk?expand=1]
[CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/14841-trunk];;;","31/Oct/18 22:58;djoshi;+1 LGTM;;;","01/Nov/18 14:21;tommy_s;But this means I can't read the repair history while I have mixed versions, It seams to work from the old version but using cqlsh on the 4.0 node I get this:
{noformat}
cassandra@cqlsh> select keyspace_name , columnfamily_name, coordinator FROM system_distributed.repair_history LIMIT 1 ;
OperationTimedOut: errors={'10.216.193.242': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=10.216.193.242:12742
cassandra@cqlsh> select * FROM system_distributed.repair_history LIMIT 1 ;
OperationTimedOut: errors={'10.216.193.242': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=10.216.193.242:12742{noformat}
and in the log on the old version I get the exception:
{noformat}
2018-11-01T14:28:23.623+0100 [MessagingService-Incoming-/10.216.193.242] ERROR o.a.c.service.CassandraDaemon$2:223 uncaughtException Exception in thread Thread[MessagingService-Incoming-/10.216.193.242,5,main]
java.lang.RuntimeException: Unknown column coordinator_port during deserialization
at org.apache.cassandra.db.Columns$Serializer.deserialize(Columns.java:433) ~[apache-cassandra-3.0.17.jar:3.0.17]
at org.apache.cassandra.db.filter.ColumnFilter$Serializer.deserialize(ColumnFilter.java:447) ~[apache-cassandra-3.0.17.jar:3.0.17]
at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:661) ~[apache-cassandra-3.0.17.jar:3.0.17]
at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:598) ~[apache-cassandra-3.0.17.jar:3.0.17]
at org.apache.cassandra.io.ForwardingVersionedSerializer.deserialize(ForwardingVersionedSerializer.java:50) ~[apache-cassandra-3.0.17.jar:3.0.17]
at org.apache.cassandra.net.MessageIn.read(MessageIn.java:98) ~[apache-cassandra-3.0.17.jar:3.0.17]
at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:201) ~[apache-cassandra-3.0.17.jar:3.0.17]
at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:178) ~[apache-cassandra-3.0.17.jar:3.0.17]
at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:92) ~[apache-cassandra-3.0.17.jar:3.0.17]{noformat};;;","01/Nov/18 14:52;aweisberg;Yes you won't be able to get repair_history while it is mixed version. We don't support cross version streaming so in a mixed version cluster you can't run repair anyways. Will that be an issue?;;;","01/Nov/18 15:13;tommy_s;Maybe I'm just picky. I agree we can't run repair in mixed version but does that mean I should not read repair history?

Will this be an issue? Maybe not. In our case we have a system for monitoring the status of the cluster which among other things reads from the repair history but I guess we could make it possible to disable that during upgrade. Monitoring the state of repairs while having mixed versions doesn't make much sens anyway.;;;","01/Nov/18 15:40;aweisberg;[~tommy_s] I polled [~iamaleksey] and [~jjirsa] and neither was in favor of having the read path automatically recognize this scenario and remove the column.

The recommendation is to avoid using ""select *"" which is dangerous if you are accessing by index since columns may get silently re-ordered as new columns are added. I will add a comment to NEWS.txt warning people about this.;;;","01/Nov/18 16:13;aweisberg;Committed as [877b08eaf0e02542c9f6d9f8cd457a8e44b4febf|https://github.com/apache/cassandra/commit/877b08eaf0e02542c9f6d9f8cd457a8e44b4febf]. Thanks!;;;","01/Nov/18 19:47;tommy_s;[~aweisberg] What you say makes good sens, I'm satisfied. Thanks.;;;","02/Nov/18 15:27;aweisberg;Tommy pointed out that due to CASSANDRA-6588 even if you select specific columns the queries will fail. I don't think that changes the outcome we will aim for, but I will have to update the warning in NEWS.txt.;;;","06/Nov/18 11:49;tommy_s;I think there is another scenario we have to consider, if we have several DC's its possible to do repair with in one dc and we would only need the nodes within that DC to be on the same version, if there are another version in another DC it should not be a problem.

And its also possible to create the new columns manually before starting the upgrade, in this case both reads and writes will work.

So just blocking the writes while having mixed versions might not be so good. Its only in the DC I'm currently upgrading that I can't run repairs but I would not be able to monitor the repair status in DC's with the new version.

I think we should make it possible to override the blocking with some configuration so you can decide to write repair history despite having mixed versions. Or remove the blocking of writes and document in the NEWS.txt how to create the new columns before starting to upgrade.;;;","06/Nov/18 19:15;aweisberg;Why would this block repairs and why would it stop you from monitoring repairs? Wouldn't you use nodetool repair admin to monitor repairs?

For repair it is just a history table. It's only populated after repair has already occurred.

I'm not sure users can alter the schema of the distributed system tables. @imaleksey is that actually possible?;;;","06/Nov/18 20:19;tommy_s;We don't use nodetool for repair, we have a repair scheduler that triggers repair jobs via jmx. The scheduling is based on repair history, checking for failed repairs that needs to be retried, how long ago something was repaired and if something is completely missing it has not been repaired at all. There is no big issue shunting down this for a while during an upgrade but I don't want to do that for a longer time then a need to, if I can do it per DC it would help.

Yes, you can alter the schema of the system distributed tables. Previously you could just do {{ALTER TABLE}} but since a few versions back its a little bit more complected, now you have to do {{INSERT}} in the system schema tables and use {{nodetool reloadlocalschema}}.;;;","12/Nov/18 16:39;aweisberg;Aleksey pointed out that in mixed version clusters we could still write to the tables and just omit the port column. Then you could add the columns and be able to read from the table. [~tommy_s] can you add as a comment a description of the steps to add the column?;;;","13/Nov/18 09:51;tommy_s;[~aweisberg] here is the steps to add the new columns before you start to upgrade.

For 3.0.15 and 3.11.1 and older versions:
{noformat}
cqlsh> ALTER TABLE system_distributed.repair_history ADD coordinator_port int;
cqlsh> ALTER TABLE system_distributed.repair_history ADD participants_v2 set<text>;{noformat}
For 3.0.16 and 3.11.2 and newer:
{noformat}
cqlsh> INSERT INTO system_schema.columns (keyspace_name , table_name , column_name , clustering_order , column_name_bytes , kind , position , type ) VALUES ( 'system_distributed', 'repair_history', 'coordinator_port', 'none', 0x636f6f7264696e61746f725f706f7274, 'regular', -1, 'int');
cqlsh> INSERT INTO system_schema.columns (keyspace_name , table_name , column_name , clustering_order , column_name_bytes , kind , position , type ) VALUES ( 'system_distributed', 'repair_history', 'participants_v2', 'none', 0x7061727469636970616e74735f7632, 'regular', -1, 'set<text>');
cqlsh> exit
$ nodetool reloadlocalschema{noformat}
Remember that the INSERT's and {{nodetool reloadschema}} must be done on the same node.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dropped columns can cause reverse sstable iteration to return prematurely,CASSANDRA-14838,13193659,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,bdeggleston,bdeggleston,bdeggleston,23/Oct/18 19:45,15/May/20 08:03,13/Jul/23 08:37,29/Oct/18 02:45,3.0.18,3.11.4,4.0,4.0-alpha1,,,Local/SSTable,,,,0,,,,"CASSANDRA-14803 fixed an issue where reading legacy sstables in reverse could return early in certain cases. It's also possible to get into this state with current version sstables if there are 2 or more indexed blocks in a row that only contain data for a dropped column. Post 14803, this will throw an exception instead of returning an incomplete response, but it should just continue reading like it does for legacy sstables",,aleksey,bdeggleston,jeromatron,KurtG,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,Correctness -> Transient Incorrect Response,,,,,,,,Challenging,Fuzz Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 29 02:45:36 UTC 2018,,,,,,,,,,,"0|i3zjvj:",9223372036854775807,,,,,,,,,samt,,samt,,,Critical,,3.0.0,,,,,,,,,,,,,,,,,"23/Oct/18 19:53;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/14838-3.0]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14838-3.0]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/14838-3.11]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14838-3.11]|
|[trunk|https://github.com/bdeggleston/cassandra/tree/14838-trunk]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14838-trunk]|;;;","26/Oct/18 10:41;samt;+1. Only nit is the unnecessary throws clauses in the test setup method declarations.;;;","29/Oct/18 02:45;bdeggleston;committed as e4bac44a04d59d93f622d91ef40b462250dac613, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect log entry during startup in 4.0,CASSANDRA-14836,13193536,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,tommy_s,tommy_s,tommy_s,23/Oct/18 10:52,16/Apr/19 09:29,13/Jul/23 08:37,23/Oct/18 20:45,,,,,,,Local/Startup and Shutdown,,,,0,,,,"When doing some testing on 4.0 I found this in the log:
{noformat}
2018-10-12T14:06:14.507+0200  INFO [main] StartupClusterConnectivityChecker.java:113 After waiting/processing for 10005 milliseconds, 1 out of 3 peers (0.0%) have been marked alive and had connections established{noformat}
1 out of 3 is not 0%:)",,aweisberg,cnlwsu,tommy_s,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,tommy_s,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 23 20:45:00 UTC 2018,,,,,,,,,,,"0|i3zj4f:",9223372036854775807,4.0,,,,,,,,aweisberg,,aweisberg,,,Low,,,,,,,,,,,,,,,,,,,"23/Oct/18 11:13;tommy_s;Patch availible here: [cassandra-14836|https://github.com/tommystendahl/cassandra/commit/bcaa224dec3939dcc883ded250fb9849bfbfa992];;;","23/Oct/18 20:17;aweisberg;+1, I made some small tweaks. I removed the extra float cast and I added string formatting so we only print two digits after the decimal point.

Running the tests now. 

https://github.com/apache/cassandra/compare/trunk...aweisberg:14836-trunk?expand=1
https://circleci.com/gh/aweisberg/cassandra/tree/14836-trunk
;;;","23/Oct/18 20:45;aweisberg;Committed as [c3ef43d45e5c9c44f22dbeb8a58232aa6f0cfd15|https://github.com/apache/cassandra/commit/c3ef43d45e5c9c44f22dbeb8a58232aa6f0cfd15] thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid keeping StreamingTombstoneHistogramBuilder.Spool in memory during the whole compaction,CASSANDRA-14834,13192860,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aholmber,marcuse,marcuse,19/Oct/18 14:28,20/Jan/21 22:28,13/Jul/23 08:37,20/Jan/21 22:28,4.0,4.0-rc1,,,,,Local/Compaction,,,,0,,,,Since CASSANDRA-13444 {{StreamingTombstoneHistogramBuilder.Spool}} is allocated to keep around an array with 131072 * 2 * 2 integers *per written sstable* during the whole compaction. With LCS at times creating 1000s of sstables during a compaction it kills the node.,,aholmber,colinkuo,e.dimitrova,jasonstack,jeromatron,KurtG,maedhroz,marcuse,mck,n.v.harikrishna,stefania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aholmber,,,,,,,,,,,,,,,,,,,,Normal,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jan 20 22:28:51 UTC 2021,,,,,,,,,,,"0|i3zeyv:",9223372036854775807,4.0,,,,,,,,,,bereng,mck,,Low,,4.0-alpha1,,,https://github.com/apache/cassandra/commit/5e8f7f591dfec5a61d8eb2e9e977ec29f3a2bbe4,,,,,,,,,"No doc.
I didn't come with a test that would prove that we would not OOM because of this. I'm open to suggestions.",,,,,"02/Dec/20 20:08;aholmber;[~marcuse] I was thinking of taking a look at this, but I see you're assigned. Do you have something in mind already? Intend to work on this?;;;","03/Dec/20 15:27;aholmber;I looked at it for a few minutes yesterday and at first blush it looked like it should be a simple solution to release the large buffer when metadata is ""finalized"". However, this [comment|https://github.com/apache/cassandra/blob/00fb6d76d0a97af06ba27c1180d6dcddfa337fea/src/java/org/apache/cassandra/utils/streamhist/StreamingTombstoneHistogramBuilder.java#L138-L140] gives me pause.

{quote}
     * Creates a 'finished' snapshot of the current state of the historgram, but leaves this builder instance
     * open for subsequent additions to the histograms. Basically, this allows us to have some degree of sanity
     * wrt sstable early open.
{quote}

It seems to suggest that we have instances being updated after finalization. Will have to dig into that a bit more. It might still be possible to update the histogram without the large buffers, but I need to understand how impactful that would be (or indeed, if the comment is still valid).

Not moving this ticket yet due to a handful of other things in-flight.;;;","14/Dec/20 20:52;aholmber;I got a chance to look at this and I think the comment above may be overreaching. I was hoping that the buffer could be released on finalize, but it's not quite that simple. I did find that MetricsCollector#finalizeMetadata (which calls build on the histo) is called several times in the lifecycle, which is makes the ""finalize"" semantics a bit confusing. I looked into memoizing the result, but found that the metrics can actually be [updated|https://github.com/apache/cassandra/blob/4c103447af3c4829e3a1c733bed3952fd059af08/src/java/org/apache/cassandra/io/compress/CompressedSequentialWriter.java#L355] between the first and last call. I did not, however, find any instance of the histogram being updated after the first call to finalize/build. Therefore, my proposal is to drop the buffers as soon as we switch to a new writer (and will have no further samples to update on the previous writer), but leave the collector in a state that things can still be updated and metrics rebuilt repeatedly. The patch does this by adding a simple call chain to explicitly ""release"" the metadata overhead when writing is done.

[patch|https://github.com/aholmberg/cassandra/pull/24]
[ci|https://app.circleci.com/pipelines/github/aholmberg/cassandra?branch=CASSANDRA-14834];;;","08/Jan/21 15:39;mck;ci-cassandra CI – https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/281/pipeline
;;;","20/Jan/21 21:43;mck;latest circleci also looks good [here|https://app.circleci.com/pipelines/github/aholmberg/cassandra?branch=CASSANDRA-14834].;;;","20/Jan/21 22:28;mck;Committed as [5e8f7f591dfec5a61d8eb2e9e977ec29f3a2bbe4|https://github.com/apache/cassandra/commit/5e8f7f591dfec5a61d8eb2e9e977ec29f3a2bbe4].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expand range tombstone validation checks to multiple interim request stages,CASSANDRA-14824,13191665,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,15/Oct/18 17:21,15/May/20 08:03,13/Jul/23 08:37,16/Oct/18 16:29,3.0.18,3.11.4,4.0,4.0-alpha1,,,Local/SSTable,,,,0,,,,"{{RTBoundValidator}} was originally only added to verify the end-game response iterator produced by {{ReadCommand}}.

However, turns out it's possible for sequencing errors in lower-level iterators to be silently erased by upstream iterators - as a result of merging two invalid iterators into one valid iterator, for example. CASSANDRA-14823 can create such a scenario.

The upcoming patch would attach the checker at the intermediate stages, ensuring that we can detect those otherwise silent corruptions.",,aleksey,bdeggleston,cscotta,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,Correctness -> Transient Incorrect Response,,,,,,,Quality Assurance,Normal,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 16 16:28:55 UTC 2018,,,,,,,,,,,"0|i3z7lz:",9223372036854775807,,,,,,,,,,,bdeggleston,samt,,Normal,,,,,,,,,,,,,,,,,,,"15/Oct/18 19:24;aleksey;Code for [3.0|https://github.com/iamaleksey/cassandra/commits/14824-3.0], CI for [3.0|https://circleci.com/workflow-run/7cd1b687-dabd-4c02-b70b-3ca908471043]; Some hiccups with 3.11 and 4.0 branches, will resolve soonish.;;;","15/Oct/18 23:24;bdeggleston;looks good to me. The only think I might change would be to move the MERGED validator out of queryStorage and into executeLocally, which would validate the pre-purge stage for 2i implementations that may not be checking, but I don't feel strongly about it.;;;","16/Oct/18 11:21;samt;LGTM, my only query would be whether {{Stage.OTHER}} is really necessary. It's only used in tests, where any of the other enum values could be used. Not something I feel strongly about though. ;;;","16/Oct/18 16:28;aleksey;Thanks. Committed to 3.0 as [5e969e9cfd7e776dadeb51d1003bbfe79544ca08|https://github.com/apache/cassandra/commit/5e969e9cfd7e776dadeb51d1003bbfe79544ca08] and merged up with 3.11 and trunk.

Noticed some omissions, and had to fight some 3.11 differences until CI for all branches was made happy, so this took a bit longer than expected.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Legacy sstables with range tombstones spanning multiple index blocks create invalid bound sequences on 3.0+,CASSANDRA-14823,13191299,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,bdeggleston,bdeggleston,bdeggleston,12/Oct/18 21:01,02/Aug/19 02:50,13/Jul/23 08:37,16/Oct/18 19:52,3.0.18,3.11.4,,,,,Local/SSTable,,,,0,,,,"During upgrade from 2.1 to 3.0, reading old sstables in reverse order would generate invalid sequences of range tombstone bounds if their range tombstones spanned multiple column index blocks. The read fails in different ways depending on whether the 2.1 tables were produced by a flush or a compaction.",,aleksey,bdeggleston,cscotta,jeromatron,jjirsa,madega,mck,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,Correctness -> Recoverable Corruption / Loss,,,,,,,,Challenging,Fuzz Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 29 16:27:01 UTC 2018,,,,,,,,,,,"0|i3z5dr:",9223372036854775807,,,,,,,,,,,aleksey,samt,,Critical,,3.0.0,,,,,,,,,,,,,,,,,"12/Oct/18 23:46;bdeggleston;[3.0 fix|https://github.com/bdeggleston/cassandra/tree/14823-3.0]
[2.1 sstable generator|https://github.com/bdeggleston/cassandra/tree/14823-2.1]
[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14823-3.0]

This fixes a few bugs related to range tombstones and iterating over legacy sstables in reverse.

First, in 2.1, range tombstones spanning multiple indexed blocks are rewritten at the beginning of each block they partially cover. 3.0 expects open markers to be noted in the column index, so it treats these as normal range tombstones, which will lead to duplicate start bounds being emitted, and incorrect dropping of rows in some cases. When generated by memtable flush, the repeated range tombstone open marker is less than the current index block first name. When generated by compaction, the index block first name is set to the repeated tombstone's open bound, which is less than the preceding index block's last name. Fixing this complicates the logic of determining if the contents of a row span multiple index blocks and correctly handling them, so that's been modified here as well.

Next, since the open and closing bound of a range tombstone are stored together on disk, we stash the closing bound in memory to read out later. However we use the file pointer to figure out when we should stop reading an index block. So any rt closing bound that should be the last thing we emit for an index block gets dropped, leading to an open ended closing bound being emitted. That's fixed here as well.;;;","12/Oct/18 23:47;bdeggleston;[~benedict] [~iamaleksey] would one (or both) of you mind taking a look at this as well?;;;","14/Oct/18 16:50;samt;This is a nasty one. The patch essentially LGTM, though I've pushed a couple of tiny commits with minor suggestions [here|https://github.com/beobal/cassandra/commits/14823-3.0]. The first fixes a couple of typos, tweaks the comments slightly in SSTRI and changes a param name. The second switches from using Guava {{Preconditions}} to {{Verify}}, which is not a big deal at all, but seems marginally better aligned semantically. e.g.
{quote}If checking whether the _caller_ has violated your method or constructor's contract (such as by passing an invalid argument), use the utilities of the Preconditions class instead.""
{quote}
from [the Verify doc|https://google.github.io/guava/releases/18.0/api/docs/com/google/common/base/Verify.html])

One last thing to note is that the merge to 3.11 is not entirely clean but it is trivially resolvable and tests do pass once merged.;;;","15/Oct/18 14:24;samt;I realised that I'd overlooked one additional aspect here: that a 2.1 has the potential to have multiple repeated RTs following a block boundary. So I've updated the test data generator to do that, though I could only figure out how to do it via the compaction path. While I was at it, I changed the naming of the test tables slightly from {{...\_compact\_...}} to {{...\_compacted\_...}} to avoid potential confusion due to the existing test tables named that way to indicate that they use {{COMPACT STORAGE}}. 
[3.0 branch|https://github.com/beobal/cassandra/tree/14823-3.0]
[2.1 sstable generator|https://github.com/beobal/cassandra/commit/420457c3192952206e07276be7c2edf86aa79a7e]
[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F14823-3.0]
;;;","15/Oct/18 21:46;bdeggleston;Changes look good to me. Pulled in, merged to 3.11 and started another run of tests:
|[3.0|https://github.com/bdeggleston/cassandra/tree/14823-3.0]|[circle|https://circleci.com/workflow-run/fd588958-4771-4d4e-8569-772415523c88]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/14823-3.11]|[circle|https://circleci.com/workflow-run/37a12a6a-f2ab-4591-a5c7-83b926ba655a]|;;;","16/Oct/18 17:00;samt;+1;;;","16/Oct/18 18:45;aleksey;LGTM;;;","16/Oct/18 19:52;bdeggleston;Committed as [285153f621a1e67212ef61686188eff3e9c80a4e |https://github.com/apache/cassandra/commit/285153f621a1e67212ef61686188eff3e9c80a4e], thanks;;;","19/Oct/18 20:49;madega;Does this fix apply to 3.11.3 release?;;;","12/Nov/18 23:18;jjirsa;[~madega] yes, it will impact 3.11.3, and will be fixed with 3.11.4 when it's released.
;;;","29/Nov/18 05:52;mck;[~bdeggleston], correct me if i'm wrong please…

For this to impact a user all the following must be true:
 - upgrading from 2.1,
 - clients were performing partition or multi-row deletes, creating range tombstones,
 - partitions have multiple rows and are {{>64KB}} (ie has IndexInfo column index blocks),
 - before the post-upgrade {{`nodetool upgradesstables`}} is complete, clients perform reads with an ordering clause (that reverses the clustering column's ordering on disk).

That is a data-model that does not do {{DELETEs}} (or only does single-row or column {{DELETEs}}) will not be affected by this bug.
 Likewise a data-model that never specifies the ""{{ORDER BY""}} clause in any {{SELECTs}} will not be affected by this bug.;;;","29/Nov/18 16:27;bdeggleston;That’s correct Mick, it almost definitely affects people upgrading from 2.2 as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade to 4.0 fails with NullPointerException,CASSANDRA-14820,13191171,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aweisberg,tommy_s,tommy_s,12/Oct/18 11:07,15/May/20 08:03,13/Jul/23 08:37,16/Oct/18 22:51,4.0,4.0-alpha1,,,,,,,,,0,,,,"I tested to upgrade an existing cluster to latest 4.0 but it fails with a NullPointerException, I upgraded from 3.0.15 but upgrading from any 3.0.x or 3.11.x to 4.0 will give the same fault.
{noformat}
 
2018-10-12T11:27:02.261+0200 ERROR [main] CassandraDaemon.java:251 Error while loading schema: 
java.lang.NullPointerException: null
 at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:156)
 at org.apache.cassandra.serializers.AbstractTextSerializer.deserialize(AbstractTextSerializer.java:41)
 at org.apache.cassandra.serializers.AbstractTextSerializer.deserialize(AbstractTextSerializer.java:28)
 at org.apache.cassandra.db.marshal.AbstractType.compose(AbstractType.java:116)
 at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:267)
 at org.apache.cassandra.schema.SchemaKeyspace.createTableParamsFromRow(SchemaKeyspace.java:997)
 at org.apache.cassandra.schema.SchemaKeyspace.fetchTable(SchemaKeyspace.java:973)
 at org.apache.cassandra.schema.SchemaKeyspace.fetchTables(SchemaKeyspace.java:927)
 at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspace(SchemaKeyspace.java:886)
 at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspacesWithout(SchemaKeyspace.java:877)
 at org.apache.cassandra.schema.SchemaKeyspace.fetchNonSystemKeyspaces(SchemaKeyspace.java:865)
 at org.apache.cassandra.schema.Schema.loadFromDisk(Schema.java:102)
 at org.apache.cassandra.schema.Schema.loadFromDisk(Schema.java:91)
 at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:247)
 at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:590)
 at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:679)
{noformat}
 The problem seams to be line 997 in SchemaKeyspace.java

 
{noformat}
.speculativeWriteThreshold(SpeculativeRetryPolicy.fromString(row.getString(""speculative_write_threshold""{noformat}
speculative_write_threshold is a new table option introduced in CASSANDRA-14404, when upgrading the table option is missing and we get a NullPointerException on this line.

 ",,aweisberg,cscotta,djoshi,jay.zhuang,jeromatron,tommy_s,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14761,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aweisberg,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 18 14:37:16 UTC 2018,,,,,,,,,,,"0|i3z4lb:",9223372036854775807,4.0,,,,,,,,,,djoshi,,,Normal,,,,,,,,,,,,,,,,,,,"12/Oct/18 16:29;aweisberg;[trunk changes|https://github.com/apache/cassandra/compare/trunk...aweisberg:14820-trunk?expand=1]
 [dtest changes|https://github.com/apache/cassandra-dtest/compare/master...aweisberg:14820?expand=1]
 [CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/14820-trunk] [!https://circleci.com/gh/aweisberg/cassandra/tree/14820-trunk.png?circle-token=d9f905fd5057cf16c947a84cfdf9b10a4f58252a! | https://circleci.com/gh/aweisberg/cassandra/tree/14820-trunk];;;","12/Oct/18 18:31;aweisberg;Along with fixing this issue I am doing the rename from CASSANDRA-14671 so that when people upgrade it doesn't eventually break due to the rename.;;;","16/Oct/18 05:17;djoshi;I found one minor issue in {{ddl.rst#L466}} - {{additional_write_policy}} description seems to be duplicated from the earlier row. Also minor nit on the white space. Please fix on commit. Thanks!;;;","16/Oct/18 14:27;aweisberg;ddl.rst isn't a mistake per se we haven't renamed  speculative_retry yet. That's going to be a separate issue. We are documenting them together because they accept the same parameters.

What is the whitespace change you are looking for?;;;","16/Oct/18 21:39;djoshi;+1 Ariel & I spoke offline about the spacing issue.;;;","16/Oct/18 22:51;aweisberg;Committed to trunk as [4ae229f5cd270c2b43475b3f752a7b228de260ea|https://github.com/apache/cassandra/commit/4ae229f5cd270c2b43475b3f752a7b228de260ea] and dtests as [104835d880b4ace131e341235359606347783102|https://github.com/apache/cassandra-dtest/commit/104835d880b4ace131e341235359606347783102].

Thanks!;;;","18/Oct/18 14:37;cscotta;Thanks [~aweisberg] and [~djoshi3].

[~tommy_s], thank you very much for this report!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SEPExecutor does not fully shut down,CASSANDRA-14815,13190994,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,11/Oct/18 17:24,15/May/20 08:05,13/Jul/23 08:37,15/Jan/19 19:20,4.0,4.0-alpha1,,,,,Local/Startup and Shutdown,,,,0,,,,"When trying to shut down an SEP Executor, a parked worked will still be parked on:
{code}
sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)
org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:88)
io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
java.lang.Thread.run(Thread.java:748)
{code}",,benedict,ifesdjeen,jjordan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 15 19:20:28 UTC 2019,,,,,,,,,,,"0|i3z3hz:",9223372036854775807,,,,,,,,,benedict,,benedict,,,Low,,,,,,,,,,,,,,,,,,,"11/Oct/18 17:31;ifesdjeen;|[patch|https://github.com/apache/cassandra/pull/281/files]|[tests|https://circleci.com/workflow-run/8022c393-5c08-4643-84ae-a778b2a53656]|;;;","30/Nov/18 16:54;benedict;I've pushed a modified approach [here|https://github.com/belliottsmith/cassandra/tree/14815]

It's a rare possible race condition, but unilaterally invoking {{set(Work.DEAD)}} would not have guaranteed that the state was set to {{DEAD}}, as there are places in the state machine where a worker may also unilaterally self-assign its state, knowing that nobody else would interfere with it.  In this case the state change would be lost.

Perhaps this wouldn't be the end of the world, but if instead we introduce a {{pool.shuttingDown}} boolean, and we check this only in the two places we might need to - namely when we cannot assign ourselves work (which will be the case if all executors have been shutdown), and when stopping ourselves (in case we have somehow been very delayed in stopping, and so the original termination did not signal us), we can also avoid introducing a new {{COWArrayList}} to manage all of the worker threads.

Oh, also, a spelling nit in the {{MAGIC}} thread name indicator.;;;","30/Nov/18 18:18;benedict;There is a downside to the approach I've taken, which is that the threads are not guaranteed to have stopped by the time shutdown exits.  I've added a follow-up commit that reduces this window, but this is still not a guarantee.  I think this commit is probably better to exclude, but I'll await your feedback.  I have a final commit that just has the test method invoke {{join()}} on each matching thread with a short wait parameter, so that they have the opportunity to stop themselves.;;;","04/Dec/18 12:05;ifesdjeen;[~benedict] Thank you for the review,

You're right, I haven't noticed there are other calls to {{set}} that would overwrite this state; CAS this used only during transition between executors, and all the other time worker is an exclusive owner. I did have a similar version to yours, but the way I implemented it initially (atomic boolean for stop signal and waiting twice on descheduled and once on spinning queues to make sure all tasks are drained), but thought we can do better. I like your version, however as it improves in terms of how we signal shutdown without having to spin.

I've pushed two minor suggestions: one is to take break out of the nested if and second - remove (seemingly) redundant call to {{terminateWorkers}}, let me know what you think.;;;","04/Dec/18 12:22;benedict;Yes, these changes are good.  +1. 

But: we should update the comments in {{terminateWorkers}}, and we should add a comment to {{STOP_SIGNALLED}} that it may ONLY be assigned in {{maybeStop}} as we now depend on this for correctness of termination of the threads.  We should perhaps also put a comment under {{doWaitSpin}} that we *must* continue here to re-check {{isShuttingDown}}.  Neither of these are strict requirements of the design, so whilst they shouldn't change, we should document the dependencies.;;;","04/Dec/18 13:32;ifesdjeen;I've pushed two more small changes: modify two of three suggested comments and revert wait-related code as unless we can wait for threads to _really_ stop, it seems to add not enough to justify the complexity. Let me know what you think.;;;","04/Dec/18 17:39;benedict;[here|https://github.com/ifesdjeen/cassandra/blob/cd7235d004d5ec99cb937fe213dd99b044782083/src/java/org/apache/cassandra/concurrent/SharedExecutorPool.java#L134] I meant for ""enter the SPINNING state,"" to become something like ""are runnable"", because we no longer require they enter the SPINNING state to check the condition (although we do enter the spinning state, it is no longer a requirement).

[here|https://github.com/ifesdjeen/cassandra/commit/cd7235d004d5ec99cb937fe213dd99b044782083#diff-d9febddfa9880b152f1b86242708b862R83] something slightly more descriptive about why would be useful, such as ""if the pool is terminating, but we have been assigned STOP_SIGNALLED, if we do not continue to re-check pool.shuttingDown this thread will block forever""

Otherwise, +1 LGTM;;;","05/Dec/18 14:44;ifesdjeen;Thank you for the review, committed as [eea68a2cfeb0134510deaaa5540afdf6d0c6ee7e|https://github.com/apache/cassandra/commit/eea68a2cfeb0134510deaaa5540afdf6d0c6ee7e] to trunk.;;;","05/Dec/18 15:03;jjordan;I assume fixver here was 4.0?  If not please correct it.;;;","05/Dec/18 15:03;benedict;Actually, it would be great to backport this to 3.0, so that we can utilise it for mixed-version in-jvm dtests once they are backported.;;;","05/Dec/18 15:12;ifesdjeen;Good point; I will reopen for back port.;;;","15/Jan/19 19:20;benedict;The backport work for this is being tracked/handled in CASSANDRA-14931.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiget Thrift query returns null records after digest mismatch,CASSANDRA-14812,13190730,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,benedict,sivukhin.nikita,sivukhin.nikita,10/Oct/18 19:31,15/May/20 08:05,13/Jul/23 08:37,02/Jul/19 10:22,3.0.19,3.11.5,4.0,4.0-alpha1,,,Consistency/Coordination,Messaging/Thrift,,,3,,,,"It seems that in Cassandra 3.0.0 a nasty bug was introduced in {{multiget}} Thrift query processing logic. When one tries to read data from several partitions with a single {{multiget}} query and {{DigestMismatch}} exception is raised during this query processing, request coordinator prematurely terminates response stream right at the point where the first \{{DigestMismatch}} error is occurring. This leads to situation where clients ""do not see"" some data contained in the database.

We managed to reproduce this bug in all versions of Cassandra starting with v3.0.0. The pre-release version 3.0.0-rc2 works correctly. It looks like [refactoring of iterator transformation hierarchy|https://github.com/apache/cassandra/commit/609497471441273367013c09a1e0e1c990726ec7] related to CASSANDRA-9975 triggers incorrect behaviour.

When concatenated iterator is returned from the [StorageProxy.fetchRows(...)|https://github.com/apache/cassandra/blob/a05785d82c621c9cd04d8a064c38fd2012ef981c/src/java/org/apache/cassandra/service/StorageProxy.java#L1770], Cassandra starts to consume this combined iterator. Because of {{DigestMismatch}} exception some elements of this combined iterator contain additional {{ThriftCounter}}, that was added during [DataResolver.resolve(...)|https://github.com/apache/cassandra/blob/ee9e06b5a75c0be954694b191ea4170456015b98/src/java/org/apache/cassandra/service/reads/DataResolver.java#L120] execution. While consuming iterator for many partitions Cassandra calls [BaseIterator.tryGetMoreContents(...)|https://github.com/apache/cassandra/blob/a05785d82c621c9cd04d8a064c38fd2012ef981c/src/java/org/apache/cassandra/db/transform/BaseIterator.java#L115] method that must switch from one partition iterator to another in case of exhaustion of the former. In this case all Transformations contained in the next iterator are applied to the combined BaseIterator that enumerates partitions sequence which is wrong. This behaviour causes BaseIterator to stop enumeration after it fully consumes partition with {{DigestMismatch}} error, because this partition iterator has additional {{ThriftCounter}} data limit.

The attachment contains the python2 script [^small_repro_script.py] that reproduces this bug within 3-nodes ccmlib controlled cluster. Also, there is an extended version of this script - [^repro_script.py] - that contains more logging information and provides the ability to test behavior for many Cassandra versions (to run all test cases from repro_script.py you can call {{python -m unittest2 -v repro_script.ThriftMultigetTestCase}}). All the necessary dependencies contained in the [^requirements.txt]

 
This bug is critical in our production environment because we can't permit any data skip.

Any ideas about a patch for this issue?",,aleksey,Andrew.Kostousov@gmail.com,benedict,cscotta,jay.zhuang,jeromatron,jjirsa,KurtG,mck,sivukhin.nikita,vinaykumarcse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14918,,,,,,,,CASSANDRA-14918,,,,,,,,,,,,"10/Oct/18 19:16;sivukhin.nikita;repro_script.py;https://issues.apache.org/jira/secure/attachment/12943295/repro_script.py","10/Oct/18 19:17;sivukhin.nikita;requirements.txt;https://issues.apache.org/jira/secure/attachment/12943294/requirements.txt","10/Oct/18 19:14;sivukhin.nikita;small_repro_script.py;https://issues.apache.org/jira/secure/attachment/12943296/small_repro_script.py","27/May/19 12:00;mck;small_repro_script_cql.py;https://issues.apache.org/jira/secure/attachment/12969891/small_repro_script_cql.py",,,,,,,,,,4.0,benedict,,,,,,,,,,,,Correctness -> Transient Incorrect Response,,,,,,,,Challenging,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 02 10:21:27 UTC 2019,,,,,,,,,,,"0|i3z1vb:",9223372036854775807,3.11.0,3.11.1,3.11.2,3.11.3,,,,,,,mck,,,Critical,,3.0.0,,,"[97eae441dab742f0eaffcedc360991350232cfd6|https://github.com/apache/cassandra/commit/97eae441dab742f0eaffcedc360991350232cfd6]",,,,,,,,,,,,,,"12/Oct/18 17:36;benedict;Hi [~sivukhin.nikita],

Thanks for the excellent bug report.  I will take a closer look at this on Monday.;;;","16/Oct/18 05:59;sivukhin.nikita;[~benedict] , any progress in investigating the issue? I create unit-test that reproduce the problem in the same way as my reproducing scripts do. You can find this test [here|https://github.com/Umqra/cassandra/commit/1de595a6d456cc78b0e3af77607814e014f6b4ed].;;;","23/Oct/18 09:25;benedict;[~sivukhin.nikita], sorry for the delay - unfortunately other things cropped up that required my attention.

I'm awaiting the results of a clean CI run, but [here|https://github.com/belliottsmith/cassandra/commit/111392281afa41ca2b22e8f90369f9c29a18bfd2] is a commit that should fix the problem for you.

The only necessary changes are those to {{PartitionIterators}} and {{BasePartitions}}

The main problem was introduced right back in the beginning with {{PartitionIterators.concat}} that used the {{MoreContents.extend()}} feature - which retains the transformations already applied to the iterator at the point of extension.  Logically, a {{concat}} should not do this, as each element is unrelated to the others.

The change to {{BasePartitions}} is necessary because of CASSANDRA-13482, which seems to have erroneously applied the child stop criteria to the whole parent iterator.  I haven't fully explored the original intent of this change, but the modification made here seems most likely what was intended from the ticket description.

The branch I have uploaded also has a back port of CASSANDRA-14821, to support a dtest to exhibit this bug as it actually occurs, though your test cases were greatly appreciated.;;;","20/Nov/18 13:10;sivukhin.nikita;Thank you, [~benedict] . We tested your patch against provided integration tests and also try more tricky scenarios (for example, when digest mismatch happened in the middle of the sequence of requested keys). In all tested cases patch is working and the bug is not reproducing. When you plan to patch the master?

Also, I want to clarify one issue, just for my confidence. Is it true, that the machinery of request processing for Thrift queries is totally different than for CQL queries? I'm asking about it because the patch is very specific and if the same bug also contained somewhere in the CQL processing pipeline then the patch will not help. The code surrounding response transformation and filtration is slightly entangled for me, so I can't be sure that there is no subtly dependency between CQL and Thrift processing pipelines that can also be under attack of a bug.

 ;;;","27/Nov/18 14:46;benedict;Thanks [~sivukhin.nikita].  Now you've confirmed, I'll work to get a patch ready for mainline; hopefully it shouldn't take too long.

At the same time, I intend to establish what effect, if any, this has on CQL queries.  I think it may (by happy coincidence) be fine, even though the same {{concat}} bug applies, as the limits are applied differently.;;;","07/Dec/18 14:34;benedict;I have pushed a minimal patch [here|https://github.com/belliottsmith/cassandra/tree/14812].

I intend to follow-up with a separate back port of the in-jvm distributed tests to include a unit test.

This issue does not, serendipitously, seemingly affect CQL queries.  The kinds of limit we apply to CQL queries would not fall prey to this issue.;;;","07/Dec/18 15:28;cscotta;Thanks! Reassuring to see that CQL is not impacted here.;;;","13/Mar/19 21:47;Andrew.Kostousov@gmail.com;Just noticed that corresponding patch was not included in the latest v3.11.4 release :(

When (at least approximately) can we expect this to be released? Note that the lack of this patch prevents us from updating our production clusters from v2.2 to more recent Cassandra versions.;;;","14/Mar/19 00:25;benedict;Yes, I'm very sorry about that.  You can see that I raised this ticket in the release vote [here|https://lists.apache.org/thread.html/33aa0441669e927e2317e82f7f076a484bdc842e01d775a94b5db2ed@%3Cdev.cassandra.apache.org%3E]; but the project has been on a major stability drive and there were a lot of pending and pressing bug fixes to also release.

Perhaps somebody in the community who is watching would be willing to review the patch, so we can include it and perhaps hold an accelerated 3.0.19 (and 3.11.5) release vote?;;;","27/May/19 11:58;mck;[~benedict], I have reviewed the patch and tested the python reproducible on 3.0.18 and 3.11.4, working with and failing without the patch applied.

I'm not competent on this area, but I am jumping in to help as we too are seeing users unable to upgrade because of this fault.

Review questions/points are: 
 - is there a way to replicate the test for the CQL equivalent? While this bug does not impact CQL it is my understanding that CQL queries with `IN` clauses will still be going through this code path… I've attached the reproducible script rewritten for CQL, is it applicable? Should it be added as a dtest? (i don't think so but double-checking)
 - I understand overriding {{`filter(..)`}} for the NONE impl, although at first it is not intuitive that  {{`DataLimits.NONE`}} is also used in thrift queries…
 - fyi the circleci results are here: https://circleci.com/workflow-run/3dd0d7f3-fa79-4118-80d8-247e85db40ea ; are these failures of concern?
 - {{""The branch I have uploaded also has a back port of CASSANDRA-14821""}}. I am confused… where is this?
 - a rebased commit for the 3.0 branch is here [mck/cassandra-3.0_14812|https://github.com/thelastpickle/cassandra/commits/mck/cassandra-3.0_14812]
 - the change in {{BasePartitions}} and the interactions from different {{StoppingTransformation}} subclasses is a bit harder to grok… It makes sense that the {{while}} loop does not need to continue in the situation where, {{stop}} has ""leaked"" and not been signalled, but where {{stopChild.isSignalled}} was. But not returning false in that same situation seems odd…? Do you want me to test the different cql interactions here (per partition, grouping, paging)?

;;;","02/Jul/19 09:51;mck;Have tested and finished review (see https://github.com/thelastpickle/cassandra/commit/1d3fa25fefa96580fee3dd469f2c9cef860e6ea3#r34153353). 
LGTM.;;;","02/Jul/19 10:21;mck;Committed as 97eae441dab742f0eaffcedc360991350232cfd6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid querying “self” through messaging service when collecting full data during read repair,CASSANDRA-14807,13189735,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,05/Oct/18 15:39,15/May/20 08:05,13/Jul/23 08:37,12/Oct/18 17:05,4.0,4.0-alpha1,,,,,Legacy/Coordination,Legacy/Local Write-Read Paths,,,0,,,,"Currently, when collecting full requests during read-repair, we go through the messaging service instead of executing the query locally.

||[patch|https://github.com/apache/cassandra/pull/278]||[dtest-patch|https://github.com/apache/cassandra-dtest/pull/39]||

|[utest|https://circleci.com/gh/ifesdjeen/cassandra/641]|[dtest-vnode|https://circleci.com/gh/ifesdjeen/cassandra/640]|[dtest-novnode|https://circleci.com/gh/ifesdjeen/cassandra/639]|",,aweisberg,cscotta,ifesdjeen,jeromatron,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 11 16:53:42 UTC 2018,,,,,,,,,,,"0|i3yvtj:",9223372036854775807,,,,,,,,,aweisberg,,aweisberg,,,Normal,,,,,,,,,,,,,,,,,,,"11/Oct/18 16:38;aweisberg;+1. My review comments are in the PRs.;;;","11/Oct/18 16:53;ifesdjeen;Thank you for the review!

[Latest test run|https://circleci.com/workflow-run/68b29a4d-e72e-4664-a2dc-92b392b59a1c]

Committed to trunk with [f24e23c5f42f27cf74297e5c12de370fc6a724bc|https://github.com/apache/cassandra/commit/f24e23c5f42f27cf74297e5c12de370fc6a724bc]

[dtest commit|https://github.com/apache/cassandra-dtest/commit/74f578abe03fd004f9ffe26868e76f63949cedec];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Running repair on multiple nodes in parallel could halt entire repair ,CASSANDRA-14804,13189347,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,chovatia.jaydeep@gmail.com,chovatia.jaydeep@gmail.com,04/Oct/18 04:34,16/Apr/19 09:29,13/Jul/23 08:37,05/Oct/18 23:08,3.0.18,,,,,,Consistency/Repair,,,,0,,,,"Possible deadlock if we run repair on multiple nodes at the same time. We have come across a situation in production in which if we repair multiple nodes at the same time then repair hangs forever. Here are the details:

Time t1
 {{node-1}} has issued repair command to {{node-2}} but due to some reason {{node-2}} didn't receive request hence {{node-1}} is awaiting at [prepareForRepair |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/ActiveRepairService.java#L333] for 1 hour *with lock*

Time t2
 {{node-2}} sent prepare repair request to {{node-1}}, some exception occurred on {{node-1}} and it is trying to cleanup parent session [here|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/repair/RepairMessageVerbHandler.java#L172] but {{node-1}} cannot get lock as 1 hour of time has not yet elapsed (above one)

snippet of jstack on {{node-1}}
{quote}""Thread-888"" #262588 daemon prio=5 os_prio=0 waiting on condition
 java.lang.Thread.State: TIMED_WAITING (parking)
 at sun.misc.Unsafe.park(Native Method)
 - parking to wait for (a java.util.concurrent.CountDownLatch$Sync)
 at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
 at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1037)
 at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)
 at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:277)
 at org.apache.cassandra.service.ActiveRepairService.prepareForRepair(ActiveRepairService.java:332)
 - locked <> (a org.apache.cassandra.service.ActiveRepairService)
 at org.apache.cassandra.repair.RepairRunnable.runMayThrow(RepairRunnable.java:214)
 at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
 at java.util.concurrent.FutureTask.run(FutureTask.java:266)
 at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
 at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$9/864248990.run(Unknown Source)
 at java.lang.Thread.run(Thread.java:748)

""AntiEntropyStage:1"" #1789 daemon prio=5 os_prio=0 waiting for monitor entry []
 java.lang.Thread.State: BLOCKED (on object monitor)
 at org.apache.cassandra.service.ActiveRepairService.removeParentRepairSession(ActiveRepairService.java:421)
 - waiting to lock <> (a org.apache.cassandra.service.ActiveRepairService)
 at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:172)
 at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67)
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
 at java.util.concurrent.FutureTask.run(FutureTask.java:266)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
 at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$9/864248990.run(Unknown Source)
 at java.lang.Thread.run(Thread.java:748){quote}
Time t3:
 {{node-2}}(and possibly other nodes {{node-3}}…) sent [prepare request |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/ActiveRepairService.java#L333] to {{node-1}}, but {{node-1}}’s AntiEntropyStage thread is busy awaiting for lock at {{ActiveRepairService.removeParentRepairSession}}, hence {{node-2}}, {{node-3}} (and possibly other nodes) will also go in 1 hour wait *with lock*. This rolling effect continues and stalls repair in entire ring.

If we totally stop triggering repair then system would recover slowly but here are the two major problems with this:
 1. Externally there is no way to decide whether to trigger new repair or wait for system to recover
 2. In this case system recovers eventually but it takes probably {{n}} hours where n = #of repair requests fired, only way to come out of this situation is either to do a rolling restart of entire ring or wait for {{n}} hours before triggering new repair request

Please let me know if my above analysis makes sense or not.",,bdeggleston,chovatia.jaydeep@gmail.com,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 05 23:08:30 UTC 2018,,,,,,,,,,,"0|i3ytfb:",9223372036854775807,,,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"04/Oct/18 04:35;chovatia.jaydeep@gmail.com;[~bdeggleston] [~jjirsa] Could you please see if this analysis makes sense or not?;;;","05/Oct/18 20:07;bdeggleston;[~chovatia.jaydeep@gmail.com] I’m not sure how we'd get to the state in t2. We wait for an hour on a semaphore we instantiate in {{prepareForRepair}}, and {{removeParentRepairSession}} is synchronized on the object monitor. One shouldn’t block the other. I think the jstack in the description is missing the thread where the {{ActiveRepairService}} monitor is being held. ;;;","05/Oct/18 20:52;chovatia.jaydeep@gmail.com;In our branch we have {{prepareForRepair}}  *{{synchronized}}* yet, it was fixed in CASSANDRA-13849 which we missed to backport. 
Let me back port CASSANDRA-13849 to our branch and then hopefully this will fix the issue.

Thanks a lot [~bdeggleston] for your help!;;;","05/Oct/18 23:08;bdeggleston;No problem, glad you got it figured out;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rows that cross index block boundaries can cause incomplete reverse reads in some cases.,CASSANDRA-14803,13189264,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,bdeggleston,bdeggleston,bdeggleston,03/Oct/18 18:34,01/Aug/21 12:24,13/Jul/23 08:37,04/Oct/18 20:08,3.0.18,3.11.5,,,,,Local/SSTable,,,,0,,,,"When we're reading 2.1 sstables in reverse, we skip the first row of an index block if it's split across index boundaries. The entire row will be read at the end of the next block. In some cases though, the only thing in this index block is the partial row, so we return an empty iterator. The empty iterator is then interpreted as the end of the row further down the call stack, so we return early without reading the rest of the data. This only affects 3.x during upgrades from 2.1",,aleksey,bdeggleston,cscotta,jeromatron,jjirsa,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,Correctness -> Transient Incorrect Response,,,,,,,,Challenging,Fuzz Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 29 23:52:29 UTC 2019,,,,,,,,,,,"0|i3ysx3:",9223372036854775807,,,,,,,,,samt,,samt,,,Critical,,3.0.0,,,,,,,,,,,,,,,,,"03/Oct/18 23:01;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/14803-3.0]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14803-3.0]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/14803-3.11]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14803-3.11]|

The sstable used for the test was generated from [here|https://github.com/bdeggleston/cassandra/tree/14803-2.1]

Since this is testing a specific problem upgrading from 2.x-3.x, it didn't seem like LegacySSTableTest was the right place for this;;;","04/Oct/18 16:05;aleksey;My apologies, was typing in what I thought was a different window, and JIRA apparently has keyboard shortcuts.;;;","04/Oct/18 16:31;samt;The fix and test itself LGTM, but I don't quite see what's differentiates this test from the others in {{LegacySSTableTest}}. What am I missing?

Also, it merges to 3.11 cleanly, but the test won't compile because {{DatabaseDescriptor::setDaemonInitialized}} has been replaced there.;;;","04/Oct/18 17:27;bdeggleston;pushed up an update that moves the test into {{LegacySSTableTest}} and fixes compile problem;;;","04/Oct/18 18:07;samt;Thanks, LGTM
 +1 assuming the 3.11 tests pass, which I'm sure they will.;;;","04/Oct/18 20:08;bdeggleston;committed as [ab0e30e75904e4d637f07b2ec64334073eb061ec|https://github.com/apache/cassandra/commit/ab0e30e75904e4d637f07b2ec64334073eb061ec], thanks;;;","29/Jan/19 23:52;jjirsa;[~bdeggleston] / [~beobal] - please set the fixver.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
calculatePendingRanges assigns more pending ranges than necessary ,CASSANDRA-14802,13189135,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,benedict,benedict,03/Oct/18 11:23,15/May/20 13:48,13/Jul/23 08:37,15/May/20 13:48,4.0,4.0-alpha1,,,,,Legacy/Coordination,Legacy/Distributed Metadata,,,0,,,,"This might be a good thing, but should probably be configurable, and made consistent.  Presently, in a number of circumstances where there are multiple range movements, {{calculatePendingRanges}} will assign a pending range to a node that will not ultimately own it.  If done consistently, this might make range movements resilient to node failures / aborted range movements, since all nodes will be receiving all ranges they might own under any incomplete range ownership movements.  But done inconsistently it seems only to reduce availability in the cluster, by potentially increasing the number of pending nodes unnecessarily.",,benedict,cscotta,jeromatron,jmckenzie,KurtG,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 15 13:48:11 UTC 2020,,,,,,,,,,,"0|i3ys4f:",9223372036854775807,,,,,,,,,,,benedict,,,Normal,,2.1.0,,,,,,,,,,,,Unit test included,,,,,"28/Jan/19 19:30;samt;During concurrent replacements without any actual range movements occurring, the pending ranges calculation can also be incorrect due to the fact that we don't differentiate between {{BOOTSTRAP}} and {{BOOTSTRAP_REPLACE}}.
Updating {{allLeftMetadata}} with the tokens & endpoint for a {{BOOTSTRAP_REPLACE}} doesn't grow the ring as it's a straight up replacement, so the subsequent removal of the new endpoint after we grab its new ranges actually constitutes a shrink, skewing the ownership represented by {{allLeftMetadata}}. If there are concurrent replacements happening, this then makes the pending ranges calculation for those tokens completely off.


;;;","29/Jan/19 17:45;benedict;I did not describe the issue when I filed this JIRA, as I had not fully investigated the extent of the problem, but to describe the issue I found that instigated filing this issue: once a node's move has been processed we remove the node from the ring, including its original position, so that future range ownership calculations operate from a ring that entirely lacks this node.  This can cause the same problem of skewing ownership calculations for later nodes, causing it to pile up onto nodes that can never actually own the range.

That is to say that these problems sound like the same basic issue, however for BOOTSTRAP_REPLACE we can maybe fix it more readily.;;;","05/Mar/19 15:24;samt;The {{BOOTSTRAP_REPLACE}} case is, I think, relatively straightforward to fix. As this is fairly simple to reason about/implement/test, IMO we could consider it for 4.0 and defer the more complex general case for now. This isn't a new regression and getting it right (& validating it) is going to be quite involved, so it feels a bit of a risk for the 4.0 timeframe.

||branch||CI||
|[14802-trunk|https://github.com/beobal/cassandra/tree/14802-trunk]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F14802-trunk]|;;;","06/Mar/19 17:12;benedict;+1

bq. This isn't a new regression and getting it right (& validating it) is going to be quite involved, so it feels a bit of a risk for the 4.0 timeframe.

FWIW, I will need to look at this class again soon because transient replication complicated it, and it is known to not work with transient replication.  In the process, I may see how challenging it would be to more comprehensively fix and test the class, as it is currently very non-obvious to work with.  It might be possible to formulate it in a clearer manner, but we'll see; obviously, not if it risk 4.0.;;;","20/Mar/19 12:45;samt;Thanks, committed to trunk in {{4f53bc87261b470adf292fdf37ed4e81bb6f8704}};;;","15/May/20 13:48;jmckenzie;Some how status: resolved w/Resolution: Unresolved. Re-opening to fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
calculatePendingRanges no longer safe for multiple adjacent range movements,CASSANDRA-14801,13189134,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,Gerrrr,benedict,benedict,03/Oct/18 11:21,15/Jul/21 09:15,13/Jul/23 08:37,14/Sep/20 12:11,4.0,4.0-beta3,,,,,Legacy/Coordination,Legacy/Distributed Metadata,,,1,pull-request-available,,,"Correctness depended upon the narrowing to a {{Set<InetAddressAndport>}}, which we no longer do - we maintain a collection of all {{Replica}}.  Our {{RangesAtEndpoint}} collection built by {{getPendingRanges}} can as a result contain the same endpoint multiple times; and our {{EndpointsForToken}} obtained by {{TokenMetadata.pendingEndpointsFor}} may fail to be constructed, resulting in cluster-wide failures for writes to the affected token ranges for the duration of the range movement.
",,benedict,frosner,Gerrrr,jasonstack,jeromatron,jmckenzie,jolynch,KurtG,marcuse,mck,n.v.harikrishna,pauloricardomg,samt,,,,,,,,,,,,,"Gerrrr commented on pull request #495: CASSANDRA-14801 4.0
URL: https://github.com/apache/cassandra/pull/495
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/20 14:03;githubbot;600","Gerrrr closed pull request #495:
URL: https://github.com/apache/cassandra/pull/495


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jul/21 09:15;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,CASSANDRA-14697,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,Gerrrr,,,,,,,,,,,,,,,,,,,,Normal,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 14 12:11:58 UTC 2020,,,,,,,,,,,"0|i3ys47:",9223372036854775807,,,,,,,,,,,marcuse,samt,,Normal,,4.0-alpha1,,,https://github.com/apache/cassandra/commit/8ba163f25a56cb507e621b89b6928c2aef0ecc57,,,,,,,,," 
 * [a534b2|https://github.com/Gerrrr/cassandra/commit/a534b2be9a653fb0cdda75043c0b79e481ca1701] adds tests that reproduce both bugs.
 * [bb36cd|https://github.com/Gerrrr/cassandra/commit/bb36cd09c164840ad9ab3231f1039c443f8f040c] fixes the newly discovered bug ({{test1Leave1Move}} in [a534b2|https://github.com/Gerrrr/cassandra/commit/a534b2be9a653fb0cdda75043c0b79e481ca1701]). There, the failure happens because for the same pending range we can include 2 replicas with the same endpoint and different ranges (violation of {{PendingRangeMaps Conflict.DUPLICATE}} policy). This happens because right now we include in {{PendingRangeMaps}} the entire new replica after leave for leave-affected ranges and only the pending part of it for move-affected ranges. This commit marks as pending only the missing part of the new replica.
 * [a33b03|https://github.com/Gerrrr/cassandra/commit/a33b032cd75044db3475505cd32e6afd6f98ad40] solves the original issue. Without this commit {{getPendingRanges}} can fail if the same replica covers more than 1 pending range. I think that this is a valid situation. Consider {{testLeave2}} in [a534b2|https://github.com/Gerrrr/cassandra/commit/a534b2be9a653fb0cdda75043c0b79e481ca1701]. In this case replica {{Full(127.0.0.1:7012,(-9,0])}} covers 2 ranges - {{(-9,-4]}} and {{(-4,0]}}. If we run the same test against 3.11 {{(-9, 0]}} is represented as {{{(-9, -4], (-4, 0]}}} and that possible representation would not trigger the bug in 4.0. However, I think that we should not base safety of {{getPendingRanges}} execution on the way a particular {{AbstractReplicationStrategy}} represents a range and we should allow duplicate entries while building pending {{RangesAtEndpoint}}.",,,,,"29/Aug/19 20:59;jolynch;[~benedict] do you think this should block the first alpha or it can wait for beta?;;;","09/Mar/20 10:14;Gerrrr;Is anyone working on this ticket? If not, I would like to work on it.;;;","09/Mar/20 10:21;benedict;Nobody is actively working on it, but this is one of the most deceptively complex tickets that needs to be accomplished before 4.0 is released.  I can see you work at DataStax, so perhaps you have the time and skill to dedicate to this, but please be confident before you address it, and be willing to wait a while for a sufficient review.  The class in which the change is needed has had numerous bugs (and in fact has inherent conceptually bugs wrt range movements that are mostly out of scope to address here), so a great deal of care is needed.  Ideally this ticket would attempt to address some of the ugliness that permitted the bug, and _certainly_ needs to be accompanied by a sophisticated-ish randomised correctness test.;;;","09/Mar/20 12:28;Gerrrr;Thank you for a comprehensive response, [~benedict]! I am quite new to C* 4.0 code, so it will take me some time to ramp up. If anyone has planned to work on this issue in the next 1-2 weeks, it probably makes sense for me to work on something else. Otherwise, I'd be happy to contribute.

In the latter case, in the next couple of days, I plan to read on how pending ranges are calculated and what changes since 3.11 introduced the bug. Then I'll write a test case that reproduces the issue.;;;","27/Mar/20 14:10;Gerrrr;I reproduced the bug using simple [randomized test|https://gist.github.com/Gerrrr/f59dc5dedaf6501bb31d79b068244213] that creates a cluster of N nodes and adds, moves, and removes nodes from it. I also found another bug where {{TokenMetadata#calculatePendingRanges}} fails during move-affected replica calculation.

Patch that addresses both problems ([link to PR|https://github.com/apache/cassandra/pull/495]):

* [a534b2|https://github.com/Gerrrr/cassandra/commit/a534b2be9a653fb0cdda75043c0b79e481ca1701] adds tests that reproduce both bugs.
* [bb36cd|https://github.com/Gerrrr/cassandra/commit/bb36cd09c164840ad9ab3231f1039c443f8f040c] fixes the newly discovered bug ({{test1Leave1Move}} in [a534b2|https://github.com/Gerrrr/cassandra/commit/a534b2be9a653fb0cdda75043c0b79e481ca1701]). There, the failure happens because for the same pending range we can include 2 replicas with the same endpoint and different ranges (violation of {{PendingRangeMaps Conflict.DUPLICATE}} policy). This happens because right now we include in {{PendingRangeMaps}} the entire new replica after leave for leave-affected ranges and only the pending part of it for move-affected ranges. This commit marks as pending only the missing part of the new replica.
* [a33b03|https://github.com/Gerrrr/cassandra/commit/a33b032cd75044db3475505cd32e6afd6f98ad40] solves the original issue. Without this commit {{getPendingRanges}} can fail if the same replica covers more than 1 pending range. I think that this is a valid situation. Consider {{testLeave2}} in [a534b2|https://github.com/Gerrrr/cassandra/commit/a534b2be9a653fb0cdda75043c0b79e481ca1701]. In this case replica {{Full(127.0.0.1:7012,(-9,0])}} covers 2 ranges - {{(-9,-4]}} and {{(-4,0]}}. If we run the same test against 3.11 {{(-9, 0]}} is represented as {{\{(-9, -4], (-4, 0]\}}} and that possible representation would not trigger the bug in 4.0. However, I think that we should not base safety of {{getPendingRanges}} execution on the way a particular {{AbstractReplicationStrategy}} represents a range and we should allow duplicate entries while building pending {{RangesAtEndpoint}}.

With these changes, the randomized test hasn't failed after running for a while. As I mentioned, it is a very simplistic test, so any suggestions for improvement are welcome. I haven't included it in the PR, as it seems to be a good candidate to become flaky. Maybe it is worth adding as a {{long}} test?;;;","03/Apr/20 07:15;mck;||branch||circleci||jenkins||
|[trunk_14801|https://github.com/apache/cassandra/compare/trunk...Gerrrr:14801-4.0]|[circleci|https://circleci.com/gh/Gerrrr/workflows/cassandra/tree/14801-4.0]|[!https://ci-cassandra.apache.org/job/Cassandra-devbranch/13/badge/icon!|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/13]|;;;","04/Apr/20 16:42;jmckenzie;Thanks for raising that this is a thorny area of the code-base where specific care needs to be taken. There would probably be a lot of value in a class level comment on TokenMetaData.java indicating this for future people as they ramp on the code-base (and likely other areas of the code-base as well!).
{quote}I can see you work at DataStax, so perhaps you have the time and skill to dedicate to this, but please be confident before you address it, and be willing to wait a while for a sufficient review.
{quote}
Regarding [earned authority|https://www.apache.org/theapacheway/]:
{quote}their influence is based on publicly earned merit – what they contribute to the community. Merit lies with the individual, does not expire, is not influenced by employment status or employer
{quote}
Please refrain from focusing on who is employed by whom and instead focus on their individual merit and contribution to the code-base in the future.;;;","04/Apr/20 22:09;benedict;bq. Please refrain from focusing on who is employed by whom and instead focus on their individual merit and contribution to the code-base in the future.

It is the case that new contributors can generally be assumed to have no knowledge of complex areas of the codebase.  There is however precisely one company I am aware of with people who may secretly have the requisite knowledge and skill.  There is also only one company with _full time_ contributors that were previously unknown to the community, who may have the time to either obtain the necessary skill, or to have weeks to dedicate to difficult tasks such as this.

Were this to be a random individual, I could quite reasonably just direct them to another ticket.  In this case I thought it was best only to issue a fair warning that this _might_ not be suitable but that I do not have sufficient knowledge to say for sure.

I am perceiving a pattern on your part, of choosing to interpret my actions in an unwarrantedly negative light.  Please check yourself, before you check others.;;;","14/Apr/20 16:29;Gerrrr;[~fcofdezc] reviewed the PR and based on his suggestion I re-wrote the randomized test using QuickTheories. As the run time of that test ended comparable to the other unit tests, I included it in the suite as well ([a0246e|https://github.com/apache/cassandra/pull/495/commits/a0246e1e8ba481b98f76896e5005e9a8cc277586]).;;;","02/Sep/20 18:30;samt;Thanks [~Gerrrr], this looks pretty good to me, with a few caveats regarding the QT test.

 * Move operations are not permitted when nodes have multiple tokens, so I think we can split the test into:
 ** multiple tokens per node with leave and bootstrap operations.
 ** single token nodes with leave, bootstrap and move operations.
 * Any node should be moved at most once per {{Cluster}}
 * The rf for each cluster was being hardcoded to 2, we ought to use {{Input.rf}} when constructing {{Cluster}}

I've pushed a commit with those suggestions [here|https://github.com/beobal/cassandra/tree/14801-trunk] and kicked off CI [here|https://app.circleci.com/pipelines/github/beobal/cassandra?branch=14801-trunk].

Would you mind taking a look at the suggestions?;;;","03/Sep/20 11:40;Gerrrr;[~samt] Your suggestions look good! As a nitpick, I propose to replace the while-true loop with filtering out moving nodes and selecting one from what's left ([8d6dfc090da2a|https://github.com/Gerrrr/cassandra/commit/8d6dfc090da2a49dcc07c65029f33deeff7e186b]).;;;","03/Sep/20 17:37;samt;Thanks [~Gerrrr], looks good as far as I'm concerned. I've pulled in your patch and I'm happy to commit pending CI and a second +1.


||branch||Circle||
|​[14801-trunk|https://github.com/beobal/cassandra/tree/14801-trunk]|[circle|https://app.circleci.com/pipelines/github/beobal/cassandra?branch=14801-trunk]​|;;;","14/Sep/20 11:59;marcuse;+1;;;","14/Sep/20 12:11;marcuse;and committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQLSSTableWriter does not support DELETE,CASSANDRA-14797,13188683,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,drohrer,urandom,urandom,01/Oct/18 19:36,27/May/22 19:25,13/Jul/23 08:37,17/Mar/22 20:27,4.1,4.1-alpha1,,,,,Dependencies,,,,1,,,,"{{CQLSSTableWriter}} doesn't work with {{DELETE}} statements, and ought to.",,brandon.williams,drohrer,jeromatron,jjirsa,samt,urandom,yifanc,,,,,,,,,,,,,,,,,,,"yifan-c commented on a change in pull request #1461:
URL: https://github.com/apache/cassandra/pull/1461#discussion_r826396021



##########
File path: src/java/org/apache/cassandra/io/sstable/CQLSSTableWriter.java
##########
@@ -239,27 +240,40 @@ public CQLSSTableWriter rawAddRow(List<ByteBuffer> values)
             throw new InvalidRequestException(String.format(""Invalid number of arguments, expecting %d values but got %d"", boundNames.size(), values.size()));
 
         QueryOptions options = QueryOptions.forInternalCalls(null, values);
-        List<ByteBuffer> keys = insert.buildPartitionKeyNames(options);
-        SortedSet<Clustering<?>> clusterings = insert.createClustering(options);
+        List<ByteBuffer> keys = modificationStatement.buildPartitionKeyNames(options);
 
         long now = currentTimeMillis();
         // Note that we asks indexes to not validate values (the last 'false' arg below) because that triggers a 'Keyspace.open'
         // and that forces a lot of initialization that we don't want.
-        UpdateParameters params = new UpdateParameters(insert.metadata,
-                                                       insert.updatedColumns(),
+        UpdateParameters params = new UpdateParameters(modificationStatement.metadata,
+                                                       modificationStatement.updatedColumns(),
                                                        ClientState.forInternalCalls(),
                                                        options,
-                                                       insert.getTimestamp(TimeUnit.MILLISECONDS.toMicros(now), options),
+                                                       modificationStatement.getTimestamp(TimeUnit.MILLISECONDS.toMicros(now), options),
                                                        (int) TimeUnit.MILLISECONDS.toSeconds(now),
-                                                       insert.getTimeToLive(options),
+                                                       modificationStatement.getTimeToLive(options),
                                                        Collections.emptyMap());
 
         try
         {
-            for (ByteBuffer key : keys)
+            if (modificationStatement.hasSlices()) {
+                Slices slices = modificationStatement.createSlices(options);

Review comment:
       nit: if `slices.isEmpty()`, it would be unnecessary to create iterators from `keys` and `slices`, just like the other call-sites of `createSlices`

##########
File path: src/java/org/apache/cassandra/io/sstable/CQLSSTableWriter.java
##########
@@ -239,27 +240,40 @@ public CQLSSTableWriter rawAddRow(List<ByteBuffer> values)
             throw new InvalidRequestException(String.format(""Invalid number of arguments, expecting %d values but got %d"", boundNames.size(), values.size()));
 
         QueryOptions options = QueryOptions.forInternalCalls(null, values);
-        List<ByteBuffer> keys = insert.buildPartitionKeyNames(options);
-        SortedSet<Clustering<?>> clusterings = insert.createClustering(options);
+        List<ByteBuffer> keys = modificationStatement.buildPartitionKeyNames(options);
 
         long now = currentTimeMillis();
         // Note that we asks indexes to not validate values (the last 'false' arg below) because that triggers a 'Keyspace.open'
         // and that forces a lot of initialization that we don't want.
-        UpdateParameters params = new UpdateParameters(insert.metadata,
-                                                       insert.updatedColumns(),
+        UpdateParameters params = new UpdateParameters(modificationStatement.metadata,
+                                                       modificationStatement.updatedColumns(),
                                                        ClientState.forInternalCalls(),
                                                        options,
-                                                       insert.getTimestamp(TimeUnit.MILLISECONDS.toMicros(now), options),
+                                                       modificationStatement.getTimestamp(TimeUnit.MILLISECONDS.toMicros(now), options),
                                                        (int) TimeUnit.MILLISECONDS.toSeconds(now),
-                                                       insert.getTimeToLive(options),
+                                                       modificationStatement.getTimeToLive(options),
                                                        Collections.emptyMap());

Review comment:
       nit: it looks like to disable read-before-write for the partition, the most suitable value to  be supplied is `null` instead of empty map, which is fine, but just not necessary. `org.apache.cassandra.cql3.UpdateParameters#getPrefetchedRow` checks whether the field value is null.
   I do not know how empty map is used historically. 

##########
File path: src/java/org/apache/cassandra/io/sstable/CQLSSTableWriter.java
##########
@@ -583,24 +599,24 @@ private TableMetadata createTable(Types types)
         }
 
         /**
-         * Prepares insert statement for writing data to SSTable
+         * Prepares modification statement for writing data to SSTable
          *
-         * @return prepared Insert statement and it's bound names
+         * @return prepared modification statement and it's bound names
          */
-        private UpdateStatement prepareInsert()
+        private ModificationStatement prepareModificationStatement()
         {
             ClientState state = ClientState.forInternalCalls();
-            UpdateStatement insert = (UpdateStatement) insertStatement.prepare(state);
-            insert.validate(state);
+            ModificationStatement preparedModificationStatement = modificationStatement.prepare(state);
+            preparedModificationStatement.validate(state);
 
-            if (insert.hasConditions())
+            if (preparedModificationStatement.hasConditions())
                 throw new IllegalArgumentException(""Conditional statements are not supported"");
-            if (insert.isCounter())
+            if (preparedModificationStatement.isCounter())
                 throw new IllegalArgumentException(""Counter update statements are not supported"");

Review comment:
       Counter _modification_ statement?




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Mar/22 22:36;githubbot;600","JeetKunDoug commented on a change in pull request #1461:
URL: https://github.com/apache/cassandra/pull/1461#discussion_r828082098



##########
File path: src/java/org/apache/cassandra/io/sstable/CQLSSTableWriter.java
##########
@@ -239,27 +240,40 @@ public CQLSSTableWriter rawAddRow(List<ByteBuffer> values)
             throw new InvalidRequestException(String.format(""Invalid number of arguments, expecting %d values but got %d"", boundNames.size(), values.size()));
 
         QueryOptions options = QueryOptions.forInternalCalls(null, values);
-        List<ByteBuffer> keys = insert.buildPartitionKeyNames(options);
-        SortedSet<Clustering<?>> clusterings = insert.createClustering(options);
+        List<ByteBuffer> keys = modificationStatement.buildPartitionKeyNames(options);
 
         long now = currentTimeMillis();
         // Note that we asks indexes to not validate values (the last 'false' arg below) because that triggers a 'Keyspace.open'
         // and that forces a lot of initialization that we don't want.
-        UpdateParameters params = new UpdateParameters(insert.metadata,
-                                                       insert.updatedColumns(),
+        UpdateParameters params = new UpdateParameters(modificationStatement.metadata,
+                                                       modificationStatement.updatedColumns(),
                                                        ClientState.forInternalCalls(),
                                                        options,
-                                                       insert.getTimestamp(TimeUnit.MILLISECONDS.toMicros(now), options),
+                                                       modificationStatement.getTimestamp(TimeUnit.MILLISECONDS.toMicros(now), options),
                                                        (int) TimeUnit.MILLISECONDS.toSeconds(now),
-                                                       insert.getTimeToLive(options),
+                                                       modificationStatement.getTimeToLive(options),
                                                        Collections.emptyMap());

Review comment:
       Yeah - the empty map has been there since the code was originally written, and it's always essentially been a no-op from what I can see. Not sure if we want to change it in this PR/change though (it adds one extra call to EmptyMap.get, which always returns null, so I don't think it's functionally that big a deal, and if we do want to change it I'd like to keep it separated from the rest of these changes and test it to make sure I'm not missing something.




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 14:43;githubbot;600","JeetKunDoug commented on a change in pull request #1461:
URL: https://github.com/apache/cassandra/pull/1461#discussion_r828095655



##########
File path: src/java/org/apache/cassandra/io/sstable/CQLSSTableWriter.java
##########
@@ -239,27 +240,40 @@ public CQLSSTableWriter rawAddRow(List<ByteBuffer> values)
             throw new InvalidRequestException(String.format(""Invalid number of arguments, expecting %d values but got %d"", boundNames.size(), values.size()));
 
         QueryOptions options = QueryOptions.forInternalCalls(null, values);
-        List<ByteBuffer> keys = insert.buildPartitionKeyNames(options);
-        SortedSet<Clustering<?>> clusterings = insert.createClustering(options);
+        List<ByteBuffer> keys = modificationStatement.buildPartitionKeyNames(options);
 
         long now = currentTimeMillis();
         // Note that we asks indexes to not validate values (the last 'false' arg below) because that triggers a 'Keyspace.open'
         // and that forces a lot of initialization that we don't want.
-        UpdateParameters params = new UpdateParameters(insert.metadata,
-                                                       insert.updatedColumns(),
+        UpdateParameters params = new UpdateParameters(modificationStatement.metadata,
+                                                       modificationStatement.updatedColumns(),
                                                        ClientState.forInternalCalls(),
                                                        options,
-                                                       insert.getTimestamp(TimeUnit.MILLISECONDS.toMicros(now), options),
+                                                       modificationStatement.getTimestamp(TimeUnit.MILLISECONDS.toMicros(now), options),
                                                        (int) TimeUnit.MILLISECONDS.toSeconds(now),
-                                                       insert.getTimeToLive(options),
+                                                       modificationStatement.getTimeToLive(options),
                                                        Collections.emptyMap());
 
         try
         {
-            for (ByteBuffer key : keys)
+            if (modificationStatement.hasSlices()) {
+                Slices slices = modificationStatement.createSlices(options);

Review comment:
       At first blush, I think it's impossible for `createSlices` to return an empty set of slices for the use-cases we are supporting (at least all unit tests that hit this have at least one slice), which is I'm assuming why Alex left the check off in his original patch. If `slices` could be empty, we'd end up having a no-key delete, which wouldn't make any sense (although it would also just skip this whole block anyway, so it wouldn't add a delete).




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 14:58;githubbot;600","JeetKunDoug commented on a change in pull request #1461:
URL: https://github.com/apache/cassandra/pull/1461#discussion_r828239226



##########
File path: src/java/org/apache/cassandra/io/sstable/CQLSSTableWriter.java
##########
@@ -583,24 +599,24 @@ private TableMetadata createTable(Types types)
         }
 
         /**
-         * Prepares insert statement for writing data to SSTable
+         * Prepares modification statement for writing data to SSTable
          *
-         * @return prepared Insert statement and it's bound names
+         * @return prepared modification statement and it's bound names
          */
-        private UpdateStatement prepareInsert()
+        private ModificationStatement prepareModificationStatement()
         {
             ClientState state = ClientState.forInternalCalls();
-            UpdateStatement insert = (UpdateStatement) insertStatement.prepare(state);
-            insert.validate(state);
+            ModificationStatement preparedModificationStatement = modificationStatement.prepare(state);
+            preparedModificationStatement.validate(state);
 
-            if (insert.hasConditions())
+            if (preparedModificationStatement.hasConditions())
                 throw new IllegalArgumentException(""Conditional statements are not supported"");
-            if (insert.isCounter())
+            if (preparedModificationStatement.isCounter())
                 throw new IllegalArgumentException(""Counter update statements are not supported"");

Review comment:
       Yup - fixed




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 17:05;githubbot;600","smiklosovic closed pull request #1461:
URL: https://github.com/apache/cassandra/pull/1461


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Mar/22 23:13;githubbot;600","smiklosovic closed pull request #1461:
URL: https://github.com/apache/cassandra/pull/1461


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Mar/22 15:05;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/22 14:56;drohrer;cassandra-14797-update1.diff;https://issues.apache.org/jira/secure/attachment/13041281/cassandra-14797-update1.diff","21/Feb/22 15:55;drohrer;cassandra-14797.patch;https://issues.apache.org/jira/secure/attachment/13040307/cassandra-14797.patch",,,,,,,,,,,,2.0,drohrer,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 17 20:27:36 UTC 2022,,,,,,,,,,,"0|i3ypcf:",9223372036854775807,,,,,,,,,,,samt,ycai,,Low,,4.1,,,https://github.com/apache/cassandra/commit/f1c1694e4329a1f6dba8b978edaed4312e235116,,,,,,,,,See PR in https://github.com/apache/cassandra/pull/1461 - new unit tests covering additional functionality added to CQLSSTableWriter included.,,,,,"01/Oct/18 20:01;urandom;Patch forthcoming (as soon as I suss out the state-of-the-art for this);;;","01/Oct/18 21:34;urandom;Convention for posting patches/branches/tests notwithstanding...


||3.11||trunk||
|[branch|https://github.com/eevans/cassandra/tree/14797-3.11]|[branch|https://github.com/eevans/cassandra/tree/14797-trunk]|
|[utest|https://circleci.com/gh/eevans/cassandra/3]|[utest|https://circleci.com/gh/eevans/cassandra/6]|;;;","10/Feb/22 19:38;drohrer;Interestingly, I also started going down this path back in 2018 and ended up dropping the work, but need to pick it up now. I'll have a patch  against current trunk that includes my previous work + Eric's patch (Thanks Eric, if you're still paying attention here!) within the next few business days.;;;","18/Feb/22 22:55;drohrer;PR in https://github.com/apache/cassandra/pull/1461 against trunk to get Circle running.;;;","21/Feb/22 15:56;drohrer;Added a patch with just the code changes and not circle config just in case... there are 2 commits on the PR branch to keep them separate as well.;;;","01/Mar/22 19:13;samt;Looks good to me. I haven't been able to get a clean CI run yet, but that looks to be down to unrelated issues. I'll keep on at that, but in the meantime this could use a second reviewer.;;;","08/Mar/22 14:39;samt;CI issues were mostly attributable to CASSANDRA-17351/CASSANDRA-15234. A fix is in flight for those, and with the workaround applied CI looks good, with only a handful of known flaky tests failing: 

|[branch|https://github.com/beobal/cassandra/tree/CASSANDRA-14797-trunk]|[J8|https://app.circleci.com/pipelines/github/beobal/cassandra/391/workflows/2f2fbea3-9234-4620-80e7-b30255cad613]|[J11|https://app.circleci.com/pipelines/github/beobal/cassandra/391/workflows/7ea28158-4ba9-48b2-b45c-a8211c49e9d1]|
 ;;;","17/Mar/22 00:07;yifanc;+1 on the patch;;;","17/Mar/22 14:57;drohrer;I've added the rebased/updated patch after addressing issues raised on the PR (https://issues.apache.org/jira/secure/attachment/13041281/cassandra-14797-update1.diff);;;","17/Mar/22 17:34;yifanc;Starting commit

CI Results:
||Branch||Source||Circle CI||
|trunk|[branch|https://github.com/yifan-c/cassandra/tree/commit_remote_branch/CASSANDRA-14797-trunk-EA428EC0-9498-4472-A3E3-D3DE17E66937]|[build|https://app.circleci.com/pipelines/github/yifan-c/cassandra?branch=commit_remote_branch%2FCASSANDRA-14797-trunk-EA428EC0-9498-4472-A3E3-D3DE17E66937]|

A few test failures that are unrelated to the patch. ;;;","17/Mar/22 20:27;yifanc;Committed into trunk as [f1c1694e4|https://github.com/apache/cassandra/commit/f1c1694e4329a1f6dba8b978edaed4312e235116];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid calling iter.next() in a loop when notifying indexers about range tombstones,CASSANDRA-14794,13187833,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,27/Sep/18 09:03,15/May/20 08:06,13/Jul/23 08:37,28/Sep/18 10:57,3.0.18,3.11.4,4.0,4.0-alpha1,,,Legacy/Local Write-Read Paths,,,,0,,,,"In [SecondaryIndexManager|https://github.com/apache/cassandra/blob/914c66685c5bebe1624d827a9b4562b73a08c297/src/java/org/apache/cassandra/index/SecondaryIndexManager.java#L901-L902] - avoid calling {{.next()}} in the {{.forEach(..)}}",,cscotta,ifesdjeen,jay.zhuang,marcuse,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,Challenging,Adhoc Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 28 10:57:25 UTC 2018,,,,,,,,,,,"0|i3yk4n:",9223372036854775807,,,,,,,,,,,ifesdjeen,samt,,Normal,,3.0.0,,,,,,,,,,,,,,,,,"27/Sep/18 16:17;marcuse;https://github.com/krummas/cassandra/commits/marcuse/14794
tests: https://circleci.com/workflow-run/8413c7d8-bd59-4c78-8ae8-7d529c55ab1f;;;","27/Sep/18 16:58;ifesdjeen;+1, thank you for the patch!;;;","28/Sep/18 08:31;samt;+1;;;","28/Sep/18 10:57;marcuse;and committed as {{30d2835809e119173b1124b3eecb134e3a8c19b6}} to 3.0 and merged up, thanks!

tests for the other branches: [3.11|https://circleci.com/workflow-run/6af0ef35-4be8-4f7c-8a8a-30ccaddc7b1f] [trunk|https://circleci.com/workflow-run/ff02648a-a795-4af1-a742-9673acccde41];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve system table handling when losing a disk when using JBOD,CASSANDRA-14793,13187646,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,blerer,marcuse,marcuse,26/Sep/18 15:32,16/Mar/22 14:01,13/Jul/23 08:37,04/Feb/21 14:14,4.0,4.0-rc1,,,,,Legacy/Core,,,,0,,,,"We should improve the way we handle disk failures when losing a disk in a JBOD setup

 One way could be to pin the system tables to a special data directory.",,adelapena,benedict,blerer,djoshi,e.dimitrova,jasonstack,jay.zhuang,jeromatron,KurtG,marcuse,urandom,zznate,,,,,,,,,,,,,,"blerer opened a new pull request #675:
URL: https://github.com/apache/cassandra/pull/675


   The patch ensure that the data of the local system keyspaces is stored by default within the first data directory (at the exception of the paxos table) to allow the node to tolerate a failure of the disks associated with the other directories.
   The patch also allow users to configure a different directory for the system keyspaces. This would allow people to use a disk providing redundancy to support the lost of any of the disks used to store the data.
   On startup existing system keyspace data will be automatically migrated to support 4.0 upgrades or configuration changes (use of a separate disk for the system keyspaces). 


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jul/20 12:09;githubbot;600","blerer opened a new pull request #100:
URL: https://github.com/apache/cassandra-dtest/pull/100


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Sep/20 13:59;githubbot;600","blerer closed pull request #100:
URL: https://github.com/apache/cassandra-dtest/pull/100


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Sep/20 14:10;githubbot;600","adelapena commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r535239383



##########
File path: conf/cassandra.yaml
##########
@@ -203,6 +203,12 @@ partitioner: org.apache.cassandra.dht.Murmur3Partitioner
 # data_file_directories:
 #     - /var/lib/cassandra/data
 
+# Directory were Cassandra should store the data of the local system keyspaces.
+# By default Cassandra will store the data of the local system keyspaces in the first of the data directories.
+# This approach ensure that if one of the other disk is lost Cassandra can continue to operate. For extra security
+# this setting allow to store those data on a different directory that provide redundancy.

Review comment:
       ```suggestion
   By default Cassandra will store the data of the local system keyspaces in the first of the data directories specified
   # by data_file_directories.
   # This approach ensures that if one of the other disks is lost Cassandra can continue to operate. For extra security
   # this setting allows to store those data on a different directory that provides redundancy.
   ```

##########
File path: NEWS.txt
##########
@@ -38,6 +38,15 @@ using the provided 'sstableupgrade' tool.
 
 New features
 ------------
+    - The data of the system keyspaces using a local strategy (at the exception of the system.paxos table)
+      is now stored by default in the first data directory. This approach will allow the server
+      to tolerate the failure of the other disks. To ensure that a disk failure will not bring
+      a node down, it is possible to use the system_data_file_directory yaml property to store
+      the system keyspaces data on a disk that provide redundancy.
+      On node startup the system keyspace data will be automatically migrated if needed to the
+      correct location. If a specific disk has been used for some time and the system keyspaces

Review comment:
       ```suggestion
         the local system keyspaces data on a directory that provides redundancy.
         On node startup the local system keyspaces data will be automatically migrated if needed to the
         correct location. If a specific disk has been used for some time and the local system keyspaces
   ```

##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1787,11 +1814,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which support redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where should be stored the local system keyspaces data.
+     *
+     * <p>If the {@code system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guaranty that the server can tolerate the lost of n - 1 disks.</p>
+     *
+     * @return the locations where should be stored the local system keyspaces data
+     */
+    public static String[] getSystemKeyspacesDataFileLocations()
+    {
+        if (conf.system_data_file_directory != null)
+            return new String[] {conf.system_data_file_directory};
+
+        return conf.data_file_directories.length == 0  ? conf.data_file_directories
+                                                       : new String[] {conf.data_file_directories[0]};
+    }
+
+    /**
+     * Returns the locations where should be stored the non local system keyspaces data.
+     *
+     * @return the locations where should be stored the non local system keyspaces data

Review comment:
       ```suggestion
        * Returns the locations where the non local system keyspaces data should be stored.
        *
        * @return the locations where the non local system keyspaces data should be stored.
   ```

##########
File path: src/java/org/apache/cassandra/db/Directories.java
##########
@@ -591,10 +589,30 @@ public static File getBackupsDirectory(File location)
         }
     }
 
+    /**
+     * Checks if the specified table should be stored with locale system data.
+     *
+     * <p> To minimize the risk of failures, SSTables for local system keyspaces must be stored in a single data
+     * directory. The only exception to this is the system paxos table as it can be a high traffic table.</p>
+     *
+     * @param keyspace the keyspace name
+     * @param table the table name
+     * @return {@code true} if the specified table should be stored with locale system data, {@code false} otherwise.

Review comment:
       ```suggestion
        * Checks if the specified table should be stored with local system data.
        *
        * <p> To minimize the risk of failures, SSTables for local system keyspaces must be stored in a single data
        * directory. The only exception to this is the system paxos table as it can be a high traffic table.</p>
        *
        * @param keyspace the keyspace name
        * @param table the table name
        * @return {@code true} if the specified table should be stored with local system data, {@code false} otherwise.
   ```

##########
File path: src/java/org/apache/cassandra/db/Directories.java
##########
@@ -631,6 +649,106 @@ public String toString()
         }
     }
 
+    /**
+     * Data directories used to store keyspace data.
+     */
+    public static final class DataDirectories implements Iterable<DataDirectory>
+    {
+        /**
+         * The directories for storing the system keyspaces.
+         */
+        private final DataDirectory[] systemKeyspaceDataDirectories;
+
+        /**
+         * The directories where should be stored the data of the non system keyspaces.
+         */
+        private final DataDirectory[] nonSystemKeyspacesDirectories;
+
+
+        public DataDirectories(String[] locationsForNonSystemKeyspaces, String[] locationsForSystemKeyspace)
+        {
+            nonSystemKeyspacesDirectories = toDataDirectories(locationsForNonSystemKeyspaces);
+            systemKeyspaceDataDirectories = toDataDirectories(locationsForSystemKeyspace);
+        }
+
+        private static DataDirectory[] toDataDirectories(String... locations)
+        {
+            DataDirectory[] directories = new DataDirectory[locations.length];
+            for (int i = 0; i < locations.length; ++i)
+                directories[i] = new DataDirectory(new File(locations[i]));
+            return directories;
+        }
+
+        /**
+         * Returns the data directories used to store the data of the specified keyspace.
+         *
+         * @param keyspace the keyspace name
+         * @return the data directories used to store the data of the specified keyspace
+         */
+        public DataDirectory[] getDataDirectoriesUsedBy(String keyspace)
+        {
+            if (SchemaConstants.SYSTEM_KEYSPACE_NAME.equals(keyspace)
+                    && !ArrayUtils.isEmpty(systemKeyspaceDataDirectories)
+                    && !ArrayUtils.contains(nonSystemKeyspacesDirectories, systemKeyspaceDataDirectories[0]))
+            {
+                DataDirectory[] directories = Arrays.copyOf(nonSystemKeyspacesDirectories, nonSystemKeyspacesDirectories.length + 1);
+                directories[directories.length - 1] = systemKeyspaceDataDirectories[0];
+                return directories;
+            }
+            return SchemaConstants.isLocalSystemKeyspace(keyspace) ? systemKeyspaceDataDirectories
+                                                                   : nonSystemKeyspacesDirectories;
+        }
+
+        /**
+         * Returns the data directories for the specified keyspace.
+         *
+         * @param table the table metadata
+         * @return the data directories for the specified keyspace

Review comment:
       ```suggestion
            * Returns the data directories for the specified table.
            *
            * @param table the table metadata
            * @return the data directories for the specified table
   ```

##########
File path: src/java/org/apache/cassandra/db/Directories.java
##########
@@ -631,6 +649,106 @@ public String toString()
         }
     }
 
+    /**
+     * Data directories used to store keyspace data.
+     */
+    public static final class DataDirectories implements Iterable<DataDirectory>
+    {
+        /**
+         * The directories for storing the system keyspaces.
+         */
+        private final DataDirectory[] systemKeyspaceDataDirectories;
+
+        /**
+         * The directories where should be stored the data of the non system keyspaces.

Review comment:
       ```suggestion
            * The directories where the data of the non local system keyspaces should be stored.
   ```

##########
File path: src/java/org/apache/cassandra/service/StorageService.java
##########
@@ -3291,12 +3291,29 @@ public String getKeyspaceReplicationInfo(String keyspaceName)
         return stringify(Gossiper.instance.getUnreachableMembers(), true);
     }
 
+    @Override
     public String[] getAllDataFileLocations()
     {
-        String[] locations = DatabaseDescriptor.getAllDataFileLocations();
-        for (int i = 0; i < locations.length; i++)
-            locations[i] = FileUtils.getCanonicalPath(locations[i]);
-        return locations;
+        return getCanonicalPaths(DatabaseDescriptor.getAllDataFileLocations());
+    }
+
+    private String[] getCanonicalPaths(String[] paths)
+    {
+        for (int i = 0; i < paths.length; i++)
+            paths[i] = FileUtils.getCanonicalPath(paths[i]);
+        return paths;
+    }

Review comment:
       Overwriting the input array seems like it could have risky side effects, like overriding the contents of `DatabaseDescriptor.conf.data_file_directories`. I think we should return a new array.

##########
File path: src/java/org/apache/cassandra/io/FSDiskFullWriteError.java
##########
@@ -18,16 +18,22 @@
 
 package org.apache.cassandra.io;
 
+import java.io.File;
+import java.io.IOException;
+
 public class FSDiskFullWriteError extends FSWriteError
 {
-    public FSDiskFullWriteError(Throwable cause, String path)
+    public FSDiskFullWriteError(String keyspace, long mutationSize)
     {
-        super(cause, path);
+        super(new IOException(String.format(""Insufficient disk space to write %s bytes into the %s keyspace"",
+                                            mutationSize,
+                                            keyspace)),
+              new File(""""));

Review comment:
       Nit: Both `FSDiskFullWriteError` and `FSNoDiskAvailableForWriteError` call the super constructor of `FSWriteError` with `new File("""")`. Maybe we could add a new `FSWriteError` constructor without the file argument.

##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1787,11 +1814,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which support redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where should be stored the local system keyspaces data.
+     *
+     * <p>If the {@code system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guaranty that the server can tolerate the lost of n - 1 disks.</p>

Review comment:
       Nit: Although strictly correct, the `can tolerate the lost of n - 1 disks` might sound like the descriptions of RAID levels where they can tolerate the loss of any n - 1 disks, where here we can only tolerate the loss of the last n - 1 disks, which is worst (but better than before). I'd rephrase to something like `can tolerate the loss of any disk but the first one`, wdyt?

##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1787,11 +1814,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which support redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where should be stored the local system keyspaces data.

Review comment:
       ```suggestion
        * Returns the locations where the local system keyspaces data should be stored.
   ```

##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1787,11 +1814,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which support redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where should be stored the local system keyspaces data.
+     *
+     * <p>If the {@code system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guaranty that the server can tolerate the lost of n - 1 disks.</p>

Review comment:
       ```suggestion
        * in the first data directory. This approach guarantees that the server can tolerate the lost of n - 1 disks.</p>
   ```

##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1787,11 +1814,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which support redundancy.

Review comment:
       ```suggestion
        * Checks if the local system data must be stored in a specific location which supports redundancy.
   ```

##########
File path: src/java/org/apache/cassandra/db/Directories.java
##########
@@ -512,6 +507,9 @@ public boolean hasAvailableDiskSpace(long estimatedSSTables, long expectedTotalW
                 allowedDirs.add(dir);
         }
 
+        if (allowedDirs.isEmpty())
+            throw new FSNoDiskAvailableForWriteError(metadata.keyspace);
+

Review comment:
       Not related to the patch, but we could simplify the call to `Collections.sort` right below with:
   ```java
   allowedDirs.sort(Comparator.comparing(o -> o.location));
   ``` 

##########
File path: src/java/org/apache/cassandra/db/ColumnFamilyStore.java
##########
@@ -2740,4 +2730,81 @@ public boolean getNeverPurgeTombstones()
     {
         return neverPurgeTombstones;
     }
+
+    /**
+     * The thread pools used to flush memtables.
+     *
+     * <p>Each disk has its own set of thread pools to perform memtable flushes.</p>
+     * <p>Based on the configuration. System keyspaces can have their own disk
+     * to allow for special redundency mechanism. If it is the case the executor services returned for
+     * system keyspace will be differents from the ones for the other keyspaces.</p>
+     */
+    private static final class PerDiskFlushExecutors
+    {
+        /**
+         * The flush executors for non system keyspaces.
+         */
+        private final ExecutorService[] nonSystemflushExecutors;
+
+        /**
+         * The flush executors for system keyspaces.
+         */
+        private final ExecutorService[] systemDiskFlushExecutors;
+
+        public PerDiskFlushExecutors(int flushWriters,
+                                     String[] locationsForNonSystemKeyspaces,
+                                     boolean useSpecificLocationForSystemKeyspaces)
+        {
+            ExecutorService[] flushExecutors = createPerDiskFlushWriters(locationsForNonSystemKeyspaces.length, flushWriters);
+            nonSystemflushExecutors = flushExecutors;
+            systemDiskFlushExecutors = useSpecificLocationForSystemKeyspaces ? new ExecutorService[] {newThreadPool(""SystemKeyspacesDiskMemtableFlushWriter"", flushWriters)}
+                                                                             : new ExecutorService[] {flushExecutors[0]};
+        }
+
+        private ExecutorService[] createPerDiskFlushWriters(int numberOfExecutors, int flushWriters)
+        {
+            ExecutorService[] flushExecutors = new ExecutorService[numberOfExecutors];
+
+            for (int i = 0; i < numberOfExecutors; i++)
+            {
+                flushExecutors[i] = newThreadPool(""PerDiskMemtableFlushWriter_"" + i, flushWriters);
+            }
+            return flushExecutors;
+        }
+
+        private static JMXEnabledThreadPoolExecutor newThreadPool(String poolName, int size)
+        {
+            return new JMXEnabledThreadPoolExecutor(size,
+                                                    Stage.KEEP_ALIVE_SECONDS,
+                                                    TimeUnit.SECONDS,
+                                                    new LinkedBlockingQueue<Runnable>(),
+                                                    new NamedThreadFactory(poolName),
+                                                    ""internal"");
+        }
+
+        /**
+         * Returns the flush executors for the specified keyspace.
+         *
+         * @param keyspaceName the keyspace name
+         * @param tableName the table name
+         * @return the flush executors that should be used for flushing the memtables of the specified keyspace.
+         */
+        public ExecutorService[] getExecutorsFor(String keyspaceName, String tableName)
+        {
+            return Directories.isStoredInSystemKeyspacesDataLocation(keyspaceName, tableName) ? systemDiskFlushExecutors
+                                                                  : nonSystemflushExecutors;
+        }
+
+        /**
+         * Appends all the {@code ExecutorService} used for flushes to the colection.
+         *
+         * @param collection the colection to append to.
+         */
+        public void appendAllExecutors(Collection<ExecutorService> collection)
+        {
+            Collections.addAll(collection, nonSystemflushExecutors);
+            if (nonSystemflushExecutors != systemDiskFlushExecutors)

Review comment:
       Nothing wrong with how the `!=` operator is used here but a comment about its intentionality and/or a `@SuppressWarnings(""ArrayEquality"")` annotation could be helpful for suspicious readers.

##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1787,11 +1814,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which support redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where should be stored the local system keyspaces data.
+     *
+     * <p>If the {@code system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guaranty that the server can tolerate the lost of n - 1 disks.</p>
+     *
+     * @return the locations where should be stored the local system keyspaces data
+     */
+    public static String[] getSystemKeyspacesDataFileLocations()
+    {
+        if (conf.system_data_file_directory != null)
+            return new String[] {conf.system_data_file_directory};
+
+        return conf.data_file_directories.length == 0  ? conf.data_file_directories
+                                                       : new String[] {conf.data_file_directories[0]};
+    }
+
+    /**
+     * Returns the locations where should be stored the non local system keyspaces data.
+     *
+     * @return the locations where should be stored the non local system keyspaces data
+     */
+    public static String[] getNonSystemKeyspacesDataFileLocations()

Review comment:
       Naming this is tricky, since these locations actually contain data of system keyspaces that are not local. Maybe `getSystemKeyspacesDataFileLocations` can be changed to the even longer `getLocalSystemKeyspacesDataFileLocations`, but I can't really think of a better name for this one.

##########
File path: src/java/org/apache/cassandra/db/ColumnFamilyStore.java
##########
@@ -2740,4 +2730,81 @@ public boolean getNeverPurgeTombstones()
     {
         return neverPurgeTombstones;
     }
+
+    /**
+     * The thread pools used to flush memtables.
+     *
+     * <p>Each disk has its own set of thread pools to perform memtable flushes.</p>
+     * <p>Based on the configuration. System keyspaces can have their own disk
+     * to allow for special redundency mechanism. If it is the case the executor services returned for
+     * system keyspace will be differents from the ones for the other keyspaces.</p>

Review comment:
       ```suggestion
        * to allow for special redundancy mechanism. If it is the case the executor services returned for
        * local system keyspaces will be different from the ones for the other keyspaces.</p>
   ```

##########
File path: src/java/org/apache/cassandra/db/Directories.java
##########
@@ -631,6 +649,106 @@ public String toString()
         }
     }
 
+    /**
+     * Data directories used to store keyspace data.
+     */
+    public static final class DataDirectories implements Iterable<DataDirectory>
+    {
+        /**
+         * The directories for storing the system keyspaces.

Review comment:
       ```suggestion
            * The directories for storing the local system keyspaces.
   ```

##########
File path: src/java/org/apache/cassandra/db/Directories.java
##########
@@ -631,6 +649,106 @@ public String toString()
         }
     }
 
+    /**
+     * Data directories used to store keyspace data.
+     */
+    public static final class DataDirectories implements Iterable<DataDirectory>
+    {
+        /**
+         * The directories for storing the system keyspaces.
+         */
+        private final DataDirectory[] systemKeyspaceDataDirectories;
+
+        /**
+         * The directories where should be stored the data of the non system keyspaces.
+         */
+        private final DataDirectory[] nonSystemKeyspacesDirectories;
+
+
+        public DataDirectories(String[] locationsForNonSystemKeyspaces, String[] locationsForSystemKeyspace)
+        {
+            nonSystemKeyspacesDirectories = toDataDirectories(locationsForNonSystemKeyspaces);
+            systemKeyspaceDataDirectories = toDataDirectories(locationsForSystemKeyspace);
+        }
+
+        private static DataDirectory[] toDataDirectories(String... locations)
+        {
+            DataDirectory[] directories = new DataDirectory[locations.length];
+            for (int i = 0; i < locations.length; ++i)
+                directories[i] = new DataDirectory(new File(locations[i]));
+            return directories;
+        }
+
+        /**
+         * Returns the data directories used to store the data of the specified keyspace.
+         *
+         * @param keyspace the keyspace name
+         * @return the data directories used to store the data of the specified keyspace
+         */
+        public DataDirectory[] getDataDirectoriesUsedBy(String keyspace)
+        {
+            if (SchemaConstants.SYSTEM_KEYSPACE_NAME.equals(keyspace)
+                    && !ArrayUtils.isEmpty(systemKeyspaceDataDirectories)
+                    && !ArrayUtils.contains(nonSystemKeyspacesDirectories, systemKeyspaceDataDirectories[0]))
+            {
+                DataDirectory[] directories = Arrays.copyOf(nonSystemKeyspacesDirectories, nonSystemKeyspacesDirectories.length + 1);
+                directories[directories.length - 1] = systemKeyspaceDataDirectories[0];
+                return directories;
+            }
+            return SchemaConstants.isLocalSystemKeyspace(keyspace) ? systemKeyspaceDataDirectories
+                                                                   : nonSystemKeyspacesDirectories;
+        }
+
+        /**
+         * Returns the data directories for the specified keyspace.
+         *
+         * @param table the table metadata
+         * @return the data directories for the specified keyspace
+         */
+        public DataDirectory[] getDataDirectoriesFor(TableMetadata table)
+        {
+            return isStoredInSystemKeyspacesDataLocation(table.keyspace, table.name) ? systemKeyspaceDataDirectories
+                                                                                     : nonSystemKeyspacesDirectories;
+        }
+
+        @Override
+        public Iterator<DataDirectory> iterator()
+        {
+            Iterator<DataDirectory> iter = Iterators.forArray(nonSystemKeyspacesDirectories);
+
+            if (nonSystemKeyspacesDirectories == systemKeyspaceDataDirectories)

Review comment:
       Same as before, we could add comment about the intentional use of `==` and/or a `@SuppressWarnings(""ArrayEquality"")` annotation.

##########
File path: src/java/org/apache/cassandra/service/CassandraDaemon.java
##########
@@ -456,6 +476,78 @@ protected void setup()
         completeSetup();
     }
 
+    /**
+     * Checks if the data of the local system keyspaces need to be migrated to a different location.
+     *
+     * @throws IOException
+     */
+    private void migrateSystemDataIfNeeded() throws IOException
+    {
+        String importSystemDataFrom = System.getProperty(""cassandra.importSystemDataFilesFrom"");
+
+        // If there is only one directory and no system keyspace directory has been specified we do not need to do
+        // anything. If it is not the case we want to try to migrate the data.
+        if (DatabaseDescriptor.useSpecificLocationForSystemData()
+                || DatabaseDescriptor.getNonSystemKeyspacesDataFileLocations().length > 1
+                || importSystemDataFrom != null)

Review comment:
       We could invert this condition and return directly to reduce nesting:
   ```java
   if (!DatabaseDescriptor.useSpecificLocationForSystemData()
       && DatabaseDescriptor.getNonSystemKeyspacesDataFileLocations().length <= 1
       && importSystemDataFrom == null)
       return;
   ```

##########
File path: src/java/org/apache/cassandra/db/Directories.java
##########
@@ -1002,8 +1120,7 @@ public long getTrueAllocatedSizeIn(File input)
 
     public static List<File> getKSChildDirectories(String ksName)
     {
-        return getKSChildDirectories(ksName, dataDirectories);
-
+        return getKSChildDirectories(ksName, dataDirectories.getDataDirectoriesUsedBy(ksName));

Review comment:
       Maybe the two versions of `getKSChildDirectories` can be merged, given that one is the only caller of the other?

##########
File path: src/java/org/apache/cassandra/db/ColumnFamilyStore.java
##########
@@ -2740,4 +2730,81 @@ public boolean getNeverPurgeTombstones()
     {
         return neverPurgeTombstones;
     }
+
+    /**
+     * The thread pools used to flush memtables.
+     *
+     * <p>Each disk has its own set of thread pools to perform memtable flushes.</p>
+     * <p>Based on the configuration. System keyspaces can have their own disk
+     * to allow for special redundency mechanism. If it is the case the executor services returned for
+     * system keyspace will be differents from the ones for the other keyspaces.</p>
+     */
+    private static final class PerDiskFlushExecutors
+    {
+        /**
+         * The flush executors for non system keyspaces.
+         */
+        private final ExecutorService[] nonSystemflushExecutors;
+
+        /**
+         * The flush executors for system keyspaces.
+         */
+        private final ExecutorService[] systemDiskFlushExecutors;
+
+        public PerDiskFlushExecutors(int flushWriters,
+                                     String[] locationsForNonSystemKeyspaces,
+                                     boolean useSpecificLocationForSystemKeyspaces)
+        {
+            ExecutorService[] flushExecutors = createPerDiskFlushWriters(locationsForNonSystemKeyspaces.length, flushWriters);
+            nonSystemflushExecutors = flushExecutors;
+            systemDiskFlushExecutors = useSpecificLocationForSystemKeyspaces ? new ExecutorService[] {newThreadPool(""SystemKeyspacesDiskMemtableFlushWriter"", flushWriters)}
+                                                                             : new ExecutorService[] {flushExecutors[0]};
+        }
+
+        private ExecutorService[] createPerDiskFlushWriters(int numberOfExecutors, int flushWriters)
+        {
+            ExecutorService[] flushExecutors = new ExecutorService[numberOfExecutors];
+
+            for (int i = 0; i < numberOfExecutors; i++)
+            {
+                flushExecutors[i] = newThreadPool(""PerDiskMemtableFlushWriter_"" + i, flushWriters);
+            }
+            return flushExecutors;
+        }
+
+        private static JMXEnabledThreadPoolExecutor newThreadPool(String poolName, int size)
+        {
+            return new JMXEnabledThreadPoolExecutor(size,
+                                                    Stage.KEEP_ALIVE_SECONDS,
+                                                    TimeUnit.SECONDS,
+                                                    new LinkedBlockingQueue<Runnable>(),

Review comment:
       ```suggestion
                                                       new LinkedBlockingQueue<>(),
   ```

##########
File path: src/java/org/apache/cassandra/service/DefaultFSErrorHandler.java
##########
@@ -26,10 +26,9 @@
 import org.apache.cassandra.config.DatabaseDescriptor;
 import org.apache.cassandra.db.DisallowedDirectories;
 import org.apache.cassandra.db.Keyspace;
-import org.apache.cassandra.io.FSError;
-import org.apache.cassandra.io.FSErrorHandler;
-import org.apache.cassandra.io.FSReadError;
+import org.apache.cassandra.io.*;
 import org.apache.cassandra.io.sstable.CorruptSSTableException;
+import org.apache.cassandra.schema.SchemaConstants;

Review comment:
       Nit: unused

##########
File path: src/java/org/apache/cassandra/service/CassandraDaemon.java
##########
@@ -456,6 +476,78 @@ protected void setup()
         completeSetup();
     }
 
+    /**
+     * Checks if the data of the local system keyspaces need to be migrated to a different location.
+     *
+     * @throws IOException
+     */
+    private void migrateSystemDataIfNeeded() throws IOException

Review comment:
       It would be nice to have some upgrade dtests for this, provided that we have the machinery to do so in either Java or Python dtests.

##########
File path: src/java/org/apache/cassandra/io/util/FileUtils.java
##########
@@ -920,4 +922,69 @@ public Object getAttribute(String attribute) throws IOException
             return fileStore.getAttribute(attribute);
         }
     }
+
+
+    /**
+     * Moves the files from a directory to another directory.
+     * <p>Once a file has been copied to the target directory it will be deleted from the source directory.
+     * If a file already exist in the target directory a warning will be logged and the file will not
+     * be deleted.</p>

Review comment:
       Nit: it also moves subdirectories.
   ```suggestion
   
       /**
        * Moves the contents of a directory to another directory.
        * <p>Once a file has been copied to the target directory it will be deleted from the source directory.
        * If a file already exists in the target directory a warning will be logged and the file will not
        * be deleted.</p>
   ```

##########
File path: src/java/org/apache/cassandra/service/StorageServiceMBean.java
##########
@@ -118,6 +118,20 @@
      */
     public String[] getAllDataFileLocations();
 
+    /**
+     * Returns the locations where should be stored the system keyspaces data.
+     *
+     * @return the locations where should be stored the system keyspaces data

Review comment:
       ```suggestion
        * Returns the locations where the local system keyspaces data should be stored.
        *
        * @return the locations where the local system keyspaces data should be stored
   ```

##########
File path: test/unit/org/apache/cassandra/db/DirectoriesTest.java
##########
@@ -88,18 +86,21 @@ public static void beforeClass() throws IOException
         tempDataDir.delete(); // hack to create a temp dir
         tempDataDir.mkdir();
 
-        Directories.overrideDataDirectoriesForTest(tempDataDir.getPath());
-        // Create two fake data dir for tests, one using CF directories, one that do not.
+       // Create two fake data dir for tests, one using CF directories, one that do not.

Review comment:
       ```suggestion
           // Create two fake data dir for tests, one using CF directories, one that do not.
   ```

##########
File path: src/java/org/apache/cassandra/service/DefaultFSErrorHandler.java
##########
@@ -67,6 +66,18 @@ public void handleFSError(FSError e)
                 StorageService.instance.stopTransports();
                 break;
             case best_effort:
+
+                // There are a few scenarios where we know that the node will not be able to operate properly

Review comment:
       ```suggestion
                   // There are a few scenarios where we know that the node will not be able to operate properly.
   ```

##########
File path: src/java/org/apache/cassandra/io/FSDiskFullWriteError.java
##########
@@ -18,16 +18,22 @@
 
 package org.apache.cassandra.io;
 
+import java.io.File;
+import java.io.IOException;
+
 public class FSDiskFullWriteError extends FSWriteError
 {
-    public FSDiskFullWriteError(Throwable cause, String path)
+    public FSDiskFullWriteError(String keyspace, long mutationSize)
     {
-        super(cause, path);
+        super(new IOException(String.format(""Insufficient disk space to write %s bytes into the %s keyspace"",

Review comment:
       ```suggestion
           super(new IOException(String.format(""Insufficient disk space to write %d bytes into the %s keyspace"",
   ```

##########
File path: src/java/org/apache/cassandra/service/CassandraDaemon.java
##########
@@ -212,6 +219,19 @@ protected void setup()
     {
         FileUtils.setFSErrorHandler(new DefaultFSErrorHandler());
 
+        // Since CASSANDRA-14793 the local system file data are not dispatched accross the data directories
+        // anymore to reduce the risks in case of disk failures. By consequence, the system need to ensure in case of
+        // upgrade that the old data files have been migrated to the new directories before we start deleting
+        // snapshot and upgrading system tables.

Review comment:
       ```suggestion
           // Since CASSANDRA-14793 the local system keyspaces data are not dispatched across the data directories
           // anymore to reduce the risks in case of disk failures. By consequence, the system need to ensure in case of
           // upgrade that the old data files have been migrated to the new directories before we start deleting
           // snapshots and upgrading system tables.
   ```

##########
File path: src/java/org/apache/cassandra/io/util/FileUtils.java
##########
@@ -920,4 +922,69 @@ public Object getAttribute(String attribute) throws IOException
             return fileStore.getAttribute(attribute);
         }
     }
+
+
+    /**
+     * Moves the files from a directory to another directory.
+     * <p>Once a file has been copied to the target directory it will be deleted from the source directory.
+     * If a file already exist in the target directory a warning will be logged and the file will not
+     * be deleted.</p>
+     *
+     * @param source the directory containing the files to move
+     * @param target the directory where the files must be moved
+     */
+    public static void moveRecursively(Path source, Path target) throws IOException
+    {
+        logger.info(""Moving {} to {}"" , source, target);
+
+        if (Files.isDirectory(source))
+        {
+            Files.createDirectories(target);
+
+            try (Stream<Path> paths = Files.list(source))
+            {
+                Path[] children = paths.toArray(Path[]::new);
+
+                for (Path child : children)
+                    moveRecursively(child, target.resolve(source.relativize(child)));
+            }
+
+            deleteDirectoryIfEmpty(source);
+        }
+        else
+        {
+            if (Files.exists(target))
+            {
+                logger.warn(""Cannot move the file {} to {} as the target file already exists."" , source, target);
+            }
+            else
+            {
+                Files.copy(source, target, StandardCopyOption.COPY_ATTRIBUTES);
+                Files.delete(source);
+            }
+        }
+    }
+
+    /**
+     * Deletes the specified directory if it is empty
+     *
+     * @param path the path to the directory
+     */
+    public static void deleteDirectoryIfEmpty(Path path) throws IOException

Review comment:
       I think this could be called with the path of a file, we should probably check that the path belongs to a directory. It would be nice to have a simple unit test for this method in `FileUtilsTest`.

##########
File path: test/unit/org/apache/cassandra/io/util/FileUtilsTest.java
##########
@@ -128,6 +128,75 @@ public void testIsContained()
         assertFalse(FileUtils.isContained(new File(""/tmp/abc/../abc""), new File(""/tmp/abcc"")));
     }
 
+    @Test
+    public void testMoveFiles() throws IOException

Review comment:
       Nice test

##########
File path: src/java/org/apache/cassandra/service/CassandraDaemon.java
##########
@@ -243,7 +263,7 @@ protected void setup()
         }
         catch (IOException e)
         {
-            exitOrFail(3, e.getMessage(), e.getCause());
+            exitOrFail(StartupException.ERR_WRONG_DISK_STATE, e.getMessage(), e.getCause());

Review comment:
       Good catch

##########
File path: src/java/org/apache/cassandra/service/CassandraDaemon.java
##########
@@ -456,6 +476,78 @@ protected void setup()
         completeSetup();
     }
 
+    /**
+     * Checks if the data of the local system keyspaces need to be migrated to a different location.
+     *
+     * @throws IOException
+     */
+    private void migrateSystemDataIfNeeded() throws IOException
+    {
+        String importSystemDataFrom = System.getProperty(""cassandra.importSystemDataFilesFrom"");
+
+        // If there is only one directory and no system keyspace directory has been specified we do not need to do
+        // anything. If it is not the case we want to try to migrate the data.
+        if (DatabaseDescriptor.useSpecificLocationForSystemData()
+                || DatabaseDescriptor.getNonSystemKeyspacesDataFileLocations().length > 1
+                || importSystemDataFrom != null)
+        {
+            // We can face several cases:
+            //  1) The system data are spread accross the data file locations and need to be moved to
+            //     the first data location (upgrade to 4.0)
+            //  2) The system data are spread accross the data file locations and need to be moved to
+            //     the system keyspace location configured by the user (upgrade to 4.0)
+            //  3) The system data are stored in the first data location and need to be moved to
+            //     the system keyspace location configured by the user (system_data_file_directory has been configured)
+            //  4) The system data have been stored in the system keyspace location configured by the user
+            //     and need to be moved to the first data location (the import of the data has been requested)

Review comment:
       Nice comment!




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Dec/20 15:36;githubbot;600","adelapena commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r535359116



##########
File path: src/java/org/apache/cassandra/service/CassandraDaemon.java
##########
@@ -456,6 +476,78 @@ protected void setup()
         completeSetup();
     }
 
+    /**
+     * Checks if the data of the local system keyspaces need to be migrated to a different location.
+     *
+     * @throws IOException
+     */
+    private void migrateSystemDataIfNeeded() throws IOException
+    {
+        String importSystemDataFrom = System.getProperty(""cassandra.importSystemDataFilesFrom"");

Review comment:
       I think that snake case is more common for system properties:
   ```suggestion
           String importSystemDataFrom = System.getProperty(""cassandra.import_system_data_files_from"");
   ```




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Dec/20 15:55;githubbot;600","adelapena commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r535359436



##########
File path: src/java/org/apache/cassandra/service/CassandraDaemon.java
##########
@@ -456,6 +476,78 @@ protected void setup()
         completeSetup();
     }
 
+    /**
+     * Checks if the data of the local system keyspaces need to be migrated to a different location.
+     *
+     * @throws IOException
+     */
+    private void migrateSystemDataIfNeeded() throws IOException
+    {
+        String importSystemDataFrom = System.getProperty(""cassandra.importSystemDataFilesFrom"");

Review comment:
       Probably we should comment the new property in [the documentation for cassandra-ev](https://github.com/apache/cassandra/blob/trunk/doc/source/configuration/cass_env_sh_file.rst) and in the [JVM options file](jvm-server.options).




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Dec/20 15:55;githubbot;600","adelapena commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r535362575



##########
File path: conf/cassandra.yaml
##########
@@ -203,6 +203,12 @@ partitioner: org.apache.cassandra.dht.Murmur3Partitioner
 # data_file_directories:
 #     - /var/lib/cassandra/data
 
+# Directory were Cassandra should store the data of the local system keyspaces.
+# By default Cassandra will store the data of the local system keyspaces in the first of the data directories.
+# This approach ensure that if one of the other disk is lost Cassandra can continue to operate. For extra security
+# this setting allow to store those data on a different directory that provide redundancy.
+# system_data_file_directory: 

Review comment:
       We should add this to [the documentation for `cassandra.yaml`](https://github.com/apache/cassandra/blob/trunk/doc/source/configuration/cass_yaml_file.rst).




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Dec/20 15:59;githubbot;600","adelapena commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r535494746



##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1787,11 +1814,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which support redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where should be stored the local system keyspaces data.
+     *
+     * <p>If the {@code system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guaranty that the server can tolerate the lost of n - 1 disks.</p>
+     *
+     * @return the locations where should be stored the local system keyspaces data
+     */
+    public static String[] getSystemKeyspacesDataFileLocations()
+    {
+        if (conf.system_data_file_directory != null)
+            return new String[] {conf.system_data_file_directory};
+
+        return conf.data_file_directories.length == 0  ? conf.data_file_directories
+                                                       : new String[] {conf.data_file_directories[0]};
+    }
+
+    /**
+     * Returns the locations where should be stored the non local system keyspaces data.
+     *
+     * @return the locations where should be stored the non local system keyspaces data
+     */
+    public static String[] getNonSystemKeyspacesDataFileLocations()

Review comment:
       I was thinking that what determines if a table goes into the sepearate directory is the fact that it uses `LocalStrategy`, so we want it safer in a single directory, possibly with redundancy. AFAIK this strategy is only used by system tables, but if somehow there were not-system keyspaces with that strategy we would want them in the safer directory, I guess. So, perhaps we could call this `getLocalKeyspacesDataFileLocations`, and the property in cassandra.yaml `local_data_file_directory`, etc.? Does it make any sense?




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Dec/20 18:52;githubbot;600","adelapena commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r535494746



##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1787,11 +1814,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which support redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where should be stored the local system keyspaces data.
+     *
+     * <p>If the {@code system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guaranty that the server can tolerate the lost of n - 1 disks.</p>
+     *
+     * @return the locations where should be stored the local system keyspaces data
+     */
+    public static String[] getSystemKeyspacesDataFileLocations()
+    {
+        if (conf.system_data_file_directory != null)
+            return new String[] {conf.system_data_file_directory};
+
+        return conf.data_file_directories.length == 0  ? conf.data_file_directories
+                                                       : new String[] {conf.data_file_directories[0]};
+    }
+
+    /**
+     * Returns the locations where should be stored the non local system keyspaces data.
+     *
+     * @return the locations where should be stored the non local system keyspaces data
+     */
+    public static String[] getNonSystemKeyspacesDataFileLocations()

Review comment:
       I was thinking that what determines if a table goes into the sepearate directory is the fact that it uses `LocalStrategy`, so we want it safer in a single directory, possibly with redundancy. AFAIK this strategy is only used by system tables, but if somehow there were not-system keyspaces with that strategy we would want them in the safer directory, I guess. So, perhaps we could call this `getNonLocalKeyspacesDataFileLocations`, and the property in cassandra.yaml `local_data_file_directory`, etc.? Does it make any sense?




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Dec/20 18:59;githubbot;600","adelapena commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r535494746



##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1787,11 +1814,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which support redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where should be stored the local system keyspaces data.
+     *
+     * <p>If the {@code system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guaranty that the server can tolerate the lost of n - 1 disks.</p>
+     *
+     * @return the locations where should be stored the local system keyspaces data
+     */
+    public static String[] getSystemKeyspacesDataFileLocations()
+    {
+        if (conf.system_data_file_directory != null)
+            return new String[] {conf.system_data_file_directory};
+
+        return conf.data_file_directories.length == 0  ? conf.data_file_directories
+                                                       : new String[] {conf.data_file_directories[0]};
+    }
+
+    /**
+     * Returns the locations where should be stored the non local system keyspaces data.
+     *
+     * @return the locations where should be stored the non local system keyspaces data
+     */
+    public static String[] getNonSystemKeyspacesDataFileLocations()

Review comment:
       I was thinking that what determines if a table goes into the sepearate directory is the fact that it uses `LocalStrategy`, so we want it safer in a single directory, possibly with redundancy. AFAIK this strategy is only used by some system tables. If somehow there were not-system keyspaces with that strategy we would also want them in the safer directory, I guess. So, perhaps we could call this `getNonLocalKeyspacesDataFileLocations`, and the property in cassandra.yaml `local_data_file_directory`, etc., if that makes any sense? I find local vs. non-local terminology easier than local-system vs. non-local-system-or-non-system. 




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Dec/20 19:07;githubbot;600","blerer commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r537676403



##########
File path: src/java/org/apache/cassandra/db/ColumnFamilyStore.java
##########
@@ -2740,4 +2730,81 @@ public boolean getNeverPurgeTombstones()
     {
         return neverPurgeTombstones;
     }
+
+    /**
+     * The thread pools used to flush memtables.
+     *
+     * <p>Each disk has its own set of thread pools to perform memtable flushes.</p>
+     * <p>Based on the configuration. System keyspaces can have their own disk
+     * to allow for special redundency mechanism. If it is the case the executor services returned for
+     * system keyspace will be differents from the ones for the other keyspaces.</p>
+     */
+    private static final class PerDiskFlushExecutors
+    {
+        /**
+         * The flush executors for non system keyspaces.
+         */
+        private final ExecutorService[] nonSystemflushExecutors;
+
+        /**
+         * The flush executors for system keyspaces.
+         */
+        private final ExecutorService[] systemDiskFlushExecutors;
+
+        public PerDiskFlushExecutors(int flushWriters,
+                                     String[] locationsForNonSystemKeyspaces,
+                                     boolean useSpecificLocationForSystemKeyspaces)
+        {
+            ExecutorService[] flushExecutors = createPerDiskFlushWriters(locationsForNonSystemKeyspaces.length, flushWriters);
+            nonSystemflushExecutors = flushExecutors;
+            systemDiskFlushExecutors = useSpecificLocationForSystemKeyspaces ? new ExecutorService[] {newThreadPool(""SystemKeyspacesDiskMemtableFlushWriter"", flushWriters)}
+                                                                             : new ExecutorService[] {flushExecutors[0]};
+        }
+
+        private ExecutorService[] createPerDiskFlushWriters(int numberOfExecutors, int flushWriters)
+        {
+            ExecutorService[] flushExecutors = new ExecutorService[numberOfExecutors];
+
+            for (int i = 0; i < numberOfExecutors; i++)
+            {
+                flushExecutors[i] = newThreadPool(""PerDiskMemtableFlushWriter_"" + i, flushWriters);
+            }
+            return flushExecutors;
+        }
+
+        private static JMXEnabledThreadPoolExecutor newThreadPool(String poolName, int size)
+        {
+            return new JMXEnabledThreadPoolExecutor(size,
+                                                    Stage.KEEP_ALIVE_SECONDS,
+                                                    TimeUnit.SECONDS,
+                                                    new LinkedBlockingQueue<Runnable>(),
+                                                    new NamedThreadFactory(poolName),
+                                                    ""internal"");
+        }
+
+        /**
+         * Returns the flush executors for the specified keyspace.
+         *
+         * @param keyspaceName the keyspace name
+         * @param tableName the table name
+         * @return the flush executors that should be used for flushing the memtables of the specified keyspace.
+         */
+        public ExecutorService[] getExecutorsFor(String keyspaceName, String tableName)
+        {
+            return Directories.isStoredInSystemKeyspacesDataLocation(keyspaceName, tableName) ? systemDiskFlushExecutors
+                                                                  : nonSystemflushExecutors;
+        }
+
+        /**
+         * Appends all the {@code ExecutorService} used for flushes to the colection.
+         *
+         * @param collection the colection to append to.
+         */
+        public void appendAllExecutors(Collection<ExecutorService> collection)
+        {
+            Collections.addAll(collection, nonSystemflushExecutors);
+            if (nonSystemflushExecutors != systemDiskFlushExecutors)

Review comment:
       After looking into it, I believe that this code is some left over from a previous version and is actually wrong. I will fix it. 




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Dec/20 17:10;githubbot;600","blerer commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r539426402



##########
File path: src/java/org/apache/cassandra/db/Directories.java
##########
@@ -631,6 +649,106 @@ public String toString()
         }
     }
 
+    /**
+     * Data directories used to store keyspace data.
+     */
+    public static final class DataDirectories implements Iterable<DataDirectory>
+    {
+        /**
+         * The directories for storing the system keyspaces.
+         */
+        private final DataDirectory[] systemKeyspaceDataDirectories;
+
+        /**
+         * The directories where should be stored the data of the non system keyspaces.
+         */
+        private final DataDirectory[] nonSystemKeyspacesDirectories;
+
+
+        public DataDirectories(String[] locationsForNonSystemKeyspaces, String[] locationsForSystemKeyspace)
+        {
+            nonSystemKeyspacesDirectories = toDataDirectories(locationsForNonSystemKeyspaces);
+            systemKeyspaceDataDirectories = toDataDirectories(locationsForSystemKeyspace);
+        }
+
+        private static DataDirectory[] toDataDirectories(String... locations)
+        {
+            DataDirectory[] directories = new DataDirectory[locations.length];
+            for (int i = 0; i < locations.length; ++i)
+                directories[i] = new DataDirectory(new File(locations[i]));
+            return directories;
+        }
+
+        /**
+         * Returns the data directories used to store the data of the specified keyspace.
+         *
+         * @param keyspace the keyspace name
+         * @return the data directories used to store the data of the specified keyspace
+         */
+        public DataDirectory[] getDataDirectoriesUsedBy(String keyspace)
+        {
+            if (SchemaConstants.SYSTEM_KEYSPACE_NAME.equals(keyspace)
+                    && !ArrayUtils.isEmpty(systemKeyspaceDataDirectories)
+                    && !ArrayUtils.contains(nonSystemKeyspacesDirectories, systemKeyspaceDataDirectories[0]))
+            {
+                DataDirectory[] directories = Arrays.copyOf(nonSystemKeyspacesDirectories, nonSystemKeyspacesDirectories.length + 1);
+                directories[directories.length - 1] = systemKeyspaceDataDirectories[0];
+                return directories;
+            }
+            return SchemaConstants.isLocalSystemKeyspace(keyspace) ? systemKeyspaceDataDirectories
+                                                                   : nonSystemKeyspacesDirectories;
+        }
+
+        /**
+         * Returns the data directories for the specified keyspace.
+         *
+         * @param table the table metadata
+         * @return the data directories for the specified keyspace
+         */
+        public DataDirectory[] getDataDirectoriesFor(TableMetadata table)
+        {
+            return isStoredInSystemKeyspacesDataLocation(table.keyspace, table.name) ? systemKeyspaceDataDirectories
+                                                                                     : nonSystemKeyspacesDirectories;
+        }
+
+        @Override
+        public Iterator<DataDirectory> iterator()
+        {
+            Iterator<DataDirectory> iter = Iterators.forArray(nonSystemKeyspacesDirectories);
+
+            if (nonSystemKeyspacesDirectories == systemKeyspaceDataDirectories)

Review comment:
       This one is also wrong. Also some leftover from my initial approach.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Dec/20 15:55;githubbot;600","blerer opened a new pull request #108:
URL: https://github.com/apache/cassandra-dtest/pull/108


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Dec/20 13:32;githubbot;600","blerer commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r545855986



##########
File path: src/java/org/apache/cassandra/service/StorageService.java
##########
@@ -3291,12 +3291,29 @@ public String getKeyspaceReplicationInfo(String keyspaceName)
         return stringify(Gossiper.instance.getUnreachableMembers(), true);
     }
 
+    @Override
     public String[] getAllDataFileLocations()
     {
-        String[] locations = DatabaseDescriptor.getAllDataFileLocations();
-        for (int i = 0; i < locations.length; i++)
-            locations[i] = FileUtils.getCanonicalPath(locations[i]);
-        return locations;
+        return getCanonicalPaths(DatabaseDescriptor.getAllDataFileLocations());
+    }
+
+    private String[] getCanonicalPaths(String[] paths)
+    {
+        for (int i = 0; i < paths.length; i++)
+            paths[i] = FileUtils.getCanonicalPath(paths[i]);
+        return paths;
+    }

Review comment:
       Good catch. :-)




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Dec/20 14:14;githubbot;600","blerer commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r545856409



##########
File path: conf/cassandra.yaml
##########
@@ -203,6 +203,12 @@ partitioner: org.apache.cassandra.dht.Murmur3Partitioner
 # data_file_directories:
 #     - /var/lib/cassandra/data
 
+# Directory were Cassandra should store the data of the local system keyspaces.
+# By default Cassandra will store the data of the local system keyspaces in the first of the data directories.
+# This approach ensure that if one of the other disk is lost Cassandra can continue to operate. For extra security
+# this setting allow to store those data on a different directory that provide redundancy.
+# system_data_file_directory: 

Review comment:
       Done




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Dec/20 14:14;githubbot;600","blerer commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r545858300



##########
File path: test/unit/org/apache/cassandra/io/util/FileUtilsTest.java
##########
@@ -128,6 +128,75 @@ public void testIsContained()
         assertFalse(FileUtils.isContained(new File(""/tmp/abc/../abc""), new File(""/tmp/abcc"")));
     }
 
+    @Test
+    public void testMoveFiles() throws IOException

Review comment:
       Thanks. :-)




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Dec/20 14:17;githubbot;600","blerer commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r545863755



##########
File path: src/java/org/apache/cassandra/io/util/FileUtils.java
##########
@@ -920,4 +922,69 @@ public Object getAttribute(String attribute) throws IOException
             return fileStore.getAttribute(attribute);
         }
     }
+
+
+    /**
+     * Moves the files from a directory to another directory.
+     * <p>Once a file has been copied to the target directory it will be deleted from the source directory.
+     * If a file already exist in the target directory a warning will be logged and the file will not
+     * be deleted.</p>
+     *
+     * @param source the directory containing the files to move
+     * @param target the directory where the files must be moved
+     */
+    public static void moveRecursively(Path source, Path target) throws IOException
+    {
+        logger.info(""Moving {} to {}"" , source, target);
+
+        if (Files.isDirectory(source))
+        {
+            Files.createDirectories(target);
+
+            try (Stream<Path> paths = Files.list(source))
+            {
+                Path[] children = paths.toArray(Path[]::new);
+
+                for (Path child : children)
+                    moveRecursively(child, target.resolve(source.relativize(child)));
+            }
+
+            deleteDirectoryIfEmpty(source);
+        }
+        else
+        {
+            if (Files.exists(target))
+            {
+                logger.warn(""Cannot move the file {} to {} as the target file already exists."" , source, target);
+            }
+            else
+            {
+                Files.copy(source, target, StandardCopyOption.COPY_ATTRIBUTES);
+                Files.delete(source);
+            }
+        }
+    }
+
+    /**
+     * Deletes the specified directory if it is empty
+     *
+     * @param path the path to the directory
+     */
+    public static void deleteDirectoryIfEmpty(Path path) throws IOException

Review comment:
       Done :-)




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Dec/20 14:27;githubbot;600","blerer commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r545864002



##########
File path: src/java/org/apache/cassandra/service/CassandraDaemon.java
##########
@@ -243,7 +263,7 @@ protected void setup()
         }
         catch (IOException e)
         {
-            exitOrFail(3, e.getMessage(), e.getCause());
+            exitOrFail(StartupException.ERR_WRONG_DISK_STATE, e.getMessage(), e.getCause());

Review comment:
       Thanks :-)




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Dec/20 14:27;githubbot;600","blerer commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r545899976



##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1787,11 +1814,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which support redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where should be stored the local system keyspaces data.
+     *
+     * <p>If the {@code system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guaranty that the server can tolerate the lost of n - 1 disks.</p>
+     *
+     * @return the locations where should be stored the local system keyspaces data
+     */
+    public static String[] getSystemKeyspacesDataFileLocations()
+    {
+        if (conf.system_data_file_directory != null)
+            return new String[] {conf.system_data_file_directory};
+
+        return conf.data_file_directories.length == 0  ? conf.data_file_directories
+                                                       : new String[] {conf.data_file_directories[0]};
+    }
+
+    /**
+     * Returns the locations where should be stored the non local system keyspaces data.
+     *
+     * @return the locations where should be stored the non local system keyspaces data
+     */
+    public static String[] getNonSystemKeyspacesDataFileLocations()

Review comment:
        I understand your logic. It just seems to me that the `system` part is important because it implies that your server relies on it. I would be more in favor of `getLocalSystemKeyspacesDataFileLocations`.
   Let's wait for the opinion of the other reviewer then I will change it to what we agree upon at that point.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Dec/20 15:14;githubbot;600","blerer commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r545929464



##########
File path: src/java/org/apache/cassandra/service/CassandraDaemon.java
##########
@@ -456,6 +476,78 @@ protected void setup()
         completeSetup();
     }
 
+    /**
+     * Checks if the data of the local system keyspaces need to be migrated to a different location.
+     *
+     * @throws IOException
+     */
+    private void migrateSystemDataIfNeeded() throws IOException
+    {
+        String importSystemDataFrom = System.getProperty(""cassandra.importSystemDataFilesFrom"");

Review comment:
       I added the property to the documentation. I am not sure if we should add it to the JVM options file as the property is a bit of a corner case. It is only usefull if you have used a dedicated directory for your system table and want to go back to using your data directories. 




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Dec/20 16:01;githubbot;600","blerer commented on pull request #675:
URL: https://github.com/apache/cassandra/pull/675#issuecomment-748185013


   Regarding the testing of the migration part. The migration part is automatically tested in all the upgrade tests that create a keyspace or/and table before upgrading and used it after.
   One part that we do not test is the import in the same version. I will look into how to test that.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Dec/20 16:20;githubbot;600","krummas commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r553354189



##########
File path: doc/source/configuration/cass_env_sh_file.rst
##########
@@ -28,6 +28,10 @@ Setting this property to true causes the dynamic snitch to ignore the severity i
 
 **Default:** false
 
+``cassandra.import_system_data_files_from``
+-------------------------------------------
+The directory location of the ``local system keyspaces`` data to import. This property can be used to move back to the first data directory the ``local system keyspaces`` data that were stored in a specific directory using the ``system_data_file_directory`` cassandra.yaml property.

Review comment:
       Could you expand on this? Not sure I understand what the use case would be for this? Is it if a user changes the configured `system_data_file_directory`? If so I think we could expect them to manually move the files as well

##########
File path: src/java/org/apache/cassandra/db/Directories.java
##########
@@ -632,6 +642,106 @@ public String toString()
         }
     }
 
+    /**
+     * Data directories used to store keyspace data.
+     */
+    public static final class DataDirectories implements Iterable<DataDirectory>
+    {
+        /**
+         * The directories for storing the local system keyspaces.
+         */
+        private final DataDirectory[] systemKeyspaceDataDirectories;
+
+        /**
+         * The directories where the data of the non local system keyspaces should be stored.
+         */
+        private final DataDirectory[] nonSystemKeyspacesDirectories;
+
+
+        public DataDirectories(String[] locationsForNonSystemKeyspaces, String[] locationsForSystemKeyspace)
+        {
+            nonSystemKeyspacesDirectories = toDataDirectories(locationsForNonSystemKeyspaces);
+            systemKeyspaceDataDirectories = toDataDirectories(locationsForSystemKeyspace);
+        }
+
+        private static DataDirectory[] toDataDirectories(String... locations)
+        {
+            DataDirectory[] directories = new DataDirectory[locations.length];
+            for (int i = 0; i < locations.length; ++i)
+                directories[i] = new DataDirectory(new File(locations[i]));
+            return directories;
+        }
+
+        /**
+         * Returns the data directories used to store the data of the specified keyspace.
+         *
+         * @param keyspace the keyspace name
+         * @return the data directories used to store the data of the specified keyspace
+         */
+        public DataDirectory[] getDataDirectoriesUsedBy(String keyspace)

Review comment:
       this is confusing - we return all directories when its a system keyspace? (guess its because of the paxos exception)
   
   looking at the uses of this method it seems to all be cleanup/check related so maybe it would make sense to remove it and just use `getAllDirectories()` ?

##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1787,11 +1814,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which support redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where should be stored the local system keyspaces data.
+     *
+     * <p>If the {@code system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guaranty that the server can tolerate the lost of n - 1 disks.</p>
+     *
+     * @return the locations where should be stored the local system keyspaces data
+     */
+    public static String[] getSystemKeyspacesDataFileLocations()
+    {
+        if (conf.system_data_file_directory != null)
+            return new String[] {conf.system_data_file_directory};
+
+        return conf.data_file_directories.length == 0  ? conf.data_file_directories
+                                                       : new String[] {conf.data_file_directories[0]};
+    }
+
+    /**
+     * Returns the locations where should be stored the non local system keyspaces data.
+     *
+     * @return the locations where should be stored the non local system keyspaces data
+     */
+    public static String[] getNonSystemKeyspacesDataFileLocations()

Review comment:
       Yeah keeping `system` in the name is probably a good idea

##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1830,11 +1857,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which supports redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where the local system keyspaces data should be stored.
+     *
+     * <p>If the {@code system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guarantees that the server can tolerate the lost of n - 1 disks.</p>
+     *
+     * @return the locations where should be stored the local system keyspaces data
+     */
+    public static String[] getSystemKeyspacesDataFileLocations()
+    {
+        if (conf.system_data_file_directory != null)
+            return new String[] {conf.system_data_file_directory};
+
+        return conf.data_file_directories.length == 0  ? conf.data_file_directories
+                                                       : new String[] {conf.data_file_directories[0]};

Review comment:
       Feels slightly wrong to rely on the order of the directories in the configuration - maybe better to sort and grab the first one. My concern is that if a user auto-generates the config which changes the order of the directories in the config file and we have to move the files every startup (not the end of the world, but anyway)

##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1830,11 +1857,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which supports redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where the local system keyspaces data should be stored.
+     *
+     * <p>If the {@code system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guarantees that the server can tolerate the lost of n - 1 disks.</p>
+     *
+     * @return the locations where should be stored the local system keyspaces data
+     */
+    public static String[] getSystemKeyspacesDataFileLocations()
+    {
+        if (conf.system_data_file_directory != null)

Review comment:
       nit: maybe `if (useSpecificLocationForSystemData())` instead of null check

##########
File path: src/java/org/apache/cassandra/io/util/FileUtils.java
##########
@@ -969,4 +973,70 @@ public Object getAttribute(String attribute) throws IOException
             return fileStore.getAttribute(attribute);
         }
     }
+
+    /**
+     * Moves the contents of a directory to another directory.
+     * <p>Once a file has been copied to the target directory it will be deleted from the source directory.
+     * If a file already exists in the target directory a warning will be logged and the file will not
+     * be deleted.</p>
+     *
+     * @param source the directory containing the files to move
+     * @param target the directory where the files must be moved
+     */
+    public static void moveRecursively(Path source, Path target) throws IOException
+    {
+        logger.info(""Moving {} to {}"" , source, target);
+
+        if (Files.isDirectory(source))
+        {
+            Files.createDirectories(target);
+
+            try (Stream<Path> paths = Files.list(source))

Review comment:
       I found this part quite hard to follow (the resolving/relativizing) 
   
   Would be a bit simpler if we could do something like this?:
   ```
   for (File f : source.toFile().listFiles())
   {
       String fileName = f.getName();
       moveRecursively(source.resolve(fileName), target.resolve(fileName));
   }
   ```
   

##########
File path: src/java/org/apache/cassandra/service/DefaultFSErrorHandler.java
##########
@@ -67,6 +65,18 @@ public void handleFSError(FSError e)
                 StorageService.instance.stopTransports();
                 break;
             case best_effort:
+
+                // There are a few scenarios where we know that the node will not be able to operate properly.
+                // For those scenarios we want to stop the transports and let the administrators handle the problem.
+                // Those scenarios are:
+                // * All the disks are full
+                // * All the disks for a given keyspace have been marked as unwriteable

Review comment:
       can't we continue serving reads here?

##########
File path: src/java/org/apache/cassandra/service/CassandraDaemon.java
##########
@@ -472,6 +492,78 @@ public void runStartupChecks()
         }
 
     }
+
+    /**
+     * Checks if the data of the local system keyspaces need to be migrated to a different location.
+     *
+     * @throws IOException
+     */
+    public void migrateSystemDataIfNeeded() throws IOException
+    {
+        String importSystemDataFrom = System.getProperty(""cassandra.import_system_data_files_from"");
+
+        // If there is only one directory and no system keyspace directory has been specified we do not need to do
+        // anything. If it is not the case we want to try to migrate the data.
+        if (!DatabaseDescriptor.useSpecificLocationForSystemData()
+                && DatabaseDescriptor.getNonSystemKeyspacesDataFileLocations().length <= 1
+                && importSystemDataFrom == null)
+            return;
+
+        // We can face several cases:
+        //  1) The system data are spread accross the data file locations and need to be moved to
+        //     the first data location (upgrade to 4.0)
+        //  2) The system data are spread accross the data file locations and need to be moved to
+        //     the system keyspace location configured by the user (upgrade to 4.0)
+        //  3) The system data are stored in the first data location and need to be moved to
+        //     the system keyspace location configured by the user (system_data_file_directory has been configured)
+        //  4) The system data have been stored in the system keyspace location configured by the user
+        //     and need to be moved to the first data location (the import of the data has been requested)
+        Path target = Paths.get(DatabaseDescriptor.getSystemKeyspacesDataFileLocations()[0]);
+
+        String[] nonSystemKeyspacesFileLocations = DatabaseDescriptor.getNonSystemKeyspacesDataFileLocations();
+        String[] sources = importSystemDataFrom != null
+            ? new String[] {importSystemDataFrom}
+            : DatabaseDescriptor.useSpecificLocationForSystemData() ? nonSystemKeyspacesFileLocations
+                                                                    : Arrays.copyOfRange(nonSystemKeyspacesFileLocations, 1, nonSystemKeyspacesFileLocations.length);
+
+        for (String source : sources)
+        {
+            Path dataFileLocation = Paths.get(source);
+
+            if (!Files.exists(dataFileLocation))
+                continue;
+
+            try (Stream<Path> locationChildren = Files.list(dataFileLocation))
+            {
+                Path[] keyspaceDirectories = locationChildren.filter(p -> SchemaConstants.isLocalSystemKeyspace(p.getFileName().toString()))
+                        .toArray(Path[]::new);
+
+                for (Path keyspaceDirectory : keyspaceDirectories)
+                {
+                    try (Stream<Path> keyspaceChildren = Files.list(keyspaceDirectory))
+                    {
+                        Path[] tableDirectories = keyspaceChildren.filter(Files::isDirectory)

Review comment:
       nit: indentation is a bit broken here - `.filter` should align with the first `.filter`

##########
File path: src/java/org/apache/cassandra/service/CassandraDaemon.java
##########
@@ -472,6 +492,78 @@ public void runStartupChecks()
         }
 
     }
+
+    /**
+     * Checks if the data of the local system keyspaces need to be migrated to a different location.
+     *
+     * @throws IOException
+     */
+    public void migrateSystemDataIfNeeded() throws IOException
+    {
+        String importSystemDataFrom = System.getProperty(""cassandra.import_system_data_files_from"");

Review comment:
       As mentioned above, I think we can remove this property which would simplify this method a bit




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Jan/21 12:41;githubbot;600","krummas commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r553354189



##########
File path: doc/source/configuration/cass_env_sh_file.rst
##########
@@ -28,6 +28,10 @@ Setting this property to true causes the dynamic snitch to ignore the severity i
 
 **Default:** false
 
+``cassandra.import_system_data_files_from``
+-------------------------------------------
+The directory location of the ``local system keyspaces`` data to import. This property can be used to move back to the first data directory the ``local system keyspaces`` data that were stored in a specific directory using the ``system_data_file_directory`` cassandra.yaml property.

Review comment:
       Could you expand on this? Not sure I understand what the use case would be for this? Is it if a user changes the configured `system_data_file_directory`? If so I think we could expect them to manually move the files as well

##########
File path: src/java/org/apache/cassandra/db/Directories.java
##########
@@ -632,6 +642,106 @@ public String toString()
         }
     }
 
+    /**
+     * Data directories used to store keyspace data.
+     */
+    public static final class DataDirectories implements Iterable<DataDirectory>
+    {
+        /**
+         * The directories for storing the local system keyspaces.
+         */
+        private final DataDirectory[] systemKeyspaceDataDirectories;
+
+        /**
+         * The directories where the data of the non local system keyspaces should be stored.
+         */
+        private final DataDirectory[] nonSystemKeyspacesDirectories;
+
+
+        public DataDirectories(String[] locationsForNonSystemKeyspaces, String[] locationsForSystemKeyspace)
+        {
+            nonSystemKeyspacesDirectories = toDataDirectories(locationsForNonSystemKeyspaces);
+            systemKeyspaceDataDirectories = toDataDirectories(locationsForSystemKeyspace);
+        }
+
+        private static DataDirectory[] toDataDirectories(String... locations)
+        {
+            DataDirectory[] directories = new DataDirectory[locations.length];
+            for (int i = 0; i < locations.length; ++i)
+                directories[i] = new DataDirectory(new File(locations[i]));
+            return directories;
+        }
+
+        /**
+         * Returns the data directories used to store the data of the specified keyspace.
+         *
+         * @param keyspace the keyspace name
+         * @return the data directories used to store the data of the specified keyspace
+         */
+        public DataDirectory[] getDataDirectoriesUsedBy(String keyspace)

Review comment:
       this is confusing - we return all directories when its a system keyspace? (guess its because of the paxos exception)
   
   looking at the uses of this method it seems to all be cleanup/check related so maybe it would make sense to remove it and just use `getAllDirectories()` ?

##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1787,11 +1814,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which support redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where should be stored the local system keyspaces data.
+     *
+     * <p>If the {@code system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guaranty that the server can tolerate the lost of n - 1 disks.</p>
+     *
+     * @return the locations where should be stored the local system keyspaces data
+     */
+    public static String[] getSystemKeyspacesDataFileLocations()
+    {
+        if (conf.system_data_file_directory != null)
+            return new String[] {conf.system_data_file_directory};
+
+        return conf.data_file_directories.length == 0  ? conf.data_file_directories
+                                                       : new String[] {conf.data_file_directories[0]};
+    }
+
+    /**
+     * Returns the locations where should be stored the non local system keyspaces data.
+     *
+     * @return the locations where should be stored the non local system keyspaces data
+     */
+    public static String[] getNonSystemKeyspacesDataFileLocations()

Review comment:
       Yeah keeping `system` in the name is probably a good idea

##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1830,11 +1857,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which supports redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where the local system keyspaces data should be stored.
+     *
+     * <p>If the {@code system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guarantees that the server can tolerate the lost of n - 1 disks.</p>
+     *
+     * @return the locations where should be stored the local system keyspaces data
+     */
+    public static String[] getSystemKeyspacesDataFileLocations()
+    {
+        if (conf.system_data_file_directory != null)
+            return new String[] {conf.system_data_file_directory};
+
+        return conf.data_file_directories.length == 0  ? conf.data_file_directories
+                                                       : new String[] {conf.data_file_directories[0]};

Review comment:
       Feels slightly wrong to rely on the order of the directories in the configuration - maybe better to sort and grab the first one. My concern is that if a user auto-generates the config which changes the order of the directories in the config file and we have to move the files every startup (not the end of the world, but anyway)

##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1830,11 +1857,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which supports redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where the local system keyspaces data should be stored.
+     *
+     * <p>If the {@code system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guarantees that the server can tolerate the lost of n - 1 disks.</p>
+     *
+     * @return the locations where should be stored the local system keyspaces data
+     */
+    public static String[] getSystemKeyspacesDataFileLocations()
+    {
+        if (conf.system_data_file_directory != null)

Review comment:
       nit: maybe `if (useSpecificLocationForSystemData())` instead of null check

##########
File path: src/java/org/apache/cassandra/io/util/FileUtils.java
##########
@@ -969,4 +973,70 @@ public Object getAttribute(String attribute) throws IOException
             return fileStore.getAttribute(attribute);
         }
     }
+
+    /**
+     * Moves the contents of a directory to another directory.
+     * <p>Once a file has been copied to the target directory it will be deleted from the source directory.
+     * If a file already exists in the target directory a warning will be logged and the file will not
+     * be deleted.</p>
+     *
+     * @param source the directory containing the files to move
+     * @param target the directory where the files must be moved
+     */
+    public static void moveRecursively(Path source, Path target) throws IOException
+    {
+        logger.info(""Moving {} to {}"" , source, target);
+
+        if (Files.isDirectory(source))
+        {
+            Files.createDirectories(target);
+
+            try (Stream<Path> paths = Files.list(source))

Review comment:
       I found this part quite hard to follow (the resolving/relativizing) 
   
   Would be a bit simpler if we could do something like this?:
   ```
   for (File f : source.toFile().listFiles())
   {
       String fileName = f.getName();
       moveRecursively(source.resolve(fileName), target.resolve(fileName));
   }
   ```
   

##########
File path: src/java/org/apache/cassandra/service/DefaultFSErrorHandler.java
##########
@@ -67,6 +65,18 @@ public void handleFSError(FSError e)
                 StorageService.instance.stopTransports();
                 break;
             case best_effort:
+
+                // There are a few scenarios where we know that the node will not be able to operate properly.
+                // For those scenarios we want to stop the transports and let the administrators handle the problem.
+                // Those scenarios are:
+                // * All the disks are full
+                // * All the disks for a given keyspace have been marked as unwriteable

Review comment:
       can't we continue serving reads here?

##########
File path: src/java/org/apache/cassandra/service/CassandraDaemon.java
##########
@@ -472,6 +492,78 @@ public void runStartupChecks()
         }
 
     }
+
+    /**
+     * Checks if the data of the local system keyspaces need to be migrated to a different location.
+     *
+     * @throws IOException
+     */
+    public void migrateSystemDataIfNeeded() throws IOException
+    {
+        String importSystemDataFrom = System.getProperty(""cassandra.import_system_data_files_from"");
+
+        // If there is only one directory and no system keyspace directory has been specified we do not need to do
+        // anything. If it is not the case we want to try to migrate the data.
+        if (!DatabaseDescriptor.useSpecificLocationForSystemData()
+                && DatabaseDescriptor.getNonSystemKeyspacesDataFileLocations().length <= 1
+                && importSystemDataFrom == null)
+            return;
+
+        // We can face several cases:
+        //  1) The system data are spread accross the data file locations and need to be moved to
+        //     the first data location (upgrade to 4.0)
+        //  2) The system data are spread accross the data file locations and need to be moved to
+        //     the system keyspace location configured by the user (upgrade to 4.0)
+        //  3) The system data are stored in the first data location and need to be moved to
+        //     the system keyspace location configured by the user (system_data_file_directory has been configured)
+        //  4) The system data have been stored in the system keyspace location configured by the user
+        //     and need to be moved to the first data location (the import of the data has been requested)
+        Path target = Paths.get(DatabaseDescriptor.getSystemKeyspacesDataFileLocations()[0]);
+
+        String[] nonSystemKeyspacesFileLocations = DatabaseDescriptor.getNonSystemKeyspacesDataFileLocations();
+        String[] sources = importSystemDataFrom != null
+            ? new String[] {importSystemDataFrom}
+            : DatabaseDescriptor.useSpecificLocationForSystemData() ? nonSystemKeyspacesFileLocations
+                                                                    : Arrays.copyOfRange(nonSystemKeyspacesFileLocations, 1, nonSystemKeyspacesFileLocations.length);
+
+        for (String source : sources)
+        {
+            Path dataFileLocation = Paths.get(source);
+
+            if (!Files.exists(dataFileLocation))
+                continue;
+
+            try (Stream<Path> locationChildren = Files.list(dataFileLocation))
+            {
+                Path[] keyspaceDirectories = locationChildren.filter(p -> SchemaConstants.isLocalSystemKeyspace(p.getFileName().toString()))
+                        .toArray(Path[]::new);
+
+                for (Path keyspaceDirectory : keyspaceDirectories)
+                {
+                    try (Stream<Path> keyspaceChildren = Files.list(keyspaceDirectory))
+                    {
+                        Path[] tableDirectories = keyspaceChildren.filter(Files::isDirectory)

Review comment:
       nit: indentation is a bit broken here - `.filter` should align with the first `.filter`

##########
File path: src/java/org/apache/cassandra/service/CassandraDaemon.java
##########
@@ -472,6 +492,78 @@ public void runStartupChecks()
         }
 
     }
+
+    /**
+     * Checks if the data of the local system keyspaces need to be migrated to a different location.
+     *
+     * @throws IOException
+     */
+    public void migrateSystemDataIfNeeded() throws IOException
+    {
+        String importSystemDataFrom = System.getProperty(""cassandra.import_system_data_files_from"");

Review comment:
       As mentioned above, I think we can remove this property which would simplify this method a bit




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jan/21 03:55;githubbot;600","blerer commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r555774406



##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1830,11 +1857,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which supports redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where the local system keyspaces data should be stored.
+     *
+     * <p>If the {@code system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guarantees that the server can tolerate the lost of n - 1 disks.</p>
+     *
+     * @return the locations where should be stored the local system keyspaces data
+     */
+    public static String[] getSystemKeyspacesDataFileLocations()
+    {
+        if (conf.system_data_file_directory != null)
+            return new String[] {conf.system_data_file_directory};
+
+        return conf.data_file_directories.length == 0  ? conf.data_file_directories
+                                                       : new String[] {conf.data_file_directories[0]};

Review comment:
       I do not know how people name their directories so I imagine that if we rely on alphabetical order the place were the system files will end up will be a bit of a random one from the user perspective. It sounded easier to me to control the directory ordering than the directory names as it could be handled at the config level.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jan/21 13:37;githubbot;600","blerer closed pull request #108:
URL: https://github.com/apache/cassandra-dtest/pull/108


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Jan/21 16:20;githubbot;600","blerer opened a new pull request #114:
URL: https://github.com/apache/cassandra-dtest/pull/114


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Jan/21 15:20;githubbot;600","blerer opened a new pull request #114:
URL: https://github.com/apache/cassandra-dtest/pull/114


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jan/21 06:40;githubbot;600","blerer commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r562638634



##########
File path: src/java/org/apache/cassandra/service/DefaultFSErrorHandler.java
##########
@@ -67,6 +65,18 @@ public void handleFSError(FSError e)
                 StorageService.instance.stopTransports();
                 break;
             case best_effort:
+
+                // There are a few scenarios where we know that the node will not be able to operate properly.
+                // For those scenarios we want to stop the transports and let the administrators handle the problem.
+                // Those scenarios are:
+                // * All the disks are full
+                // * All the disks for a given keyspace have been marked as unwriteable

Review comment:
       The problem I see if we keep serving reads but cannot serve writes is that the dataset will slowly become outdated. I am also not sure of how to do that.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jan/21 13:43;githubbot;600","adelapena commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r563725819



##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1830,11 +1857,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which supports redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where the local system keyspaces data should be stored.
+     *
+     * <p>If the {@code system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guarantees that the server can tolerate the lost of n - 1 disks.</p>
+     *
+     * @return the locations where should be stored the local system keyspaces data
+     */
+    public static String[] getSystemKeyspacesDataFileLocations()
+    {
+        if (conf.system_data_file_directory != null)
+            return new String[] {conf.system_data_file_directory};
+
+        return conf.data_file_directories.length == 0  ? conf.data_file_directories
+                                                       : new String[] {conf.data_file_directories[0]};

Review comment:
       To me using the first one as they are specified in `data_file_directories` sounds easier to understand than an alphabetical selection. The phrase `Cassandra will store the data of the local system keyspaces the first of the data directories specified by data_file_directories` in the documentation seems pretty clear about how the selection is done. We could also add a note in the documentation for `data_file_directories` mentioning that the data of the local system keyspaces will be stored in the first directory, and perhaps warning about the consequences of changing the order of the specified data directories, if we are concerned about accidental order changes.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Jan/21 13:34;githubbot;600","adelapena commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r563801852



##########
File path: src/java/org/apache/cassandra/db/Directories.java
##########
@@ -592,10 +581,34 @@ public static File getBackupsDirectory(File location)
         }
     }
 
+    /**
+     * Checks if the specified table should be stored with local system data.
+     *
+     * <p> To minimize the risk of failures, SSTables for local system keyspaces must be stored in a single data
+     * directory. The only exception to this is the system paxos table as it can be a high traffic table.</p>

Review comment:
       I think that after the last changes paxos is not the only exception

##########
File path: src/java/org/apache/cassandra/db/Directories.java
##########
@@ -632,6 +645,89 @@ public String toString()
         }
     }
 
+    /**
+     * Data directories used to store keyspace data.
+     */
+    public static final class DataDirectories implements Iterable<DataDirectory>
+    {
+        /**
+         * The directories for storing the local system keyspaces.
+         */
+        private final DataDirectory[] localSystemKeyspaceDataDirectories;
+
+        /**
+         * The directories where the data of the non local system keyspaces should be stored.
+         */
+        private final DataDirectory[] nonLocalSystemKeyspacesDirectories;
+
+
+        public DataDirectories(String[] locationsForNonSystemKeyspaces, String[] locationsForSystemKeyspace)
+        {
+            nonLocalSystemKeyspacesDirectories = toDataDirectories(locationsForNonSystemKeyspaces);
+            localSystemKeyspaceDataDirectories = toDataDirectories(locationsForSystemKeyspace);
+        }
+
+        private static DataDirectory[] toDataDirectories(String... locations)
+        {
+            DataDirectory[] directories = new DataDirectory[locations.length];
+            for (int i = 0; i < locations.length; ++i)
+                directories[i] = new DataDirectory(new File(locations[i]));
+            return directories;
+        }
+
+        /**
+         * Returns the data directories for the specified table.
+         *
+         * @param table the table metadata
+         * @return the data directories for the specified table
+         */
+        public DataDirectory[] getDataDirectoriesFor(TableMetadata table)
+        {
+            return isStoredInLocalSystemKeyspacesDataLocation(table.keyspace, table.name) ? localSystemKeyspaceDataDirectories
+                                                                                          : nonLocalSystemKeyspacesDirectories;
+        }
+
+        @Override
+        public Iterator<DataDirectory> iterator()
+        {
+            return getAllDirectories().iterator();
+        }
+
+        public Set<DataDirectory> getAllDirectories()
+        {
+            Set<DataDirectory> directories = new LinkedHashSet<>(nonLocalSystemKeyspacesDirectories.length + localSystemKeyspaceDataDirectories.length);
+            Collections.addAll(directories, nonLocalSystemKeyspacesDirectories);
+            Collections.addAll(directories, localSystemKeyspaceDataDirectories);
+            return directories;
+        }
+
+        @Override
+        public boolean equals(Object o)
+        {
+            if (this == o) return true;
+            if (o == null || getClass() != o.getClass()) return false;
+
+            DataDirectories that = (DataDirectories) o;
+
+            return Arrays.equals(this.localSystemKeyspaceDataDirectories, that.localSystemKeyspaceDataDirectories)
+                && Arrays.equals(this.nonLocalSystemKeyspacesDirectories, that.nonLocalSystemKeyspacesDirectories);
+        }
+
+        @Override
+        public int hashCode()
+        {
+            return Objects.hash(localSystemKeyspaceDataDirectories, nonLocalSystemKeyspacesDirectories);
+        }
+
+        public String toString()
+        {
+            return ""DataDirectories {"" +
+                   ""systemKeyspaceDataDirectories="" + localSystemKeyspaceDataDirectories +
+                   "", nonSystemKeyspacesDirectories="" + nonLocalSystemKeyspacesDirectories +

Review comment:
       Nit: we could use something like `Arrays.toString` to better print the directories. Also, the method could use an `@Override` annotation.

##########
File path: src/java/org/apache/cassandra/service/CassandraDaemon.java
##########
@@ -472,6 +492,73 @@ public void runStartupChecks()
         }
 
     }
+
+    /**
+     * Checks if the data of the local system keyspaces need to be migrated to a different location.
+     *
+     * @throws IOException
+     */
+    public void migrateSystemDataIfNeeded() throws IOException
+    {
+        // If there is only one directory and no system keyspace directory has been specified we do not need to do
+        // anything. If it is not the case we want to try to migrate the data.
+        if (!DatabaseDescriptor.useSpecificLocationForLocalSystemData()
+                && DatabaseDescriptor.getNonLocalSystemKeyspacesDataFileLocations().length <= 1)
+            return;
+
+        // We can face several cases:
+        //  1) The system data are spread accross the data file locations and need to be moved to
+        //     the first data location (upgrade to 4.0)
+        //  2) The system data are spread accross the data file locations and need to be moved to
+        //     the system keyspace location configured by the user (upgrade to 4.0)
+        //  3) The system data are stored in the first data location and need to be moved to
+        //     the system keyspace location configured by the user (system_data_file_directory has been configured)

Review comment:
       I understand that we can't have the opposite case, that the system data was originally stored in a separate directory but the user wants to move it to the first data location. In such case `system_data_file_directory` wouldn't be set so we can't know where the local system data is. That configuration change would require the user to manually move the data to the first data directory. Is this right? 

##########
File path: src/java/org/apache/cassandra/db/ColumnFamilyStore.java
##########
@@ -2806,4 +2796,88 @@ public boolean getNeverPurgeTombstones()
     {
         return neverPurgeTombstones;
     }
-}
\ No newline at end of file
+
+    /**
+     * The thread pools used to flush memtables.
+     *
+     * <p>Each disk has its own set of thread pools to perform memtable flushes.</p>
+     * <p>Based on the configuration. Local system keyspaces can have their own disk
+     * to allow for special redundancy mechanism. If it is the case the executor services returned for
+     * local system keyspaces will be different from the ones for the other keyspaces.</p>
+     */
+    private static final class PerDiskFlushExecutors
+    {
+        /**
+         * The flush executors for non local system keyspaces.
+         */
+        private final ExecutorService[] nonLocalSystemflushExecutors;
+
+        /**
+         * The flush executors for the local system keyspaces.
+         */
+        private final ExecutorService[] localSystemDiskFlushExecutors;
+
+        /**
+         * {@code true} if local system keyspaces are stored in their own directory and use an extra flush executor,
+         * {@code false} otherwise.
+         */
+        private boolean useSpecificExecutorForSystemKeyspaces;
+
+        public PerDiskFlushExecutors(int flushWriters,
+                                     String[] locationsForNonSystemKeyspaces,
+                                     boolean useSpecificLocationForSystemKeyspaces)
+        {
+            ExecutorService[] flushExecutors = createPerDiskFlushWriters(locationsForNonSystemKeyspaces.length, flushWriters);
+            nonLocalSystemflushExecutors = flushExecutors;
+            useSpecificExecutorForSystemKeyspaces = useSpecificLocationForSystemKeyspaces;
+            localSystemDiskFlushExecutors = useSpecificLocationForSystemKeyspaces ? new ExecutorService[] {newThreadPool(""LocalSystemKeyspacesDiskMemtableFlushWriter"", flushWriters)}
+                                                                             : new ExecutorService[] {flushExecutors[0]};
+        }
+
+        private ExecutorService[] createPerDiskFlushWriters(int numberOfExecutors, int flushWriters)
+        {
+            ExecutorService[] flushExecutors = new ExecutorService[numberOfExecutors];
+
+            for (int i = 0; i < numberOfExecutors; i++)
+            {
+                flushExecutors[i] = newThreadPool(""PerDiskMemtableFlushWriter_"" + i, flushWriters);
+            }
+            return flushExecutors;
+        }
+
+        private static JMXEnabledThreadPoolExecutor newThreadPool(String poolName, int size)
+        {
+            return new JMXEnabledThreadPoolExecutor(size,
+                                                    Stage.KEEP_ALIVE_SECONDS,
+                                                    TimeUnit.SECONDS,
+                                                    new LinkedBlockingQueue<>(),
+                                                    new NamedThreadFactory(poolName),
+                                                    ""internal"");
+        }
+
+        /**
+         * Returns the flush executors for the specified keyspace.
+         *
+         * @param keyspaceName the keyspace name
+         * @param tableName the table name
+         * @return the flush executors that should be used for flushing the memtables of the specified keyspace.
+         */
+        public ExecutorService[] getExecutorsFor(String keyspaceName, String tableName)
+        {
+            return Directories.isStoredInLocalSystemKeyspacesDataLocation(keyspaceName, tableName) ? localSystemDiskFlushExecutors
+                                                                                              : nonLocalSystemflushExecutors;

Review comment:
       Nit: misaligned
   ```suggestion
                                                                                                      : nonLocalSystemflushExecutors;
   ```

##########
File path: src/java/org/apache/cassandra/db/ColumnFamilyStore.java
##########
@@ -2806,4 +2796,88 @@ public boolean getNeverPurgeTombstones()
     {
         return neverPurgeTombstones;
     }
-}
\ No newline at end of file
+
+    /**
+     * The thread pools used to flush memtables.
+     *
+     * <p>Each disk has its own set of thread pools to perform memtable flushes.</p>
+     * <p>Based on the configuration. Local system keyspaces can have their own disk
+     * to allow for special redundancy mechanism. If it is the case the executor services returned for
+     * local system keyspaces will be different from the ones for the other keyspaces.</p>
+     */
+    private static final class PerDiskFlushExecutors
+    {
+        /**
+         * The flush executors for non local system keyspaces.
+         */
+        private final ExecutorService[] nonLocalSystemflushExecutors;
+
+        /**
+         * The flush executors for the local system keyspaces.
+         */
+        private final ExecutorService[] localSystemDiskFlushExecutors;
+
+        /**
+         * {@code true} if local system keyspaces are stored in their own directory and use an extra flush executor,
+         * {@code false} otherwise.
+         */
+        private boolean useSpecificExecutorForSystemKeyspaces;

Review comment:
       Nit: might be final

##########
File path: src/java/org/apache/cassandra/db/ColumnFamilyStore.java
##########
@@ -2806,4 +2796,88 @@ public boolean getNeverPurgeTombstones()
     {
         return neverPurgeTombstones;
     }
-}
\ No newline at end of file
+
+    /**
+     * The thread pools used to flush memtables.
+     *
+     * <p>Each disk has its own set of thread pools to perform memtable flushes.</p>
+     * <p>Based on the configuration. Local system keyspaces can have their own disk
+     * to allow for special redundancy mechanism. If it is the case the executor services returned for
+     * local system keyspaces will be different from the ones for the other keyspaces.</p>
+     */
+    private static final class PerDiskFlushExecutors
+    {
+        /**
+         * The flush executors for non local system keyspaces.
+         */
+        private final ExecutorService[] nonLocalSystemflushExecutors;
+
+        /**
+         * The flush executors for the local system keyspaces.
+         */
+        private final ExecutorService[] localSystemDiskFlushExecutors;
+
+        /**
+         * {@code true} if local system keyspaces are stored in their own directory and use an extra flush executor,
+         * {@code false} otherwise.
+         */
+        private boolean useSpecificExecutorForSystemKeyspaces;
+
+        public PerDiskFlushExecutors(int flushWriters,
+                                     String[] locationsForNonSystemKeyspaces,
+                                     boolean useSpecificLocationForSystemKeyspaces)
+        {
+            ExecutorService[] flushExecutors = createPerDiskFlushWriters(locationsForNonSystemKeyspaces.length, flushWriters);
+            nonLocalSystemflushExecutors = flushExecutors;
+            useSpecificExecutorForSystemKeyspaces = useSpecificLocationForSystemKeyspaces;
+            localSystemDiskFlushExecutors = useSpecificLocationForSystemKeyspaces ? new ExecutorService[] {newThreadPool(""LocalSystemKeyspacesDiskMemtableFlushWriter"", flushWriters)}
+                                                                             : new ExecutorService[] {flushExecutors[0]};

Review comment:
       Nit: misaligned
   ```suggestion
                                                                                     : new ExecutorService[] {flushExecutors[0]};
   ```

##########
File path: src/java/org/apache/cassandra/db/ColumnFamilyStore.java
##########
@@ -2806,4 +2796,88 @@ public boolean getNeverPurgeTombstones()
     {
         return neverPurgeTombstones;
     }
-}
\ No newline at end of file
+
+    /**
+     * The thread pools used to flush memtables.
+     *
+     * <p>Each disk has its own set of thread pools to perform memtable flushes.</p>
+     * <p>Based on the configuration. Local system keyspaces can have their own disk
+     * to allow for special redundancy mechanism. If it is the case the executor services returned for
+     * local system keyspaces will be different from the ones for the other keyspaces.</p>
+     */
+    private static final class PerDiskFlushExecutors
+    {
+        /**
+         * The flush executors for non local system keyspaces.
+         */
+        private final ExecutorService[] nonLocalSystemflushExecutors;
+
+        /**
+         * The flush executors for the local system keyspaces.
+         */
+        private final ExecutorService[] localSystemDiskFlushExecutors;
+
+        /**
+         * {@code true} if local system keyspaces are stored in their own directory and use an extra flush executor,
+         * {@code false} otherwise.
+         */
+        private boolean useSpecificExecutorForSystemKeyspaces;
+
+        public PerDiskFlushExecutors(int flushWriters,
+                                     String[] locationsForNonSystemKeyspaces,
+                                     boolean useSpecificLocationForSystemKeyspaces)
+        {
+            ExecutorService[] flushExecutors = createPerDiskFlushWriters(locationsForNonSystemKeyspaces.length, flushWriters);
+            nonLocalSystemflushExecutors = flushExecutors;
+            useSpecificExecutorForSystemKeyspaces = useSpecificLocationForSystemKeyspaces;
+            localSystemDiskFlushExecutors = useSpecificLocationForSystemKeyspaces ? new ExecutorService[] {newThreadPool(""LocalSystemKeyspacesDiskMemtableFlushWriter"", flushWriters)}
+                                                                             : new ExecutorService[] {flushExecutors[0]};
+        }
+
+        private ExecutorService[] createPerDiskFlushWriters(int numberOfExecutors, int flushWriters)

Review comment:
       Might be static:
   ```suggestion
           private static ExecutorService[] createPerDiskFlushWriters(int numberOfExecutors, int flushWriters)
   ```

##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1830,11 +1857,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which supports redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForLocalSystemData()
+    {
+        return conf.local_system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where the local system keyspaces data should be stored.
+     *
+     * <p>If the {@code local_system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guarantees that the server can tolerate the lost of n - 1 disks.</p>

Review comment:
       Rather than saying than `the server can tolerate the lost of n - 1 disks`, I think I'd say `the server can tolerate the lost of all the disks but the first one`, to avoid the risk of readers getting the wrong idea that they can lost *any* n - 1 disks.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Jan/21 18:17;githubbot;600","adelapena commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r563725819



##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1830,11 +1857,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which supports redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForSystemData()
+    {
+        return conf.system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where the local system keyspaces data should be stored.
+     *
+     * <p>If the {@code system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guarantees that the server can tolerate the lost of n - 1 disks.</p>
+     *
+     * @return the locations where should be stored the local system keyspaces data
+     */
+    public static String[] getSystemKeyspacesDataFileLocations()
+    {
+        if (conf.system_data_file_directory != null)
+            return new String[] {conf.system_data_file_directory};
+
+        return conf.data_file_directories.length == 0  ? conf.data_file_directories
+                                                       : new String[] {conf.data_file_directories[0]};

Review comment:
       To me using the first one as they are specified in `data_file_directories` sounds easier to understand than an alphabetical selection. The phrase `Cassandra will store the data of the local system keyspaces the first of the data directories specified by data_file_directories` in the documentation seems pretty clear about how the selection is done. We could also add a note in the documentation for `data_file_directories` mentioning that the data of the local system keyspaces will be stored in the first directory, and perhaps warning about the consequences of changing the order of the specified data directories, if we are concerned about accidental order changes.

##########
File path: src/java/org/apache/cassandra/db/Directories.java
##########
@@ -592,10 +581,34 @@ public static File getBackupsDirectory(File location)
         }
     }
 
+    /**
+     * Checks if the specified table should be stored with local system data.
+     *
+     * <p> To minimize the risk of failures, SSTables for local system keyspaces must be stored in a single data
+     * directory. The only exception to this is the system paxos table as it can be a high traffic table.</p>

Review comment:
       I think that after the last changes paxos is not the only exception

##########
File path: src/java/org/apache/cassandra/db/Directories.java
##########
@@ -632,6 +645,89 @@ public String toString()
         }
     }
 
+    /**
+     * Data directories used to store keyspace data.
+     */
+    public static final class DataDirectories implements Iterable<DataDirectory>
+    {
+        /**
+         * The directories for storing the local system keyspaces.
+         */
+        private final DataDirectory[] localSystemKeyspaceDataDirectories;
+
+        /**
+         * The directories where the data of the non local system keyspaces should be stored.
+         */
+        private final DataDirectory[] nonLocalSystemKeyspacesDirectories;
+
+
+        public DataDirectories(String[] locationsForNonSystemKeyspaces, String[] locationsForSystemKeyspace)
+        {
+            nonLocalSystemKeyspacesDirectories = toDataDirectories(locationsForNonSystemKeyspaces);
+            localSystemKeyspaceDataDirectories = toDataDirectories(locationsForSystemKeyspace);
+        }
+
+        private static DataDirectory[] toDataDirectories(String... locations)
+        {
+            DataDirectory[] directories = new DataDirectory[locations.length];
+            for (int i = 0; i < locations.length; ++i)
+                directories[i] = new DataDirectory(new File(locations[i]));
+            return directories;
+        }
+
+        /**
+         * Returns the data directories for the specified table.
+         *
+         * @param table the table metadata
+         * @return the data directories for the specified table
+         */
+        public DataDirectory[] getDataDirectoriesFor(TableMetadata table)
+        {
+            return isStoredInLocalSystemKeyspacesDataLocation(table.keyspace, table.name) ? localSystemKeyspaceDataDirectories
+                                                                                          : nonLocalSystemKeyspacesDirectories;
+        }
+
+        @Override
+        public Iterator<DataDirectory> iterator()
+        {
+            return getAllDirectories().iterator();
+        }
+
+        public Set<DataDirectory> getAllDirectories()
+        {
+            Set<DataDirectory> directories = new LinkedHashSet<>(nonLocalSystemKeyspacesDirectories.length + localSystemKeyspaceDataDirectories.length);
+            Collections.addAll(directories, nonLocalSystemKeyspacesDirectories);
+            Collections.addAll(directories, localSystemKeyspaceDataDirectories);
+            return directories;
+        }
+
+        @Override
+        public boolean equals(Object o)
+        {
+            if (this == o) return true;
+            if (o == null || getClass() != o.getClass()) return false;
+
+            DataDirectories that = (DataDirectories) o;
+
+            return Arrays.equals(this.localSystemKeyspaceDataDirectories, that.localSystemKeyspaceDataDirectories)
+                && Arrays.equals(this.nonLocalSystemKeyspacesDirectories, that.nonLocalSystemKeyspacesDirectories);
+        }
+
+        @Override
+        public int hashCode()
+        {
+            return Objects.hash(localSystemKeyspaceDataDirectories, nonLocalSystemKeyspacesDirectories);
+        }
+
+        public String toString()
+        {
+            return ""DataDirectories {"" +
+                   ""systemKeyspaceDataDirectories="" + localSystemKeyspaceDataDirectories +
+                   "", nonSystemKeyspacesDirectories="" + nonLocalSystemKeyspacesDirectories +

Review comment:
       Nit: we could use something like `Arrays.toString` to better print the directories. Also, the method could use an `@Override` annotation.

##########
File path: src/java/org/apache/cassandra/service/CassandraDaemon.java
##########
@@ -472,6 +492,73 @@ public void runStartupChecks()
         }
 
     }
+
+    /**
+     * Checks if the data of the local system keyspaces need to be migrated to a different location.
+     *
+     * @throws IOException
+     */
+    public void migrateSystemDataIfNeeded() throws IOException
+    {
+        // If there is only one directory and no system keyspace directory has been specified we do not need to do
+        // anything. If it is not the case we want to try to migrate the data.
+        if (!DatabaseDescriptor.useSpecificLocationForLocalSystemData()
+                && DatabaseDescriptor.getNonLocalSystemKeyspacesDataFileLocations().length <= 1)
+            return;
+
+        // We can face several cases:
+        //  1) The system data are spread accross the data file locations and need to be moved to
+        //     the first data location (upgrade to 4.0)
+        //  2) The system data are spread accross the data file locations and need to be moved to
+        //     the system keyspace location configured by the user (upgrade to 4.0)
+        //  3) The system data are stored in the first data location and need to be moved to
+        //     the system keyspace location configured by the user (system_data_file_directory has been configured)

Review comment:
       I understand that we can't have the opposite case, that the system data was originally stored in a separate directory but the user wants to move it to the first data location. In such case `system_data_file_directory` wouldn't be set so we can't know where the local system data is. That configuration change would require the user to manually move the data to the first data directory. Is this right? 

##########
File path: src/java/org/apache/cassandra/db/ColumnFamilyStore.java
##########
@@ -2806,4 +2796,88 @@ public boolean getNeverPurgeTombstones()
     {
         return neverPurgeTombstones;
     }
-}
\ No newline at end of file
+
+    /**
+     * The thread pools used to flush memtables.
+     *
+     * <p>Each disk has its own set of thread pools to perform memtable flushes.</p>
+     * <p>Based on the configuration. Local system keyspaces can have their own disk
+     * to allow for special redundancy mechanism. If it is the case the executor services returned for
+     * local system keyspaces will be different from the ones for the other keyspaces.</p>
+     */
+    private static final class PerDiskFlushExecutors
+    {
+        /**
+         * The flush executors for non local system keyspaces.
+         */
+        private final ExecutorService[] nonLocalSystemflushExecutors;
+
+        /**
+         * The flush executors for the local system keyspaces.
+         */
+        private final ExecutorService[] localSystemDiskFlushExecutors;
+
+        /**
+         * {@code true} if local system keyspaces are stored in their own directory and use an extra flush executor,
+         * {@code false} otherwise.
+         */
+        private boolean useSpecificExecutorForSystemKeyspaces;
+
+        public PerDiskFlushExecutors(int flushWriters,
+                                     String[] locationsForNonSystemKeyspaces,
+                                     boolean useSpecificLocationForSystemKeyspaces)
+        {
+            ExecutorService[] flushExecutors = createPerDiskFlushWriters(locationsForNonSystemKeyspaces.length, flushWriters);
+            nonLocalSystemflushExecutors = flushExecutors;
+            useSpecificExecutorForSystemKeyspaces = useSpecificLocationForSystemKeyspaces;
+            localSystemDiskFlushExecutors = useSpecificLocationForSystemKeyspaces ? new ExecutorService[] {newThreadPool(""LocalSystemKeyspacesDiskMemtableFlushWriter"", flushWriters)}
+                                                                             : new ExecutorService[] {flushExecutors[0]};
+        }
+
+        private ExecutorService[] createPerDiskFlushWriters(int numberOfExecutors, int flushWriters)
+        {
+            ExecutorService[] flushExecutors = new ExecutorService[numberOfExecutors];
+
+            for (int i = 0; i < numberOfExecutors; i++)
+            {
+                flushExecutors[i] = newThreadPool(""PerDiskMemtableFlushWriter_"" + i, flushWriters);
+            }
+            return flushExecutors;
+        }
+
+        private static JMXEnabledThreadPoolExecutor newThreadPool(String poolName, int size)
+        {
+            return new JMXEnabledThreadPoolExecutor(size,
+                                                    Stage.KEEP_ALIVE_SECONDS,
+                                                    TimeUnit.SECONDS,
+                                                    new LinkedBlockingQueue<>(),
+                                                    new NamedThreadFactory(poolName),
+                                                    ""internal"");
+        }
+
+        /**
+         * Returns the flush executors for the specified keyspace.
+         *
+         * @param keyspaceName the keyspace name
+         * @param tableName the table name
+         * @return the flush executors that should be used for flushing the memtables of the specified keyspace.
+         */
+        public ExecutorService[] getExecutorsFor(String keyspaceName, String tableName)
+        {
+            return Directories.isStoredInLocalSystemKeyspacesDataLocation(keyspaceName, tableName) ? localSystemDiskFlushExecutors
+                                                                                              : nonLocalSystemflushExecutors;

Review comment:
       Nit: misaligned
   ```suggestion
                                                                                                      : nonLocalSystemflushExecutors;
   ```

##########
File path: src/java/org/apache/cassandra/db/ColumnFamilyStore.java
##########
@@ -2806,4 +2796,88 @@ public boolean getNeverPurgeTombstones()
     {
         return neverPurgeTombstones;
     }
-}
\ No newline at end of file
+
+    /**
+     * The thread pools used to flush memtables.
+     *
+     * <p>Each disk has its own set of thread pools to perform memtable flushes.</p>
+     * <p>Based on the configuration. Local system keyspaces can have their own disk
+     * to allow for special redundancy mechanism. If it is the case the executor services returned for
+     * local system keyspaces will be different from the ones for the other keyspaces.</p>
+     */
+    private static final class PerDiskFlushExecutors
+    {
+        /**
+         * The flush executors for non local system keyspaces.
+         */
+        private final ExecutorService[] nonLocalSystemflushExecutors;
+
+        /**
+         * The flush executors for the local system keyspaces.
+         */
+        private final ExecutorService[] localSystemDiskFlushExecutors;
+
+        /**
+         * {@code true} if local system keyspaces are stored in their own directory and use an extra flush executor,
+         * {@code false} otherwise.
+         */
+        private boolean useSpecificExecutorForSystemKeyspaces;

Review comment:
       Nit: might be final

##########
File path: src/java/org/apache/cassandra/db/ColumnFamilyStore.java
##########
@@ -2806,4 +2796,88 @@ public boolean getNeverPurgeTombstones()
     {
         return neverPurgeTombstones;
     }
-}
\ No newline at end of file
+
+    /**
+     * The thread pools used to flush memtables.
+     *
+     * <p>Each disk has its own set of thread pools to perform memtable flushes.</p>
+     * <p>Based on the configuration. Local system keyspaces can have their own disk
+     * to allow for special redundancy mechanism. If it is the case the executor services returned for
+     * local system keyspaces will be different from the ones for the other keyspaces.</p>
+     */
+    private static final class PerDiskFlushExecutors
+    {
+        /**
+         * The flush executors for non local system keyspaces.
+         */
+        private final ExecutorService[] nonLocalSystemflushExecutors;
+
+        /**
+         * The flush executors for the local system keyspaces.
+         */
+        private final ExecutorService[] localSystemDiskFlushExecutors;
+
+        /**
+         * {@code true} if local system keyspaces are stored in their own directory and use an extra flush executor,
+         * {@code false} otherwise.
+         */
+        private boolean useSpecificExecutorForSystemKeyspaces;
+
+        public PerDiskFlushExecutors(int flushWriters,
+                                     String[] locationsForNonSystemKeyspaces,
+                                     boolean useSpecificLocationForSystemKeyspaces)
+        {
+            ExecutorService[] flushExecutors = createPerDiskFlushWriters(locationsForNonSystemKeyspaces.length, flushWriters);
+            nonLocalSystemflushExecutors = flushExecutors;
+            useSpecificExecutorForSystemKeyspaces = useSpecificLocationForSystemKeyspaces;
+            localSystemDiskFlushExecutors = useSpecificLocationForSystemKeyspaces ? new ExecutorService[] {newThreadPool(""LocalSystemKeyspacesDiskMemtableFlushWriter"", flushWriters)}
+                                                                             : new ExecutorService[] {flushExecutors[0]};

Review comment:
       Nit: misaligned
   ```suggestion
                                                                                     : new ExecutorService[] {flushExecutors[0]};
   ```

##########
File path: src/java/org/apache/cassandra/db/ColumnFamilyStore.java
##########
@@ -2806,4 +2796,88 @@ public boolean getNeverPurgeTombstones()
     {
         return neverPurgeTombstones;
     }
-}
\ No newline at end of file
+
+    /**
+     * The thread pools used to flush memtables.
+     *
+     * <p>Each disk has its own set of thread pools to perform memtable flushes.</p>
+     * <p>Based on the configuration. Local system keyspaces can have their own disk
+     * to allow for special redundancy mechanism. If it is the case the executor services returned for
+     * local system keyspaces will be different from the ones for the other keyspaces.</p>
+     */
+    private static final class PerDiskFlushExecutors
+    {
+        /**
+         * The flush executors for non local system keyspaces.
+         */
+        private final ExecutorService[] nonLocalSystemflushExecutors;
+
+        /**
+         * The flush executors for the local system keyspaces.
+         */
+        private final ExecutorService[] localSystemDiskFlushExecutors;
+
+        /**
+         * {@code true} if local system keyspaces are stored in their own directory and use an extra flush executor,
+         * {@code false} otherwise.
+         */
+        private boolean useSpecificExecutorForSystemKeyspaces;
+
+        public PerDiskFlushExecutors(int flushWriters,
+                                     String[] locationsForNonSystemKeyspaces,
+                                     boolean useSpecificLocationForSystemKeyspaces)
+        {
+            ExecutorService[] flushExecutors = createPerDiskFlushWriters(locationsForNonSystemKeyspaces.length, flushWriters);
+            nonLocalSystemflushExecutors = flushExecutors;
+            useSpecificExecutorForSystemKeyspaces = useSpecificLocationForSystemKeyspaces;
+            localSystemDiskFlushExecutors = useSpecificLocationForSystemKeyspaces ? new ExecutorService[] {newThreadPool(""LocalSystemKeyspacesDiskMemtableFlushWriter"", flushWriters)}
+                                                                             : new ExecutorService[] {flushExecutors[0]};
+        }
+
+        private ExecutorService[] createPerDiskFlushWriters(int numberOfExecutors, int flushWriters)

Review comment:
       Might be static:
   ```suggestion
           private static ExecutorService[] createPerDiskFlushWriters(int numberOfExecutors, int flushWriters)
   ```

##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1830,11 +1857,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which supports redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForLocalSystemData()
+    {
+        return conf.local_system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where the local system keyspaces data should be stored.
+     *
+     * <p>If the {@code local_system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guarantees that the server can tolerate the lost of n - 1 disks.</p>

Review comment:
       Rather than saying than `the server can tolerate the lost of n - 1 disks`, I think I'd say `the server can tolerate the lost of all the disks but the first one`, to avoid the risk of readers getting the wrong idea that they can lost *any* n - 1 disks.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jan/21 04:20;githubbot;600","adelapena commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r564437724



##########
File path: NEWS.txt
##########
@@ -38,6 +38,14 @@ using the provided 'sstableupgrade' tool.
 
 New features
 ------------
+    - The data of the system keyspaces using a local strategy (at the exception of the system.batches,
+      system.paxos, system.compaction_history, system.prepared_statements and system.repair tables)
+      is now stored by default in the first data directory. This approach will allow the server

Review comment:
       Perhaps `is now stored by default in the first data directory, instead of being distributed among all the data directories.`, to be even more clear?




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jan/21 11:22;githubbot;600","adelapena commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r564446973



##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1830,11 +1857,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which supports redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForLocalSystemData()
+    {
+        return conf.local_system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where the local system keyspaces data should be stored.
+     *
+     * <p>If the {@code local_system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guarantees that the server can tolerate the lost of n - 1 disks.</p>
+     *
+     * @return the locations where should be stored the local system keyspaces data
+     */
+    public static String[] getLocalSystemKeyspacesDataFileLocations()

Review comment:
       We could return a single `String` instead of an array, since it is always a single directory. I gave it a go and doing so doesn't seem problematic with the current callers of the method, and `CassandraDaemon#migrateSystemDataIfNeeded()` is explicitly extracting the first element of the array.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jan/21 11:39;githubbot;600","blerer commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r564564848



##########
File path: src/java/org/apache/cassandra/db/Directories.java
##########
@@ -632,6 +645,89 @@ public String toString()
         }
     }
 
+    /**
+     * Data directories used to store keyspace data.
+     */
+    public static final class DataDirectories implements Iterable<DataDirectory>
+    {
+        /**
+         * The directories for storing the local system keyspaces.
+         */
+        private final DataDirectory[] localSystemKeyspaceDataDirectories;
+
+        /**
+         * The directories where the data of the non local system keyspaces should be stored.
+         */
+        private final DataDirectory[] nonLocalSystemKeyspacesDirectories;
+
+
+        public DataDirectories(String[] locationsForNonSystemKeyspaces, String[] locationsForSystemKeyspace)
+        {
+            nonLocalSystemKeyspacesDirectories = toDataDirectories(locationsForNonSystemKeyspaces);
+            localSystemKeyspaceDataDirectories = toDataDirectories(locationsForSystemKeyspace);
+        }
+
+        private static DataDirectory[] toDataDirectories(String... locations)
+        {
+            DataDirectory[] directories = new DataDirectory[locations.length];
+            for (int i = 0; i < locations.length; ++i)
+                directories[i] = new DataDirectory(new File(locations[i]));
+            return directories;
+        }
+
+        /**
+         * Returns the data directories for the specified table.
+         *
+         * @param table the table metadata
+         * @return the data directories for the specified table
+         */
+        public DataDirectory[] getDataDirectoriesFor(TableMetadata table)
+        {
+            return isStoredInLocalSystemKeyspacesDataLocation(table.keyspace, table.name) ? localSystemKeyspaceDataDirectories
+                                                                                          : nonLocalSystemKeyspacesDirectories;
+        }
+
+        @Override
+        public Iterator<DataDirectory> iterator()
+        {
+            return getAllDirectories().iterator();
+        }
+
+        public Set<DataDirectory> getAllDirectories()
+        {
+            Set<DataDirectory> directories = new LinkedHashSet<>(nonLocalSystemKeyspacesDirectories.length + localSystemKeyspaceDataDirectories.length);
+            Collections.addAll(directories, nonLocalSystemKeyspacesDirectories);
+            Collections.addAll(directories, localSystemKeyspaceDataDirectories);
+            return directories;
+        }
+
+        @Override
+        public boolean equals(Object o)
+        {
+            if (this == o) return true;
+            if (o == null || getClass() != o.getClass()) return false;
+
+            DataDirectories that = (DataDirectories) o;
+
+            return Arrays.equals(this.localSystemKeyspaceDataDirectories, that.localSystemKeyspaceDataDirectories)
+                && Arrays.equals(this.nonLocalSystemKeyspacesDirectories, that.nonLocalSystemKeyspacesDirectories);
+        }
+
+        @Override
+        public int hashCode()
+        {
+            return Objects.hash(localSystemKeyspaceDataDirectories, nonLocalSystemKeyspacesDirectories);
+        }
+
+        public String toString()
+        {
+            return ""DataDirectories {"" +
+                   ""systemKeyspaceDataDirectories="" + localSystemKeyspaceDataDirectories +
+                   "", nonSystemKeyspacesDirectories="" + nonLocalSystemKeyspacesDirectories +

Review comment:
       Good catch




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jan/21 14:44;githubbot;600","blerer commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r564564996



##########
File path: src/java/org/apache/cassandra/service/CassandraDaemon.java
##########
@@ -472,6 +492,73 @@ public void runStartupChecks()
         }
 
     }
+
+    /**
+     * Checks if the data of the local system keyspaces need to be migrated to a different location.
+     *
+     * @throws IOException
+     */
+    public void migrateSystemDataIfNeeded() throws IOException
+    {
+        // If there is only one directory and no system keyspace directory has been specified we do not need to do
+        // anything. If it is not the case we want to try to migrate the data.
+        if (!DatabaseDescriptor.useSpecificLocationForLocalSystemData()
+                && DatabaseDescriptor.getNonLocalSystemKeyspacesDataFileLocations().length <= 1)
+            return;
+
+        // We can face several cases:
+        //  1) The system data are spread accross the data file locations and need to be moved to
+        //     the first data location (upgrade to 4.0)
+        //  2) The system data are spread accross the data file locations and need to be moved to
+        //     the system keyspace location configured by the user (upgrade to 4.0)
+        //  3) The system data are stored in the first data location and need to be moved to
+        //     the system keyspace location configured by the user (system_data_file_directory has been configured)

Review comment:
       Yes




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jan/21 14:44;githubbot;600","blerer commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r564579905



##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1830,11 +1857,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which supports redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForLocalSystemData()
+    {
+        return conf.local_system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where the local system keyspaces data should be stored.
+     *
+     * <p>If the {@code local_system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guarantees that the server can tolerate the lost of n - 1 disks.</p>
+     *
+     * @return the locations where should be stored the local system keyspaces data
+     */
+    public static String[] getLocalSystemKeyspacesDataFileLocations()

Review comment:
       I hit some issues with that when writing the initial patch when the data directories where empty. Some tests run with such a config. I came up with that approach because it was allowing the code to behave in the same way as before the patch.
   Unfortunately, I do not remeber all the details. I would have to try again if you think it is a better approach to use a String.  




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jan/21 15:03;githubbot;600","adelapena commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r565440824



##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -1830,11 +1857,57 @@ public static void setInterDCStreamThroughputOutboundMegabitsPerSec(int value)
         conf.inter_dc_stream_throughput_outbound_megabits_per_sec = value;
     }
 
-    public static String[] getAllDataFileLocations()
+    /**
+     * Checks if the local system data must be stored in a specific location which supports redundancy.
+     *
+     * @return {@code true} if the local system keyspaces data must be stored in a different location,
+     * {@code false} otherwise.
+     */
+    public static boolean useSpecificLocationForLocalSystemData()
+    {
+        return conf.local_system_data_file_directory != null;
+    }
+
+    /**
+     * Returns the locations where the local system keyspaces data should be stored.
+     *
+     * <p>If the {@code local_system_data_file_directory} was unspecified, the local system keyspaces data should be stored
+     * in the first data directory. This approach guarantees that the server can tolerate the lost of n - 1 disks.</p>
+     *
+     * @return the locations where should be stored the local system keyspaces data
+     */
+    public static String[] getLocalSystemKeyspacesDataFileLocations()

Review comment:
       I see the problem with tests, `DatabaseDescriptor#getLocalSystemKeyspacesDataFileLocations` is using an empty array to represent the fact that `data_file_directories` hasn't been set. [Here](https://github.com/adelapena/cassandra/commit/93bbe2e3a983811cce91c3ded989346ee8ea7c4d) is what I tried, returning a single `String` that can be null if `data_file_directories` is empty. Managing this situation inside the constructor of `DataDirectories` makes those tests to pass ([here](https://app.circleci.com/pipelines/github/adelapena/cassandra/167/workflows/aed26434-3657-47fa-a3a5-c5b20eced2aa)), while having a `DatabaseDescriptor#getLocalSystemKeyspacesDataFileLocation` method returning a nullable string (it could also be an optional). Nevertheless I think this is not a big deal and I guess we can just return an array in `getLocalSystemKeyspacesDataFileLocations`.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Jan/21 16:15;githubbot;600","krummas commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r569394141



##########
File path: .circleci/config.yml.HIGHRES
##########
@@ -2183,6 +2183,98 @@ jobs:
     - CCM_HEAP_NEWSIZE: 256M
     - JAVA_HOME: /usr/lib/jvm/java-8-openjdk-amd64
     - JDK_HOME: /usr/lib/jvm/java-8-openjdk-amd64
+  utests_system_keyspace_directory:
+    docker:
+    - image: nastra/cassandra-testing-ubuntu1910-java11-w-dependencies:20200603

Review comment:
       this should use the same docker image as the other test targets (`apache/cassandra-testing-ubuntu2004-java11-w-dependencies:20210105`)




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Feb/21 13:06;githubbot;600","blerer commented on a change in pull request #675:
URL: https://github.com/apache/cassandra/pull/675#discussion_r569405822



##########
File path: .circleci/config.yml.HIGHRES
##########
@@ -2183,6 +2183,98 @@ jobs:
     - CCM_HEAP_NEWSIZE: 256M
     - JAVA_HOME: /usr/lib/jvm/java-8-openjdk-amd64
     - JDK_HOME: /usr/lib/jvm/java-8-openjdk-amd64
+  utests_system_keyspace_directory:
+    docker:
+    - image: nastra/cassandra-testing-ubuntu1910-java11-w-dependencies:20200603

Review comment:
       Good catch! 




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Feb/21 13:19;githubbot;600","smiklosovic closed pull request #675:
URL: https://github.com/apache/cassandra/pull/675


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 14:01;githubbot;600",,,,,,,,,,0,24000,,,0,24000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,blerer,,,,,,,,,,,,,,,,,,,,Normal,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 04 14:15:28 UTC 2021,,,,,,,,,,,"0|i3yiz3:",9223372036854775807,,,,,,,,,,,adelapena,marcuse,,Normal,,NA,,,https://github.com/apache/cassandra/commit/f69b11eee9605add3a006de46eedb773a984d90b,,,,,,,,,"* Run of JUnit tests with separate directory for local system keyspaces
* New unit tests for some of the new code.",,,,,"27/Sep/18 10:44;KurtG;IDK that pinning the system tables really fixes the issue as we'll still have the problem of losing one disk means a whole node replacement.
Theoretically it should be possible to rebuild the (important) system tables in the case they get corrupted. Most things currently should be able to be retrieved from gossip/other nodes. How about if there's something we currently can't retrieve we just store it in the peers table on all the nodes. I can't imagine the amount of info we'd have to store would be very big. Then we can just provide a startup flag to indicate that a node should try and rebuild its system tables.
;;;","27/Sep/18 13:24;urandom;Awesome; I'd been meaning to open this very ticket.

I had planned to suggest what [~krummas] did, that it be possible to put {{system}} in a different data directory. At least if this were possible, {{system}} could be put on a RAID.  And, at least in our environments, if the expectation is that you can survive a single device failure, then the OS is likely already on RAID-1 or similar.

Of course, if the tables in {{system}} could be regenerated, that would be better still but I'm not sure what that looks like complexity-wise versus pinning it.;;;","29/May/20 14:09;blerer;{quote}
Of course, if the tables in system could be regenerated, that would be better still but I'm not sure what that looks like complexity-wise versus pinning it.
{quote}

Regenerating the system tables is far more complex that simply pinning the system tables to a given disk and by consequence more risky.

I made an initial patch [here|https://github.com/apache/cassandra/compare/trunk...blerer:CASSANDRA-14793]. The patch allow to configure a specific directory for storing system keyspaces data (local and replicated) through the {{system_keyspaces_data_file_directory}} property in the cassandra.yaml file. This directory must be different from the other data directories to ensure that it cannot be filled by the data of the other keyspaces. If the {{system_keyspaces_data_file_directory}} property was not set the behavior was the same as the one we currently have.

Another approach suggested by [~marcuse] would be to pin the system keyspaces data to the first of the of the {{data_file_directories}}.

For both solutions we would need to find a way for the user to easily migrates it systems tables data into the new directories if needed.

I do not have a strong opinion on which solution is better and would like to get some feedback from people having more experience that me with large production clusters.;;;","29/May/20 14:41;benedict;It would be possible to replicate the data across multiple disks, but only to compact one of these disks, and to replicate the result of compaction to the others?  This would create redundancy for the relatively cheaply-maintained system tables?

Note that either way the Paxos table might need to be dealt with differently to other system tables, since it is potentially high traffic, and constraining it to one disk might be problematic.  I don't recall if we ever got round to introducing the range-aware compaction Marcus put together, but the Paxos table seems more suited to this approach.
;;;","10/Jul/20 12:14;blerer;The patch ensure that the data of the local system keyspaces is stored by default within the first data directory (at the exception of the paxos table) to allow the node to tolerate a failure of the disks associated with the other directories.
   The patch also allow users to configure a different directory for the system keyspaces. This would allow people to use a disk providing redundancy to support the lost of any of the disks used to store the data.
   On startup existing system keyspace data will be automatically migrated to support 4.0 upgrades or configuration changes (use of a separate disk for the system keyspaces). ;;;","28/Sep/20 15:43;blerer;[~marcuse], [~benedict] How do you think we should handle the case where the {{disk_failure_policy}} is {{best_effort}} and the disk containing the system data is marked as {{unreadable}} or {{unwritable}} ? ;;;","28/Sep/20 17:07;benedict;Perhaps shut the server down for all writers and compaction, and serve only reads?  I've not got a strong opinion about it though - hard to run safely in this context, so would seem fine to just admit defeat in this case.  This does strengthen the argument for replication system keyspace data to multiple disks discussed above.;;;","01/Oct/20 15:00;blerer;[PR|https://github.com/apache/cassandra/pull/675] [CI|https://app.circleci.com/pipelines/github/blerer/cassandra/34/workflows/34ff6aa9-2ee9-4d3e-8929-3c9f048ce357];;;","22/Jan/21 13:49;blerer;I pushed some extra commits to address the rewiev comments.

[Casssandra PR|https://github.com/apache/cassandra/pull/675], [Cassandra-Dtest PR|https://github.com/apache/cassandra-dtest/pull/114], [Circle-ci j8|https://app.circleci.com/pipelines/github/blerer/cassandra/95/workflows/3eceedd1-e063-4eef-9336-8a49867fb7e9], [Circle-ci j11|https://app.circleci.com/pipelines/github/blerer/cassandra/95/workflows/c174d359-cfe5-4a21-a57b-dd5f67a0e3ad];;;","03/Feb/21 13:06;marcuse;+1 (just a final small comment on the pr);;;","03/Feb/21 15:42;adelapena;Changes look good to me, +1;;;","04/Feb/21 14:14;blerer;Cassandra patch committed into trunk at f69b11eee9605add3a006de46eedb773a984d90b
DTest patch committed into trunk at 28d0a668bf4f23d6b16ec47dc3f2ddab24769105;;;","04/Feb/21 14:15;blerer;[~adelapena], [~marcuse] Thanks a lot for the deep reviews.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
skip TestRepair.test_dead_coordinator dtest in 4.0,CASSANDRA-14792,13187462,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,25/Sep/18 22:57,15/May/20 08:06,13/Jul/23 08:37,26/Sep/18 23:46,4.0,4.0-alpha1,,,,,Test/dtest/python,,,,0,,,,"CASSANDRA-14763 changed the coordinator behavior to not cleanup old repair sessions, so this test doesn't really make sense anymore. We should just skip it in 4.0",,bdeggleston,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 26 23:46:26 UTC 2018,,,,,,,,,,,"0|i3yhu7:",9223372036854775807,,,,,,,,,ifesdjeen,,ifesdjeen,,,Low,,,,,,,,,,,,,,,,,,,"25/Sep/18 23:01;bdeggleston;[dtest|https://github.com/bdeggleston/cassandra-dtest/tree/14792]
[circle|https://circleci.com/workflow-run/bfc92bc4-5d31-4303-865b-b27515f0de00];;;","26/Sep/18 17:31;ifesdjeen;+1, thank you for fixing this one! 

For completeness, the test was failing with 

{code}
java.lang.RuntimeException: java.lang.IllegalArgumentException: Invalid state transition FINALIZED -> FAILED
        at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:214)
        at org.apache.cassandra.net.MessageDeliveryTask.process(MessageDeliveryTask.java:92)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:54)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
{code}

This happens only every once in a while, since a completed task is racing with cancel. We could improve a failure message on cancelation if we had a distinction between failed state and cancelled one, but this might be not worth it.;;;","26/Sep/18 23:46;bdeggleston;committed as f4888c8976c2012e9de3b92dedb0ae1a3c984a4b, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[utest] tests unable to write system tmp directory,CASSANDRA-14791,13187460,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,25/Sep/18 22:35,15/May/20 08:00,13/Jul/23 08:37,08/Oct/18 15:51,4.0,4.0-alpha1,,,,,Legacy/Testing,,,,0,,,,"Some tests are failing from time to time because it cannot write to directory {{/tmp/}}:
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-trunk-test/lastCompletedBuild/testReport/org.apache.cassandra.streaming.compression/CompressedInputStreamTest/testException/

{noformat}
java.lang.RuntimeException: java.nio.file.AccessDeniedException: /tmp/na-1-big-Data.db
	at org.apache.cassandra.io.util.SequentialWriter.openChannel(SequentialWriter.java:119)
	at org.apache.cassandra.io.util.SequentialWriter.<init>(SequentialWriter.java:152)
	at org.apache.cassandra.io.util.SequentialWriter.<init>(SequentialWriter.java:141)
	at org.apache.cassandra.io.compress.CompressedSequentialWriter.<init>(CompressedSequentialWriter.java:82)
	at org.apache.cassandra.streaming.compression.CompressedInputStreamTest.testCompressedReadWith(CompressedInputStreamTest.java:119)
	at org.apache.cassandra.streaming.compression.CompressedInputStreamTest.testException(CompressedInputStreamTest.java:78)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.AccessDeniedException: /tmp/na-1-big-Data.db
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:177)
	at java.nio.channels.FileChannel.open(FileChannel.java:287)
	at java.nio.channels.FileChannel.open(FileChannel.java:335)
	at org.apache.cassandra.io.util.SequentialWriter.openChannel(SequentialWriter.java:100)
{noformat}

 I guess it's because some Jenkins slaves don't have proper permission set. For slave {{cassandra16}}, the tests are fine:
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-trunk-test/723/testReport/junit/org.apache.cassandra.streaming.compression/CompressedInputStreamTest/testException/history/",,jay.zhuang,KurtG,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 08 15:51:53 UTC 2018,,,,,,,,,,,"0|i3yhtr:",9223372036854775807,,,,,,,,,marcuse,,marcuse,,,Low,,,,,,,,,,,,,,,,,,,"26/Sep/18 16:23;jay.zhuang;Hi [~mshuler], [~spodxx@gmail.com], any idea if there's a permission setting we could set for the Jenkins Job/Slave?;;;","27/Sep/18 10:47;KurtG;All the hosts are meant to be set up in the same way... However INFRA use puppet, which I have absolutely no faith in. The slaves are all completely managed by INFRA so it's probably a case for an INFRA ticket :/;;;","27/Sep/18 11:50;marcuse;should we move unit tests to run in docker to get more control?;;;","27/Sep/18 16:42;jay.zhuang;[~mshuler] talked about the docker option in the last NGCC: https://github.com/ngcc/ngcc2017/blob/master/Help_Test_Apache_Cassandra-NGCC_2017.pdf . Any idea how we can move forward with this?;;;","08/Oct/18 03:28;jay.zhuang;The root cause of this test failure is not because {{/tmp/}} directory is not writable. But because the unittest generated tmp files {{/tmp/na-1-big-Data.db}} and {{/tmp/na-1-big-CompressionInfo.db}} are not deleted after the test. So I guess on these nodes, the test was run by other user, which left the tmp files that the current user cannot override. I'm able to reproduce the same error message by:
{noformat}
sudo chown root:root /tmp/na-1-big-Data.db
{noformat}

Here is a patch for trunk:
| Branch | uTest |
| [14791|https://github.com/cooldoger/cassandra/tree/14791] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14791.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14791] |

Passed the tests in Jenkins:
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/36/testReport/org.apache.cassandra.streaming.compression/CompressedInputStreamTest/;;;","08/Oct/18 04:43;marcuse;lgtm, +1;;;","08/Oct/18 15:51;jay.zhuang;Thanks [~krummas] for the review. Committed as [{{73ebd20}}|https://github.com/apache/cassandra/commit/73ebd200c04335624f956e79624cf8494d872f19].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"nodetool status ""Load"" columns has wrong width",CASSANDRA-14787,13186972,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,kirktrue,lapo@lapo.it,lapo@lapo.it,24/Sep/18 10:04,07/Mar/23 11:52,13/Jul/23 08:37,22/Jul/19 23:25,4.1,4.1-alpha1,,,,,Tool/nodetool,,,,0,lhf,nodetool,pull-request-available,"Using Cassandra 3.11.2 on FreeBSD, I get:
{code:java}
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address            Load       Tokens       Owns (effective)  Host ID ...
UN  server1.andxor.it  11.11 MiB  256          39.6%             ...
UN  server2.andxor.it  32.04 MiB  256          41.8%             ...
UN  server3.andxor.it  519.33 KiB  256          40.0%             ...
UN  server4.andxor.it  10.95 MiB  256          40.3%             ...
UN  server5.andxor.it  11.03 MiB  256          38.4%             ...
{code}
AFAICT this is caused by {{""%-9s""}} in [Status.java:292|https://github.com/apache/cassandra/blob/06209037ea56b5a2a49615a99f1542d6ea1b2947/src/java/org/apache/cassandra/tools/nodetool/Status.java#L292] which should be probably a 10 instead of 9.",,cnlwsu,kirktrue,lapo@lapo.it,tcooke,,,,,,,,,,,,,,,,,,,,,,"GitHub user kirktrue opened a pull request:

    https://github.com/apache/cassandra/pull/286

    Align load column in nodetool status output

    patch by Kirk True; reviewed by Y for CASSANDRA-14787

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/kirktrue/cassandra 14787-trunk

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/286.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #286
    
----
commit 6fef7edfbef9ef096d12463149fd6decc9c45f78
Author: Kirk True <kirk@...>
Date:   2018-10-16T23:35:04Z

    Align load column in nodetool status output
    
    patch by Kirk True; reviewed by Y for CASSANDRA-14787

----
;16/Oct/18 23:38;githubbot;600","smiklosovic closed pull request #286:
URL: https://github.com/apache/cassandra/pull/286


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 12:03;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/19 22:46;kirktrue;14787-trunk.txt;https://issues.apache.org/jira/secure/attachment/12974882/14787-trunk.txt","16/Oct/18 23:37;kirktrue;CASSANDRA-14787.txt;https://issues.apache.org/jira/secure/attachment/12944241/CASSANDRA-14787.txt",,,,,,,,,,,,2.0,kirktrue,,,,,,,,,,,,,,,,,,,,Low Hanging Fruit,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 22 23:25:59 UTC 2019,,,,,,,,,,,"0|i3yeu7:",9223372036854775807,3.11.2,5.0,,,,,,,,,cnlwsu,,,Low,,3.11.2,,,"[893908e2dc1fe011e38c847395cb39be4ca53d89|https://github.com/apache/cassandra/commit/893908e2dc1fe011e38c847395cb39be4ca53d89]",,,,,,,,,Tested via the command line. No (known) existing unit test available.,,,,,"25/Sep/18 13:12;lapo@lapo.it;The same cluster gives a ""correct"" answer when called from a different host with LANG environment (and others?) are set to Italian:
{code:java}
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address            Load       Tokens       Owns (effective)  Host ID ...
UN  server1.andxor.it  11.11 MiB  256          39,6%             ...
UN  server2.andxor.it  32.04 MiB  256          41,8%             ...
UN  server3.andxor.it  519.3 KiB  256          40,0%             ...
UN  server4.andxor.it  10.95 MiB  256          40,3%             ...
UN  server5.andxor.it  11.03 MiB  256          38,4%             ...
{code};;;","27/Sep/18 11:40;lapo@lapo.it;Today I have had even an 11 columns result: {{1010.18 KiB}}.;;;","18/Oct/18 14:23;cnlwsu;The offset should be from the max width instead of hard coded since it can vary so much. The other nodetool outputs have been moving to use {{TableBuilder}} so that it always aligns properly. Perhaps this table can be changed to use it as well.;;;","16/Jul/19 22:47;kirktrue;Using {{TableBuilder}} in latest patch.;;;","22/Jul/19 23:25;cnlwsu;+1, Thanks committed as [893908e2dc1fe011e38c847395cb39be4ca53d89|https://github.com/apache/cassandra/commit/893908e2dc1fe011e38c847395cb39be4ca53d89];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log message when mutation passed to CommitLog#add(Mutation) is too large is not descriptive enough,CASSANDRA-14781,13186708,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,n.v.harikrishna,jwest,jwest,21/Sep/18 19:29,30/Apr/21 06:46,13/Jul/23 08:37,04/May/20 18:33,4.0,4.0-beta1,,,,,Consistency/Hints,Local/Commit Log,Messaging/Client,,1,protocolv5,,,"When hitting [https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/commitlog/CommitLog.java#L256-L257], the log message produced does not help the operator track down what data is being written. At a minimum the keyspace and cfIds involved would be useful (and are available) – more detail might not be reasonable to include. ",,aleksey,carlo_4002,cnlwsu,e.dimitrova,jasonstack,jeromatron,jjirsa,joao.reis,jwest,leonz,mbyrd,n.v.harikrishna,samt,tpetracca,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-12231,CASSANDRA-14782,CASSANDRA-15741,,,,,,,,,,"24/Sep/18 16:46;tpetracca;CASSANDRA-14781.patch;https://issues.apache.org/jira/secure/attachment/12941074/CASSANDRA-14781.patch","25/Sep/18 02:36;tpetracca;CASSANDRA-14781_3.0.patch;https://issues.apache.org/jira/secure/attachment/12941143/CASSANDRA-14781_3.0.patch","04/Oct/19 18:34;leonz;CASSANDRA-14781_3.11.patch;https://issues.apache.org/jira/secure/attachment/12982249/CASSANDRA-14781_3.11.patch",,,,,,,,,,,3.0,n.v.harikrishna,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 05 16:57:53 UTC 2020,,,,,,,,,,,"0|i3yd7j:",9223372036854775807,,,,,,,,,cnlwsu,,cnlwsu,jwest,,Normal,,4.0-beta1,,,https://github.com/apache/cassandra/commit/d3dadcd6f3bbde471e972f8332eb62de0f2d4aae,,,,,,,,,,,,,,"24/Sep/18 16:46;tpetracca;Related: https://issues.apache.org/jira/browse/CASSANDRA-12231

[^CASSANDRA-14781.patch]

^The attached trivial patch (built off of 2.2.X) is an easy way to get more info^;;;","25/Sep/18 01:49;jwest;[~tpetracca] thanks for the patch! {{Mutation::getColumnFamilies}} was removed in Cassandra 3.0. Would you be interested in updating your patch for 3.0+ and I can review?;;;","25/Sep/18 02:37;tpetracca;I'm significantly less familiar with the 3.0+ code but at a quick glance it looks like every partition update should actually contain all this information at this point.

[^CASSANDRA-14781_3.0.patch];;;","18/Oct/18 14:31;cnlwsu;To cover extreme cases, can you add a limit of like 10 tables then add a "" ... and X more"" or something? Also can you include the partition keys with a similar cap to prevent the string from getting too large. For the common case where theres only 1 large mutation or a bunch to a single partition that would be monumentally helpful.;;;","04/Oct/19 18:35;leonz;[^CASSANDRA-14781_3.11.patch]

Hey [~cnlwsu] [~jrwest], I've attached a patch for 3.11 (should be the same for 3.0) that prints (cf, key) pairs with a limit of 10.;;;","08/Oct/19 18:20;cnlwsu;Thanks for the update! A few more things things:

* Lets make this work for 4.0 first, I am not sure about backporting it to 3.11 or 3.0 at this point
* Can you move the {{limit}} up to a constant? Be great if that was something we can override too in yaml/jmx override incase need to to see more than 10 and accept risk but that can come later if dont want to include it in this patch. 
* If limited, can you make sure to show largest keys first instead of just taking first off the updates.
* If we have metadata and the key, instead of doing a toString on the decorated key which gives a large hex dump, you can use {{partitionKeyType.getString(keybytes)}} to get the human readable version.

something like (this is untested, just example):
{code}
                 mutation.getPartitionUpdates().stream()
                         .sorted(Comparator.comparingInt(PartitionUpdate::dataSize).reversed())
                         .limit(LIMIT)
                         .map(upd -> String.format(""%s[%s]"",
                             upd.metadata().name,
                             upd.metadata().partitionKeyType.getString(upd.partitionKey().getKey())))
                         .collect(Collectors.joining("", ""));
{code}
;;;","12/Nov/19 16:30;aleksey;I'd say this is a little insufficient. You have the exact same situation with writing hints in {{HintsBuffer.allocate()}}.

What you want is to add an extra validation downstream all the way to {{ModificationStatement}}, so that you can return a meaningful exception to the client immediately - rather than ending up timing out the response.;;;","12/Nov/19 16:34;aleksey;I would also not bother with listing individual tables; keyspace and partition key should hopefully be sufficient enough, and memoise calculated {{Mutation}} size in the {{Mutation}} object (see {{serializedSize*}} fields in {{Message}} in 4.0) to prevent redundant calculations by subsequent stages.;;;","02/Jan/20 18:50;n.v.harikrishna;[~tpetracca] Would you mind if I submit a patch for this? I ran into this requirement and made some progress on the patch.;;;","06/Jan/20 08:25;n.v.harikrishna;[~jrwest] Made a patch with the changes. [~tpetracca] apologies if I am overtaking you.

Here is the [patch|https://github.com/nvharikrishna/cassandra/commit/5b3af390ce64860505dfeb3a3549cc9897987771] and [CI|https://app.circleci.com/jobs/github/nvharikrishna/cassandra/95].

I have a small question though. In [CommitLog.java|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLog.java#L274], it considers commit log entry overhead with mutation size to compare with max mutation size. Shouldn't it consider only the mutation size? I made changes to be compatible with overhead (I can change based on your comments).

{{SizeHolder}} introduced in this patch can be used in {{Message}} as well. I can make this change too if it is okay to mixup.;;;","06/Jan/20 21:28;jwest;Thanks for picking this up [~n.v.harikrishna]. Comments below. 

 

General:
 * Since this change logs keys which in some use cases may contain PII that users may not want logged, should we add a flag to gate this feature? 
 * I don’t believe this address [~aleksey]'s concern that the exception be propagated to the client vs. timing out. At a minimum, we should add an in-jvm dtest to show it does (I’m happy to help contribute this unless you’d like to). Its possible to split the patch into two parts (one that changes the logging and one that propagates the exception), however, in prod I’ve seen the lack of this logging and the timeouts be an issue so they are definitely both worth addressing. 

CommitLog:
 * With your change to validate the size using serializedSize, it looks like its no longer be necessary to use the scratch buffer (which seems to be used primarily to get the length w/o calculating serialized size). However, I wonder if the original approach is better performance wise since we are already serializing the mutation. It would be good to benchmark workloads with all mutations exceeding max size, no mutations exceeding max size, and mixed to check because I can imagine how what I said is wrong as well. We should check if there is any difference or if its negligible. 

SizeHolder:
 * Since the only call to {{#getSize()}} currently is {{IMutation#validateSize}} which is only called once from CommitLog.java. Consider removing the memoization. 
 * If the memoization is still desired, I don’t think its necessary to have fields for each version since a mutation is executed within a single version of Cassandra (it may cross network boundaries and thus versions but at this point we have a new SizeHolder). Using a single variable would cleanup {{#getSize}} considerably. Further, it would be cleaner/less error prone to memoize the size in the mutation since nothing maps the object passed to {{#getSize}} to the stored value. 

MutationExceededMaxSizeException:
 * {{prepareMessage}} recalculates the mutation size. While this is an exceptional case it is still common. Perhaps a place where the memoization could be helpful? 
 * Spelling error: maxMutaionSize
 * I almost always prefer a configurable limit to something hardcoded and requiring recompilation to change 

Other:
 * I don’t think {{IMutation.getMaxMutationSize()}} is necessary. Use {{DatabaseDescriptor}} instead. This is more typical of Cassandra config code. Further, the original code only called {{DatabaseDescriptor.getMaxMutationSize()}} once instead of for every write. Unless we are going to make it a hot-prop its worth putting back to how it was. 
 * We have a few {{Serializer}} interfaces in C* already, consider renaming {{SizeHolder.Serializer}} if its kept;;;","07/Jan/20 12:51;aleksey;For what it's worth, I would strongly prefer the memoised fields to be inlined into {{Mutation}}, same way they are in {{Message}}. It's a very common object, and the bloat of the overhead of an extra {{SizeHolder}} object vs. just the 3 inlined fields matters.

We do want the memoisation itself, as otherwise we would be performing this calculation at least twice - once when validation, once when calculating message size for internode.;;;","07/Jan/20 18:53;n.v.harikrishna;{quote}Since the only call to #getSize() currently is IMutation#validateSize which is only called once from CommitLog.java. Consider removing the memoization.
{quote}
Size is validated at org.apache.cassandra.service.reads.repair.BlockingReadRepairs#createRepairMutation method too. It may use different version for serialization based on destination. I feel memoization is required. Somehow I missed to include changes for this method in the patch. Including this change while addressing other review comments. Will update the patch asap.;;;","07/Jan/20 19:26;jwest;Thanks [~n.v.harikrishna] . My comment re: memoization was more to give an option if we didn't need it. Since we do (based on my later comments, your comments, and [~aleksey] comments), I agree w/ [~aleksey] suggestion re: removing {{SizeHolder}}. ;;;","08/Jan/20 19:18;n.v.harikrishna;[~jrwest] Made the changes. 

Here is the [updated patch|https://github.com/nvharikrishna/cassandra/commit/1eb9a9846187f669516c88c85fa3550e4efb08f7] and [CI|https://app.circleci.com/jobs/github/nvharikrishna/cassandra/120]. 

Summary of changes:
 * Removed SizeHolder
 * MutationExceededMaxSizeException
 ** Avoided calculating size again.
 ** Changed constant for limiting no.of keys to size of the message. We are mostly concerned about dumping huge message to log. No.of keys to log has to vary based on its size and there won't be an ideal config by no.of keys. So changed it to message size (1kb for now, we can increase it further).
 * IMutation
 ** removed getMaxMutationSize and replaced it with constant from CommitLog. 
 * Replaced Mutation.serializer.serializedSize with mutation.serializedSize.;;;","10/Jan/20 14:24;aleksey;Could you flip the size fields to {{int}} from {{long}}? (not a full review, just a super quick skim related to my only previous point);;;","10/Jan/20 15:46;jwest;A few code review comments below. I did want to discuss if we are going to address the user facing concerns Aleksey brought up in this ticket? The patch addresses the operators lack of visibility into keyspace/table/partitions but still results in timeouts for the user. Are we going to address those in a separate ticket? My thought is that something for the operators is better than no patch (having been blind in this situation before besides custom tools) but if the user facing changes require protocol changes we should probably fix it pre-4.0 like we have or plan to w/ other similar tickets – but that could still be in a separate ticket.

 

Code Comments:  
 * -{{Mutation#serializedSize}}: you should only need one field to memoize the size and can you pass version directly to {{Serializer#serializedSize}} instead of the switch afterwards?- never mind [~n.v.harikrishna] pointed out code in {{BlockingReadRepairs}} that makes this statement false. 
 * We also shouldn’t duplicate the implementations between counter and regular mutations
 * {{validateSize}}: since the two implementations are identical you could move them to a \{{default}} implementation in {{IMutation}}
 * {\{MaxMutationExceededException}}: the sort in {{#prepareMessage}} could get pretty expensive, is it necessary? It also looks like there is an edge case where “and more” will be added even when there aren’t more. Using {{listIterator.hasNext()}} instead of {{topPartitions.size() > 0}} should fix that;;;","14/Jan/20 20:19;n.v.harikrishna;[~jrwest]
{quote}A few code review comments below. I did want to discuss if we are going to address the user facing concerns Aleksey brought up in this ticket? The patch addresses the operators lack of visibility into keyspace/table/partitions but still results in timeouts for the user. Are we going to address those in a separate ticket? My thought is that something for the operators is better than no patch (having been blind in this situation before besides custom tools) but if the user facing changes require protocol changes we should probably fix it pre-4.0 like we have or plan to w/ other similar tickets – but that could still be in a separate ticket.
{quote}
I would prefer to have a separate ticket. +1 on having something better than no patch.
{quote}
* We also shouldn’t duplicate the implementations between counter and regular mutations
* validateSize: since the two implementations are identical you could move them to a default implementation in IMutation
{quote}
validateSize methods implementation looks similar, but they use different serialisers (Mutation uses MutationSerializer and CounterMutation uses CounterMutationSerializer) which are not visible in IMutation interface. serializedSize() methods needs memoization (needs serializedSize* fields) be cause of which we cannot move to interface as fields will be final. We had ruled out option having a separate class (i.e. SizeHolder). Now it makes me think about 2 options.

1. I could not find any validation of size for {{CounterMutation}}. If it is expected or not required, then can remove {{CounterMutaiton}} changes and use\{{ Mutation.validateSize}} directly (instead of defining it in {{IMutation}}). The disadvantage I see with this approach is caller has to be aware of implementation and it makes things hard to abstract (code has to be aware of implementation instead of {{IMutation}}).

2. Expect {{VirtualMutaiton}}, I see mutations are expected to be serialized and/or deserialized. Provide serialize, serialziedSize and deserialize methods as part of {{IMutaiton}} (so that we can abstract out direct usages of {{Mutation.serializer}} and {{CounterMutation.serializer}}) with an abstract class in between having common functionality.

Or else pay the price of duplicate code. What do you think?
{quote}MaxMutationExceededException: the sort in #prepareMessage could get pretty expensive, is it necessary?
{quote}
In Mutation, I see that there is only one PartitionUpdate per Table, and according to Mutation.merge() logic, a mutation can have changes related to only one keyspace and one key. Even if there are multiple updates for different rows of same partition, they are merged into single PartitionUpdate.

When I ran a small test for sorting list of Longs (laptop having i7, 6 core and 16gb ram) it took approximately 33ms, 6ms and 1ms for 100K, 10k and 1k respectively.

According to merge logic and test numbers, unless there are thousands of tables in a key space and trying to update all of them at once, I dont see a scenario where sorting can hurt (time taken for sort > 1 or 2ms).
{quote}It also looks like there is an edge case where “and more” will be added even when there aren’t more. Using listIterator.hasNext() instead of topPartitions.size() > 0 should fix that
{quote}
I had moved the code into separte funtion and added unit test cases. It is working as expected. Using listIterator.hasNext() caused few tests to fail. Did I miss any scenario to test?

Converted serializedSize* long fields to int as suggested by Aleksey. Changes are here: https://github.com/apache/cassandra/compare/trunk...nvharikrishna:14781-trunk?expand=1

 ;;;","17/Jan/20 15:52;jwest;Thanks [~n.v.harikrishna]. I think the duplicate code is probably simpler than making things more nuanced and thanks for testing the sorting performance. The code looks ok me and I am ok w/ the separate ticket approach so that there is some improvements for operators at a minimum but I would like to get [~aleksey]'s input as well. ;;;","22/Jan/20 14:04;aleksey;I'm perfectly fine with this level of duplication. Don't mind a separate ticket, either. Although FWIW that other ticket is the important one - we should never ever get to the point when an oversized mutation makes it to the commitlog at all, outside of potentially boundary mixed mode conditions - when a mutation validates for the current messaging version but ends up being slightly over the size for legacy.;;;","23/Mar/20 21:22;e.dimitrova;Was additional ticket opened?
What shall we do with this one?;;;","08/Apr/20 21:35;jwest;I believe this patch is ready (and has one +1) but needs a committer to review. [~n.v.harikrishna] was another ticket opened? ;;;","20/Apr/20 05:51;n.v.harikrishna;[~jrwest] raised CASSANDRA-15741 for validation and/or fixing client timeout when mutation exceeds max size.;;;","30/Apr/20 14:39;jwest;Hi [~n.v.harikrishna]. I've picked this back up and am getting it ready to commit. Thanks for your patience. 

 

I've squashed your branch here: [https://github.com/jrwest/cassandra/commits/14781-trunk.] I made a few minor changes along the way (I also re-reviewed since it had been a little bit since I had read the patch):

 
 * Modified {{CHANGES.txt}}
 * Modified {{IMutation#validateSize}} javadoc
 * Moved call to {{Keyspace.open}} into the catch block of {{BlockingReadRepairs#createRepairMutation}}. It was only used if we reached that block anyways.
 * Fixed whitespace formatting in {{MutationExceededMaxSizeException#prepareMessage}}

 

I ran a build prior to these changes. The build looked good (better than trunk actually) and any failures do not seem related: [https://app.circleci.com/pipelines/github/jrwest/cassandra/4/workflows/e43918eb-40d2-45ad-80c3-dbeaa5ee186b]

 

I've kicked off a new build with the changes above and with the squash performed: [https://app.circleci.com/pipelines/github/jrwest/cassandra/6/workflows/3c3f674e-db89-488a-bbd5-98f04de4fd0d]

EDIT:

I was slightly concerned about the failure in {{read_repair_test.py}}'s {{test_speculative_data_request}}. Looking closer at the test runs, its flaky and doesn't look like that flakiness could be related to the changes here (since the mutation sizes are static). 

 

I've also kicked off a Jenkins build for good measure: [https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/107/] 

 

 ;;;","04/May/20 18:20;jwest;Tests looked good. Any failures are suspected flaky tests and are unrelated. Committed as d3dadcd6f3bbde471e972f8332eb62de0f2d4aae. ;;;","05/May/20 16:57;n.v.harikrishna;Thanks a lot [~jwest] for taking it forward!!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid creating empty compaction tasks after truncate,CASSANDRA-14780,13186656,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,marcuse,marcuse,marcuse,21/Sep/18 16:36,15/May/20 08:00,13/Jul/23 08:37,21/Sep/18 18:04,4.0,4.0-alpha1,,,,,Local/Compaction,,,,0,,,,There is a chance we create empty {{RepairFinishedCompactionTask}} after a table is truncated,,bdeggleston,jeromatron,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,Normal,Adhoc Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 21 18:04:25 UTC 2018,,,,,,,,,,,"0|i3ycw7:",9223372036854775807,,,,,,,,,bdeggleston,,bdeggleston,,,Low,,,,,,,,,,,,,,,,,,,"21/Sep/18 16:41;marcuse;https://github.com/krummas/cassandra/commits/marcuse/14780
tests: https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2F14780;;;","21/Sep/18 16:51;bdeggleston;+1 on successful circle run;;;","21/Sep/18 18:04;bdeggleston;committed to trunk as [44cffc0b16a1b55f26996d9aee2d3ffa63bb0512|https://github.com/apache/cassandra/commit/44cffc0b16a1b55f26996d9aee2d3ffa63bb0512], thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Overflow of 32-bit integer during compaction.,CASSANDRA-14773,13186592,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,fcofdezc,vladimir.bukhtoyarov,vladimir.bukhtoyarov,21/Sep/18 10:58,21/Dec/20 08:08,13/Jul/23 08:37,09/Apr/20 13:23,4.0,4.0-alpha4,,,,,Local/Compaction,,,,0,pull-request-available,,,"In scope of CASSANDRA-13444 the compaction was significantly improved from CPU and memory perspective. Hovewer this improvement introduces the bug in rounding. When rounding the expriration time which is close to  *Cell.MAX_DELETION_TIME*(it is just *Integer.MAX_VALUE*) the math overflow happens(because in scope of -CASSANDRA-13444-) data type for point was changed from Long to Integer in order to reduce memory footprint), as result point became negative and acts as silent poison for internal structures of StreamingTombstoneHistogramBuilder like *DistanceHolder* and *DataHolder*. Then depending of point intervals:
 * The TombstoneHistogram produces wrong values when interval of points is less then binSize, it is not critical.
 * Compaction crashes with ArrayIndexOutOfBoundsException if amount of point intervals is great then  binSize, this case is very critical.

 

This is pull request [https://github.com/apache/cassandra/pull/273] that reproduces the issue and provides the fix. 

 

The stacktrace when running(on codebase without fix) *testMathOverflowDuringRoundingOfLargeTimestamp* without -ea JVM flag
{noformat}
java.lang.ArrayIndexOutOfBoundsException
at java.lang.System.arraycopy(Native Method)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder$DistanceHolder.add(StreamingTombstoneHistogramBuilder.java:208)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder.flushValue(StreamingTombstoneHistogramBuilder.java:140)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder$$Lambda$1/1967205423.consume(Unknown Source)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder$Spool.forEach(StreamingTombstoneHistogramBuilder.java:574)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder.flushHistogram(StreamingTombstoneHistogramBuilder.java:124)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder.build(StreamingTombstoneHistogramBuilder.java:184)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilderTest.testMathOverflowDuringRoundingOfLargeTimestamp(StreamingTombstoneHistogramBuilderTest.java:183)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:497)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41)
at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173)
at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
at org.junit.runners.ParentRunner.run(ParentRunner.java:220)
at org.junit.runner.JUnitCore.run(JUnitCore.java:159)
at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)

{noformat}
 

The stacktrace when running(on codebase without fix)  *testMathOverflowDuringRoundingOfLargeTimestamp* with enabled -ea JVM flag
{noformat}
ava.lang.AssertionError: point2 should follow point1

at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder$DataHolder.merge(StreamingTombstoneHistogramBuilder.java:324)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder.mergeBin(StreamingTombstoneHistogramBuilder.java:158)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder.flushValue(StreamingTombstoneHistogramBuilder.java:145)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder$$Lambda$1/1967205423.consume(Unknown Source)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder$Spool.forEach(StreamingTombstoneHistogramBuilder.java:574)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder.flushHistogram(StreamingTombstoneHistogramBuilder.java:124)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder.build(StreamingTombstoneHistogramBuilder.java:184)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilderTest.testMathOverflowDuringRoundingOfLargeTimestamp(StreamingTombstoneHistogramBuilderTest.java:183)

{noformat}
 ",,benedict,blerer,cscotta,csplinter,e.dimitrova,fcofdezc,jasonstack,jeromatron,jjirsa,KurtG,shaurya10000,snazy,stillalex,tcooke,vladimir.bukhtoyarov,zznate,,,,,,,,,,"fcofdez commented on pull request #491: CASSANDRA-14773: Fix overflows on StreamingTombstoneHistogramBuilder produced by large deletion times.
URL: https://github.com/apache/cassandra/pull/491
 
 
   This patch also simplifies the underlying structures of StreamingTombstoneHistogramBuilder.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Mar/20 14:34;githubbot;600","fcofdez closed pull request #491:
URL: https://github.com/apache/cassandra/pull/491


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/May/20 08:13;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,CASSANDRA-14775,,,CASSANDRA-13444,,,CASSANDRA-14775,,,,,,,,,,,,,,,,,,,,,,,,,0.0,fcofdezc,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 09 13:23:40 UTC 2020,,,,,,,,,,,"0|i3ychz:",9223372036854775807,,,,,,,,,benedict,,blerer,snazy,,Critical,,4.0-alpha,4.0-alpha1,,https://github.com/apache/cassandra/commit/00fb6d76d0a97af06ba27c1180d6dcddfa337fea,,,,,,,,,Unit tests,,,,,"21/Sep/18 11:16;vladimir.bukhtoyarov;Feel free to ask any questions about StreamingTombstoneHistogramBuilder, I spent a four days in attempt to understand how it works, and looks like I have achieved full understanding about each line of code of this class.;;;","21/Sep/18 11:40;benedict;Hi [~vladimir.bukhtoyarov].  Thanks for the bug report and patch.

This doesn't look like the correct way to saturate this addition, however.  We should ensure that we never go past {{MAX_DELETION}}; by saturating to {{NO_DELETION}}, we are effectively removing the TTL.;;;","21/Sep/18 11:48;vladimir.bukhtoyarov;[~benedict],

 

no problem, I will replace Cell.NO_DELETION by max possible value, please wait a few minutes;;;","21/Sep/18 12:02;vladimir.bukhtoyarov;Request has been updated according your note.;;;","21/Sep/18 12:17;benedict;Thanks [~vladimir.bukhtoyarov]

A couple of points:
# Since {{NO_DELETION_TIME}} > {{MAX_DELETION_TIME}}, we need to check {{if (point < 0 || point == Cell.NO_DELETION_TIME)}}, or perhaps more simply {{if (point + 1 <= 0)}} - though we would need a nice comment explaining the {{+ 1}} in this case
# I'm not sure we need to compute the max rounded deletion time - since this is an optimisation to minimise the number of buckets and merges, I think we'd be OK straightforwardly saturating to {{Cell.MAX_DELETION_TIME}}

It might be nice also to assert that we are never provided a value equal to {{Cell.NO_DELETION_TIME}}, since this *should* be impossible and would be a bug (both for it to appear in the stream, and also in how this method would behave);;;","21/Sep/18 12:57;vladimir.bukhtoyarov;{quote}or perhaps more simply {{if (point + 1 <= 0)}}
{quote}
the code become too hard for understanding, so I decided to write two independent if statements(each marked by independent comment that describes the reason of choosing Cell.MAX_DELETION_TIME

 

Also I fixed the case when result of rounding will be {{Cell.NO_DELETION_TIME}};;;","21/Sep/18 13:02;vladimir.bukhtoyarov;[~benedict],

according to assertion for incoming data. Are you sure that it would be better to just add assert statements  instead of throwing IllegalArgumentException when client of StreamingTombstoneHistogramBuilder tries to pass Cell.NO_DELETION_TIME? Usually *-ea* flag is turned off on production, so the problem will not be detected.;;;","21/Sep/18 15:11;benedict;You should probably be testing {{if (point > Cell.MAX_DELETION_TIME)}} - your first clause right now is a no-op.

I don't really mind if it's an assertion or an exception, but it's fairly normal to run C* with exceptions enabled, and as a project we perform a lot of checks through assertions (perhaps too many).  In this case, it seems more an exception than not, to me; we seem to always be ensuring this never happens already.

While we're here, in {{merge}}, it might be worth ensuring that we cast to {{long}} the denominator for the {{newPoint}} calculation too.

;;;","21/Sep/18 16:22;vladimir.bukhtoyarov;Ok, I have added assert for incoming values and several unit tests.

 

According overflow on merging two points;

When x1,x2,y1,y1<=Integer.MAX_VALUE the expression *(x1*y1 + x2*y2)/(y1+y2)* can lead to overflow of 64-bit long there  *(x1*y1 + x2*y2)*.

However if rewrite the expression to *x1*y1/(y1+y2) + x2*y2/(y1+y2)* then overflow will never happen. What do you think about this fix?;;;","21/Sep/18 16:25;benedict;It's OK to overflow to {{long}}, the issue is that the {{y1+y2}} remain integers until they are added; we should cast one of them to a {{long}} upfront, so that they do not overflow before dividing the numerator.;;;","21/Sep/18 16:25;benedict;Have you pushed your latest changes?  I still see the issue with the first clause of the if statement.;;;","21/Sep/18 16:31;vladimir.bukhtoyarov;Yes, I have pushed all that I want to be included.;;;","21/Sep/18 17:37;benedict;I've left a couple of comments on your pull request.;;;","21/Sep/18 19:13;vladimir.bukhtoyarov;I have fixed all notes on request, also new test-case testThatPointIsNotMissedBecauseOfRoundingToNoDeletionTime has been added.{color:#ffc66d}
{color};;;","25/Sep/18 05:39;vladimir.bukhtoyarov;Is any action required from me?;;;","25/Sep/18 11:19;benedict;No, thanks [~vladimir.bukhtoyarov].  I think it's complete.  I just need to find time to run it through CI and do a final review before committing.;;;","18/Nov/18 03:48;cscotta;Marking ""Patch Available on behalf of [~vladimir.bukhtoyarov]:  [https://github.com/apache/cassandra/pull/273];;;","22/Nov/18 16:25;benedict;[~vladimir.bukhtoyarov]: very sorry for letting this languish for so long.

This breaks some unit tests - these seem to be issues with the tests, but I wonder if you would be willing to take a look?  If not, I will find some time next week to resolve them.

https://circleci.com/gh/belliottsmith/cassandra/981#tests/containers/14;;;","27/Nov/18 12:09;vladimir.bukhtoyarov;[~benedict] from quick look failures should not be related to this pull request, I made this conclusion by analyzing test names. I can take detailed look at next week.;;;","27/Nov/18 12:11;benedict;They are related to this change - several of the tests at least are supplying negative local deletion times in order to produce corrupt range tombstones.  This is very easily argued to be a bug with the test, but we can't commit this with test breakages;;;","09/Apr/19 12:19;snazy;TBH, I doubt that the changes in the proposed PR are enough to fix the existing issues in {{StreamingTombstoneHistogramBuilder}}. Some comments:
 * Nit: the method {{roundKey}} should be called {{ceilKey}}
 * The overall benefit of {{DistanceHolder}} is probably outweighed by the overhead to manage this structure. Maybe it's better to revert to the previous way (iterate to find the shortest distance). Removing {{DistanceHolder}} would also simplify the code a lot.
 * The calculation of {{sum}} in {{merge()}} is prone to an int overflow. Better make {{sum}} a {{long}} and guard the calculation of {{newPoint}} with {{Math.toExactInt()}}.
 * {{wrap}} should take {{value}} as a {{long}} and prevent getting a negative result (int overflow).
 * The code sequence {{boxed().map()}} found in the class can be simplified to {{mapToObj()}}
 * The calculations in {{roundKey}} should use {{long}}s and use a saturating cast (limit return values to 0..Cell.MAX_DELETION_TIME)
 * Deserialization in TombstoneHistogram is also prone to int overflows and should use a saturating cast (see CASSANDRA-14092)
 * {{tryCell}} should also not just blindly do a {{+=}} but a saturating cast of the old value + delta
 * Test cases for all the possible int overflows are missing

 ;;;","09/Apr/19 12:24;benedict;[~snazy] see also CASSANDRA-14775, which necessitates slightly more fundamental revisiting of the new structure IMO, during which I hoped we would address these wider issues.  Certainly appreciate you pointing some of them out here, though.;;;","13/Jan/20 17:02;benedict;FYI, I deliberately unassigned this to help track the likely need for another contributor to complete the work.;;;","03/Feb/20 15:52;e.dimitrova;Hi [~vladimir.bukhtoyarov] and [~benedict],

Maybe I can contribute to this one? As far as I see from the latest 2019 updates failing tests had to be fixed at first?

[~vladimir.bukhtoyarov] are you still working on this one?

 

 ;;;","03/Feb/20 18:20;vladimir.bukhtoyarov;Hi [~e.dimitrova] 

No, I am not working on this.;;;","26/Mar/20 14:37;fcofdezc;I've pushed a patch following the same ideas proposed originally and I've simplified the code as well. I think we're covering all overflows now.

[https://github.com/apache/cassandra/pull/491]

 

[~blerer] [~snazy] can you take a look? ;;;","06/Apr/20 09:12;fcofdezc;I've updated the patch including more test coverage.;;;","09/Apr/20 12:59;blerer;Jenkins test failures seems unrelated [Jenkins|https://ci-cassandra.apache.org/job/Cassandra-devbranch/31/]
and the circleCI test looks good.;;;","09/Apr/20 13:23;blerer;Committed into trunk at 00fb6d76d0a97af06ba27c1180d6dcddfa337fea;;;","09/Apr/20 13:23;blerer;Thanks for the patch [~fcofdezc];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix issues in audit / full query log interactions,CASSANDRA-14772,13186565,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,21/Sep/18 08:26,21/Dec/20 08:08,13/Jul/23 08:37,16/Apr/20 12:21,4.0,4.0-beta1,,,,,Legacy/CQL,Legacy/Tools,,,0,,,,"There are some problems with the audit + full query log code that need to be resolved before 4.0 is released:
* Fix performance regression in FQL that makes it less usable than it should be.
* move full query log specific code to a separate package 
* do some audit log class renames (I keep reading {{BinLogAuditLogger}} vs {{BinAuditLogger}} wrong for example)
* avoid parsing the CQL queries twice in {{QueryMessage}} when audit log is enabled.
* add a new tool to dump audit logs (ie, let fqltool be full query log specific). fqltool crashes when pointed to them.
",,6172161023@student.chula.ac.th,aleksey,benedict,csplinter,djoshi,eperott,jeromatron,jjordan,JoshuaMcKenzie,jwest,marcuse,samt,vinaykumarcse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14885,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 16 12:21:31 UTC 2020,,,,,,,,,,,"0|i3ycbz:",9223372036854775807,,,,,,,,,,,aleksey,eperott,,Normal,,4.0-alpha,4.0-alpha1,,https://github.com/apache/cassandra/commit/5242f7d4306f034f32e26834df06ff7fc85a3c16,,,,,,,,,unit tests + existing dtests,,,,,"21/Sep/18 13:23;eperott;bq. add a new tool to dump audit logs (ie, let fqltool be full query log specific)

ATM fqltool isn't compatible with BinLog audit logs:
{noformat}
$ ./tools/bin/fqltool dump ~/.ccm/trunk/node1/audit/
error: null
-- StackTrace --
java.lang.NullPointerException
	at io.netty.buffer.Unpooled.wrappedBuffer(Unpooled.java:155)
	at org.apache.cassandra.fqltool.commands.Dump.lambda$dump$0(Dump.java:96)
	at net.openhft.chronicle.queue.impl.single.SingleChronicleQueueExcerpts$StoreTailer.readDocument(SingleChronicleQueueExcerpts.java:1099)
	at org.apache.cassandra.fqltool.commands.Dump.dump(Dump.java:137)
	at org.apache.cassandra.fqltool.commands.Dump.run(Dump.java:66)
	at org.apache.cassandra.fqltool.FullQueryLogTool.main(FullQueryLogTool.java:65)
{noformat}

Seem to be broken since CASSANDRA-14675.

Not sure if I should create a separate ticket for this, or if we resolve it within the scope of this ticket.
;;;","21/Sep/18 13:34;marcuse;[~eperott] yeah I was planning on fixing it here;;;","21/Sep/18 15:22;aleksey;[~eperott] Yeah. It was very odd to find audit-log print-out code hidden in an if-else branch of a tool called {{fqltool dump}}, so I removed it - with an expectation that the missing functionality would be added *properly* at some point later. A bit surprised it's throwing an NPE though - I think it's supposed to throw an {{UnsupportedOperationException}} there, but it looks like Marcus is on it here.;;;","24/Sep/18 14:02;JoshuaMcKenzie;I wouldn't consider a refactor to be a bug-fix. While there's a small minority of this ticket that's defect-related, I think committing a refactor post-freeze date for 4.0 is not good hygiene.

Am I missing something here?;;;","24/Sep/18 14:06;marcuse;the bug part would be that fqltool dump throws npe, but not critical I guess, I'll mark it 4.0.x;;;","24/Sep/18 14:20;aleksey;It's a follow-up to a feature introduced in an unreleased version (4.0). There are some issues with it that need to be addressed, including a performance regression in FQL that makes it less usable than it should be.

So 4.0 is a fitting fixver for at least part of this JIRA, yes. Additionally, the API will be harder to change post-stable 4.0, but arguably it needs to. So again, 4.0 is the correct fixver here.;;;","24/Sep/18 14:31;JoshuaMcKenzie;Having not looked at the code, is there value in moving the API changes and defect fixes to their own ticket targeting 4.0 and doing the other parts 4.0.x (black box refactors, etc)?

I'm mostly coming at this from a ""broken windows theory"" angle of us having discipline w/the concept of a code-freeze. Don't want us to get into the habit of sneaking things in past the date, which then further risks destabilizing the testing period.;;;","24/Sep/18 14:39;jjordan;If this is not a refactor ticket, but a fix problems ticket, then the title and description should state that. I have updated them with my understanding. Feel free to update them further with all the issues to be fixed here that make this ticket needed for 4.0 while we are in a code freeze.;;;","24/Sep/18 14:44;aleksey;This is too narrow of an interpretation of a freeze.

What can't and shouldn't go in is new features (that are not testing related).

Sometimes you will have issues with code that made it in before the freeze that needs to be addressed, and doing a full-on revert is too drastic a response.

Sometimes you'll have small features that are required strictly for testing purposes - think logging extra information in FQL to enable replay testing, or returning extra metadata from the server to enable cluster fuzz testing and validation.

Certain cleanup and maintenance also easily falls under stabilisation umbrella.

Again, you have to be reasonable about it and exercise judgement on case-by-case basis.;;;","24/Sep/18 17:35;JoshuaMcKenzie;Opinion presented as fact ftw. :)

There are a variety of ways to interpret ""freeze"", and we could (and probably should) take that debate/discussion to the ML.

What I will say here: refactoring code almost always introduces new defects while fixing old ones; this is a lesson we should all know all too well on this project.;;;","24/Sep/18 18:32;djoshi;{quote}What I will say here: refactoring code almost always introduces new defects while fixing old ones; this is a lesson we should all know all too well on this project.
{quote}
By that logic, we should not change any code ;) Exercising caution and good judgment in refactors is important. I think all the bullets that [~krummas] listed seem to be reasonable. See specifics below.

 
{quote}Fix performance regression in FQL that makes it less usable than it should be.
{quote}
Bug
{quote}move full query log specific code to a separate package
{quote}
Low risk change
  
{quote}do some audit log class renames (I keep reading {{BinLogAuditLogger}} vs {{BinAuditLogger}} wrong for example)
{quote}
Low risk change
  
{quote}avoid parsing the CQL queries twice in {{QueryMessage}} when audit log is enabled.
{quote}
Bug
  
{quote}add a new tool to dump audit logs (ie, let fqltool be full query log specific). fqltool crashes when pointed to them.
{quote}
This is avoiding changes to a public API (I consider tools as Public API) in the next release. From a user's standpoint it's a good thing to have.;;;","24/Sep/18 19:05;benedict;bq. Opinion presented as fact ftw.

I've looked up [the original discussion thread|https://lists.apache.org/thread.html/61f29a67b4570dac87047f64b846c6f83e8bcb39b6b67143c9a485c2@%3Cdev.cassandra.apache.org%3E], and the predominant phrasing was ""feature freeze"" which seems fairly reasonable to interpret _factually_ as a freeze on features, not simply as an opinion.;;;","24/Sep/18 19:21;jwest;[~JoshuaMcKenzie] I appreciate you ensuring we keep to the freeze. I agree that once we start sneaking things past we open pandora’s box and make the freeze much less valuable. My understanding is that we agreed to address exceptions on a case-by-case basis, which we seem to be doing. We don’t have a formal process for doing so but a good litmus test imo is will the change improve the stability (where stability means performance and correctness) of Cassandra 4.0 and at what cost/risk does that improvement come with. I had more written on using that test here but I think folks have already summarized how this change improves stability with low risk — in particular if the refactors are fully automated by a tested tool like IntelliJ.;;;","24/Sep/18 19:29;JoshuaMcKenzie;{quote}I've looked up [the original discussion thread|https://lists.apache.org/thread.html/61f29a67b4570dac87047f64b846c6f83e8bcb39b6b67143c9a485c2@%3Cdev.cassandra.apache.org%3E], and the predominant phrasing was ""feature freeze""
{quote}
Ah hah - there's our disconnect. I incorrectly recalled us discussing it as a *code freeze*, not a *feature freeze*.
 Two very different things. Refactor away to your heart's content, I won't fight it. :)

And fwiw - I'm in no way advocating for us to never refactor code; just take a look at the CommitLogReader during the CDC impl time frame. I'm against refactors post *code freeze*, which is irrelevant given the earlier misunderstanding.

Carry on, sorry for the unnecessary traffic.;;;","24/Sep/18 19:39;benedict;Also, FWIW, I'm completely in favour of clarifying further for future releases; perhaps having stricter, well defined stages of freeze, culminating in a total code freeze.  If we had this, there probably wouldn't have been this confusion (and I think there's been a lot of that)

This release cycle is a bit of an experiment IMO, and we should definitely iterate on it for next time.  Let's collect our observations and opinions for how we can improve it, and revisit as a community once we have a complete picture.;;;","03/Apr/19 19:42;eperott;I'd like to add one improvement to the original list in this ticket.

Currently the FQL have a record header which indicate {{version}} and {{type}} of record. The record header for audit logs only a {{type}}. Would it make sense to add a {{version}} field to the audit records as well so that the format can evolve without breaking the tools.

This would also make it possible for the {{auditlogviewer}} and the {{fqltool}} commands to give more descriptive error messages if the wrong tool is used.;;;","03/Apr/19 21:26;vinaykumarcse;Sounds reasonable to me [~eperott], please open a separate Jira, so that we can discuss more on this idea and take it forward. ;;;","04/Apr/19 06:31;marcuse;yep, sounds good, but this ticket is just for restructuring the code, not changing the formats, so, as [~vinaykumarcse] says, a separate JIRA would be good;;;","04/Apr/19 07:01;eperott;Created CASSANDRA-15076 to discuss record header format.;;;","11/Jun/19 08:57;marcuse;Patch [here|https://github.com/apache/cassandra/compare/trunk...krummas:marcuse/14772?expand=1] and tests running [here|https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2F14772]

* Separates full query logging and audit logging - fql and audit logging now have no dependencies on each other, but both use {{BinLog}} to write log events to disk
* Introduces {{QueryEvents}} + {{AuthEvents}} where it is possible to register listeners for these events
* {{AuditLogManager}} and {{FullQueryLogger}} are now getting log events using these listeners
* Introduces a small change in the {{QueryHandler}} interface - the {{process}} method now takes a parsed {{CQLStatement}} - this was done to avoid re-parsing the statement when logging and a {{parse}} method was added to create this {{CQLStatement}}
* Move the {{enableFullQueryLogger}} etc methods from {{StorageProxy}} to {{StorageService}}
;;;","16/Jun/19 17:32;eperott;Had a look at your patch [~krummas].

Overall it looks good to me. In particular I like the way things are more decoupled with the {{QueryEvents}}/{{AuthEvents}}. One thing though, that feels a little bit more complicated than necessary is the enable/disable mechanics in {{AuditLogManager}}. There is:
* the {{enabled}} flag
* the {{auditLogger}} which can be a {{NoOpAuditLogger}} instance or something else
* and now also the registerAsListener()/unregisterAsListener()
Since the listener subscription will make sure that we don't get calls to log() when audit logs are disabled, it should now be possible to drop the {{enable}} flag without performance impact. What do you think?

A few minor comments:
* {{AuditLogManager}} still has a comment referring to FQL.
* {{AuditLogManager.log(AuditLogEntry)}} has a null-check which I believe is unnecessary now. Same in {{AuditLogManager.log(AuditLogEntry, Exception)}}.
* {{AuditLogManager.isAutidLogEnabled}} is still referred to in a comment in the class
* Rename {{QueryEventTest}} to {{QueryEventsTest}
* Start the DTests on CirlceCI
;;;","24/Jun/19 15:14;marcuse;thanks for having a look [~eperott], pushed up fixes to your comments

and tests have run here: https://circleci.com/workflow-run/58e385bb-0298-4136-b58c-3dd83643e774;;;","27/Jun/19 10:27;eperott;+1

I'm not able to reproduce the failing dtest locally.;;;","03/Sep/19 11:03;aleksey;Good stuff.

1. Would prefer if {{QueryEvents#startTime()}} weren’t conditional on listeners size; this, coupled with the {{time > 0}} check conflates the two concerns (current timestamp and presence of listereners)
2. Following up on that, would prefer if the {{time > 0}} check was gone from {{QueryEvents#notify*}} methods; instead, to prevent potential allocation of a capturing lambda with empty listeners, replace {{listeners#forEach()}} calls with for-each loops
3. Minor, but I don’t see why {{getInstance()}} method exists in {{QueryEvents}} and {{AuthEvents}}. It has no value and goes against our guidelines (prefer public final fields to getter methods) and precedent (public static {{INSTANCE}} singletons)
4. {{AuditLogManager#executeFailure()}} - there are to {{toString()}} calls for {{setOperation()}}; Seem to not be intentional to me?
5. {{IAuditLogger#path()}} method should be removed; nit: rename {{enabled()}} to a proper predicate name like {{isEnabled()}}?;;;","19/Nov/19 08:01;marcuse;thanks for the review, pushed 2 new commits fixing the comments

[branch|https://github.com/apache/cassandra/compare/trunk...krummas:marcuse/14772?expand=1]
[tests|https://circleci.com/workflow-run/08cfe810-4deb-40e1-98a6-0f72c6f224de];;;","07/Apr/20 16:26;aleksey;LGTM now, +1;;;","16/Apr/20 12:21;marcuse;committed, thanks!

test runs:
[unit tests|https://circleci.com/gh/krummas/cassandra/3204]
[jvm dtests|https://circleci.com/gh/krummas/cassandra/3205]
[jvm upgrade dtests|https://circleci.com/gh/krummas/cassandra/3209]
[dtests vnodes|https://circleci.com/gh/krummas/cassandra/3206]
[dtests novnodes|https://circleci.com/gh/krummas/cassandra/3207];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DESC order reads can fail to return the last Unfiltered in the partition in a legacy sstable,CASSANDRA-14766,13186036,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,aleksey,aleksey,aleksey,19/Sep/18 11:26,02/Aug/19 02:53,13/Jul/23 08:37,25/Sep/18 16:08,3.0.18,3.11.4,,,,,Local/SSTable,,,,0,,,,"{{OldFormatDeserializer}}’s {{hasNext()}} method can and will consume two {{Unfiltered}} from the underlying iterator in some scenarios - intentionally.

But in doing that it’s losing intermediate state of {{lastConsumedPosition}}. If that last block, when iterating backwards, only has two {{Unfiltered}}, the first one will be returned, and the last one won’t as the reverse iterator would incorrectly things that the deserisalizer is past the index block, despite still having one {{Unfiltered}} unreturned.",,aleksey,benedict,cscotta,jjirsa,KurtG,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,Correctness -> Transient Incorrect Response,,,,,,,,Challenging,Fuzz Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 25 16:08:13 UTC 2018,,,,,,,,,,,"0|i3y92v:",9223372036854775807,,,,,,,,,,,benedict,samt,,Critical,,3.0.0,,,,,,,,,,,,,,,,,"25/Sep/18 14:12;aleksey;3.0: [code|https://github.com/iamaleksey/cassandra/commits/14766-3.0], [CI|https://circleci.com/workflow-run/b86af556-5675-4a83-8de8-21941175608f]
3.11: [code|https://github.com/iamaleksey/cassandra/commits/14766-3.11], [CI|https://circleci.com/workflow-run/d23b37c9-225a-4e13-85fd-484bda81c37b]
A small dtest for illustration (in addition to a regression unit test in C* repo) can be found [here|https://github.com/iamaleksey/cassandra-dtest/commits/14766].;;;","25/Sep/18 14:30;benedict;one tiny nit: should probably set {{couldBeStartOfPartition = false}} outside of the {{if}} branch, so we don't unnecessarily perform the RT tests.  

It shouldn't have any impact besides possibly some probably immeasurable performance cost, but probably better that way.;;;","25/Sep/18 14:38;samt;+1

The fact that we're now resetting {{couldBeStartOfPartition}} in {{clearState}}, which wasn't done before probably fixes an overlooked edge case in CASSANDRA-13236. We greedily read the static row in {{AbstractSSTableIterator}}'s constructor, where {{isFirst}} ({{couldBeStartOfPartition}} as was) would trigger the swapping in the presence of the required tombstone. {{IndexState::setToBlock}} attempts to skip past the static row if we end up iterating backward to block 0 so we don't re-read the static row, but without resetting this flag it would only skip past the RT, leaving the statics to be read next which would then hit the the error from 13236.;;;","25/Sep/18 14:39;aleksey;Argh. You are right. Not sure how I missed it. Pushed the updates.;;;","25/Sep/18 15:22;benedict;+1;;;","25/Sep/18 16:08;aleksey;Thanks. Committed to 3.0 as [45937def313bbb32024ae890f830e23bcc6ccae5|https://github.com/apache/cassandra/commit/45937def313bbb32024ae890f830e23bcc6ccae5] and merged to 3.11, and with {{-s ours}} into trunk.

Dtest committed as [02c1cd77439b220a09df1d53891441bb80dcf944|https://github.com/apache/cassandra-dtest/commit/02c1cd77439b220a09df1d53891441bb80dcf944].

NOTE: I did fold {{iterator.hasNext()}} check into the if-branch above on commit, to remove one extra level of nesting from the code.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail incremental repair prepare phase if it encounters sstables from un-finalized sessions,CASSANDRA-14763,13185903,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,18/Sep/18 21:00,15/May/20 08:02,13/Jul/23 08:37,21/Sep/18 17:05,4.0,4.0-alpha1,,,,,Consistency/Repair,,,,0,,,,"Raised in CASSANDRA-14685. If we encounter sstables from other IR sessions during an IR prepare phase, we should fail the new session. If we don't, the expectation that all data received before a repair session is consistent when it completes wouldn't always be true.

In more detail: 
We don’t have a foolproof way of determining if a repair session has hung. To prevent hung repair sessions from locking up sstables indefinitely, incremental repair sessions will auto-fail after 24 hours. During this time, the sstables for this session will remain isolated from the rest of the data set. Afterwards, the sstables are moved back into the unrepaired set.
 
During the prepare phase of an incremental repair, we isolate the data to be repaired. However, we ignore other sstables marked pending repair for the same token range. I think the intention here was to prevent a hung repair from locking up incremental repairs for 24 hours without manual intervention. Assuming the session succeeds, it’s data will be moved to repaired. _However the data from a hung session will eventually be moved back to unrepaired._ This means that you can’t use the most recent successful incremental repair as the high water mark for fully repaired data.",,bdeggleston,cscotta,marcuse,mbyrd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,Correctness -> Consistency,,,,,,,,Challenging,Adhoc Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 21 17:05:32 UTC 2018,,,,,,,,,,,"0|i3y89j:",9223372036854775807,,,,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"18/Sep/18 21:47;bdeggleston;[trunk|https://github.com/bdeggleston/cassandra/tree/14763]
[circle|https://circleci.com/workflow-run/03a36dfd-74b9-40e7-9284-b079f931f991];;;","19/Sep/18 08:25;marcuse;a few comments;

* The error message given by the failing nodetool could be a bit better: {{Repair job has failed with the error message: [2018-09-19 10:01:51,386] null}} maybe we could add that the user should have a look in the logs for further details
* a comment about isPending() on the commit on github

wrote a dtest making sure that we throw an exception if this happens: https://github.com/krummas/cassandra-dtest/commits/marcuse/14763

also looks like a few repair dtests needs fixing;;;","19/Sep/18 17:32;bdeggleston;Pushed up fixes and started a new circle run.

Good catch with the race condition. I realized there’s another race where another pending anti-compaction could complete after we’d filtered our sstables and before we locked the sstables, potentially leaking data from one session to another which I fixed in AcquisitionCallback.;;;","20/Sep/18 07:04;marcuse;pushed a few fixes: https://github.com/krummas/cassandra/commits/blake/14763
* re-added the range intersection check
* Iterables.filter is ""lazy"" - we would have to iterate over the sstables returned to populate {{conflictingSessions}}

tests running here: https://circleci.com/gh/krummas/cassandra/tree/blake%2F14763;;;","20/Sep/18 08:38;marcuse;seems repair_tests.py test_dead_coordinator is a legit failure;;;","20/Sep/18 15:59;bdeggleston;Ok dead coordinator test needed to be updated to cancel the hung repair session, that's fixed (and pulled in your dtest changes) here: [https://github.com/bdeggleston/cassandra-dtest/tree/14763]

New run here: [https://circleci.com/workflow-run/9d4564d2-acbb-4c5a-b296-aa7977d66da4]

 

 ;;;","20/Sep/18 17:59;marcuse;LGTM, +1;;;","21/Sep/18 17:05;bdeggleston;Thanks, committed to trunk as [167ebbcf4304512fa538e9cfc18da4295511d16c|https://github.com/apache/cassandra/commit/167ebbcf4304512fa538e9cfc18da4295511d16c];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transient node receives full data requests in dtests,CASSANDRA-14762,13185857,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,benedict,aweisberg,aweisberg,18/Sep/18 17:42,15/May/20 08:01,13/Jul/23 08:37,03/Oct/18 13:29,4.0,4.0-alpha1,,,,,Feature/Transient Replication,,,,0,,,,"I saw this running them on my laptop with rapid write protection disabled. Attached is a patch for disabling rapid write protection in the transient dtests.

{noformat}
.Exception in thread Thread-19:
Traceback (most recent call last):
  File ""/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/Users/aweisberg/repos/cassandra-dtest/venv/src/ccm/ccmlib/cluster.py"", line 180, in run
    self.scan_and_report()
  File ""/Users/aweisberg/repos/cassandra-dtest/venv/src/ccm/ccmlib/cluster.py"", line 173, in scan_and_report
    on_error_call(errordata)
  File ""/Users/aweisberg/repos/cassandra-dtest/dtest_setup.py"", line 137, in _log_error_handler
    pytest.fail(""Error details: \n{message}"".format(message=message))
  File ""/Users/aweisberg/repos/cassandra-dtest/venv/lib/python3.6/site-packages/_pytest/outcomes.py"", line 96, in fail
    raise Failed(msg=msg, pytrace=pytrace)
Failed: Error details:
Errors seen in logs for: node3
node3: ERROR [ReadStage-1] 2018-09-18 12:28:48,344 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-1,5,main]
org.apache.cassandra.exceptions.InvalidRequestException: Attempted to serve transient data request from full node in org.apache.cassandra.db.ReadCommandVerbHandler@3c55e0ff
	at org.apache.cassandra.db.ReadCommandVerbHandler.validateTransientStatus(ReadCommandVerbHandler.java:104)
	at org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:53)
	at org.apache.cassandra.net.MessageDeliveryTask.process(MessageDeliveryTask.java:92)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:54)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162)
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134)
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:110)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
{noformat}",,aweisberg,benedict,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14697,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,benedict,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 03 13:28:53 UTC 2018,,,,,,,,,,,"0|i3y7zb:",9223372036854775807,,,,,,,,,aweisberg,,aweisberg,,,Normal,,,,,,,,,,,,,,,,,,,"20/Sep/18 15:48;benedict;This error message has its parameters the wrong way around; it should read

""Attempted to serve full data request from transient node""

Which is obviously much worse;;;","20/Sep/18 15:53;benedict;I'm not sure why rapid write protection should affect this?;;;","20/Sep/18 16:29;aweisberg;It probably didn't I just didn't find out if it reproduced without it.;;;","20/Sep/18 18:02;benedict;I'm pretty sure the issue here is that, in SRP and AbstractReadRepair, we're not correctly setting the transient flags.  I've got a patch queued in CI that should correct it.;;;","20/Sep/18 22:37;benedict;[4.0|https://github.com/belliottsmith/cassandra/tree/14762] [CI|https://circleci.com/workflow-run/c389a15a-c2f4-4ad7-8ea1-921266d18bf5];;;","25/Sep/18 22:31;aweisberg;[Is this just a clarification?|https://github.com/belliottsmith/cassandra/commit/0f8a736316feded871231e2ad571d53182d33ee2#diff-0920d7430e98bf3c6a3c4cb88056f8f5R293]

Maybe [~bdeggleston] should do a quick review as well since it is a small change. I just want to sanity check that we should be reading from transient replicas in both these cases.

It seems to me we use read repair to assemble the read after digest mismatch between full replicas and that is why we it might send messages to transient replicas? That is what it seems like to me. A minor out of scope improvement would be to use the existing response and not repeat the read?

It seems to me also that we would read from transients as part of short read protection (they are just another member of the group), and they aren't special so we should issue them the query.
;;;","26/Sep/18 08:16;benedict;Thanks for the review.

bq. Is this just a clarification?

Yes; it is a no-effect change.

bq. It seems to me we use read repair to assemble the read after digest mismatch between full replicas and that is why we it might send messages to transient replicas? That is what it seems like to me.
Even though we don't send repair mutations after read-repair from transient replicas, on digest mismatch we still perform reads to other replicas - including those that we may not have contacted initially, which might include new transient nodes.

bq. A minor out of scope improvement would be to use the existing response and not repeat the read?
Agreed, see CASSANDRA-14733.  I considered doing that optimisation for this ticket, but given the above fact that we might issue new transient reads it seemed to unnecessarily complicate fixing this bug.

bq. It seems to me also that we would read from transients as part of short read protection (they are just another member of the group), and they aren't special so we should issue them the query.

Is this simply you reasoning out the rationale for issuing the requests, or have you spotted an issue with the patch that means we are not doing so?

It does look to me like we should be perhaps switching to {{acceptsTransient}} for the local query also, and validating transient status in {{LocalReadRunnable}} - but, for now, we don't do this.  Perhaps we could fix this also in this patch, or otherwise file a follow up.;;;","26/Sep/18 14:59;aweisberg;bq. Is this simply you reasoning out the rationale for issuing the requests, or have you spotted an issue with the patch that means we are not doing so?
Sorry it's just socratic code review. I don't see any problems.

+1

Checking locally is nice, but I'm not sure it is as valuable just because the race is much smaller since it's not O(gossip) it's O(time to switch threads). 

If you want to do it here or in another ticket it's still good to have.;;;","26/Sep/18 16:45;benedict;bq. I'm not sure it is as valuable just because the race is much smaller since it's not O(gossip) it's O(time to switch threads).

I was thinking of programmer error more than the race condition, but I agree it's much less impactful.  I might rustle it up anyway, while we're here.;;;","03/Oct/18 13:28;benedict;I've committed as-is, to [467068d1e9d84e6cca1f9dd5a4eff5f80d027c2e|https://github.com/apache/cassandra/commit/467068d1e9d84e6cca1f9dd5a4eff5f80d027c2e].  

dtests are now very flaky on CircleCI, but all of the failures in the latest run have shown up in trunk runs for me on CircleCI.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transient->Full movements mishandle consistency level upgrade,CASSANDRA-14759,13185829,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,benedict,benedict,benedict,18/Sep/18 15:14,15/May/20 08:01,13/Jul/23 08:37,03/Oct/18 13:50,4.0,4.0-alpha1,,,,,Legacy/Core,,,,0,Availability,correctness,transient-replication,"While we need treat a transitioning node as ‘full’ for writes, so that it can safely begin serving full data requests once it has finished, we cannot maintain it in the ‘pending’ collection else we will also increase our consistency requirements by a node that doesn’t exist.",,aweisberg,benedict,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14697,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,benedict,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 03 13:50:40 UTC 2018,,,,,,,,,,,"0|i3y7t3:",9223372036854775807,,,,,,,,,,,aweisberg,ifesdjeen,,Normal,,,,,,,,,,,,,,,,,,,"18/Sep/18 15:21;benedict;I thought I had already committed this as one of my other patches, but it seems not.  I hope you don't mind both being tagged reviewers, since you've been reviewing my other TR follow ups, but shis bug is a proper Blocker for 4.0.

[PR|https://github.com/apache/cassandra/pull/270] [CI|https://circleci.com/workflow-run/636b2eae-2d9e-4245-ad6b-462eab583f13]

I will follow up with some tests.;;;","18/Sep/18 15:47;aweisberg;I think I understand this. It's fixing a deficiency in write availability where transient -> full would cause that replica to be unavailable for meeting write quorum requirements. Since it would remain present in pending it would also increase group size in blockFor.

{code}
return natural.filter(r -> !r.isTransient() || !pending.contains(r.endpoint(), true));
{code}

Specifically the {{!pending.contains(r.endpoint(), true)}} (use of an enum instead of true/false might be a hair easier to follow for that API) causes it to no longer appear in natural and in pending instead.

Once you have the tests I'll do a final review.;;;","18/Sep/18 15:56;benedict;bq. use of an enum instead of true/false might be a hair easier to follow for that API

-Agreed, I had that thought before too.  I'll update the API.-

Actually, the method is no longer used, so I'll remove it (for now);;;","19/Sep/18 09:06;benedict;I have added a simple test case for this functionality.;;;","20/Sep/18 19:34;aweisberg;+1;;;","24/Sep/18 13:37;ifesdjeen;+1 as logic looks right, just two minor things for discussion:

  * should we get rid of generics {{<E extends Endpoints<E>> E}} there since we only have token writes and these methods are called with {{EndpointsForToken}} only?
  * semantically, wouldn't it be better to leave full pending in pending and filter natural transient out? It might be slightly confusing that natural replica flips from transient to full: it doesn't seem as we have inherent limitations that prevent us from that, or? 
;;;","24/Sep/18 14:25;benedict;bq. semantically, wouldn't it be better to leave full pending in pending and filter natural transient out? It might be slightly confusing that natural replica flips from transient to full: it doesn't seem as we have inherent limitations that prevent us from that, or?

It might be, but unfortunately pending is used - almost exclusively - to calculate how much we should boost our CL blockFor requirements by.  The sufficiency logic is the last bit of this codepath that could do with being refactored, but until we do this, it is probably more dangerous to leave it in the pending collection and risk boosting blockFor accidentally.;;;","24/Sep/18 14:29;benedict;bq. should we get rid of generics <E extends Endpoints<E>> E there since we only have token writes and these methods are called with EndpointsForToken only?

I've made this change, and once I get a clean CI response I'll merge the patch in.  Thanks for the review!;;;","03/Oct/18 13:50;benedict;I've committed as [daa3619ae63bb8b06d532890e51d288c189c787c|https://github.com/apache/cassandra/commit/daa3619ae63bb8b06d532890e51d288c189c787c]

dtests are now very flaky on CircleCI, but all of the failures in the latest run have shown up in trunk runs for me on CircleCI.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Remove ""audit"" entry from .gitignore",CASSANDRA-14758,13185799,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,marcuse,marcuse,marcuse,18/Sep/18 12:53,15/May/20 08:02,13/Jul/23 08:37,18/Sep/18 13:24,4.0,4.0-alpha1,,,,,Build,,,,0,,,,"Seems there was a ""audit"" entry added to the .gitignore file in CASSANDRA-9608, it makes it kind of hard to work with files in the {{org.apache.cassandra.audit}} package",,jasobrown,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 18 13:24:31 UTC 2018,,,,,,,,,,,"0|i3y7mf:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Low,,,,,,,,,,,,,,,,,,,"18/Sep/18 12:56;marcuse;https://github.com/krummas/cassandra/commits/marcuse/14758
;;;","18/Sep/18 13:03;jasobrown;+1;;;","18/Sep/18 13:24;marcuse;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"GCInspector ""Error accessing field of java.nio.Bits"" under java11",CASSANDRA-14757,13185795,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,snazy,jasobrown,jasobrown,18/Sep/18 12:41,15/May/20 08:04,13/Jul/23 08:37,27/Jun/19 12:32,4.0,4.0-alpha1,,,,,Observability/Metrics,,,,0,Java11,pull-request-available,,"Running under java11, {{GCInspector}} throws the following exception:
{noformat}
DEBUG [main] 2018-09-18 05:18:25,905 GCInspector.java:78 - Error accessing field of java.nio.Bits
java.lang.NoSuchFieldException: totalCapacity
        at java.base/java.lang.Class.getDeclaredField(Class.java:2412)
        at org.apache.cassandra.service.GCInspector.<clinit>(GCInspector.java:72)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:308)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:590)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:679)
{noformat}
This is because {{GCInspector}} uses reflection to read the {{totalCapacity}} from {{java.nio.Bits}}. This field was renamed to {{TOTAL_CAPACITY}} somewhere between java8 and java11.

Note: this is a rather harmless error, as we only look at {{Bits.totalCapacity}} for metrics collection on how much direct memory is being used by {{ByteBuffer}}s. If we fail to read the field, we simply return -1 for the metric value.",,jasobrown,jeromatron,mck,snazy,,,,,,,,,,,,,,,,,,,,,,"GitHub user snazy opened a pull request:

    https://github.com/apache/cassandra/pull/272

    CASSANDRA-14757 Fix accessing java.nio.Bits.totalCapacity/TOTAL_CAPACITY and AtomicLong

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/snazy/cassandra 14757-gcinspector-bits

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/272.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #272
    
----
commit e9ac192def04b513cfc7e9b4964f571c87416dee
Author: Robert Stupp <snazy@...>
Date:   2018-09-18T17:05:17Z

    Fix accessing java.nio.Bits.totalCapacity/TOTAL_CAPACITY and AtomicLong

----
;18/Sep/18 17:06;githubbot;600","Github user JeremiahDJordan commented on the issue:

    https://github.com/apache/cassandra/pull/272
  
    Rather than introducing more compiler version dependent classes, why not just catch the java.lang.NoSuchFieldException if it happens and then use the other name?  This only happens once during static initialization.
;17/Oct/18 22:05;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,CASSANDRA-9608,,,,,,,,,,,,,,,,,,,,,,,0.0,mck,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 27 12:32:38 UTC 2019,,,,,,,,,,,"0|i3y7lj:",9223372036854775807,,,,,,,,,,,snazy,,,Low,,4.0,,,https://github.com/apache/cassandra/commit/2ed2b87b634c1b9d9ec9b3ba3f580f1be753972a,,,,,,,,,,,,,,"07/Apr/19 11:48;mck;Re-did this, post CASSANDRA-14607


|| branch || circleci || asf jenkins ||
| [CASSANDRA-14757|https://github.com/thelastpickle/cassandra/commit/54c2f7f0c5e2223f726d2b1e2949f96390a17670]	| [circleci|https://circleci.com/workflow-run/9c42f4d6-c9e4-4905-9b21-009cfd08fe25]	| [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/38//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/38/] |


Let me know what you think [~snazy], [~jjordan].;;;","19/Jun/19 10:56;snazy;[~mck] LGTM - let's change roles (assignee/reviewer);;;","20/Jun/19 03:08;mck;done. am i free to merge [~snazy]?;;;","20/Jun/19 09:51;snazy;+1;;;","27/Jun/19 12:32;mck;Committed with 2ed2b87b634c1b9d9ec9b3ba3f580f1be753972a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reinstate repaired data tracking when ReadRepair == NONE,CASSANDRA-14755,13185490,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,samt,samt,17/Sep/18 12:11,15/May/20 08:02,13/Jul/23 08:37,20/Sep/18 12:56,4.0,4.0-alpha1,,,,,Legacy/Local Write-Read Paths,,,,0,,,,"Some of the refactoring in CASSANDRA-14698 breaks repaired data tracking when read repair is disabled as it skips wrapping the {{MergeIterator}} in {{DataResolver::wrapMergeListener}}. If repaired tracking is enabled, the iterator still needs to be extended so that it calls {{RepairedDataTracker::verify}} on close. This wasn't easy to spot as the new dtests for CASSANDRA-14145 haven't yet been merged. 
",,ifesdjeen,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 20 12:56:06 UTC 2018,,,,,,,,,,,"0|i3y5pz:",9223372036854775807,,,,,,,,,ifesdjeen,,ifesdjeen,,,Normal,,,,,,,,,,,,,,,,,,,"17/Sep/18 12:12;samt;Here's the dtest PR which contains the new tests and also turns on repaired tracking for all dtests by default: https://github.com/apache/cassandra-dtest/pull/37 

C* Patch and CI runs:
||branch||utests||dtests||
|[14755-trunk|https://github.com/beobal/cassandra/tree/14755-trunk]|[utests|https://circleci.com/gh/beobal/cassandra/444]|[vnodes|https://circleci.com/gh/beobal/cassandra/445] / [no vnodes|https://circleci.com/gh/beobal/cassandra/446]|
;;;","18/Sep/18 10:44;ifesdjeen;[~beobal] sorry: I was looking closely at all the {{on*}} methods to ensure they do not have any effects, but overlooked the {{close}}. Thank you for fixing it.

+1, LGTM!;;;","20/Sep/18 12:56;samt;Thanks [~ifesdjeen], committed to trunk in {{ee9e06b5a75c0be954694b191ea4170456015b98}} and will merge the dtest PR shortly.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
serializers/BooleanSerializer.java is using static bytebuffers which may cause problem for subsequent operations,CASSANDRA-14752,13185075,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,e.dimitrova,varuna,varuna,14/Sep/18 07:50,07/Mar/23 11:52,13/Jul/23 08:37,17/Apr/22 18:51,3.0.27,3.11.13,4.0.4,4.1,4.1-alpha1,,Legacy/Core,,,,1,,,,"[https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/serializers/BooleanSerializer.java#L26] It has two static Bytebuffer variables:-
{code:java}
private static final ByteBuffer TRUE = ByteBuffer.wrap(new byte[]{1});
private static final ByteBuffer FALSE = ByteBuffer.wrap(new byte[]{0});{code}
What will happen if the position of these Bytebuffers is being changed by some other operations? It'll affect other subsequent operations. -IMO Using static is not a good idea here.-

A potential place where it can become problematic: [https://github.com/apache/cassandra/blob/cassandra-2.1.13/src/java/org/apache/cassandra/db/marshal/AbstractCompositeType.java#L243] Since we are calling *`.remaining()`* It may give wrong results _i.e 0_ if these Bytebuffers have been used previously.

Solution: 
 [https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/serializers/BooleanSerializer.java#L42] Every time we return new bytebuffer object. Please do let me know If there is a better way. I'd like to contribute. Thanks!!
{code:java}
public ByteBuffer serialize(Boolean value)
{
return (value == null) ? ByteBufferUtil.EMPTY_BYTE_BUFFER
: value ? ByteBuffer.wrap(new byte[] {1}) : ByteBuffer.wrap(new byte[] {0}); // false
}
{code}",,benedict,blerer,brandon.williams,e.dimitrova,jeromatron,maedhroz,marcuse,rishikthr,stefania,varuna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-16310,,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/18 07:58;varuna;patch;https://issues.apache.org/jira/secure/attachment/12939673/patch","17/Sep/18 03:08;varuna;patch-modified;https://issues.apache.org/jira/secure/attachment/12939911/patch-modified",,,,,,,,,,,,2.0,marcuse,stefania,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Apr 17 18:49:58 UTC 2022,,,,,,,,,,,"0|i3y35r:",9223372036854775807,5.0,,,,,,,,,,blerer,e.dimitrova,marcuse,Normal,,3.0.26,,,https://github.com/apache/cassandra/commit/fb66800a00aeaa8046cb3e6b1401fdc4f81848d5,,,,,,,,," [3.11 CircleCI|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1525/workflows/86e5bb0d-2e14-472e-a9f3-312b24ac1a38]

4.0 [j8|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1526/workflows/9855f9f5-4543-4fa6-a2a9-11ec0718f1e4], [j11|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1526/workflows/e4dbf043-aaf3-471a-bae6-931676df893c]

trunk [j8|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1527/workflows/81685d1c-5349-4f44-b39b-d655ccb00cee], [j11|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1527/workflows/efbb02f0-f6fd-4d8f-92d1-404f1699d7b0]

 

New test from Marcus also added",,,,,"17/Sep/18 07:48;blerer;bq. What will happen if the position of these Bytebuffers is being changed by some other operations?

In C* you should not change the position unless you have duplicated your `ByteBuffer` first or you really know what you are doing. All the data being stored in the memtables for example are shared so if you change the position on a `ByteBuffer` coming from there you can corrupt the data in a much worst way.

Personally, I would not try to change that code as its effect would simply be to cause more garbage.;;;","17/Sep/18 08:40;varuna; 

[~blerer] Thanks for your reply. In one of our tool, we use below code to generate the DecoratedKey from String and In the case of boolean type, we are facing this issue.
{code:java}
DatabaseDescriptor.getPartitioner().decorateKey(getKeyValidator(row.getColumnFamily())
.fromString(stringKey));
{code}

[https://github.com/apache/cassandra/blob/cassandra-2.1.13/src/java/org/apache/cassandra/db/marshal/AbstractCompositeType.java#L255] `byteBuffer.put` changes the position. Though it has a comment: *// it's ok to consume component as we won't use it anymore.*

 ;;;","17/Sep/18 09:14;varuna;I found there are too many usages of `AbstractCompositeType#fromString()`. 
One way to corrupt the data:-

Table schema:-
{code:java}
CREATE TABLE ks1.table1 (
t_id boolean,
id boolean,
ck boolean,
nk boolean,
PRIMARY KEY ((t_id,id),ck)
);{code}
Insert statement:-
{code:java}
insert into ks1.table1 (t_id, ck, id, nk)
VALUES (true, false, false, true);
{code}
Now run nodetool command to get the SSTable for given key:-
{code:java}
bin/nodetool getsstables  ks1 table1 ""false:true""
{code}
Basically, this operation will modify the positions.

Insert again:-
{code:java}
insert into ks1.table1 (t_id, ck, id, nk)
VALUES (true, true, false, true);
{code}
select data from this table:-
{code:java}
true,false,false,true
null,null,null,null
{code}
So now all boolean type data will be written as null.;;;","12/Dec/18 11:38;varuna; 
||MR||
|[trunk\|https://github.com/Barala/cassandra/commits/CASSANDRA-14752-trunk]|

I came up with a different approach where BooleanSerializer's static ByteBuffer can be detected using a reference equality check. This approach will avoid new object creations of ByteBuffers.

I raised MR for trunk. If it passes the review then I'll raise MR to patch other affected versions.;;;","09/Sep/21 13:12;e.dimitrova;Hey [~varuna], apologize for the very late response.

I think we can simplify a bit by reducing the patch to changing [this line|https://github.com/Barala/cassandra/commit/2328cda4d4e12e75bc1e9606969f18dd72d4fc27#diff-821957692da5432eb814312397d145fc6445ac35856563b21895e0c1df82ca3aL235]

to 
{code:java}
bb.put(component.duplicate()); // it's not ok to consume component as we did not create it{code}
What do you think?
[~blerer] , do you mind also to review it? I see it is also marked for 4.x but I think we can port it also to at least 4.0.x.

 ;;;","06/Oct/21 17:35;e.dimitrova;I reworked 3.11 and 4.0 and pushed additional changes for trunk based on Stefania Alborghetti's patch. (I will add her as author at the end before commit)

*trunk:* Byte buffers of {{BooleanSerializer}} are now read-only. We cannot make them on-heap read-only, as we would need to change our code in several key places which seems not worth it at this point. However, buffers can be off-heap read-only. Note that this only offers partial protection against put calls done on the buffer itself. It will not protect, amongst several cases, for put calls where the read-only buffer is the source, as it was the case in {{AbstractCompositeType}}. In this case, the position will still be advanced. To make these buffers completely safe we would need to duplicate them and accept the additional GC pressure.
||Patch||CI||
|​[3.11|https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:14752-3.11?expand=1]|[Jenkins|https://ci-cassandra.apache.org/job/Cassandra-devbranch/1171/#showFailuresLink]|
|​[4.0|https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:14752-4.0?expand=1]|[Jenkins|https://ci-cassandra.apache.org/job/Cassandra-devbranch/1172/#showFailuresLink]|
|​[trunk|https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:14752-trunk?expand=1]|[Jenkins|https://ci-cassandra.apache.org/job/Cassandra-devbranch/1183/#showFailuresLink]|

I don't see any related issues in the CI runs.

[~blerer], [~ifesdjeen], as you are already familiar with this, does anyone of you have time for review?;;;","10/Dec/21 08:05;marcuse;lgtm, +1;;;","10/Dec/21 12:20;benedict;This patch could potentially have serious performance consequences, by making many call-sites megamorphic that were previously bimorphic for clusters using e.g. offheap_buffers (and perhaps offheap_objects)...;;;","10/Dec/21 12:25;marcuse;yeah my bad, only checked the 3.11 patch, assumed they were the same;;;","10/Dec/21 15:32;e.dimitrova;Thank you both, actually Benjamin made a pass and there was also another issue with the trunk patch that the tests didn't catch but I got side-tracked and left to follow up on this when there is more time to work on it and not jumping in between other tasks as it is important to do it right. 

Quick suggestion - should we apply the 3.11 patch and fix the bug for our users and open follow up ticket if you think it is really worth it to pursue something more at this point? [~marcuse] , [~blerer] , [~benedict] , what do you think about that?;;;","10/Dec/21 15:49;marcuse;bq. should we apply the 3.11 patch and fix the bug for our users and open follow up ticket if you think it is really worth it to pursue something more at this point?
yes this sounds good to me;;;","08/Apr/22 20:31;e.dimitrova;Based on feedback and also Slack discussion with [~marcuse], I rebased and updated all branches.

Added one more test by [~marcuse], CI started:

[3.11| https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:14752-3.11?expand=1]|[Jenkins CI| https://jenkins-cm4.apache.org/job/Cassandra-devbranch/1587/]

[4.0| https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:14752-4.0?expand=1]|[Jenkins CI| https://jenkins-cm4.apache.org/job/Cassandra-devbranch/1588/]

[trunk| https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:14752-trunk?expand=1]|[Jenkins CI| https://jenkins-cm4.apache.org/job/Cassandra-devbranch/1589/]  ;;;","11/Apr/22 02:22;e.dimitrova;Jenkins 3.11 results look awful, seems like I ran it by mistake with the old not rebased branch.... Pushed it again now [here|https://jenkins-cm4.apache.org/job/Cassandra-devbranch/1591/];;;","11/Apr/22 02:28;e.dimitrova;4.0 has one new failure but I don't see how it can be related - [https://jenkins-cm4.apache.org/job/Cassandra-devbranch/1588/testReport/junit/org.apache.cassandra.distributed.test/RepairTest/testForcedNormalRepairWithOneNodeDown/]

I will run it with the multiplexer in a loop tomorrow. 

The trunk version has suffered from full disk... I will run these tests again probably in Circle tomorrow morning. ;;;","11/Apr/22 17:37;e.dimitrova;So Jenkins failed with some weird rpm errors. I realized I didn't use the image used for CI but one built more recently so there might be some differences causing troubles. Pushed with the right image, nightlies connection issues but those errors I saw before are gone. I wanted to be sure about them as CircleCI does not test our packaging, only Jenkins.

I pushed CircleCI runs for all three branches, still running: 

 [3.11 CircleCI|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1525/workflows/86e5bb0d-2e14-472e-a9f3-312b24ac1a38]

4.0 [j8|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1526/workflows/9855f9f5-4543-4fa6-a2a9-11ec0718f1e4], [j11|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1526/workflows/e4dbf043-aaf3-471a-bae6-931676df893c]

trunk [j8|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1527/workflows/81685d1c-5349-4f44-b39b-d655ccb00cee], [j11|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1527/workflows/efbb02f0-f6fd-4d8f-92d1-404f1699d7b0];;;","11/Apr/22 23:26;e.dimitrova;The upgrade_udtfix_test look suspicious from the perspective they don't fail in Jenkins (talking about Cassandra-3.11).

I pushed in a loop with the patch:

[https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1532/workflows/dc444e1f-42ca-4993-b053-5a9c69ba1c2d]

Without the patch:

[https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1533/workflows/118fff4b-1d4f-4fd6-8b2d-c33dd8f76bde]

 

Everything is green and when you try to look at the logs - test runs were just skipped.... this made me check Jenkins, it seems those tests are skipped also there... 

Maybe [~jlewandowski], [~brandon.williams] or [~mck] will know something? I see the three of you were interacting with those tests before.  

 

The rest of the failures are all known with associated tickets.;;;","13/Apr/22 01:36;e.dimitrova;So I reran the upgrade tests with 3.11 in CircleCI without the patch. - https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1533/workflows/118fff4b-1d4f-4fd6-8b2d-c33dd8f76bde/jobs/9933/tests
The only two tests that failed with my patch and I don't see in the list of failures without are:
upgrade_tests.cql_tests.TestCQLNodes2RF1_Upgrade_indev_3_11_x_To_indev_trunk - I found it in Butler, trunk (https://ci-cassandra.apache.org/job/Cassandra-trunk/1076/testReport/dtest-upgrade.upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_indev_3_0_x_To_indev_trunk/test_noncomposite_static_cf/) 
 
test_noncomposite_static_cf - found it again on trunk... https://ci-cassandra.apache.org/job/Cassandra-trunk/1076/testReport/dtest-upgrade.upgrade_tests.cql_tests/cls/test_noncomposite_static_cf/

So to conclude, I see the same failures, except two which for some reason in Butler appear under trunk and not 3.11... I am not sure why I see what I see...

I got +1 on the patch on green CI from [~blerer] in Slack.
I consider all failures known and I will open a ticket to align how we run the upgrade tests in Circle and in Jenkins to ensure we don't miss anything... 

I will wait until tomorrow to commit the patch as there is some Jenkins issue and I see the last runs marked in red. Infra is checking. 
;;;","13/Apr/22 16:06;e.dimitrova;While waiting for Jenkins issues to be cleared I realized the issue was not reported for 3.0 but it also exists so pushed there the same patch and tests. 3.0 will be supported one more year so let's be good citizens.

[3.0 patch|https://github.com/apache/cassandra/compare/cassandra-3.0...ekaterinadimitrova2:14752-3.0]| [CI|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1534/workflows/9ceb0797-e623-4b1e-b3f6-f8ef9fa29f6d]

CI only known issues and the same upgrade tests issue I see with 3.11 and not related to the patch;;;","14/Apr/22 02:33;e.dimitrova;[~blerer] , [~marcuse]  do you also agree on the 3.0 patch? I can commit all branches tomorrow;;;","14/Apr/22 09:01;blerer;+1 :);;;","15/Apr/22 23:17;e.dimitrova;Starting commit, pending CI:

[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...ekaterinadimitrova2:14752-3.0] | [CI|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1539/workflows/45e492b7-bdb7-43d3-ac9c-194675661b86]

[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...ekaterinadimitrova2:14752-3.11?expand=1] | [CI|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1540/workflows/92dd6163-c11a-41e7-8156-acfff45010db] 

[4.0|https://github.com/apache/cassandra/compare/cassandra-4.0...ekaterinadimitrova2:14752-4.0?expand=1] | [CI J8|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1541/workflows/5a606780-788f-4ce7-a769-47f2c91e9398] | [CI J11|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1541/workflows/cc29f66c-3679-48ca-9c49-388106a02162] 

[trunk|https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:14752-trunk?expand=1] | [CI J8|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1542/workflows/823731cb-54f6-4208-a619-6e77af751fa0] | [CI J11|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1542/workflows/11215d18-48d0-4b7a-938b-0b2c0e4dc3f8];;;","17/Apr/22 17:50;e.dimitrova;I hate this CI game :( I think there was some environmental issue with 4.0 and there are 6 tests failing with J11 on J8.

I reran the suite now - [https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1541/workflows/5a606780-788f-4ce7-a769-47f2c91e9398/jobs/10057.] All green

Most of the failures were timeouts and busy addresses. There were two failures which I've not seen before and looked suspicious; not from the perspective of this patch but in general.

I cannot reproduce with or without the patch in 100 runs and it seems possible to be related to the other environmental twists:

1) [https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1544/workflows/0e743bbc-694b-4105-811a-504822334f97/jobs/10060/parallel-runs/13?filterBy=ALL]

[https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1545/workflows/864bee95-d173-41aa-9702-889e39ee7117/jobs/10061]

2) test_expiration_overflow_policy_capnowarn

with the patch

https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1546/workflows/4c16d1d4-309a-4623-b4a0-2eccd9569dd5

without the patch

https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1547/workflows/13c1e0a1-aee2-462a-93ea-6132aa9e7c84

 

Committing this in a bit... ;;;","17/Apr/22 18:49;e.dimitrova;Committed to all branches:

To https://github.com/apache/cassandra.git

   84bc0e8c3b..fb66800a00  cassandra-3.0 -> cassandra-3.0

   cde152cdb7..eeb89ea53b  cassandra-3.11 -> cassandra-3.11

   d1270c204f..a46e467737  cassandra-4.0 -> cassandra-4.0

   74bb6d8496..03ef67c9d5  trunk -> trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing check for receiving digest response from transient node,CASSANDRA-14750,13184988,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,13/Sep/18 19:43,16/Apr/19 09:29,13/Jul/23 08:37,21/Sep/18 14:57,,,,,,,Legacy/Coordination,,,,0,pull-request-available,,,Range reads do not check for transient nodes returning a request. Read Command also currently allows a combination of transient node and digest query.,,aweisberg,benedict,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,"GitHub user ifesdjeen opened a pull request:

    https://github.com/apache/cassandra/pull/266

    Add a check for receiving digest response from transient node

    for CASSANDRA-14750

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ifesdjeen/cassandra CASSANDRA-14750

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/266.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #266
    
----
commit 735b8da5379c7e3556b05dc450741bf6cae09f28
Author: Alex Petrov <oleksandr.petrov@...>
Date:   2018-09-12T21:20:34Z

    Add a check for receiving digest response from transient node
    
    Patch by Alex Petrov; reviewed by Benedict Elliot Smith for CASSANDRA-14750

----
;14/Sep/18 09:38;githubbot;600","Github user ifesdjeen closed the pull request at:

    https://github.com/apache/cassandra/pull/266
;21/Sep/18 13:12;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,CASSANDRA-14697,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 21 13:13:11 UTC 2018,,,,,,,,,,,"0|i3y2mf:",9223372036854775807,,,,,,,,,benedict,,benedict,,,Normal,,,,,,,,,,,,,,,,,,,"20/Sep/18 12:53;benedict;Sorry, I filed this mentally under things I had reviewed.  +1;;;","21/Sep/18 13:13;ifesdjeen;Thank you, committed to trunk with [59de353325768b6bb8f4dc18a1a2ace5071f8f84|https://github.com/apache/cassandra/commit/59de353325768b6bb8f4dc18a1a2ace5071f8f84];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Collection Deletions for Dropped Columns in 2.1/3.0 mixed-mode can delete rows,CASSANDRA-14749,13184944,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,benedict,benedict,benedict,13/Sep/18 16:02,02/Aug/19 02:48,13/Jul/23 08:37,17/Sep/18 13:13,3.0.18,,,,,,Messaging/Internode,,,,0,,,,"Similar to CASSANDRA-14568, if a 2.1 node sends a response to a 3.0 node containing a deletion for a dropped collection column, instead of deleting the collection, we will delete the row containing the collection.

 

This is an admittedly unlikely cluster state but, during such a state, a great deal of data loss could happen.",,aleksey,benedict,jeromatron,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,benedict,,,,,,,,,,,,Correctness -> Recoverable Corruption / Loss,,,,,,,,Challenging,Adhoc Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 17 10:57:22 UTC 2018,,,,,,,,,,,"0|i3y2cv:",9223372036854775807,,,,,,,,,,,aleksey,slebresne,,Critical,,3.0.0,,,,,,,,,,,,,,,,,"13/Sep/18 16:23;benedict;[3.0|https://github.com/belliottsmith/cassandra/tree/14749] [CI|https://circleci.com/workflow-run/c877e18e-ea8e-45ec-b090-4652445f487d]

Since this has already been reviewed in CASSANDRA-14568, if either of you could give a quick cursory +1 to the split, that would be great.  I felt it warranted its own CHANGES.txt entry and JIRA.;;;","13/Sep/18 16:55;aleksey;Should probably pass {{isStatic}} to that {{CFMetaData.getDroppedColumnDefinition()}} call, or else it just defaults to {{false}}. May or may not have effect here, but since you have that flag in hand already, might as well use it. +1 otherwise.

On a related note, I'm not 100% certain that other calls of {{CFMetaData.getDroppedColumnDefinition()}} are oll korrect, it's something to look into later.;;;","13/Sep/18 17:04;benedict;-Good job I did (split this out) - this time, Aleksey pointed out the {{getDroppedColumn}} invokes another method {{getDroppedColumn(name, isStatic)}} with a default parameter of {{false}} for {{isStatic}}-  (That's what I get for not refreshing the page)

I've pushed an update, but I think the defaulting variant of {{getDroppedColumn}} probably shouldn't exist.;;;","13/Sep/18 18:10;aleksey;LGTM. I'll file a JIRA to follow-up on the defaulting variant audit and/or removal.;;;","13/Sep/18 18:13;benedict;-FWIW, I've been planning to file a ticket to audit the non-use of {{getDroppedColumn}} also, off the back of this, and perhaps we should combine the two efforts.  I'm sure most uses of {{getColumn}} should have a corresponding use of {{getDroppedColumn}} and I'm sure there have been other oversights than this.  We could keep two separate endeavours though.-

At least in trunk, it seems dropped columns have been considered in all places that we invoke {{getColumn}}, besides in test cases;;;","14/Sep/18 10:29;benedict;Thanks.  Committed as [06c55f779ae68de98cce531e0b78be5716849003|https://github.com/apache/cassandra/commit/06c55f779ae68de98cce531e0b78be5716849003].;;;","17/Sep/18 10:20;snazy;Seems that this commit broke the build in cassandra-3.11:
{code:java}
build-test:
[javac] Compiling 540 source files to /home/automaton/cassandra-src/build/test/classes
[javac] /home/automaton/cassandra-src/test/unit/org/apache/cassandra/db/LegacyLayoutTest.java:279: error: Clustering is abstract; cannot be instantiated
[javac] builder.newRow(new Clustering(UTF8Serializer.instance.serialize(""a"")));
[javac] ^
[javac] /home/automaton/cassandra-src/test/unit/org/apache/cassandra/db/LegacyLayoutTest.java:280: error: no suitable method found for live(CFMetaData,ColumnDefinition,long,ByteBuffer,<null>)
[javac] builder.addCell(BufferCell.live(table, v, 0L, Int32Serializer.instance.serialize(1), null));
[javac] ^
[javac] method BufferCell.live(ColumnDefinition,long,ByteBuffer) is not applicable
[javac] (actual and formal argument lists differ in length)
[javac] method BufferCell.live(ColumnDefinition,long,ByteBuffer,CellPath) is not applicable
[javac] (actual and formal argument lists differ in length)
[javac] Note: Some input files use or override a deprecated API.
[javac] Note: Recompile with -Xlint:deprecation for details.
[javac] Note: Some input files use unchecked or unsafe operations.
[javac] Note: Recompile with -Xlint:unchecked for details.
[javac] 2 errors
{code};;;","17/Sep/18 10:57;benedict;Hmm.  Sorry about that.  I ran {{ant clean && ant}} but clearly didn't look closely at the result.  I'll ninja a fix in shortly, since it's only tests affected.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Recycler$WeakOrderQueue occupies Heap,CASSANDRA-14748,13184749,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,haoxu@paloaltonetworks.com,haoxu@paloaltonetworks.com,12/Sep/18 23:16,01/Aug/21 11:17,13/Jul/23 08:37,01/Apr/20 16:12,3.11.7,4.0,4.0-alpha4,,,,Legacy/Core,,,,0,,,,"Heap constantly high on some of the nodes in the cluster, I dump the heap and open it through Eclipse Memory Analyzer, looks like Recycler$WeakOrderQueue occupies most of the heap. 

 
||Package||Retained Heap||Retained Heap, %||# Top Dominators||
|!/jira/icons/i5.gif! <all>|7,078,140,136|100.00%|379,627|
|io|5,665,035,800|80.04%|13,306|
|netty|5,665,035,800|80.04%|13,306|
|util|5,568,107,344|78.67%|2,965|
|Recycler$WeakOrderQueue|4,950,021,544|69.93%|2,169|",The netty Cassandra using is netty-all-4.0.39.Final.jar,benedict,blerer,haoxu@paloaltonetworks.com,jasonstack,jeromatron,yifanc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-8457,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 01 16:12:40 UTC 2020,,,,,,,,,,,"0|i3y167:",9223372036854775807,,,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"13/Sep/18 07:11;benedict;I guess I should go and revisit the WeakOrderQueue and bound its laziness, as this is a recurring them - on the Netty project bug tracker as well.

It was originally assumed that all recycling would happen on the same thread, but that we must support occasional recycles from another thread.  So it was anticipated that the WeakOrderQueue would be used infrequently.  But, it seems to get exercised often in some use cases - ours apparently being one.  It's actually probably not too tricky to resolve this, so I'll see about opening a ticket on the netty project.

We should anyway try to track down where we are doing this in the project, as it is suboptimal, and not how the recycler is intended to be used.

Could you post the full heap dump, or at least the full class histogram?  The WeakOrderQueue should be holding on to some child objects.;;;","12/Feb/20 03:08;yifanc;I did a quick check. 

Memory leaks was observed in {{netty-all-4.0.39.Final}} as reported [in the issue|https://github.com/netty/netty/issues/5563]. The cause of the linked issue is that WeakOrderQueue not being collected fast enough and the fix is [here|https://github.com/netty/netty/pull/5569]. It looks very likely to the one described here. 

The first commit that has {{netty 4.0.39.Final}} is [2aa76632d61c60f1bc115289750fd594949ff98f|https://github.com/apache/cassandra/commit/2aa76632d61c60f1bc115289750fd594949ff98f]. I can look deeper into it when get time. ;;;","18/Feb/20 12:12;benedict;Thanks [~yifanc], that's probably good enough since we've not seen any other reproductions, and I would expect any issue with the recycling itself to show up more reliably.;;;","01/Apr/20 12:24;blerer;[~yifanc], [~benedict] Do you want to upgrade to a newer version of Netty to fix that issue?;;;","01/Apr/20 15:50;yifanc;[~blerer], the netty-all version being used is 4.1.*, which should have the fix already. ;;;","01/Apr/20 16:12;blerer;The problem is fixed since CASSANDRA-8457 which upgraded netty to 4.1.14 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race Condition in batchlog replica collection,CASSANDRA-14742,13184662,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,12/Sep/18 16:18,15/May/20 08:02,13/Jul/23 08:37,27/Sep/18 16:05,4.0,4.0-alpha1,,,,,,,,,0,pull-request-available,,,"When we collect nodes for it in {{StorageProxy#getBatchlogReplicas}}, we already filter out down replicas; subsequently they get picked up and taken for liveAndDown.

There's a possible race condition due to picking tokens from token metadata twice (once in {{StorageProxy#getBatchlogReplicas}} and second one in {{ReplicaPlan#forBatchlogWrite}})",,benedict,ifesdjeen,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,,"GitHub user ifesdjeen opened a pull request:

    https://github.com/apache/cassandra/pull/267

    Consolidate batch write code

    for CASSANDRA-14742

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ifesdjeen/cassandra CASSANDRA-14742

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/267.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #267
    
----

----
;17/Sep/18 14:12;githubbot;600","Github user ifesdjeen closed the pull request at:

    https://github.com/apache/cassandra/pull/267
;04/Oct/18 09:20;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,CASSANDRA-14697,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 27 16:05:24 UTC 2018,,,,,,,,,,,"0|i3y0mv:",9223372036854775807,,,,,,,,,benedict,,benedict,,,Normal,,,,,,,,,,,,,,,,,,,"17/Sep/18 14:17;ifesdjeen;We're picking batchlog CL of {{ONE}} or {{TWO}} since the logic in {{BatchlogManager#EndpointFilter}} allows either one node (local, in case of single-node data centers), or two replicas from different racks.;;;","26/Sep/18 11:19;benedict;Patch looks good overall, just a few nits:

# Right now, {{ReplicaPlans}} is organised into counter writes, regular writes, regular write utilities, reads, reads utilities; I think it would be cleanest to keep the batch write utilities similarly proximal to the batch writes themselves, for consistency
# {{syncWriteBatchedMutations}} and {{forBatchlogWrite}} each accept a {{localDc}} parameter - this seems a bit weird, since it's a global variable, and only ever invoked with this (but also, we obtain it inconsistently, by asking the snitch instead of the cached {{localDc}}.  Perhaps they should each just use the latter, without requiring it as a parameter?  (I realise this is pre-existing)
# Unused imports in {{ReplicaPlans}};;;","27/Sep/18 08:47;ifesdjeen;Thank you, have addressed all three comments and took a  liberty to introduce {{localDatacenter}} and {{localRack}} shortcuts, waiting for one more CI run.;;;","27/Sep/18 08:59;benedict;Thanks.

I'm a little concerned about the inconsistency of our {{DatabaseDescriptor.getLocalDataCenter}} and this {{IEndpointSnitch.getLocalDataCenter}}.  I had intended to simply refer to the former (although, debatably, only the latter should exist - since the former is not consistently updated, although it should for correctness never change).

I'm honestly not clear what the best clean up of this mess is, particularly with the distinction between the per-replication strategy snitch and the global snitch (the former of which seem to simply cache the latter), and perhaps it can be deferred to a later dedicated cleanup ticket anyway.

Otherwise, +1;;;","27/Sep/18 16:05;ifesdjeen;Thank you,

Committed to trunk as [29f83b88821c4792087df19d829ac87b5c06e9e6|https://github.com/apache/cassandra/commit/29f83b88821c4792087df19d829ac87b5c06e9e6];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BlockingReadRepair does not maintain monotonicity during range movements,CASSANDRA-14740,13184634,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,benedict,benedict,benedict,12/Sep/18 14:39,21/Dec/20 08:08,13/Jul/23 08:37,04/Feb/20 14:08,4.0,4.0-alpha4,,,,,Legacy/Coordination,,,,0,correctness,,,"The BlockingReadRepair code introduced by CASSANDRA-10726 requires that each of the queried nodes are written to, but pending nodes are not considered.  If there is a pending range movement, one of these writes may be ‘lost’ when the range movement completes.",,abdulazizali,aleksey,benedict,csplinter,dcapwell,jasonstack,jay.zhuang,jeromatron,n.v.harikrishna,samt,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14722,,,,,,,,,,,,,CASSANDRA-10726,,,,,,,,,,,,,,,,,,,,0.0,benedict,,,,,,,,,,,,Correctness -> Transient Incorrect Response,,,,,,,,Normal,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 04 18:39:31 UTC 2020,,,,,,,,,,,"0|i3y0gn:",9223372036854775807,,,,,,,,,,,samt,,,Critical,,4.0-alpha,4.0-alpha1,,https://github.com/apache/cassandra/commit/0f22dab1a015cb84d9857f940de5a256bfbee083,,,,,,,,,dtest included,,,,,"23/Jul/19 23:43;benedict;Patch available [here|https://github.com/belliottsmith/cassandra/tree/14740]; [tests|https://circleci.com/workflow-run/9b4bc0a4-ba24-4a29-a6fd-4bf78f125432];;;","24/Jul/19 22:30;benedict;The basic approach is quite simple: when we repair, we build a {{WritePlan}}, but we only select those nodes we need to meet the consistency level of the operation we are performing, and we only consider live nodes.  We prefer those nodes we have read from.  If they are all present, and they are sufficient to meet consistency, we behave as before.  In any other scenario, we build a partition representing all differences we have seen, and propagate this to any node that wasn't one of the original targets.

There are some minor inefficiencies, such as not handling the case where the ownership of only one node has changed, and there is no node pending, in which case we _might_ be able to only propagate the difference found on reconciling the presumably replaced node (though if unsafe bootstrap occurred even this would not be acceptable, but this might be an acceptable consistency failure given the semantic guarantees of this).  

We also don't bother to avoid merging the complete diff row with any other pending repairs if we have to perform an additional write.  It's assume that these scenario are rare, and not worth the significant extra complexity.

Unfortunately fixing unit tests was painful and not super beautiful.  This is because read-repair now consults the ring to decide who a repair should be routed to, instead of assuming it is sufficient to write to those we read from.  The tests as written assume the ring can be empty, and also that replication factor isn't relevant, so to avoid completely rewriting the tests, I have done some ugly things.;;;","21/Jan/20 11:18;samt;This is nice. As well as addressing the bug, using a {{ReplicaPlan}} for the writes really clarifies {{BlockingPartitionRepair}} & {{BlockingReadRepair}}.

There's a copy/paste bug in {{RowIteratorMergeListener::applyToPartition}} in the {{buildFullDiff}} branch, where it should be setting {{repairs[repairs.length - 1]}}. This is causing the new {{movingTokenReadRepairTest}} failure, but locally at least, it doesn't affect the other tests in that fixture so I'm not sure what's going on in CircleCI.

The patch also needs a rebase, but it doesn't look too onerous.

Nits:
* {{BlockingReadRepair}} #54: extraneous comment
* {{ReadRepairTest}} #199: formatting
* {{RowIteratorMergeListener}}: unused import
* I find the style of having the conditions and statements for an {{if/else}} on the same line ({{RowIteratorMergeListener}} #373) makes them harder to parse. This is clearly rather subjective though and maybe just because it's not followed more universally in the project - feel free to ignore.
;;;","27/Jan/20 16:54;benedict;Thanks, I've addressed your comments and pushed.  I still need to figure out how to parse CircleCI results.;;;","30/Jan/20 11:04;benedict;[CircleCI|https://circleci.com/workflow-run/1e09aaed-8345-484f-bc9d-a9b018005520] looks clean (I think? I'm losing faith in my ability to understand its UX, or its ability to understand our output)

;;;","04/Feb/20 14:08;samt;Committed to trunk in {{0f22dab1a015cb84d9857f940de5a256bfbee083}}. Final CI run after rebasing with only previously known failures: [jdk8|https://circleci.com/workflow-run/3cb3c20e-6dea-4fd6-8aff-193e1e7d298b] & [jdk11|https://circleci.com/workflow-run/024ba3cd-c112-4f73-a562-b364ea420e3c];;;","04/Feb/20 18:32;dcapwell;spoke to sam about this; MessageForwardingTest is a regression and not an existing issue.

This test used to fail on java 11 (was stable on java 8) because of a change in behavior of java Streams; Blake fixed that in 1f7e3c2835c79363025a01a8470ee85d17457cf8 and since then the test has been stable.

The test is now failing in both java 8 and java 11 and isn't flaky, fails 100% of the time.;;;","04/Feb/20 18:39;samt;Sorry, this broke {{MessageForwardingTest}} by forcing the RF of the test keyspace to 3; I mistakenly thought that test was already failing before. 
Fixed in a follow up commit [{{cb4314ee96}}|https://github.com/apache/cassandra/commit/cb4314ee96922f870f5b30d6594a42d2007a5bb2] with CI runs for [jdk8|https://circleci.com/workflow-run/3e9a34a0-a6b7-4b41-9e7f-667233f858c6] and [jdk11|https://circleci.com/workflow-run/442cd66f-25c4-4d39-988b-7bdd3aceae5a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"LOCAL_QUORUM may speculate to non-local nodes, resulting in Timeout instead of Unavailable",CASSANDRA-14735,13184626,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,benedict,benedict,benedict,12/Sep/18 14:32,15/May/20 07:59,13/Jul/23 08:37,26/Sep/18 09:57,4.0,4.0-alpha1,,,,,,,,,0,,,,"This issue applies to all of: rapid read protection, read repair's rapid read protection and read repair's rapid write protection.",,aweisberg,benedict,jeromatron,rha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14697,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,benedict,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 26 09:57:08 UTC 2018,,,,,,,,,,,"0|i3y0ev:",9223372036854775807,,,,,,,,,,,aweisberg,ifesdjeen,,Low,,,,,,,,,,,,,,,,,,,"20/Sep/18 09:49;benedict;Patch available [here|https://github.com/belliottsmith/cassandra/tree/14735]

This patch begins by cleaning up {{ConsistencyLevel}} to remove (most of) the functionality that doesn't logically belong there:

* DC locality testing (to {{InOurDcTester}})
* Replica counting per DC and replication type (to {{Replicas}})
* Endpoint sufficiency testing / filtering to {{ReplicaPlans}}

The last commit makes the only functional changes, reworking the filtering code into separate {{candidatesForRead}} and {{contactForRead}} methods, as well as moving from {{HashMap<String, Integer>}} to {{ObjectIntOpenHashMap}} in the replica counting methods.;;;","20/Sep/18 09:50;benedict;[~ifesdjeen], [~aweisberg], would either (or both) of you mind reviewing?;;;","20/Sep/18 23:03;aweisberg;I ran out of time today. The thing I couldn't convince myself of was how this fixes rapid read protection and read repair. I need to look up how they assemble their reads.

I guess I'm confused how it was broken before because we filter based on DC locality even without this change?;;;","20/Sep/18 23:18;benedict;It’s possible there were too many extraneous cleanup changes; the fix, fundamentally, is splitting filterForQuery into candidatesForRead and contactsForRead; the former now also filters to DC local, whereas previously candidates were not DC-local.;;;","21/Sep/18 00:33;aweisberg;Oh I get it now. It previously selected the candidates and then filtered after resulting in a candidates collection with too many incorrect options. Other DCs are not candidates for LOCAL_*.

+1;;;","26/Sep/18 09:57;benedict;Thanks, committed as [8554d6b35dcc5eec46ed7edc809a36c1f7fa588f|https://github.com/apache/cassandra/commit/8554d6b35dcc5eec46ed7edc809a36c1f7fa588f];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"sstabledump displays incorrect value for ""position"" key",CASSANDRA-14721,13184541,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,cam1982,Damien Stevenson,Damien Stevenson,12/Sep/18 08:18,15/May/20 08:53,13/Jul/23 08:37,31/Dec/19 14:36,3.0.20,3.11.6,4.0,4.0-alpha3,,,Legacy/Tools,,,,0,,,,"When partitions with multiple rows are displayed using sstabledump, the ""position"" value the first row of each partition is incorrect.

For example:
{code:java}
sstabledump mc-1-big-Data.db
[
  {
    ""partition"" : {
      ""key"" : [ ""1"", ""24"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 66, 
        ""clustering"" : [ ""2013-12-10 00:00:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-09-12T05:01:09.290086Z"" },
        ""cells"" : [
          { ""name"" : ""centigrade"", ""value"" : 8 },
          { ""name"" : ""chanceofrain"", ""value"" : 0.1 },
          { ""name"" : ""feelslike"", ""value"" : 8 },
          { ""name"" : ""humidity"", ""value"" : 0.76 },
          { ""name"" : ""wind"", ""value"" : 10.0 }
        ]
      },
      {
        ""type"" : ""row"",
        ""position"" : 66, 
        ""clustering"" : [ ""2013-12-11 00:00:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-09-12T05:01:09.295369Z"" },
        ""cells"" : [
          { ""name"" : ""centigrade"", ""value"" : 4 },
          { ""name"" : ""chanceofrain"", ""value"" : 0.3 },
          { ""name"" : ""feelslike"", ""value"" : 4 },
          { ""name"" : ""humidity"", ""value"" : 0.9 },
          { ""name"" : ""wind"", ""value"" : 12.0 }
        ]
      },
      {
        ""type"" : ""row"",
        ""position"" : 105,
        ""clustering"" : [ ""2013-12-12 00:00:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-09-12T05:01:09.300841Z"" },
        ""cells"" : [
          { ""name"" : ""centigrade"", ""value"" : 3 },
          { ""name"" : ""chanceofrain"", ""value"" : 0.2 },
          { ""name"" : ""feelslike"", ""value"" : 3 },
          { ""name"" : ""humidity"", ""value"" : 0.68 },
          { ""name"" : ""wind"", ""value"" : 6.0 }
        ]
      }
    ]
  }
]
{code}
 The expected output is:
{code:java}
[
  {
    ""partition"" : {
      ""key"" : [ ""1"", ""24"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 28,
        ""clustering"" : [ ""2013-12-10 00:00:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-09-12T05:01:09.290086Z"" },
        ""cells"" : [
          { ""name"" : ""centigrade"", ""value"" : 8 },
          { ""name"" : ""chanceofrain"", ""value"" : 0.1 },
          { ""name"" : ""feelslike"", ""value"" : 8 },
          { ""name"" : ""humidity"", ""value"" : 0.76 },
          { ""name"" : ""wind"", ""value"" : 10.0 }
        ]
      },
      {
        ""type"" : ""row"",
        ""position"" : 66,
        ""clustering"" : [ ""2013-12-11 00:00:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-09-12T05:01:09.295369Z"" },
        ""cells"" : [
          { ""name"" : ""centigrade"", ""value"" : 4 },
          { ""name"" : ""chanceofrain"", ""value"" : 0.3 },
          { ""name"" : ""feelslike"", ""value"" : 4 },
          { ""name"" : ""humidity"", ""value"" : 0.9 },
          { ""name"" : ""wind"", ""value"" : 12.0 }
        ]
      },
      {
        ""type"" : ""row"",
        ""position"" : 105,
        ""clustering"" : [ ""2013-12-12 00:00:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-09-12T05:01:09.300841Z"" },
        ""cells"" : [
          { ""name"" : ""centigrade"", ""value"" : 3 },
          { ""name"" : ""chanceofrain"", ""value"" : 0.2 },
          { ""name"" : ""feelslike"", ""value"" : 3 },
          { ""name"" : ""humidity"", ""value"" : 0.68 },
          { ""name"" : ""wind"", ""value"" : 6.0 }
        ]
      }
    ]
  }
]
{code}",,cam1982,cnlwsu,Damien Stevenson,jeromatron,jjirsa,KurtG,mck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/18 09:36;cam1982;cassandra-dump.patch;https://issues.apache.org/jira/secure/attachment/12939380/cassandra-dump.patch",,,,,,,,,,,,,1.0,cam1982,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Dec 31 14:36:15 UTC 2019,,,,,,,,,,,"0|i3xzw7:",9223372036854775807,3.0.17,4.0,,,,,,,,,cnlwsu,mck,,Low,,3.0.4,,,https://github.com/apache/cassandra/commit/de4e2504f4533f923ac4de2cac44abd86407e5a4,,,,,,,,,,,,,,"12/Sep/18 09:36;cam1982;[^cassandra-dump.patch]

 

^Here my fix for 3.11.3^;;;","19/Oct/18 21:14;cnlwsu;I am +1 removing the classpath change

| [branch|https://github.com/clohfink/cassandra/tree/14721] | [unit tests|https://circleci.com/gh/clohfink/cassandra/432] | [dtests|https://circleci.com/gh/clohfink/cassandra/436] |;;;","22/Oct/18 20:22;jjirsa;[~cam1982] / [~cnlwsu] - which versions are affected? Does the patch apply cleanly to all of them?
;;;","22/Oct/18 21:29;cnlwsu;all versions are effected, and from what i can tell will apply cleanly (didn't try all though).;;;","29/Dec/19 17:49;mck;||branch||circleci||asf jenkins testall||asf jenkins dtests||
|[cassandra-3.0_14721|https://github.com/apache/cassandra/compare/cassandra-3.0...thelastpickle:mck/cassandra-3.0_14721]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-3.0_14721]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/36/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/36/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/709/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/709]|
|[cassandra-3.11_14721|https://github.com/apache/cassandra/compare/cassandra-3.11...thelastpickle:mck/cassandra-3.11_14721]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-3.11_14721]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/37/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/37/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/710/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/710]|
|[trunk_14721|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/trunk_14721]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Ftrunk_14721]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/38/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/38/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/711/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/711]|;;;","31/Dec/19 14:36;mck;Committed as de4e2504f4533f923ac4de2cac44abd86407e5a4

Thanks [~Damien Stevenson], [~cam1982], [~cnlwsu].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTables may not be properly removed from original strategy when repair status changed,CASSANDRA-14720,13184522,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasonstack,jasonstack,jasonstack,12/Sep/18 03:56,15/May/20 08:01,13/Jul/23 08:37,12/Sep/18 06:33,4.0,4.0-alpha1,,,,,Local/Compaction,,,,0,,,,"In {{CSM.handleRepairStatusChangedNotification()}}, CASSANDRA-14621 changed the original semantic of {{removing sstables in repaired first}} to {{adding sstables into unrepaired first}}...

In case of LCS, adding sstables may modify their levels, so they won't be removed from {{repaired}} which locates sstables by level. 

| [trunk|https://github.com/apache/cassandra/compare/trunk...jasonstack:CASSANDRA-14621-follow-up?expand=1] | [circle|https://circleci.com/gh/jasonstack/cassandra/742] |",,jasonstack,jeromatron,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasonstack,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 12 07:52:29 UTC 2018,,,,,,,,,,,"0|i3xzrz:",9223372036854775807,,,,,,,,,marcuse,,marcuse,,,Normal,,4.0,,,,,,,,,,,,,,,,,"12/Sep/18 03:58;jasonstack;[~bdeggleston] do you mind reviewing?;;;","12/Sep/18 06:33;marcuse;nice catch, thanks!

committed as {{f100024eb3becf53042823ce1008d3d5ec4e5f86}};;;","12/Sep/18 07:52;jasonstack;Thanks for the quick review!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Protocol frame checksumming options should not be case sensitive,CASSANDRA-14716,13184390,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,samt,samt,11/Sep/18 15:52,15/May/20 08:02,13/Jul/23 08:37,27/Nov/18 14:22,4.0,4.0-alpha1,,,,,Legacy/CQL,,,,0,,,,"Protocol v5 adds support for checksumming of native protocol frame bodies. The checksum type is negotiated per-connection via the \{{STARTUP}} message, with two types currently supported, Adler32 and CRC32. The mapping of the startup option value requested by the client to a \{{ChecksumType}} should not be case sensitive, but currently it is.",,aleksey,djoshi,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 27 14:22:28 UTC 2018,,,,,,,,,,,"0|i3xyyn:",9223372036854775807,,,,,,,,,aleksey,,aleksey,,,Normal,,,,,,,,,,,,,,,,,,,"11/Sep/18 16:05;samt;Patch + unit test: [here|https://github.com/beobal/cassandra/commit/f9cda7d55cc2b57ba5905f0635275f569d56d4c9]
||utests||dtests||
|[utests|https://circleci.com/gh/beobal/cassandra/437]|[vnodes|https://circleci.com/gh/beobal/cassandra/436] / [no vnodes|https://circleci.com/gh/beobal/cassandra/438]|;;;","26/Nov/18 20:20;aleksey;I might be missing it, and probably am, but what is the reason for not simply doing {{ChecksumType.valueOf(name.toUpperCase)}}, and rethrowing {{IllegalArgumentException}} as {{ProtocolException}}?

EDIT: Ok, I see, it's {{Adler32}} not being low or upper case itself. Is it too late to change {{Adler32}} to {{ADLER32}} at this point? Would it be desirable?;;;","26/Nov/18 20:30;aleksey;That said, +1 either way.;;;","27/Nov/18 06:49;djoshi;Another option would be to use a static, immutable map of upper cased values to {{ChecksumType}} and use that.;;;","27/Nov/18 09:58;samt;{quote}Is it too late to change {{Adler32}} to {{ADLER32}} at this point? Would it be desirable?{quote}

No, I think that's acceptable and desirable, if only for consistency. This is the only usage of {{ChecksumType}} where the algo is configurable, every other usage is hardcoded to {{CRC32}}. We included support for {{Adler32}} in CASSANDRA-13304, rather than just removing that {{ChecksumType}} to enable future potential perf improvements to be taken advantage of (https://issues.apache.org/jira/browse/CASSANDRA-13304?focusedCommentId=15923191&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15923191). 

Before committing I'll rename the enum value, re-run CI and post a message to dev list to be sure nobody objects to this post-freeze change. Thanks.;;;","27/Nov/18 12:51;samt;Rebased and updated branch to uppercase the enum value.
||branch||CI||
|[14716-trunk|https://github.com/beobal/cassandra/tree/14716-trunk]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F14716-trunk]|
;;;","27/Nov/18 13:41;aleksey;[~djoshi3] yeah, that would also work.

[~beobal] +1;;;","27/Nov/18 14:22;samt;Thanks, committed to trunk in {{cdeac4992bdb1f569c3a04b628ded7e5351364ee}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add fqltool and auditlogviewer to rpm and deb packages,CASSANDRA-14712,13183987,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stefan.miklosovic,spod,spod,10/Sep/18 09:37,09/Sep/20 09:36,13/Jul/23 08:37,03/Jun/20 17:23,4.0,4.0-beta1,,,,,Packaging,,,,0,Java11,pull-request-available,,"Currently it's not possible to build any native packages (.deb/.rpm) for trunk.

cassandra-builds - docker/*-image.docker
 * Add Java11 to debian+centos build image
 * (packaged ant scripts won't work with Java 11 on centos, so we may have to install ant from tarballs)

cassandra-builds - docker/build-*.sh
 * set JAVA8_HOME to Java8
 * set JAVA_HOME to Java11 (4.0) or Java8 (<4.0)

cassandra - redhat/cassandra.spec
 * Check if patches still apply after CASSANDRA-14707
 * Add fqltool as %files

We may also have to change the version handling in build.xml or build-*.sh, depending how we plan to release packages during beta, or if we plan to do so at all before GA.",,jwest,mck,mshuler,stefan.miklosovic,yakir.g,zznate,,,,,,,,,,,,,,,,,,,,"smiklosovic commented on pull request #9: CASSANDRA-14712 - Cassandra 4.0 packaging support
URL: https://github.com/apache/cassandra-builds/pull/9
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/19 06:33;githubbot;600","smiklosovic commented on pull request #9: CASSANDRA-14712 - Cassandra 4.0 packaging support
URL: https://github.com/apache/cassandra-builds/pull/9#discussion_r269420661
 
 

 ##########
 File path: docker/build-debs.sh
 ##########
 @@ -78,7 +87,12 @@ fi
 
 # Install build dependencies and build package
 echo ""y"" | sudo mk-build-deps --install
+
 
 Review comment:
   This had to be done otherwise following `dpkg-buildpackage` complained that JAVA_HOME can not be set like it was set at the beginning of this file.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/19 06:35;githubbot;600","smiklosovic commented on pull request #9: CASSANDRA-14712 - Cassandra 4.0 packaging support
URL: https://github.com/apache/cassandra-builds/pull/9#discussion_r269420661
 
 

 ##########
 File path: docker/build-debs.sh
 ##########
 @@ -78,7 +87,12 @@ fi
 
 # Install build dependencies and build package
 echo ""y"" | sudo mk-build-deps --install
+
 
 Review comment:
   This had to be done otherwise following `dpkg-buildpackage` complained that JAVA_HOME can not be set like it was set at the beginning of this file.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/19 06:36;githubbot;600","smiklosovic commented on pull request #9: CASSANDRA-14712 - Cassandra 4.0 packaging support
URL: https://github.com/apache/cassandra-builds/pull/9#discussion_r269420748
 
 

 ##########
 File path: docker/build-debs.sh
 ##########
 @@ -78,7 +87,12 @@ fi
 
 # Install build dependencies and build package
 echo ""y"" | sudo mk-build-deps --install
+
+unset JAVA8_HOME
+unset JAVA_HOME
+
 
 Review comment:
   This had to be done otherwise following `dpkg-buildpackage` complained that JAVA_HOME can not be set like it was set at the beginning of this file.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/19 06:36;githubbot;600","smiklosovic commented on pull request #9: CASSANDRA-14712 - Cassandra 4.0 packaging support
URL: https://github.com/apache/cassandra-builds/pull/9#discussion_r269420748
 
 

 ##########
 File path: docker/build-debs.sh
 ##########
 @@ -78,7 +87,12 @@ fi
 
 # Install build dependencies and build package
 echo ""y"" | sudo mk-build-deps --install
+
+unset JAVA8_HOME
+unset JAVA_HOME
+
 
 Review comment:
   This had to be done otherwise following `dpkg-buildpackage` complained that `JAVA_HOME` can not be set like it was set at the beginning of this file.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/19 06:39;githubbot;600","smiklosovic commented on pull request #9: CASSANDRA-14712 - Cassandra 4.0 packaging support
URL: https://github.com/apache/cassandra-builds/pull/9#discussion_r269421929
 
 

 ##########
 File path: docker/stretch-image.docker
 ##########
 @@ -1,32 +1,34 @@
-FROM debian:jessie-backports
 
 Review comment:
   `jessie-backports` does not exist anymore, the subsequent update returns 404 on repos
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/19 06:42;githubbot;600","smiklosovic commented on pull request #9: CASSANDRA-14712 - Cassandra 4.0 packaging support
URL: https://github.com/apache/cassandra-builds/pull/9#discussion_r269422027
 
 

 ##########
 File path: docker/stretch-image.docker
 ##########
 @@ -1,32 +1,34 @@
-FROM debian:jessie-backports
+FROM debian:stretch-backports
 
 ENV DEB_DIST_DIR=/dist
 ENV BUILD_HOME=/home/build
 ENV CASSANDRA_DIR=$BUILD_HOME/cassandra
+ENV ANT_VERSION=1.10.5
 
-LABEL org.cassandra.buildenv=jessie
+LABEL org.cassandra.buildenv=stretch
 
 VOLUME ${DEB_DIST_DIR}
 
 # install deps
 RUN apt-get update && apt-get -y install \
-   ant \
    build-essential \
    curl \
    devscripts \
    git \
    sudo
 
-RUN apt-get -y -t jessie-backports --no-install-recommends install \
-   openjdk-7-jdk \
 
 Review comment:
   `openjdk-7-jdk` is not in stretch anymore, Ive removed 7 from centos too then 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/19 06:42;githubbot;600","smiklosovic commented on issue #9: CASSANDRA-14712 - Cassandra 4.0 packaging support
URL: https://github.com/apache/cassandra-builds/pull/9#issuecomment-477001506
 
 
   There needs to be merged / reviewed this too: 
   
   https://github.com/smiklosovic/cassandra/commit/4a3926aa58f2b8a4a1e4b02b67810817243ff83e
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/19 06:44;githubbot;600","mshuler commented on pull request #9: CASSANDRA-14712 - Cassandra 4.0 packaging support
URL: https://github.com/apache/cassandra-builds/pull/9#discussion_r269569635
 
 

 ##########
 File path: docker/centos7-image.docker
 ##########
 @@ -4,26 +4,27 @@ ENV BUILD_HOME=/home/build
 ENV RPM_BUILD_DIR=$BUILD_HOME/rpmbuild
 ENV RPM_DIST_DIR=/dist
 ENV CASSANDRA_DIR=$BUILD_HOME/cassandra
+ENV ANT_VERSION=1.10.5
 
 LABEL org.cassandra.buildenv=centos
 
 VOLUME ${RPM_DIST_DIR}
 
 # install deps
 RUN yum -y install \
-   ant \
-   ant-junit \
    epel-release \
    git \
-   java-1.7.0-openjdk-devel \
    java-1.8.0-openjdk-devel \
+   java-11-openjdk-devel \
    make \
    rpm-build \
    sudo
 
 # via epel-releases
 RUN yum -y install python2-pip
 
+RUN mkdir -p /opt && curl http://www-eu.apache.org/dist/ant/binaries/apache-ant-$ANT_VERSION-bin.tar.gz | tar xz -C /opt && mv /opt/apache-ant-$ANT_VERSION /opt/ant && ln -s /opt/ant/bin/ant /usr/local/bin/ant
 
 Review comment:
   why?
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/19 13:48;githubbot;600","mshuler commented on pull request #9: CASSANDRA-14712 - Cassandra 4.0 packaging support
URL: https://github.com/apache/cassandra-builds/pull/9#discussion_r269569842
 
 

 ##########
 File path: docker/stretch-image.docker
 ##########
 @@ -1,32 +1,34 @@
-FROM debian:jessie-backports
+FROM debian:stretch-backports
 
 ENV DEB_DIST_DIR=/dist
 ENV BUILD_HOME=/home/build
 ENV CASSANDRA_DIR=$BUILD_HOME/cassandra
+ENV ANT_VERSION=1.10.5
 
-LABEL org.cassandra.buildenv=jessie
+LABEL org.cassandra.buildenv=stretch
 
 VOLUME ${DEB_DIST_DIR}
 
 # install deps
 RUN apt-get update && apt-get -y install \
-   ant \
    build-essential \
    curl \
    devscripts \
    git \
    sudo
 
-RUN apt-get -y -t jessie-backports --no-install-recommends install \
-   openjdk-7-jdk \
-   openjdk-8-jdk
+RUN apt-get -y -t stretch-backports --no-install-recommends install \
+   openjdk-8-jdk \
+   openjdk-11-jdk
 
-RUN apt-get -y -t jessie-backports install \
+RUN apt-get -y -t stretch-backports install \
    python-sphinx \
    python-sphinx-rtd-theme
 
 RUN update-java-alternatives --set java-1.8.0-openjdk-amd64
 
+RUN mkdir -p /opt && curl http://www-eu.apache.org/dist/ant/binaries/apache-ant-$ANT_VERSION-bin.tar.gz | tar xz -C /opt && mv /opt/apache-ant-$ANT_VERSION /opt/ant && ln -s /opt/ant/bin/ant /usr/local/bin/ant
 
 Review comment:
   why again?
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/19 13:48;githubbot;600","mshuler commented on issue #9: CASSANDRA-14712 - Cassandra 4.0 packaging support
URL: https://github.com/apache/cassandra-builds/pull/9#issuecomment-477163370
 
 
   Not sure why you're downloading a specific version of ant. Please, squash your commits.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/19 13:49;githubbot;600","smiklosovic commented on pull request #9: CASSANDRA-14712 - Cassandra 4.0 packaging support
URL: https://github.com/apache/cassandra-builds/pull/9#discussion_r269855955
 
 

 ##########
 File path: docker/stretch-image.docker
 ##########
 @@ -1,32 +1,34 @@
-FROM debian:jessie-backports
+FROM debian:stretch-backports
 
 ENV DEB_DIST_DIR=/dist
 ENV BUILD_HOME=/home/build
 ENV CASSANDRA_DIR=$BUILD_HOME/cassandra
+ENV ANT_VERSION=1.10.5
 
-LABEL org.cassandra.buildenv=jessie
+LABEL org.cassandra.buildenv=stretch
 
 VOLUME ${DEB_DIST_DIR}
 
 # install deps
 RUN apt-get update && apt-get -y install \
-   ant \
    build-essential \
    curl \
    devscripts \
    git \
    sudo
 
-RUN apt-get -y -t jessie-backports --no-install-recommends install \
-   openjdk-7-jdk \
-   openjdk-8-jdk
+RUN apt-get -y -t stretch-backports --no-install-recommends install \
+   openjdk-8-jdk \
+   openjdk-11-jdk
 
-RUN apt-get -y -t jessie-backports install \
+RUN apt-get -y -t stretch-backports install \
    python-sphinx \
    python-sphinx-rtd-theme
 
 RUN update-java-alternatives --set java-1.8.0-openjdk-amd64
 
+RUN mkdir -p /opt && curl http://www-eu.apache.org/dist/ant/binaries/apache-ant-$ANT_VERSION-bin.tar.gz | tar xz -C /opt && mv /opt/apache-ant-$ANT_VERSION /opt/ant && ln -s /opt/ant/bin/ant /usr/local/bin/ant
 
 Review comment:
   @mshuler  fixed in both cases
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Mar/19 04:47;githubbot;600","smiklosovic commented on pull request #307: CASSANDRA-14712 - Cassandra 4.0 packaging support
URL: https://github.com/apache/cassandra/pull/307
 
 
   should go with https://github.com/apache/cassandra-builds/pull/9
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Mar/19 07:50;githubbot;600","michaelsembwever commented on pull request #9:
URL: https://github.com/apache/cassandra-builds/pull/9#issuecomment-638200017


   i believe we can close this as `wont fix` ?
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jun/20 13:33;githubbot;600","michaelsembwever closed pull request #9:
URL: https://github.com/apache/cassandra-builds/pull/9


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Jun/20 09:15;githubbot;600","michaelsembwever commented on pull request #9:
URL: https://github.com/apache/cassandra-builds/pull/9#issuecomment-640182465


   no longer valid. see jira ticket.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Jun/20 09:15;githubbot;600","smiklosovic closed pull request #307:
URL: https://github.com/apache/cassandra/pull/307


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Sep/20 09:36;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,10200,,,0,10200,,,,,,,,,,,,,,,CASSANDRA-15838,,CASSANDRA-14714,,,,,,,,,,,,,,,,,,,,,,,0.0,stefan.miklosovic,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 03 17:23:44 UTC 2020,,,,,,,,,,,"0|i3xwjj:",9223372036854775807,,,,,,,,,,,mck,,,Normal,,4.0-alpha1,,,https://github.com/apache/cassandra/commit/ecf2c9ccd482c4b08355a288213326dc7a8cdcef,,,,,,,,,"Manually using cassandra-*-packaging.sh scripts.
(Will become automatic with CASSANDRA-15838 )",,,,,"28/Mar/19 23:24;stefan.miklosovic;[~spodxx@gmail.com] could you please review both PRs and tell me whatever else you would like to add or modify?

I have tested that RPMs and DEBs are installing ok in the latest RHEL7 and Debian Stretch 9 and fqltool as well as auditlogviewer are available to operator.

There was not need to package custom Ant (1.10.5) from tarball, Ant packages from repositories which are still of version 1.9 worked just fine.

Debian Jesse was in my case so old that backport repository was not existing anymore (it gave me 404 on apt update) so I updated it to Debian Stretch.

We could also update version of CentOS to 7.6 instead of the current 7.0 just to be updated. I saw the warning about the deprecation of Python 2.7 but it was still building fine in the end.

I am still not completely sure whats the deal with Java 8 / Java 11 combo, as per conversation with [~bdeggleston] here (1) he was able to build trunk in both cases on 8 or 11 only. There is currently still a prerequisite to build trunk on Java 11 to get distribution tarballs and it does not make a lot of sense to me why it is the case if one is reportedly able to build trunk with 8 only. (2)

(1) [https://lists.apache.org/thread.html/0501ad77c74f5bc83e84e4d31a063ad09b60a4d9b42112646d97e7e8@%3Cdev.cassandra.apache.org%3E]

(2) [https://github.com/apache/cassandra/blob/trunk/build.xml#L1049-L1051]

 

 ;;;","02/Jun/20 10:32;mck;Looping back on this ticket, as I'm working on CASSANDRA-15838.

bq. Currently it's not possible to build any native packages (.deb/.rpm) for trunk.

Much of the description of this ticket does not appear up to date. deb/rpm packages can indeed be built off trunk, as both CASSANDRA-15838 and all the 4.0-alpha* releases demonstrate.

bq. I am still not completely sure whats the deal with Java 8 / Java 11 combo…

The need to defined {{JAVA8_HOME}} no longer exists. In CASSANDRA-15838 it becomes more evident how to build the artefacts with either jdk8 or jdk11. We want nightlies of both, but releases will remain only jdk8 generated (for now). While CASSANDRA-15838 will fix deb/rpm jdk11 generation, it will get plugged into the ASF CI (Jenkins) via CASSANDRA-15809

bq. Debian Jesse was in my case so old…

The `jessie-image.docker` is no longer required. You can see in CASSANDRA-15838 ""cassandra-deb-packaging.sh"" that it now uses `buster-image.docker`. 

h4. Ticket Status?

My understanding is that #9 is no longer required. And that this ticket can solely cover what is provided in #307, which looks good to me. If this is the case, could you update the ticket summary accordingly [~stefan.miklosovic]? while i'll test #307 a bit more and commit if all is well.;;;","02/Jun/20 14:12;spod;The ticket was created quite a while ago, thanks for confirming that most of the described issues have been solved by now!;;;","03/Jun/20 14:38;mck;A rebase off trunk of #307 is available at https://github.com/apache/cassandra/compare/trunk...thelastpickle:CASSANDRA-14712

Have tested it. (Had to add back the line {{rm -f bin/*.orig}} to {{cassandra.spec}}.

+1;;;","03/Jun/20 17:23;mck;Committed as [ecf2c9ccd482c4b08355a288213326dc7a8cdcef |https://github.com/apache/cassandra/commit/ecf2c9ccd482c4b08355a288213326dc7a8cdcef].

Thanks [~stefan.miklosovic]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use quilt to patch cassandra.in.sh in Debian packaging,CASSANDRA-14710,13183900,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mshuler,mshuler,mshuler,09/Sep/18 17:33,15/May/20 08:01,13/Jul/23 08:37,09/May/19 15:59,4.0,4.0-alpha1,,,,,Packaging,,,,0,,,,"While working on CASSANDRA-14707, I found the debian/cassandra.in.sh file is outdated and is missing some elements from bin/cassandra.in.sh. This should not be a separately maintained file, so let's use quilt to patch the few bits that need to be updated on Debian package installations.
 * rm debian/cassandra.in.sh
 * create quilt patch for path updates needed
 * update debian/cassandra.install to install our patched bin/cassandra.in.sh",,Jan Karlsson,mshuler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/18 18:42;mshuler;CASSANDRA-14710_c.in.sh.patch.txt;https://issues.apache.org/jira/secure/attachment/12938997/CASSANDRA-14710_c.in.sh.patch.txt",,,,,,,,,,,,,1.0,mshuler,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 09 15:59:14 UTC 2019,,,,,,,,,,,"0|i3xw0f:",9223372036854775807,,,,,,,,,,,Jan Karlsson,,,Normal,,4.0,,,,,,,,,,,,,,,,,"09/Sep/18 18:48;mshuler;Attached [^CASSANDRA-14710_c.in.sh.patch.txt] to drop separate debian/cassandra.in.sh file and install quilt-patched bin/cassandra.in.sh.

Github branch is here: https://github.com/mshuler/cassandra/tree/CASSANDRA-14710_c.in.sh;;;","06/May/19 12:23;Jan Karlsson;Took a look at the patch and LGTM. Seems to all apply cleanly and installs just fine in a Debian docker container.

 ;;;","09/May/19 15:16;mshuler;Thanks for reviewing [~Jan Karlsson].;;;","09/May/19 15:59;mshuler;Pushed commit [3a87604e4c|[https://github.com/apache/cassandra/commit/3a87604e4c83939f05d1a51e577e69d45dc7345d]] to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Building deb packages fails on trunk,CASSANDRA-14707,13183845,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mshuler,rustyrazorblade,rustyrazorblade,08/Sep/18 18:58,15/May/20 08:01,13/Jul/23 08:37,09/Sep/18 02:02,4.0,4.0-alpha1,,,,,Build,,,,0,lhf,,,"Looks like there were some changes to either {{conf/cassandra-env.sh}} and/or bin/cassandra that's screwing up the 002cassandra_logdir_fix patch.  I think it's the result of CASSANDRA-9608.  

Here's the error I get when doing the build:

{noformat}
build-cassandra_1  | applying patch 001cassandra_yaml_dirs to ./ ... ok.
build-cassandra_1  | applying patch 002cassandra_logdir_fix to ./ ... failed.
build-cassandra_1  | /usr/share/dpatch/dpatch.make:27: recipe for target 'patch-stamp' failed
build-cassandra_1  | make: *** [patch-stamp] Error 1
build-cassandra_1  | dpkg-buildpackage: error: debian/rules build gave error exit status 2
aws_cluster_build-cassandra_1 exited with code 0
{noformat}

Seems like an easy fix, should be able to apply the same edits to those 2 files and regenerate the patch.
",,mshuler,rustyrazorblade,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/18 21:02;mshuler;dpatch2quilt.diff.txt;https://issues.apache.org/jira/secure/attachment/12938969/dpatch2quilt.diff.txt","08/Sep/18 21:04;mshuler;dpatch2quilt_build-and-clean.log.txt;https://issues.apache.org/jira/secure/attachment/12938971/dpatch2quilt_build-and-clean.log.txt",,,,,,,,,,,,2.0,mshuler,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Sep 09 02:02:28 UTC 2018,,,,,,,,,,,"0|i3xvo7:",9223372036854775807,,,,,,,,,rustyrazorblade,,rustyrazorblade,,,Normal,,,,,,,,,,,,,,,,,,,"08/Sep/18 21:05;mshuler; [^dpatch2quilt.diff.txt] patch attached for trunk to switch the Debian packaging patch tool from dpatch (long deprecated) to quilt, along with patch refresh.

 [^dpatch2quilt_build-and-clean.log.txt] log attached, testing out Debian package build and clean successfully.

Pushed branch to github, if that's easier: https://github.com/mshuler/cassandra/tree/dpatch2quilt;;;","09/Sep/18 00:36;rustyrazorblade;Built the package without issue, installed on an ubuntu AWS instance, verified location of logs & data. 

+1, ship it.

;;;","09/Sep/18 02:02;mshuler;Thanks for the check! Committed to trunk: [f424e03|https://github.com/apache/cassandra/commit/f424e03a445080b937605515210a061061c7906b].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validate transient status on query,CASSANDRA-14704,13183662,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,07/Sep/18 15:45,16/Apr/19 09:29,13/Jul/23 08:37,12/Sep/18 15:05,,,,,,,,,,,0,,,,"Validate transient status on query:

|[patch|https://github.com/apache/cassandra/pull/261]|[utest|https://circleci.com/gh/ifesdjeen/cassandra/393]|[dtest-novnode|https://circleci.com/gh/ifesdjeen/cassandra/394]|[dtest-vnode|https://circleci.com/gh/ifesdjeen/cassandra/392]|",,benedict,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14697,,,,,CASSANDRA-14964,,,,,,,,,,,,,,,,,,,,,,,,,0.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 12 15:04:54 UTC 2018,,,,,,,,,,,"0|i3xujj:",9223372036854775807,,,,,,,,,benedict,,benedict,,,Normal,,,,,,,,,,,,,,,,,,,"12/Sep/18 09:12;benedict;I'm not sure it matters, but it's possibly better in {{validateTransientStatus}} to only fail the query if we received a non-transient request on a transient node.  This is what was meant to be implied by {{acceptsTransient}} - it doesn't *require* transient.  This would mean more work on the node and more network traffic, but that's maybe better than failing the query?  Not sure.

Otherwise LGTM, +1;;;","12/Sep/18 15:04;ifesdjeen;Thank you for the review!

You're right, I've adjusted the logic and comment according to your suggestion.

Committed with [2046c30adec194fb07bc5dd1c31fc19a64e7895c|https://github.com/apache/cassandra/commit/2046c30adec194fb07bc5dd1c31fc19a64e7895c] to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup (and other) compaction type(s) not counted in compaction remaining time,CASSANDRA-14701,13183597,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,tsteinmaurer,tsteinmaurer,07/Sep/18 09:19,27/May/22 19:25,13/Jul/23 08:37,21/Jul/21 19:19,3.0.25,3.11.11,4.0.1,4.1,4.1-alpha1,,Legacy/Observability,,,,1,,,,"Opened a ticket, as discussed in user list.

Looks like compaction remaining time only includes compactions of type COMPACTION and other compaction types like cleanup etc. aren't part of the estimation calculation.

E.g. from one of our environments:
{noformat}
nodetool compactionstats -H

pending tasks: 1
   compaction type   keyspace           table   completed     total    unit   progress
           Cleanup        XXX             YYY   908.16 GB   1.13 TB   bytes     78.63%
Active compaction remaining time :   0h00m00s
{noformat}
",,adelapena,e.dimitrova,jeromatron,kkierer,lapo@lapo.it,mck,tsteinmaurer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 21 19:19:14 UTC 2021,,,,,,,,,,,"0|i3xu53:",9223372036854775807,,,,,,,,,,,adelapena,brandon.williams,,Normal,,NA,,,https://github.com/apache/cassandra/commit/426253a34085f7f11fcef2b90b3abf8fce715583,,,,,,,,,changelog,,,,,"11/Sep/18 10:31;spod;Link to [cassandra-user thread|https://lists.apache.org/thread.html/6f355d1c2258478dbd7ea3e7960a03ed480daa4cab91d31e71de1000@%3Cuser.cassandra.apache.org%3E].;;;","06/May/21 23:29;brandon.williams;||Branch||CI||
|[3.0|https://github.com/driftx/cassandra/tree/CASSANDRA-14701-3.0]|[!https://ci-cassandra.apache.org/job/Cassandra-devbranch/751/badge/icon!|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/751/pipeline]|
|[3.11|https://github.com/driftx/cassandra/tree/CASSANDRA-14701-3.11]|[!https://ci-cassandra.apache.org/job/Cassandra-devbranch/746/badge/icon!|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/746/pipeline]|
|[4.0|https://github.com/driftx/cassandra/tree/CASSANDRA-14701-4.0]|[!https://ci-cassandra.apache.org/job/Cassandra-devbranch/747/badge/icon!|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/747/pipeline]|
|[trunk|https://github.com/driftx/cassandra/tree/CASSANDRA-14701]|[!https://ci-cassandra.apache.org/job/Cassandra-devbranch/748/badge/icon!|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/748/pipeline]|

The good news is, this is a cosmetic decision/bug in nodetool only.  I did some git spelunking to see if there was any reasoning behind it, and it goes all the way back to inception in CASSANDRA-4167 which doesn't go into depth.  I don't see any reason for it to do this, especially this way, since all it does is make the calculations incorrect for other compaction types.
;;;","21/Jul/21 16:39;adelapena;It seems this needs a (trivial) rebase and new CI runs:

||branch||CI||
|[3.0|https://github.com/adelapena/cassandra/tree/14701-3.0]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/687/workflows/ea9f146d-a64c-4371-865d-32e973249115]|
|[3.11|https://github.com/adelapena/cassandra/tree/14701-3.11]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/688/workflows/38ee8ece-9a72-49a0-8874-68060cc8d03e]|
|[4.0|https://github.com/adelapena/cassandra/tree/14701-4.0]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/686/workflows/cc7d4bab-bf44-443d-bdc1-454cfd139b62] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/686/workflows/50a2f017-971a-41bf-83c6-53d54df24bbc]|
|[trunk|https://github.com/adelapena/cassandra/tree/14701-trunk]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/685/workflows/b858542a-279d-4ffd-b409-bd695994e863] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/685/workflows/7bc6dda9-d95a-40fd-b93e-9201c8b74325]|;;;","21/Jul/21 17:09;adelapena;The change makes sense to me, +1 if CI looks good.

A minor detail that can be addressed during commit is that now the import of {{OperationType}} is unused, so we can remove it.;;;","21/Jul/21 19:19;brandon.williams;No related failures in CI, committed.  Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up repair path after Transient Replication ,CASSANDRA-14698,13183440,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,06/Sep/18 16:12,16/Apr/19 09:29,13/Jul/23 08:37,12/Sep/18 14:58,,,,,,,,,,,0,pull-request-available,,,"Clean up repair path after Transient Replication since TR doesn't do RR: 

|[patch|https://github.com/apache/cassandra/pull/259]|[dtest|https://circleci.com/gh/ifesdjeen/cassandra/367]|[dtest-vnode|https://circleci.com/gh/ifesdjeen/cassandra/366]|[utest|https://circleci.com/gh/ifesdjeen/cassandra/368]|",,aweisberg,bdeggleston,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,"Github user ifesdjeen commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/265#discussion_r217012379
  
    --- Diff: src/java/org/apache/cassandra/service/reads/DigestResolver.java ---
    @@ -93,16 +93,14 @@ public PartitionIterator getData()
             {
                 // This path can be triggered only if we've got responses from full replicas and they match, but
                 // transient replica response still contains data, which needs to be reconciled.
    -            DataResolver<E, L> dataResolver = new DataResolver<>(command,
    -                                                                 replicaLayout,
    -                                                                 (ReadRepair<E, L>) NoopReadRepair.instance,
    -                                                                 queryStartNanoTime);
    +            DataResolver<E, P> dataResolver
    +                    = new DataResolver<>(command, replicaPlan, (ReadRepair<E, P>) NoopReadRepair.instance, queryStartNanoTime);
     
                 dataResolver.preprocess(dataResponse);
                 // Forward differences to all full nodes
    --- End diff --
    
    I think this one should be already fixed in CASSANDRA-14698
;12/Sep/18 12:39;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,CASSANDRA-14697,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 12 13:50:36 UTC 2018,,,,,,,,,,,"0|i3xt6f:",9223372036854775807,,,,,,,,,bdeggleston,,aweisberg,bdeggleston,,Low,,,,,,,,,,,,,,,,,,,"06/Sep/18 17:26;aweisberg;+1, Love it! Much clearer now without all that dead code misdirecting people to think that we do read repair with transient replicas.;;;","07/Sep/18 19:06;bdeggleston;+1;;;","12/Sep/18 13:50;ifesdjeen;[~aweisberg] [~bdeggleston] I've found a small issue with creating merge listener from the wrong layout. It's fixed in the new patch [here|https://github.com/ifesdjeen/cassandra/tree/CASSANDRA-14698] but the change was tiny.

Committed with [8a73427c6543c94ce49da0ed1f833ec5b8ed4f18|https://github.com/apache/cassandra/commit/8a73427c6543c94ce49da0ed1f833ec5b8ed4f18] to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DecayingEstimatedHistogramReservoir.EstimatedHistogramReservoirSnapshot returns wrong value for size() and incorrectly calculates count,CASSANDRA-14696,13183249,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aweisberg,aweisberg,aweisberg,05/Sep/18 22:00,15/May/20 08:00,13/Jul/23 08:37,07/Sep/18 19:14,4.0,4.0-alpha1,,,,,Legacy/Core,,,,0,4.0-pre-rc-bugs,,,"It appears to return the wrong value for no reason. There isn't a single instance in the code base where we use size. There is an internal count that is calculated, but it is calculated over an empty array. 

Fix both of these bugs and then use the size to only populate read and write latency values used for speculation when there are actual samples being used.",,aweisberg,benedict,cnlwsu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14697,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aweisberg,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 07 19:14:13 UTC 2018,,,,,,,,,,,"0|i3xs0f:",9223372036854775807,,,,,,,,,cnlwsu,,cnlwsu,,,Normal,,,,,,,,,,,,,,,,,,,"05/Sep/18 22:06;aweisberg;This is used to fix a more general issue with read and write latency tracking where when we have no data we end up using 0 as the value for speculation timeouts. This is different from using the default (1/2 the timeout) or the value from back when we had data.;;;","06/Sep/18 14:13;aweisberg;[CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/cassandra-14696-trunk];;;","06/Sep/18 15:06;cnlwsu;Is the read_repair_test.TestSpeculativeReadRepair dtest failure related?

 

[In PR - I thought these were copied into a comment from bot?]

Since the size > 0 is really a precondition for calculateThreshold to run successfully, it seems like the check should be there instead in the policies that require it vs an implicit undocumented precondition to be checked before in only the existing calls (incase a new entry point ever created).;;;","06/Sep/18 16:37;aweisberg;They are supposed to be? I think it may be broken. It's supposed to show up under work log.

It's unrelated, we know about that failure and I think Benedict is working on the fix.;;;","06/Sep/18 16:49;aweisberg;Updated to fix the bug you found where it was ignoring fixed policies and the like that don't rely on the latency information.;;;","06/Sep/18 17:40;aweisberg;At Benedict's suggestion I made the inner class static and reference the enclosing class fields via reference. Some of the fields were being shadowed so it was a bit tricky.;;;","07/Sep/18 18:23;cnlwsu;I like the static inner class change a lot. +1 on pr now as well.;;;","07/Sep/18 18:40;benedict;+1;;;","07/Sep/18 19:14;aweisberg;Committed as [8d443805f06e7abb25f768f6c800b7ae71bd4a41|https://github.com/apache/cassandra/commit/8d443805f06e7abb25f768f6c800b7ae71bd4a41]. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revert parameterization of AuthCacheMBean interface,CASSANDRA-14687,13182683,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,samt,samt,03/Sep/18 18:35,15/May/20 08:04,13/Jul/23 08:37,04/Sep/18 11:35,4.0,4.0-alpha1,,,,,Feature/Authorization,,,,0,,,,"In CASSANDRA-14662, a type parameter {{<T>}} and {{invalidate<T t>}} method were added to {{AuthCacheMBean}} with the intention that this would automatically expose via JMX the {{invalidate}} method from {{AuthCache}} itself. Actually, this is not the case, as type erasure obscures the actual type of the parameter and so JMX clients like jconsole and jmc disable access to the method. 

Only {{PasswordAuthenticator.CredentialsCache}} provided method like this previously, via {{CredentialsCacheMBean extends AuthCacheMBean}}, so the most straightforward fix is to just revert the change to {{AuthCacheMBean}}. Future/alternative cache implementations can continue to specify their own MBean interfaces to add specialised methods as before.

I should've caught this before committing the CASSANDRA-14662 as it broke a couple of dtests in AuthTest, mea culpa.
",,KurtG,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 04 11:35:46 UTC 2018,,,,,,,,,,,"0|i3xonz:",9223372036854775807,,,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"03/Sep/18 18:40;samt;||branch||utests||dtests||
|[branch|https://github.com/beobal/cassandra/tree/14687-trunk]|[utests|https://circleci.com/gh/beobal/cassandra/398]|[vnodes|https://circleci.com/gh/beobal/cassandra/399] / [no vnodes|https://circleci.com/gh/beobal/cassandra/397]|
;;;","04/Sep/18 09:12;KurtG;+1. My bad, should have checked that.;;;","04/Sep/18 11:35;samt;Thanks [~KurtG], committed to trunk in {{d26f142b34681d047fe010c8ec9097add0b44d2a}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SafeMemoryWriterTest doesn't compile on trunk,CASSANDRA-14681,13182174,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,snazy,snazy,snazy,30/Aug/18 16:27,15/May/20 08:01,13/Jul/23 08:37,30/Aug/18 17:13,4.0,4.0-alpha1,,,,,,,,,0,Java11,,,"{{SafeMemoryWriterTest}} references {{sun.misc.VM}}, which doesn't exist in Java 11, so the build fails.

Proposed patch makes the test work against Java 8 + 11.",,blambov,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,snazy,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 30 17:13:35 UTC 2018,,,,,,,,,,,"0|i3xljj:",9223372036854775807,,,,,,,,,,,,,,Low,,4.0,,,,,,,,,,,,,,,,,"30/Aug/18 16:29;snazy;[branch|https://github.com/snazy/cassandra/tree/14681-fix-SafeMemoryWriterTest-trunk]

Verified that it both builds against Java 8 and 11+8 and passes against 8 + 11.;;;","30/Aug/18 16:31;blambov;LGTM;;;","30/Aug/18 17:13;snazy;Thanks for the review!

 

Committed as [8b1a6247ec5aabad92a664ff2cbf6d6529d8ceb7|https://github.com/apache/cassandra/commit/8b1a6247ec5aabad92a664ff2cbf6d6529d8ceb7] to [trunk|https://github.com/apache/cassandra/tree/trunk];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"After deleting data in 3.11.3, reads fail with ""open marker and close marker have different deletion times""",CASSANDRA-14672,13181807,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,aleksey,sivann,sivann,29/Aug/18 07:09,15/May/20 08:01,13/Jul/23 08:37,25/Sep/18 16:48,3.0.18,3.11.4,4.0,4.0-alpha1,,,Local/SSTable,,,,0,,,,"We had 3.11.0, then we upgraded to 3.11.3 last week. We routinely perform deletions as the one described below. After upgrading we run the following deletion query:

 
{code:java}
DELETE FROM measurement_events_dbl WHERE measurement_source_id IN ( 9df798a2-6337-11e8-b52b-42010afa015a,  9df7717e-6337-11e8-b52b-42010afa015a, a08b8042-6337-11e8-b52b-42010afa015a, a08e52cc-6337-11e8-b52b-42010afa015a, a08e6654-6337-11e8-b52b-42010afa015a, a08e6104-6337-11e8-b52b-42010afa015a, a08e6c76-6337-11e8-b52b-42010afa015a, a08e5a9c-6337-11e8-b52b-42010afa015a, a08bcc50-6337-11e8-b52b-42010afa015a) AND year IN (2018) AND measurement_time >= '2018-07-19 04:00:00'{code}
 

Immediately after that, trying to read the last value produces an error:
{code:java}
select * FROM measurement_events_dbl WHERE measurement_source_id = a08b8042-6337-11e8-b52b-42010afa015a AND year IN (2018) order by measurement_time desc limit 1;
ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 2 failures"" info={'failures': 2, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}{code}
 

And the following exception: 
{noformat}
WARN [ReadStage-4] 2018-08-29 06:59:53,505 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-4,5,main]: {}
java.lang.RuntimeException: java.lang.IllegalStateException: UnfilteredRowIterator for pvpms_mevents.measurement_events_dbl has an illegal RT bounds sequence: open marker and close marker have different deletion times
 at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2601) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_181]
 at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.3.jar:3.11.3]
 at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181]
Caused by: java.lang.IllegalStateException: UnfilteredRowIterator for pvpms_mevents.measurement_events_dbl has an illegal RT bounds sequence: open marker and close marker have different deletion times
 at org.apache.cassandra.db.transform.RTBoundValidator$RowsTransformation.ise(RTBoundValidator.java:103) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.transform.RTBoundValidator$RowsTransformation.applyToMarker(RTBoundValidator.java:81) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:148) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:136) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:92) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:79) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:308) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:187) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:180) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:176) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.ReadResponse.createDataResponse(ReadResponse.java:76) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:352) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1889) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2597) ~[apache-cassandra-3.11.3.jar:3.11.3]
 ... 5 common frames omitted
 Suppressed: java.lang.IllegalStateException: UnfilteredRowIterator for pvpms_mevents.measurement_events_dbl has an illegal RT bounds sequence: expected all RTs to be closed, but the last one is open
 at org.apache.cassandra.db.transform.RTBoundValidator$RowsTransformation.ise(RTBoundValidator.java:103) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.transform.RTBoundValidator$RowsTransformation.onPartitionClose(RTBoundValidator.java:96) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.transform.BaseRows.runOnClose(BaseRows.java:91) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.transform.BaseIterator.close(BaseIterator.java:86) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:309) ~[apache-cassandra-3.11.3.jar:3.11.3]
 ... 12 common frames omitted
 
{noformat}
 

We have 9 nodes ~2TB each, leveled compaction, repairs run daily in sequence.

Table definition is:
{noformat}
CREATE TABLE pvpms_mevents.measurement_events_dbl (
 measurement_source_id uuid,
 year int,
 measurement_time timestamp,
 event_reception_time timestamp,
 quality double,
 value double,
 PRIMARY KEY ((measurement_source_id, year), measurement_time)
) WITH CLUSTERING ORDER BY (measurement_time ASC)
 AND bloom_filter_fp_chance = 0.1
 AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
 AND comment = ''
 AND compaction = {'class': 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'}
 AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
 AND crc_check_chance = 1.0
 AND dclocal_read_repair_chance = 0.1
 AND default_time_to_live = 0
 AND gc_grace_seconds = 864000
 AND max_index_interval = 2048
 AND memtable_flush_period_in_ms = 0
 AND min_index_interval = 128
 AND read_repair_chance = 0.0
 AND speculative_retry = '99PERCENTILE';{noformat}
 

We host those on GCE and recreated all the nodes with disk snapshots, and we reproduced the error: after re-running the DELETE with all nodes up and no other queries running, the error was reproduced immediately.

 

We tried so far:

re-running repairs on all nodes and running nodetool garbagecollect with no success.

We downgraded to 3.11.2 for now, no issues so far.

This may be related to CASSANDRA-14515","CentOS 7, GCE, 9 nodes, 4TB disk/~2TB full each, level compaction, timeseries data",aleksey,bdeggleston,jeromatron,jjirsa,mck,mitsis,sbtourist,sivann,tommy_s,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14515,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,Correctness -> Recoverable Corruption / Loss,,,,,,,,Challenging,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 25 16:48:30 UTC 2018,,,,,,,,,,,"0|i3xjaf:",9223372036854775807,3.0.17,3.11.3,,,,,,,bdeggleston,,bdeggleston,,,Critical,,3.0.0,,,,,,,,,,,,,,,,,"17/Sep/18 08:29;mitsis;We did some testing on a replica environment. Specifically added some printf statements to check the timestamps on open and close marker that Cassandra complains.

The replication factor is 3, nodes 6, 8 & 9 contain the data. Node 8 & 9 are throwing the exception, node 6 when queried with consistency one does not fail (returns proper data - see below why).

On node8 & node9 the following timestamps are found:

Printf:

{noformat}
==> openMarkerDeletionTime: null
==> openMarkerDeletionTime: deletedAt=1537103654634113, localDeletion=1537103654
==> deletionTime: deletedAt=1530205388555918, localDeletion=1530205388
{noformat}

Exception:
{noformat}
WARN [ReadStage-1] 2018-09-16 14:40:44,252 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-1,5,main]: {}
java.lang.IllegalStateException: ==> UnfilteredRowIterator for pvpms_mevents.measurement_events_dbl has an illegal RT bounds sequence: open marker and close marker have different deletion times [deletedAt=1537103654634113, localDeletion=1537103654 deletedAt=1530205388555918, localDeletion=1530205388]
{noformat}

Converted timestamps:
{noformat}
openDeletionTime: Sun Sep 16 13:14:14 UTC 2018
closeDeletionTime: Thu Jun 28 17:03:08 UTC 2018
{noformat}

DELETE FROM command above was run on Sep 16.

On node6 :

Printf:
{noformat}
==> openMarkerDeletionTime: null
==> openMarkerDeletionTime: deletedAt=1537103654634113, localDeletion=1537103654
==> deletionTime: deletedAt=1537103654634113, localDeletion=1537103654
{noformat}

Converted timestamps:
{noformat}
Sun Sep 16 13:14:14 UTC 2018
{noformat}

No Exception!

We did a json dump of the specified data from node 8 & 9 and found a range_tombstone_bound with timestamp start/end of ""2018-06-28T17:03:08Z"" that contains data from Jul & Aug (see rows below). On node 6 the same same data are not within a tombstone marker (it’s the same json without the range_tombstone_bound).

{noformat}
   ""partition"" : {
      {
        ""type"" : ""range_tombstone_bound"",
        ""start"" : {
          ""type"" : ""exclusive"",
          ""clustering"" : [ ""2018-06-27 04:55:00.000Z"" ],
          ""deletion_info"" : { ""marked_deleted"" : ""2018-06-28T17:03:08.555918Z"", ""local_delete_time"" : ""2018-06-28T17:03:08Z"" }
        }
      },
      {
        ""type"" : ""row"",
        ""position"" : 83860313,
        ""clustering"" : [ ""2018-06-27 05:00:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-06-28T19:45:30.803293Z"" },
        ""cells"" : [
          { ""name"" : ""event_reception_time"", ""value"" : ""2018-06-28 19:45:30.784Z"" },
          { ""name"" : ""quality"", ""value"" : 100.0 },
          { ""name"" : ""value"", ""value"" : 408307.66 }
        ]
      },
…
      {
        ""type"" : ""row"",
        ""position"" : 83953463,
        ""clustering"" : [ ""2018-07-19 03:45:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-07-19T03:46:29.195118Z"" },
        ""cells"" : [
          { ""name"" : ""event_reception_time"", ""value"" : ""2018-07-19 03:46:29.193Z"" },
          { ""name"" : ""quality"", ""value"" : 100.0 },
          { ""name"" : ""value"", ""value"" : 593846.06 }
        ]
      },
…
      {
        ""type"" : ""row"",
        ""position"" : 84054985,
        ""clustering"" : [ ""2018-08-11 04:00:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-08-11T04:01:15.708470Z"" },
        ""cells"" : [
          { ""name"" : ""event_reception_time"", ""value"" : ""2018-08-11 04:01:15.703Z"" },
          { ""name"" : ""quality"", ""value"" : 100.0 },
          { ""name"" : ""value"", ""value"" : 372654.53 }
        ]
      },
      {
        ""type"" : ""range_tombstone_bound"",
        ""end"" : {
          ""type"" : ""inclusive"",
          ""deletion_info"" : { ""marked_deleted"" : ""2018-06-28T17:03:08.555918Z"", ""local_delete_time"" : ""2018-06-28T17:03:08Z"" }
        }
      }
{noformat}

We have downgraded on the mean time to Cassandra 3.11.2. Shouldn't at least these inconsistencies have more graceful assertions?
;;;","17/Sep/18 13:29;jjirsa;In chatting with the folks who wrote CASSANDRA-14515 offline (namely [~iamaleksey] and [~benedict] ), it sounds like what you're seeing is likely corruption that CASSANDRA-14515 was meant to protect. That is: the bug in cassandra 3.11.0 to 3.11.2 that causes data loss (14515) is also leaving your data files in an invalid corrupt state. The exception is letting you know it's broken, and in this case, that you've probably lost data due to that bug.

[~iamaleksey] / [~benedict] - any thoughts on how to prove this is really just 14515 corruption? Any ideas on recovery (will scrub help here)? ;;;","18/Sep/18 09:56;aleksey;[~jjirsa] I don't think we can say that this was CASSANDRA-14515 corruption specifically.

[~mitsis] Where is this dump from? I'm interested in seeing both mismatched bounds in a dump from an affected node, to prove conclusively that there is corrupt data on disk - and I'm yet to see the half with deletion time in September.;;;","18/Sep/18 11:55;mitsis;[~iamaleksey] The dump is from the ""bad"" node 9 and truncated heavily. I'm presenting the dumps that contain the specific partition key below. These dumps are after DELETE (see in ticket the full command) was run on Sep 16 on a replica cluster created with snapshots from Aug 29.

From node 9 there are 3 SSTables with that partition key:

* SSTable 1 below (I've truncated a lot of rows):

{noformat}
mc-2228-big-Data.json 
  {
    ""partition"" : {
      ""key"" : [ ""a08b8042-6337-11e8-b52b-42010afa015a"", ""2018"" ],
      ""position"" : 83756793
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 83756879,
        ""clustering"" : [ ""2018-06-03 18:45:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-06-03T19:48:27.570903Z"" },
        ""cells"" : [
          { ""name"" : ""event_reception_time"", ""value"" : ""2018-06-03 19:48:27.570Z"" },
          { ""name"" : ""quality"", ""value"" : 100.0 },
          { ""name"" : ""value"", ""value"" : 185532.77 }
        ]
      },
*
* many more rows
*
      {
        ""type"" : ""row"",
        ""position"" : 83860241,
        ""clustering"" : [ ""2018-06-27 04:45:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-06-27T04:50:08.969007Z"" },
        ""cells"" : [
          { ""name"" : ""event_reception_time"", ""value"" : ""2018-06-27 04:50:08.968Z"" },
          { ""name"" : ""quality"", ""value"" : 100.0 },
          { ""name"" : ""value"", ""value"" : 408307.66 }
        ]
      },
      {
        ""type"" : ""range_tombstone_bound"",
        ""start"" : {
          ""type"" : ""exclusive"",
          ""clustering"" : [ ""2018-06-27 04:55:00.000Z"" ],
          ""deletion_info"" : { ""marked_deleted"" : ""2018-06-28T17:03:08.555918Z"", ""local_delete_time"" : ""2018-06-28T17:03:08Z"" }
        }
      },
      {
        ""type"" : ""row"",
        ""position"" : 83860313,
        ""clustering"" : [ ""2018-06-27 05:00:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-06-28T19:45:30.803293Z"" },
        ""cells"" : [
          { ""name"" : ""event_reception_time"", ""value"" : ""2018-06-28 19:45:30.784Z"" },
          { ""name"" : ""quality"", ""value"" : 100.0 },
          { ""name"" : ""value"", ""value"" : 408307.66 }
        ]
      },
*
* many more rows
* 
      {
        ""type"" : ""row"",
        ""position"" : 84054985,
        ""clustering"" : [ ""2018-08-11 04:00:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-08-11T04:01:15.708470Z"" },
        ""cells"" : [
          { ""name"" : ""event_reception_time"", ""value"" : ""2018-08-11 04:01:15.703Z"" },
          { ""name"" : ""quality"", ""value"" : 100.0 },
          { ""name"" : ""value"", ""value"" : 372654.53 }
        ]
      },
      {
        ""type"" : ""range_tombstone_bound"",
        ""end"" : {
          ""type"" : ""inclusive"",
          ""deletion_info"" : { ""marked_deleted"" : ""2018-06-28T17:03:08.555918Z"", ""local_delete_time"" : ""2018-06-28T17:03:08Z"" }
        }
      }
    ]
  }
{noformat}

* SSTable 2 (I've truncated NOTHING)

{noformat}
mc-35045-big-Data.json 
  {
    ""partition"" : {
      ""key"" : [ ""a08b8042-6337-11e8-b52b-42010afa015a"", ""2018"" ],
      ""position"" : 194666614
    },
    ""rows"" : [
      {
        ""type"" : ""range_tombstone_bound"",
        ""start"" : {
          ""type"" : ""inclusive"",
          ""clustering"" : [ ""2018-07-19 04:00:00.000Z"" ],
          ""deletion_info"" : { ""marked_deleted"" : ""2018-09-16T13:14:14.634113Z"", ""local_delete_time"" : ""2018-09-16T13:14:14Z"" }
        }
      },
      {
        ""type"" : ""range_tombstone_bound"",
        ""end"" : {
          ""type"" : ""inclusive"",
          ""deletion_info"" : { ""marked_deleted"" : ""2018-09-16T13:14:14.634113Z"", ""local_delete_time"" : ""2018-09-16T13:14:14Z"" }
        }
      }
    ]
  }
{noformat}

* SSTable 3 contains rows with dates *before ""2018-06-03 18:45:00.000Z""* with no tombstone information so not presented here.

From node 6 (the ""good"" node) there are also 3 SSTables with that partition key:

* SSTable 1 below (I've truncated NOTHING):

{noformat}
mc-2763209-big-Data.json 
  {
    ""partition"" : {
      ""key"" : [ ""a08b8042-6337-11e8-b52b-42010afa015a"", ""2018"" ],
      ""position"" : 101207516
    },
    ""rows"" : [
      {
        ""type"" : ""range_tombstone_bound"",
        ""start"" : {
          ""type"" : ""inclusive"",
          ""clustering"" : [ ""2018-07-19 04:00:00.000Z"" ],
          ""deletion_info"" : { ""marked_deleted"" : ""2018-09-16T08:37:18.429276Z"", ""local_delete_time"" : ""2018-09-16T08:37:18Z"" }
        }
      },
      {
        ""type"" : ""range_tombstone_bound"",
        ""end"" : {
          ""type"" : ""inclusive"",
          ""deletion_info"" : { ""marked_deleted"" : ""2018-09-16T08:37:18.429276Z"", ""local_delete_time"" : ""2018-09-16T08:37:18Z"" }
        }
      }
    ]
  }
{noformat}

* SSTable 2 below (I've truncated NOTHING):

{noformat}
mc-2763214-big-Data.json 
  {
    ""partition"" : {
      ""key"" : [ ""a08b8042-6337-11e8-b52b-42010afa015a"", ""2018"" ],
      ""position"" : 191837790
    },
    ""rows"" : [
      {
        ""type"" : ""range_tombstone_bound"",
        ""start"" : {
          ""type"" : ""inclusive"",
          ""clustering"" : [ ""2018-07-19 04:00:00.000Z"" ],
          ""deletion_info"" : { ""marked_deleted"" : ""2018-09-16T13:14:14.634113Z"", ""local_delete_time"" : ""2018-09-16T13:14:14Z"" }
        }
      },
      {
        ""type"" : ""range_tombstone_bound"",
        ""end"" : {
          ""type"" : ""inclusive"",
          ""deletion_info"" : { ""marked_deleted"" : ""2018-09-16T13:14:14.634113Z"", ""local_delete_time"" : ""2018-09-16T13:14:14Z"" }
        }
      }
    ]
  }
{noformat}

* SSTable 3 contains rows with dates *up to ""2018-08-17 13:30:00.000Z""* with no tombstone information so not presented here.
;;;","18/Sep/18 12:18;aleksey;Thanks. At first glance, individually these dumps seem normal. So this could be a bug in reading/merging, or even a bug in RT validation logic.

I'll do my best to investigate further this week.;;;","18/Sep/18 12:30;aleksey;[~mitsis] JIRA no longer displays email addresses (presumably due to GDPR). If you could maybe share those sstables alongside schema with me (privately and confidentially) please contact me at aleksey@apache.org. Having the sstables will go a long way with helping to reproduce locally and determine what the problem is.;;;","21/Sep/18 16:38;aleksey;Quick update: the bug is legit, the problem lies in {{PurgeFunction}} applied to the merged iterator, before the filtering step. Its logic to partially purge a range tombstone is broken ({{PurgeFunction.applyToMarker()}}), or, rather, what's broken are {{RangeTombstoneBoundaryMarker.createCorrespondingCloseMarker()}} and {{RangeTombstoneBoundaryMarker.createCorrespondingOpenMarker()}} methods - in case of reverse iteration they don't swap the deletion times, leading to creation of a range tombstone marker with an invalid deletion time when reading in DESC order.

The patch is a trivial two-liner, but I'll need to finish unit tests and complete analysis of worst-case impact of the issue in 3.11.2 and corresponding 3.0 versions before {{RTBoundValidator}} introduction.;;;","24/Sep/18 15:34;aleksey;3.0: [code|https://github.com/iamaleksey/cassandra/commits/14672-3.0], [utests|https://circleci.com/gh/iamaleksey/cassandra/771], [dtests|https://circleci.com/gh/iamaleksey/cassandra/789]
3.11: [code|https://github.com/iamaleksey/cassandra/commits/14672-3.11], [utests|https://circleci.com/gh/iamaleksey/cassandra/772], [dtests|https://circleci.com/gh/iamaleksey/cassandra/784]
4.0: [code|https://github.com/iamaleksey/cassandra/commits/14672-4.0], [utests|https://circleci.com/gh/iamaleksey/cassandra/775], [dtests|https://circleci.com/gh/iamaleksey/cassandra/786];;;","24/Sep/18 15:39;aleksey;Will rerun dtests once the underlying issue with the yaml has been taken care of, but it's otherwise ready to review.;;;","24/Sep/18 21:57;aleksey;[~bdeggleston] CI's fine now;;;","25/Sep/18 16:02;bdeggleston;+1;;;","25/Sep/18 16:48;aleksey;Thanks, committed to 3.0 as [d496dca6729853ece49d68c4837fed35149c95d0|https://github.com/apache/cassandra/commit/d496dca6729853ece49d68c4837fed35149c95d0] and merged upwards with 3.11 and 4.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle failures in upgradesstables/cleanup/relocate,CASSANDRA-14657,13179999,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,20/Aug/18 16:29,15/May/20 08:05,13/Jul/23 08:37,14/Sep/18 06:46,3.0.18,3.11.4,4.0,4.0-alpha1,,,Local/SSTable,,,,0,,,,"If a compaction in {{parallelAllSSTableOperation}} throws exception, all current transactions are closed, this can make us close a transaction that has not yet finished (since we can run many of these compactions in parallel). This causes this error:
{code}
java.lang.IllegalStateException: Cannot prepare to commit unless IN_PROGRESS; state is ABORTED
{code}
and this can get the leveled manifest (if running LCS) in a bad state causing this error message:
{code}
Could not acquire references for compacting SSTables ...
{code}",,benedict,cscotta,jjirsa,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,Challenging,Adhoc Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 14 06:46:50 UTC 2018,,,,,,,,,,,"0|i3x867:",9223372036854775807,,,,,,,,,benedict,,benedict,,,Normal,,,,,,,,,,,,,,,,,,,"20/Aug/18 16:32;marcuse;patch here: https://github.com/krummas/cassandra/commits/marcuse/14657

needs a couple of tests, will add soon;;;","05/Sep/18 12:52;marcuse;and dtest to reproduce: https://github.com/krummas/cassandra-dtest/commits/marcuse/14657;;;","13/Sep/18 11:41;marcuse;dtest run against the branch above:
https://circleci.com/workflow-run/315d4f3d-18b7-4af2-aa46-ef6a2a57d37e;;;","13/Sep/18 12:13;benedict;LGTM.  Do we think it's maybe worth cancelling the existing operations?  It looks like there's unfortunately no trivial way of doing it, as we don't currently check for {{Thread.isInterrupted}} when checking {{isStopRequested}}, so perhaps not worth worrying about.  Does seem a shame to leave all that work on going after an exception, but perhaps we should simply file a follow-up ticket.;;;","13/Sep/18 12:16;marcuse;[~benedict] yeah I considered that but if we run for example `upgradesstables` it would probably be more wasteful to cancel the potentially successful upgrades;;;","13/Sep/18 12:27;benedict;I guess we could specify per operation if it's valuable, but this all seems out of scope for this ticket.  Ship it, +1.;;;","14/Sep/18 06:46;marcuse;committed as {{9be437064f5348fe7f8fc6665b747ad751699f49}} to 3.0 and merged up, thanks!

all test results:
3.0: https://circleci.com/workflow-run/315d4f3d-18b7-4af2-aa46-ef6a2a57d37e
3.11: https://circleci.com/workflow-run/b14718fb-474f-4a90-bf70-98af8df49820
trunk: https://circleci.com/workflow-run/ac121b2d-0f81-4198-b20c-3fd5c3bf1567;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No longer possible to specify cassandra_dir via pytest.ini on cassandra-dtest,CASSANDRA-14651,13179398,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,pauloricardomg,pauloricardomg,pauloricardomg,16/Aug/18 16:34,29/Jun/21 15:52,13/Jul/23 08:37,29/Jun/21 15:52,,,,,,,Test/dtest/python,,,,0,,,,"It seems like ability to specify {{cassandra_dir}} via {{pytest.init}}, as [stated on the doc|https://github.com/apache/cassandra-dtest/blame/master/README.md#L79] was lost after CASSANDRA-14449. We should either get it back or remove it from the doc.",,aweisberg,jwest,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 29 15:52:35 UTC 2021,,,,,,,,,,,"0|i3x4gv:",9223372036854775807,,,,,,,,,jwest,,jwest,,,Low,,,,,,,,,,,,,,,,,,,"16/Aug/18 17:02;pauloricardomg;trivial patch adding back ability to specify cassandra dir via cassandra_dir pytest.ini property: [branch|http://www.github.com/pauloricardomg/cassandra-dtest/tree/CASSANDRA-14651]

mind taking a look [~aweisberg] [~jrwest] ?;;;","16/Aug/18 17:59;jwest;[~pauloricardomg] I believe it was CASSANDRA-14134 that intentionally removed {{ini}} support. That said, I'm not personally against it, especially since its documented behavior. I will review. ;;;","16/Aug/18 18:04;pauloricardomg;bq.  I believe it was CASSANDRA-14134 that intentionally removed ini support. 

oh right, sorry for the confusion.

one downside of supporting {{pytest.ini}} as currently is, is that if you make any custom changes it will show up in the git diff.. I wonder if we should turn that into a template file instead.

btw, just ninja-ed adding {{.pytest_cache}} on .gitignore to prevent it from being added by mistake to the repo.;;;","17/Aug/18 17:07;jwest;On the code, the only comment I have, which is a very minor/optional nit, is it would be nice to encapsulate the fetching of cassandra_dir in a method. {{.pytest_cache}} also seems like a good thing to put in {{.gitignore}}. Its in [Github's official one for Python|https://github.com/github/gitignore/commit/f651f0d3eef062a8592e017a194e703d93f3e5c9].;;;","29/Jun/21 15:52;brandon.williams;I removed mention of cassandra_dir in pytest.ini [here|https://github.com/apache/cassandra-dtest/commit/e362f60996d39d253dbc765c13eb8d8166624cff];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Index summaries fail when their size gets > 2G and use more space than necessary,CASSANDRA-14649,13179350,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,blambov,blambov,blambov,16/Aug/18 13:13,07/Mar/23 11:52,13/Jul/23 08:37,21/Aug/18 08:58,2.2.14,3.0.18,3.11.4,4.0,4.0-alpha1,,,,,,0,,,,"After building a summary, {{IndexSummaryBuilder}} tries to trim the memory writers by calling {{SafeMemoryWriter.setCapacity(capacity())}}. Instead of trimming, this ends up allocating at least as much extra space and failing the {{Buffer.position()}} call when the size is greater than {{Integer.MAX_VALUE}}.",,benedict,blambov,jeromatron,jjirsa,jjordan,stefania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15084,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,blambov,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 21 08:58:39 UTC 2018,,,,,,,,,,,"0|i3x467:",9223372036854775807,3.0.x,3.11.x,5.0,,,,,,benedict,,benedict,,,Normal,,2.2.0,,,,,,,,,,,,,,,,,"16/Aug/18 13:24;blambov;Patch uploaded here: https://github.com/blambov/cassandra/tree/14649;;;","16/Aug/18 14:09;benedict;I think this patch is also broken.

Previously, the logical trim invoked {{setCapacity(length())}}, which I can see is buggy for a size > 2GiB (but is otherwise consistent).  Now, it seems to be invoking {{setCapacity(capacity())}}, which is surely a no-op?

It seems that there's a bunch of bugs here, and that really we should be:
# fix {{length}} to work for sizes > 2GiB
# implement {{trim}} as {{resizeTo(length())}}
# rename {{reallocate}} to something like {{ensureCapacity}}, to avoid this kind of misuse mistake in future

;;;","16/Aug/18 14:45;blambov;Right, I messed it up too... I can't see any problems with {{length}}, though.

Updated patch to address the other comments and also added size checking to the test.;;;","16/Aug/18 16:19;benedict;You're right, I was just assuming {{length()}} was inherited from {{DataOutputBuffer}}

I realise {{ensureCapacity}} was a terrible suggestion, but {{ensureHeadroom}} might be clearer than {{expandToFit}} since it might be that you're trying to fit N total, not N extra.

But no strong feeling, and +1 either way.;;;","16/Aug/18 19:18;jjirsa;Safe to relate to CASSANDRA-12014 (haven't read the patch, just sounds familiar)? 
;;;","20/Aug/18 14:36;blambov;CircleCI doesn't seem to like the new test but AFAICT is otherwise fine: [2.2|https://circleci.com/gh/blambov/workflows/cassandra/tree/14649-2.2] [3.0|https://circleci.com/gh/blambov/workflows/cassandra/tree/14649-3.0] [3.11|https://circleci.com/gh/blambov/workflows/cassandra/tree/14649-3.11] [trunk|https://circleci.com/gh/blambov/workflows/cassandra/tree/14649-trunk]

Should I remove the >2G test, or is there something I need to set up to be able to run tests needing more memory?;;;","20/Aug/18 15:36;benedict;I think you need to set yourself up on a CircleCI account that supports larger instances, then modify the .circleci/config.yml

[For example|https://github.com/belliottsmith/cassandra/commit/b1cbd819274e3095f348402bca257ad4e6765f22];;;","21/Aug/18 08:27;blambov;I realized we still have a problem if the size grows by over 2G, i.e. if it becomes >4G and needs to grow. Pushed another commit to fix and test this and limit the test size if there isn't enough memory: [new commit|https://github.com/blambov/cassandra/commit/65798672eff79bd1c97b960ed965f0e908f6c23e] [branch|https://github.com/blambov/cassandra/tree/14649-trunk] [test|https://circleci.com/gh/blambov/workflows/cassandra/tree/14649-trunk];;;","21/Aug/18 08:50;benedict;+1

I do wonder if we should revisit requiring linear memory for all of this, but really we should probably instead revisit if such huge sstables are a good idea.;;;","21/Aug/18 08:58;blambov;Committed as [49adbe7e0f0c8a83f3b843b65612528498b5c9a5|https://github.com/apache/cassandra/commit/49adbe7e0f0c8a83f3b843b65612528498b5c9a5].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading cardinality from Statistics.db failed,CASSANDRA-14647,13179321,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,n.v.harikrishna,nezdali,nezdali,16/Aug/18 10:26,15/May/20 08:02,13/Jul/23 08:37,09/May/19 12:37,4.0,4.0-alpha1,,,,,Observability/Metrics,,,,0,,,,"There is some issue with sstable metadata which is visible in system.log, the messages says:
{noformat}
WARN  [Thread-6] 2018-07-25 07:12:47,928 SSTableReader.java:249 - Reading cardinality from Statistics.db failed for /opt/data/disk5/data/keyspace/table/mc-big-Data.db.{noformat}
Although there is no such file. 

The message has appeared after i've changed the compaction strategy from SizeTiered to Leveled. Compaction strategy has been changed region by region (total 3 regions) and it has coincided with the double client write traffic increase.
 I have tried to run nodetool scrub to rebuilt the sstable, but that does not fix the issue.

So very hard to define the steps to reproduce, probably it will be:
 # run stress tool with write traffic
 # under load change compaction strategy from SireTiered to Leveled for the bunch of hosts
 # add more write traffic

Reading the code it is said that if this metadata is broken, then ""estimating the keys will be done using index summary"". 
 [https://github.com/apache/cassandra/blob/cassandra-3.0.17/src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java#L247]
  ","Clients are doing only writes with Local One, cluster consist of 3 regions with RF3.
Storage is configured wth jbod/XFS on 10 x 1Tb disks
IOPS limit for each disk 500 (total 5000 iops)
Bandwith for each disk 60mb/s (600 total)
OS is Debian linux.",djoshi,marcuse,n.v.harikrishna,nezdali,rha,spod,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/19 09:33;n.v.harikrishna;14647-trunk-1.patch;https://issues.apache.org/jira/secure/attachment/12957234/14647-trunk-1.patch","16/Aug/18 08:31;nezdali;cassandra_compaction_pending_tasks_7days.png;https://issues.apache.org/jira/secure/attachment/12935840/cassandra_compaction_pending_tasks_7days.png",,,,,,,,,,,,2.0,n.v.harikrishna,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 09 12:37:08 UTC 2019,,,,,,,,,,,"0|i3x3zr:",9223372036854775807,3.0.15,3.0.17,,,,,,,marcuse,,marcuse,,,Low,,2.1 beta1,,,,,,,,,,,,adds test case,,,,,"16/Aug/18 22:17;djoshi;Hey Vitali, thanks for the report. [~krummas] any idea what is going on here? Would changing compaction strategies cause this issue?;;;","17/Aug/18 05:17;marcuse;[~nezdali] could you post logs?;;;","17/Aug/18 13:15;rha;This is not due to STCS -> LCS. I have the same behavior on one cluster with LCS and heavy writes. STCS has never been configured on it.;;;","29/Aug/18 14:26;marcuse;Looks like this can happen when the table metric `EstimatedPartitionCount` is [queried |https://github.com/apache/cassandra/blob/d049c6b9b4af4f663aac2bf90d860c3b0c20684a/src/java/org/apache/cassandra/metrics/TableMetrics.java#L307] - it grabs the CANONICAL sstables without referencing them, so if there are very many sstables some might get compacted away while calculating the partition count and we get this warning

If this is the case it is not really a problem (other than the annoying warn message in the log files)

[~rha] could you verify that you are querying this metric?

[~rha] / [~nezdali] could you pause querying this metric and check if the error stops appearing?

Thanks for providing the logs over email btw [~nezdali];;;","29/Aug/18 14:37;nezdali;Thanks Marcus. Stopped quering EstimatedPartitionCount metric from on 1 node.;;;","30/Aug/18 13:45;nezdali;Unfortunately this did no help, the failed cardinality message is still in the log.;;;","31/Aug/18 15:54;rha;I do query partition count through Datadog JMX agent, so it happens all the time. I'll try to disabled it - although it didn't work for [~nezdali];;;","04/Sep/18 16:44;rha;After removing {{EstimatedPartitionCount}} from Datadog's cassandra.yaml on one node, I don't see any warning for 4 days.

[~nezdali] can you double check that your change was effective (e.g. monitoring service restarted, etc.)? ;;;","04/Sep/18 18:03;marcuse;there is an alias for the metric called {{EstimatedRowCount}} which should also be disabled

I'll work on a fix;;;","05/Sep/18 14:44;nezdali;Tested again, and there are no more warning in the log for the whole day. Turned out that monitoring system is querying all keys via jmx and then filters them based on the configuration file.;;;","01/Feb/19 09:34;n.v.harikrishna;[~krummas] I have created a patch for this and uploaded to this ticket  [^14647-trunk-1.patch];;;","25/Feb/19 09:07;marcuse;The {{TableMetrics}} change looks good to me (grabbing refs to the sstables before checking key count), but I don't think we need the {{SSTableReader.refAndGetApproximateKeyCount}} part - in the cases this is called we always already have a ref to the sstable.;;;","01/Mar/19 15:14;n.v.harikrishna;[~krummas] Updated the patch. 
||Branch||CircleCI||
|[14647-trunk|https://github.com/apache/cassandra/compare/trunk...nvharikrishna:14647-trunk]|[link|https://circleci.com/gh/nvharikrishna/cassandra/3#tests/containers/2]|;;;","09/May/19 12:34;marcuse;+1

re-ran the tests and failures look unrelated;

https://circleci.com/workflow-run/264bb5cb-fd18-4ea2-8b1d-768d9ab96d94;;;","09/May/19 12:37;marcuse;sorry for the delay on this - committed as {{ded62076e7fdfd1cfdcf96447489ea607ca796a0}} to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
built_views entries are not removed after dropping keyspace,CASSANDRA-14646,13179260,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasonstack,jasonstack,jasonstack,16/Aug/18 06:03,15/May/20 08:04,13/Jul/23 08:37,16/Aug/18 17:08,4.0,4.0-alpha1,,,,,Feature/Materialized Views,Legacy/Distributed Metadata,,,0,,,,"If we restore view schema after dropping keyspace, view build won't be triggered because it was marked as SUCCESS in {{built_views}} table.

| patch | CI | 
| [trunk|https://github.com/jasonstack/cassandra/commits/mv_drop_ks] | [utest|https://circleci.com/gh/jasonstack/cassandra/739] |
| [dtest|https://github.com/apache/cassandra-dtest/pull/36]|",,jasonstack,jeromatron,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasonstack,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 18 05:19:44 UTC 2018,,,,,,,,,,,"0|i3x3m7:",9223372036854775807,,,,,,,,,pauloricardomg,,pauloricardomg,,,Normal,,,,,,,,,,,,,,,,,,,"16/Aug/18 17:08;pauloricardomg;LGTM, commited dtest as {{e426ce1daa1d52983b8823388e568e5097254c0c}} to master and patch as {{d3e6891ec33a6e65cf383ad346c452293cfe50ec}} to trunk.

ftr, dtest looks good on private CI and also double checked locally.

Thanks!;;;","18/Aug/18 05:19;jasonstack;Thanks for reviewing;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Column result order can change in 'SELECT *' results when upgrading from 2.1 to 3.0 causing response corruption for queries using prepared statements when static columns are used,CASSANDRA-14638,13178327,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,andrew.tolbert,andrew.tolbert,10/Aug/18 17:13,15/May/20 08:02,13/Jul/23 08:37,16/Aug/18 14:50,3.0.18,3.11.4,4.0,4.0-alpha1,,,CQL/Interpreter,,,,0,,,,"When performing an upgrade from C* 2.1.20 to 3.0.17 I observed that the order of columns returned from a 'SELECT *' query changes, particularly when static columns are involved.

This may not seem like that much of a problem, however if using Prepared Statements, any clients that remain connected during the upgrade may encounter issues consuming results from these queries, as data is reordered and the client not aware of it.  The result definition is sent in the original prepared statement response, so if order changes the client has no way of knowing (until C* 4.0 via CASSANDRA-10786) without re-preparing, which is non-trivial as most client drivers cache prepared statements.

This could lead to reading the wrong values for columns, which could result in some kind of deserialization exception or if the data types of the switched columns are compatible, the wrong values.  This happens even if the client attempts to retrieve a column value by name (i.e. row.getInt(""colx"")).

Unfortunately I don't think there is an easy fix for this.  If the order was changed back to the previous format, you risk issues for users upgrading from older 3.0 version.  I think it would be nice to add a note in the NEWS file in the 3.0 upgrade section that describes this issue, and how to work around it (specify all column names of interest explicitly in query).

Example schema and code to reproduce:

 
{noformat}
create keyspace ks with replication = {'class': 'SimpleStrategy', 'replication_factor': 1};

create table ks.tbl (p0 text,
  p1 text,
  m map<text, text> static,
  t text,
  u text static,
  primary key (p0, p1)
);

insert into ks.tbl (p0, p1, m, t, u) values ('p0', 'p1', { 'm0' : 'm1' }, 't', 'u');{noformat}
 

When querying with 2.1 you'll observe the following order via cqlsh:
{noformat}
 p0 | p1 | m            | u | t
----+----+--------------+---+---
 p0 | p1 | {'m0': 'm1'} | u | t{noformat}
 

With 3.0, observe that u and m are transposed:

 
{noformat}
 p0 | p1 | u | m            | t
----+----+---+--------------+---
 p0 | p1 | u | {'m0': 'm1'} | t{noformat}
 

 
{code:java}
import com.datastax.driver.core.BoundStatement;
import com.datastax.driver.core.Cluster;
import com.datastax.driver.core.ColumnDefinitions;
import com.datastax.driver.core.PreparedStatement;
import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Row;
import com.datastax.driver.core.Session;
import com.google.common.util.concurrent.Uninterruptibles;
import java.util.concurrent.TimeUnit;

public class LiveUpgradeTest {

  public static void main(String args[]) {
    Cluster cluster = Cluster.builder().addContactPoints(""127.0.0.1"").build();
    try {
      Session session = cluster.connect();
      PreparedStatement p = session.prepare(""SELECT * from ks.tbl"");

      BoundStatement bs = p.bind();

      // continually query every 30 seconds
      while (true) {
        try {
          ResultSet r = session.execute(bs);
          Row row = r.one();
          int i = 0;
          // iterate over the result metadata in order printing the
          // index, name, type, and length of the first row of data.
          for (ColumnDefinitions.Definition d : r.getColumnDefinitions()) {
            System.out.println(
                i++
                    + "": ""
                    + d.getName()
                    + "" -> ""
                    + d.getType()
                    + "" -> val = ""
                    + row.getBytesUnsafe(d.getName()).array().length);
          }
        } catch (Throwable t) {
          t.printStackTrace();
        } finally {
          Uninterruptibles.sleepUninterruptibly(30, TimeUnit.SECONDS);
        }
      }
    } finally {
      cluster.close();
    }
  }
}
{code}
To reproduce, set up a cluster, the schema, and run this script.  Then upgrade the cluster to 3.0.17 (with ccm, ccm stop; ccm node1 setdir -v 3.0.17; ccm start works) and observe after the client is able to reconnect that the results are in a different order.  i.e.:

 

With 2.x:

 
{noformat}
0: p0 -> varchar -> val = 2
1: p1 -> varchar -> val = 2
2: m -> map<varchar, varchar> -> val = 16
3: u -> varchar -> val = 1
4: t -> varchar -> val = 1{noformat}
 

With 3.x:

 
{noformat}
0: p0 -> varchar -> val = 2
1: p1 -> varchar -> val = 2
2: m -> map<varchar, varchar> -> val = 1
3: u -> varchar -> val = 16 (<-- the data for 'm' is now at index 3)
4: t -> varchar -> val = 1{noformat}
 

 

 

 ",Single C* node ccm cluster upgraded from C* 2.1.20 to 3.0.17,aleksey,andrew.tolbert,bcicchi,benedict,jeromatron,jjirsa,jjordan,slebresne,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,Correctness -> Transient Incorrect Response,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 16 14:49:42 UTC 2018,,,,,,,,,,,"0|i3wxvj:",9223372036854775807,,,,,,,,,benedict,,benedict,,,Normal,,3.0.0,,,,,,,,,,,,,,,,,"15/Aug/18 16:12;aleksey;Unfortunately this is an accidental, lower level bug in 3.0, that I believe should be fixed in the next minor.

The internal contract for wildcard order in 2.1, and, really, everywhere since introduction of static columns is the following:
1. Partition key columns in their positional order
2. Clustering columns in their positional order
3. All static columns, ordered alphabetically by name, irregardless of whether they are simple or complex
4. All regular columns, ordered alphabetically by name, irregardless of whether they are simple or complex

To illustrate, let's say we have a table defined as
{code}
CREATE TABLE ks.tbl (p0 text, p1 text, a set<text>, b set<text> static, c text, d text static, e set<text>, f set<text> static, g text, h text static, i set<text>, j set<text> static, PRIMARY KEY (p0, p1));
{code}
And one inserted row:
{code}
INSERT INTO ks.tbl (p0, p1, a, b, c, d, e, f, g, h, i, j) VALUES ('p0', 'p1', {'a'}, {'b'}, 'c', 'd', {'e'}, {'f'}, 'g', 'h', {'i'}, {'j'});
{code}

In 2.1, performing a {{SELECT * FROM ks.tbl;}} query would return the following result set:
{code}
 p0 | p1 | b     | d | f     | h | j     | a     | c | e     | g | i
----+----+-------+---+-------+---+-------+-------+---+-------+---+-------
 p0 | p1 | {'b'} | d | {'f'} | h | {'j'} | {'a'} | c | {'e'} | g | {'i'}
{code}
3.0, in contrast, would return the following result set:
{code}
 p0 | p1 | d | h | b     | f     | j     | a     | c | e     | g | i
----+----+---+---+-------+-------+-------+-------+---+-------+---+-------
 p0 | p1 | d | h | {'b'} | {'f'} | {'j'} | {'a'} | c | {'e'} | g | {'i'}
{code}

Both correctly return {{PRIMARY KEY}} columns in their defined order first, then all the static columns (although 3.0 order for them is all wrong), followed by all regular columns in alphabetic order, correct in both 2.1 and 3.0.

What 3.0 does incorrectly is ordering the static columns it returns. Instead of merging simple and complex static columns, we are getting all simple static columns first, followed by all complex static columns.

The bug is deep in {{Columns}} class, more specifically in {{findFirstComplexIdx()}} method. To find the first complex column index we are using a surrogate {{ColumnDefinition}} value {{FIRST_COMPLEX}}, which is of kind {{REGULAR}} for {{Columns}} of both {{REGULAR}} and {{STATIC}} kinds. This quite clearly breaks {{Columns}} of kind {{STATIC}}, making the container believe that all of its columns are {{COMPLEX}}.;;;","15/Aug/18 16:14;aleksey;Proposed work-in-progress fix (test coverage pending) that addresses the underlying issue can be found [here|https://github.com/iamaleksey/cassandra/commits/14638-3.0], with CI upcoming [here|https://circleci.com/workflow-run/55195e85-6d1a-4864-8c0f-ca656b843f56].;;;","15/Aug/18 16:22;aleksey;bq. Unfortunately I don't think there is an easy fix for this.  If the order was changed back to the previous format, you risk issues for users upgrading from older 3.0 version.

This is true. That said, there are huge fleets still on 2.1 and 2.2 in the wild, so leaving this unfixed is also a choice that puts some users in potential danger.

Our internal contract is quite clear, and is quite clearly broken. I would prefer to address it here and now, rather than maintaining broken behaviour just for the sake of it. For the record, here are the relevant bits:
{code}
    /**
     * An iterator that returns the columns of this object in ""select"" order (that
     * is in global alphabetical order, where the ""normal"" iterator returns simple
     * columns first and the complex second).
     *
     * @return an iterator returning columns in alphabetical order.
     */
    public Iterator<ColumnDefinition> selectOrderIterator()
    {
        // In wildcard selection, we want to return all columns in alphabetical order,
        // irregarding of whether they are complex or not
        return Iterators.mergeSorted(ImmutableList.of(complexColumns(), simpleColumns()),
                                     (s, c) ->
                                     {
                                         assert !s.kind.isPrimaryKeyKind();
                                         return s.name.bytes.compareTo(c.name.bytes);
                                     });
    }
{code}

The contact has been defined this way since 2.0, when static columns where introduced, and was broken relatively recently when 3.0 came out. I would argue that the correct thing to do is to fix it in the next 3.0 and 3.11 minors, with a loud NEWS.txt entry and an email to dev@ and users@ warning users who are both on 2.1/2.2 and 3.0+ and up about the current breakage and the upcoming fix.;;;","15/Aug/18 16:51;aleksey;[~slebresne] I assume you may have an opinion on this. If so, please share.;;;","15/Aug/18 17:03;jjordan;I would think we should be able to do a protocol version bump or something and know ""when talking to X put them in this order, when talking to Y put them in this other order"", such that we do not break things with current 3.0 and 3.11 nodes on upgrade?;;;","15/Aug/18 17:05;andrew.tolbert;{quote}
What 3.0 does incorrectly is ordering the static columns it returns. Instead of merging simple and complex static columns, we are getting all simple static columns first, followed by all complex static columns.
{quote}

Ah, that explains it!  Thanks for looking into this.  I was trying to understand why it was ordering the static columns the way it was, that makes sense.

{quote}
This is true. That said, there are huge fleets still on 2.1 and 2.2 in the wild, so leaving this unfixed is also a choice that puts some users in potential danger.
{quote}

That's a great point.  We need to balance the risk between users upgrading from 2.x to those upgrading from previous 3.x versions.  I may not have the best sense of which should win out, but I think I agree that if we can make it consistent and work the way it did pre-3.0, that is probably for the best long term.;;;","15/Aug/18 17:24;benedict;[~jjordan]:  there is no well defined behaviour for a client version, only for a C* version, so unless we reject all connections from older client versions (which would be unacceptable) I don't think this buys us much.  

The only properly backward compatible solution I can imagine would be extremely costly, i.e. on startup, fetch all prepared statements from other nodes in the cluster, and persist the UUID and the version it was prepared against, and forever more treat this statement as that select order.  Even this would not be perfect, as there's the possibility of a statement falling out of cache.

Another option would be to roll the hash of all prepared statements, just once, so that submitting the query to an upgraded node would return NOT_PREPARED, but this would cause a lot of re-prepare traffic during upgrade and I'm not sure all clients would handle this gracefully (they may assume, for instance, that the UUID does not change when the statement is re-prepared).

I don't think there's a neat solution to this, though I'm happy to be proven wrong.;;;","15/Aug/18 17:26;jjirsa;We're talking about this in the context of prepared statements, but even if you roll the hash to force a re-prepare, someone who's fetching by index will still be surprised if we re-order the index, and that's protocol and driver independent.;;;","15/Aug/18 17:32;benedict;Very true, and they're basically a lost cause.  The only way to fix for them would be to cement the column order at the point of upgrade for all queries in perpetuity (i.e., if 2.1->fixed 3.0, use original; if broken 3.0->fixed 3.0, use new broken order), and even this wouldn't fix applications that were running through an upgrade from 2.1 -> broken 3.0.  It would also mean docs couldn't say what the column order was defined as!

All in all, I think the best option is to ""fix the bug"" and leave it at that.  It's dissatisfying, but it does put a clean end to the problem.;;;","15/Aug/18 17:53;jjordan;I was just thinking for the internode part. Back to clients I can’t think of a good way either.;;;","15/Aug/18 18:31;aleksey;bq. I was just thinking for the internode part.

Luckily internode messaging is not affected by this bug.;;;","15/Aug/18 19:13;aleksey;Branches with the fix for [3.0|https://github.com/iamaleksey/cassandra/commits/14638-3.0], [3.11|https://github.com/iamaleksey/cassandra/commits/14638-3.11], and [4.0|https://github.com/iamaleksey/cassandra/commits/14638-4.0]. CI, respectively: [3.0|https://circleci.com/workflow-run/6be0f2b2-292c-4c9e-81fe-a1b90a86421e], [3.11|https://circleci.com/workflow-run/a771039b-aa8f-4eea-a268-822c759a1c8e], and [4.0|https://circleci.com/workflow-run/ce2620d7-2317-43ef-8f96-90d4f53ee175].;;;","16/Aug/18 07:28;slebresne;I agree both that this is unfortunate, and that I don't see any reasonable backward compatible option. And I'm also in favor or restoring the pre-3.0 order, partly because this is indeed the order we've used for the longest, but mainly because it doesn't make sense to have inconsistent ordering for static and normal columns.

I would also argue that relying that strongly on the order of columns of a {{SELECT *}} is a bad idea in the first place, and to the best of my knowledge, no official documentation has ever pretended that the order was guaranteed (don't get me wrong, this is a weak argument in that our documentation are historically bad and if we were to guarantee only what is documented, we wouldn't be guaranteeing much; still, it would clearly have been worth if we had ever documented the order as guaranteed), so hopefully this won't break too many users. In fact, on top of the big fat NEWS entry already mentioned, I'd be in favor of updating the documentation to explicitly mention that the order of columns is not specified and not guaranteed to be stable, and should thus not be relied on. ;;;","16/Aug/18 10:19;benedict;The patch looks good to me, so I'd be happy to commit as-is.  

There is an alternative approach that might be simpler, or at least fewer edits; namely, to modify findFirstComplexId to auto-detect isStatic, and avoid the caller worrying about the distinction.  Since we already get the end ColumnDefinition to avoid a binary search in the case there are no complex cells, we can also extract its Kind for (almost) free.

There is an argument to be made either way, since we implicitly require that you never mix regular and static columns in one of these collections, but presently never actually impose it either semantically or via assertion.;;;","16/Aug/18 12:37;aleksey;[~benedict] That's indeed better and less invasive. Force-pushed the new version, updated CI links in-place in the previous comment.

[~slebresne] Agreed on all points. Updated documentation in the v2 of the patch.;;;","16/Aug/18 14:40;benedict;+1;;;","16/Aug/18 14:49;aleksey;Cheers, committed as [236c47e65ce95dcc7c8c75706715b5a2a88fd237|https://github.com/apache/cassandra/commit/236c47e65ce95dcc7c8c75706715b5a2a88fd237] to 3.0 and merged up to 3.11 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revert 4.0 GC alg back to CMS,CASSANDRA-14636,13178265,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasobrown,jasobrown,jasobrown,10/Aug/18 12:45,15/May/20 08:04,13/Jul/23 08:37,10/Aug/18 17:45,4.0,4.0-alpha1,,,,,,,,,0,Java11,,,"CASSANDRA-9608 accidentally swapped the default GC algorithm from CMS to G1. Until further community consensus is achieved about swapping the default alg, we should switch back to CMS.

As reported by [~rustyrazorblade] on the [dev@ ML|https://lists.apache.org/thread.html/0b30f9c84457033583e9a3e0828adc603e01f1ca03ce0816098883cc@%3Cdev.cassandra.apache.org%3E] ",,jasobrown,jeromatron,jwest,slachiewicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 10 17:45:26 UTC 2018,,,,,,,,,,,"0|i3wxhr:",9223372036854775807,,,,,,,,,jwest,,jwest,,,Normal,,,,,,,,,,,,,,,,,,,"10/Aug/18 15:55;jasobrown;Using the [previous, 3.11 settings|https://github.com/apache/cassandra/blob/cassandra-3.11/conf/jvm.options#L197] as a guide, reverting the java8 settings was simple. However, java11 was not as trivial. A naive reenabling of java8's GC settings yielded:

{noformat}
Unrecognized VM option 'UseParNewGC'                                                                                                                                                                               
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
{noformat}

{{UseParNewGC}} was deprecated in java 9 and [removed in java 10|https://bugs.openjdk.java.net/browse/JDK-8151084]. Once that flag is removed, cassandra can start and run properly. However, this is this log statement upon process start:

{noformat}
OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.                                                                          
{noformat}

For those who haven't been playing along with every gyration of the JDK, the [CMS collector is deprecated|https://bugs.openjdk.java.net/browse/JDK-8142518]. However, CMS [might not be removed|https://bugs.openjdk.java.net/browse/JDK-8163329], but might not be supported directly by the OpenJDK project.

Either way, it appears that simply removing {{UseParNewGC}} form the java 11 config gets us back to on-par with the java 8 config.

Patch available here:

||14636||
|[branch|https://github.com/jasobrown/cassandra/tree/14636]|
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14636]|
||

Failed dtests are known issues.;;;","10/Aug/18 17:10;jwest;+1. 

My understanding is {{UseParNewGC}} is implicit in Java 10+. Otherwise it might be worth adding a note about that specific change. ;;;","10/Aug/18 17:45;jasobrown;Committed as sha {{ed806594e5169458d744a06c73ec224a1f37abce}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CASSANDRA-9608 broke running Cassandra and tests in IntelliJ under Java 8,CASSANDRA-14627,13177602,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jwest,jwest,jwest,08/Aug/18 06:28,16/Apr/19 09:29,13/Jul/23 08:37,10/Aug/18 12:42,,,,,,,Legacy/Testing,,,,1,Java11,,,"CASSANDRA-9608 added a couple hard-coded options to workspace.xml that are not supported in Java 8: https://github.com/apache/cassandra/commit/6ba2fb9395226491872b41312d978a169f36fcdb#diff-59e65c5abf01f83a11989765ada76841. 

{code}
Unrecognized option: --add-exports
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
{code}

To reproduce:
1. Update to the most recent trunk
2. rm -rf .idea && ant generate-idea-files
3. Re-open the project in IntelliJ (using Java 8) and run Cassandra or a test. ",,aleksey,djoshi,jasobrown,jeromatron,jwest,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jwest,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 10 12:42:32 UTC 2018,,,,,,,,,,,"0|i3wtef:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Critical,,,,,,,,,,,,,,,,,,,"08/Aug/18 06:55;djoshi;I can repro this issue.;;;","08/Aug/18 10:13;aleksey;See CASSANDRA-14613 too.;;;","08/Aug/18 16:44;jwest;This is slightly different from CASSANDRA-14613 in that {{ide/workspace.xml}} is broken instead of {{ide/idea-iml-file.xml}} but I'm happy to dupe this to it. I do think a short term fix for this is warranted: at a minimum, breaking Java 11 in the IDE instead of Java 8.;;;","08/Aug/18 16:45;jwest;I should also note, the local workaround, in the meantime, is to manually delete the Java 11 arguments from {{ide/workspace.xml}} or from the specific IntelliJ configurations being used. ;;;","08/Aug/18 17:56;jasobrown;I propose we remove the java11 support from the idea files for now. Should probably look at the eclipse files, as well.;;;","08/Aug/18 17:58;jwest;I'll post a fix later today or tomorrow that breaks Java 11 support in IDEA and fixes Java 8. We can open a separate ticket to track fixing Java 11 separately. ;;;","09/Aug/18 16:00;jwest;[branch|https://github.com/jrwest/cassandra/tree/14627-trunk] [tests | https://circleci.com/gh/jrwest/cassandra/tree/14627-trunk];;;","10/Aug/18 12:42;jasobrown;+1. committed as sha {{d78310d53f9d00dcd26feb0ec4802a2a182fdd24}}. Thanks for the patch!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-stress write hangs with default options,CASSANDRA-14616,13175905,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jay.zhuang,cnlwsu,cnlwsu,31/Jul/18 17:57,15/May/20 08:02,13/Jul/23 08:37,07/Dec/18 03:04,3.0.18,3.11.4,4.0,4.0-alpha1,,,Tool/stress,,,,0,,,,"Cassandra stress sits there for incredibly long time after connecting to JMX. To reproduce {code}./tools/bin/cassandra-stress write{code}

If you give it a -n its not as bad which is why dtests etc dont seem to be impacted. Does not occur in 3.0 branch but does in 3.11 and trunk",,aweisberg,cnlwsu,jay.zhuang,jeromatron,mbyrd,stefania,Yarnspinner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14890,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Dec 07 03:04:21 UTC 2018,,,,,,,,,,,"0|i3wj7z:",9223372036854775807,3.11.0,4.0,,,,,,,stefania,,stefania,,,Normal,,3.11.0,,,,,,,,,,,,,,,,,"13/Aug/18 09:46;Yarnspinner;Hello, I would like to try solving this issue.

I have done some preliminary testing and it appears that it is caused by cassandra-stress waiting for uncertainty to stabilize, the trace from jstack is included below.

{quote}
java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000076d873e20> (a java.util.concurrent.CountDownLatch$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
        at org.apache.cassandra.stress.util.Uncertainty$WaitForTargetUncertainty.await(Uncertainty.java:56)
        at org.apache.cassandra.stress.util.Uncertainty.await(Uncertainty.java:85)
        at org.apache.cassandra.stress.report.StressMetrics.waitUntilConverges(StressMetrics.java:135)
        at org.apache.cassandra.stress.StressAction.run(StressAction.java:269)
        at org.apache.cassandra.stress.StressAction.warmup(StressAction.java:121)
        at org.apache.cassandra.stress.StressAction.run(StressAction.java:70)
        at org.apache.cassandra.stress.Stress.run(Stress.java:143)
        at org.apache.cassandra.stress.Stress.main(Stress.java:62)
{quote}

I also did some printlns for debugging in 3.11.
{quote}
uncertainty: NaN targetUncertainty: 0.020000 
measurements: 1 minMeasurements: 30 
measurements: 1 maxMeasurements: 200 
uncertainty: NaN targetUncertainty: 0.020000 
measurements: 2 minMeasurements: 30 
measurements: 2 maxMeasurements: 200 
...
uncertainty: NaN targetUncertainty: 0.020000 
measurements: 200 minMeasurements: 30 
measurements: 200 maxMeasurements: 200 
{quote}

In the warmup phase, the program aims for either uncertainty to fall below 0.02 with at least 30 measurements or to hit 200 measurements. It ends up waiting for 200 measurements since the uncertainty is always NaN. The same problem doesn't occur in 3.0 because the Runnable (https://github.com/apache/cassandra/blob/cassandra-3.0/tools/stress/src/org/apache/cassandra/stress/StressMetrics.java#L86) calls wakeAll after 2 iterations. However, uncertainty is still always NaN in 3.0.

 The problem arises in 3.11 and trunk as that runnable loop was refactored  into reportingLoop which waited for all 200 tries first. https://github.com/apache/cassandra/blob/cassandra-3.11/tools/stress/src/org/apache/cassandra/stress/report/StressMetrics.java#L154

Here's what it looks like for 3.0.
{quote}
Warming up WRITE with 0 iterations...
_______________________
Updated value: NaN 
uncertainty: NaN targetUncertainty: 0.020000 
measurements: 1 minMeasurements: 30 
measurements: 1 maxMeasurements: 200 
_______________________
Updated value: NaN 
uncertainty: NaN targetUncertainty: 0.020000 
measurements: 2 minMeasurements: 30 
measurements: 2 maxMeasurements: 200 
latch counted down via wakeall
wakeAll via line 123 in stressmetrics
WARNING: uncertainty mode (err<) results in uneven workload between thread runs, so should be used for high level analysis only
Running with 4 threadCount
{quote}

I think this is being caused by having 0 iterations for warmup. The number of iterations is decided at the start by 
 {{Math.min(50000, (int) (settings.command.count * 0.25)) * settings.node.nodes.size();}}. 
https://github.com/apache/cassandra/blob/cassandra-3.11/tools/stress/src/org/apache/cassandra/stress/StressAction.java#L108 . 

When {{./tools/bin/cassandra-stress write}} is called without any arguments, settings.command.count evaluates to -1 and  {{Math.min(50000, (int) (settings.command.count * 0.25)) * settings.node.nodes.size();}} evaluates to 0 so we always end up with 0 iterations. 

One proposed fix is to choose a minimum nonzerovalue for iterations in the warmup phase. Something like https://github.com/yarnspinnered/cassandra/commit/33cf059f63b56ac17a3f66869615d3d7cc52f8a9 . I tried this and it no longer hangs but I'm not sure on the exact value or if there is a better way to fix this.;;;","15/Nov/18 00:05;jay.zhuang;Hi [~Yarnspinner], the fix looks good. I had the similar fix which re-enables {{warm-up}} to 50k as before ([{{StressAction.java}}|https://github.com/apache/cassandra/commit/6a1b1f26b7174e8c9bf86a96514ab626ce2a4117#diff-fd2f2d2364937fcb1c0d73c8314f1418L90])

|Branch|uTest|dTest|
|[14890-3.0|https://github.com/cooldoger/cassandra/tree/14890-3.0]|[!https://circleci.com/gh/cooldoger/cassandra/tree/14890-3.0.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14890-3.0]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/661/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/661/]|
|[14890-3.11|https://github.com/cooldoger/cassandra/tree/14890-3.11]|[!https://circleci.com/gh/cooldoger/cassandra/tree/14890-3.11.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14890-3.11]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/662/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/662/]|
|[14890-trunk|https://github.com/cooldoger/cassandra/tree/14890-trunk]|[!https://circleci.com/gh/cooldoger/cassandra/tree/14890-trunk.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14890-trunk]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/663/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/663/]|

Here is a dTest to reproduce the problem:
|[14890|https://github.com/cooldoger/cassandra-dtest/tree/14890]|;;;","15/Nov/18 01:07;jay.zhuang;The failed the utest is because of CASSANDRA-14891;;;","15/Nov/18 02:19;stefania;[~jay.zhuang] , [~Yarnspinner] , thanks for fixing this problem and writing the test.

I checked both patches, they are both good. Perhap's Jay's approach of assuming 50,000 iterations for duration tests is slightly preferable since that was the old behavior.

 ;;;","19/Nov/18 01:58;stefania;[~Yarnspinner], [~jay.zhuang] are you OK with committing Jay's approach? I don't mind too much which approach, they are both OK, it's just a matter of picking a default value.

Jay you are a committer correct? So if Jeremey is OK committing your patch, I assume you prefer to merge it yourself?;;;","07/Dec/18 03:04;jay.zhuang;Thank you [~Stefania] for the review. Committed as [{{bbf7dac}}|https://github.com/apache/cassandra/commit/bbf7dac87cdc41bf8e138a99f630e7a827ad0d98]. The dTest is committed as [{{325ef3f}}|https://github.com/apache/cassandra-dtest/commit/325ef3fa063252e6dad88473613abbd829e8c24d].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CircleCI config has dtests enabled but not the correct resources settings,CASSANDRA-14614,13175668,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jwest,jwest,jwest,30/Jul/18 20:30,13/May/20 13:18,13/Jul/23 08:37,31/Jul/18 11:30,,,,,,,CI,Test/dtest/python,,,0,CI,,,"The commit for  -CASSANDRA-9608- enabled the {{with_dtests_jobs}} configuration in {{.circleci/config.yml}} but not the necessary env var settings. We should revert this, unless we planned to start running dtests with the correct resources on every master commit, in which case we should fix the resources.

(cc [~snazy] [~jasobrown])",,jasobrown,jwest,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jwest,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 31 11:30:14 UTC 2018,,,,,,,,,,,"0|i3whrj:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Low,,,,,,,,,,,,,,,,,,,"30/Jul/18 20:31;jwest;I'll have a patch up later this afternoon if no one wants to take this. Just need to run to a few meetings. ;;;","30/Jul/18 23:44;jwest;[branch|https://github.com/jrwest/cassandra/tree/14614-trunk]  | [tests|https://circleci.com/gh/jrwest/cassandra/tree/14614-trunk];;;","31/Jul/18 02:23;jasobrown;+1. Commit later tonight.;;;","31/Jul/18 11:30;jasobrown;Committed as sha {{e663ccd98dbb0696db0f511c5f3428c13e7f8c73}}. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ant generate-idea-files / generate-eclipse-files needs update after CASSANDRA-9608,CASSANDRA-14613,13175553,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,snazy,marcuse,marcuse,30/Jul/18 13:13,15/May/20 08:04,13/Jul/23 08:37,08/Apr/19 15:41,4.0,4.0-alpha1,,,,,Build,,,,0,,,,"{{ide/idea-iml-file.xml}} looks hard coded to include {{src/java11}} when creating the project, this should probably detect what version we are building for instead

cc [~snazy]",,aleksey,benedict,jasobrown,jeromatron,marcuse,mck,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,snazy,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 08 15:41:26 UTC 2019,,,,,,,,,,,"0|i3wh1z:",9223372036854775807,,,,,,,,,benedict,,benedict,,,Normal,,,,,,,,,,,,,,,,,,,"30/Jul/18 17:40;snazy;{quote}detect what version we are building for
{quote}
WDYM? That's in trunk - and trunk has src/java11;;;","30/Jul/18 17:44;marcuse;yeah but I don't have java11 installed on my machine, so making the idea project default to java11 is wrong;;;","30/Jul/18 17:48;benedict;{quote}yeah but I don't have java11 installed on my machine, so making the idea project default to java11 is wrong
{quote}
+1;;;","30/Jul/18 17:51;snazy;Java 11 sources are part of the source code, so it's correct to include those sources.

 ;;;","30/Jul/18 17:58;marcuse;[~snazy] since Runtime.version() is not available in java8, building from inside (and running unit tests etc) idea fails:
{code}
Error:(47, 20) java: cannot find symbol
  symbol:   method version()
  location: class java.lang.Runtime
{code}

the language level is also set wrong (to 10?) but not sure if we can set that in the files we generate;;;","30/Jul/18 18:12;snazy;As a workaround, just remove {{src/java11}} and add {{src/java8}}.

I won't have much spare time to work on this in the next 3-4 weeks.;;;","30/Jul/18 18:27;aleksey;[~snazy] I'm sorry, Robert, but this is an unacceptable answer.

I feel like a more appropriate workaround would be to revert CASSANDRA-9608 - until someone has time to do it properly.;;;","30/Jul/18 18:49;marcuse;I’ll do the simple fix tomorrow;;;","30/Jul/18 23:02;jasobrown;[~krummas] Thank you for volunteering; I will review.

bq. [~iamaleksey] I feel like a more appropriate workaround would be to revert CASSANDRA-9608 

This seems unnecessarily heavy-handed. Let's fix the known, tractable problems before throwing everything out the window.;;;","31/Jul/18 06:15;marcuse;patch [here|https://github.com/krummas/cassandra/commits/marcuse/14613] which simply defaults everything to java8 - if anyone wants to run java11 they can probably update the project themselves (language level should be 8 until we only support 11 though);;;","31/Jul/18 09:41;benedict;[~jasobrown]: [~iamaleksey] can correct me if I'm wrong, but I think the comment was not intended to suggest we actually rolled back the patch, but as a juxtaposition to the other unreasonable ""workaround"" suggested by [~snazy]

I have to agree that it's uncool to commit a patch without making any time to fix the problems it introduces for other contributors.

Thanks [~krummas] for fixing it, anyway!;;;","31/Jul/18 09:43;benedict;[~krummas]: also, I'm +1 the patch, though I think it's probably interim and perhaps doesn't close the ticket?  Ideally it would detect the version as you suggested before, but perhaps we can leave that until [~snazy] has time to investigate;;;","31/Jul/18 14:00;marcuse;ok, committed the change, leaving open until it is not needed anymore (by CASSANDRA-14607) or by having the version that detects what version we are running;;;","08/Apr/19 15:41;benedict;Closing as fixed, as work has already been committed, and further work is superceded by CASSANDRA-14607;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[dtest] read_repair_test.TestReadRepair,CASSANDRA-14603,13174884,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,jasobrown,jasobrown,26/Jul/18 15:05,16/Apr/19 09:29,13/Jul/23 08:37,30/Jul/18 08:43,2.2.13,3.0.17,,,,,Test/dtest/python,,,,0,dtest,,,"tests {{test_alter_rf_and_run_read_repair}} and {{test_read_repair_chance}} consistently fail on 3.0; the latter also fails on 2.2. I suspect it's the same cause, as the output from pytest shows the same error in the same shared function ({{check_data_on_each_replica}}):

{noformat}
            res = rows_to_list(session.execute(stmt))
            logger.debug(""Actual result: "" + str(res))
            expected = [[1, 1, 1]] if expect_fully_repaired or n == initial_replica else [[1, 1, None]]
            if res != expected:
>               raise NotRepairedException()
E               read_repair_test.NotRepairedException

read_repair_test.py:204: NotRepairedException
{noformat}",,jasobrown,jay.zhuang,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 30 08:43:54 UTC 2018,,,,,,,,,,,"0|i3wcxb:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"27/Jul/18 14:24;samt;The problem is with the tests - when I updated these for CASSANDRA-14134 I forgot that value skipping during reads was disabled until CASSANDRA-10657 re-introduced in 3.4

PR for the dtest fixes: https://github.com/apache/cassandra-dtest/pull/33
CI runs with the fix: [2.2|https://circleci.com/gh/beobal/cassandra/250], [3.0|https://circleci.com/gh/beobal/cassandra/252], [3.11|https://circleci.com/gh/beobal/cassandra/252];;;","28/Jul/18 16:14;jasobrown;+1;;;","30/Jul/18 08:43;samt;Thanks, committed to cassandra-dtest in [daa37bf|https://github.com/apache/cassandra-dtest/commit/daa37bfb56fbcf627abc7179fd5924af08bac429];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[dtest] test_sstableofflinerelevel - offline_tools_test.TestOfflineTools,CASSANDRA-14602,13174881,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,marcuse,jasobrown,jasobrown,26/Jul/18 14:59,16/Apr/19 09:29,13/Jul/23 08:37,30/Jul/18 14:18,,,,,,,Test/dtest/python,,,,0,dtest,,,"consistently failing dtest on 3.0 (no other branches). Output from pytest:

{noformat}
        output, _, rc = node1.run_sstableofflinerelevel(""keyspace1"", ""standard1"")
>       assert re.search(""L0=1"", output)
E       AssertionError: assert None
E        +  where None = <function search at 0x7f99afffbe18>('L0=1', 'New leveling: \nL0=0\nL1 10\n')
E        +    where <function search at 0x7f99afffbe18> = re.search

offline_tools_test.py:160: AssertionError
{noformat}
",,jasobrown,jay.zhuang,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 30 14:18:34 UTC 2018,,,,,,,,,,,"0|i3wcwn:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Low,,,,,,,,,,,,,,,,,,,"27/Jul/18 16:17;marcuse;patch to make sure we only get a single sstable in the first assert here: https://github.com/krummas/cassandra-dtest/commits/marcuse/14602 - ran 30 times locally and passed every time

test run against 3.0: https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F14602;;;","28/Jul/18 16:31;jasobrown;+1;;;","30/Jul/18 14:18;marcuse;committed as {{c13a78c533c9e025c6ea705fc010f67cf2224d55}}, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[dtest] test_failure_threshold_deletions - paging_test.TestPagingWithDeletions,CASSANDRA-14601,13174879,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jay.zhuang,jasobrown,jasobrown,26/Jul/18 14:56,16/Apr/19 09:29,13/Jul/23 08:37,30/Jul/18 17:30,,,,,,,Test/dtest/python,,,,0,dtest,,,"failing dtest on 3.11 only. Error output from pytest:

{noformat}
        except ReadFailure as exc:
            if supports_v5_protocol:
>               assert exc.error_code_map is not None
E               assert None is not None
E                +  where None = ReadFailure('Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 r...d 2 failures"" info={\'consistency\': \'ALL\', \'required_responses\': 2, \'received_responses\': 0, \'failures\': 2}',).error_code_map

paging_test.py:3447: AssertionError
{noformat}
",,jasobrown,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 30 17:30:20 UTC 2018,,,,,,,,,,,"0|i3wcw7:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Low,,,,,,,,,,,,,,,,,,,"29/Jul/18 04:33;jay.zhuang;The problem is because [{{supports_v5_protocol}}|https://github.com/apache/cassandra-dtest/blob/master/paging_test.py#L3422] is enabled in the test, but not in the client session: [{{dtest_setup.py:333}}|https://github.com/apache/cassandra-dtest/blob/master/dtest_setup.py#L333], so the {{exc.error_code_map}} is not set. Here is a patch to make the check consistent, please review:
|[14601|https://github.com/cooldoger/cassandra-dtest/tree/14601]|

Technically, v5_protocol is supported from {{3.10}}, but changing [{{dtest_setup.py:333}}|https://github.com/apache/cassandra-dtest/blob/master/dtest_setup.py#L333] to {{3.10}} will have the same problem as CASSANDRA-14596: unable to parse the response from prepare statement.;;;","30/Jul/18 12:37;jasobrown;tested locally and passed multiple times. +1;;;","30/Jul/18 17:30;jay.zhuang;Thanks [~jasobrown] for the review. Committed as [{{79d8ddd}}|https://github.com/apache/cassandra-dtest/commit/79d8ddd4a418c52d913ccd5f535f75228877301d].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[dtest] test_system_auth_ks_is_alterable - auth_test.TestAuth,CASSANDRA-14600,13174874,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jay.zhuang,jasobrown,jasobrown,26/Jul/18 14:45,16/Apr/19 09:29,13/Jul/23 08:37,30/Jul/18 17:36,,,,,,,Test/dtest/python,,,,0,dtest,,,"Test 'fails' on 3.0 and 3.11 with this error from pytest:

 
{noformat}
test teardown failure

Unexpected error found in node logs (see stdout for full details). Errors: [ERROR [Native-Transport-Requests-1] 2018-07-23 18:14:34,585 Message.java:629 - Unexpected exception during request; channel = [id: 0x0ffc99f5, L:/127.0.0.3:9042 - R:/127.0.0.1:54516]
java.lang.RuntimeException: org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM
	at org.apache.cassandra.auth.CassandraRoleManager.getRole(CassandraRoleManager.java:518) ~[main/:na]
	at org.apache.cassandra.auth.CassandraRoleManager.canLogin(CassandraRoleManager.java:312) ~[main/:na]
	at org.apache.cassandra.service.ClientState.login(ClientState.java:281) ~[main/:na]
	at org.apache.cassandra.transport.messages.AuthResponse.execute(AuthResponse.java:80) ~[main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [main/:na]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_154-cassandra]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [main/:na]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [main/:na]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_154-cassandra]
Caused by: org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM
	at org.apache.cassandra.db.ConsistencyLevel.assureSufficientLiveNodes(ConsistencyLevel.java:334) ~[main/:na]
	at org.apache.cassandra.service.AbstractReadExecutor.getReadExecutor(AbstractReadExecutor.java:162) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$SinglePartitionReadLifecycle.<init>(StorageProxy.java:1779) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:1741) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy.readRegular(StorageProxy.java:1684) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:1599) ~[main/:na]
	at org.apache.cassandra.db.SinglePartitionReadCommand$Group.execute(SinglePartitionReadCommand.java:1176) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:315) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:285) ~[main/:na]
	at org.apache.cassandra.auth.CassandraRoleManager.getRoleFromTable(CassandraRoleManager.java:526) ~[main/:na]
	at org.apache.cassandra.auth.CassandraRoleManager.getRole(CassandraRoleManager.java:508) ~[main/:na]
	... 13 common frames omitted, ERROR [Native-Transport-Requests-3] 2018-07-23 18:14:35,759 Message.java:629 - Unexpected exception during request; channel = [id: 0x3bf15467, L:/127.0.0.3:9042 - R:/127.0.0.1:54528]
{noformat}

Not sure if we need just another log error exclude, or if this is legit.",,jasobrown,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 30 17:36:44 UTC 2018,,,,,,,,,,,"0|i3wcv3:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Low,,,,,,,,,,,,,,,,,,,"29/Jul/18 02:21;jay.zhuang;Seems the issue because the client tries to re-connect while the server is stopping. When node-1, node-2 are down, but node-3 is still up, QUORUM cannot meet. That's why it's always node-3 has the exception. Here is a patch, please review:
|[14600|https://github.com/cooldoger/cassandra-dtest/tree/14600]|

Verified locally with {{3.0 / 3.11}} and works fine:
{noformat}
pytest --count 10 -p no:flaky --cassandra-dir=~/ws/cassandra auth_test.py::TestAuth::test_system_auth_ks_is_alterable
{noformat}
;;;","30/Jul/18 13:26;jasobrown;I verified locally, as well, and test passes. +1;;;","30/Jul/18 17:36;jay.zhuang;Thanks [~jasobrown]. Committed as [{{17e0656}}|https://github.com/apache/cassandra-dtest/commit/17e0656a948d52cc55a40e8bf6dc139dd5fa6920].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[dtest] test_functional - global_row_key_cache_test.TestGlobalRowKeyCache,CASSANDRA-14599,13174868,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,jasobrown,jasobrown,26/Jul/18 14:32,05/Aug/20 09:29,13/Jul/23 08:37,30/Jul/18 14:19,,,,,,,Test/dtest/python,,,,0,dtest,,,"dtest fails all the time on 3.0, but not other branches. Error from pytest output:

{code}
test teardown failure
Unexpected error found in node logs (see stdout for full details). Errors: [WARN  [main] 2018-07-23 18:53:10,075 Uns.java:169 - Failed to load Java8 implementation ohc-core-j8 : java.lang.NoSuchMethodException: org.caffinitas.ohc.linked.UnsExt8.<init>(java.lang.Class), WARN  [main] 2018-07-23 18:53:56,966 Uns.java:169 - Failed to load Java8 implementation ohc-core-j8 : java.lang.NoSuchMethodException: org.caffinitas.ohc.linked.UnsExt8.<init>(java.lang.Class), WARN  [main] 2018-07-23 18:55:54,508 Uns.java:169 - Failed to load Java8 implementation ohc-core-j8 : java.lang.NoSuchMethodException: org.caffinitas.ohc.linked.UnsExt8.<init>(java.lang.Class), WARN  [main] 2018-07-23 18:56:42,688 Uns.java:169 - Failed to load Java8 implementation ohc-core-j8 : java.lang.NoSuchMethodException: org.caffinitas.ohc.linked.UnsExt8.<init>(java.lang.Class), WARN  [main] 2018-07-23 18:53:10,075 Uns.java:169 - Failed to load Java8 implementation ohc-core-j8 : java.lang.NoSuchMethodException: org.caffinitas.ohc.linked.UnsExt8.<init>(java.lang.Class)]
{code}",,jasobrown,jay.zhuang,marcuse,,,,,,,,,,,,,,,,,,,,,,,"smiklosovic opened a new pull request #699:
URL: https://github.com/apache/cassandra/pull/699


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jul/20 07:53;githubbot;600","smiklosovic opened a new pull request #707:
URL: https://github.com/apache/cassandra/pull/707


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Aug/20 06:41;githubbot;600","smiklosovic closed pull request #707:
URL: https://github.com/apache/cassandra/pull/707


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/20 09:28;githubbot;600","smiklosovic commented on pull request #707:
URL: https://github.com/apache/cassandra/pull/707#issuecomment-669087318


   closing as it was merged.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/20 09:28;githubbot;600","smiklosovic commented on pull request #699:
URL: https://github.com/apache/cassandra/pull/699#issuecomment-669087837


   closing as it was merged.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/20 09:29;githubbot;600","smiklosovic closed pull request #699:
URL: https://github.com/apache/cassandra/pull/699


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/20 09:29;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 30 14:19:04 UTC 2018,,,,,,,,,,,"0|i3wctr:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"29/Jul/18 05:31;jay.zhuang;Seems like it's caused by CASSANDRA-12133:
{quote}[~snazy]:
 Mike, you can safely ignore this message (see OHC issue).
 It's fixed in OHC 0.4.4 (not sure why I forgot to submit a patch for that until now)..

Using this ticket to upgrade OHC - but not in 3.0.x as it's just an ""annoying"" message.
{quote}
 I actually think we should upgrade OHC for 3.0 branch too (from {{0.4.3}} to {{0.4.4}}). As a user, we also monitor the {{ERROR/WARN}} in our production environment. The {{WARN}} message is harmless but misleading.;;;","30/Jul/18 05:44;marcuse;patch to ignore the WARN message here: https://github.com/krummas/cassandra-dtest/commits/marcuse/14599

dtest run against 3.0: https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F14599

[~jay.zhuang] if we want to upgrade OHC we should probably do it in a separate ticket;;;","30/Jul/18 11:58;jasobrown;+1 to [~krummas]'s patch.

 Also, I'm -1 on upgrading OHC so late into 3.0's lifecycle - unless, of course, there's some CVE or other security/stability problem;;;","30/Jul/18 14:19;marcuse;committed as {{32b53217da060343bcb8dcd272edf46df38d90b7}}, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[dtest] snapshot_test.TestArchiveCommitlog,CASSANDRA-14597,13174862,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,spod,jasobrown,jasobrown,26/Jul/18 14:16,16/Apr/19 09:29,13/Jul/23 08:37,13/Aug/18 16:32,,,,,,,,,,,0,dtest,,,"All TestArchiveCommitLog dtests fail on 3.0, but no other branches. Output from pytest error:

{noformat}
            assert (
                time.time() <= stop_time), ""It's been over a {s}s and we haven't written a new "" + \
>               ""commitlog segment. Something is wrong."".format(s=timeout)
E           AssertionError: It's been over a {s}s and we haven't written a new commitlog segment. Something is wrong.

tools/hacks.py:61: AssertionError
{noformat}
",,jasobrown,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/18 11:57;spod;2_2.txt;https://issues.apache.org/jira/secure/attachment/12933753/2_2.txt","31/Jul/18 11:57;spod;3_0.txt;https://issues.apache.org/jira/secure/attachment/12933754/3_0.txt","31/Jul/18 11:57;spod;3_11_3.txt;https://issues.apache.org/jira/secure/attachment/12933755/3_11_3.txt",,,,,,,,,,,3.0,spod,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 13 16:32:31 UTC 2018,,,,,,,,,,,"0|i3wcsf:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Low,,,,,,,,,,,,,,,,,,,"31/Jul/18 11:59;spod;Looks like the test itself works fine, but the segment allocation behaviour is changed across different versions and 3.0 doesn't allocate a new segment within the timeout to make the test pass.;;;","31/Jul/18 12:37;spod;2.2 seems to consume space in segments much faster compared to 3.x, so it's allocating a new segment within shorter time and the test passes.

3.x allocates a new segment early, while still writing into an existing one. Maybe this behaviour has been changed in CASSANDRA-10202?

3.0 will slowly continue to write to the segment and eventually will allocate a new segment if the timeout is sufficiently long. I'd therefor propose to change the timeout to 120 seconds to make the test pass.;;;","10/Aug/18 17:40;jasobrown;+1 on this solution. We could, alternatively, set the timeout only on the call to [advance_to_next_cl_segment in TestCommitlogArchive.run_archive_commitlog()|https://github.com/riptano/cassandra-dtest/blob/master/snapshot_test.py#L252], rather than change the default for the function in hacks.py.

Either way is fine with me, let's just get this one knocked out.

Thanks for looking into this one, [~spodxx@gmail.com].;;;","13/Aug/18 16:32;spod;Merged as bd419a7ae;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[dtest] test_mutation_v5 - write_failures_test.TestWriteFailures,CASSANDRA-14596,13174858,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jay.zhuang,jasobrown,jasobrown,26/Jul/18 14:08,16/Apr/19 09:29,13/Jul/23 08:37,10/Aug/18 22:42,,,,,,,Test/dtest/python,,,,0,dtest,,,"dtest fails with the following pytest error:

{noformat}
s = b'\x00\x00'

>   unpack = lambda s: packer.unpack(s)[0]
E   struct.error: unpack requires a buffer of 4 bytes
{noformat}

Test fails on 3.11 (was introduced for 3.10), but succeeds on trunk",,jasobrown,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 10 22:42:24 UTC 2018,,,,,,,,,,,"0|i3wcrj:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Low,,,,,,,,,,,,,,,,,,,"28/Jul/18 18:18;jay.zhuang;The problem is because the [{{result_metadata_id}}|https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v5.spec#L414] is introduced in protocol V5 but not included in 3.11 branch: CASSANDRA-10786. It's a backward incompatible change (which I think is fine for a beta protocol). So the [newer driver is trying to read|https://github.com/datastax/python-driver/blob/master/cassandra/protocol.py#L679] the {{result_metadata_id}}, which causes failure to parse prepare statement response.

It's impacting the customer who is:
1. Using 3.11 and
2. Using V5 protocol (Beta) and
3. Newer driver after CASSANDRA-10786

Here are a few solutions I can think of:
1. backport the feature to 3.11;
2. add a dummy {{result_metadata_id}} in the prepare statement response just to make it compatible with the latest v5 protocol, so we don't have to backport the full feature (may have other consequence);
3. Disable the test and only run it for 4.0. For the customer using 3.11 + V5 protocol, they have to use the older driver before CASSANDRA-10786.

cc. [~ifesdjeen];;;","28/Jul/18 18:30;jasobrown;If a user has to go out of their way to enable v5 when running against 3.11 (meaning, it's not the default behavior), I'd say we punt on the dtest for 3.11 and just run it on 4.0. (Option 3)

Like [~jay.zhuang], let's see what [~ifesdjeen] thinks.;;;","01/Aug/18 23:14;jay.zhuang;As we're already only test v5 protocol for {{4.0}}: [{{dtest_setup.py:333}}|https://github.com/apache/cassandra-dtest/blob/master/dtest_setup.py#L333], I also prefer option 3, here is a patch, please review:
|[{{14596}}|https://github.com/cooldoger/cassandra-dtest/tree/14596]|;;;","10/Aug/18 16:29;jasobrown;+1;;;","10/Aug/18 22:42;jay.zhuang;Thanks [~jasobrown] for the review. Committed as [{{2572ddc}}|https://github.com/apache/cassandra-dtest/commit/2572ddce6c9a33ae81e1543195bfae084f835d6d].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reconcile should not be dependent on nowInSec,CASSANDRA-14592,13174399,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,benedict,benedict,benedict,25/Jul/18 16:04,15/May/20 08:04,13/Jul/23 08:37,23/Aug/18 13:16,4.0,4.0-alpha1,,,,,,,,,0,,,,"To have the arrival time of a mutation on a replica determine the reconciliation priority seems to provide for unintuitive database behaviour.  It seems we should formalise our reconciliation logic in a manner that does not depend on this, and modify our internal APIs to prevent this dependency.
 
Take the following example, where both writes have the same timestamp:
 
Write X with a value A, TTL of 1s
Write Y with a value B, no TTL
 
If X and Y arrive on replicas in < 1s, X and Y are both live, so record Y wins the reconciliation.  The value B appears in the database.
However, if X and Y arrive on replicas in > 1s, X is now (effectively) a tombstone.  This wins the reconciliation race, and NO value is the result.
 
Note that the weirdness of this is more pronounced than it might first appear.  If write X gets stuck in hints for a period on the coordinator to one replica, the value B appears in the database until the hint is replayed.  So now we’re in a very uncertain state - will hints get replayed or not?  If they do, the value B will disappear; if they don’t it won’t.  This is despite a QUORUM of replicas ACKing both writes, and a QUORUM of readers being engaged on read; the database still changes state to the user suddenly at some arbitrary future point in time.
 
It seems to me that a simple solution to this, is to permit TTL’d data to always win a reconciliation against non-TTL’d data (of same timestamp), so that we are consistent across TTLs being transformed into tombstones.
 
4.0 seems like a good opportunity to fix this behaviour, and mention in CHANGES.txt.",,aleksey,bdeggleston,benedict,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,benedict,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 23 23:39:03 UTC 2018,,,,,,,,,,,"0|i3w9xr:",9223372036854775807,,,,,,,,,aleksey,,aleksey,,,Normal,,,,,,,,,,,,,,,,,,,"31/Jul/18 20:14;benedict;[patch|https://github.com/belliottsmith/cassandra/tree/CASSANDRA-14592], [Circle CI|https://circleci.com/workflow-run/8eed6e83-be3d-473c-9390-8e908e84bcfd];;;","21/Aug/18 15:29;benedict;Pushed an update that addresses (I think, it's been a while) Aleksey's offline review comments.

We collaborated to modify the reconcile semantics a little further, so that reconciliation is as consistent as possible.  Now the only situations that might arise with inconsistent reconciliation occur when one cell is expiring, another is a tombstone, and only at the point where both are logically a tombstone.  Specifically, we now prefer:

# The most recent timestamp
# If either are a tombstone or expiring
## If one is regular, select the tombstone or expiring
## If one is expiring, select the tombstone
## The most recent deletion time
# The highest value (by raw ByteBuffer comparison);;;","21/Aug/18 17:00;aleksey;+1;;;","23/Aug/18 13:16;benedict;Thanks; [committed to trunk|https://github.com/apache/cassandra/commit/e225c88a65f2e8091f8ea6212c291416674882a1].;;;","23/Aug/18 23:28;bdeggleston;Looks like this broke {{org.apache.cassandra.db.CellTest#testExpiringCellReconile}}. There was also a merge mixup with CASSANDRA-10726 which I ninja'd [here|https://github.com/apache/cassandra/commit/6e35cf340a4da8409482230b170a8b7546a6569b];;;","23/Aug/18 23:39;benedict;Thanks.  I've also ninja'd the test [here|https://github.com/apache/cassandra/commit/f5adeeb8da15db0336db741bd39da46117fa9b73];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLogReplayer.handleReplayError should print stack traces ,CASSANDRA-14589,13174396,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,benedict,benedict,benedict,25/Jul/18 15:59,16/Apr/19 09:29,13/Jul/23 08:37,30/Nov/18 12:17,3.0.18,,,,,,Legacy/Observability,,,,0,,,,"handleReplayError does not accept an explicit Throwable parameter, so callers only integrate the exception’s message text into the log entry.  This means a loss of debug information for operators.

Note, this was fixed by CASSANDRA-8844 for 3.x+, only 3.0.x is affected.",,benedict,djoshi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,benedict,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 30 12:17:38 UTC 2018,,,,,,,,,,,"0|i3w9x3:",9223372036854775807,,,,,,,,,,,djoshi,,,Low,,,,,,,,,,,,,,,,,,,"31/Jul/18 14:52;benedict;[patch|https://github.com/belliottsmith/cassandra/tree/CASSANDRA-14589-3.0]; [CircleCI|https://circleci.com/workflow-run/64b1b22f-c20f-4cd3-8d36-3e3e845aac16]

dtest failures appear to be pre-existing and unrelated.;;;","27/Nov/18 19:08;djoshi;This looks good. Just a minor whitespace issue [CommitLogReplayer.java#L411|https://github.com/apache/cassandra/compare/trunk...belliottsmith:CASSANDRA-14589-3.0#diff-348a1347dacf897385fb0a97116a1b5eR411] that you can fix on commit. +1.;;;","30/Nov/18 12:17;benedict;Thanks, committed as [0c97908b2f185615c0134572c4f276cd2c5a3f55|https://github.com/apache/cassandra/commit/0c97908b2f185615c0134572c4f276cd2c5a3f55] to 3.0 (only);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unfiltered.isEmpty conflicts with Row extends AbstractCollection.isEmpty,CASSANDRA-14588,13174395,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,benedict,benedict,benedict,25/Jul/18 15:55,15/May/20 08:02,13/Jul/23 08:37,29/Nov/18 16:01,3.0.18,3.11.4,4.0,4.0-alpha1,,,Legacy/Local Write-Read Paths,,,,0,,,,"The isEmpty() method’s definition for a Row is incompatible with that for a Collection.  The former can return false even if there is no ColumnData for the row (i.e. the collection is of size 0).
 
This currently, by chance, doesn’t cause us any problems.  But if we ever pass a Row as a Collection to a method that invokes isEmpty() and then expects (for correctness) that the _collection_ portion is not empty, it will fail.
 
We should probably have an asCollection() method to obtain a collection from a Row, and not implement Collection directly.",,bdeggleston,benedict,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,benedict,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 29 16:01:50 UTC 2018,,,,,,,,,,,"0|i3w9wv:",9223372036854775807,,,,,,,,,bdeggleston,,bdeggleston,,,Low,,,,,,,,,,,,,,,,,,,"31/Jul/18 16:25;benedict;[3.0 patch|https://github.com/belliottsmith/cassandra/tree/CASSANDRA-14588-3.0]; [Circle CI|https://circleci.com/workflow-run/345bec8d-f6eb-4654-b33b-2942752d1ba6];;;","27/Nov/18 16:37;bdeggleston;+1;;;","28/Nov/18 00:29;bdeggleston; 

In my initial review I missed that AbstractCollection has a useful toString method, and it would be nice if it was duplicated it in AbstractRow to help with debugging. That can just be added on commit though.;;;","28/Nov/18 00:51;benedict;Ah, good catch.  Thanks, I'll sneak that in before I commit tomorrow.;;;","29/Nov/18 16:01;benedict;Committed as [8404260f1640efd14613c4591e5e918786fcde10|https://github.com/apache/cassandra/commit/8404260f1640efd14613c4591e5e918786fcde10] to 3.0 and up.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[DTEST] fix write_failures_test.py::TestWriteFailures::test_thrift,CASSANDRA-14583,13173740,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,23/Jul/18 08:02,16/Apr/19 09:29,13/Jul/23 08:37,25/Jul/18 05:56,,,,,,,,,,,0,dtest,,,"seems it needs a {{WITH COMPACT STORAGE}} to avoid failing like this:
{code}
write_failures_test.py::TestWriteFailures::test_thrift swapoff: Not superuser.
01:23:57,245 ccm DEBUG Log-watching thread starting.

INTERNALERROR> Traceback (most recent call last):
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/main.py"", line 178, in wrap_session
INTERNALERROR>     session.exitstatus = doit(config, session) or 0
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/main.py"", line 215, in _main
INTERNALERROR>     config.hook.pytest_runtestloop(session=session)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 617, in __call__
INTERNALERROR>     return self._hookexec(self, self._nonwrappers + self._wrappers, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 222, in _hookexec
INTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 216, in <lambda>
INTERNALERROR>     firstresult=hook.spec_opts.get('firstresult'),
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 201, in _multicall
INTERNALERROR>     return outcome.get_result()
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 76, in get_result
INTERNALERROR>     raise ex[1].with_traceback(ex[2])
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 180, in _multicall
INTERNALERROR>     res = hook_impl.function(*args)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/main.py"", line 236, in pytest_runtestloop
INTERNALERROR>     item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 617, in __call__
INTERNALERROR>     return self._hookexec(self, self._nonwrappers + self._wrappers, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 222, in _hookexec
INTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 216, in <lambda>
INTERNALERROR>     firstresult=hook.spec_opts.get('firstresult'),
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 201, in _multicall
INTERNALERROR>     return outcome.get_result()
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 76, in get_result
INTERNALERROR>     raise ex[1].with_traceback(ex[2])
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 180, in _multicall
INTERNALERROR>     res = hook_impl.function(*args)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/flaky/flaky_pytest_plugin.py"", line 81, in pytest_runtest_protocol
INTERNALERROR>     self.runner.pytest_runtest_protocol(item, nextitem)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/runner.py"", line 64, in pytest_runtest_protocol
INTERNALERROR>     runtestprotocol(item, nextitem=nextitem)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/runner.py"", line 79, in runtestprotocol
INTERNALERROR>     reports.append(call_and_report(item, ""call"", log))
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/flaky/flaky_pytest_plugin.py"", line 120, in call_and_report
INTERNALERROR>     report = hook.pytest_runtest_makereport(item=item, call=call)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 617, in __call__
INTERNALERROR>     return self._hookexec(self, self._nonwrappers + self._wrappers, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 222, in _hookexec
INTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 216, in <lambda>
INTERNALERROR>     firstresult=hook.spec_opts.get('firstresult'),
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 196, in _multicall
INTERNALERROR>     gen.send(outcome)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/skipping.py"", line 123, in pytest_runtest_makereport
INTERNALERROR>     rep = outcome.get_result()
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 76, in get_result
INTERNALERROR>     raise ex[1].with_traceback(ex[2])
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 180, in _multicall
INTERNALERROR>     res = hook_impl.function(*args)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/runner.py"", line 331, in pytest_runtest_makereport
INTERNALERROR>     longrepr = item.repr_failure(excinfo)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/python.py"", line 675, in repr_failure
INTERNALERROR>     return self._repr_failure_py(excinfo, style=style)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/python.py"", line 668, in _repr_failure_py
INTERNALERROR>     return super(FunctionMixin, self)._repr_failure_py(excinfo, style=style)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/nodes.py"", line 295, in _repr_failure_py
INTERNALERROR>     tbfilter=tbfilter,
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 476, in getrepr
INTERNALERROR>     return fmt.repr_excinfo(self)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 717, in repr_excinfo
INTERNALERROR>     reprtraceback = self.repr_traceback(excinfo)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 664, in repr_traceback
INTERNALERROR>     reprentry = self.repr_traceback_entry(entry, einfo)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 624, in repr_traceback_entry
INTERNALERROR>     s = self.get_source(source, line_index, excinfo, short=short)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 568, in get_source
INTERNALERROR>     lines.extend(self.get_exconly(excinfo, indent=indent, markall=True))
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 575, in get_exconly
INTERNALERROR>     exlines = excinfo.exconly(tryshort=True).split(""\n"")
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 426, in exconly
INTERNALERROR>     lines = format_exception_only(self.type, self.value)
INTERNALERROR>   File ""/usr/lib/python3.6/traceback.py"", line 136, in format_exception_only
INTERNALERROR>     return list(TracebackException(etype, value, None).format_exception_only())
INTERNALERROR>   File ""/usr/lib/python3.6/traceback.py"", line 462, in __init__
INTERNALERROR>     _seen.add(exc_value)
INTERNALERROR> TypeError: unhashable type: 'InvalidRequestException'
{code}",,githubbot,jasobrown,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 25 05:56:29 UTC 2018,,,,,,,,,,,"0|i3w5vb:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"24/Jul/18 12:03;jasobrown;[~krummas] What version of c* was this against?;;;","24/Jul/18 12:08;marcuse;[~jasobrown] 3.0, but looks similar on 2.2 and 3.11: https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-2.2-dtest/120/console https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-3.11-dtest/346/console;;;","24/Jul/18 17:29;marcuse;""clean"" test run: https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/594/
dtest patch: https://github.com/krummas/cassandra-dtest/commits/marcuse/14583;;;","24/Jul/18 19:34;jasobrown;+1;;;","25/Jul/18 05:53;githubbot;GitHub user krummas opened a pull request:

    https://github.com/apache/cassandra-dtest/pull/32

    compact storage when testing thrift

    Patch by marcuse; reviewed by Jason Brown for CASSANDRA-14583

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/krummas/cassandra-dtest marcuse/14583

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra-dtest/pull/32.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #32
    
----
commit 194ad8a22315f0410155ab5eb2283d006a4fdd37
Author: Marcus Eriksson <marcuse@...>
Date:   2018-07-23T08:03:17Z

    compact storage when testing thrift
    
    Patch by marcuse; reviewed by Jason Brown for CASSANDRA-14583

----
;;;","25/Jul/18 05:53;githubbot;Github user krummas closed the pull request at:

    https://github.com/apache/cassandra-dtest/pull/32
;;;","25/Jul/18 05:56;marcuse;and committed as {{194ad8a22315f0410155ab5eb2283d006a4fdd37}} to cassandra-dtest, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set broadcast address in internode messagaing handshake,CASSANDRA-14579,13173550,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aweisberg,jasobrown,jasobrown,20/Jul/18 21:25,15/May/20 08:07,13/Jul/23 08:37,20/Jul/18 22:21,4.0,4.0-alpha1,,,,,,,,,0,,,,"I am incorrectly setting the local address into the internode messaging handshake, rather than the broadcast address. This bug existed since CASSANDRA-8457, but had no effect until CASSANDRA-14485. Originally discovered by [~aweisberg].",,aweisberg,jasobrown,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aweisberg,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 20 22:21:11 UTC 2018,,,,,,,,,,,"0|i3w4p3:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"20/Jul/18 21:28;jasobrown; One-liner fix here:

||14579||
|[branch|https://github.com/jasobrown/cassandra/tree/14579]|
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14579]|
||

;;;","20/Jul/18 21:42;aweisberg;+1 This fix has been working for me and when I researched things this is what we used in prior versions for this message.;;;","20/Jul/18 22:21;jasobrown;committed as sha {{c4263d26b43a4a65a31f213a10f6fbd68217825c}}. Thanks, [~aweisberg];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incomplete handling of exceptions when decoding incoming messages,CASSANDRA-14574,13172775,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasobrown,aleksey,aleksey,17/Jul/18 21:14,15/May/20 08:02,13/Jul/23 08:37,17/Aug/18 12:59,4.0,4.0-alpha1,,,,,Legacy/Streaming and Messaging,,,,0,,,,"{{MessageInHandler.decode()}} occasionally reads the payload incorrectly, passing the full message to {{MessageIn.read()}} instead of just the payload bytes.

You can see the stack trace in the logs from this [CI run|https://circleci.com/gh/iamaleksey/cassandra/437#tests/containers/38].

{code}
Caused by: java.lang.AssertionError: null
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:351)
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:371)
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:335)
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:158)
	at org.apache.cassandra.net.async.MessageInHandler.decode(MessageInHandler.java:132)
{code}

Reconstructed, truncated stream passed to {{MessageIn.read()}}:
{{0000000b000743414c5f42414301002a01e1a5c9b089fd11e8b517436ee1243007040000005d10fc50ec}}
You can clearly see parameters in there encoded before the payload:
{{[43414c5f424143 - CAL_BAC] [01 - ONE_BYTE] [002a - 42, payload size] 01 e1 a5 c9 b0 89 fd 11 e8 b5 17 43 6e e1 24 30 07 04 00 00 00 1d 10 fc 50 ec}}
",,aleksey,cscotta,djoshi,jasobrown,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 17 12:59:21 UTC 2018,,,,,,,,,,,"0|i3vzxj:",9223372036854775807,,,,,,,,,djoshi,,djoshi,,,Normal,,,,,,,,,,,,,,,,,,,"17/Jul/18 21:16;aleksey;[~jasobrown] ^;;;","17/Jul/18 22:17;jasobrown;Probably a regression due to CASSANDRA-14485. Will investigate;;;","18/Jul/18 06:22;jasobrown;I can repro both on circleci and my laptop, running the dtest that failed for [~iamaleksey]: materialized_views_test.py::TestMaterializedViews::test_populate_mv_after_insert_wide_rows;;;","18/Jul/18 11:53;aleksey;bq. Probably a regression due to CASSANDRA-14485. Will investigate

FWIW, I think I saw that same issue in CI before CASSANDRA-14485 got committed. So I think it's likelier that it wasn't caused by it than it was.;;;","19/Jul/18 17:38;jasobrown;In short, I wasn't handling all error cases correctly. I was correctly handling the case where there is a single message contained in the {{ByteBuf}} that is fully deserialized, and then if some exception happens in the pipeline, we close the channel and everything is fine. However, if there are multiple messages in the buffer, or the buffer is not fully consumed when deserializing, this is where the problems are. In the catch block of {{MessageInHandler.decode()}}, I am calling {{exceptionHandled()}}, which closes the channel. However, as we derive from {{ByteToMessageDecoder}}, as it is responding to the channel close event, it will see there are unconsumed bytes in the buffer (called {{cumulator}} in the class), and (re-)invoke {{decode()}}. Unfortunately, if you are in a bad state and partway through the stream, you will fail to correctly deserialize any messages and it's downhill from there (you start looping over the same failure pattern: exception, call close channel, {{ByteToMessageDecoder}} calls {{decode()}}, repeat ...). The most safe thing to do here is pass the caught exception to {{ByteToMessageDecoder}}, and prevent any future processing in the {{decode()}} method.

The patch here resolves the error handling in the inbound pipeline (see below for details on the failing dtest):
||14574||
|[branch|https://github.com/jasobrown/cassandra/tree/14574]|
|[utests & dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14574]|

This patch does several things in the {{MessageInHandler.decode()}} method's exception block (which is where the problems lie):
 - explicitly throws the exception from the handler to the parent {{ByteToMessageDecoder}}, where it can properly break out of the while loop in {{callDecode()}}, and more properly send the exception to the {{exceptionHandled()}} method (which is overridden in {{BaseMessageInHandler}}) where we close the channel.
 - moves the {{ByteBuf}} 's readIndex to the end of the buffer, to make it appear as though the buffer has been fully 'consumed'. This optimizes (and helps with correctness of) {{ByteToMessageDecoder}}, because when the channel is closed, {{ByteToMessageDecoder.channelInputClosed()}} attempts, several times, to ensure all the bytes from the backing {{ByteBuf}} ({{cumulator}}) are consumed. Even though the state of the implementing handler is borked, the parent {{ByteToMessageDecoder}} will still keep trying to make sure all the bytes in {{cumulator}} are consumed before closing the channel. Thus, forcing the readIndex to the end of the buffer avoids that situation.
 - adds an explicit {{CLOSED}} state to the {{MessageInHandler}}, and the handler's state is set to {{CLOSED}} when a message fails to be deserialized or other error, for example: when the table doesn't exist (see below). While this is probably not completely necessary for correctness due to the other changes (primarily the one about moving the readIndex to the end of the buffer), it makes the state of the handler much more explicit, depends less on knowledge of the internal details of netty, and more resilient to implementation changes in the netty library itself.

 

After this fix, the {{materialized_views_test.py::TestMaterializedViews::test_populate_mv_after_insert_wide_rows}} dtest still fails, however, not with the same exception stack trace as reported. Instead, this one:
{noformat}
io.netty.handler.codec.DecoderException: org.apache.cassandra.exceptions.UnknownTableException: Couldn't find table with id 3694c0c0-8b6b-11e8-841f-cd3e85e9c250. If a table was just created, this is likely due to the schemanot being fully propagated.  Please wait for schema agreement on table creation.
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:459)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1342)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:934)
	at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:979)
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:404)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:307)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.cassandra.exceptions.UnknownTableException: Couldn't find table with id 3694c0c0-8b6b-11e8-841f-cd3e85e9c250. If a table was just created, this is likely due to the schemanot being fully propagated.  Please wait for schema agreement on table creation.
	at org.apache.cassandra.schema.Schema.getExistingTableMetadata(Schema.java:438)
	at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize(PartitionUpdate.java:612)
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:353)
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:371)
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:335)
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:158)
	at org.apache.cassandra.net.async.MessageInHandler.decode(MessageInHandler.java:130)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:489)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:428)
{noformat}
Thus the problem here appears to be request not finding the correct table in the schema. In my test local runs with the above patch applied, that table is id correct and eventually exists (for the MView), but not when the message comes in.

The reason why I had not seen this dtest failure in the past (and dtest runs were green), is because it was only exposed by the recent commit for CASSANDRA-13426. I bisected back to a few commits before CASSANDRA-14485 (started on sha {{2bad5d5b6d2134ecd3db63d02aa2274299d1d748}}), and it identified CASSANDRA-13426 as the commit that caused {{materialized_views_test.py::TestMaterializedViews::test_populate_mv_after_insert_wide_rows}} to start failing. My fix corrects the nastier part of that failure, but there's another issue that is outside the scope of the internode messaging.;;;","19/Jul/18 17:51;jasobrown;UPDATE: looks like [~iamaleksey] had already seen the original dtest failure for that dtest, and [commented here|https://issues.apache.org/jira/browse/CASSANDRA-13426?focusedCommentId=16436169&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16436169]. Thus, I'm gonna ignore the dtest fail, as well.;;;","19/Jul/18 19:04;djoshi;Hi [~jasobrown] and [~iamaleksey], I don't wish to hijack this ticket but I strongly recommend marking this test as an expected failure. See my comment on [CASSANDRA-14571|https://issues.apache.org/jira/browse/CASSANDRA-14571].;;;","19/Jul/18 19:40;aleksey;[~djoshi3] Nah. The patch for it is incoming (see [here|https://github.com/iamaleksey/cassandra/commits/14571-4.0]).;;;","19/Jul/18 20:06;djoshi;Thanks, [~iamaleksey].;;;","19/Jul/18 22:16;aleksey;[~jasobrown] Thanks for looking into it quickly and coming up with a patch. I'm wondering however if maybe this is a good time to improve handling of errors like this by skipping the remaining payload bytes, to leave the stream in a valid state, without closing down connections.;;;","19/Jul/18 22:52;aleksey;Changed the title to reflect the actual issue versus my initial incorrect/incomplete diagnosis (nothing racy here).;;;","20/Jul/18 00:20;aleksey;No longer important, since the source is now known, but I got a different instance of what I think is the same bug, triggered [here|https://circleci.com/gh/iamaleksey/cassandra/468#tests/containers/26].;;;","20/Jul/18 04:38;jasobrown;bq.  longer important, since the source is now known, but I got a different instance of what I think is the same bug, triggered here

Yup, looks like the broken behavior of constantly evaluating the buffer even though we're in a bad (incorrect) spot in the stream.

bq. I'm wondering however if maybe this is a good time to improve handling of errors like this by skipping the remaining payload bytes, to leave the stream in a valid state, without closing down connections.

I agree, especially after seeing that we drop the connection when we get a message for a table we don't know about yet. (I'll have to spelunk the git log to uncover the original logic for that one). That's one example, of course. However, I'd like to separate that reevaluation effort from resolving this issue. That way we can unblock this faster.;;;","20/Jul/18 04:42;jasobrown;created CASSANDRA-14575 to explore when we can safely ignore a failed internode message;;;","20/Jul/18 10:41;aleksey;bq. I agree, especially after seeing that we drop the connection when we get a message for a table we don't know about yet. (I'll have to spelunk the git log to uncover the original logic for that one). 

That would be CASSANDRA-14168. As for when it's safe to ignore a failed to deser message - at least in the case of unknown table id it is, and that's a common enough scenario. Think someone creates a table and starts writes before waiting for schema to propagate. Or batchlog replays a mutation to a node on which a table is either not yet known, or has been dropped since. Or, occasionally, when we add new tables and use them during mixed mode/upgrade period. I'm pretty sure there is a JIRA somewhere, by Tyler, to address just this, but we never quite came around to it.;;;","15/Aug/18 09:13;djoshi;Hi [~jasobrown], overall the changes look good. I have a few changes that would eliminate some code duplication, adds annotation for methods exposed for testing. Other than the refactor, I have moved to using {{ByteBuf::skipBytes(int)}} instead of explicitly setting the readerIndex. Other decoders in the Netty code prefer that as well. Using {{skipBytes}} also goes through Netty's leak detection mechanism while setting the {{readerIndex}} doesn't seem to trigger it.

I have mocked up the changes in this branch - [https://github.com/apache/cassandra/compare/trunk...dineshjoshi:jasobrown-14574-trunk-review?expand=1]

I also think we should add a dtest which simulates a corruption in the byte stream possibly using Byteman.;;;","15/Aug/18 15:21;jasobrown;[~djoshi3] Thanks for reviewing. I agree with the changes you've proposed (nice find on the {{ByteBuf.skipBytes()}}), and have cherry picked the two commits from your branch and rerun the tests:

||14574||
|[branch|https://github.com/jasobrown/cassandra/tree/14574]|
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14574]|
||

Working on a dtest now; not so straight-forward, but can be done.;;;","16/Aug/18 13:41;jasobrown;dtest branch [added here|https://github.com/jasobrown/cassandra-dtest/tree/14574]. Waiting for tests to run.;;;","16/Aug/18 22:09;djoshi;The dtest looks good! I'm +1 on the patch.;;;","17/Aug/18 12:59;jasobrown;Committed to c* as sha {{298416a7445aa50874caebc779ca3094b32f3e31}}, committed to dtest as sha {{6e80b1846c308bb13d0b700263c89f10caa17d28}}. Thanks, all!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix race condition in MV build/propagate when there is existing data in the base table,CASSANDRA-14571,13172697,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,17/Jul/18 15:44,04/Jun/20 12:34,13/Jul/23 08:37,20/Jul/18 08:55,4.0,4.0-alpha1,,,,,Feature/Materialized Views,,,,0,,,,"CASSANDRA-13426 exposed a race in MV initialisation and building, which now breaks, consistently, {{materialized_views_test.py::TestMaterializedViews::test_populate_mv_after_insert_wide_rows}}.

CASSANDRA-14168 is also directly related.",,aleksey,bereng,djoshi,jasonstack,jeromatron,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15845,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 04 12:34:01 UTC 2020,,,,,,,,,,,"0|i3vzg7:",9223372036854775807,,,,,,,,,samt,,samt,,,Normal,,,,,,,,,,,,,,,,,,,"17/Jul/18 21:15;aleksey;Also related - as in found while working on this ticket - CASSANDRA-14574.;;;","19/Jul/18 19:04;djoshi;I recommend marking this test as an expected failure until this ticket is fixed. Having tests fail and then ignored causes confusion (See: CASSANDRA-14574, CASSANDRA-13426).
{code}
@xfail(""Should be addressed with CASSANDRA-14571"")
def test_populate_mv_after_insert_wide_rows(self):{code}
That way the test will still be run and still be included in the report but it wont fail the test suite. This is better than just ignoring dtest run failures.;;;","20/Jul/18 00:19;aleksey;[~beobal] Pushed the branch [here|https://github.com/iamaleksey/cassandra/commits/14571-4.0].

Had CircleCI run all three jobs multiple times. Examples [here|https://circleci.com/gh/iamaleksey/cassandra/465], [here|https://circleci.com/gh/iamaleksey/cassandra/467], and [here|https://circleci.com/gh/iamaleksey/cassandra/462], and more where it came from.

It's a tiny patch, it makes a best-effort attempt at waiting for schema to converge. If it fails, it just logs. Hard-throwing caused some other MV-related tests to time out occasionally, sadly.

If you can review it (it's really small, I promise) or maybe even commit, with whatever changes you feel necessary, tomorrow - I'll super appreciate it (I'll be away, but trunk dtests never rest and will keep falling until this gets in).

Cheers (:;;;","20/Jul/18 08:55;samt;LGTM, committed to trunk in {{13150b001a8ddf82a77ac9525c446b7e9e32325c}};;;","04/Jun/20 12:34;bereng;Looking at 4.0 test failures this one was failing again. There is still a window for schema agreement to happen but the view not be available yet in some nodes. Marking the flaky test in CASSANDRA-15855 and CASSANDRA-15845 to address that root cause eventually. #justfyi;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improper default value of cdc_total_space_in_mb,CASSANDRA-14570,13172526,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,shichao.an,shichao.an,shichao.an,17/Jul/18 00:40,15/Jul/21 20:02,13/Jul/23 08:37,15/Jul/21 20:01,,,,,,,Local/Config,,,,0,CDC,,,"The code for calculating cdc_total_space_in_mb in DatabaseDescriptor.java does not reflect the latest architecture introduced by CASSANDRA-12148.

In short, cdc_total_space_in_mb should be equal or greater than commitlog_total_space_in_mb; otherwise, the writes will fail when on-disk commit logs size reaches the value of cdc_total_space_in_mb. For example, If cdc_total_space_in_mb is 4096 and commitlog_total_space_in_mb is 8192, when we enabled the cdc_enabled flag (even if we didn't enable cdc=true on any table), when total size of commit logs reaches 4096 MB, there is the same of amount of hard links in cdc_raw directory, that is, 4096 MB of cdc logs. Then, if a table has cdc=true, the writes will fail in that table because the CommitLogSegmentManagerCDC will be unable to process new CDC segments. (See allocate and processNewSegment in CommitLogSegmentManagerCDC class)

 

I will attach patches later.",,blerer,jay.zhuang,jmckenzie,JoshuaMcKenzie,shichao.an,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15688,,,,,,,,,,,,,,,,,,0.0,shichao.an,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 15 20:02:01 UTC 2021,,,,,,,,,,,"0|i3vye7:",9223372036854775807,4.0,,,,,,,,,,jmckenzie,,,Low,,,,,,,,,,,,,,,,,,,"17/Jul/18 01:06;shichao.an;Here's the patch for trunk:
||Branch||uTest||
|[14570-trunk|https://github.com/shichao-an/cassandra/commits/14570-trunk]|[!https://circleci.com/gh/shichao-an/cassandra/tree/14570-trunk.svg?style=svg!|https://circleci.com/gh/shichao-an/cassandra/tree/14570-trunk]|;;;","17/Jul/18 03:55;jay.zhuang;Hi [~shichao.an], the size calculation in 3.11 branch is right, as CASSANDRA-12148 is only for 4.0.
For the trunk change, it looks good to me. cc [~JoshuaMcKenzie].;;;","17/Jul/18 17:54;shichao.an;[~jay.zhuang] Thank you for pointing out. I removed the 3.11 change.;;;","17/Jul/18 19:03;JoshuaMcKenzie;A couple concerns:
 # If your volume for CDC logs doesn't have available space left for 
{code:java}
size + conf.commitlog_total_space_in_mb{code}
, assuming size 0 and cl size > free_space on volume, this patch will still set that value to our cdc_total in mb which would be problematic. We should take into consideration whether size + cl size is > the estimated value of the volume and consider the best way to handle that error there (be it warn, fail startup, respect CL failure policy, etc)
 # This is now a misleading property name since cdc storage space will now be ""desired cdc 'buffer' plus cl size on disk"". It would probably make sense (and be more user friendly) to rename \{cdc_total_space_in_mb} to \{cdc_extra_buffer_space} and comment that param to mention the relationship between it and the CL space. Probably also want to keep respecting and using the old property name for backwards compat. purposes.

That being said, great catch on those silly / nonsensical defaults, problem w/CDC post hard-linking.;;;","18/Jul/18 00:21;shichao.an;There are some mistakes in my initial description of the issue. I just updated it. The correction is: when total size of commit logs reaches commitlog_total_space_in_mb, only the writes to cdc=true tables will fail, rather than all writes will fail. (mutation.trackedByCDC()).

[~JoshuaMcKenzie] ""cl size > free_space on volume"". I think you mean total space of the volume, right? i.e. ""whether size + cl size > total space of the volume"". We can definitely handle it, but it seems it's more of the problem of handling commitlog_total_space_in_mb, as the user can set it to an arbitrary value, which can be greater than total disk space. 

For you second point, I definitely agree. The cdc_total_space_in_mb is kind of misleading. ""extra buffer space"" name makes more sense.;;;","14/Jul/21 12:29;blerer;[~shichao.an], [~JoshuaMcKenzie] it is not clear to me what is the status of this ticket. Are some change required or is the patch complete?;;;","14/Jul/21 16:57;jmckenzie;I believe this is superseded by CASSANDRA-15688.;;;","15/Jul/21 08:23;blerer;[~jmckenzie] Should we close this ticket as duplicate?
;;;","15/Jul/21 20:02;jmckenzie;Done.

 

Thanks for the work on this [~shichao.an], and sorry it didn't end up getting used!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Static collection deletions are corrupted in 3.0 -> 2.{1,2} messages",CASSANDRA-14568,13172138,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,benedict,benedict,benedict,13/Jul/18 22:31,02/Aug/19 02:47,13/Jul/23 08:37,14/Sep/18 10:28,3.0.17,3.11.3,,,,,Messaging/Internode,,,,0,,,,"In 2.1 and 2.2, row and complex deletions were represented as range tombstones.  LegacyLayout is our compatibility layer, that translates the relevant RT patterns in 2.1/2.2 to row/complex deletions in 3.0, and vice versa.  Unfortunately, it does not handle the special case of static row deletions, they are treated as regular row deletions. Since static rows are themselves never directly deleted, the only issue is with collection deletions.

Collection deletions in 2.1/2.2 were encoded as a range tombstone, consisting of a sequence of the clustering keys’ data for the affected row, followed by the bytes representing the name of the collection column.  STATIC_CLUSTERING contains zero clusterings, so by treating the deletion as for a regular row, zero clusterings are written to precede the column name of the erased collection, so the column name is written at position zero.

This can exhibit itself in at least two ways:
 # If the type of your first clustering key is a variable width type, new deletes will begin appearing covering the clustering key represented by the column name.
 ** If you have multiple clustering keys, you will receive a RT covering all those rows with a matching first clustering key.
 ** This RT will be valid as far as the system is concerned, and go undetected unless there are outside data quality checks in place.
 # Otherwise, an invalid size of data will be written to the clustering and sent over the network to the 2.1 node.
 ** The 2.1/2.2 node will handle this just fine, even though the record is junk.  Since it is a deletion covering impossible data, there will be no user-API visible effect.  But if received as a write from a 3.0 node, it will dutifully persist the junk record.
 ** The 3.0 node that originally sent this junk, may later coordinate a read of the partition, and will notice a digest mismatch, read-repair and serialize the junk to disk
 ** The sstable containing this record is now corrupt; the deserialization expects fixed-width data, but it encounters too many (or too few) bytes, and is now at an incorrect position to read its structural information
 ** (Alternatively when the 2.1 node is upgraded this will occur on eventual compaction)",,aleksey,benedict,cscotta,jay.zhuang,jeromatron,pauloricardomg,slebresne,spod,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,benedict,,,,,,,,,,,,Correctness -> Recoverable Corruption / Loss,,,,,,,,Challenging,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 14 10:28:24 UTC 2018,,,,,,,,,,,"0|i3vw0f:",9223372036854775807,,,,,,,,,,,aleksey,slebresne,,Critical,,3.0.0,,,,,,,,,,,,,,,,,"13/Jul/18 23:35;benedict;Patch available [here|https://github.com/belliottsmith/cassandra/tree/CASSANDRA-14568];;;","16/Jul/18 14:09;aleksey;+1, ship it.;;;","16/Jul/18 17:44;benedict;Committed as d52c7b8c59, 31d5d870f9 and d3a994b105

tests were run against [circleci|https://circleci.com/workflow-run/c818c58a-83e3-4731-89d7-ab0b04d26d62] (unfortunately right now this link is down); this showed some failing auth dtests, but these code paths were unaffected by this patch and I confirmed they pass locally.;;;","15/Aug/18 15:59;slebresne;Sorry I'm a bit slow to check this one, but I don't think the fix here is correct.

That is, while it makes sure complex deletions are handled for static rows, I believe it also doesn't encode the ""composite"" in the same way than 2.x was (the added unit test passes because it only check the encoding round-trips, which it does, but I believe things don't work properly if tested against a true 2.1 node).

Namely, the encoding of the ""clustering prefix"" (what comes before the column name) for a static cell in 2.x uses N empty components (see [here|https://github.com/apache/cassandra/blob/cassandra-2.1/src/java/org/apache/cassandra/db/composites/CompoundSparseCellNameType.java#L68-L69]), where N is the table clustering size, while the patch uses an empty clustering (that is, put the column name as first component, no matter how many clustering the table has; see [here|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/LegacyLayout.java#L210-L214] and, in [serializeCompound|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/LegacyLayout.java#L2394-L2400], there will be no clustering elements since {{STATIC_PREFIX}} has size 0 and {{CompositeType.Builder}} does not compensate for that in any way). ;;;","15/Aug/18 18:07;benedict;Thanks for the input, when fixing this I had indeed assumed the issue was only with deserialization on 3.0, not serialization also.

I can now see there is an existing method for creating a staticBound, although it's not presently clear to me if it was ever plausibly invoked, so it will take a while to corroborate that we don't break any other assumptions by setting the LegacyLayout.bound to something that breaks the 3.0 definition of a static clustering.  AFAICT, we do directly reference it in places where the distinction matters, so we may have to introduce a legacyBound and modernBound for which a distinction is only made in the static case.;;;","16/Aug/18 07:50;slebresne;It's absolutely possible I'm missing some problem, as god knows those backward compatibility issues often comes with nasty surprises, but I don't think the problem is too hard to solve.

That is, {{LegacyBound}} is only here for backward compatibility and never used by 3.0 before being converting first, so I'm sure to understand what you mean by ""(setting it) to something that breaks the 3.0 definition of a static clustering"".

I believe we should simply make sure that when encoding to 2.x a {{STATIC_CLUSTERING}}, we append ""clustering size"" empty values before adding the rest (the column name), and when decoding, we assume those empty values are here (but ignore them). This is what we do for ""cell names"" ([here|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/LegacyLayout.java#L287-L291] and [here|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/LegacyLayout.java#L329-L330]) and to the best of my knowledge is working properly.

In other words, I ""think"" we need to do the 2 following changes:
# revert the change made by this patch in [{{LegacyLayout.decodeBound}}|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/LegacyLayout.java#L210-L215].
# make sure that in [{{LegacyLayout.serializeCompound}}|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/LegacyLayout.java#L2394-L2400] (and {{serializeSizeCompound}}), the static case is specialized to automatically add the empty values, which might possibly be better handler directly within {{CompositeType.Builder}}.
;;;","16/Aug/18 08:10;benedict;Wouldn't the ""more correct"" place to change it be [in the bound construction|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/LegacyLayout.java#L803]?  Presumably the LegacyBound.bound property should be correctly legacy-fied so that any other LegacyLayout code correctly interprets / sorts it?  This is what I was planning on with [my wip branch|https://github.com/belliottsmith/cassandra/commit/5b6742a7a6104bbd88d784db4d3bf7cd990cf057#diff-1a4af2aebd51e301cf0e73126722a8a4R805].

My concern was that LRT.start.bound is [directly referenced in UnfilteredDeserializer when converting to a RTMarker|https://github.com/belliottsmith/cassandra/blob/14568-2/src/java/org/apache/cassandra/db/UnfilteredDeserializer.java#L728].  I realise in the light of morning that this would not be a problem during _serialization_, but maybe this is another bug in deserialization?
;;;","16/Aug/18 09:07;benedict;bq. My concern was that LRT.start.bound is directly referenced in UnfilteredDeserializer when converting to a RTMarker.

Hmm.  Thinking on it some more, I guess this is not a problem due to the fact that we never (in any extant version) actually issue any deletions that (in 3.0) would be represented as RTs spanning static rows, so the problematic cases *should* all be converted to collection tombstones only.  I will add some comments to LegacyLayout elaborating the inconsistencies of modern/legacy static clusterings as part of the patch.

So, I'm now comfortable with fixing either location, I think.  Though I need to code dive a bit more to be absolutely certain.;;;","16/Aug/18 09:47;benedict;OK. So, [here|https://github.com/belliottsmith/cassandra/commits/14568-2] is may second attempt at fixing this.

In the process of adding improved assertion logic, I realised we might have another bug around dropped static collection column, that could have resulted in decoding a static collection deletion as a whole-static-row deletion (with unknown semantics, since I vaguely recall that our correctness in some places depends on there being no such deletions).  

In essence: if on looking up the collectionNameBytes, we found no collectionName (due, for instance, to it having been dropped), we would be left with a only a complete static row bound to construct.  

Perhaps I should split this fix into a separate ticket, for a separate CHANGES.txt mention?

We clearly need to introduce some upgrade dtests to cover these cases as well;;;","24/Aug/18 13:42;slebresne;Sorry for the slow turn over. Last version lgtm, though as mention, upgrade dtests would be great to validate it.

Re: dropped static collection columns, the bug looks legit as well so kudos for noticing that. And I personally don't mind committing it here directly.;;;","24/Aug/18 13:46;benedict;Great, thanks for taking the time to confirm.

bq. upgrade dtests would be great to validate it.

Yep, absolutely.  That will have to wait until things settle down in a couple of weeks,  after the freeze.  I may commit this ahead of then, just so we have the bug (at least much more probably) fixed for anyone who might need it more urgently.;;;","24/Aug/18 15:49;slebresne;bq. I may commit this ahead of then

Wfm. Maybe have [~iamaleksey] check our logic one more time but otherwise +1 from me.;;;","28/Aug/18 17:12;aleksey;Looks good to me as well. Sorry for failing to spot this in the first place. Old memories are fading.;;;","13/Sep/18 16:25;benedict;[3.0|https://github.com/belliottsmith/cassandra/tree/14568] [CI|https://circleci.com/workflow-run/70397395-3585-4b4c-904f-a55c070cf359]

Thanks both for your review.  I have split out CASSANDRA-14749, and pushed an updated patch simply missing this part.  If either of you could give a quick cursory +1, I'll commit them both.;;;","14/Sep/18 08:36;slebresne;+1 on this one.;;;","14/Sep/18 10:28;benedict;Thanks.  Committed as [68dbeb34c9404ee3cd7db00cc112e27c9a4b1f6f|https://github.com/apache/cassandra/commit/68dbeb34c9404ee3cd7db00cc112e27c9a4b1f6f].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Adding regular column to COMPACT tables without clustering columns should trigger an InvalidRequestException,CASSANDRA-14564,13171115,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stefan.miklosovic,laxmikant99,laxmikant99,10/Jul/18 10:55,27/May/22 19:24,13/Jul/23 08:37,06/Sep/21 09:11,3.0.26,3.11.12,4.0.2,4.1,4.1-alpha1,,Legacy/Core,Legacy/CQL,,,0,lhf,,,"I have upgraded my system from cassandra 2.1.16 to 3.11.2. We had some tables with COMPACT STORAGE enabled. We see some weird   behaviour of cassandra while adding a column into it.

Cassandra does not give any error while altering  however the added column is invisible. 

Same behaviour when we create a new table with compact storage and try to alter it. Below is the commands ran in sequence: 

 
{code:java}
x@cqlsh:xuser> CREATE TABLE xuser.employee(emp_id int PRIMARY KEY,emp_name text, emp_city text, emp_sal varint, emp_phone varint ) WITH  COMPACT STORAGE;
x@cqlsh:xuser> desc table xuser.employee ;

CREATE TABLE xuser.employee (
emp_id int PRIMARY KEY,
emp_city text,
emp_name text,
emp_phone varint,
emp_sal varint
) WITH COMPACT STORAGE
AND bloom_filter_fp_chance = 0.01
AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
AND comment = ''
AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
AND crc_check_chance = 1.0
AND dclocal_read_repair_chance = 0.1
AND default_time_to_live = 0
AND gc_grace_seconds = 864000
AND max_index_interval = 2048
AND memtable_flush_period_in_ms = 0
AND min_index_interval = 128
AND read_repair_chance = 0.0
AND speculative_retry = '99PERCENTILE';{code}
Now altering the table by adding a new column:
  
{code:java}
x@cqlsh:xuser>  alter table employee add profile text;
x@cqlsh:xuser> desc table xuser.employee ;

CREATE TABLE xuser.employee (
    emp_id int PRIMARY KEY,
    emp_city text,
    emp_name text,
    emp_phone varint,
    emp_sal varint
) WITH COMPACT STORAGE
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
{code}
notice that above desc table result does not have newly added column profile. However when i try to add it again it gives column already exist;
{code:java}
x@cqlsh:xuser>  alter table employee add profile text;
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Invalid column name profile because it conflicts with an existing column""
x@cqlsh:xuser> select emp_name,profile from employee;

 emp_name | profile
----------+---------

(0 rows)
x@cqlsh:xuser>
{code}
Inserting also behaves strange:
{code:java}
x@cqlsh:xuser> INSERT INTO employee (emp_id , emp_city , emp_name , emp_phone , emp_sal ,profile) VALUES ( 1, 'ggn', 'john', 123456, 50000, 'SE');
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Some clustering keys are missing: column1""
x@cqlsh:xuser> INSERT INTO employee (emp_id , emp_city , emp_name , emp_phone , emp_sal ,profile,column1) VALUES ( 1, 'ggn', 'john', 123456, 50000, 'SE',null);
x@cqlsh:xuser> select * from employee;

 emp_id | emp_city | emp_name | emp_phone | emp_sal
--------+----------+----------+-----------+---------

(0 rows)
{code}


*How to solve that ticket* ([~blerer])-------------------------------------------------------------------------------------- 
Adding regular columns to non-dense compact tables should be forbidden as it is the case for other column types. To do that {{AlterTableStatement}} should be modified to fire an {{InvalidRequestException}} when a user attempts to add a regular column to a  a COMPACT TABLE without clustering columns.
The fix should include a unit tests for that scenario  ",,blerer,e.dimitrova,laxmikant99,stefan.miklosovic,VincentWhite,,,,,,,,,,,,,,,,,,,,,"smiklosovic closed pull request #1145:
URL: https://github.com/apache/cassandra/pull/1145


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Sep/21 11:06;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,CASSANDRA-13920,,CASSANDRA-13920,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,stefan.miklosovic,,,,,,,,,,,,,,,,,,,,Low Hanging Fruit,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 25 10:07:11 UTC 2021,,,,,,,,,,,"0|i3vpwf:",9223372036854775807,3.11.2,,,,,,,,,,blerer,,,Normal,,3.0.25,,,https://github.com/apache/cassandra/commit/267d3ce04efb3bef228c9c1226cd5f938c5d60a3,,,,,,,,,unit tests,,stefan.miklosovic,,,"03/Jan/19 06:09;VincentWhite;Looks like the behaviour your seeing is because Cassandra's running with assertions turned off (It's recommended they are left on even in production environments for C*). Because non-dense compact storage columns are implemented as static columns in the new engine, with assertions turned on you would have seen an assertionError from {{org/apache/cassandra/db/CompactTables.java:67}} when trying to add a non-static column. I've had a quick a look and haven't seen any reason why we couldn't add new static columns to such a table. Haven't had a chance to run the full test suite but here's a patch to transparently make the new column static. I figure it's ok to keep that transparent to the user rather than requiring them to add the {{static}} keyword since these columns aren't listed as static anywhere else.  
||C* 3.11||
|[PoC Patch|https://github.com/vincewhite/cassandra/commit/88d432f349fdd49517352987b587dbf1354fcdd8]|;;;","01/Jul/21 15:23;blerer;[~VincentWhite] as mentioned in CASSANDRA-13920 we do not allow adding columns to other type of COMPACT tables. It makes more sense to me to go for the solution sugested in CASSANDRA-13920 which is to reject the statement as invalid, specially considering that COMPACT tables are now deprecated.
Are you interested in providing a patch for it?;;;","28/Jul/21 13:04;blerer;As I did not get any feedback. I will remove the assigned an mark the ticket as LHF for newcomers to pick it up if they are interested. ;;;","16/Aug/21 16:20;stefan.miklosovic;Hi [~blerer],

Vincent is not longer with us in the company so I guess he is not going to deliver this anytime soon.

I have fixed it here: [https://github.com/apache/cassandra/pull/1145] (I ll test that afterwards)

Would you like to take a look?

One question I have, that case starts with ""if it is dense then ..."" with an exception message which is actually more suitable for this situation we are trying to solve, is that some kind of a legacy?

Next, this description of the solution in the descripion of this ticket confused me a little bit:

... attempts to add a regular column to a a COMPACT TABLE without clustering columns ....

if I did if (meta.isCompactTable && meta.clusteringColumns().isEmpty) throw ...

on the model example it has not worked so I guess just mere checking on meta.isCompactTable() should be enough ...;;;","17/Aug/21 09:21;stefan.miklosovic;[https://github.com/instaclustr/cassandra/tree/CASSANDRA-14564]

[https://github.com/instaclustr/cassandra/tree/CASSANDRA-14564-3.11]

[https://github.com/instaclustr/cassandra/tree/CASSANDRA-14564-4.0]

[https://github.com/instaclustr/cassandra/tree/CASSANDRA-14564-trunk];;;","19/Aug/21 12:19;blerer;Hi [~stefan.miklosovic], 

Thanks for taking over the patch and sorry for the delay of my response. A {{dense}} table is a compact table without clustering columns (see  [here|https://github.com/instaclustr/cassandra/blob/ce7b54a37a2aca7fe2da2f04a425a5c282574443/src/java/org/apache/cassandra/cql3/statements/CreateTableStatement.java#L277]). By consequence, you can simply replace {{if (meta.isDense()) by {{if (meta.isCompactTable())}}. ;;;","19/Aug/21 13:01;stefan.miklosovic;Thanks Benjamin, that is easy to fix. I will get back to you rather shortly.;;;","20/Aug/21 10:31;stefan.miklosovic;[~blerer] work is in same 4 branches as above
CI builds (in progress as of now)
3.11 https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1048/
4.0 https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1049/
trunk https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1050/
3.0 https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1051/;;;","20/Aug/21 13:52;blerer;Thanks Stefan, the patches look good. Feel free to commit if the CI results are good.  ;;;","25/Aug/21 10:07;stefan.miklosovic;I am not completely sure about the test results, I ll try to debug it more. I am not sure if it is just flaky or I introduced some regression.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check for endpoint collision with hibernating nodes ,CASSANDRA-14559,13170405,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stefan.miklosovic,VincentWhite,VincentWhite,06/Jul/18 04:05,08/May/21 00:11,13/Jul/23 08:37,04/Aug/20 22:48,3.0.22,3.11.8,4.0,4.0-beta2,,,Consistency/Bootstrap and Decommission,,,,0,,,,"I ran across an edge case when replacing a node with the same address. This issue results in the node(and its tokens) being unsafely removed from gossip.

Steps to replicate:

1. Create 3 node cluster.
2. Stop a node
3. Replace the stopped node with a node using the same address using the replace_address flag
4. Stop the node before it finishes bootstrapping
5. Remove the replace_address flag and restart the node to resume bootstrapping (if the data dir is also cleared at this point the node will also generate new tokens when it starts)
6. Stop the node before it finishes bootstrapping again
7. 30 Seconds later the node will be removed from gossip because it now matches the check for a FatClient

I think this is only an issue when replacing a node with the same address because other replacements now use STATUS_BOOTSTRAPPING_REPLACE and leave the dead node unchanged.

I believe the simplest fix for this is to add a check that prevents a non-bootstrapped node (without the replaces_address flag) starting if there is a gossip entry for the same address in the hibernate state. 

[3.11 PoC |https://github.com/apache/cassandra/compare/trunk...vincewhite:check_for_hibernate_on_start]


 ",,brandon.williams,e.dimitrova,jeromatron,KurtG,stefan.miklosovic,Suess,VincentWhite,,,,,,,,,,,,,,,,,,,"smiklosovic opened a new pull request #700:
URL: https://github.com/apache/cassandra/pull/700


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jul/20 07:37;githubbot;600","smiklosovic opened a new pull request #87:
URL: https://github.com/apache/cassandra-dtest/pull/87


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jul/20 07:57;githubbot;600","smiklosovic closed pull request #700:
URL: https://github.com/apache/cassandra/pull/700


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/20 09:28;githubbot;600","smiklosovic commented on pull request #700:
URL: https://github.com/apache/cassandra/pull/700#issuecomment-669087667


   closing as it was merged


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/20 09:28;githubbot;600","smiklosovic opened a new pull request #88:
URL: https://github.com/apache/cassandra-dtest/pull/88


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Aug/20 08:06;githubbot;600","michaelsembwever closed pull request #88:
URL: https://github.com/apache/cassandra-dtest/pull/88


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Oct/20 12:23;githubbot;600","michaelsembwever closed pull request #87:
URL: https://github.com/apache/cassandra-dtest/pull/87


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Oct/20 12:23;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,4200,,,0,4200,,,,,,,,,,,,,,,CASSANDRA-16662,,,,,,,,,,,,,,,,,,,,,,,,,0.0,stefan.miklosovic,,,,,,,,,,,,,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 11 06:05:24 UTC 2020,,,,,,,All,,,,"0|i3vllj:",9223372036854775807,3.0.x,3.11.x,,,,,,,,,brandon.williams,,,Normal,,1.0.0,,,https://github.com/apache/cassandra/commit/c94ececec0fcd87459858370396d6cd586853787,,,,,,,,,,,,,,"30/Jul/18 00:22;Suess;I have written a dtest for the POC ( [https://github.com/apache/cassandra/compare/trunk...vincewhite:check_for_hibernate_on_start] )

https://github.com/B0073D/cassandra-dtest/commit/1e4601ea230d9e8f20622efc2d47b0f9e6be002b;;;","28/Jul/20 11:40;stefan.miklosovic;PRs for trunk and 3.11

3.11: [https://github.com/apache/cassandra/pull/700/commits]

trunk: [https://github.com/apache/cassandra/pull/699]

 

dtest PR: 

[https://github.com/apache/cassandra-dtest/pull/87]

 

[~dcapwell] [~Bereng] would any of you mind to go over this? thanks!;;;","29/Jul/20 07:53;stefan.miklosovic;dtest [https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/225/pipeline/256/];;;","29/Jul/20 09:05;stefan.miklosovic;failed dtests unrelated to this change: [https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/225/tests]

dtests were executed against dtest PR / personal dtest respository.;;;","29/Jul/20 18:36;brandon.williams;+1, but is there no reason to also fix this in all the lower branches as well?;;;","29/Jul/20 19:08;stefan.miklosovic;I can check 3.0 but I am not sure if the logic is same. The original author of that patch was targetting only 3.11 and I found that the code is same in trunk but it might significantly differ in 3.0 though. Ill check.;;;","03/Aug/20 06:44;stefan.miklosovic;PR for branch 3.0

[https://github.com/apache/cassandra/pull/707]

tests for branch 3.0

[https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/227/pipeline];;;","04/Aug/20 17:41;brandon.williams;+1;;;","04/Aug/20 22:48;brandon.williams;Committed.;;;","10/Aug/20 23:54;e.dimitrova;[~brandon.williams] [~stefan.miklosovic] this test fails on trunk, can you, please, check it?

Ticket was raised for it - CASSANDRA-16030;;;","11/Aug/20 06:05;stefan.miklosovic;the test is flaky, I created an PR https://github.com/apache/cassandra-dtest/pull/88;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest: log-watching thread leak and thread starvation,CASSANDRA-14558,13170063,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,spod,spod,spod,04/Jul/18 12:29,16/Apr/19 09:29,13/Jul/23 08:37,09/Jul/18 07:33,,,,,,,Test/dtest/python,,,,0,dtest,,,"We get occasional build timeouts on b.a.o after pytest becomes unresponsive for over 20 minutes. This will result in a thread dump like this one:

[https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-2.2-dtest/117/consoleFull]

If you look for ""Log-watching thread starting"" messages and the dump, it becomes quite obvious whats the issue here.

I had a quick look at the python3 / pytest related changes in CASSANDRA-14134 and it seems that some of the handling around dtest_setup's {{log_watch_thread}} var has been changed in a way that would prevent eventually yielding the allocated thread.",,KurtG,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,spod,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 09 07:33:43 UTC 2018,,,,,,,,,,,"0|i3vjhj:",9223372036854775807,,,,,,,,,KurtG,,KurtG,,,Normal,,,,,,,,,,,,,,,,,,,"06/Jul/18 10:23;spod;[~KurtG], can you elaborate a bit on why you think this isn't an actual issue, as you mentioned in #dev? Shouldn't we have ""Log-watching thread exiting"" messages in the log, if the behaviour isn't broken in a way as described in this ticket?
;;;","07/Jul/18 00:12;KurtG;Turns out I was imagining a return statement when I glanced quickly yesterday. Patch is fine and fixes the described issue, but not sure why I'm still getting the same errors after applying the patch. I'll do a couple more runs and see if it's something environment related.;;;","09/Jul/18 03:19;KurtG;Looks like it was something in my environment. Working perfectly now. +1 on patch.;;;","09/Jul/18 07:33;spod;Committed as c98469d86 , thanks for bringing it up and reviewing my patch, Kurt!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming needs to synchronise access to LifecycleTransaction,CASSANDRA-14554,13169701,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stefania,djoshi,djoshi,03/Jul/18 06:00,25/Oct/20 23:39,13/Jul/23 08:37,10/Dec/18 15:33,3.0.18,3.11.4,4.0,4.0-alpha1,,,Messaging/Internode,,,,0,,,,"When LifecycleTransaction is used in a multi-threaded context, we encounter this exception -
{quote}java.util.ConcurrentModificationException: null
 at java.util.LinkedHashMap$LinkedHashIterator.nextNode(LinkedHashMap.java:719)
 at java.util.LinkedHashMap$LinkedKeyIterator.next(LinkedHashMap.java:742)
 at java.lang.Iterable.forEach(Iterable.java:74)
 at org.apache.cassandra.db.lifecycle.LogReplicaSet.maybeCreateReplica(LogReplicaSet.java:78)
 at org.apache.cassandra.db.lifecycle.LogFile.makeRecord(LogFile.java:320)
 at org.apache.cassandra.db.lifecycle.LogFile.add(LogFile.java:285)
 at org.apache.cassandra.db.lifecycle.LogTransaction.trackNew(LogTransaction.java:136)
 at org.apache.cassandra.db.lifecycle.LifecycleTransaction.trackNew(LifecycleTransaction.java:529)
{quote}
During streaming we create a reference to a {{LifeCycleTransaction}} and share it between threads -

[https://github.com/apache/cassandra/blob/5cc68a87359dd02412bdb70a52dfcd718d44a5ba/src/java/org/apache/cassandra/db/streaming/CassandraStreamReader.java#L156]

This is used in a multi-threaded context inside {{CassandraIncomingFile}} which is an {{IncomingStreamMessage}}. This is being deserialized in parallel.

{{LifecycleTransaction}} is not meant to be used in a multi-threaded context and this leads to streaming failures due to object sharing. On trunk, this object is shared across all threads that transfer sstables in parallel for the given {{TableId}} in a {{StreamSession}}. There are two options to solve this - make {{LifecycleTransaction}} and the associated objects thread safe, scope the transaction to a single {{CassandraIncomingFile}}. The consequences of the latter option is that if we experience streaming failure we may have redundant SSTables on disk. This is ok as compaction should clean this up. A third option is we synchronize access in the streaming infrastructure.",,bdeggleston,benedict,djoshi,jeromatron,KurtG,marcuse,samt,stefania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-16225,,,,,,,,,,,,,,,,,,,,,,,0.0,stefania,,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,Challenging,Adhoc Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Dec 10 15:08:38 UTC 2018,,,,,,,,,,,"0|i3vh93:",9223372036854775807,,,,,,,,,,,benedict,snazy,,Normal,,,,,,,,,,,,,,,,,,,"07/Nov/18 06:33;stefania;We had a related issue where one of our customers ended up with a corrupt txn log file during streaming, with an ADD record following an ABORT record. We couldn't look at the logs as they weren't available any longer, since the customer only noticed the problem when the node would not restart 22 days later. However, it's pretty obvious in my opinion that one thread aborted the streaming session whilst the receiving thread was adding a new sstable. So this seems the same root cause as reported in this ticket, which is that streaming is using the txn in a thread unsafe way. In my opinion, the problem exists since 3.0. However it becomes significanlty more likely with the Netty streaming refactoring. Our customer was on a branch based on 3.11.

We took a very conservative approach with the fix, in that we didn't want to fully synchronize abstract transactional and the lifecycle transaction on released branches. We could consider synchronizing these classes for 4.0 however, or reworking streaming.

Here are the 3.11 changes, if there is interest in this approach I can create patches for 3.0 and trunk as well:

[https://github.com/apache/cassandra/compare/cassandra-3.11...stef1927:db-2633-3.11]

We simply extracted a new interface, the [sstable tracker|https://github.com/apache/cassandra/compare/cassandra-3.11...stef1927:db-2633-3.11#diff-9d71c7ad9ad16368bd0429d3b34e2b21R15], which is also [implemented|https://github.com/apache/cassandra/compare/cassandra-3.11...stef1927:db-2633-3.11#diff-1a464da4a62ac4a734c725059cbc918bR144] by {{StreamReceiveTask}} by synchronizing the access to the txn, just like it does for all its other accesses to the txn. Whilst it's not ideal to have an additional interface, the change should be quite safe for released branches.;;;","07/Nov/18 16:49;benedict;Hi [~Stefania]. Thanks for the patch!

I haven't reviewed it, but just skimming it, I wonder if you had considered (and potentially discarded) what might be a slightly simpler approach of allocating a separate {{LifecycleTransaction}} for each operation, and atomically transferring their contents as they ""complete"" to the shared {{LivecycleTransaction}}?

Semantically the behaviour remains the same as today, but we should need to minimally change existing code - just encapsulate the offending {{LifecycleTransaction}} to avoid future temptation for unsafe access, and introduce a {{transferTo}} method (or equivalent) to update the shared state.

What do you think?;;;","08/Nov/18 02:47;stefania;You're welcome [~benedict] !

bq.  I wonder if you had considered (and potentially discarded) what might be a slightly simpler approach of allocating a separate LifecycleTransaction for each operation, and atomically transferring their contents as they ""complete"" to the shared LivecycleTransaction?

No I hadn't considered it. It sounds elegant in principle but in order to atomically transfer child transactions to their parent, we'd have to add some complexity to transactions that I'm not sure we need. Obviously, the state of the parent transaction could change at any time (due to an abort), including whilst a child transaction is trying to transfer its state. So this would require some form of synchronization or CAS. The same is true for two child transactions transferring their state simultaneously. The state on disk should be fine as long as child transactions are never committed but only transferred. Child transaction should be allowed to abort independently though. So different rules for child and parent transactions would apply. 

I'm not sure we need this additional complexity because the txn state only changes rarely. {{LifecycleTransaction}} exposes a large API, but many methods are probably only used during compaction. Extracting a more comprehensive interface that can be implemented with a synchronized wrapper may be an easier approach.

I submitted a safe patch that fixes a known problem with streaming and that is safe for branches that will not undergo a major release testing cycle. Unfortunately, I do not have the time to work on a more comprehensive solution, at least not right now. I could however review whichever approach we choose.;;;","08/Nov/18 17:00;benedict;I was actually thinking of something very simple.  Child transactions would not have any direct relationship to parents, there would just be a method to transfer their contents, and this method would be synchronised.  The other methods on a {{LifecycleTransaction}} could simply be marked synchronised as well.  I don't think there would be any major problem with this?  It's not a high-traffic object, so the cost would be low even without extracting a synchronised interface, particularly as this object requires regular fsyncs.

I completely understand that you may be too busy to try this alternative approach.  I think it would be _preferable_ for somebody to have a brief try at the alternative, just to see if we can isolate the complexity, but if we find we don't have time I think your patch looks good too (modulo a proper review).  

Perhaps we should wait and see how things pan out with finding time for review, as I know [~djoshi3] had been planning to take a crack at this too.;;;","09/Nov/18 03:17;stefania;bq.  The other methods on a LifecycleTransaction could simply be marked synchronised as well.

If we are synchronizing the LifecycleTransaction methods anyway, I'm not sure I understand why we need child transactions. Even in 4.0, where Netty threads call {{trackNew}}, I don't think we're adding sstables so frequently to introduce contention on a shared, synchronized txn. Considering {{trackNew}} performs a file sync as you correctly reminded me, surely this blocks Netty threads more than a synchronized {{trackNew}}. Maybe if many sstables are created concurrently during streaming, child transactions would make sense. I'm still not totally sure.

It's fine with me if we prefer to try a different alternative, the patch is available at any time. This code is not changing much so there is little risk of the patch getting stale. For info, internally [~snazy] already reviewed the patch.
;;;","12/Nov/18 14:13;benedict;bq. If we are synchronizing the LifecycleTransaction methods anyway, I'm not sure I understand why we need child transactions.

The only reason would be simplifying analysis of the code's behaviour.  For instance, it's not clear to me how we either would (or should) behave in the stream writers actively working (and creating sstable files) but for whom the transaction has already been cancelled.  Does such a scenario even arise?  Is it possible it would leave partially written sstables?

A separate transaction is very easy to reason about, so we have only to consider what happens when we transfer ownership.

I agree that there is no sensible reason to worry about blocking behaviour specifically, and perhaps synchronising the transaction object is a simple first step we can follow-up later (we could even do it with a delegating SynchronizedLifecycleTransaction, which would seem to be equivalent to your patch, but with the changes isolated to a couple of classes, I think?);;;","13/Nov/18 02:03;stefania;{quote}The only reason would be simplifying analysis of the code's behaviour. For instance, it's not clear to me how we either would (or should) behave in the stream writers actively working (and creating sstable files) but for whom the transaction has already been cancelled. Does such a scenario even arise? Is it possible it would leave partially written sstables?
{quote}
I'm not sure if this scenario may arise when a streaming transaction is aborted, it depends on streaming details which I've forgotten, but let's step through it:
 - The new sstables are recorded as new records before the files are created. If the recording fails, because the transaction was aborted, the streamer will abort with an exception. Fine.
 - So long as the sstables are recorded, the transaction tidier will delete the files on disk and so the contents will be removed from disk as soon as the streamer finishes writing. Also fine.
 - We may however have a race is if the streamer has added a new record to a txn that is about to be aborted, and the streamer hasn't created sstable files when the transaction tidier is running. This could leave files on disk. It's an extremely small window, but it's not impossible.

We keep a reference to the txn only for obsoleted readers of existing files, we should also keep a reference to the txn until all new files are at least created and the directory has been synced. Child transactions would solve this without the need for this extra reference, but we would need to enforce them for all multi-threaded code (the presence of synchronized methods may lure people on sharing transactions). The alternative to child transactions is to force writers to reference the txn.
{quote}we could even do it with a delegating SynchronizedLifecycleTransaction, which would seem to be equivalent to your patch
{quote}
This was exactly the starting point of my patch. I did not implement a fully synchronized transaction because the API is quite large. I thought it may need some cleanup in order to extract the methods related to the transaction behavior. I did not have the time to look into this, and also cleaning up the API is not an option on our released branches, due to the risk of introducing problems, so I extracted the three methods that are used by the writers and implemented the easiest and safest approach.;;;","13/Nov/18 14:40;benedict;I think the situation would actually be, essentially, undefined behaviour?  Since even with this patch we've got a race for modifying the various collections between {{abort()}} and {{trackNew()}}. Even if this window is narrow, this is probably not acceptable for the fix?

We could perhaps create a synchronised transaction object that wraps the {{LifecycleTransaction}} and exposes largely the API you have exposed, but also synchronises its prepare/commit/abort phases? This would avoid the possibility of corrupting the internal state and producing undefined behaviour.

But we're still at least broken on Windows platforms (unless their FS semantics have changed), because we abort the shared transaction before we have closed our in-progress file handles, and so we cannot delete all of the sstables. Not sure what state that leaves us in, but hopefully it would be recovered on restart.

Ultimately, the child transaction approach still feels easier to reason about for me.  With the right comments, I don't see why we should worry about people misusing it for concurrent scenario - and we could only synchronise the internal methods (and only synchronise externally the transfer method, for clarity).

On a bikeshedding note, I'm very unconvinced by the name {{SSTableTracker}}. It's generic, and very close to {{Tracker}} which is a much more global thing. Perhaps {{LifecycleTransactionNewTracker}} or something, to make clear its scope? The variables wouldn't need to be renamed, also. I don't see us ever wanting to back this by something other than a {{LifecycleTransaction}}?

Probably we should have originally called {{LifecycleTransaction}} {{LifecycleTxn}}, or simply {{SSTableTxn}};;;","14/Nov/18 01:53;stefania;Looking at the 3.11 code, {{StreamReceiveTask}} does synchronize all access to the txn: update/abort/finish. With the proposed patch the same lock is used for track/untrackNew.  So this patch takes care of the txn status at least up to 3.11. Looking briefly at the code on trunk, it seems the txn has been moved to {{CassandraStreamReceiver}}. I think the intention was still to synchronize access to it, but I haven't checked in detail whether that was achieved correctly.

Ultimately we also need to fix the problem of writers still not having created files when the txn is cleared, or still having files open on Windows.

If you are confident that child transactions are the best solution I'm not opposed but the same can be achieved with a synchronized transaction and a reference to the txn kept by the writers.
;;;","14/Nov/18 08:45;benedict;{quote}If you are confident that child transactions are the best solution I'm not opposed but the same can be achieved with a synchronized transaction and a reference to the txn kept by the writers.
{quote}
Well, we've seen a few edge cases now where the synchronised version was difficult to reason about and left problematic behaviours, and it might be more brittle in future.

But if you're confident you can resolve the remaining issues through synchronisation (and we can establish if trunk is (effectively) synchronised, or should be), that has the advantage of already being (almost) done.;;;","19/Nov/18 03:58;stefania;For trunk, we should probably either introduce a synchronized transaction and add references in the writers, or look into child transactions.

The patch I proposed is suitable for released branches. I see this ticket is now classified for 4.0 only. In my opinion, there is an issue also for 3.0 and 3.11. The explanation is in my original [comment|https://issues.apache.org/jira/browse/CASSANDRA-14554?focusedCommentId=16677739&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16677739]. Perhaps we should create a different ticket for this problem and commit the patch only to 3.0 and 3.11?

I also would like to follow up with a more comprehensive approach for trunk but I don't know when I'll have time to work on this. I'll post another update if I do start work.

 ;;;","19/Nov/18 05:41;djoshi;Hi [~Stefania] thanks for submitting a patch. When I originally created this ticket, I had scoped it for 4.0. I did not investigate it for previous versions. It is fine to use this ticket to address previous versions as well.;;;","20/Nov/18 02:25;stefania;Thanks [~djoshi3] !;;;","05/Dec/18 21:56;benedict;[~krummas]: Could you take a look at [this line|https://github.com/apache/cassandra/compare/cassandra-3.11...stef1927:db-2633-3.11#diff-5d2d6808d387dcbd1937ee23496ac10aR95]?  It looks like it was introduced in CASSANDRA-6696, but I'm not sure why?

I'll be posting a 3.0, 3.11 and trunk patch tomorrow.

Stefania, are you comfortable with calling the new interface {{LifecycleNewTracker}}?  Or do you have another proposal?  I'd simply prefer a term that doesn't clash so strongly with {{Tracker}} which is a much more general concept.;;;","06/Dec/18 02:01;stefania;[~benedict], I'm fine with renaming the new interface {{LifecycleNewTracker}}, or with any other change that you see fit.;;;","06/Dec/18 09:43;marcuse;bq. Marcus Eriksson: Could you take a look at this line? It looks like it was introduced in CASSANDRA-6696, but I'm not sure why?
with 6696 we might create sstablewriters but not write anything to them - say you have 2 disks but flush a single partition, the empty writer is then .abort:ed, but the full transaction is not, so we make sure the files are deleted by untrack:ing them. What I can't remember is why {{SSTableWriter#abort}} is not removing the files though, that feels like it would be the correct solution to the problem...

edit: We do the same in [SSTableRewriter|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/SSTableRewriter.java#L311] when we encounter an empty writer;;;","06/Dec/18 11:19;benedict;Thanks.  I was just a little confused as to why it was in {{abort}} - in {{SSTableRewriter}} we do this on commit / switchWriter.  I can see now that there's an invocation of {{SSTableMultiWriter.abortOrDie}}  (which invokes {{abort}}) in {{RangeAwareSSTableWriter}} on commit).  

Probably this should all be rejigged slightly as you say, moving both of these to {{SSTableWriter}}, perhaps with a parameter to {{abort}} to indicate you want to do this (because in the case of a full abort it's preferable to leave cleanup to the {{LogFile}}'s abort).;;;","07/Dec/18 12:45;benedict;||3.0||3.11||trunk||
|[patch|https://github.com/belliottsmith/cassandra/tree/14554-3.0]|[patch|https://github.com/belliottsmith/cassandra/tree/14554-3.11]|[patch|https://github.com/belliottsmith/cassandra/tree/14554-trunk]|
|[CI|https://circleci.com/workflow-run/3e615cf7-a985-448b-9ba0-6d52f9b87eec]|[CI|https://circleci.com/workflow-run/a74915a5-ea21-49ed-ab0a-389964b606f4]|[CI|https://circleci.com/workflow-run/64915295-ab06-4946-b736-85b799765003]|

AFAICT the CI failures are related to CASSANDRA-14921.  There was a [brief single unit test failure|https://circleci.com/gh/belliottsmith/cassandra/1032#tests/containers/2] for one run, but probably environment or timing related.

This patch is simply a slightly modified and ported version of Stefania's patch above.  As discussed, this doesn't necessarily solve the problem perfectly, but it is close enough that it's worth applying now and worrying about that later.

[~bdeggleston]: I would appreciate it if you could take a quick look at [this|https://github.com/belliottsmith/cassandra/commit/fd54c420da81ee6ebfa1d29f45b8edc4922c8bdb#diff-374d64d7ac810fe7be021a2ef356c071R216], as it's not clear to me why there is a separate synchronised {{finishTransaction}} method.  Was there anticipated to be some kind of potential deadlock?
;;;","07/Dec/18 17:49;bdeggleston;[~benedict] I'm afraid I don't have a great explanation other than that's just how it was in StreamReceiveTask before the refactor.;;;","07/Dec/18 18:00;benedict;Thanks.  I had a suspicion it might have simply been migrated, but I wasn't quite sure where form.  Looks like it goes all the way back to 2016; I don't think [~yukim] is around anymore?

I guess we'll just leave it be; I'm not terribly thrilled at the asymmetry between fully synchronising {{abort}} (and hence accesses to {{sstables}}), and only synchronising {{finishTransaction}} in {{finish}} - though this *should* be fine, the asymmetry suggests maybe there's something we're missing.  Anyway, it certainly maintains the prior behaviour.;;;","08/Dec/18 07:04;djoshi;[~benedict] thanks, I have assigned the ticket to you. I can review it and [~Stefania] please feel free to add yourself as a reviewer too.;;;","08/Dec/18 11:09;benedict;The bulk of the work was Stefania's, so I'll leave myself reviewer.;;;","10/Dec/18 01:44;stefania;Thanks for the pull requests [~benedict], they LGTM. Do put both names as authors, since you've done quite a fair bit of work too.;;;","10/Dec/18 15:08;benedict;Thanks [~Stefania].  I've committed to [84ffcb82a74667b957201f2cdae2d6b308956549|https://github.com/apache/cassandra/commit/84ffcb82a74667b957201f2cdae2d6b308956549] on 3.0 and merged upwards.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OffsetAwareConfigurationLoader doesn't set ssl storage port causing bind errors in CircleCI,CASSANDRA-14546,13168909,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,djoshi,djoshi,aweisberg,28/Jun/18 16:29,15/May/20 08:04,13/Jul/23 08:37,28/Jun/18 16:59,4.0,4.0-alpha1,,,,,CI,Legacy/Testing,,,0,CI,,,"This is causing test failures in CircleCI

https://circleci.com/gh/dineshjoshi/cassandra/336#tests/containers/4

 ",,aweisberg,djoshi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,djoshi,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 28 17:16:08 UTC 2018,,,,,,,,,,,"0|i3vcdb:",9223372036854775807,,,,,,,,,aweisberg,,aweisberg,,,Normal,,,,,,,,,,,,,,,,,,,"28/Jun/18 16:29;aweisberg;[https://github.com/apache/cassandra/compare/trunk...dineshjoshi:messagingservice-test-failure-fix?expand=1]

[https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/messagingservice-test-failure-fix];;;","28/Jun/18 16:58;aweisberg;Passing unit tests https://circleci.com/gh/aweisberg/cassandra/1278;;;","28/Jun/18 16:58;aweisberg;Commited as [4cb83cb81abe6990820f76c0addbd172d9f248a6|https://github.com/apache/cassandra/commit/4cb83cb81abe6990820f76c0addbd172d9f248a6]. Thanks!;;;","28/Jun/18 17:16;djoshi;Thank you [~aweisberg]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtests: fix pytest.raises argument names,CASSANDRA-14545,13168542,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,spod,spod,spod,27/Jun/18 07:26,16/Apr/19 09:29,13/Jul/23 08:37,25/Jul/18 07:22,,,,,,,Test/dtest/python,,,,0,dtest,,,"I've been through a couple of [dtest results|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/580/#showFailuresLink] lately and notices some interpreter errors regarding how we call pytest.raises. The [reference|https://docs.pytest.org/en/latest/assert.html#assertions-about-expected-exceptions] is pretty clear on what would be the correct arguments, but still want to make sure we're not working on different pytest versions. 
[~mkjellman] can you quickly check the following inconsistencies and look at my patch (msg->message, matches->match)?
{noformat}
git show 49b2dda4 |egrep 'raises.*, m' {noformat}",,jasobrown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14560,,,,"27/Jun/18 07:27;spod;CASSANDRA-14545.patch;https://issues.apache.org/jira/secure/attachment/12929347/CASSANDRA-14545.patch",,,,,,,,,,,,,1.0,spod,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 25 07:22:28 UTC 2018,,,,,,,,,,,"0|i3va3z:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"24/Jul/18 12:09;jasobrown;[~mkjellman] isn't around these days, so I've taken a look. Double checked {{master}} for other {{msg}}/{{matches}} and this patch got them all. I also checked the pytest source to make sure {{match}}/{{message}} are the correct param names (they are).

+1;;;","25/Jul/18 07:22;spod;Committed as f210e532e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Report why native_transport_port fails to bind,CASSANDRA-14544,13168464,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jroper,jroper,jroper,26/Jun/18 20:48,15/May/20 08:01,13/Jul/23 08:37,27/Jun/18 22:42,4.0,4.0-alpha1,,,,,,,,,0,,,,"On line 164 of {{org/apache/cassandra/transport/Server.java}}, the cause of a failure to bind to the server port is swallowed:

[https://github.com/apache/cassandra/blob/06209037ea56b5a2a49615a99f1542d6ea1b2947/src/java/org/apache/cassandra/transport/Server.java#L163-L164]

{code:java}
        if (!bindFuture.awaitUninterruptibly().isSuccess())
            throw new IllegalStateException(String.format(""Failed to bind port %d on %s."", socket.getPort(), socket.getAddress().getHostAddress()));
{code}

So we're told that the bind failed, but we're left guessing as to why. The cause of the bind failure should be passed to the {{IllegalStateException}}, so that we can then proceed with debugging, like so:

{code:java}
        if (!bindFuture.awaitUninterruptibly().isSuccess())
            throw new IllegalStateException(String.format(""Failed to bind port %d on %s."", socket.getPort(), socket.getAddress().getHostAddress()),
                bindFuture.cause());
{code}",,aweisberg,djoshi,jroper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jroper,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 27 22:42:41 UTC 2018,,,,,,,,,,,"0|i3v9mv:",9223372036854775807,,,,,,,,,djoshi,,djoshi,,,Normal,,,,,,,,,,,,,,,,,,,"26/Jun/18 21:58;djoshi;[~jroper] would you like to submit a patch for this?;;;","26/Jun/18 22:26;jroper;Patch is here:

https://github.com/apache/cassandra/compare/trunk...jroper:throw-cause

You can pull it by running:

{noformat}
git pull https://github.com/jroper/cassandra throw-cause
{noformat};;;","27/Jun/18 18:51;djoshi;Thanks, [~jroper] - the tests are running: [https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/throw-cause];;;","27/Jun/18 22:00;djoshi;So... after struggling with {{MessagingServiceTest}} failures on CircleCI, I was able to determine that the failures on CircleCI are unrelated to this patch. It seems CircleCI containers are not isolated and the failure is due to multiple tests attempting to listen on the same IP/Port combination simultaneously.

Anyway, I'm +1 on this patch. [~aweisberg] could you please help commit this patch?;;;","27/Jun/18 22:42;aweisberg;Committed as [85ceec8855683b8bf71e009c8ed102ec91d85a41|https://github.com/apache/cassandra/commit/85ceec8855683b8bf71e009c8ed102ec91d85a41]. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Order of warning and custom payloads is unspecified in the protocol specification,CASSANDRA-14541,13167857,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,samt,avi.kivity,avi.kivity,24/Jun/18 07:23,22/Jan/21 15:11,13/Jul/23 08:37,22/Jan/21 15:11,2.2.20,3.0.24,3.11.10,4.0,,,Legacy/Documentation and Website,,,,0,protocolv4,protocolv5,,"Section 2.2 of the protocol specification documents the types of tracing, warning, and custom payloads, but does not document their order in the body.",,avi.kivity,jjirsa,mck,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/18 07:25;avi.kivity;v1-0001-Document-order-of-tracing-warning-and-custom-payl.patch;https://issues.apache.org/jira/secure/attachment/12928910/v1-0001-Document-order-of-tracing-warning-and-custom-payl.patch",,,,,,,,,,,,,1.0,samt,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,Clients,Docs,,Fri Jan 22 15:11:33 UTC 2021,,,,,,,,,,,"0|i3v64n:",9223372036854775807,,,,,,,,,mck,,mck,,,Low,,2.2.0,,,https://github.com/apache/cassandra/commit/732ec7723856fdab43e6e4dd297c1335c818686c,,,,,,,,,Documentation update only.,,,,,"27/Jun/18 23:48;jjirsa;[~ifesdjeen] - you've spent some time on protocol / driver, is this something you can glance at?;;;","05/Jan/19 08:02;mck;[~avi.kivity], i'm taking on a review of this. as i'm new to this code so could you help me out with the following…

The order of `tracing, custom-payload, warnings` is easily confirmed by looking at [Message.java|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/transport/Message.java#L317-L319]

But where is the ""padding"" happening in the code?


Regarding the formatting of the text, it seems a little odd for the spec document. i don't see such style anywhere else. Also what you're adding is not only about the flags in the specification, but about the order of contents in the *body*. That is it is dealing with a broader context than that of 'Section 2. Frame Header'.

To meet the existing format layout/style and correct context, I'm wondering if it would make more sense to rewrite the patch into something along the lines of 
{noformat}
4. Messages

Dependant on the flags specified in the header, the overall body of the message body must be:
    [<tracing_id>][<warnings>][<custom_payload>]<opcode_message>[<padding>]
  where:
    - <tracing_id> is a UUID tracing ID (if this is a request message, and if the Tracing flag is set).
    - <warnings> is a string list of warnings (if this is a request message, if the Warning flag is set).
    - <custom_payload> is bytes map for the serialised custom payload (if the Custom payload flag is set)
    - <opcode_message> as defined below through sections 4 and 5.
    - <padding> possibly empty padding to fill out the body to the message length as specified in the header. ???is this accurate?

4.1. Requests
{noformat}

On a separate note, while we're tackling this, the sentence ""The rest of the body will then be the usual body corresponding to the response opcode."" could be removed. It doesn't make sense in the middle of the flags section.;;;","03/Jan/21 16:55;mck;[~samt], [~ifesdjeen], since recent work on CASSANDRA-14688, what are you thoughts to my suggested patch in the previous^ comment?;;;","19/Jan/21 09:56;samt;[~mck] your patch LGTM. FTR, the ordering of tracing ID and warnings was specified in section 2.2 (flags), but the position of custom payload was still potentially ambiguous so I've added some text to clean that up. This affects both protocol v4 and v5 specs, so I've pushed modified versions of both [here|https://github.com/beobal/cassandra/commit/219ee43af65690131fa87829e7d43558f20f1474]. If they look right to you, I'll commit them to the appropriate branches. ;;;","21/Jan/21 14:42;mck;[~samt], your updates to v4 and v5 specs look good to me! ;;;","22/Jan/21 15:11;samt;Committed, thanks [~mck].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Internode messaging handshake sends wrong messaging version number,CASSANDRA-14540,13167817,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jasobrown,jasobrown,jasobrown,23/Jun/18 12:16,15/May/20 08:06,13/Jul/23 08:37,25/Jun/18 13:42,4.0,4.0-alpha1,,,,,Legacy/Streaming and Messaging,,,,0,,,,"With the refactor of internode messaging to netty in 4.0, we abstracted the protocol handshakes messages into a class and handlers. There is a bug when the initiator of the connection sends, in the third message of the handshake, it's own default protocol version number ({{MessagingService.current_version}}), rather than the negotiated version. This was not causing any obvious problems when CASSANDRA-8457 was initially committed, but the bug is exposed after CASSANDRA-7544. The problem is during rolling upgrades of 3.0/3.X to 4.0, nodes cannot correctly connect. ",,cscotta,djoshi,jasobrown,jasonstack,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14485,,,,,,,,,,,,,,,0.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 25 13:42:58 UTC 2018,,,,,,,,,,,"0|i3v5vz:",9223372036854775807,,,,,,,,,djoshi,,djoshi,,,Critical,,,,,,,,,,,,,,,,,,,"23/Jun/18 12:18;jasobrown;Here is a one-line patch to send the correctly negotiated version.

||internode-msg-version-fix||
|[branch|https://github.com/jasobrown/cassandra/tree/internode-msg-version-fix]|
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/internode-msg-version-fix]|
||

Note: the failing {{MessagingServiceTest}} utest is unrelated to this change, and is failing on [vanilla trunk|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/trunk-circle]. I'll resolve that in a separate ticket.

[~aweisberg] / [~djoshi3]: Can one of you review, please?;;;","24/Jun/18 20:03;djoshi;Hi [~jasobrown], this is a good catch. Just curious if the upgrade tests dtest caught this?

Regarding, the change, would it be possible to add a test in {{OutboundHandshakeHandlerTest}} to check that we use the negotiated version number?;;;","24/Jun/18 22:29;jasobrown;upgrade_tests are currently disabled/non-funcational (https://issues.apache.org/jira/browse/CASSANDRA-14421) (sadpanda). I'm working on that soon-ish. 

I can add a test, and thanks for the push!;;;","24/Jun/18 23:03;jasobrown;Tests added to {{OutboundHandshakeHandlerTest}}, and a new commit is on the same branch.;;;","25/Jun/18 05:16;djoshi;Thanks, [~jasobrown] LGTM +1;;;","25/Jun/18 13:42;jasobrown;committed as sha {{5db822b71ad7278ca6443455d029dd79e22388d8}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to start repair process,CASSANDRA-14530,13166955,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,,pheonixs,pheonixs,19/Jun/18 14:24,16/Apr/19 09:29,13/Jul/23 08:37,21/Jun/18 11:55,,,,,,,Consistency/Repair,Legacy/Core,Local/Compaction,,0,,,," 

Then repair starts, cassandra goes down.","Description: Debian GNU/Linux 8.5 (jessie)
 Codename: jessie

16GB mem. 6 allocated by java only.

Linux XXXX04 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt25-2+deb8u3 (2016-07-02) x86_64 GNU/Linux

cassandra 3.11.2

 

Output of command

XProf file attached below
{code:java}
XXXXXXXXX04:/var/lib/cassandra# nodetool repair archive statuses -full
[2018-06-19 17:17:38,170] Starting repair command #1 (84fd0590-73cb-11e8-9815-6d9cda004dd5), repairing keyspace archive with repair options (parallelism: parallel, primary range: false, incremental: false, job threads: 1, ColumnFamilies: [statuses], dataCenters: [], hosts: [], # of ranges: 967, pull repair: false)
Exception occurred during clean-up. java.lang.reflect.UndeclaredThrowableException
Cassandra has shutdown.
error: [2018-06-19 17:17:42,233] JMX connection closed. You should check server log for repair status of keyspace archive(Subsequent keyspaces are not going to be repaired).
-- StackTrace --
java.io.IOException: [2018-06-19 17:17:42,233] JMX connection closed. You should check server log for repair status of keyspace archive(Subsequent keyspaces are not going to be repaired).
at org.apache.cassandra.tools.RepairRunner.handleConnectionFailed(RepairRunner.java:98)
at org.apache.cassandra.utils.progress.jmx.JMXNotificationProgressListener.handleNotification(JMXNotificationProgressListener.java:86)
at javax.management.NotificationBroadcasterSupport.handleNotification(NotificationBroadcasterSupport.java:275)
at javax.management.NotificationBroadcasterSupport$SendNotifJob.run(NotificationBroadcasterSupport.java:352)
at javax.management.NotificationBroadcasterSupport$1.execute(NotificationBroadcasterSupport.java:337)
at javax.management.NotificationBroadcasterSupport.sendNotification(NotificationBroadcasterSupport.java:248)
at javax.management.remote.rmi.RMIConnector.sendNotification(RMIConnector.java:441)
at javax.management.remote.rmi.RMIConnector.access$1200(RMIConnector.java:121)
at javax.management.remote.rmi.RMIConnector$RMIClientCommunicatorAdmin.gotIOException(RMIConnector.java:1531)
at javax.management.remote.rmi.RMIConnector$RMINotifClient.fetchNotifs(RMIConnector.java:1352)
at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.fetchOneNotif(ClientNotifForwarder.java:655)
at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.fetchNotifs(ClientNotifForwarder.java:607)
at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.doRun(ClientNotifForwarder.java:471)
at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.run(ClientNotifForwarder.java:452)
at com.sun.jmx.remote.internal.ClientNotifForwarder$LinearExecutor$1.run(ClientNotifForwarder.java:108)

{code}",pheonixs,shaurya10000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/18 14:24;pheonixs;hs_err_1529417753.log;https://issues.apache.org/jira/secure/attachment/12928363/hs_err_1529417753.log",,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 21 11:55:19 UTC 2018,,,,,,,,,,,"0|i3v0kn:",9223372036854775807,3.11.2,,,,,,,,,,,,,Critical,,,,,,,,,,,,,,,,,,,"21/Jun/18 11:55;pheonixs;Seems to be fixed by setting vm.max_map_count = 1048575;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool import row cache invalidation races with adding sstables to tracker,CASSANDRA-14529,13166796,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jwest,jwest,jwest,18/Jun/18 20:49,15/May/20 07:59,13/Jul/23 08:37,20/Jun/18 13:13,4.0,4.0-alpha1,,,,,,,,,0,,,,"CASSANDRA-6719 introduced {{nodetool import}} with row cache invalidation, which [occurs before adding new sstables to the tracker|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/SSTableImporter.java#L137-L178]. Stale reads will result after a read is interleaved with the read row's invalidation and adding the containing file to the tracker.  ",,cscotta,jasobrown,jeromatron,jwest,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jwest,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 20 13:13:21 UTC 2018,,,,,,,,,,,"0|i3uzlb:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"18/Jun/18 21:59;jwest;Made the cache invalidation run after the files are added to the tracker. This is similar to [streaming|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/streaming/CassandraStreamReceiver.java#L207-L210]. There is still a race condition but the worst case is only invalidation of a cached copy of the newly added data. 

Branch: [https://github.com/jrwest/cassandra/commits/14529-trunk]
 Tests: [https://circleci.com/gh/jrwest/cassandra/tree/14529-trunk];;;","20/Jun/18 13:13;jasobrown;Committed as sha {{73e70340a173b8ff56665cddb70756e83f7d37b0}}. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
streaming failure during bootstrap makes new node into inconsistent state,CASSANDRA-14525,13166262,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,chovatia.jaydeep@gmail.com,chovatia.jaydeep@gmail.com,chovatia.jaydeep@gmail.com,15/Jun/18 04:42,16/Jul/20 18:16,13/Jul/23 08:37,31/Dec/18 06:19,2.2.14,3.0.18,3.11.4,4.0,4.0-alpha1,,Legacy/Core,,,,0,,,,"If bootstrap fails for newly joining node (most common reason is due to streaming failure) then Cassandra state remains in {{joining}} state which is fine but Cassandra also enables Native transport which makes overall state inconsistent. This further creates NullPointer exception if auth is enabled on the new node, please find reproducible steps here:

For example if bootstrap fails due to streaming errors like
{quote}java.util.concurrent.ExecutionException: org.apache.cassandra.streaming.StreamException: Stream failed
 at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299) ~[guava-18.0.jar:na]
 at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286) ~[guava-18.0.jar:na]
 at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116) ~[guava-18.0.jar:na]
 at org.apache.cassandra.service.StorageService.bootstrap(StorageService.java:1256) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:894) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.service.StorageService.initServer(StorageService.java:660) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.service.StorageService.initServer(StorageService.java:573) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:330) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:567) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:695) [apache-cassandra-3.0.16.jar:3.0.16]
 Caused by: org.apache.cassandra.streaming.StreamException: Stream failed
 at org.apache.cassandra.streaming.management.StreamEventJMXNotifier.onFailure(StreamEventJMXNotifier.java:85) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at com.google.common.util.concurrent.Futures$6.run(Futures.java:1310) ~[guava-18.0.jar:na]
 at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:457) ~[guava-18.0.jar:na]
 at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156) ~[guava-18.0.jar:na]
 at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145) ~[guava-18.0.jar:na]
 at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:202) ~[guava-18.0.jar:na]
 at org.apache.cassandra.streaming.StreamResultFuture.maybeComplete(StreamResultFuture.java:211) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.streaming.StreamResultFuture.handleSessionComplete(StreamResultFuture.java:187) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.streaming.StreamSession.closeSession(StreamSession.java:440) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.streaming.StreamSession.onError(StreamSession.java:540) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:307) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_121]
{quote}
then variable [StorageService.java::dataAvailable |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L892] will be {{false}}. Since {{dataAvailable}} is {{false}} hence it will not call [StorageService.java::finishJoiningRing |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L933] and as a result [StorageService.java::doAuthSetup|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L999] will not be invoked.

API [StorageService.java::joinTokenRing |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L763] returns without any problem. After this [CassandraDaemon.java::start|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/CassandraDaemon.java#L584] is invoked which starts native transport at 
 [CassandraDaemon.java::startNativeTransport |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/CassandraDaemon.java#L478]

At this point daemon’s bootstrap is still not finished and transport is enabled. So client will connect to the node and will encounter {{java.lang.NullPointerException}} as following:
{quote}ERROR [SharedPool-Worker-2] Message.java:647 - Unexpected exception during request; channel = [id: 0x412a26b3, L:/a.b.c.d:9042 - R:/p.q.r.s:20121]
 java.lang.NullPointerException: null
 at org.apache.cassandra.auth.PasswordAuthenticator.doAuthenticate(PasswordAuthenticator.java:160) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.auth.PasswordAuthenticator.authenticate(PasswordAuthenticator.java:82) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.auth.PasswordAuthenticator.access$100(PasswordAuthenticator.java:54) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.auth.PasswordAuthenticator$PlainTextSaslAuthenticator.getAuthenticatedUser(PasswordAuthenticator.java:198) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.transport.messages.AuthResponse.execute(AuthResponse.java:78) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:535) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:429) [apache-cassandra-3.0.16.jar:3.0.16]
 at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.1.0.CR6.jar:4.1.0.CR6]
 at io.netty.channel.ChannelHandlerInvokerUtil.invokeChannelReadNow(ChannelHandlerInvokerUtil.java:83) [netty-all-4.1.0.CR6.jar:4.1.0.CR6]
 at io.netty.channel.DefaultChannelHandlerInvoker$7.run(DefaultChannelHandlerInvoker.java:159) [netty-all-4.1.0.CR6.jar:4.1.0.CR6]
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_121]
 at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.0.16.jar:3.0.16]
 at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
{quote}
At this point if we run {{nodetool status}} then it will show this new node in {{UJ}} state, however clients can connect to this node over {{CQL}} and will receive {{java.lang.NullPointerException}}",,aweisberg,chovatia.jaydeep@gmail.com,cscotta,djoshi,jasobrown,jay.zhuang,KurtG,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15952,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,chovatia.jaydeep@gmail.com,,,,,,,,,,,,Availability -> Unavailable,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 10 06:14:26 UTC 2019,,,,,,,,,,,"0|i3uwav:",9223372036854775807,,,,,,,,,KurtG,,KurtG,,,Normal,,,,,,,,,,,,,,,,,,,"15/Jun/18 05:11;chovatia.jaydeep@gmail.com;In my opinion daemon should enable native transport only after successful bootstrap to avoid inconsistent state.

Please find patch with the fix here:
||trunk||3.0||2.x||
|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-trunk.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/76]|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-3.0.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/73]  |  [!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-2.2.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/74]|
|[patch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-trunk] |[patch|https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-3.0] |[patch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-2.2] |

Please review it and let me know your opinion.

 ;;;","15/Jun/18 11:38;jasobrown;This is not a review, but you should have some dtests to validate your patch.;;;","15/Jun/18 11:45;chovatia.jaydeep@gmail.com;Sure [~jasobrown] I will add a dTest, created sub-task CASSANDRA-14526;;;","18/Jun/18 05:07;chovatia.jaydeep@gmail.com;[~jasobrown] I've added a dtest for this which passes with this fix and fails on current trunk, please find dtest here:
||dtest||
|[patch |https://github.com/apache/cassandra-dtest/compare/master...jaydeepkumar1984:14526-trunk]|

 ;;;","18/Jun/18 11:15;KurtG;We've already had a ticket (and a _very_ similar patch) for this since November last year... CASSANDRA-14063 ;;;","18/Jun/18 15:43;chovatia.jaydeep@gmail.com;I see [~KurtG], sorry I missed it. In my opinion this is a bug and needs to be fixed (CASSANDRA-14063 should be considered as per FCFS priority). We also need to fix dtest along with this so CASSANDRA-14526 also needs to be landed.;;;","19/Jun/18 05:20;KurtG;Thanks [~chovatia.jaydeep@gmail.com], I agree. No fault on your part, more a problem with the consistent lack of reviewers we have who can prioritise review work. Just unfortunate that it's wasted more time than necessary for everyone. I think [~VincentWhite] would appreciate the acknowledgement (especially after such a long time) so FCFS makes sense to me, but there's no use doing the work twice, just take into account the two patches slight discrepancies when reviewing I guess.;;;","19/Jun/18 18:33;chovatia.jaydeep@gmail.com;I agree there are few discrepancy between two patches as following:
 My patch:
 - It was missing check before starting RPC server. I've incorporated this and my latest patch has this.

[~VincentWhite]'s patch:
 - Missing to start {{cql}} post successful {{bootstrap}}
 - Not sure if we need to use separate variable {{streamingSuccessful}} as we can directly use {{SystemKeyspace.bootstrapComplete()}}
 - We should have consistent error message _{{node is not yet bootstraped completely. Use nodetool to check bootstrap state and resume. For more, see nodetool help bootstrap}}_
 - We should make {{isSurveyMode}} volatile as it is being updated by different thread
 - Need to make sure we get patches for all branches and verify CASSANDRA-14526

[~VincentWhite] Do you want to make a complete patch by incorporating above items?

 

[~KurtG] Does this sound ok to you?;;;","22/Jun/18 17:34;chovatia.jaydeep@gmail.com;Hi [~KurtG]

It seems [~VincentWhite] is not responding, could you please review patch from this ticket?

Jaydeep;;;","25/Jun/18 04:38;KurtG;Sorry about that, we had a pretty busy week last week and Vince probably won't have time. I'll review.;;;","26/Jun/18 05:27;KurtG;A few things:
 If we fail streaming and {{isSurveyMode}} is true we still get the NPE if auth is enabled when trying to connect to C* on that node. Not much we can do about this because auth isn't initialised until we join the ring, but I'm not sure why we should handle this situation differently, and also it's currently kind of broken. At the moment if you resume bootstrap after a streaming failure _while in write survey mode_, you will leave write survey mode on completion of bootstrapping (ouch).


 I think we should handle write survey bootstrapping the same as normal bootstrap, where if we get an error during streaming we don't start transports. Then, on resume, handle survey mode so that we _don't_ join the ring on completion of bootstrapping, but we do still start transports.


 On top of that, seeing as we're in this code anyway, I think it would be reasonable if we could look at handling the auth case a bit better when write survey is enabled as well. Ideally, if auth is required I see no point in starting the transports seeing as you'll always get an NPE, so maybe we can add a check for that in {{CassandraDaemon#start()?}}

{{DatabaseDescriptor.getAuthenticator().requireAuthentication()}} should be enough here I think.

 

Some things regarding the error message:
 We've got repeated information in our error:
{code:java}
WARN  [main] 2018-06-25 09:13:24,136 StorageService.java:935 - Some data streaming failed. Use nodetool to check bootstrap state and resume. For more, see `nodetool help bootstrap`. IN_PROGRESS
ERROR [main] 2018-06-25 09:13:32,190 CassandraDaemon.java:445 - Node is not yet bootstraped hence not enabling native transport. Use nodetool to check bootstrap state and resume. For more, see `nodetool help    bootstrap`
{code}
I think our new message should either be INFO or WARN (INFO is in line with other messages in {{start()}}, and I think it would make more sense if the original message in \{{StorageService}} was ERROR. We could change the message in CassandraDaemon to:
{code:java}
Not starting client transports as bootstrap has not completed.{code}
or something similar, to be more in line with the other info messages.

Finally, with your patch if we resume bootstrap we don't start thrift. As per Vince's patch, daemon.start() is desirable here over startNativeTransport so that we always start thrift and CQL.;;;","26/Jun/18 23:33;chovatia.jaydeep@gmail.com;[~KurtG] Thanks for your review comments. I've made patch simple by not allowing transport if bootstrap is not complete irrespective of {{survey}} mode. Also incorporated other comments, please find updated patch here:

Please find patch with the fix here:
||trunk||3.0||2.x||
|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-trunk.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/76]|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-3.0.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/73]  |  [!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-2.2.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/74]|
|[patch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-trunk] |[patch|https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-3.0] |[patch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-2.2] |
;;;","27/Jun/18 10:07;KurtG;Mostly looks good, however we're still leaving write_survey mode after a resume bootstrap completes when we were started in write survey mode. Also just noticed but as we hackily re-use isSurveyMode when resuming a bootstrap we always log the following message regardless of if we were in write survey mode originally or not.
{code}Leaving write survey mode and joining ring at operator request{code}
I think at this point we could solve these 2 problems by simply calling {{finishJoiningRing}} explicitly when we successfully bootstrap after a resume in {{resumeBootstrap}}, rather than indirectly through {{joinRing}, and also handle write_survey in the same place.

Also, another small nit, can we change the spelling of {{bootstraped}} to {{bootstrapped}} in the exception messages?;;;","28/Jun/18 02:55;chovatia.jaydeep@gmail.com;[~KurtG] I think there was a bug in my previous patch in which it will not start native transport in normal scenario if {{isSurveyMode}} is {{true}}. 
 Also I've discovered another bug exists in current open source code in which if {{isSurveyMode}} is {{true}} and streaming fails (i.e. {{isBootstrapMode}} is {{true}}) then also one can call {{nodetool join}} without {{nodetool bootstrap resume}} and have that node join the ring.

 

I've taken care of this bug and your review comments,please find updated patch here:
||trunk||3.0||2.x||
|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-trunk.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/76]|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-3.0.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/73]  |  [!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-2.2.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/74]|
|[patch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-trunk] |[patch|https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-3.0] |[patch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-2.2] |;;;","02/Jul/18 08:22;KurtG;{quote}Also I've discovered another bug exists in current open source code in which if isSurveyMode is true and streaming fails (i.e. isBootstrapMode is true) then also one can call nodetool join without nodetool bootstrap resume and have that node join the ring.
{quote}
Great catch. I found a couple more small issue w.r.t {{nodetool join}} as well while I was testing this.
 # If in write_survey and you join the ring after bootstrap, transports won't be enabled. can we call {{CassandraDaemon#start()}} here?
 # nodetool join fails silently if write_survey is true and we haven't completed bootstrapping, but server log prints the following
{code:java}
WARN [RMI TCP Connection(5)-127.0.0.1] 2018-06-29 12:39:49,735 StorageService.java:1008 - Some data streaming failed. Use nodetool to check bootstrap state and resume. For more, see `nodetool help bootstrap`. IN_PROGRESS
{code}
nodetool join should say something along the lines of ""{{Can't join the ring because in write_survey mode and bootstrap hasn't completed}}""

Also another minor nit w.r.t logging; you can get the following log message after successfully bootstrapping if you were in write survey mode:
{code:java}
INFO [main] 2018-06-29 12:12:39,071 CassandraDaemon.java:479 - Not starting client transports as bootstrap has not completed
{code}
Probably better to split CassandraDaemon.start() if block so that we print ""{{Not starting client transports as write_survey mode is enabled.}}""

And finally, there's still 2 occurences of ""bootstraped"" in the exception messages in {{startNativeTransport}} and {{startRPCServer}}.;;;","02/Jul/18 22:18;chovatia.jaydeep@gmail.com;[~KurtG] Please find updated review with all of the review comments incorporated.
||trunk||3.0||2.x||
|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-trunk.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/76]|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-3.0.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/73]  |  [!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-2.2.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/74]|
|[patch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-trunk] |[patch|https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-3.0] |[patch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-2.2] |;;;","05/Jul/18 11:31;KurtG;Thanks [~chovatia.jaydeep@gmail.com]. I figured it was unfair making you do all the work so I made some changes myself. Basically we were still silently failing with {{nodetool join}}, so I've added {{isBootstrapMode()}} to {{StorageServiceMBean}} and {{NodeProbe}} and added a check for it in {{Join.java}}. I've also tried to simplify some of the if statements (edge cases are killing me) and added a bit of documentation. I also created a 3.11 branch from our changes as there were a few conflicts from 3.0.

Finally, I extended your dtest from CASSANDRA-14526 to cover the write_survey, join and resume cases to be in line with the changes we've made, and also changed the byteman rule to trigger on {{maybeCompleted()}} as it wasn't consistently triggering on {{startStreamingFiles()}}

Now I think we're pretty much ready to go, but if you want to give my changes a once over to make sure I haven't missed anything as I'm sure you're aware the startup code is an absolute nightmare.

Also, I haven't run the whole dtest suite, but I've at least run all the bootstrap dtests and they are all passing, so I'm taking that as a good sign.
||2.2||3.0||3.11||trunk||dtests||
|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...kgreav:14525-2.2]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...kgreav:14525-3.0]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:14525-3.11]|[branch|https://github.com/apache/cassandra/compare/trunk...kgreav:14525-trunk]|[branch|https://github.com/apache/cassandra-dtest/compare/master...kgreav:14526-trunk-k]|
|[!https://circleci.com/gh/kgreav/cassandra/tree/14525-2.2.svg?style=svg! |https://circleci.com/gh/kgreav/cassandra/178]|[!https://circleci.com/gh/kgreav/cassandra/tree/14525-3.0.svg?style=svg! |https://circleci.com/gh/kgreav/cassandra/181]|[!https://circleci.com/gh/kgreav/cassandra/tree/14525-3.11.svg?style=svg! |https://circleci.com/gh/kgreav/cassandra/180]|[!https://circleci.com/gh/kgreav/cassandra/tree/14525-trunk.svg?style=svg! |https://circleci.com/gh/kgreav/cassandra/183]|;;;","05/Jul/18 17:47;chovatia.jaydeep@gmail.com;[~KurtG] Thanks for making the changes! 

Latest code changes looks good to me.;;;","14/Jul/18 11:01;KurtG;So I messed up a merge to 3.11 and upon doing further dtests found some issues and inefficiencies. These have all been fixed up now and branches above should be all up to date and passing tests so going to mark this as ready to commit.
||2.2||3.0||3.11||trunk||dtests||
|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...kgreav:14525-2.2]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...kgreav:14525-3.0]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:14525-3.11]|[branch|https://github.com/apache/cassandra/compare/trunk...kgreav:14525-trunk]|[branch|https://github.com/apache/cassandra-dtest/compare/master...kgreav:14526-trunk]|
|[!https://circleci.com/gh/kgreav/cassandra/tree/14525-2.2.svg?style=svg! |https://circleci.com/gh/kgreav/cassandra/202]|[!https://circleci.com/gh/kgreav/cassandra/tree/14525-3.0.svg?style=svg! |https://circleci.com/gh/kgreav/cassandra/199]|[!https://circleci.com/gh/kgreav/cassandra/tree/14525-3.11.svg?style=svg! |https://circleci.com/gh/kgreav/cassandra/201]|[!https://circleci.com/gh/kgreav/cassandra/tree/14525-trunk.svg?style=svg! |https://circleci.com/gh/kgreav/cassandra/200]|;;;","19/Jul/18 23:21;djoshi;[~KurtG] - I pulled your branch and have run the dtests with some failures. See [here.|https://circleci.com/workflow-run/035dca04-5b2b-4fb2-aebe-3089d876f995];;;","20/Jul/18 04:45;KurtG;Thanks heaps [~djoshi3], I'll look into it.;;;","28/Aug/18 07:12;chovatia.jaydeep@gmail.com;[~KurtG] I did analyze all the 9 failing dtests from [~djoshi3]'s run and found that they all expect CQL up and running w/o bootstrap, and as per our new design in this ticket, it is simply not allowed.

So I believe we simply have to remove those 9 dtests. Please check and let me know if you need my help anywhere.

Thanks [~djoshi3] for doing a dtest run for us!;;;","28/Aug/18 12:15;KurtG;Thanks so much. I have been meaning to get back to this but since we missed the boat on 3.11.3 I figured this could wait until Sept 1st. If you want you can go ahead and fix the tests, otherwise if not I'll have a look next week :);;;","29/Aug/18 05:26;chovatia.jaydeep@gmail.com;[~KurtG] I've fixed the failing dtests by changing some part in Cassandra and some part in dtests. Please review them whenever you get a chance. Thank You!
||2.2||3.0||3.11||trunk||dtests||
|[branch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-2.2] |[branch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-3.0] |[branch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-3.11] |[branch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-trunk]|[branch |https://github.com/apache/cassandra-dtest/compare/master...jaydeepkumar1984:14526-trunk]|
|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-2.2.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/=132]|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-3.0.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/=133]  |  [!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-3.11.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/=128]|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-trunk.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/=129]|;;;","19/Sep/18 04:45;KurtG;Looks good to me. Probably need another run of the dtests though :/;;;","26/Oct/18 23:42;chovatia.jaydeep@gmail.com;[~jay.zhuang] [~djoshi3] Can one of you please help run a final round of dtest so that this ticket can be committed and closed? ;;;","27/Oct/18 21:03;jay.zhuang;Sure, I'll kick off the tests.;;;","31/Oct/18 05:48;djoshi;Sorry, did not see your comment earlier. I think we should update the dtests rather than removing them. ;;;","21/Dec/18 22:43;jay.zhuang;Rebased the code and started the tests:
| Branch | uTest | dTest |
| [14525-2.2|https://github.com/cooldoger/cassandra/tree/14525-2.2] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14525-2.2.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14525-2.2] | [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/664/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/664/] |
| [14525-3.0|https://github.com/cooldoger/cassandra/tree/14525-3.0] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14525-3.0.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14525-3.0] | [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/665/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/665/] |
| [14525-3.11|https://github.com/cooldoger/cassandra/tree/14525-3.11] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14525-3.11.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14525-3.11] | [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/666/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/666/] |
| [14525-trunk|https://github.com/cooldoger/cassandra/tree/14525-trunk] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14525-trunk.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14525-trunk] | [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/667/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/667/] |;;;","28/Dec/18 20:04;jay.zhuang;The uTest failure is because CASSANDRA-14946. The check conditions are hard to read, how about switching it from:
{noformat}
        // We only start transports if bootstrap has completed and we're not in survey mode, OR if we are in
        // survey mode and streaming has completed but we're not using auth.
        // OR if we have not joined the ring yet.
        if (StorageService.instance.hasJoined() &&
                ((!StorageService.instance.isSurveyMode() && !SystemKeyspace.bootstrapComplete()) ||
                (StorageService.instance.isSurveyMode() && StorageService.instance.isBootstrapMode())))
        {
            logger.info(""Not starting client transports as bootstrap has not completed"");
            return;
        }
        else if (StorageService.instance.hasJoined() &&  StorageService.instance.isSurveyMode() &&
                DatabaseDescriptor.getAuthenticator().requireAuthentication())
        {
            // Auth isn't initialised until we join the ring, so if we're in survey mode auth will always fail.
            logger.info(""Not starting client transports as write_survey mode and authentication is enabled"");
            return;
        }
{noformat}
to:
{noformat}
        // Do not start the transports if we already joined the ring AND
        // if we are in survey mode, streaming has not completed or auth is enabled
        // if we are not in survey mode, bootstrap has not completed
        if (StorageService.instance.hasJoined())
        {
            if (StorageService.instance.isSurveyMode())
            {
                if (StorageService.instance.isBootstrapMode() ||
                    DatabaseDescriptor.getAuthenticator().requireAuthentication())
                {
                    logger.info(""Not starting client transports in write_survey mode as it's bootstrapping or auth is enabled"");
                    return;
                }
            }
            else
            {
                if (!SystemKeyspace.bootstrapComplete()) {
                    logger.info(""Not starting client transports as bootstrap has not completed"");
                    return;
                }
            }
        }
{noformat};;;","29/Dec/18 00:07;chovatia.jaydeep@gmail.com;Thanks [~jay.zhuang] for the review, I've incorporated the changes. Please find latest patch details here:
||Branch ||uTest ||
|[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...jaydeepkumar1984:14525-2.2?expand=1] |[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/bqr.svg?style=svg! | https://circleci.com/gh/jaydeepkumar1984/cassandra/161]  |
|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...jaydeepkumar1984:14525-3.0?expand=1] |[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/bqr.svg?style=svg! | https://circleci.com/gh/jaydeepkumar1984/cassandra/160]  |
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...jaydeepkumar1984:14525-3.11?expand=1] |[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/bqr.svg?style=svg! | https://circleci.com/gh/jaydeepkumar1984/cassandra/162]  |
|[trunk|https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-trunk?expand=1] |[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/bqr.svg?style=svg! | https://circleci.com/gh/jaydeepkumar1984/cassandra/164]  |;;;","29/Dec/18 01:41;jay.zhuang;Hi, just a question : {{SystemKeyspace.bootstrapComplete()}} is checked here: [https://github.com/apache/cassandra/commit/9c3fb65e697d810321936e06504de4b2f7cf633f#diff-b76a607445d53f18a98c9df14323c7ddR392]

But not here: [https://github.com/apache/cassandra/commit/9c3fb65e697d810321936e06504de4b2f7cf633f#diff-b76a607445d53f18a98c9df14323c7ddR351]

Is that expected?;;;","30/Dec/18 22:14;chovatia.jaydeep@gmail.com;Yes [~jay.zhuang] that is expected, because {{isSurveyMode=true}} mode and {{SystemKeyspace.bootstrapComplete()=true}} does not co-exist

e.g. 
 [If in survey mode then bootstrap will not be COMPLETE |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L929]
 [If we give-up survey mode then bootstrap will be COMPLETE |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L985]

So this check {{!SystemKeyspace.bootstrapComplete()}} at [this line|https://github.com/apache/cassandra/commit/9c3fb65e697d810321936e06504de4b2f7cf633f#diff-b76a607445d53f18a98c9df14323c7ddR392] was redundant which is been removed in the latest diffs.;;;","31/Dec/18 06:17;jay.zhuang;Thanks [~chovatia.jaydeep@gmail.com] and [~KurtG]. Committed as [{{a6196a3}}|https://github.com/apache/cassandra/commit/a6196a3a79b67dc6577747e591456328e57c314f].;;;","31/Dec/18 19:11;chovatia.jaydeep@gmail.com;Thanks [~jay.zhuang] [~KurtG] [~djoshi3];;;","08/Jan/19 21:49;aweisberg;This breaks bootstrap_test.py:TestBootstrap.test_resumable_bootstrap. The test expects the cluster to start the native interface when bootstrap fails.;;;","08/Jan/19 21:52;aweisberg;I think test_resume secondary_indexes_test.py:TestPreJoinCallback.test_resume has the same issue.
;;;","08/Jan/19 22:48;chovatia.jaydeep@gmail.com;[~aweisberg] I've already taken care of dests as part of https://issues.apache.org/jira/browse/CASSANDRA-14526, here is the [patch for dtest|https://github.com/apache/cassandra-dtest/compare/master...jaydeepkumar1984:14526-trunk]. Not sure if [~jay.zhuang] got a chance to fire dtest, if possible could you please help me start dtest with this patch?;;;","09/Jan/19 16:47;aweisberg;OK, it's better to avoid committing any dtest breakage. If there are 10 developers occasionally committing dtest breakage then each of them has to look at the current set of breakage and figure out if it was their changes that are causing issues or some other set of changes.

And I am very aware I am in a glass house when I say this!

If you want to break things into multiple JIRAs or commits it's fine, but it's less disruptive if you wait until everything is complete before landing any piece of it.

If Jay is reviewing the dtest changes I'll kick it off a dtest run, but let him finish the review.;;;","10/Jan/19 02:21;chovatia.jaydeep@gmail.com;Apologize for not having both the fixes landed at the same time, will take utmost care in the future.

[~jay.zhuang] If you have sometime then could you please validate the dtest and land it if it looks fine?;;;","10/Jan/19 06:14;jay.zhuang;I'm sorry for not committing the dtest change. Will do that ASAP (I'm still trying to confirm a few flaky tests are not introduced by the changes).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstableloader should use discovered broadcast address to connect intra-cluster,CASSANDRA-14522,13166176,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,Yarnspinner,jeromatron,jeromatron,14/Jun/18 18:47,16/Apr/19 09:29,13/Jul/23 08:37,06/Aug/18 16:12,3.0.18,3.11.4,,,,,Legacy/Tools,,,,0,lhf,,,"Currently, in the LoaderOptions for the BulkLoader, the user can give a list of initial host addresses.  That's to do the initial connection to the cluster but also to stream the sstables.  If you have two physical interfaces, one for rpc, the other for internode traffic, then bulk loader won't currently work.  It will throw an error such as:

{quote}
> sstableloader -v -u cassadmin -pw xxx -d 10.133.210.101,10.133.210.102,10.133.210.103,10.133.210.104 /var/lib/cassandra/commitlog/backup_tmp/test_bkup/bkup_tbl
Established connection to initial hosts
Opening sstables and calculating sections to stream
Streaming relevant part of /var/lib/cassandra/commitlog/backup_tmp/test_bkup/bkup_tbl/mc-1-big-Data.db /var/lib/cassandra/commitlog/backup_tmp/test_bkup/bkup_tbl/mc-2-big-Data.db  to [/10.133.210.101, /10.133.210.103, /10.133.210.102, /10.133.210.104]
progress: total: 100% 0  MB/s(avg: 0 MB/s)ERROR 10:16:05,311 [Stream #9ed00130-6ff6-11e8-965c-93a78bf96e60] Streaming error occurred
java.net.ConnectException: Connection refused
        at sun.nio.ch.Net.connect0(Native Method) ~[na:1.8.0_101]
        at sun.nio.ch.Net.connect(Net.java:454) ~[na:1.8.0_101]
        at sun.nio.ch.Net.connect(Net.java:446) ~[na:1.8.0_101]
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[na:1.8.0_101]
        at java.nio.channels.SocketChannel.open(SocketChannel.java:189) ~[na:1.8.0_101]
        at org.apache.cassandra.tools.BulkLoadConnectionFactory.createConnection(BulkLoadConnectionFactory.java:60) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamSession.createConnection(StreamSession.java:266) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.ConnectionHandler.initiate(ConnectionHandler.java:86) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamSession.start(StreamSession.java:253) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamCoordinator$StreamSessionConnector.run(StreamCoordinator.java:212) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-all-4.0.54.Final.jar:4.0.54.Final]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_101]
ERROR 10:16:05,312 [Stream #9ed00130-6ff6-11e8-965c-93a78bf96e60] Streaming error occurred
java.net.ConnectException: Connection refused
        at sun.nio.ch.Net.connect0(Native Method) ~[na:1.8.0_101]
        at sun.nio.ch.Net.connect(Net.java:454) ~[na:1.8.0_101]
        at sun.nio.ch.Net.connect(Net.java:446) ~[na:1.8.0_101]
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[na:1.8.0_101]
        at java.nio.channels.SocketChannel.open(SocketChannel.java:189) ~[na:1.8.0_101]
        at org.apache.cassandra.tools.BulkLoadConnectionFactory.createConnection(BulkLoadConnectionFactory.java:60) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamSession.createConnection(StreamSession.java:266) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.ConnectionHandler.initiate(ConnectionHandler.java:86) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamSession.start(StreamSession.java:253) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamCoordinator$StreamSessionConnector.run(StreamCoordinator.java:212) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-all-4.0.54.Final.jar:4.0.54.Final]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_101]
ERROR 10:16:05,312 [Stream #9ed00130-6ff6-11e8-965c-93a78bf96e60] Streaming error occurred
java.net.ConnectException: Connection refused
        at sun.nio.ch.Net.connect0(Native Method) ~[na:1.8.0_101]
        at sun.nio.ch.Net.connect(Net.java:454) ~[na:1.8.0_101]
        at sun.nio.ch.Net.connect(Net.java:446) ~[na:1.8.0_101]
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[na:1.8.0_101]
        at java.nio.channels.SocketChannel.open(SocketChannel.java:189) ~[na:1.8.0_101]
        at org.apache.cassandra.tools.BulkLoadConnectionFactory.createConnection(BulkLoadConnectionFactory.java:60) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamSession.createConnection(StreamSession.java:266) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.ConnectionHandler.initiate(ConnectionHandler.java:86) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamSession.start(StreamSession.java:253) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamCoordinator$StreamSessionConnector.run(StreamCoordinator.java:212) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-all-4.0.54.Final.jar:4.0.54.Final]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_101]
progress: total: 100% 0  MB/s(avg: 0 MB/s)WARN  10:16:05,320 [Stream #9ed00130-6ff6-11e8-965c-93a78bf96e60] Stream failed
Streaming to the following hosts failed:
[/10.133.210.101, /10.133.210.103, /10.133.210.102]
java.util.concurrent.ExecutionException: org.apache.cassandra.streaming.StreamException: Stream failed
        at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)
        at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)
        at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
        at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:122)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at com.datastax.bdp.tools.ShellToolWrapper.main(ShellToolWrapper.java:34)
Caused by: org.apache.cassandra.streaming.StreamException: Stream failed
        at org.apache.cassandra.streaming.management.StreamEventJMXNotifier.onFailure(StreamEventJMXNotifier.java:85)
        at com.google.common.util.concurrent.Futures$6.run(Futures.java:1310)
        at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:457)
        at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156)
        at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145)
        at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:202)
        at org.apache.cassandra.streaming.StreamResultFuture.maybeComplete(StreamResultFuture.java:215)
        at org.apache.cassandra.streaming.StreamResultFuture.handleSessionComplete(StreamResultFuture.java:191)
        at org.apache.cassandra.streaming.StreamSession.closeSession(StreamSession.java:449)
        at org.apache.cassandra.streaming.StreamSession.onError(StreamSession.java:549)
        at org.apache.cassandra.streaming.StreamSession.start(StreamSession.java:259)
        at org.apache.cassandra.streaming.StreamCoordinator$StreamSessionConnector.run(StreamCoordinator.java:212)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:745)
WARN  10:16:05,322 [Stream #9ed00130-6ff6-11e8-965c-93a78bf96e60] Stream failed
WARN  10:16:05,322 [Stream #9ed00130-6ff6-11e8-965c-93a78bf96e60] Stream failed
{quote}",,aweisberg,djoshi,jay.zhuang,jblangston@datastax.com,jeromatron,jjirsa,tommy_s,weideng,Yarnspinner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13639,,,,,,,,CASSANDRA-14623,,,,,,,,,,,,"01/Aug/18 19:46;jblangston@datastax.com;CASSANDRA-14522.patch;https://issues.apache.org/jira/secure/attachment/12933965/CASSANDRA-14522.patch",,,,,,,,,,,,,1.0,Yarnspinner,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 06 16:12:20 UTC 2018,,,,,,,,,,,"0|i3uvhz:",9223372036854775807,3.0.15,,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"02/Jul/18 14:07;Yarnspinner;Hello I would like to try working on this issue;;;","02/Jul/18 15:19;jjirsa;Assigned to you [~Yarnspinner] , good luck! Feel free to ask questions here or the dev@ list if needed. ;;;","03/Jul/18 06:35;tommy_s;This looks similar to CASSANDRA-13639, at least configuration should be done the same way.;;;","03/Jul/18 16:36;aweisberg;I think the solution presented in CASSANDRA-13639 is probably not what we are looking for. I am 90% confident this is the same complaint though. This is just the outgoing network interface that is chosen incorrectly I think.

Seems like this is a bit of a headache to test as well.;;;","08/Jul/18 13:18;Yarnspinner;Sorry for any silly questions. This is my first time working on a big open source project and I have an idea or two about how to fix it but I'm not sure if it's completely off-track. Would really appreciate osme help.

I poked around in the code and it seems that the sstableloader starts out by launching org.apache.cassandra.tools.BulkLoader which then creates an ExternalClient which is a NativeSSTableLoaderClient extended with 2 fields. The only place i see the storagePort used in there is 

{code:java}
try (Cluster cluster = builder.build(); Session session = cluster.connect()){
for (TokenRange tokenRange : tokenRanges)
            {
                Set<Host> endpoints = metadata.getReplicas(Metadata.quote(keyspace), tokenRange);
                Range<Token> range = new Range<>(tokenFactory.fromString(tokenRange.getStart().getValue().toString()),
                                                 tokenFactory.fromString(tokenRange.getEnd().getValue().toString()));
                for (Host endpoint : endpoints)
                {
                    int portToUse;
                    if (allowServerPortDiscovery)
                    {
                        portToUse = endpoint.getBroadcastAddressOptPort().portOrElse(storagePort);
                    }
                    else
                    {
                        portToUse = storagePort;
                    }
                    addRangeForEndpoint(range, InetAddressAndPort.getByNameOverrideDefaults(endpoint.getAddress().getHostAddress(), portToUse));
                }
            }
}
{code}

So I think there might be 2 ways to fix it?

#
Instead of using endpoint.getAddress().getHostAddress(), use endpoint.getBroadcastAddressOptPort().address?

#
Use the session to execute a CQLSH query on the system.peers table and then parse the broadcast addresses from there?

Also, is there a way to test this on a single node? Or would I need to go get 2 AWS nodes and configure broadcast addresses that differ from the RPC address before testing it on them?

Thank you!;;;","09/Jul/18 19:37;aweisberg;There are (almost) no silly questions.

You can run a cluster of Cassandra nodes on a single machine using https://github.com/riptano/ccm 

We write automated tests that use ccm called dtests https://github.com/apache/cassandra-dtest which I don't think you will have to work with for this.

You will need some familiarity with Python and virtual environments and setting up requirements files. We use Python 3 now for the tests but cqlsh is Python 2. Let me know if you need help.

For testing this you don't need multiple Cassandra processes, but you would need to setup up a second loopback interface that isn't reachable from your usual one so if it picks the incorrect outbound interface it fails to connect. You can probably do this with a firewall rule.

So that code you are looking at isn't the problem code I think. It's picking the correct interface to connect to based on the cluster metadata from the driver, but it's probably picking the wrong interface to connect on in BulkLoadConnectionFactory. On trunk this code has changed so I am not sure if it's still an issue https://github.com/apache/cassandra/commit/fc92db2b9b56c143516026ba29cecdec37e286bb#diff-fc44570de4b634df61bd83b639db98d4L51 and where things go wrong is buried a little deeper. https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/streaming/StreamSession.java#L197

[~jasobrown] ^ I think this may be an issue with Netty  intracluster streaming and security? Is it going to pick the wrong address just like the bulk loader? I am assuming not because the server loads the YAML and it all works out fine even in an AWS like setup.

In 3.0 where this is reported it is https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/tools/BulkLoadConnectionFactory.java#L51 and it's calling FBUtilities.getLocalAddress() to pick the outbound interface. 

Looking at the ticket I see connection refused so it's reaching the other node, but the other node isn't bound. Just be aware if you test this you will get a different result if there is no route.

TL;DR in all cases I think what we want is FBUtilities.getLocalAddress() to be used to provide a default interface, but there is a command line option to specify the outbound interface for streaming. In 3.0 and other versions forwards (3.11) and backwards (2.2, 2.1) this propagates into BulkLoadConnectionFactory. In trunk/4.0 it will have to propagate all the way into where StreamSession is calling FBUtilities.getLocalAddress(). One option might be to manually set the value in FBUtilities. You will need to create and test a patch for each version.;;;","09/Jul/18 19:45;aweisberg;And yes AWS with a small cheap instance may be the fastest easiest way to test this if you don't already know the voodoo for firewall rules, routing, and network interfaces to test this on your local machine.;;;","11/Jul/18 00:07;Yarnspinner;[~aweisberg] Thank you very much for the explanation! I will try out the testing setup and see how it goes.;;;","01/Aug/18 06:43;Yarnspinner;Hello,

I reproduced the error on my computer by setting different ip addresses for rpc_address, listen_address and local_address on my local cassandra instance. I then fixed it for 3.0 by querying system.peers and system.local to map the rpc_address to the listen_address in NativeSSTableLoaderClient.init. Is this a good way to go about it? 

Diff:
https://github.com/apache/cassandra/compare/trunk...yarnspinnered:14522-3.0

The tests pass except a few which failed due to timeout. However, these tests are unrelated and also fail on a clean copy cloned from the repo. Tests like 'testUpdateColumnNotInViewWithFlush,  testClusteringKeyMultiColumnRestrictions,  testClusteringOrder'.;;;","01/Aug/18 19:47;jblangston@datastax.com;There is a simpler fix. The Host object being iterated over in that loop has methods to get the listen address directly. You just need to change endpoint.getAddress to endpoint.getBroadcastAddress.  I have also attached a patch.  The patch is against Cassandra 3.0 and should merge forward cleanly.

Note: there is also a Host.getListenAddress method which returns the local listen address, but we want to use the broadcast address in case the sstableloader is run in a different DC that cannot communicate with a remote node over the local listen address.  

I also noticed that you checked in your changes to cassandra.yaml that you were using to test this. That should be reverted.;;;","01/Aug/18 21:25;djoshi;Has someone tried reproducing this on trunk? I suspect this is fixed on trunk. See: CASSANDRA-14389;;;","02/Aug/18 14:45;jblangston@datastax.com;Yes, it does appear to be fixed in trunk.;;;","02/Aug/18 16:06;aweisberg;OK, apparently I am deeply confused and this and CASSANDRA-13639 is not the same as this and you correctly fixed this issue. [~Yarnspinner] you might want to try CASSANDRA-13639 next it is also relatively straightforward and my [earlier comment|https://issues.apache.org/jira/browse/CASSANDRA-14522?focusedCommentId=16537451&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16537451] applies to it.

It looks like [~jblangston@datastax.com]'s patch should apply cleanly from 2.2-3.11 where this bug exists so I'm going to go with that.;;;","02/Aug/18 16:37;aweisberg;In 2.2 the Java driver doesn't provide the information from {{system.peers}} I think. It only provides {{getAddress()}}. So it will need to go and query {{system.peers}}.

[~Yarnspinner] in your patch you read {{local}} and {{peers}} but with the driver's load balancing it's not guaranteed that {{local}} and {{peers}} will be read from the same node so you can still miss one. I am not sure if the {{listenAddress}} value is better? It's not broadcast address so it's not 100% correct.

It's kind of a mess because the comments in that version of the driver say this:
{code:java}
    // The listen_address (really, the broadcast one) as know by Cassandra. We use that internally because
    // that's the 'peer' in the 'System.peers' table and avoids querying the full peers table in
    // ControlConnection.refreshNodeInfo. We don't want to expose however because we don't always have the info
    // (partly because the 'System.local' doesn't have it for some weird reason for instance).
    volatile InetAddress listenAddress;
{code}

Not quite sure how to fix that in 2.2. Maybe you can research how to fix this in 2.2.;;;","06/Aug/18 15:17;aweisberg;I'm going to commit and close this for 3.0 and 3.11 and create a separate ticket for 2.2 since it is pretty involved to figure out how to do it right there.

[3.0 CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/cassandra-14522-3.0]
[3.11 CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/cassandra-14522-3.11]

dtest failures appear unrelated.;;;","06/Aug/18 16:12;aweisberg;Committed as [3d48cbc74cb50e02e85ed51ccf6a3a46690c3a99|https://github.com/apache/cassandra/commit/3d48cbc74cb50e02e85ed51ccf6a3a46690c3a99]. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
filter sstables by min/max clustering bounds during reads,CASSANDRA-14516,13165632,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,n.v.harikrishna,bdeggleston,bdeggleston,12/Jun/18 17:34,15/May/20 08:01,13/Jul/23 08:37,20/May/19 19:32,4.0,4.0-alpha1,,,,,Legacy/Local Write-Read Paths,,,,0,,,,"In SinglePartitionReadCommand, we don't filter out sstables whose min/max clustering bounds don't intersect with the clustering bounds being queried. This causes us to do extra work on the read path.",,aleksey,bdeggleston,jeromatron,n.v.harikrishna,rha,samt,vinaykumarcse,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,n.v.harikrishna,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 20 19:32:05 UTC 2019,,,,,,,,,,,"0|i3us53:",9223372036854775807,,,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"17/May/19 08:08;n.v.harikrishna;I tried to reproduce the issue by following these steps.

1. Create keyspace
{code:java}
CREATE KEYSPACE ks1 WITH replication = {'class': 'SimpleStrategy' , 'replication_factor': 1 };{code}
2. Create table
{code:java}
CREATE TABLE ks1.t1 ( k1 int , c1 int, c2 int, PRIMARY KEY (k1, c1, c2));{code}
3. Insert few rows
{code:java}
INSERT INTO ks1.t1 (k1 , c1 , c2 ) VALUES ( 5, 1, 1);
INSERT INTO ks1.t1 (k1 , c1 , c2 ) VALUES ( 5, 1, 2);
INSERT INTO ks1.t1 (k1 , c1 , c2 ) VALUES ( 5, 2, 3);
INSERT INTO ks1.t1 (k1 , c1 , c2 ) VALUES ( 5, 2, 4);
{code}
4. Nodetool flush.
 5. Insert more rows
{code:java}
INSERT INTO ks1.t1 (k1 , c1 , c2 ) VALUES ( 5, 7, 5);
INSERT INTO ks1.t1 (k1 , c1 , c2 ) VALUES ( 5, 7, 6);
INSERT INTO ks1.t1 (k1 , c1 , c2 ) VALUES ( 5, 8, 7);
INSERT INTO ks1.t1 (k1 , c1 , c2 ) VALUES ( 5, 8, 8);
{code}
6. Nodetool flush.
 7. Now run select command with clustering bounds
{code:java}
SELECT * FROM ks1.t1 WHERE k1 = 5 and c1 > 1 and c1 < 3;

 k1 | c1 | c2
----+----+----
  5 |  2 |  3
  5 |  2 |  4

(2 rows)

Tracing session: bc217bd0-7818-11e9-a606-b7a04374fcea

 activity                                                                                       | timestamp                  | source    | source_elapsed | client
------------------------------------------------------------------------------------------------+----------------------------+-----------+----------------+-----------
                                                                             Execute CQL3 query | 2019-05-17 01:55:24.237000 | 127.0.0.1 |              0 | 127.0.0.1
 Parsing SELECT * FROM ks1.t1 WHERE k1 = 5 and c1 > 1 and c1 < 3; [Native-Transport-Requests-1] | 2019-05-17 01:55:24.238000 | 127.0.0.1 |            321 | 127.0.0.1
                                              Preparing statement [Native-Transport-Requests-1] | 2019-05-17 01:55:24.238000 | 127.0.0.1 |            674 | 127.0.0.1
                                           Executing single-partition query on t1 [ReadStage-2] | 2019-05-17 01:55:24.239000 | 127.0.0.1 |           1402 | 127.0.0.1
                                                     Acquiring sstable references [ReadStage-2] | 2019-05-17 01:55:24.239000 | 127.0.0.1 |           1500 | 127.0.0.1
        Skipped 1/2 non-slice-intersecting sstables, included 0 due to tombstones [ReadStage-2] | 2019-05-17 01:55:24.239000 | 127.0.0.1 |           1654 | 127.0.0.1
                                                      Key cache hit for sstable 1 [ReadStage-2] | 2019-05-17 01:55:24.239000 | 127.0.0.1 |           1787 | 127.0.0.1
                                        Merged data from memtables and 1 sstables [ReadStage-2] | 2019-05-17 01:55:24.240000 | 127.0.0.1 |           2067 | 127.0.0.1
                                           Read 2 live rows and 0 tombstone cells [ReadStage-2] | 2019-05-17 01:55:24.240000 | 127.0.0.1 |           2181 | 127.0.0.1
                                                                               Request complete | 2019-05-17 01:55:24.239666 | 127.0.0.1 |           2666 | 127.0.0.1
{code}
 

I can see one SSTable skipped while executing the query from the trace. Looks like working as expected.

A check is already in the code: [https://github.com/apache/cassandra/blob/b80f6c65fb0b97a8c79f6da027deac06a4af9801/src/java/org/apache/cassandra/db/SinglePartitionReadCommand.java#L650];;;","20/May/19 19:32;bdeggleston;Thanks for looking into this [~n.v.harikrishna]. Sorry for the false alarm.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Short read protection in presence of almost-purgeable range tombstones may cause permanent data loss,CASSANDRA-14515,13165618,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,aleksey,aleksey,aleksey,12/Jun/18 16:46,15/May/20 08:01,13/Jul/23 08:37,20/Jun/18 15:15,3.0.17,3.11.3,4.0,4.0-alpha1,,,Consistency/Coordination,Local/SSTable,,,0,,,,"Because read responses don't necessarily close their open RT bounds, it's possible to lose data during short read protection, if a closing bound is compacted away between two adjacent reads from a node.",,aleksey,bdeggleston,benedict,cscotta,jay.zhuang,jeromatron,laxmikant99,marcuse,mck,mshuler,samt,sbtourist,shaurya10000,sivann,weideng,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14672,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,Correctness -> Unrecoverable Corruption / Loss,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Dec 31 12:50:51 UTC 2018,,,,,,,,,,,"0|i3us1z:",9223372036854775807,,,,,,,,,bdeggleston,,bdeggleston,,,Critical,,,,,,,,,,,,,,,,,,,"12/Jun/18 16:56;aleksey;It came up during CASSANDRA-14330 investigation/resolution that a read response doesn't necessarily close its outstanding RT. This happens because we stop constructing the response as soon as we've counted sufficient rows to satisfy the requested limit from a node. The fix was incomplete, however, and rather than fixing the assertion we should instead fix the underlying issue, and put an artificial lid on any read response. Otherwise the following sequence of events is possible:

1. The coordinator is sending one of requests to node {{A}}, with limit of {{n}}
2. Node {{A}} replies with a sequence: {{rt-[}}, {{row-0}}, {{row-1}}, {{row-2}}, ..., {{row-n}}
3. {{rt}} is past gc grace, and gets compacted away
4. Some of the rows from {{A}} end up shadowed by deletions from other replicas, and SRP triggers a follow-up read request
5. Node {{A}} replies with a sequence that doesn't contain {{rt-]}}, because it's been compacted away

As a result we have an open-ended RT that can propagate over RR and erase rows it was never intended to erase.;;;","19/Jun/18 23:07;aleksey;3.0 branch [here|https://github.com/iamaleksey/cassandra/commits/14515-3.0]. Also, in addition to the unit tests in the branch, 14330 dtest is now essentially a dtest for this ticket.;;;","19/Jun/18 23:29;aleksey;[3.11 branch|https://github.com/iamaleksey/cassandra/commits/14515-3.11] and [4.0 branch|https://github.com/iamaleksey/cassandra/commits/14515-4.0].

CI [here|https://circleci.com/workflow-run/98c78715-3725-422f-99c3-81fa4e199666], [here|https://circleci.com/workflow-run/ead96ef6-1d10-4d39-af0b-16a656add658], and [here|https://circleci.com/workflow-run/cce784b8-3131-48b5-aa0d-a3be98cc9741].;;;","20/Jun/18 00:07;bdeggleston;+1, assuming the tests look good. Also, you left some circleci stuff in there which should be removed on commit.;;;","20/Jun/18 00:22;cscotta;Thanks for the quick review! [~beobal], can you review as wel?;;;","20/Jun/18 14:46;samt;+1 LGTM too. Tests do look good - there's 1 failure in a semi-relevant dtest ({{consistency_test.py::TestConsistency::test_short_read}} on the 3.11 branch), but I've verified it's unrelated to this patch.;;;","20/Jun/18 15:15;aleksey;Thanks guys. Committed to 3.0 as [4e23c9e4dba6ee772531d82980f73234bd41869a|https://github.com/apache/cassandra/commit/4e23c9e4dba6ee772531d82980f73234bd41869a] and merged upwards.

The nits you both had (offline) were addressed on commit.;;;","29/Aug/18 07:10;sivann;please check CASSANDRA-14672 it appears this commit might have caused a new issue.;;;","31/Dec/18 12:50;laxmikant99;Hi All,

Just want to understand the impact area of this ticket  for ppl using 3.11.2 (if table has PK as (id, clust1, clust2) )
{code:java}
1. DELETE FROM test.tombstones WHERE id = ? (causes partition tombstone)
2. DELETE FROM test.tombstones WHERE id = ? AND clust1 = ? (causes multi row tombstone) 
3. DELETE FROM test.tombstones WHERE id = ? AND clust1 = ? AND clust2 = ? (causes single row tombstone) 
4. DELETE FROM test.tombstones WHERE id = ? AND clust1 > ? AND clust1 <= ? (causes range tombstone) 
5. INSERT INTO test.tombstones (id, clust1 ,clust2, val1) VALUES ('id1', 'a','b','c' ) USING TTL 1; (causes TTL tombstone) 
6. DELETE val1 from test.tombstones WHERE id = ? AND clust1 = ? AND clust2 = ? (causes cell tombstone){code}
I assume that 6th query (cell tombstone) is out of danger but what about other queries ?
 Is the only 4th query is impacted or all of the above (except 6th one) ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reverse order queries in presence of range tombstones may cause permanent data loss,CASSANDRA-14513,13165591,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,samt,samt,samt,12/Jun/18 15:01,15/May/20 08:02,13/Jul/23 08:37,14/Jun/18 17:23,3.0.17,3.11.3,4.0,4.0-alpha1,,,Legacy/Core,Legacy/CQL,Legacy/Local Write-Read Paths,,0,,,,"Slice queries in descending sort order can create oversized artificial range tombstones. At CL > ONE, read repair can propagate these tombstones to all replicas, wiping out vast data ranges that they mistakenly cover.",,aleksey,christopher.lambert,cscotta,djoshi,drolando,jasonstack,jay.zhuang,jeromatron,jjirsa,marcuse,mshuler,n.v.harikrishna,nachiket_patil,nff,philipthompson,samt,sbtourist,shaurya10000,vinaykumarcse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,Correctness -> Unrecoverable Corruption / Loss,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 14 18:52:21 UTC 2018,,,,,,,,,,,"0|i3urvz:",9223372036854775807,,,,,,,,,aleksey,,aleksey,,,Critical,,,,,,,,,,,,,,,,,,,"12/Jun/18 15:56;samt;The problem manifests when executing a slice query with reverse ordering against an indexed partition if the upper bound of the query precedes the first clustering in the partition for a given SSTable.

The initial search of the index correctly identifies that the slice bounds are not contained within the partition and {{ReverseIndexedReader::setForSlice}} returns an empty iterator. However, it doesn’t update the pointer to the current index block in {{IndexState}}. The pointer remains set to the size of the column index, so that when the initial empty iterator is exhausted {{ReversedIndexReader::hasNextInternal}} incorrectly assumes that there is more to do, bumps the pointer back one to the last index block and starts reading.

If a range tombstone spans the boundary between the penultimate and final index blocks, the iterator will emit the end marker after first altering the bounds to match those of the query. The assumption made is that only data that falls within the bounds of the query slice will be read from disk and so adjusting the tombstone bounds in this way is simply a narrowing of the range tombstone. The index block pointer bug invalidates this assumption and so a wholly new and invalid marker is generated.

On a single node this new marker alone can shadow live data in other sstables, but the effect is transient. A tombstone never gets written to disk and when the SSTable is compacted, the layout of the partition on disk will _likely_ no longer trigger the bug (though is no guarantee of this).

In a multi-node scenario read repair can cause the erroneous marker to be matched to an (unrelated) marker from another replica, creating a new tombstone, potentially with a very wide range. This is then propagated to all replicas, causing data loss from the partition.;;;","12/Jun/18 16:01;aleksey;A dtest representing both scenarios can be found [here|https://github.com/iamaleksey/cassandra-dtest/commits/14513].

{{test_14513_transient}} shows that the issue can be reproduced with just one node - although there is no permanent data loss here, just queries not returning all the results they are supposed to. Which is bad in itself, but not as bad as the other scenario.

{{test_14513_permanent}} illustrates how that oversized tombstone can be propagated by read repair to every replica and wipe out the partition.

Both tests are a bit longer than they need be - minimal reproduction can be achieved in half as much code, but I opted for showing the full impact in an intentionally more verbose manner.;;;","12/Jun/18 16:41;samt;Trivial fix is to correctly adjust the current index pointer in IndexState when the slice bounds are found to be wholly before the start of the partition. It might make sense to open a follow up JIRA to investigate whether the modification of the tombstone bounds (in {{RangeTombstoneList::reverseIterator}}, and maybe {{forwardIterator}} if necessary) can be tightened up by asserting that any newly generated bounds are not disjoint from the query slice.
||branch||CircleCI||
|[3.0|https://github.com/beobal/cassandra/tree/14513-3.0]|[circle|https://circleci.com/workflow-run/16abca6e-d7e8-4671-aaeb-4f9de32b8190]|
|[3.11|https://github.com/beobal/cassandra/tree/14513-3.11]|[circle|https://circleci.com/workflow-run/7c79b1d7-26db-436f-b156-cdf05284b85a]|
|[trunk|https://github.com/beobal/cassandra/tree/14513-4.0]|[circle|https://circleci.com/workflow-run/076bf598-8a40-4637-9de2-f936c62f8863]|

 CI runs are using [~iamaleksey]'s dtest branch mentioned in the previous comment.;;;","14/Jun/18 11:16;aleksey;+1;;;","14/Jun/18 17:23;samt;Thanks. Committed to 3.0 as {{eb91942f64972bef04c4e965dcdf788ae1f21a60}} and merged to 3.11 & trunk.;;;","14/Jun/18 18:52;aleksey;Committed the dtests as [51c8352020b8df3fe04344ae88c29d2a73a228bd|https://github.com/apache/cassandra-dtest/commit/51c8352020b8df3fe04344ae88c29d2a73a228bd].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AsyncOneResponse uses the incorrect timeout,CASSANDRA-14509,13165050,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,djoshi,djoshi,djoshi,08/Jun/18 23:37,15/May/20 08:00,13/Jul/23 08:37,11/Jun/18 15:57,4.0,4.0-alpha1,,,,,Legacy/Core,,,,0,,,,"{{AsyncOneResponse}} has a bug where it uses the initial timeout value instead of the adjustedTimeout. Combined with passing in the wrong {{TimeUnit}}, it leads to a shorter timeout than expected. This can have unintended consequences for example, in {{StorageService::sendReplicationNotification}} instead of waiting 10 seconds ({{request_timeout_in_ms}}), we wait for {{10000}} Nano Seconds.
",,djoshi,jay.zhuang,jeromatron,marcuse,weideng,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14510,,,,,,,,,,CASSANDRA-14514,,,,,,,,,,,,,,,,,,,,,,,0.0,djoshi,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 11 15:57:55 UTC 2018,,,,,,,,,,,"0|i3uojz:",9223372036854775807,,,,,,,,,marcuse,,marcuse,,,Normal,,,,,,,,,,,,,,,,,,,"08/Jun/18 23:42;djoshi;||14509||
|[branch|https://github.com/dineshjoshi/cassandra/tree/trunk-14509]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/trunk-14509]|
||;;;","09/Jun/18 00:53;djoshi;[~krummas] I have updated the branch with a unit test.;;;","11/Jun/18 15:57;marcuse;and committed as {{6da5fb56c8e0777843e88359a45a461a9f9eb639}} without the test.runners change - if that is a problem we should probably fix that in a separate ticket, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fqltool should open chronicle queue read only and a GC bug,CASSANDRA-14504,13164747,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aweisberg,aweisberg,aweisberg,07/Jun/18 17:42,15/May/20 08:02,13/Jul/23 08:37,18/Jun/18 16:53,4.0,4.0-alpha1,,,,,Legacy/Tools,,,,0,fqltool,,,"There are two issues with fqltool.

The first is that it doesn't open the chronicle queue read only so it won't work if it doesn't have write permissions and it's not clear if it's safe to open the queue to write if the server is also still appending.

The next issue is that NativeBytesStore.toTemporaryDirectByteBuffer() returns a ByteBuffer that doesn't strongly reference the memory it refers to resulting it in sometimes being reclaimed and containing the wrong data when we go to read from it. At least that is the theory. Simple solution is to use toByteArray() and that seems to make it work consistently.",,aweisberg,eperott,jay.zhuang,jeromatron,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aweisberg,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 18 16:53:38 UTC 2018,,,,,,,,,,,"0|i3umon:",9223372036854775807,,,,,,,,,samt,,samt,,,Normal,,,,,,,,,,,,,,,,,,,"07/Jun/18 18:31;aweisberg;[Fixes|https://github.com/apache/cassandra/compare/trunk...aweisberg:cassandra-14504-trunk?expand=1]
[CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/cassandra-14504-trunk];;;","18/Jun/18 16:10;samt;LGTM (minus the circle yaml change ofc);;;","18/Jun/18 16:53;aweisberg;Commited as [c6570fac180b6f816efb47cbd9b7fe30c771835d|https://github.com/apache/cassandra/commit/c6570fac180b6f816efb47cbd9b7fe30c771835d]. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
toDate() CQL function is instantiated for wrong argument type,CASSANDRA-14502,13164639,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,blerer,Sarna,Sarna,07/Jun/18 08:44,15/May/20 08:02,13/Jul/23 08:37,31/Jul/18 13:03,4.0,4.0-alpha1,,,,,Legacy/CQL,,,,0,,,,"{{toDate()}} function is instantiated to work for {{timeuuid}} and {{date}} types passed as an argument, instead of {{timeuuid}} and {{timestamp}}, as stated in this documentation: [http://cassandra.apache.org/doc/latest/cql/functions.html#datetime-functions]

As a result it's possible to convert a {{date}} into {{date}}, but not a {{timestamp}} into {{date}}, which is probably what was meant.",,blerer,jeromatron,rha,Sarna,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/18 16:09;blerer;CASSANDRA-14502.txt;https://issues.apache.org/jira/secure/attachment/12933609/CASSANDRA-14502.txt",,,,,,,,,,,,,1.0,blerer,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 31 13:03:41 UTC 2018,,,,,,,,,,,"0|i3um0v:",9223372036854775807,,,,,,,,,snazy,,snazy,,,Low,,,,,,,,,,,,,,,,,,,"31/Jul/18 07:28;snazy;+1;;;","31/Jul/18 13:03;blerer;Committed in trunk as 76c7d02c94c735bc5e7ef6ff45d9ddc777b176dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Audit log does not include statements on some system keyspaces,CASSANDRA-14498,13164206,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vinaykumarcse,eperott,eperott,05/Jun/18 15:36,15/May/20 08:02,13/Jul/23 08:37,19/Nov/18 11:38,4.0,4.0-alpha1,,,,,Feature/Authorization,,,,0,audit,lhf,security,"Audit logs does not include statements on the ""system"" and ""system_schema"" keyspace.

It may be a common use case to whitelist queries on these keyspaces, but Cassandra should not make assumptions. Users who don't want these statements in their audit log are still able to whitelist them with configuration.",,eperott,jeromatron,marcuse,vinaykumarcse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/18 20:26;vinaykumarcse;14498-trunk.txt;https://issues.apache.org/jira/secure/attachment/12946611/14498-trunk.txt",,,,,,,,,,,,,1.0,vinaykumarcse,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 19 11:38:10 UTC 2018,,,,,,,,,,,"0|i3ujcv:",9223372036854775807,4.0,,,,,,,,,,eperott,marcuse,,Normal,,4.0,,,,,,,,,,,,,,,,,"06/Jun/18 18:27;vinaykumarcse;Just curious, are there any use cases where you would to audit system keyspaces? fwiw auditing these generate lot of noise as C* calls system keyspaces in many places throughout its lifetime;;;","07/Jun/18 06:30;eperott;bq. are there any use cases where you would to audit system keyspaces?

One use case would be to get audit logs on all operations from selected users.

bq. auditing these generate lot of noise as C* calls system keyspaces in many places

Internal calls in C* will not come through the audit logger. Right? I've observed that client drivers will emit some queries on their own. This typically happens when a user login or when there are schema changes. But that only represents a fraction of all operations coming from a client.

The problem I see with a hard coded filter is that it will not only filter out queries from the driver, but also any query issued by the client application on those keyspaces.

The decision should be with the administrator of the cluster and it will still be possible to whitelist these queries with configuration. We could add some documentation on this so that users will not get surprised when they see queries in the log that they didn't expect.;;;","27/Jun/18 00:12;vinaykumarcse;{quote}The problem I see with a hard coded filter is that it will not only filter out queries from the driver, but also any query issued by the client application on those keyspaces.

The decision should be with the administrator of the cluster and it will still be possible to whitelist these queries with configuration. We could add some documentation on this so that users will not get surprised when they see queries in the log that they didn't expect.
{quote}
I buy this argument, will start working on it, we can ship with default excluding system keyspaces and let administrator tweak it as needed.;;;","03/Jul/18 12:27;eperott;Thanks! I'm happy to review.;;;","17/Oct/18 06:26;eperott;[~vinaykumarcse], any progress on this ticket?

Would be nice to get this into 4.0.;;;","18/Oct/18 07:56;vinaykumarcse;[~eperott] Attached the patch to remove {{system}}, {{system_schema}} keyspaces exclusion from AuditLogManager. This patch allows the user to enable the audit log for system keyspaces. ;;;","19/Oct/18 14:58;eperott;Thanks! The patch looks mostly good to me.

There is one corner case that I'm not able to cover properly though; that is having audit logs on _all_ keyspaces. This is not possible to configure, I think, since it is not possible to have an empty {{excluded_keyspaces}} list in the yaml, and anything listed in {{included_keyspaces}} will be overruled by things in the {{excluded_keyspaces}} list. The only way around it would be to configure a single non-existing keyspace in the {{excluded_keyspaces}} list, but that doesn't feel quite right.

Also, most out of curiosity, what's the reasoning for adding {{system_virtual_schema}} to the default exclude-list?

 ;;;","19/Oct/18 21:45;vinaykumarcse;Thanks for reviewing the patch [~eperott]
{quote}There is one corner case that I'm not able to cover properly though; that is having audit logs on all keyspaces.
{quote}
You could have an empty {{excluded_keyspaces}} in yaml without mentioning any keyspaces. 
e.g.,
{code:java}
excluded_keyspaces: """"
{code}
{quote}Also, most out of curiosity, what's the reasoning for adding system_virtual_schema to the default exclude-list?
{quote}
{{system_virtual_schema}} was introduced as part of CASSANDRA-7622, these column families tend to get queries from tools which are often at high frequency and generates a lot of audit messages from internal tools. However, if an operator decides to audit this keyspace they can do that from {{cassandra.yaml}} file

Let me know if this does not satisfy your requirements.;;;","22/Oct/18 10:51;eperott;{quote}You could have an empty {{excluded_keyspaces}} in yaml without mentioning any keyspaces.
{quote}
Ahh, right! That works for me. And same approach is applicable for the nodetool options.

I'm +1 on this patch!;;;","31/Oct/18 16:03;eperott;[~krummas], since you're looking into releated parts in CASSANDRA-14772, would you be able to review (and merge) this patch?;;;","02/Nov/18 15:33;marcuse;sure, I'll try to have a look next week;;;","16/Nov/18 14:12;marcuse;this lgtm, with a tiny nit: https://github.com/krummas/cassandra/commit/ae103bfef73abdaa5f91bb7a0be75cbcbcd3ae62

running tests here: https://circleci.com/workflow-run/1d78e8cc-98a2-4acd-af47-505064f94c6f - will commit if they look ok;;;","16/Nov/18 15:19;marcuse;seems a bunch of unit tests fail:
https://circleci.com/gh/krummas/cassandra/1031#tests/containers/13;;;","16/Nov/18 22:36;vinaykumarcse;Thanks for reviewing, I am looking into those failed tests. ;;;","17/Nov/18 07:24;vinaykumarcse;
[~krummas] Updated the patch to fix the tests (attached patch worked on trunk without CASSANDRA-13668 changes). Below is the branch and CircleCI unit tests, there is one unit test failing which is being addressed in CASSANDRA-14889


||trunk||Circle CI||
|[trunk_CASS-14498|https://github.com/vinaykumarchella/cassandra/tree/trunk_CASS-14498]|[utests|https://circleci.com/gh/vinaykumarchella/cassandra/321#tests/containers/68]|;;;","19/Nov/18 11:38;marcuse;tests look good, committed as {{f46762eeca9f5d7e32e731573a8c3e521b70fc05}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TWCS erroneously disabling tombstone compactions when unchecked_tombstone_compaction=true,CASSANDRA-14496,13164021,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,alex.ivakov,tarrall,tarrall,05/Jun/18 00:21,27/May/22 19:25,13/Jul/23 08:37,02/Jul/21 18:52,3.11.11,4.0.1,4.1,4.1-alpha1,,,Local/Compaction,,,,0,,,,"This code:
{code:java}
this.options = new TimeWindowCompactionStrategyOptions(options);
if (!options.containsKey(AbstractCompactionStrategy.TOMBSTONE_COMPACTION_INTERVAL_OPTION) && !options.containsKey(AbstractCompactionStrategy.TOMBSTONE_THRESHOLD_OPTION))
{
disableTombstoneCompactions = true;
logger.debug(""Disabling tombstone compactions for TWCS"");
}
else
logger.debug(""Enabling tombstone compactions for TWCS"");
}
{code}
... in TimeWindowCompactionStrategy.java disables tombstone compactions in TWCS if you have not *explicitly* set either tombstone_compaction_interval or tombstone_threshold.  Adding 'tombstone_compaction_interval': '86400' to the compaction stanza in a table definition has the (to me unexpected) side effect of enabling tombstone compactions. 

This is surprising and does not appear to be mentioned in the docs.

I would suggest that tombstone compactions should be run unless these options are both set to 0.

If the concern is that (as with DTCS in CASSANDRA-9234) we don't want to waste time on tombstone compactions when we expect the tables to eventually be expired away, perhaps we should also check unchecked_tombstone_compaction and still enable tombstone compactions if that's set to true.

May also make sense to set defaults for interval & threshold to 0 & disable if they're nonzero so that setting non-default values, rather than setting ANY value, is what determines whether tombstone compactions are enabled?",,alex.ivakov,jeromatron,KurtG,marcuse,rustyrazorblade,tarrall,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,alex.ivakov,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 02 18:52:29 UTC 2021,,,,,,,,,,,"0|i3ui7r:",9223372036854775807,,,,,,,,,,,marcuse,,,Low,,NA,,,https://github.com/apache/cassandra/commit/fd9a2820a1ceb1812fce4f4cc71abc0324d5908f,,,,,,,,,,,,,,"05/Jun/18 16:27;rustyrazorblade;I'm not a fan of enabling them by default.  There's no value for the frequency that make any sense at all given that a TTL could either be 5 minutes or 5 years.   

If someone supplies the {{tombstone_compaction_interval}} I suppose it wouldn't be unreasonable to enable them.;;;","05/Jun/18 23:23;tarrall;I definitely agree with respect to ""don't enable by default.""

However, I believe if I have set {{unchecked_tombstone_compaction}} to true, I'm asking for these compactions.  I may however be missing something – is there another purpose for that option?  I.e. might someone else be setting that to true who would be surprised to find it enables tombstone compactions?  I can't find any documentation which explains that {{'unchecked_tombstone_compaction': 'true'}} has no effect unless you also explicitly set other options, and I see discussions in blog posts that suggest people think that setting is how you enable tombstone compactions in TWCS, and I had to rummage around in source code for a while to work out why I wasn't getting those compactions.

Coming at this from the other direction may help.  If I have the following compaction defined:
{code:java}
'class': 'org.apache.cassandra.db.compaction.TimeWindowCompactionStrategy', 'compaction_window_size': '1', 'compaction_window_unit': 'HOURS', 'unchecked_tombstone_compaction': 'true'{code}
... I don't get tombstone compactions.  When I change that to:
{code:java}
'class': 'org.apache.cassandra.db.compaction.TimeWindowCompactionStrategy', 'compaction_window_size': '1', 'compaction_window_unit': 'HOURS', 'tombstone_compaction_interval': '86400', 'unchecked_tombstone_compaction': 'true'
{code}
... I have just switched from ""no tombstone compactions"" to ""tombstone compactions"".  This seems like a surprising side effect – one would not expect that explicitly setting an option to its default value would change behavior like this.

If there's another purpose for unchecked_tombstone_compaction, I'd recommend the defaults for TWCS make it clear that tombstone compactions intentionally act differently from STCS; instead of interval & threshold defaults of 86400 & 0.2, they should be infinitely high, and documented as such, so that you must specify non-default values in order to get tombstone compactions.;;;","06/Jun/18 03:00;tarrall;BTW rereading my original ticket here, I see I totally failed to mention I had enabled {{unchecked_tombstone_compaction}} ... which was really the key to why I opened the ticket.

To hopefully clarify: if {{unchecked_tombstone_compaction}} is set to true, tombstone compactions should be enabled, even in TWCS.  We should not have to also set interval or threshold in order to enable.

Also, it would probably be better to have maximally high default values for interval and threshold to more correctly indicate default behavior.  The default isn't really 86400 and 0.2; it's ""never"".  That's the right choice for most TWCS users, but isn't clear from the docs.;;;","21/Jun/18 06:28;alex.ivakov;[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...aivakov:CASSANDRA-14496]

Attaching a patch for the above. Simply checking if unchecked_tombstone_compactions is set to true, regardless of tombstone_compaction_interval and tombstone_threshold being set. Also added a unit test.;;;","01/Jul/21 20:02;brandon.williams;||Branch||CI||
|[3.11|https://github.com/driftx/cassandra/tree/CASSANDRA-14496]|[circle|https://app.circleci.com/pipelines/github/driftx/cassandra?branch=CASSANDRA-14496]|
|[4.0|https://github.com/driftx/cassandra/tree/CASSANDRA-14496-4.0]|[circle|https://app.circleci.com/pipelines/github/driftx/cassandra?branch=CASSANDRA-14496-4.0]|
|[trunk|https://github.com/driftx/cassandra/tree/CASSANDRA-14496-trunk]|[circle|https://app.circleci.com/pipelines/github/driftx/cassandra?branch=CASSANDRA-14496-trunk]|
;;;","01/Jul/21 22:11;brandon.williams;Circle looks good, +1.

/cc [~marcuse];;;","02/Jul/21 06:51;marcuse;Shouldn't we enable tombstone compactions if any of those 3 options is explicitly set? Now we would force a user to use {{UNCHECKED_TOMBSTONE_COMPACTION_OPTION}} if they want to enable tombstone compactions at all for TWCS.;;;","02/Jul/21 15:29;brandon.williams;You're right, I've misread the logic.

bq. Shouldn't we enable tombstone compactions if any of those 3 options is explicitly set?

Yep.  I've changed the patch to enable it if any of those is set and not equal to ""false"" here:

||Branch||CI||
|[3.11|https://github.com/driftx/cassandra/tree/CASSANDRA-14496]|[circle|https://app.circleci.com/pipelines/github/driftx/cassandra?branch=CASSANDRA-14496]|
|[4.0|https://github.com/driftx/cassandra/tree/CASSANDRA-14496-4.0]|[circle|https://app.circleci.com/pipelines/github/driftx/cassandra?branch=CASSANDRA-14496-4.0]|
|[trunk|https://github.com/driftx/cassandra/tree/CASSANDRA-14496-trunk]|[circle|https://app.circleci.com/pipelines/github/driftx/cassandra?branch=CASSANDRA-14496-trunk]|


;;;","02/Jul/21 16:00;marcuse;+1;;;","02/Jul/21 18:52;brandon.williams;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unset GREP_OPTIONS,CASSANDRA-14487,13163569,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,j.casares,j.casares,j.casares,01/Jun/18 23:42,16/Mar/22 11:44,13/Jul/23 08:37,21/Feb/20 01:18,4.0,4.0-alpha4,,,,,Packaging,,,,0,,,,"I have always had GREP_OPTIONS set to \{{–color=always}}.

Recently, on OS X, this bit me here:

* [https://github.com/apache/cassandra/blob/069e383f57e3106bbe2e6ddcebeae77da1ea53e1/conf/cassandra-env.sh#L132]

Because GREP_OPTIONS is also deprecated, it's suggested you use the following format instead:

{NOFORMAT}

alias grep=""grep --color=always""

{NOFORMAT}

We have two paths forward:

* {{unset GREP_OPTIONS}}
* Force the affected line to be {{grep --color=never}}",,githubbot,j.casares,rustyrazorblade,,,,,,,,,,,,,,,,,,,,,,,"smiklosovic closed pull request #229:
URL: https://github.com/apache/cassandra/pull/229


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 11:44;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,j.casares,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 21 01:18:27 UTC 2020,,,,,,,,,,,"0|i3uffb:",9223372036854775807,,,,,,,,,rustyrazorblade,,rustyrazorblade,,,Normal,,3.0.0,,,https://github.com/apache/cassandra/commit/d629a58045c764bc0bd6ce1d28a47e4e46fa13fc,,,,,,,,,,,,,,"01/Jun/18 23:47;githubbot;GitHub user joaquincasares opened a pull request:

    https://github.com/apache/cassandra/pull/229

    CASSANDRA-14487 unset GREP_OPTIONS

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/joaquincasares/cassandra CASSANDRA-14487

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/229.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #229
    
----
commit ab86727f3eb91736bc74b92befd576b7e0213462
Author: Joaquin Casares <joaquin@...>
Date:   2018-06-01T23:47:32Z

    CASSANDRA-14487 unset GREP_OPTIONS

----
;;;","01/Jun/18 23:48;j.casares;https://github.com/apache/cassandra/pull/229;;;","21/Feb/20 01:18;rustyrazorblade;Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using nodetool status after enabling Cassandra internal auth for JMX access fails with currently documented permissions,CASSANDRA-14481,13162924,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dataindataout,dataindataout,dataindataout,30/May/18 14:53,15/May/20 08:05,13/Jul/23 08:37,21/Jun/18 03:08,4.0,4.0-alpha1,,,,,Legacy/Documentation and Website,,,,2,security,,,"Using the documentation here:

[https://cassandra.apache.org/doc/latest/operating/security.html#cassandra-integrated-auth]

Running `nodetool status` on a cluster fails as follows:
{noformat}
error: Access Denied
-- StackTrace --
java.lang.SecurityException: Access Denied
at org.apache.cassandra.auth.jmx.AuthorizationProxy.invoke(AuthorizationProxy.java:172)
at com.sun.proxy.$Proxy4.invoke(Unknown Source)
at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1468)
at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:76)
at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1309)
at java.security.AccessController.doPrivileged(Native Method)
at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1408)
at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:829)
at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:357)
at sun.rmi.transport.Transport$1.run(Transport.java:200)
at sun.rmi.transport.Transport$1.run(Transport.java:197)
at java.security.AccessController.doPrivileged(Native Method)
at sun.rmi.transport.Transport.serviceCall(Transport.java:196)
at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:573)
at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:835)
at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:688)
at java.security.AccessController.doPrivileged(Native Method)
at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:687)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
at sun.rmi.transport.StreamRemoteCall.exceptionReceivedFromServer(StreamRemoteCall.java:283)
at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:260)
at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:161)
at com.sun.jmx.remote.internal.PRef.invoke(Unknown Source)
at javax.management.remote.rmi.RMIConnectionImpl_Stub.invoke(Unknown Source)
at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.invoke(RMIConnector.java:1020)
at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:298)
at com.sun.proxy.$Proxy7.effectiveOwnership(Unknown Source)
at org.apache.cassandra.tools.NodeProbe.effectiveOwnership(NodeProbe.java:489)
at org.apache.cassandra.tools.nodetool.Status.execute(Status.java:74)
at org.apache.cassandra.tools.NodeTool$NodeToolCmd.run(NodeTool.java:255)
at org.apache.cassandra.tools.NodeTool.main(NodeTool.java:169) {noformat}
Permissions on two additional mbeans were required:
{noformat}
GRANT EXECUTE ON MBEAN 'org.apache.cassandra.db:type=StorageService' TO jmx;
GRANT EXECUTE ON MBEAN 'org.apache.cassandra.db:type=EndpointSnitchInfo' TO jmx;
{noformat}
I've updated the documentation in my fork here and would like to do a pull request for the addition:

[https://github.com/dataindataout/cassandra/blob/docs_operating_security/doc/source/operating/security.rst]

 ","Apache Cassandra 3.11.2

Centos 6.9",dataindataout,eperott,jeromatron,jjirsa,pedro_gordo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,dataindataout,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 21 03:08:56 UTC 2018,,,,,,,,,,,"0|i3ubg7:",9223372036854775807,,,,,,,,,eperott,,eperott,,,Low,,,,,,,,,,,,,,,,,,,"30/May/18 14:54;dataindataout;This is my first ""PR"" to this project. I read the contributions document, but please let me know if I've missed any required tags or otherwise need to edit my submission. Thank you very much.;;;","20/Jun/18 10:00;eperott;Thanks for your contribution.

I've verified the issue and the proposed fix on 3.11.2 and trunk.

Some comments on your proposed changes.
{quote}GRANT EXECUTE ON MBEAN ‘org.apache.cassandra.db:type=EndpointSnitchInfo’ TO jmx;
 GRANT SELECT, EXECUTE ON MBEAN ‘org.apache.cassandra.db:type=StorageService’ TO jmx;
{quote}
Please update your patch such that it will contain relevant lines only. Right now you're duplicating parts of the example. 

Make sure to use a straight quotes {{'}}, not curved quotes {{’}} around the mbean names.

There is no need to grant both {{SELECT}} and {{EXECUTE}} on the {{StorageService}} as {{SELECT}} is granted to {{ALL MBEANS}} already in the example. And CQL don't let you grant two permissions in one statement anyway.

For small fixes like this, committers seem to prefer to get proposed fixes [like this|https://cassandra.apache.org/doc/latest/development/documentation.html#github-based-work-flow]. It is not clear to me how documentation is maintained and published on different branches/versions of Cassandra, but perhaps someone else can give advice on that.;;;","20/Jun/18 13:58;dataindataout;Thank you very much for this information, [~eperott]. I will edit the PR as you have suggested and resubmit.;;;","20/Jun/18 18:55;dataindataout;I've edited the file to support this PR in the following ways:

- changed curly quotes to flat quotes
- removed repeated lines
- moved changes to a branch called docs_operating_security;;;","21/Jun/18 03:08;jjirsa;Thanks! Committed as {{4f02db5c45ece38dda48e0d19667888e9f46536e}} with [~eperott] as reviewer. The site won't update immediately, but will take effect on next rebuild.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The check of num_tokens against the length of inital_token in the yaml triggers unexpectedly,CASSANDRA-14477,13162811,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stefan.miklosovic,VincentWhite,VincentWhite,30/May/18 07:12,18/Nov/20 21:10,13/Jul/23 08:37,18/Nov/20 12:34,3.0.23,3.11.9,4.0,4.0-beta4,,,Local/Config,,,,0,,,,"In CASSANDRA-10120 we added a check that compares num_tokens against the number of tokens supplied in the yaml via initial_token. From my reading of CASSANDRA-10120 it was to prevent cassandra starting if the yaml contained contradictory values for num_tokens and initial_tokens which should help prevent misconfiguration via human error. The current behaviour appears to differ slightly in that it performs this comparison regardless of whether num_tokens is included in the yaml or not. Below are proposed patches to only perform the check if both options are present in the yaml.
||Branch||
|[3.0.x|https://github.com/apache/cassandra/compare/cassandra-3.0...vincewhite:num_tokens_30]|
|[3.x|https://github.com/apache/cassandra/compare/cassandra-3.11...vincewhite:num_tokens_test_1_311]|",,e.dimitrova,KurtG,mck,stefan.miklosovic,VincentWhite,,,,,,,,,,,,,,,,,,,,,"smiklosovic opened a new pull request #795:
URL: https://github.com/apache/cassandra/pull/795


   …ssandra.yaml triggers unexpectedly


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Oct/20 16:58;githubbot;600","smiklosovic opened a new pull request #796:
URL: https://github.com/apache/cassandra/pull/796


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Oct/20 16:59;githubbot;600","smiklosovic opened a new pull request #797:
URL: https://github.com/apache/cassandra/pull/797


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Oct/20 17:00;githubbot;600","michaelsembwever commented on a change in pull request #796:
URL: https://github.com/apache/cassandra/pull/796#discussion_r521259043



##########
File path: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
##########
@@ -314,7 +314,8 @@ private static InetAddress getNetworkInterfaceAddress(String intf, String config
         }
     }
 
-    private static void setConfig(Config config)
+    @VisibleForTesting
+    public static void setConfig(Config config)

Review comment:
       Would it be safer (better practice) for the tests to work with their own `Config` instance, avoiding using static state of `DatabaseDescriptor`? 
   
   With method signature `applyTokensConfig(Config config)` you can avoid making public the `DatabaseDescriptor.setConfig(..)` method. This is the approach that was done in the 3.0 patch, see https://github.com/apache/cassandra/pull/795/files#diff-054af65b8d690b0fddc3e0a4ef05a80d8f1d6689b4f77912795fec019200666cR293




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Nov/20 10:24;githubbot;600","smiklosovic closed pull request #797:
URL: https://github.com/apache/cassandra/pull/797


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Nov/20 21:08;githubbot;600","smiklosovic closed pull request #795:
URL: https://github.com/apache/cassandra/pull/795


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Nov/20 21:10;githubbot;600","smiklosovic closed pull request #796:
URL: https://github.com/apache/cassandra/pull/796


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Nov/20 21:10;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,4200,,,0,4200,,,,,,,,,,,,,,,,,CASSANDRA-10120,,,,,,,,,,,,,,,,,,,,,,,0.0,stefan.miklosovic,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,,,Wed Nov 18 12:34:37 UTC 2020,,,,,,,,,,,"0|i3uar3:",9223372036854775807,3.0.x,3.11.x,,,,,,,,,mck,,,Low,,3.0.6,,,https://github.com/apache/cassandra/commit/8ef5a886312e20f09cd4b0358c71018908341796,,,,,,,,,CI,,,,,"10/Oct/19 08:33;mck;[~VincentWhite], 
 - could you split the test method into three methods, one for each of the configuration scenarios being tested?
 - in the 3.11 branch could you return {{num_tokens}} to a boxed type, CASSANDRA-12199 only intended primitives to be used where there was a default value, and here you've demonstrated that there shouldn't be a default value as {{num_tokens}} and {{initial_token}} are exclusive,
 - and, could you rebase both branches please? (test currently fails for me without rebasing);;;","29/Oct/20 17:00;stefan.miklosovic;3.0

[https://github.com/apache/cassandra/pull/795]

3.11

[https://github.com/apache/cassandra/pull/796]

trunk

[https://github.com/apache/cassandra/pull/797]

 

for 3.0, I am not sure what is wrong with tests but they pass individually but fail when whole class is tested in bulk (in IDEA). I was not able to find out why it happens.;;;","01/Nov/20 20:34;mck;1. The {{DatabaseDescriptorTest.testApplyInitialTokensInitialTokensSetNumTokensNotSet}} demonstrates an unspecified {{num_tokens}} becomes the default of {{1}} but can still be combined with any number of {{initial_token}} values. Are they any compatibility issues with changing {{DatabaseDescriptor}} to throw a {{ConfigurationException}} in this situation? I cannot see how any existing cluster would have been configured with this combination.

2. The 3.0 tests fail for me because of {{ConfigurationException(""Missing required directive CommitLogSync"", false)}} from {{DatabaseDescriptor.load(config)}}. Can we move the lines (658-677) related to tokens in {{DatabaseDescriptor}} to a separate method, like {{applyTokensConfig(Config config)}}.
Example [here|https://github.com/apache/cassandra/commit/b0855523f6e80ecdffde41170a410a71380a683a] (ontop your 3.0 branch).






;;;","02/Nov/20 15:31;stefan.miklosovic;[~mck] 

1) I dont follow. 

The code in e.g. trunk PR branch looks like:
{code:java}
if (conf.initial_token != null)
{
    Collection<String> tokens = tokensFromString(conf.initial_token);
    if (conf.num_tokens != null && tokens.size() != conf.num_tokens)
    {
       throw ....
    }

    for (String token : tokens)
        partitioner.getTokenFactory().validate(token);
}
else if (conf.num_tokens == null)
{
    conf.num_tokens = 1;
}
{code}
You say that ....  (it) demonstrates an unspecified {{num_tokens}} becomes the default of {{1}} but can still be combined with any number of {{initial_token}} values.

Is this really true? if num_tokens is not specified, it will become 1 only in case initial_token is null. If initial_token is not null, it will be checked on size against num_tokens (if it is not null) and throws only in case lenght does not match. Hence I do not see how it might be possible that num_tokens is set to 1 but initial_token might be set wrongly.

 

 ;;;","02/Nov/20 16:21;mck;1)
Quite right! {{num_tokens}} does not end being defaulted to the {{1}} value. 

I still believe a problem still exists… (neither values, null or one) are correct representations to the node running.

If I have a yaml like
{code}
           initial_token: 0,256,1024
           #num_tokens:
{code}
we end up with a runtime configuration of 
{code}
            config.initial_token = {0,256,1024};
            config.num_tokens = null;
{code}
This isn't valid imho.

I'm thinking the code should be…
{code}
if (conf.initial_token != null)
{
    Collection<String> tokens = tokensFromString(conf.initial_token);
    if (conf.num_tokens == null || tokens.size() != conf.num_tokens) // THIS LINE IS CHANGED
    {
       throw ....
    }
    …
{code}
Make sense?;;;","03/Nov/20 13:38;stefan.miklosovic;[~mck] changed applied, the links with PRs are still valid, there is a link to each build there.;;;","10/Nov/20 12:48;mck;The patch for 3.0 looks good.

My apologies, my comment about moving the section of code into a new method {{applyTokensConfig(Config config)}} was for all branches. Can you apply it to the 3.11 and trunk patches too please.;;;","16/Nov/20 21:11;stefan.miklosovic;required changes reflected in PRs;;;","17/Nov/20 13:42;mck;+1 on PRs.

CI runs for [3.0|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/210/], [3.11|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/211/], and [trunk|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/212/pipeline].;;;","17/Nov/20 21:59;mck;Committed as [8ef5a886312e20f09cd4b0358c71018908341796|https://github.com/apache/cassandra/commit/8ef5a886312e20f09cd4b0358c71018908341796].;;;","17/Nov/20 23:11;mck;This broke all the {{dtest-novnode}} dtests :(

I suspect…
 - the check for num_tokens being defined should be skipped if initial_tokens defines only one token, as it unlikely to be a typo, folk would rarely be configuring two tokens, and just one initial_token is the traditional non-vnodes configuration predating the use of num_tokens
 - dtests define {{num_tokens=1}} for non-nvodes, [here|https://github.com/apache/cassandra-dtest/blob/d61eff9b89a2f4bbe1791e1cd71a5e3456d6cc94/dtest_setup.py#L426],
 - the [Cassandra-devbranch|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/] jenkins pipeline should be including {{dtest-novnode}};;;","18/Nov/20 07:00;mck;bq. dtests define num_tokens=1 for non-nvodes, here,

[~brandon.williams] was to the rescue with this [commit|https://github.com/apache/cassandra-dtest/commit/845a82a38d3c4bb6d14c0bdb3341bfdc36ebbb02].;;;","18/Nov/20 09:32;mck;bq. the Cassandra-devbranch jenkins pipeline should be including dtest-novnode

patch for that [here|https://github.com/apache/cassandra-builds/compare/trunk...thelastpickle:mck/14477];;;","18/Nov/20 11:55;mck;bq. the check for num_tokens being defined should be skipped if initial_tokens defines only one token, as it unlikely to be a typo, folk would rarely be configuring two tokens, and just one initial_token is the traditional non-vnodes configuration predating the use of num_tokens

[~stefan.miklosovic] has added fixes for each branch on the same PRs above.

CI run [here|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/216/pipeline/].;;;","18/Nov/20 12:13;mck;bq. the check for num_tokens being defined should be skipped if initial_tokens defines only one token, as it unlikely to be a typo, folk would rarely be configuring two tokens, and just one initial_token is the traditional non-vnodes configuration predating the use of num_tokens
 
Committed as [bfd5d20a13501d897d8d34acce9b0394fa1cf00b|https://github.com/apache/cassandra/commit/bfd5d20a13501d897d8d34acce9b0394fa1cf00b].;;;","18/Nov/20 12:34;mck;bq. the Cassandra-devbranch jenkins pipeline should be including dtest-novnode

Committed as [69cfcb31078dd9d79d19d29d5c4543832fa00ffa|https://github.com/apache/cassandra-builds/commit/69cfcb31078dd9d79d19d29d5c4543832fa00ffa].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"nodetool - Occasional high CPU on large, CPU capable machines",CASSANDRA-14475,13162534,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,tsteinmaurer,tsteinmaurer,tsteinmaurer,29/May/18 09:09,15/May/20 08:03,13/Jul/23 08:37,29/May/18 12:22,3.11.3,4.0,4.0-alpha1,,,,Tool/nodetool,,,,0,,,,"Periodically calling nodetool every 5 min results in increased CPU usage by nodetool only on a machine with 32 physical cores (64 vCPUs) according to our monitoring:
!nodetool_highcpu_gcthreads1_cassandra_JIRA.png|width=600!

Investigation and testing has shown that it is related to running with default number of parallel GC threads which is 43 on this particular machine. We see a System.gc() according to flight recorder but no real evidence from where it comes from. The nodetool call in question is simply gathering e.g. the version with ""nodetool version"".

After explicitly setting the number of parallel GC threads to 1, the high CPU is entirely gone (see chart above), without impacting nodetool being executed successfully. 1 parallel GC thread should be sufficient for nodetool anyway I think.",,jasobrown,KurtG,tsteinmaurer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/May/18 09:26;tsteinmaurer;nodetool_gc_threads.patch;https://issues.apache.org/jira/secure/attachment/12925521/nodetool_gc_threads.patch","29/May/18 09:19;tsteinmaurer;nodetool_highcpu_gcthreads1_cassandra_JIRA.png;https://issues.apache.org/jira/secure/attachment/12925520/nodetool_highcpu_gcthreads1_cassandra_JIRA.png",,,,,,,,,,,,2.0,tsteinmaurer,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 29 13:05:38 UTC 2018,,,,,,,,,,,"0|i3u91r:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"29/May/18 09:27;tsteinmaurer;Attached patch is based on Cassandra 3.11 and for Linux only.;;;","29/May/18 11:09;KurtG;+1 from me. I can't see any valid reason why you would want to have more than 1 GC thread for nodetool. All the work is done within Cassandra, so there shouldn't be any adverse effects to only having 1 GC thread for the tool (famous last words).;;;","29/May/18 12:18;spod;This is probably not really nodetool specific, but an acknowledged JVM issue, fixed in Java 9.

https://bugs.java.com/bugdatabase/view_bug.do?bug_id=6858051;;;","29/May/18 12:22;jasobrown;lgtm. committed as sha {{b8cbdde2b854229d950d7087ac1847f8453d2b1e}}. Thanks, [~tsteinmaurer]!;;;","29/May/18 12:26;tsteinmaurer;[~jasobrown], thanks! Sorry for being a pain, but any chance to get this back-ported for 2.1+?;;;","29/May/18 13:05;jasobrown;2.1 is critical fixes only, and 2.2 is close in that category. I debated if this patch should go into 3.0 or not, but then opted for stability in the 3.0 branch. Also, your patch as for 3.11, so I used that as my base barnch :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Unable to parse targets for index"" on upgrade to Cassandra 3.0.10-3.0.16",CASSANDRA-14468,13161735,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jwjwyoung,wadey,wadey,24/May/18 10:39,27/Jan/23 05:44,13/Jul/23 08:37,31/Jul/18 19:35,3.0.18,3.11.4,,,,,,,,,0,,,,"I am attempting to upgrade from Cassandra 2.2.10 to 3.0.16. I am getting this error:

{code}
org.apache.cassandra.exceptions.ConfigurationException: Unable to parse targets for index idx_foo (""666f6f"")
	at org.apache.cassandra.index.internal.CassandraIndex.parseTarget(CassandraIndex.java:800) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.index.internal.CassandraIndex.indexCfsMetadata(CassandraIndex.java:747) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:645) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:251) [apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:569) [apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:697) [apache-cassandra-3.0.16.jar:3.0.16]
{code}

It looks like this might be related to CASSANDRA-14104 that was just added to 3.0.16 ",,aleksey,jasobrown,jwest,KurtG,wadey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14104,CASSANDRA-12516,,,,,,,,,"01/Jun/18 12:29;wadey;data.tar.gz;https://issues.apache.org/jira/secure/attachment/12926084/data.tar.gz",,,,,,,,,,,,,1.0,jwjwyoung,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 31 19:34:44 UTC 2018,,,,,,,,,,,"0|i3u453:",9223372036854775807,3.0.10,3.0.12,3.0.15,3.0.16,,,,,aleksey,,aleksey,,,Normal,,3.0.10,,,,,,,,,,,,,,,,,"24/May/18 10:50;wadey;I get the same error trying to upgrade from 2.2.10 -> 3.0.15, so it looks like the error is unrelated to CASSANDRA-14104;;;","24/May/18 10:55;wadey;Upgrade from 2.2.10 -> 3.0.9 does not hit this error, so I will search for the version that introduces the failure.;;;","24/May/18 10:59;wadey;The issue first appears to 3.0.10. Upgrades to 3.0.9 work but upgrades to 3.0.10+ fail with the error.;;;","24/May/18 11:55;wadey;Simplified a bit, the schema basically looks like this (from cassandra-cli, this is a legacy schema):

{code}
create column family foos
  with column_type = 'Standard'
  and comparator = 'BytesType'
  and default_validation_class = 'UTF8Type'
  and key_validation_class = 'UTF8Type'
  and read_repair_chance = 0.0
  and dclocal_read_repair_chance = 0.1
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'
  and caching = 'KEYS_ONLY'
  and cells_per_row_to_cache = '0'
  and default_time_to_live = 0
  and speculative_retry = 'NONE'
  and bloom_filter_fp_chance = 0.01
  and column_metadata = [
    {column_name : '666f6f',
    validation_class : UTF8Type,
    index_name : 'idx_foo',
    index_type : 0}]
and compression_options = {'sstable_compression' : 'org.apache.cassandra.io.compress.LZ4Compressor'};
{code}

It looks like this in cqlsh (from 2.2):

{code}
CREATE TABLE myks.foos (
    key text PRIMARY KEY,
    ""666f6f"" text
) WITH COMPACT STORAGE
    AND bloom_filter_fp_chance = 0.01
    AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = 'NONE';
CREATE INDEX idx_foo ON myks.foos (""666f6f"");
{code};;;","24/May/18 12:38;wadey;I have a guess that it might be due to CASSANDRA-12516, because my column names are of type BytesType but that change forces column identifiers to load as UTF8Type.;;;","31/May/18 23:29;jasobrown;[~jrwest] interesting in taking a look at this?;;;","01/Jun/18 06:20;jwest;Can take a look next week;;;","01/Jun/18 12:05;wadey;Ok I've dug a bit further and found out my simplified schema will not reproduce it. After running a debugger, it appears that the issue somehow related to ColumnIdentifier.internedInstances. For some reason, some entries in this map have their ""text"" set to the actual name of the column (like ""foo"") and some have their text set to the hex bytes of the name (like ""666f6f""). I think the issue is that the InternedKey only matches on the bytes of the column name, so both lookups with ""foo"" and ""666f6f"" will pull the same interned entry from the map. The stacktrace happens when the schema has the hex byte version of the name, but the internedmap has the ascii version.

My current guess is there are similar column names in other tables and they are getting interned incorrectly. I'll see if I can dig into this more.;;;","01/Jun/18 12:10;wadey;Interestingly, it looks like the ticket that caused this issue was trying to solve a very similar issue! CASSANDRA-12516

The problem is that the type is listed as ""UTF8Type"" for both calls to getInterned. I'm guessing when its being called with hex bytes it should not be UTF8Type and this is the bug.;;;","01/Jun/18 12:15;wadey;I think the issue is that {{type}} on this line is the type for the value of the column, and not the type for the name of the column: https://github.com/apache/cassandra/blob/cassandra-3.0.16/src/java/org/apache/cassandra/schema/SchemaKeyspace.java#L1063;;;","01/Jun/18 12:28;wadey;I was able to reproduce by creating two column families. Each with a column named ""foo"", one with comparator type BytesType and one with UTF8Type.

I will attach the Cassandra 2.2.10 data directory from this reproduction. If you start up Cassandra 3.0.10+ with this data it will reproduce the error.;;;","24/Jul/18 01:39;jwest;[~wadey], sorry for taking so long to get to this but I finally had some time today. I agree with your assessment so far but unfortunately don’t have much to add. It looks like CASSANDRA-12516 fixed what the cache was keyed on but not all the {{getInterned}} call sites. Indeed the {{type}} column is the CQL (value) type. Further, we no longer have the comparator after {{LegacySchemaMigrator}} runs (of note, {{LegacySchemaMigrator}} does use {{getInterned}} as intended but since we lose the comparator that only makes things worse)*.

[~iamaleksey], do you have any thoughts on this since you reported the original issue?

\* I’m actually just getting familiar with this code but I think [~jasobrown] referred this ticket to me because of the initial relation to 2i;;;","24/Jul/18 11:18;aleksey;Ugh. This is both unfortunate and tricky. Let me think how we can mitigate this.;;;","24/Jul/18 15:12;aleksey;[~wadey] [~jrwest] You are right with your analysis. I'm sorry I missed this one during CASSANDRA-12516 review. The good news is that it should be trivial to fix (unless I'm missing something).

All we need to do is skip interning in {{SchemaKeyspace.createColumnFromRow()}}.;;;","24/Jul/18 19:06;jwest;[~iamaleksey] tried that locally. It looks like {{ColumnDefinition}} requires {{ColumnIdentifier}} to be interned: [https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/config/ColumnDefinition.java#L156.] ;;;","24/Jul/18 19:19;aleksey;[~jrwest] Right. I'm trying to find out why it asserts that, but can't. Even equals impl. doesn't have a == shortcut. I *think* it should be safe to drop that assertion altogether (maybe with some more research first).

The alternative is not pretty at all, and won't work in 4.0 - the alternative being replicating {{CFMetaData.getColumnDefinitionNameComparator()}} method but without having all of the data yet. Ugh.;;;","24/Jul/18 19:45;jwest;[~iamaleksey] ugh indeed. I'll lend an extra pair of eyes to whether or not that assertion is needed. ;;;","26/Jul/18 18:29;jwest;[~iamaleksey] reading the code again, I *think* it should be safe to drop as well, for the reasons you list. The {{ColumnIdentifier}} in the {{ColumnDefinition}}/{{ColumnMetadata}} will be different (by reference) than the ones returned by {{Literal#prepare}} but since they are structurally equal that should be ok. Otherwise, its hard to separate out its initial intention since it was committed as part of CASSANDRA-8099. ;;;","26/Jul/18 19:11;aleksey;[~jrwest] Agreed. Do it (or [~wadey], if he prefers), and I'll review promptly.;;;","26/Jul/18 20:17;jwest;Assigned to myself;;;","31/Jul/18 15:23;jwest;I would like to add a dtest for this but wanted to push up the patch to get review started.

||trunk||3.0||
|[branch|https://github.com/jrwest/cassandra/tree/14468-trunk]|[branch|https://github.com/jrwest/cassandra/tree/14468-3.0]|
|[tests|https://circleci.com/gh/jrwest/cassandra/tree/14468-trunk]|[tests|https://circleci.com/gh/jrwest/cassandra/tree/14468-3.0]|;;;","31/Jul/18 16:32;aleksey;+1, I'll commit shortly.;;;","31/Jul/18 19:34;aleksey;Committed as [4b00601e831690e4ccf4ea95f70c09381d0ce49a|https://github.com/apache/cassandra/commit/4b00601e831690e4ccf4ea95f70c09381d0ce49a] to 3.0 and merged up. Thanks. A regression dtest would be nice, though not necessary. But whenever you have one, let me know and I'll commit pronto.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent the generation of new tokens when using replace_address flag,CASSANDRA-14463,13161307,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,VincentWhite,VincentWhite,VincentWhite,23/May/18 06:14,27/May/22 19:25,13/Jul/23 08:37,23/Jul/21 16:29,3.0.25,3.11.11,4.0.1,4.1,4.1-alpha1,,Legacy/Distributed Metadata,,,,0,,,,"This is a follow up to/replacement of CASSANDRA-14073.

The behaviour that I want to avoid is someone trying to replace a node with the replace_address flag and mistakenly having that node listed in its own seed list which causes the node to generate a new set of random tokens before joining the ring. 

Currently anytime an unbootstrapped node is listed in its own seed list and initial_token isn't set in the yaml, Cassandra will generate a new set of random tokens and join the ring regardless of whether it was replacing a previous node or not. 

We could simply check for this configuration and refuse to start but I it's probably better (particularly for 3.0.X) if it's handled in the same manner as skipping streaming with the allow_unsafe_replace flag that was introduced in 3.X . This would still allow 3.0.X users the ability to re-bootstrap nodes without needing to re-stream all the data to the node again, which can be useful. 

We currently handle replacing without streaming different;y between 3.0.X and 3.X. In 3.X we have the allow_unsafe_replace JVM flag to allow the use of auto_bootstrap: false in combination with the replace_address option.  But in 3.0.X to perform the replacement of a node with the same IP address without streaming I believe you need to:
 * Set replace_address (because the address is already in gossip)
 * Include the node in its own seed list (to skip bootstrapping/streaming)
 * Set the initial_token to the token/s owned by the previous node (to prevent it generating new tokens.

I believe if 3.0.X simply refused to start when a node has itself in its seed list and replace_address set this will completely block this operation. 

Example patches to fix this edge case using allow_unsafe_replace:

 
||Branch||
|[3.0.x|https://github.com/apache/cassandra/compare/trunk...vincewhite:30-no_clobber]|
|[3.x|https://github.com/apache/cassandra/compare/trunk...vincewhite:311-no_clobber]|",,bdeggleston,e.dimitrova,KurtG,maedhroz,tcooke,VincentWhite,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14073,,,,,,,,,,,,,,,,,0.0,VincentWhite,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 22 20:48:39 UTC 2021,,,,,,,,,,,"0|i3u1i7:",9223372036854775807,,,,,,,,,,,brandon.williams,e.dimitrova,,Low,,NA,,,https://github.com/apache/cassandra/commit/1205a9de226c3b77bdb1440818daf5f1f34cf0c9,,,,,,,,,,,,,,"19/Jul/21 18:40;brandon.williams;This is a worthy protection to add and the patches look good to me.  +1;;;","19/Jul/21 21:22;e.dimitrova;I rebased the branches and pushed CI runs:
||Branch||CI|
| [3.0|https://github.com/apache/cassandra/compare/trunk...vincewhite:30-no_clobber]|[Jenkins |https://jenkins-cm4.apache.org/job/Cassandra-devbranch/958/]
|[3.11|https://github.com/apache/cassandra/compare/trunk...vincewhite:311-no_clobber]|[Jenkins |https://jenkins-cm4.apache.org/job/Cassandra-devbranch/957/|]

I guess we should also merge to 4.0 and trunk.
I can help with that if [~VincentWhite] doesn't have the time;;;","20/Jul/21 16:43;brandon.williams;Those merge cleanly.

||Branch||CI|
|[4.0|https://github.com/driftx/cassandra/tree/CASSANDRA-14463-4.0]|[!https://ci-cassandra.apache.org/job/Cassandra-devbranch/964/badge/icon!|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/964/pipeline]|
|[trunk|https://github.com/driftx/cassandra/tree/CASSANDRA-14463-trunk]|[!https://ci-cassandra.apache.org/job/Cassandra-devbranch/965/badge/icon!|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/965/pipeline]|;;;","22/Jul/21 16:32;brandon.williams;CI looks good, I've added a dtest for this [here|https://github.com/driftx/cassandra-dtest/tree/CASSANDRA-14463];;;","23/Jul/21 15:08;brandon.williams;[~e.dimitrova] can you review?;;;","23/Jul/21 15:20;e.dimitrova;In the making... I was actually just preparing to spin the great new test you wrote in the multiplexer to ensure the timeout is enough and the test will not be flaky. :) ;;;","23/Jul/21 15:57;e.dimitrova;+1 on green [CI |https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1039/workflows/cf8512a6-95fd-4bc7-9e43-fdb6bec9e323/jobs/6189] (the new test is run in the multiplexer to check whether the timeout is enough; I did it only for the 4.0 branch as I wouldn't expect difference for this one between the different branches)

One super nit on the dtest(it can be addressed on commit). Thank you!;;;","23/Jul/21 16:29;brandon.williams;Committed w/nit addressed.  Thank you!;;;","22/Sep/21 20:43;maedhroz;[~brandon.williams] [~e.dimitrova] Did the [new dtest|https://github.com/driftx/cassandra-dtest/commit/3df0ff6e88fcc55bd93e62261a6d7fe869199f6d] for this ever get committed?;;;","22/Sep/21 20:48;brandon.williams;Ever? Yes... [now|https://github.com/apache/cassandra-dtest/commit/10dd53fcaceabec556f43f67d90dfaf75f2dbbfc].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Infinity ms Commit Log Sync,CASSANDRA-14451,13160049,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jasobrown,ozzieisaacs,ozzieisaacs,17/May/18 15:08,24/Mar/21 00:44,13/Jul/23 08:37,05/Jun/18 20:56,3.0.17,3.11.3,4.0,4.0-alpha1,,,,,,,0,,,,"Its giving commit log sync warnings where there were apparently zero syncs and therefore gives ""Infinityms"" as the average duration

{code:java}
WARN [PERIODIC-COMMIT-LOG-SYNCER] 2018-05-16 21:11:14,294 NoSpamLogger.java:94 - Out of 0 commit log syncs over the past 0.00s with average duration of Infinityms, 1 have exceeded the configured commit interval by an average of 74.40ms 
WARN [PERIODIC-COMMIT-LOG-SYNCER] 2018-05-16 21:16:57,844 NoSpamLogger.java:94 - Out of 0 commit log syncs over the past 0.00s with average duration of Infinityms, 1 have exceeded the configured commit interval by an average of 198.69ms 
WARN [PERIODIC-COMMIT-LOG-SYNCER] 2018-05-16 21:24:46,325 NoSpamLogger.java:94 - Out of 0 commit log syncs over the past 0.00s with average duration of Infinityms, 1 have exceeded the configured commit interval by an average of 264.11ms 
WARN [PERIODIC-COMMIT-LOG-SYNCER] 2018-05-16 21:29:46,393 NoSpamLogger.java:94 - Out of 32 commit log syncs over the past 268.84s with, average duration of 17.56ms, 1 have exceeded the configured commit interval by an average of 173.66ms{code}",3.11.2 - 2 DC,aby786,bradfordcp,cnlwsu,jasobrown,jay.zhuang,jeromatron,jwest,lizg,ozzieisaacs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-11307,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 05 20:56:44 UTC 2018,,,,,,,,,,,"0|i3tu5j:",9223372036854775807,,,,,,,,,jwest,,jwest,,,Low,,,,,,,,,,,,,,,,,,,"17/May/18 16:03;jasobrown;Is this on a new cluster, or after an upgrade from a previous version?;;;","17/May/18 16:06;ozzieisaacs;Upgraded from 3.10;;;","17/May/18 17:25;jasobrown;This is probably me, related to CASSANDRA-14108. I'll take a look into it.;;;","22/May/18 15:47;aby786;all,

 

even i recently upgraded from 3.10 to 3.11.2 and recieving similar errros PERIODIC-COMMIT-LOG-SYNCER.

One thing i observed is even cqlsh login takes over a minute when these erros are reported., also my lowest non prod with 4 nodes 1 DC setup is showing this error and memory was configured as 16GB, the usage went on to 13 to 14 GB. seems to be related .

Is this bug with 3.11.2 ?
 ;;;","01/Jun/18 12:14;jasobrown;As I suspected, this is just a bug with logging (due to CASSANDRA-14108), and no real behavior or correctness is impacted. I have a patch for 3.11, but need to port to 3.0 and trunk.;;;","01/Jun/18 21:55;jasobrown;Below are branches that resolve the problem
||3.0||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/14451-3.0]|[branch|https://github.com/jasobrown/cassandra/tree/14451-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/14451-trunk]|
|[utests & dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14451-3.0]|[utests & dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14451-3.11]|[utests & dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14451-trunk]|

The bug was using {{markerIntervalNanos}} in the {{wakeUpAt}} variable as basis for determining if we've lagged in the actual flush to disk. The code should use {{syncIntervalNanos}} for that determination. Once again, the only problem here is around determining if we should log about the commitlog flushing falling behind, not that the commitlog is actually falling behind (it ins't, at least as far as the overlogging here is concerned).

Most of the change was moving the logging code out of the primary {{AbstractCommitLogService.SyncRunnable#sync()}} and into a subroutine. This allowed me to add unit tests, as well as clean up/clarify the {{sync()}} method.;;;","03/Jun/18 00:20;aby786;Can this cause Memory usage to go high ?

 after 3.11.2 upgrade i am getting high memory usage alerts and nothing in errorlog sometime , but sometime i see same PERIODIC-COMMIT-LOG-SYNCER message;;;","03/Jun/18 02:09;jasobrown;[~aby786] I would highly doubt the logging thing would cause your memory problems. You should probably open a separate ticket and describe all the symptoms you are seeing, with as much detail as possible.;;;","05/Jun/18 02:42;jwest;[~jasobrown] change LGTM. A few questions and minor comments:
 * Are the ArchiveCommitLog dtest failures expected on the 3.0 branch? 
 * The “sleep any time we have left” comment would be more appropriate above the assignment of {{wakeUpAt}}. 
 * Mark {{maybeLogFlushLag}} and {{getTotalSyncDuration}} as {{@VisibleForTesting}}
 * Just wanted to check that the change in behavior of updating {{totalSyncDuration}} is intentional. It makes sense to me that we only increment it if a sync actually occurs but that wasn’t the case before. 
 * Is there are reason you opted for the “excessTimeToFlush” approach in 3.0 but the “maxFlushTimestamp” approach on 3.11 and trunk? The only difference I see is the unit of time. ;;;","05/Jun/18 12:35;jasobrown;bq. Are the ArchiveCommitLog dtest failures expected on the 3.0 branch? 

Yes, 3.0 dtests consistently have about 14 failures, including {{ArchiveCommitLog}}. So, unfortunately, this is expected.

bq. The “sleep any time we have left” comment would be more appropriate above the assignment of wakeUpAt.

I left it where it was previously located, but can move it to the more logical spot.

bq. change in behavior of updating totalSyncDuration is intentional

lol, it wasn't intentional, but it now does the correct thing! You are right that in CASSANDRA-14108 I was adding time to mark the headers (without flushing) to {{totalSyncDuration}}, which is incorrect.

bq. Is there are reason you opted for the “excessTimeToFlush” approach in 3.0 but the “maxFlushTimestamp” approach on 3.11 and trunk?

I wanted to keep the logic as close to the original as possible, since 3.0 is far along in it's age. I suppose it doesn't matter that much, though, and can change if you think it's worthwhile. wdyt?;;;","05/Jun/18 15:07;jwest;{quote} I left it where it was previously located, but can move it to the more logical spot.
{quote}
I don't find it very useful where it is now. Would vote to move it or remove it (the code is pretty clear).
{quote}I wanted to keep the logic as close to the original as possible, since 3.0 is far along in it's age. I suppose it doesn't matter that much, though, and can change if you think it's worthwhile. wdyt?
{quote}
From the review perspective it was just a second implementation to check for correctness and it seems like either implementation could be used. Would vote for them to be the same but fine as is if you prefer.

 

Otherwise, +1;;;","05/Jun/18 20:56;jasobrown;Made all the last recs from [~jrwest], and committed as sha {{214a3abfcc25460af50805b543a5833697a1b341}}. Thanks, all!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DelimiterAnalyzer: IllegalArgumentException: The key argument was zero-length,CASSANDRA-14450,13159991,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mck,mck,mck,17/May/18 11:03,15/May/20 08:05,13/Jul/23 08:37,18/May/18 04:29,3.11.3,4.0,4.0-alpha1,,,,Feature/SASI,,,,0,,,,"The [DelimiterAnalyzer|https://issues.apache.org/jira/browse/CASSANDRA-14247] can throw an IllegalArgumentException if there is no text between two delimiters. 

{noformat}
ERROR [MutationStage-1] 2018-05-17 13:55:09,734 StorageProxy.java:1417 - Failed to apply mutation locally : {}
java.lang.RuntimeException: The key argument was zero-length for ks: zipkin2, table: span
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1353) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:626) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:470) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:227) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:232) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:241) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.service.StorageProxy$8.runMayThrow(StorageProxy.java:1411) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:2650) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_171]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171]
Caused by: java.lang.IllegalArgumentException: The key argument was zero-length
	at com.googlecode.concurrenttrees.radix.ConcurrentRadixTree.putInternal(ConcurrentRadixTree.java:520) ~[concurrent-trees-2.4.0.jar:na]
	at com.googlecode.concurrenttrees.radix.ConcurrentRadixTree.putIfAbsent(ConcurrentRadixTree.java:123) ~[concurrent-trees-2.4.0.jar:na]
	at org.apache.cassandra.index.sasi.memory.TrieMemIndex$ConcurrentPrefixTrie.putIfAbsent(TrieMemIndex.java:178) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.index.sasi.memory.TrieMemIndex$ConcurrentTrie.add(TrieMemIndex.java:123) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.index.sasi.memory.TrieMemIndex.add(TrieMemIndex.java:94) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.index.sasi.memory.IndexMemtable.index(IndexMemtable.java:65) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.index.sasi.conf.ColumnIndex.index(ColumnIndex.java:104) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.index.sasi.SASIIndex$1.insertRow(SASIIndex.java:258) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.index.SecondaryIndexManager$WriteTimeTransaction.onInserted(SecondaryIndexManager.java:915) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.partitions.AtomicBTreePartition$RowUpdater.apply(AtomicBTreePartition.java:333) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.partitions.AtomicBTreePartition$RowUpdater.apply(AtomicBTreePartition.java:295) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.utils.btree.BTree.buildInternal(BTree.java:139) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.utils.btree.BTree.build(BTree.java:121) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.utils.btree.BTree.update(BTree.java:178) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.partitions.AtomicBTreePartition.addAllWithSizeDelta(AtomicBTreePartition.java:156) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.Memtable.put(Memtable.java:282) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1335) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
{noformat}",,jasobrown,mck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,mck,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 18 04:29:23 UTC 2018,,,,,,,,,,,"0|i3ttsf:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"17/May/18 11:44;mck;|| Branch || uTest || aTest || dTest ||
|[cassandra-3.11_14450|https://github.com/thelastpickle/cassandra/tree/mck/cassandra-3.11_14450]|[!https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-3.11_14450.svg?style=svg!|https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-3.11_14450]| [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/33/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/33] | [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/565/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/565] |
|[trunk_14450|https://github.com/thelastpickle/cassandra/tree/mck/trunk_14450]|[!https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Ftrunk_14450.svg?style=svg!|https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Ftrunk_14450]| [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/34/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/34] | [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/568/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/568] |;;;","17/May/18 23:01;mck;[~mkjellman], have you got any time for a review? it's a simple fix and would be good to get corrected before the class comes out in any released version.;;;","17/May/18 23:04;jasobrown;I can take look tomorrow, as I'm not sure about [~mkjellman]'s availability these days.;;;","18/May/18 01:36;jasobrown;change looks fine to me, +1.

 ;;;","18/May/18 02:38;mck;Thanks. Committed as 7068ef62548c1ff8d17be7d0a1e71a5f098010e6;;;","18/May/18 04:29;jasobrown;don't forget to update the status and the fix version ;);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-stress throws NPE if insert section isn't specified in user profile,CASSANDRA-14426,13155923,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,alexott,alexott,alexott,28/Apr/18 18:02,15/May/20 08:02,13/Jul/23 08:37,28/Apr/18 21:04,4.0,4.0-alpha1,,,,,Tool/stress,,,,0,,,,"When user profile file is used, and insert section isn't specified, then cassandra-stress is using default values instead.

Since support for LWTs was added, absence of the insert section lead to throwing of NullPointerException when generating inserts:

{noformat}
java.lang.NullPointerException
	at org.apache.cassandra.stress.StressProfile.getInsert(StressProfile.java:546)
	at org.apache.cassandra.stress.StressProfile.printSettings(StressProfile.java:126)
	at org.apache.cassandra.stress.settings.StressSettings.lambda$printSettings$1(StressSettings.java:311)
	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684)
	at org.apache.cassandra.stress.settings.StressSettings.printSettings(StressSettings.java:311)
	at org.apache.cassandra.stress.Stress.run(Stress.java:108)
	at org.apache.cassandra.stress.Stress.main(Stress.java:63)
{noformat}

Fix is trivial, and will be provided as PR",,alexott,githubbot,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,alexott,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Apr 28 21:12:29 UTC 2018,,,,,,,,,,,"0|i3t55z:",9223372036854775807,,,,,,,,,jjirsa,,jjirsa,,,Low,,,,,,,,,,,,,,,,,,,"28/Apr/18 18:05;githubbot;GitHub user alexott opened a pull request:

    https://github.com/apache/cassandra/pull/221

    fix for CASSANDRA-14426

    This commit fixes NPE when the `insert` section is missing from user-defined profile

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/alexott/cassandra CASSANDRA-14426

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/221.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #221
    
----
commit 403e463a81333dec232d353feb28f31611e4bb08
Author: Alex Ott <alexott@...>
Date:   2018-04-28T18:04:13Z

    fix for CASSANDRA-14426
    
    This commit fixes NPE when the `insert` section is missing from user-defined profile

----
;;;","28/Apr/18 18:09;jjirsa;Is trunk/4.x sufficient?;;;","28/Apr/18 18:12;alexott;Yes, I think that trunk is sufficient - this is more edge case...;;;","28/Apr/18 18:21;jjirsa;Appreciate the debugging and the fix. I think your fix is fine, but I'd like to change it a bit to move the:

{code}
        if (insert == null)
            insert = new HashMap<>();
        lowerCase(insert);
{code}

From below the {{if (!isKeyOnlyTable)}} if/else block to above, making your check irrelevant.  Are you OK with that change? ;;;","28/Apr/18 18:22;jjirsa;Explicitly, this is what I propose (and I'm happy to do it on commit for you):

{code}
diff --git a/tools/stress/src/org/apache/cassandra/stress/StressProfile.java b/tools/stress/src/org/apache/cassandra/stress/StressProfile.java
index 6c44ff298a..2338873a8d 100644
--- a/tools/stress/src/org/apache/cassandra/stress/StressProfile.java
+++ b/tools/stress/src/org/apache/cassandra/stress/StressProfile.java
@@ -499,6 +499,10 @@ public class StressProfile implements Serializable
                         }
                     }

+                    if (insert == null)
+                        insert = new HashMap<>();
+                    lowerCase(insert);
+
                     //Non PK Columns
                     StringBuilder sb = new StringBuilder();
                     if (!isKeyOnlyTable)
@@ -543,7 +547,7 @@ public class StressProfile implements Serializable

                         //Put PK predicates at the end
                         sb.append(pred);
-                        if (insert != null && insert.containsKey(""condition""))
+                        if (insert.containsKey(""condition""))
                         {
                             sb.append("" "" + insert.get(""condition""));
                             insert.remove(""condition"");
@@ -563,10 +567,6 @@ public class StressProfile implements Serializable
                         sb.append("") "").append(""values("").append(value).append(')');
                     }

-                    if (insert == null)
-                        insert = new HashMap<>();
-                    lowerCase(insert);
-
                     partitions = select(settings.insert.batchsize, ""partitions"", ""fixed(1)"", insert, OptionDistribution.BUILDER);
                     selectchance = select(settings.insert.selectRatio, ""select"", ""fixed(1)/1"", insert, OptionRatioDistribution.BUILDER);
                     rowPopulation = select(settings.insert.rowPopulationRatio, ""row-population"", ""fixed(1)/1"", insert, OptionRatioDistribution.BUILDER);
{code}
;;;","28/Apr/18 21:04;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/cassandra/pull/221
;;;","28/Apr/18 21:04;jjirsa;Going to assume you're ok with my changes (if you're not, let me know), committed as {{71a27ee2b93a47e177edc16571f91bb5d592899e}} ;;;","28/Apr/18 21:12;alexott;Thank you Jeff!
Your solution is more generic - I simply wasn't sure that it would be a good style to introduce some kind of ""defaults"" there.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-stress doesn't report correctly when profile file is not found,CASSANDRA-14425,13155915,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,alexott,alexott,alexott,28/Apr/18 17:34,16/Mar/22 11:42,13/Jul/23 08:37,05/Aug/20 16:57,4.0,4.0-beta2,,,,,Tool/stress,,,,0,,,,"Right now, when there is an error in the profile file name cassandra-stress reports very not understandable error ""URI is not absolute"" instead of reporting something like ""File doesn't exist"" or something like.

It would be very helpful to improve user experience by reporting correct reason for error.",,aholmber,alexott,githubbot,,,,,,,,,,,,,,,,,,,,,,,"smiklosovic closed pull request #220:
URL: https://github.com/apache/cassandra/pull/220


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 11:42;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,alexott,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,,Wed Aug 05 16:57:03 UTC 2020,,,,,,,,,,,"0|i3t547:",9223372036854775807,3.11.2,,,,,,,,,,brandon.williams,,,Low,,3.6,,,https://github.com/apache/cassandra/commit/6d9465a00dc26ad0ba589252686de52b4a537bd9,,,,,,,,,"Manual verification might be sufficient.

No docs are required – it's just better error reporting.",,,,,"28/Apr/18 17:38;githubbot;GitHub user alexott opened a pull request:

    https://github.com/apache/cassandra/pull/220

    Fix for CASSANDRA-14425

    Fix affects 2 files:
    - `SettingsCommandUser.java` where there is a check, is the file exists
    - `Stress.java` where error handling happens.  The main reason for this is that when file
       is loaded from URI, then the caught exception is wrapped into generic `IOError`
       exception that isn't handled explicitly.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/alexott/cassandra CASSANDRA-14425

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/220.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #220
    
----
commit 35095c04ac6951f9e2c9d99981580f040c7a9f37
Author: Alex Ott <alexott@...>
Date:   2018-04-28T17:35:17Z

    Fix for CASSANDRA-14425
    
    Fix affects 2 parts:
    - `SettingsCommandUser.java` where there is a check, is the file exists
    - `Stress.java` where error handling happens.  The main reason for this is that when file
       is loaded from URI, then the caught exception is wrapped into generic `IOError`
       exception that isn't handled explicitly.

----
;;;","04/Aug/20 18:35;aholmber;Moving this to Patch Available after stumbling across the PR.;;;","05/Aug/20 16:57;brandon.williams;Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTables stop being compacted,CASSANDRA-14423,13155605,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,KurtG,KurtG,KurtG,27/Apr/18 04:05,16/Apr/19 09:29,13/Jul/23 08:37,29/Jun/18 08:16,2.2.13,3.0.17,3.11.3,,,,Local/Compaction,,,,1,,,,"So seeing a problem in 3.11.0 where SSTables are being lost from the view and not being included in compactions/as candidates for compaction. It seems to get progressively worse until there's only 1-2 SSTables in the view which happen to be the most recent SSTables and thus compactions completely stop for that table.

The SSTables seem to still be included in reads, just not compactions.

The issue can be fixed by restarting C*, as it will reload all SSTables into the view, but this is only a temporary fix. User defined/major compactions still work - not clear if they include the result back in the view but is not a good work around.

This also results in a discrepancy between SSTable count and SSTables in levels for any table using LCS.
{code:java}
Keyspace : xxx
Read Count: 57761088
Read Latency: 0.10527088681224288 ms.
Write Count: 2513164
Write Latency: 0.018211106398149903 ms.
Pending Flushes: 0
Table: xxx
SSTable count: 10
SSTables in each level: [2, 0, 0, 0, 0, 0, 0, 0, 0]
Space used (live): 894498746
Space used (total): 894498746
Space used by snapshots (total): 0
Off heap memory used (total): 11576197
SSTable Compression Ratio: 0.6956629530569777
Number of keys (estimate): 3562207
Memtable cell count: 0
Memtable data size: 0
Memtable off heap memory used: 0
Memtable switch count: 87
Local read count: 57761088
Local read latency: 0.108 ms
Local write count: 2513164
Local write latency: NaN ms
Pending flushes: 0
Percent repaired: 86.33
Bloom filter false positives: 43
Bloom filter false ratio: 0.00000
Bloom filter space used: 8046104
Bloom filter off heap memory used: 8046024
Index summary off heap memory used: 3449005
Compression metadata off heap memory used: 81168
Compacted partition minimum bytes: 104
Compacted partition maximum bytes: 5722
Compacted partition mean bytes: 175
Average live cells per slice (last five minutes): 1.0
Maximum live cells per slice (last five minutes): 1
Average tombstones per slice (last five minutes): 1.0
Maximum tombstones per slice (last five minutes): 1
Dropped Mutations: 0
{code}
Also for STCS we've confirmed that SSTable count will be different to the number of SSTables reported in the Compaction Bucket's. In the below example there's only 3 SSTables in a single bucket - no more are listed for this table. Compaction thresholds haven't been modified for this table and it's a very basic KV schema.
{code:java}
Keyspace : yyy
    Read Count: 30485
    Read Latency: 0.06708991307200263 ms.
    Write Count: 57044
    Write Latency: 0.02204061776873992 ms.
    Pending Flushes: 0
        Table: yyy
        SSTable count: 19
        Space used (live): 18195482
        Space used (total): 18195482
        Space used by snapshots (total): 0
        Off heap memory used (total): 747376
        SSTable Compression Ratio: 0.7607394576769735
        Number of keys (estimate): 116074
        Memtable cell count: 0
        Memtable data size: 0
        Memtable off heap memory used: 0
        Memtable switch count: 39
        Local read count: 30485
        Local read latency: NaN ms
        Local write count: 57044
        Local write latency: NaN ms
        Pending flushes: 0
        Percent repaired: 79.76
        Bloom filter false positives: 0
        Bloom filter false ratio: 0.00000
        Bloom filter space used: 690912
        Bloom filter off heap memory used: 690760
        Index summary off heap memory used: 54736
        Compression metadata off heap memory used: 1880
        Compacted partition minimum bytes: 73
        Compacted partition maximum bytes: 124
        Compacted partition mean bytes: 96
        Average live cells per slice (last five minutes): NaN
        Maximum live cells per slice (last five minutes): 0
        Average tombstones per slice (last five minutes): NaN
        Maximum tombstones per slice (last five minutes): 0
        Dropped Mutations: 0 
{code}
{code:java}
Apr 27 03:10:39 cassandra[9263]: TRACE o.a.c.d.c.SizeTieredCompactionStrategy Compaction buckets are [[BigTableReader(path='/var/lib/cassandra/data/yyy/yyy-5f7a2d60e4a811e6868a8fd39a64fd59/mc-67168-big-Data.db'), BigTableReader(path='/var/lib/cassandra/data/yyy/yyy-5f7a2d60e4a811e6868a8fd39a64fd59/mc-67167-big-Data.db'), BigTableReader(path='/var/lib/cassandra/data/yyy/yyy-5f7a2d60e4a811e6868a8fd39a64fd59/mc-67166-big-Data.db')]]
{code}
Also for every LCS table we're seeing the following warning being spammed (seems to be in line with anticompaction spam):
{code:java}
Apr 26 21:30:09 cassandra[9263]: WARN  o.a.c.d.c.LeveledCompactionStrategy Live sstable /var/lib/cassandra/data/xxx/xxx-8c3ef9e0e3fc11e6868a8fd39a64fd59/mc-79024-big-Data.db from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.{code}
This is a vnodes cluster with 256 tokens per node, and the only thing that seems like it could be causing issues is anticompactions.

CASSANDRA-14079 might be related but doesn't quite describe the same issue, and in this case we're using only a single disk for data. Have yet to reproduce but figured worth reporting here first.",,Blake Atkinson,eanujwa,jasonstack,jay.zhuang,jeromatron,KurtG,marcuse,mck,mshuler,ozzieisaacs,tommy_s,vinaykumarcse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14550,,,,,,,,,,,,,,,,,,,,,,,0.0,KurtG,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 11 00:26:11 UTC 2018,,,,,,,,,,,"0|i3t3a7:",9223372036854775807,3.11.0,3.11.2,,,,,,,spod,,marcuse,,,Critical,,,,,,,,,,,,,,,,,,,"14/Jun/18 03:03;KurtG;Figured this out. tl;dr is that full repairs are completely broken. We add the repaired sstables to the transaction for anti-compaction but we never do anything with them or re-write them (because they are already repaired), and thus they get ""removed"" from the compaction strategies SSTables along with the unrepaired SSTables that got anti-compacted.

 

This essentially means that full repairs has been terribly broken for a long time, haven't checked how far back yet but it seems reasonable to say 2.1. Going to mark this as a blocker for 3.11.3 only while doing more research.;;;","14/Jun/18 03:12;KurtG;Looks like we only _stopped_ anti-compacting repaired SSTables in CASSANDRA-13153, so this bug only occurs since 2.2.10, 3.0.13, and 3.11.0.;;;","14/Jun/18 11:02;KurtG;got a patch & test for 3.11 [here|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:14423-3.11]. Started on the wrong branch so will backport to 2.2 and 3.0 tomorrow...;;;","14/Jun/18 13:59;spod;{quote} and thus they get ""removed"" from the compaction strategies SSTables along with the unrepaired SSTables that got anti-compacted.{quote}

Where exactly does this happen?;;;","15/Jun/18 03:46;KurtG;In {{org.apache.cassandra.db.compaction.CompactionManager#submitAntiCompaction}} we create a transaction over all SSTables included in the repair (including repaired SSTables when doing full repair) and pass that through to {{performAntiCompaction}} in which two things can happen:

1. The SSTable is fully contained within the repairing ranges, and in that case we mutate repairedAt to the current time of repair and add it to {{mutatedRepairStatuses}}
2. The SSTable isn't fully contained within the repairing ranges (highly likely if vnodes or single tokens with >RF nodes). In this case we don't add the _already repaired_ SSTable to {{mutatedRepairStatuses}}.

We then remove all SSTables from the transaction in {{mutatedRepairStatuses}} [here|https://github.com/apache/cassandra/blob/191ad7b87a4ded26be4ab0bd192ef676f059276c/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L704].

If *2* occured, the already repaired SSTables were not in {{mutatedRepairStatuses}} and thus didn't get removed from the transaction and when {{txn.finish()}} is called they get removed from the CompactionStrategy's view of sstables via {{org.apache.cassandra.db.lifecycle.LifecycleTransaction#doCommit}} calling {{Tracker#notifySSTablesChanged}} which will not include the already repaired SSTables.

The reason CASSANDRA-13153 brought this bug to light was because up until that point we _were_ anti-compacting already repaired SSTables, and thus upon anti-compaction (rewrite) they would be added back into the transaction and the old SSTable would be removed as usual and the new SSTable would take its place.

Seeing as the existing consensus seems to be that there's no real value at the moment in mutating repaired times on already repaired SSTables I think the best solution is to not include the repaired SSTables in the transaction in the first place. This corresponds with how trunk currently works and also is a lot cleaner, which is how it works in my patch mentioned above. The alternative would be to remove them from the transaction regardless of if they were mutated, but this seems pointless considering we don't do anything with it. If we ever decide there is value in updating repairedAt on already repaired SSTables, we can add it back and handle it then. ;;;","15/Jun/18 05:05;KurtG;Patches for all branches:

|[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...kgreav:14423-2.2]|
|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...kgreav:14423-3.0]|
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:14423-3.11]|;;;","15/Jun/18 11:46;spod;Thanks for the detailed description. Finishing a LifecycleTransaction does indeed seem to remove the sstables the transactions has been created for from the compaction strategy. Simply ignoring repaired sstables, by filtering them as currently implemented, should be changed in that case. 

But it looks like your patch effectively reverts [67637d1|https://github.com/apache/cassandra/commit/67637d1] (CASSANDRA-11172) which needs some more careful consideration.

/cc [~krummas]

;;;","18/Jun/18 01:59;KurtG;[~spodxx@gmail.com] That change was to ensure that we didn't send repair status change notifications where the SSTable was already marked as repaired. My change doesn't revert that,  as we no longer pass through the repaired SSTables to {{performAntiCompaction}} so there is no need to filter them out. Granted it does change the behaviour of {{performAntiCompaction}} and if someone were to call it and pass in repaired SSTables, it would produce the old behaviour. But arguably you should never be passing already repaired SSTables to {{performAntiCompaction}}. At the moment {{performAntiCompaction}} is only ever used by submitAntiCompaction in the codebase, so it's only a problem if 3rd party tools are using it. If we're worried maybe adding a test to the start of {{performAntiCompaction}} to check the SSTables aren't already repaired would be the way to go?

utests:
[3.11|https://circleci.com/gh/kgreav/cassandra/163]
[3.0|https://circleci.com/gh/kgreav/cassandra/167]
[2.2|https://circleci.com/gh/kgreav/cassandra/165];;;","18/Jun/18 11:58;spod;Can we move the status check into {{performAnticompaction}} by adding already repaired sstables to {{nonAnticompacting}}? I think filtering there would be more coherent, given we also create a corresponding log message and use the same code path for canceling/releasing such sstables. We also keep updating repairedAt this way, in case of fully contained sstables (and the triggered event notification related to that).;;;","18/Jun/18 12:30;spod;I'd like to add for interested readers that full repairs on subranges (e.g. using reaper) will not be affected by this issue. In this case, ""Not a global repair, will not do anticompaction"" will occur in your logs.;;;","19/Jun/18 12:27;KurtG;OK. I've updated all 3 branches to use {{nonAnticompacting}} for filtering. Personally, I'm not a huge fan of this but it avoids the behaviour change issues previously mentioned and still fixes the problem. Also as I see it, a cleaner fix would require some refactoring of {{submitAntiCompaction}} and {{performAntiCompaction}} to decouple anti-compaction from repair, which is not possible in current releases anyway (to clarify, I don't think {{performAntiCompaction}} should care about whether SSTables are repaired or not). I can see a future though where it's possible to anti-compact SSTables separate to a repair, but I'll think about that for a separate ticket for trunk.

Side-notes related to patches: I took the liberty of making one log message less spammy ""SSTable x will be anticompacted on range..."" - with many vnodes this becomes ridiculously spammy. It's not a lot better in the patched version, but at least it's a one line log message with no repeated information.

And I took the liberty of fixing the ""anti-compacted from x to y SSTables"" log message in {{doAntiCompaction}}. This was broken as {{repaired.originals.size}} could/would return a wrong size after \{{repaired.split(sstableGroup)}} was called.

Also, the assert at the end of {{performAntiCompaction}} had to be removed as we're removing {{nonAnticompacting}} from the transaction which violates the assert, this is fine however as the assert was effectively incorrect in the first place, and only valid due to the bug.

Finally, I split the tests out into a separate method ({{shouldAntiCompact}}), and added more test cases

|[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...kgreav:14423-2.2]|[utests|https://circleci.com/gh/kgreav/cassandra/169]|
|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...kgreav:14423-3.0]|[utests|https://circleci.com/gh/kgreav/cassandra/171]|
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:14423-3.11]|[utests|https://circleci.com/gh/kgreav/cassandra/173]|

I'm working on a unit test for trunk, but it's going to have to be a fair bit different, as the repaired SSTables are filtered out prior to creation of the transaction. I don't think it needs to block review of the other branches however.

 

 ;;;","21/Jun/18 07:14;spod;I think this is going in the right direction.

As for log messages, the ""does not intersect repaired ranges"" message seems to be now incorrectly shown for already repaired sstables.
{quote}Also, the assert at the end of performAntiCompaction had to be removed as we're removing nonAnticompacting from the transaction which violates the assert, this is fine however as the assert was effectively incorrect in the first place, and only valid due to the bug.
{quote}
Help me out, in which case do we end up removing an sstable in nonAnticompacting from txn.originals that would not also be removed from sstableIterator?;;;","26/Jun/18 12:08;spod;Took another closer look at the patch today and I think that this should work. I put the assert back in, as I still can't see why it should not hold anymore. Also did some minor wording changes to a log message, changelog and git log, if that's ok for you, [~KurtG].
 
* [2.2|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-14423-2.2]
 [circleci|https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-14423-2.2]
[dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/578/]
* [3.0|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-14423-3.0]
[circleci|https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-14423-3.0]
[dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/579/]
* [3.11|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-14423-3.11]
[circleci|https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-14423-3.11]
[dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/580/]

The 2.2 circleci test failures seem to be flaky. Dtests still running.;;;","26/Jun/18 22:43;KurtG;Sorry for the delay, was looking at this yesterday but yeah, it turned out the assert failing was just me imagining things. That all seems good to me [~spodxx@gmail.com].;;;","27/Jun/18 08:18;marcuse;Should we stop doing anticompaction at all after full repairs instead? Clearly no one does {{--full}} repairs right now and letting users do non-incremental full repairs might be good until 4.0 (CASSANDRA-9143).;;;","27/Jun/18 10:34;KurtG;[~krummas] I've suggested that we at the very least provide a flag to skip anti-compaction from full repairs before, but it was all deemed too [complicated|https://issues.apache.org/jira/browse/CASSANDRA-13885?focusedCommentId=16206922&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16206922]. Regardless, I think a flag would be perfectly fine and it's still desirable so at least people can go back to at the very least running full repairs successfully without having to worry about SSTables being marked repaired. However, I don't think we can go and change the default behaviour, purely because people _could_ still be running full repairs on earlier versions of 3.x/3.0 before this bug came along.;;;","27/Jun/18 11:13;marcuse;the patches lgtm;;;","28/Jun/18 08:02;spod;[~KurtG], do you mind to go on by committing 2.2/3.0/3.11 patches now and address trunk in a separate ticket?;;;","29/Jun/18 00:45;KurtG;[~spodxx@gmail.com], not at all. Probably won't have time to look at trunk until next week, and I'm fairly sure it'll just be a case of making the test work so I'll create another ticket for trunk.;;;","29/Jun/18 00:51;KurtG;Created CASSANDRA-14550 for the trunk port.;;;","29/Jun/18 07:20;mck;Jumping on this to commit. Two (other) reviewers have +1 this now, and tests have passed.

Repeating for clarity, the patches and their tests were…
||Branch||uTest||dTest||
|[cassandra-2.2_14423|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-14423-2.2]|[!https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-14423-2.2.svg?style=svg!|https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-14423-2.2]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/583/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/583]|
|[cassandra-3.0_14423|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-14423-3.0]|[!https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-14423-3.0.svg?style=svg!|https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-14423-3.0]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/581/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/581]|
|[cassandra-3.11_14423|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-14423-3.11]|[!https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-14423-3.11.svg?style=svg!|https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-14423-3.11]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/580/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/580]|;;;","29/Jun/18 08:16;mck;committed as  f8912ce9329a8bc360e93cf61e56814135fbab39;;;","09/Jul/18 13:49;mshuler;Build fails on JDK7.
{noformat}
((2.2.13-tentative))mshuler@hana:~/git/cassandra$ java -version
java version ""1.7.0_80""
Java(TM) SE Runtime Environment (build 1.7.0_80-b15)
Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)
((2.2.13-tentative))mshuler@hana:~/git/cassandra$ ant
Buildfile: /home/mshuler/git/cassandra/build.xml

init:
    [mkdir] Created dir: /home/mshuler/git/cassandra/build/classes/main
    [mkdir] Created dir: /home/mshuler/git/cassandra/build/classes/thrift
    [mkdir] Created dir: /home/mshuler/git/cassandra/build/test/lib
    [mkdir] Created dir: /home/mshuler/git/cassandra/build/test/classes
    [mkdir] Created dir: /home/mshuler/git/cassandra/src/gen-java
    [mkdir] Created dir: /home/mshuler/git/cassandra/build/lib
    [mkdir] Created dir: /home/mshuler/git/cassandra/build/jacoco
    [mkdir] Created dir: /home/mshuler/git/cassandra/build/jacoco/partials

maven-ant-tasks-localrepo:
     [copy] Copying 1 file to /home/mshuler/git/cassandra/build

maven-ant-tasks-download:

maven-ant-tasks-init:

maven-declare-dependencies:

maven-ant-tasks-retrieve-build:
[artifact:dependencies] Building ant file: /home/mshuler/git/cassandra/build/build-dependencies.xml
[artifact:dependencies] Building ant file: /home/mshuler/git/cassandra/build/build-dependencies-sources.xml
     [copy] Copying 65 files to /home/mshuler/git/cassandra/build/lib/jars
     [copy] Copying 41 files to /home/mshuler/git/cassandra/build/lib/sources
     [copy] Copying 25 files to /home/mshuler/git/cassandra/build/lib/jars
    [unzip] Expanding: /home/mshuler/git/cassandra/build/lib/jars/org.jacoco.agent-0.7.5.201505241946.jar into /home/mshuler/git/cassandra/build/lib/jars

check-gen-cql3-grammar:

gen-cql3-grammar:
     [echo] Building Grammar /home/mshuler/git/cassandra/src/java/org/apache/cassandra/cql3/Cql.g  ...

generate-cql-html:

build-project:
     [echo] apache-cassandra: /home/mshuler/git/cassandra/build.xml
    [javac] Compiling 45 source files to /home/mshuler/git/cassandra/build/classes/thrift
    [javac] warning: Supported source version 'RELEASE_6' from annotation processor 'org.openjdk.jmh.generators.BenchmarkProcessor' less than -source '1.7'
    [javac] Note: /home/mshuler/git/cassandra/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java uses or overrides a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 1 warning
    [javac] Compiling 1168 source files to /home/mshuler/git/cassandra/build/classes/main
    [javac] Note: Processing compiler hints annotations
    [javac] warning: Supported source version 'RELEASE_6' from annotation processor 'org.openjdk.jmh.generators.BenchmarkProcessor' less than -source '1.7'
    [javac] Note: Processing compiler hints annotations
    [javac] Note: Writing compiler command file at META-INF/hotspot_compiler
    [javac] Note: Done processing compiler hints annotations
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/db/compaction/CompactionManager.java:584: error: cannot find symbol
    [javac]                     logger.info(""SSTable {} ({}) will be anticompacted on ranges: {}"", sstable, sstableBounds, String.join("", "", anticompactRanges));
    [javac]                                                                                                                      ^
    [javac]   symbol:   method join(String,List<String>)
    [javac]   location: class String
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 1 error
    [javac] 1 warning

BUILD FAILED
/home/mshuler/git/cassandra/build.xml:827: Compile failed; see the compiler error output for details.

Total time: 32 seconds
{noformat};;;","10/Jul/18 00:13;mck;This will break running Cassandra-2.2 on jdk1.7
;;;","10/Jul/18 00:31;mck;||Branch||uTest||
|[cassandra-2.2_14423.1|https://github.com/thelastpickle/cassandra/tree/mck/cassandra-2.2_14423.1]|[!https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-2.2_14423.1.svg?style=svg!|https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-2.2_14423.1]|

[~mshuler], can you quick review this?

(also created CASSANDRA-14563);;;","10/Jul/18 15:17;mshuler;Thanks! That commit looks good to me.
- {{ant artifacts}} builds again successfully on JDK7
- {{ant test -Dtest.name=AntiCompactionTest}} passes successfully on JDK7;;;","11/Jul/18 00:26;mck;Committed as 3482370df5672c9337a16a8a52baba53b70a4fe8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing dependencies airline and ohc-core-j8 for pom-all,CASSANDRA-14422,13155508,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,shichao.an,shichao.an,shichao.an,26/Apr/18 19:01,15/May/20 08:00,13/Jul/23 08:37,31/May/18 05:12,3.0.17,3.11.3,4.0,4.0-alpha1,,,Build,,,,0,,,,"I found two missing dependencies for pom-all (cassandra-all):
 * airline
 * ohc-core-j8

 

This doesn't affect current build scheme because their jars are hardcoded in the lib directory. However, if we depend on cassandra-all in our downstream projects to resolve and fetch dependencies (instead of using the official tarball), Cassandra will have problems, e.g. airline is required by nodetool, and it will fail our dtests.

I will attach the patch shortly",,jay.zhuang,jjirsa,mshuler,shichao.an,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/May/18 09:02;spod;deps-tree-311-no_patch.txt;https://issues.apache.org/jira/secure/attachment/12925704/deps-tree-311-no_patch.txt",,,,,,,,,,,,,1.0,shichao.an,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 31 18:02:11 UTC 2018,,,,,,,,,,,"0|i3t2ov:",9223372036854775807,,,,,,,,,jay.zhuang,,jay.zhuang,,,Low,,,,,,,,,,,,,,,,,,,"26/Apr/18 19:09;shichao.an;Here's the patch for trunk, 3.0 and 3.11:
||Branch||uTest||
|[14422-trunk|https://github.com/shichao-an/cassandra/commits/14422-trunk]|[!https://circleci.com/gh/shichao-an/cassandra/tree/14422-trunk.svg?style=svg!|https://circleci.com/gh/shichao-an/cassandra/tree/14422-trunk]|
|[14422-3.0|https://github.com/shichao-an/cassandra/commits/14422-3.0]|[!https://circleci.com/gh/shichao-an/cassandra/tree/14422-3.0.svg?style=svg!|https://circleci.com/gh/shichao-an/cassandra/tree/14422-3.0]|
|[14422-3.11|https://github.com/shichao-an/cassandra/commits/14422-3.11]|[!https://circleci.com/gh/shichao-an/cassandra/tree/14422-3.11.svg?style=svg!|https://circleci.com/gh/shichao-an/cassandra/tree/14422-3.11]|;;;","29/May/18 22:37;jay.zhuang;The change looks good to me. @[~mshuler], @[~spodxx@gmail.com], what do you guys think?;;;","30/May/18 00:53;mshuler;1) I stopped looking for what changed in 3.0 diff after scrolling on lots of whitespace changes.

2) What bug does this fix with dtest, exactly?;;;","30/May/18 04:28;jjirsa;Should definitely minimize the whitespace changes.
;;;","30/May/18 05:17;jay.zhuang;Sure, I split the change for easy review:

||Diff||
|[14422-3.0|https://github.com/cooldoger/cassandra/commit/31c4724c10830700356ed9cbfee24dab4601e0de]|
|[14422-3.11|https://github.com/cooldoger/cassandra/commit/7d2abcd2c29fa7cbcd114fc7e3152e39ed1016f6]|

The patch is to fix a dependency management issue, it won't change any cassandra binaries or adding/removing any JARs. It only changes the metadata pom. There're missing dependencies in the pom, for example: http://central.maven.org/maven2/org/apache/cassandra/cassandra-all/3.11.2/cassandra-all-3.11.2.pom, it's missing {{airline}} and {{ohc-core-j8}} in {{project->dependencies}} list. If there's a project depends on [{{cassandra-all-3.11.2.jar}}|http://central.maven.org/maven2/org/apache/cassandra/cassandra-all/3.11.2/cassandra-all-3.11.2.jar], the dependency management tool (maven, gradle, etc.) won't be able to download {{ariline}}/{{ohc-core-j8}} which are needed by {{cassandra-all-3.11.2.jar}}.;;;","30/May/18 09:02;spod;The dependencies are listed in the parent pom, but not as direct dependencies in cassandra-all. As a result, both artifacts won't become a transitive dependencies by projects depending on cassandra-all and must get explicitly pulled in by such projects. See {{deps-tree-311-no_patch.txt}} on how the current dependency tree for 3.11 cassandra-all pom looks like (you won't find them there).

Now the question is, will any downstream projects actually need these dependencies. But we should probably just be consistent by adding all known runtime dependencies to cassandra-all, whether used by nodetool or any other place.;;;","30/May/18 20:43;jay.zhuang;Thanks [~spodxx@gmail.com]. That's exactly the problem that transitive dependencies are not set correctly. As we published the JAR, we should set that right. I'd like to commit the change later today if there's no objection.;;;","30/May/18 21:59;mshuler;Thanks for the info and no-whitespace diff. Looks good to me!;;;","30/May/18 22:22;shichao.an;You can ignore whitespace GitHub by adding a ?w=1 in the URL, for example:

 

[https://github.com/shichao-an/cassandra/commit/c1962e32e0a3bf1dde8973855f108ec1a4aeb5d6?w=1]

 ;;;","31/May/18 05:12;jay.zhuang;Thanks [~shichao.an] for the fix. The change is committed as [38096da|https://github.com/apache/cassandra/commit/38096da25bd72346628c001d5b310417f8f703cd].;;;","31/May/18 16:15;mshuler;For future reference, here is the code style ""policy""/suggestion on whitespace:

[https://cassandra.apache.org/doc/latest/development/code_style.html#whitespace]

Apologies if my suggestion was vague on the removal of whitespace changes to the particular branch diffs, also seconded by Jeff. I assumed the subsequent diffs posted for 3.0/3.11 were the ones that would be committed.

{{git blame build.xml}} forever more will have an irrelevant commit on many lines, when only the code changes should have been committed to those branches. Personally, I use blame frequently when looking for changes, so github trickery means nothing and code reviews should follow the documented code style suggestions.;;;","31/May/18 17:57;jay.zhuang;Thanks [~mshuler] for the information. Will follow the code style policy and have the tailing space change as a separate commit in future.;;;","31/May/18 18:02;shichao.an;[~mshuler] Thanks for your explanation on the code style. I will remember that for future contributions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtests not determining C* version correctly,CASSANDRA-14420,13155436,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,samt,samt,26/Apr/18 15:20,16/Apr/19 09:29,13/Jul/23 08:37,09/May/18 18:37,,,,,,,,,,,0,dtest,,,"In the course of CASSANDRA-14134, the means of extracting the C* version under test before starting a cluster became broken. This is necessary in cases where we want to gate values in cassandra.yaml based on version, so a couple of tests are affected. The specifics are that the global {{CASSANDRA_VERSION_FROM_BUILD}} was hardcoded to '4.0' and the ways in which the various tests use it have meant that it was undetected until now.

Also, the {{fixture_since}} which we use to implement the {{@since}} annotation is broken when a {{--cassandra-version}} is supplied, rather than {{--cassandra-dir}}, meaning testing against released versions from git isn't working right now.

Tests directly affected:
 * {{auth_test.py}} - CASSANDRA-13985 added some gating of yaml props and additional checks on CQL results based on the build version. These failed on 3.11, which is how this issue was uncovered, but they're also broken on 2.2 on builds.apache.org
 * {{user_functions_test.py}} - gates setting a yaml property when version < 3.0. Failing on 2.2.
 * {{upgrade_tests}} - a number of these use the variable, but I don't think they're actually being run at the moment.
 * {{repair_tests/repair_test.py}}, {{replace_address_test.py}} & {{thrift_test}} all use the global, but only to verify that the version is not 3.9. As we're not running CI for that version, no-one noticed.",,bdeggleston,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 09 18:37:19 UTC 2018,,,,,,,,,,,"0|i3t293:",9223372036854775807,,,,,,,,,bdeggleston,,bdeggleston,,,Normal,,,,,,,,,,,,,,,,,,,"27/Apr/18 17:22;samt;I've pushed a branch [here|https://github.com/beobal/cassandra-dtest/commits/14420] which removes {{dtest.py::CASSANDRA_VERSION_FROM_BUILD}} and refactors {{auth_test::TestAuthRoles}}. dtests look good (i.e. back to how they were) against [2.2|https://circleci.com/workflow-run/ace01378-6551-424d-b1d4-b3358271b599], [3.11|https://circleci.com/workflow-run/894a2090-a78f-4763-aab9-b338273ddf6d] and [trunk|https://circleci.com/workflow-run/51cb13e7-1e38-4796-a950-4a3c95fd561c] (there's one failing dtest on trunk, but it's failed occasionally before so we already have CASSANDRA-14157 for it).

[~bdeggleston], seeing as you touched {{auth_test}} just recently, would you mind taking a look please?
;;;","01/May/18 16:58;bdeggleston;I have a few notes:

* could we rename parse_dtest_config to dtest_config. While I realize the fixture function itself is doing the parsing, it seems a little strange to be passing the dtest config into the misc setup methods with that name
* you could probably set the parse_dtest_config fixture scope to something like module or session, so it's not reinstantiated for every test
* auth_test:TestAuthRoles.role has a mutable default argument which can lead to difficult to diagnose bugs. It's default should be None, then evaluated in the function body as {{options = options or {}}}
* I don't feel too strongly about this, but it looks like the parse_dtest_config argument is only used in a handful of fixture_dtest_setup_override implementations. Maybe it would be better to have a separate fixture setup for places where we need to consult the config? Otoh, passing the config into one of the main setup method seems like a reasonable thing to do. WDYT?
* There's a {{parse_dtest_config}} definition in user_functions_test:TestUserFunctions that's just behaving as a pass through. Is this left over from some debugging something, or is there a reason it's there? If it's doing something, could you add a comment explaining what?;;;","03/May/18 07:34;samt;Thanks [~bdeggleston], I've pushed an additional commit addressing your comments and re-run CI:
 * [2.2|https://circleci.com/gh/beobal/cassandra/199]
 * [3.0|https://circleci.com/gh/beobal/cassandra/197]
 * [3.11|https://circleci.com/gh/beobal/cassandra/198]
 * [trunk|https://circleci.com/gh/beobal/cassandra/200];;;","09/May/18 16:14;bdeggleston;+1, thanks;;;","09/May/18 18:37;samt;Thanks, committed to master in [a06c0e70|https://github.com/apache/cassandra-dtest/commit/a06c0e700f335cbdfcd683cadc358766d959aca0];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resume compresed hints delivery broken,CASSANDRA-14419,13155345,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,tommy_s,tommy_s,tommy_s,26/Apr/18 09:08,16/Apr/19 09:29,13/Jul/23 08:37,03/Jul/18 13:25,3.0.17,,,,,,Consistency/Hints,,,,0,,,,"We are using Cassandra 3.0.15 and are using compressed hints, but if hint delivery is interrupted resuming hint delivery is failing.

{code}

2018-04-04T13:27:48.948+0200 ERROR [HintsDispatcher:14] CassandraDaemon.java:207 Exception in thread Thread[HintsDispatcher:14,1,main]
java.lang.IllegalArgumentException: Unable to seek to position 1789149057 in /var/lib/cassandra/hints/9592c860-1054-4c60-b3b8-faa9adc6d769-1522838912649-1.hints (118259682 bytes) in read-only mode
        at org.apache.cassandra.io.util.RandomAccessReader.seek(RandomAccessReader.java:287) ~[apache-cassandra-clientutil-3.0.15.jar:3.0.15]
        at org.apache.cassandra.hints.HintsReader.seek(HintsReader.java:114) ~[apache-cassandra-3.0.15.jar:3.0.15]
        at org.apache.cassandra.hints.HintsDispatcher.seek(HintsDispatcher.java:83) ~[apache-cassandra-3.0.15.jar:3.0.15]
        at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.deliver(HintsDispatchExecutor.java:263) ~[apache-cassandra-3.0.15.jar:3.0.15]
        at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.dispatch(HintsDispatchExecutor.java:248) ~[apache-cassandra-3.0.15.jar:3.0.15]
        at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.dispatch(HintsDispatchExecutor.java:226) ~[apache-cassandra-3.0.15.jar:3.0.15]
        at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.run(HintsDispatchExecutor.java:205) ~[apache-cassandra-3.0.15.jar:3.0.15]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_152]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_152]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_152]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_152]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [apache-cassandra-3.0.15.jar:3.0.15]
        at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_152]

{code}

 I think the problem is similar to CASSANDRA-11960.",,aleksey,eperott,jeromatron,JoshuaMcKenzie,KurtG,tommy_s,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,tommy_s,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 03 13:34:25 UTC 2018,,,,,,,,,,,"0|i3t1ov:",9223372036854775807,3.0.15,3.0.16,,,,,,,aleksey,,aleksey,,,Low,,3.0.3,,,,,,,,,,,,,,,,,"26/Apr/18 10:26;tommy_s;I think the fix is similar as the fix for CASSANDRA-11960 so I have tried to backport that pach to the 3.0 branch: [cassandra-14419-30|https://github.com/tommystendahl/cassandra/tree/cassandra-14419-30].
But the unit tests are broken, HintsCompressionTest failse with:
{code}
java.io.IOException: Digest mismatch exception
    FSReadError in /tmp/1524648911701-0/f3647df0-486b-11e8-a9fe-05db90a7d16a-1524648911695-1.hints
    at org.apache.cassandra.hints.HintsReader$HintsIterator.computeNext(HintsReader.java:199)
    at org.apache.cassandra.hints.HintsReader$HintsIterator.computeNext(HintsReader.java:164)
    at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
    at org.apache.cassandra.hints.HintsCompressionTest.multiFlushAndDeserializeTest(HintsCompressionTest.java:124)
    at org.apache.cassandra.hints.HintsCompressionTest.lz4Compressor(HintsCompressionTest.java:143)
    Caused by: java.io.IOException: Digest mismatch exception
    at org.apache.cassandra.hints.HintsReader$HintsIterator.computeNextInternal(HintsReader.java:216)
    at org.apache.cassandra.hints.HintsReader$HintsIterator.computeNext(HintsReader.java:190)
{code};;;","02/May/18 12:16;tommy_s;Backporting the fix for CASSANDRA-11960 to the 3.0 branch might not be as easy as I thought, changes introduced by CASSANDRA-5863 messed up things for me, thats why the unit tests failed. But I still think the solution is similar to CASSANDRA-11960.;;;","08/May/18 11:39;tommy_s;I have found some conflicts from CASSANDRA-5863 and fixed them, but I'm not sure I have found everything yet but at least the unit tests are are working now.

My latest version of the commit is here: [cassandra-14419-30|https://github.com/tommystendahl/cassandra/tree/cassandra-14419-30]

I had to remove {{HintsServiceTest.java}} since it depended on CASSANDRA-12016 which is not availible on the 3.0 branch but I have a local branch where I have that test working. I have also modified it to test with compression on and it works fine.;;;","16/May/18 12:29;tommy_s;I have convinced myself that my patch is complete now and I have done testing using ccm and and it seams to be working now so I this should be ready for review now.

[cassandra-14419-30|https://github.com/tommystendahl/cassandra/tree/cassandra-14419-30];;;","12/Jun/18 10:49;tommy_s;I looked over my patch again and realized that I had included some unnecessary changes so I removed them to reduce the size of the patch, hopefully a bit easier to review.

[cassandra-14419-30|https://github.com/tommystendahl/cassandra/tree/cassandra-14419-30];;;","12/Jun/18 11:30;aleksey;Will try to review this week, or at worst next one.

Thanks for the patch.;;;","16/Jun/18 20:24;JoshuaMcKenzie;""Anonymous"" needs to stop setting things to ""Ready to Commit"" once every two months. ;);;;","28/Jun/18 19:56;aleksey;[~tommy_s] the patch looks good to me. There are some things I'd do slightly differently, but it's really not worth introducing a delta between 3.0 and 3.11/4.0 here if avoidable, so I'm fine with the backport as is.

That said, can you please backport {{AlteredHints}} and, yes, {{HintsServiceTest}}? I'm not comfortable committing this otherwise.

Thanks!;;;","02/Jul/18 14:29;tommy_s;[~iamaleksey] thanks for the review.

I don't think we need the base class {{AlteredHints}} so I moved the relevant changes into {{HintsCompressionTest}}. I updated my branch with a commit for this, [cassandra-14419-30|https://github.com/tommystendahl/cassandra/tree/cassandra-14419-30].

For {{HintsServiceTest}} do you think I should backport the complete patch for CASSANDRA-12016 or should I copy the bare minimum needed for {{HintsServiceTest}}? It's relatively easy to backport the complete patch and I can't see any side effects except that part of it isn't needed for {{HintsServiceTest}}.;;;","02/Jul/18 15:38;aleksey;[~tommy_s] I don't have a strong opinion here. It is a safe commit to backport wholesale - so do whichever you feel like in this case.;;;","03/Jul/18 11:12;tommy_s;[~iamaleksey] I have created a new branch [cassandra-14419-30-v2|https://github.com/tommystendahl/cassandra/tree/cassandra-14419-30-v2] with two commits. One for the backport of the complete CASSANDRA-12016 and one for backport of the complete CASSANDRA-11960 incl the updates in {{HintsCompressionTest}}.;;;","03/Jul/18 13:25;aleksey;[~tommy_s] cheers. I ended up essentially cherry-picking CASSANDRA-12016 commit - to retain authorship, but committed your backport of CASSANDRA-11960 as is (except added the missing ASF license  header to {{InputPosition}}).

Committed as {{c4982587bfe3cb6946daa2912fe46146edef7fbf}} to 3.0 only and merged upwards with -s ours.;;;","03/Jul/18 13:34;tommy_s;[~iamaleksey] Thats great, thanks a lot!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra not starting when using enhanced startup scripts in windows,CASSANDRA-14418,13155164,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sphirke,sphirke,sphirke,25/Apr/18 15:45,15/May/20 08:05,13/Jul/23 08:37,12/May/18 12:34,3.0.17,3.11.3,4.0,4.0-alpha1,,,,,,,0,,,,"I am using Apache Cassandra 3.11.2 with my application. 

My application is getting installed under C:/Program Files/My Application/Some Folder/.

And cassandra C:/Program Files/My Application/Some Folder/cassandra.

So when I am using enhanced startup scripts cassandra not getting up and running and I am getting below error:

""Error: Could not find or load main class Files\My""

One of the solution I got is moving cassandra to another location where location path does not contain spaces. But this is not good way of getting this problem resolved.

After doing detailed analysis of all the scripts I found the solution below:

Inside file cassandra-env.ps1 at line number 380:

Replace line:

$env:JVM_OPTS = ""$env:JVM_OPTS -XX:CompileCommandFile=$env:CASSANDRA_CONF\hotspot_compiler""

with line

$env:JVM_OPTS = ""$env:JVM_OPTS -XX:CompileCommandFile=""""$env:CASSANDRA_CONF\hotspot_compiler""""""

Fix here is the double quotes added before $env:CASSANDRA_CONF and at the end.

At other places this case is well handled. But missed at this place.

 ",,djoshi,jasobrown,JoshuaMcKenzie,sphirke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,sphirke,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 15 12:52:51 UTC 2018,,,,,,,,,,,"0|i3t0kv:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"25/Apr/18 22:37;jasobrown;the fix 'seems' legit, but as I'm not a Windows user I cannot confirm.

If there's any Windows user who can confirm, I'm happy to commit. /cc [~JoshuaMcKenzie] [~blerer] are either of you two still using Windows, and can check out this fix?;;;","25/Apr/18 23:23;djoshi;[~jasobrown] i have a windows VM - i can check.;;;","26/Apr/18 20:09;JoshuaMcKenzie;This fix immediately passes the smell test for me. Those double quotes on env vars were the bane of my existence when I was working on those scripts.

I'll leave you to the testing Dinesh, but I have high hopes.;;;","10/May/18 05:32;sphirke;Any progress further..? Will this be available officially in the upcoming releases?;;;","10/May/18 12:02;jasobrown;[~djoshi3] did you have a chance to give this shot?;;;","12/May/18 08:24;djoshi;[~sphirke] [~jasobrown] Apologies for the delay. I have been busy. I can confirm that the fix works. It would be useful to double check if there are other places in the script where we may encounter similar issues.;;;","12/May/18 12:34;jasobrown;Thanks for confirming, [~djoshi3].

Looks like this line (and a similar one with the same quoting problem) was introduced in CASSANDRA-10939 (committed to 3.0+). I've gone ahead and fixed both those lines for 3.0 and up. Committed as sha {{b9b2a4e1a07af518cebd4441469c940d5ac0c2ea}}. Thanks, all!;;;","15/May/18 12:51;sphirke;Any idea about 3.11.3 release date?;;;","15/May/18 12:52;sphirke;And thank you very much guys for help.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression in queries for distinct keys,CASSANDRA-14415,13154888,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sklock,sklock,sklock,24/Apr/18 18:11,08/Jul/21 10:59,13/Jul/23 08:37,08/Jul/21 10:58,3.11.11,4.0.1,,,,,Legacy/Local Write-Read Paths,,,,0,performance,,,"Running Cassandra 3.0.16, we observed a major performance regression affecting {{SELECT DISTINCT keys}}-style queries against certain tables.  Based on some investigation (guided by some helpful feedback from Benjamin on the dev list), we tracked the regression down to two problems.
 * One is that Cassandra was reading more data from disk than was necessary to satisfy the query.  This was fixed under CASSANDRA-10657 in a later 3.x release.
 * If the fix for CASSANDRA-10657 is incorporated, the other is this code snippet in {{RebufferingInputStream}}:
{code:java}
    @Override
    public int skipBytes(int n) throws IOException
    {
        if (n < 0)
            return 0;
        int requested = n;
        int position = buffer.position(), limit = buffer.limit(), remaining;
        while ((remaining = limit - position) < n)
        {
            n -= remaining;
            buffer.position(limit);
            reBuffer();
            position = buffer.position();
            limit = buffer.limit();
            if (position == limit)
                return requested - n;
        }
        buffer.position(position + n);
        return requested;
    }
{code}
The gist of it is that to skip bytes, the stream needs to read those bytes into memory then throw them away.  In our tests, we were spending a lot of time in this method, so it looked like the chief drag on performance.

We noticed that the subclass of {{RebufferingInputStream}} in use for our queries, {{RandomAccessReader}} (over compressed sstables), implements a {{seek()}} method.  Overriding {{skipBytes()}} in it to use {{seek()}} instead was sufficient to fix the performance regression.

The performance difference is significant for tables with large values.  It's straightforward to evaluate with very simple key-value tables, e.g.:

{{CREATE TABLE testtable (key TEXT PRIMARY KEY, value BLOB);}}

We did some basic experimentation with the following variations (all in a single-node 3.11.2 cluster with off-the-shelf settings running on a dev workstation):
 * small values (1 KB, 100,000 entries), somewhat larger values (25 KB, 10,000 entries), and much larger values (1 MB, 10,000 entries);
 * compressible data (a single byte repeated) and uncompressible data (output from {{openssl rand $bytes}}); and
 * with and without sstable compression.  (With compression, we use Cassandra's defaults.)

The difference is most conspicuous for tables with large, uncompressible data and sstable decompression (which happens to describe the use case that triggered our investigation).  It is smaller but still readily apparent for tables with effective compression.  For uncompressible data without compression enabled, there is no appreciable difference.

Here's what the performance looks like without our patch for the 1-MB entries (times in seconds, five consecutive runs for each data set, all exhausting the results from a {{SELECT DISTINCT key FROM ...}} query with a page size of 24):
{noformat}
working on compressible
5.21180510521
5.10270500183
5.22311806679
4.6732840538
4.84219098091
working on uncompressible_uncompressed
55.0423607826
0.769015073776
0.850513935089
0.713396072388
0.62596988678
working on uncompressible
413.292617083
231.345913887
449.524993896
425.135111094
243.469946861
{noformat}
and with the fix:
{noformat}
working on compressible
2.86733293533
1.24895811081
1.108907938
1.12742400169
1.04647302628
working on uncompressible_uncompressed
56.4146180153
0.895509958267
0.922824144363
0.772884130478
0.731923818588
working on uncompressible
64.4587619305
1.81325793266
1.52577018738
1.41769099236
1.60442209244
{noformat}
The long initial runs for the uncompressible data presumably come from repeatedly hitting the disk.  In contrast to the runs without the fix, the initial runs seem to be effective at warming the page cache (as lots of data is skipped, so the data that's read can fit in memory), so subsequent runs are faster.

For smaller data sets, {{RandomAccessReader.seek()}} and {{RebufferingInputStream.skipBytes()}} are approximately equivalent in their behavior (reducing to changing the position pointer of an in-memory buffer most of the time), so there isn't much difference.  Here's before the fix for the 1-KB entries:
{noformat}
working on small_compressible
8.34115099907
8.57280993462
8.3534219265
8.55130696297
8.17362189293
working on small_uncompressible_uncompressed
7.85155582428
7.54075288773
7.50106596947
7.39202189445
7.95735621452
working on small_uncompressible
7.89256501198
7.88875198364
7.9013261795
7.76551413536
7.84927678108
{noformat}
and after:
{noformat}
working on small_compressible
8.29225707054
7.57822394371
8.10092878342
8.21332192421
8.19347810745
working on small_uncompressible_uncompressed
7.74823594093
7.81218004227
7.68660092354
7.95432114601
7.77612304688
working on small_uncompressible
8.18260502815
8.21010804176
8.1233921051
7.31543707848
7.91079998016
{noformat}
The effect is similar for the 25-KB entries, which might enjoy a slight performance benefit from the patch (perhaps because they're larger than the default buffer size defined in {{RandomAccessReader}}).  Before:
{noformat}
working on medium_compressible
0.988080978394
1.02464294434
0.977658033371
1.02553391457
0.769363880157
working on medium_uncompressible_uncompressed
1.07718396187
1.08547902107
1.12398791313
1.10300898552
1.08757281303
working on medium_uncompressible
0.940990209579
0.917474985123
0.768013954163
0.871683835983
0.814841985703
{noformat}
and after:
{noformat}
working on medium_compressible
0.829009056091
0.705173015594
0.603646993637
0.820069074631
0.873830080032
working on medium_uncompressible_uncompressed
0.785156965256
0.808106184006
0.848286151886
0.857885837555
0.825689077377
working on medium_uncompressible
0.845101118088
0.913790941238
0.824147939682
0.849114894867
0.85981798172
{noformat}
In short, this looks like a pretty straightforward performance win with negligible cost.  (It's worth noting that for our use case, disabling sstable compression is clearly the _best_ solution, but there's still reasonably clear benefit from this minor fix for data sets with larger, compressible values, as well as presumably data sets with a mix of compressible and uncompressible values in environments where storage is limited.)",,aleksey,benedict,blerer,hkroger,jjirsa,KurtG,sklock,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,sklock,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 08 10:59:14 UTC 2021,,,,,,,,,,,"0|i3syvj:",9223372036854775807,3.0.16,3.11.2,,,,,,,,,benedict,blerer,dissent,Normal,,3.0.0,,,https://github.com/apache/cassandra/commit/f49b86dc47fb1f09a8aaa6642b1b456ef4292391,,,,,,,,,,,,,,"24/Apr/18 18:15;sklock;Patch available [here|https://github.com/akasklock/cassandra/tree/CASSANDRA-14415-Use-seek-for-skipBytes-3.11.2].;;;","09/May/18 03:01;jjirsa;Is this patch useful in 3.0.x branch without the fix from CASSANDRA-10657 ? 

;;;","09/May/18 13:46;sklock;We haven't tested, but I doubt the patch would be much help for _this_ workflow without CASSANDRA-10657.  If there are other contexts where Cassandra wants to skip large chunks of a (compressed) file it's modeling as a stream, then the patch might provide some meaningful benefit for 3.0.x.  I don't know if there are any though.;;;","14/May/18 01:17;KurtG;[~sklock] responding as per your request on the ML:
{quote}Can someone please take a look at CASSANDRA-14415 when you have chance?
Getting a fix into a Cassandra release is not especially urgent for us,
but in lieu of that we would like to know whether it's safe to include
in our local build of Cassandra before attempting to deploy it.
{quote}
Seems to me that it would be fine. If you want to be sure (and this is probably a good idea w.r.t patching 3.11 anyway) you could write a test that ensures the behaviour of {{RAR.skipBytes()}} is always what you'd expect from {{RIS.skipBytes()}}.

Only thing I didn't fully understand w.r.t patch is
{code}
        if (n < 0 || buffer == null)
            return super.skipBytes(n);
{code}
In the case that n < 0, delegating back to {{RIS.skipBytes()}} will return the same thing, and in the case that buffer == null, {{RIS.skipBytes()}} will NPE. If the extra check is necessary seems to me it should apply to both methods, but all you can do is return 0. Might be better to just leave the extra check out...?;;;","14/May/18 13:19;sklock;Thanks for taking a look, [~KurtG]. Adding a test is certainly reasonable feedback; we'll make that change soon.
{quote}In the case that n < 0, delegating back to {{RIS.skipBytes()}} will return the same thing, and in the case that buffer == null, {{RIS.skipBytes()}} will NPE. If the extra check is necessary seems to me it should apply to both methods, but all you can do is return 0. Might be better to just leave the extra check out...?
{quote}
The intent of this check is to conservatively preserve the existing behavior for {{n}} < 0 or a null {{buffer}}. Regarding the NPE behavior in particular: having a {{skipBytes()}} implementation NPE is at least surprising (and possibly a bug), but it's not straightforward to tell if there's any logic in Cassandra that depends on it. So we elected not to change that behavior. Delegating to the superclass seemed like a good way to ensure that the behavior of {{skipBytes()}} is kept consistent in case the superclass implementation does ever end up changing.

That said: happy not to delegate to the superclass (or add an explanatory comment) if that'd be preferable. For an implementation here:
 * If {{n}} < 0, the obvious choice is to return 0 without mutating the reader's state.
 * If {{buffer}} is null, if we just fall through to {{seek()}}, we'll throw an {{IllegalStateException}}, which seems undesirable. The contract for {{skipBytes()}} suggests returning 0 would be a reasonable choice, although it might be better to signal a problem to the caller by throwing an {{IOException}}. That would be a pretty abrasive change in behavior, however.;;;","14/May/18 19:49;sklock;[The branch|https://github.com/apache/cassandra/compare/trunk...akasklock:CASSANDRA-14415-Use-seek-for-skipBytes-3.11.2] now includes test coverage.  CI runs: [3.11|https://circleci.com/gh/akasklock/cassandra/6] and [trunk|https://circleci.com/gh/akasklock/cassandra/9].;;;","15/May/18 02:04;KurtG;bq. Regarding the NPE behavior in particular: having a skipBytes() implementation NPE is at least surprising (and possibly a bug), but it's not straightforward to tell if there's any logic in Cassandra that depends on it. So we elected not to change that behavior. Delegating to the superclass seemed like a good way to ensure that the behavior of skipBytes() is kept consistent in case the superclass implementation does ever end up changing.
 Yeah that makes sense, it's pretty hard to tell all the ways {{RAR.skipBytes()}} gets used so I agree changing behaviour here is bad idea. I can't imagine anything is relying on an NPE being thrown (I sincerely hope not). Ideally I'd like to change it to an explicitly thrown IOException but yeah this won't work in 3.11.

Others opinions would be welcome here but I'd say it should be safe to change it to explicitly throw an {{IOException}} in trunk, and leave 3.11 as is.

Other than that patch LGTM with only minor nit that I'd prefer to use the expected exception annotation rather than try/catch with a fail. e.g
{code:java}
    @Test(expected = NullPointerException.class)
    public void testSkipBytesBoundaryCases() throws IOException
{code}

Also, there was a test failure on trunk which is likely unrelated, but worth rerunning tests anyway.;;;","15/May/18 15:28;sklock;Thanks.  NPE in 3.11 and {{IOException}} in trunk sounds very reasonable.  The patches now reflect that feedback.

||Patch||Tests||
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...akasklock:CASSANDRA-14415-Use-seek-for-skipBytes-3.11.2] |[link|https://circleci.com/gh/akasklock/cassandra/12] |
|[trunk|https://github.com/apache/cassandra/compare/trunk...akasklock:CASSANDRA-14415-Use-seek-for-skipBytes-trunk] |[link|https://circleci.com/gh/akasklock/cassandra/13] |

* All of the tests for trunk passed in this run.
* The patch for 3.11 should also apply to 3.0, but as noted above, we're not confident it would be useful without CASSANDRA-10657, at least for the workflow we're concerned about in this issue.;;;","15/Aug/19 15:37;sklock;Ping.  Are there any blockers to merging this?  We've been using this software fix in our local Cassandra distribution for some time without any problems, but we're happy to make additional changes to make this ready for the community.;;;","16/Aug/19 09:21;benedict;Late to the party, but I agree with Kurt that we should simply {{return 0}} for {{n < 0}}, and we should probably let {{seek}} handle the {{null}} buffer.

Looks like a good simple patch.  I don't see any blockers to this.;;;","16/Aug/19 17:06;sklock;Thanks for the feedback.  I've tweaked the 3.11 patch accordingly.  (Minor wrinkle: we don't end up deferring to {{seek()}} in the {{null}} buffer case as {{current()}}, which uses the buffer, is called first.);;;","30/Jan/20 14:45;sklock;Pinging again.  Are there any remaining blockers?;;;","07/Jul/21 12:58;blerer;+1 on my side. [~benedict] and [~KurtG] already gave there +1 too.

I will rebase the patches and run CI.;;;","08/Jul/21 10:00;blerer;CI looks good.
|| Branch || CI ||
| 3.11 | [j8|https://app.circleci.com/pipelines/github/blerer/cassandra/180/workflows/cfbc1ee2-ccd6-4041-9180-d1e6ec5ae215] |
| 4.0 | [j8|https://app.circleci.com/pipelines/github/blerer/cassandra/179/workflows/5fb36b8f-a86a-4270-808f-33617558f09f], [j11|https://app.circleci.com/pipelines/github/blerer/cassandra/179/workflows/67a3365f-90b4-421a-81c1-56db4550f466] |   ;;;","08/Jul/21 10:58;blerer;Committed into cassandra-3.11 at f49b86dc47fb1f09a8aaa6642b1b456ef4292391 and merged into cassandra-4.0 and trunk;;;","08/Jul/21 10:59;blerer;Thanks a lot for the patch [~sklock]. Sorry it took so long to get it committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restore automatic snapshot of system keyspace during upgrade,CASSANDRA-14412,13154459,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,samt,samt,23/Apr/18 11:52,15/May/20 08:07,13/Jul/23 08:37,17/Apr/19 14:35,4.0,4.0-alpha1,,,,,Local/Startup and Shutdown,,,,0,,,,"Since 2.2, the installed version is compared with the version persisted in system.local (if any) at startup. If these versions differ, the system keyspace is snapshotted before proceeding in order to enable a rollback if any other issue prevents startup from completing. Although the method to perform this check & snapshot is still present in {{SystemKeyspace}}, its only callsite was mistakenly removed from {{CassandraDaemon}} in CASSANDRA-12716.",,aleksey,samt,tommy_s,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 17 14:35:40 UTC 2019,,,,,,,,,,,"0|i3sw87:",9223372036854775807,,,,,,,,,,,aleksey,tommy_s,,Normal,,4.0,,,,,,,,,,,,,,,,,"23/Apr/18 12:42;samt;Pushed a branch [here|https://github.com/beobal/cassandra/tree/14412] which adds back the call to {{SystemKeyspace::snapshotOnVersionChange}}, minus the call to {{SystemKeyspace::migrateDataDirs}} that it was previously guarding, but which is genuinely no longer necessary. As snapshots are reasonably cheap and an upgrade should be a rare event anyway, I've extended the original method to also snapshot {{system_schema}}.

CI here: https://circleci.com/workflow-run/677d3b4e-85e5-4b90-bb41-309cd4b361f2 
;;;","10/Apr/19 13:35;tommy_s;It would be nice to have this feature enabled again, it definitely looks like it was removed by mistake. Extending this with snapshots of the {{system_schema}} is a good thing.

I have looked at the patch and it LGTM with one comment:
 * I think the test case {{SystemKeyspaceTest.snapshotSystemKeyspaceIfUpgrading()}} should be extended so it also check the snapshot for {{system_schema}}.

otherwise I'm +1 for this.;;;","16/Apr/19 12:13;samt;Thanks [~tommy_s], I've added extra checks to the test, rebased and pushed:

||branch||CI||
|[14412-trunk|https://github.com/beobal/cassandra/tree/14412-trunk]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F14412-trunk]|;;;","17/Apr/19 09:07;tommy_s;I looked at the changes to the test case and it looks fine.;;;","17/Apr/19 12:37;aleksey;LGTM as well.;;;","17/Apr/19 14:35;samt;Thanks [~tommy_s], [~iamaleksey] - committed to trunk in {{372a6cfa7b0c5cf52b2db84edf210fa3d5c7f78e}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use Bounds instead of Range to represent sstable first/last token when checking how to anticompact sstables,CASSANDRA-14411,13154380,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,23/Apr/18 06:23,15/May/20 08:00,13/Jul/23 08:37,24/Apr/18 07:02,2.2.13,3.0.17,3.11.3,4.0,4.0-alpha1,,Consistency/Repair,,,,0,,,,"There is currently a chance of missing marking a token as repaired due to the fact that we use Range which are (a, b] to represent first/last token in sstables instead of Bounds which are [a, b].",,bdeggleston,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 24 07:02:18 UTC 2018,,,,,,,,,,,"0|i3svqn:",9223372036854775807,,,,,,,,,bdeggleston,,bdeggleston,,,Normal,,,,,,,,,,,,,,,,,,,"23/Apr/18 06:25;marcuse;https://github.com/krummas/cassandra/commits/marcuse/14411

tests:
https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F14411;;;","23/Apr/18 07:27;marcuse;minimal patches for 2.2 -> 3.11: 
https://github.com/krummas/cassandra/commits/marcuse/14411-2.2
https://github.com/krummas/cassandra/commits/marcuse/14411-3.0
https://github.com/krummas/cassandra/commits/marcuse/14411-3.11

tests for 3.11: https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F14411-3%2E11;;;","23/Apr/18 16:44;bdeggleston;circle seems to be down, but +1 assuming tests look good;;;","24/Apr/18 07:02;marcuse;committed as {{334dca9aa825e6d353aa04fd97016ac1077ff132}}, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tablehistograms with non-existent table gives an exception,CASSANDRA-14410,13154339,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,hkroger,hkroger,hkroger,22/Apr/18 19:47,27/May/22 19:25,13/Jul/23 08:37,08/Aug/19 18:06,4.1,4.1-alpha1,,,,,Legacy/Tools,,,,0,lhf,,,"nodetool tablehistograms with non-existent table gives a crazy exception. It should give a nice error message like ""Table acdc.abba doesn't exist"" or something like that.

 

Example:
{code:java}
$ nodetool tablehistograms acdc.abba
error: org.apache.cassandra.metrics:type=Table,keyspace=acdc,scope=abba,name=EstimatedPartitionSizeHistogram
-- StackTrace --
javax.management.InstanceNotFoundException: org.apache.cassandra.metrics:type=Table,keyspace=acdc,scope=abba,name=EstimatedPartitionSizeHistogram
    at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
    at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:643)
    at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)
    at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1445)
    at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:76)
    at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1309)
    at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1401)
    at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:639)
    at sun.reflect.GeneratedMethodAccessor297.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:324)
    at sun.rmi.transport.Transport$1.run(Transport.java:200)
    at sun.rmi.transport.Transport$1.run(Transport.java:197)
    at java.security.AccessController.doPrivileged(Native Method)
    at sun.rmi.transport.Transport.serviceCall(Transport.java:196)
    at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:568)
    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:826)
    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:683)
    at java.security.AccessController.doPrivileged(Native Method)
    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:682)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
    at sun.rmi.transport.StreamRemoteCall.exceptionReceivedFromServer(StreamRemoteCall.java:283)
    at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:260)
    at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:161)
    at com.sun.jmx.remote.internal.PRef.invoke(Unknown Source)
    at javax.management.remote.rmi.RMIConnectionImpl_Stub.getAttribute(Unknown Source)
    at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.getAttribute(RMIConnector.java:903)
    at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:273)
    at com.sun.proxy.$Proxy20.getValue(Unknown Source)
    at org.apache.cassandra.tools.NodeProbe.getColumnFamilyMetric(NodeProbe.java:1334)
    at org.apache.cassandra.tools.nodetool.TableHistograms.execute(TableHistograms.java:62)
    at org.apache.cassandra.tools.NodeTool$NodeToolCmd.run(NodeTool.java:254)
    at org.apache.cassandra.tools.NodeTool.main(NodeTool.java:168){code}
 ",,cnlwsu,eperott,hkroger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15060,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,hkroger,,,,,,,,,,,,,,,,,,,,Low Hanging Fruit,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Dec 07 00:21:02 UTC 2018,,,,,,,,,,,"0|i3svhj:",9223372036854775807,3.11.0,,,,,,,,cnlwsu,,cnlwsu,,,Normal,,4.0,,,https://github.com/apache/cassandra/commit/d6c049f0835f137fc07711ec5cf9adc323347c65,,,,,,,,,,,,,,"25/Apr/18 03:59;hkroger;Branch on top of cassandra-3.11 with a fix: [https://github.com/hkroger/cassandra/tree/CASSANDRA-14410]

 ;;;","25/Apr/18 04:37;cnlwsu;Can you make a trunk version? This logic has changed a little since your branch with allowing entire keyspace or all tables to be printed out.;;;","25/Apr/18 04:57;hkroger;Based on what I see in the trunk version is that you can now run tablehistograms also without arguments and then it displays histograms for all tables. But what I didn't see is the option to limit it to just one keyspace.

Anyways: Here is the same thing on top of trunk: [https://github.com/hkroger/cassandra/tree/CASSANDRA-14410-trunk]

 ;;;","25/Apr/18 05:33;cnlwsu;You are missing the case where args.size() == 1 and keyspace.table is passed instead of ""keyspace table"".

nitpick: {{Collections.singletonList(table)}} instead of {{new ArrayList<String>(Arrays.asList(table))}} (I realize this is from previous not added by your patch);;;","25/Apr/18 16:19;hkroger;ah, right. I changed that a bit now.;;;","25/Apr/18 16:23;cnlwsu;+1 from me;;;","05/Dec/18 20:52;cnlwsu;if want this in 3.11 can you update that branch as well with fixes for args.size()==1 ?;;;","07/Dec/18 00:21;hkroger;[~cnlwsu] Did that now. I also improved the error message layout a bit.

In both branches the error looks like this:
{code:java}
nodetool: Unknown table system.peers1
See 'nodetool help' or 'nodetool help <command>'.{code}
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Client TOPOLOGY_CHANGE  messages have wrong port.,CASSANDRA-14398,13153507,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,aweisberg,greg.bestland,greg.bestland,18/Apr/18 22:00,15/May/20 08:00,13/Jul/23 08:37,19/Apr/18 16:19,4.0,4.0-alpha1,,,,,Legacy/Core,,,,0,,,,"Summary:

TOPOLOGY_CHANGE events that are recieved by the client(Driver), with C* 4.0 (Trunk). Contain the storage port (7000) rather than the native port (9042). I believe this is due to changes stuck in for CASSANDRA-7544.  

I was able to track it down to this specific call here.
 [https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/StorageService.java#L1703]

We need an accurate port number from this Topology change event, otherwise we won't be able to correlate node up event against the system.peers_v2 table accurately.

C* version I'm testing against : 4.0.x (trunk)

Steps to reproduce:

1. create a single node, cluster, in this case I'm using ccm.
{code:java}
ccm create 400-1 --install-dir=/Users/gregbestland/git/cassandra
ccm populate -n 1
ccm start
{code}
2. Connect the java driver, and check metadata.
 see that there is one node with the correct native port (9042).

3. Add a brand new node:
{code:java}
ccm add node2 -i 127.0.0.2
ccm node2 start
{code}
4. Incoming topology change message to client will have the storage port, rather then the native port in the message.",,aweisberg,djoshi,greg.bestland,jeromatron,omichallat,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aweisberg,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,Clients,,,Thu Apr 19 16:19:38 UTC 2018,,,,,,,,,,,"0|i3sqev:",9223372036854775807,,,,,,,,,greg.bestland,,greg.bestland,,,Critical,,,,,,,,,,,,,,,,,,,"19/Apr/18 01:06;aweisberg;https://circleci.com/gh/aweisberg/cassandra/tree/cassandra-14398-trunk
Proposed fix:
https://github.com/apache/cassandra/compare/trunk...aweisberg:cassandra-14398-trunk?expand=1;;;","19/Apr/18 15:39;greg.bestland;Accidentally marked as ready to commit. Still reviewing and testing will be done shortly.;;;","19/Apr/18 15:43;greg.bestland;+1 works perfectly. Thanks a ton.;;;","19/Apr/18 16:19;aweisberg;Committed as [cb67bfc1639ded1b6937e7347ad42177ea3f24e3|https://github.com/apache/cassandra/commit/cb67bfc1639ded1b6937e7347ad42177ea3f24e3]. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resolve local address binding in 4.0,CASSANDRA-14389,13152996,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,djoshi,jasobrown,jasobrown,17/Apr/18 12:52,15/May/20 08:02,13/Jul/23 08:37,22/Apr/18 23:38,4.0,4.0-alpha1,,,,,Legacy/Streaming and Messaging,,,,0,,,,CASSANDRA-8457/CASSANDRA-12229 introduced a regression against CASSANDRA-12673. This was discovered with CASSANDRA-14362 and moved here for resolution independent of that ticket.,,djoshi,jasobrown,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,djoshi,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Apr 22 23:38:38 UTC 2018,,,,,,,,,,,"0|i3sn9j:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"19/Apr/18 15:40;djoshi;I found the issue. When you leave the local side of the socket unbound, the kernel will prefer the IP address that matches the remote IP. Say node1 with IP {{127.0.0.1}} wants to open a connection to node2 with IP {{127.0.0.2}}, the socket would look like {{<127.0.0.2:61002, 127.0.0.2:7000>}} on node1. This seems to confuse the streaming code. Here's how -

Say we have three nodes node1, node2 & node3 with IPs {{127.0.0.1, 127.0.0.2, 127.0.0.3}}. node1 has data and node3 is bootstrapping. It requests a stream from node1. So node3 is the `peer` in this case and node1's code execution is described below -

* node1 receives the request ({{StreamingInboundHandler#deriveSession}}) and {{StreamResultFuture#initReceivingSide}} creates a new {{StreamResultFuture}} and calls {{attachConnection()}}. At this point it has two sets of IP & Ports from the peer. They are identified by the variable `{{from}}` & expression `{{channel.remoteAddress()}}` a.k.a `{{connecting}}` ).
* {{StreamResultFuture#attachConnection calls StreamCoordinator#getOrCreateSessionById}} passing the from IP & {{InetAddressAndPort.getByAddressOverrideDefaults(connecting, from.port)}} (!!!)
* The key observation here is `from` is the IP that the peer sent in the `{{StreamMessageHeader}}` while `connecting` is the remote IP of the peer.
* {{StreamCoordinator#getOrCreateSessionById}} subsequently calls {{StreamCoordinator#getOrCreateHostData(peer)}}. So we're indexing the {{peerSessions}} by the `{{peer}}` IP address. We also end up creating a `{{StreamSession}}` in the process.
* During `{{StreamSession}}` creation, we end up passing the `{{peer}}` and `{{connecting}}` IPs. We use the `connecting` IP to establish the outbound connection to the peer. ({{NettyStreamingMessageSender}} is now connected to `{{connecting}}` IP on port {{7000}}).

In our case, since we leave the local side of the socket unbound, although the `{{peer}}` correctly sets its IP to {{127.0.0.3}} in the {{StreamMessageHeader}}, the {{localAddress}} that the kernel chooses for it is {{127.0.0.1}}. On the inbound node1 seems to think that the `peer` is {{127.0.0.3}} however the connecting IP address should be {{127.0.0.1}}. Therefore, it prefers that IP when trying to establish an outbound session. In fact it establishes a connection to itself leading to the `{{Unknown peer requested: 127.0.0.1:7000}}` exception. Note that along the way it actually drops the ephemeral port and instead uses the port returned by {{MessagingService#portFor}}.

Streaming code seems to rely on the perceived remote IP address of the host rather than the one that is set in the message header. I am not sure if preferring the IP address set in the header is the correct approach.;;;","20/Apr/18 04:52;djoshi;Here's a fix along with restoring the behavior from CASSANDRA-12673

||trunk||
|[branch|https://github.com/dineshjoshi/cassandra/tree/CASSANDRA-14389-trunk-fix-streaming]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/CASSANDRA-14389-trunk-fix-streaming]|
||;;;","20/Apr/18 20:02;jasobrown;[~djoshi3], This looks pretty good. However, I wonder if we can dump all the places where we naively plumb the 'connecting' address though. I took a pass at it here:

||14389||
|[branch|https://github.com/jasobrown/cassandra/tree/14389]|
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14389]|
||

The only change of interest is in {{MessagingService#getPreferredRemoteAddr()}}, I check into {{SystemKeyspace.getPreferredIP(to)}} if there was no {{OutboundMessagingPool}} set up in {[MessagingService#channelManagers}}. wdyt?;;;","20/Apr/18 21:43;djoshi;Hi [~jasobrown], thank you for reviewing. I have incorporated your changes. I also added a test for {{StreamSession}}.;;;","21/Apr/18 00:55;djoshi;I have incorporated the changes from your commit as well as added tests for \{{MessagingService#getPreferredRemoteAddr}}. Also squashed all commit into one.;;;","21/Apr/18 16:13;jasobrown;I didn't understand the {{NettyStreamingMessageSenderFactory}} and subclassing of {{NettyStreamingMessageSender}} in the {{StreamSessionTest}}. You weren't really taking any advantage of the subclass as all {{NSMSStub}} did was override the parent's constructor, only to call the parent's constructor. Thus I've removed {{NettyStreamingMessageSenderFactory}} (ignore the comment on the commit) and cleaned up the test. I've pushed up a commit to my branch, on top of your squash, and ran the testa again. If you are good with that, I'm +1 on the rest of the patch.;;;","21/Apr/18 21:56;djoshi;Yes that looks good. I was fiddling with different ways of writing the test and the constructor that I introduced in {{StreamSession}} made the factory redundant. Thank you for taking care of it.;;;","22/Apr/18 23:38;jasobrown;committed as sha {{63945228fc0fabea2cfcf1f1b4d0a29ed3964107}}. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix setting min/max compaction threshold with LCS,CASSANDRA-14388,13152896,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,17/Apr/18 05:43,15/May/20 08:00,13/Jul/23 08:37,19/Jun/18 16:04,4.0,4.0-alpha1,,,,,Local/Compaction,,,,0,lcs,,,To be able to actually set max/min_threshold in compaction options we need to remove it from the options map when validating.,,cnlwsu,ifesdjeen,jeromatron,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-12526,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 19 16:03:56 UTC 2018,,,,,,,,,,,"0|i3smnb:",9223372036854775807,,,,,,,,,ifesdjeen,,ifesdjeen,,,Normal,,,,,,,,,,,,,,,,,,,"17/Apr/18 05:55;marcuse;https://github.com/krummas/cassandra/commits/marcuse/14388

This patch also replaces the use of {{MAX_COMPACTING_L0}} with {{max_threshold}} in {{LeveledManifest}} - not 100% sure this is the best way since it will increase the number of sstables needed to run STCS in L0, but at the same time it will run actual LCS with more sstables. An alternative would be to check if there is more than 32 ({{MAX_COMPACTING_L0}}) sstables in L0, if so, grab {{max_threshold}} sstables and run STCS on them, wdyt [~cnlwsu]?;;;","17/Apr/18 14:11;cnlwsu;I like the alternative personally
{quote}An alternative would be to check if there is more than 32 ({{MAX_COMPACTING_L0}}) sstables in L0, if so, grab {{max_threshold}} sstables and run STCS on them
{quote}
Thinking mostly for cases when its critically far behind (ie 50k in L0) and a lot are tiny (from bad repairs), just want to quickly reduce the number of them. The {{MAX_COMPACTING_L0}} I think makes sense for normal use, but when theres a huge backlog just want STCS to chew up backlog faster than 32 at a time. But if increase L0 max compacting to 1000 it may not kick off STCS until already negatively impacting things.

Might be nice to have {{MAX_COMPACTING_L0}} as another table option but that could be done another ticket.;;;","18/Apr/18 07:12;marcuse;pushed a commit with MAX_COMPACTING_L0 as the trigger for STCS in L0: https://github.com/krummas/cassandra/commits/marcuse/14388
tests: https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F14388;;;","30/May/18 07:21;ifesdjeen;Should we add a news entry about the fact that {{MAX_COMPACTING_L0}} will now be overridden by the CFS max compaction threshold? And/or a ticket to make {{MAX_COMPACTING_L0}} configurable.;;;","30/May/18 08:55;marcuse;setting [~ifesdjeen] as reviewer

Pushed a commit with an updated NEWS.txt entry - we could make MAX_COMPACTING_L0 configurable later if someone thinks it is necessary but 32 seems to be a good value right now, especially after this since it will only decide when to run a STCS compaction;;;","19/Jun/18 16:03;ifesdjeen;Committed as d52bdaefda366b4485acb4e8852b3c0549b184bd to [trunk|https://github.com/apache/cassandra/commit/d52bdaefda366b4485acb4e8852b3c0549b184bd].
Thank you for the patch!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableReaderTest#testOpeningSSTable fails on macOS,CASSANDRA-14387,13152775,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,djoshi,djoshi,djoshi,16/Apr/18 17:17,15/May/20 08:00,13/Jul/23 08:37,16/Apr/18 18:38,3.0.17,3.11.3,4.0,4.0-alpha1,,,,,,,0,,,,"I ran into an issue with {{SSTableReaderTest#testOpeningSSTable}} test failure on macOS. The reason for failure seems that on macOS, the file modification timestamps are at a second granularity (See: https://stackoverflow.com/questions/18403588/how-to-return-millisecond-information-for-file-access-on-mac-os-x-in-java and https://developer.apple.com/legacy/library/technotes/tn/tn1150.html#HFSPlusDates). The fix is simple - bumping up the sleep time to 1 second instead of 10ms.


{noformat}
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testOpeningSSTable(org.apache.cassandra.io.sstable.SSTableReaderTest):	FAILED
    [junit] Bloomfilter was not recreated
    [junit] junit.framework.AssertionFailedError: Bloomfilter was not recreated
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.testOpeningSSTable(SSTableReaderTest.java:421)
    [junit]
    [junit]
    [junit] Test org.apache.cassandra.io.sstable.SSTableReaderTest FAILED
{noformat}

Related issue: CASSANDRA-11163",,aleksey,cnlwsu,djoshi,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-11163,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,djoshi,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 30 18:32:25 UTC 2018,,,,,,,,,,,"0|i3slwv:",9223372036854775807,,,,,,,,,cnlwsu,,cnlwsu,,,Low,,,,,,,,,,,,,,,,,,,"16/Apr/18 17:19;djoshi;||trunk||
|[branch|https://github.com/dineshjoshi/cassandra/tree/sstable-junit-failure]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/sstable-junit-failure]|
||;;;","16/Apr/18 17:36;cnlwsu;+1;;;","16/Apr/18 18:38;jjirsa;Committed to trunk as 8a5e1cbe293ee7c83efba0d0101ada0a80cfaf00
;;;","30/Apr/18 13:34;aleksey;Can you please commit to 3.0 and 3.11 too? Because tests are failing on Jenkins :\ Cheers.;;;","30/Apr/18 18:14;aleksey;Cherry-plicked into 3.0 as [e16f0ed0698c5cb47ab2bb0a0b04966d5bdbcde0|https://github.com/apache/cassandra/commit/e16f0ed0698c5cb47ab2bb0a0b04966d5bdbcde0] and merged upwards, replacing the somewhat silly millis to millis conversion with {{TimeUnit.sleep()}} in the process.;;;","30/Apr/18 18:32;djoshi;Thank you for taking care of this [~iamaleksey];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool listsnapshots is missing local system keyspace snapshots,CASSANDRA-14381,13152104,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,cscetbon,cscetbon,12/Apr/18 22:33,15/May/20 08:02,13/Jul/23 08:37,08/Jan/20 05:03,4.0,4.0-alpha1,,,,,Legacy/Core,,,,0,remove-reopen,,,"The output of *nodetool listsnapshots* is inconsistent with the snapshots created :
{code:java}
$ nodetool listsnapshots
Snapshot Details:
There are no snapshots

$ nodetool snapshot -t tag1 --table local system
Requested creating snapshot(s) for [system] with snapshot name [tag1] and options {skipFlush=false}
Snapshot directory: tag1

$ nodetool snapshot -t tag2 --table local system
Requested creating snapshot(s) for [system] with snapshot name [tag2] and options {skipFlush=false}
Snapshot directory: tag2

$ nodetool listsnapshots
Snapshot Details:
There are no snapshots

$ ls /usr/local/var/lib/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/snapshots/
tag1 tag2{code}
 

 ","MacOs 10.12.5

Java 1.8.0_144

Cassandra 3.11.2 (brew install)",aleksey,aweisberg,blerer,cscetbon,cscotta,jay.zhuang,jeromatron,jjirsa,kohlisankalp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jan 08 05:03:39 UTC 2020,,,,,,,,,,,"0|i3sht3:",9223372036854775807,3.11.2,,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"12/Apr/18 23:11;jay.zhuang;{{listsnapshots}} excludes local system keyspaces ({{system}} and {{system_schema}}): [{{StorageService.java:3290}}|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/service/StorageService.java#L3290], but not sure why.;;;","13/Apr/18 01:18;cscetbon;hmm, and it's there in 2.1 too [https://github.com/apache/cassandra/blob/cassandra-2.1/src/java/org/apache/cassandra/service/StorageService.java#L2629-L2630] and it's been there for 4 years [https://github.com/apache/cassandra/commit/719103b649c1c5459683a8ffd1c013664f1ffbb6]

I really don't know why it's there. What if we need to restore the whole node/cluster for some reason ??

 ;;;","13/Apr/18 04:08;jay.zhuang;The snapshot is taken and the user is still able to restore data for system keyspaces, just the snapshot is not in the {{listsnapshots}} output. Sometimes we do need to restore data for system tables, for example, if the schema is corrupted: [{{SchemaKeyspace.java:951}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/schema/SchemaKeyspace.java#L951]
@[~aweisberg], @[~stefania_alborghetti], what do you think to list system/system_schema snapshots?;;;","13/Apr/18 16:35;aweisberg;[~kohlisankalp] can you explain why you added this check? [https://github.com/apache/cassandra/commit/719103b649c1c5459683a8ffd1c013664f1ffbb6#diff-b76a607445d53f18a98c9df14323c7ddR2239]

If we removed it how bad would it be?

 

I'm not sure what the role of snapshotting system tables is. Sounds kind of sketchy rolling back internal DB state, but if it's technically possible seems like they should show up.;;;","13/Apr/18 16:58;kohlisankalp;I dont remember fully but why we need snapshot for internal tables;;;","13/Apr/18 17:00;cscetbon;what if the table was corrupted locally ? why then a global snapshot includes it, but it's not listed with that command ?;;;","13/Apr/18 17:06;kohlisankalp;We can include it as I dont see a harm in listing it. ;;;","13/Apr/18 18:01;jay.zhuang;[~cscetbon], are you interested in putting a quick patch for it? I think it should be {{trunk}} only, as it's just a nodetool display problem, and we don't want to change the behavior for {{3.0}} and {{3.11}}.;;;","14/Apr/18 07:43;jjirsa;Snapshotting system tables is perfectly valid - it allows you to roll back a major version upgrade (with data files at the same time of snapshot, losing data written in the new version).

 ;;;","17/Apr/18 15:13;cscetbon;Okay that's what I thought. Whenever I find some time, I should be able to remove that piece of code that skips the system keyspace;;;","17/Apr/18 16:46;aweisberg;I had a free minute so put up a patch for this.
[Trunk code|https://github.com/apache/cassandra/compare/trunk...aweisberg:cassandra-14381-trunk?expand=1]
[CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/cassandra-14381-trunk];;;","17/Apr/18 16:57;jay.zhuang;+1

LGTM. Is the circleci configuration change needed?;;;","17/Apr/18 17:13;cscetbon;Thanks [~aweisberg];;;","17/Apr/18 17:31;aweisberg;The cicleci change is not part of the change it's just necessary to get CircleCI to run the dtests for me.;;;","19/Apr/18 22:18;aweisberg;[~cscetbon] I need your explicit +1 to use you as the reviewer. Thanks.;;;","19/Apr/18 22:33;cscetbon;+1 on [https://github.com/apache/cassandra/commit/a0ceb3];;;","20/Apr/18 15:54;aweisberg;Committed as [e0524c099e0b59cccfd1a9cb9424147cfd001a32|https://github.com/apache/cassandra/commit/e0524c099e0b59cccfd1a9cb9424147cfd001a32]. Thanks!;;;","24/Apr/18 09:25;blerer;It seems to me that it is a bug and that we should also have made the change in the other branches. Not only in trunk.;;;","24/Apr/18 17:07;aweisberg;How far back should we go? All the way to 2.1 or just 3.0? ;;;","25/Apr/18 01:42;cscetbon;I'd say the patch is so simple let's push it to 2.1 and 3.0 (I still have 2.1 nodes running);;;","25/Apr/18 09:12;aleksey;2.1 is critical bug fixes only at this point, simple or not, so please don’t.;;;","08/Jan/20 05:03;cscotta;Marking as resolved as this has been committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Returning invalid JSON for NaN and Infinity float values,CASSANDRA-14377,13151647,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,blerer,Sarna,Sarna,11/Apr/18 13:11,15/May/20 08:04,13/Jul/23 08:37,02/Aug/18 09:36,2.2.14,3.0.18,3.11.4,4.0,4.0-alpha1,,Legacy/CQL,,,,0,,,,"After inserting special float values like NaN and Infinity into a table:

{{CREATE TABLE testme (t1 bigint, t2 float, t3 float, PRIMARY KEY (t1));}}
{{INSERT INTO testme (t1, t2, t3) VALUES (7, NaN, Infinity);}}

and returning them as JSON...

{{cqlsh:demodb> select json * from testme;}}
{{ [json]}}
{{--------------------------------------}}
{{ \{""t1"": 7, ""t2"": NaN, ""t3"": Infinity}}}

 

... the result will not be validated (e.g. with [https://jsonlint.com/|https://jsonlint.com/)] ) because neither NaN nor Infinity is a valid JSON value. The consensus seems to be returning JSON's `null` in these cases, based on this article [https://stackoverflow.com/questions/1423081/json-left-out-infinity-and-nan-json-status-in-ecmascript] and other similar ones.",,avi.kivity,blerer,fcofdezc,Sarna,vinaykumarcse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,blerer,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 02 09:36:02 UTC 2018,,,,,,,,,,,"0|i3sezz:",9223372036854775807,2.2.12,3.11.2,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,,"17/Apr/18 08:38;blerer;I pushed a patch [here|https://github.com/apache/cassandra/compare/trunk...blerer:14377-2.2]. CI results look good.

[~fcofdezc] Could you review? ;;;","02/Aug/18 07:10;fcofdezc;The patch LGTM.;;;","02/Aug/18 09:35;blerer;Thanks for the review.;;;","02/Aug/18 09:36;blerer;Committed in to 2.2 at 2bd733264ea0a30f2d62f62195a9bb7860904f83 and merged into 3.0, 3.11 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Speculative retry parsing breaks on non-english locale,CASSANDRA-14374,13151469,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,pauloricardomg,pauloricardomg,pauloricardomg,10/Apr/18 19:28,15/May/20 08:04,13/Jul/23 08:37,17/Apr/18 18:51,4.0,4.0-alpha1,,,,,,,,,0,,,,"I was getting the following error when running unit tests on my machine:
{code:none}
Error setting schema for test (query was: CREATE TABLE cql_test_keyspace.table_32 (a int, b int, c text, primary key (a, b)))
java.lang.RuntimeException: Error setting schema for test (query was: CREATE TABLE cql_test_keyspace.table_32 (a int, b int, c text, primary key (a, b)))
	at org.apache.cassandra.cql3.CQLTester.schemaChange(CQLTester.java:819)
	at org.apache.cassandra.cql3.CQLTester.createTable(CQLTester.java:632)
	at org.apache.cassandra.cql3.CQLTester.createTable(CQLTester.java:624)
	at org.apache.cassandra.cql3.validation.operations.DeleteTest.testDeleteWithNonoverlappingRange(DeleteTest.java:663)
Caused by: org.apache.cassandra.exceptions.ConfigurationException: Specified Speculative Retry Policy [99,00p] is not supported
	at org.apache.cassandra.service.reads.SpeculativeRetryPolicy.fromString(SpeculativeRetryPolicy.java:135)
	at org.apache.cassandra.schema.SchemaKeyspace.createTableParamsFromRow(SchemaKeyspace.java:1006)
	at org.apache.cassandra.schema.SchemaKeyspace.fetchTable(SchemaKeyspace.java:981)
	at org.apache.cassandra.schema.SchemaKeyspace.fetchTables(SchemaKeyspace.java:941)
	at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspace(SchemaKeyspace.java:900)
	at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspaces(SchemaKeyspace.java:1301)
	at org.apache.cassandra.schema.Schema.merge(Schema.java:608)
	at org.apache.cassandra.schema.MigrationManager.announce(MigrationManager.java:425)
	at org.apache.cassandra.schema.MigrationManager.announceNewTable(MigrationManager.java:239)
	at org.apache.cassandra.schema.MigrationManager.announceNewTable(MigrationManager.java:224)
	at org.apache.cassandra.schema.MigrationManager.announceNewTable(MigrationManager.java:204)
	at org.apache.cassandra.cql3.statements.CreateTableStatement.announceMigration(CreateTableStatement.java:88)
	at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.executeInternal(SchemaAlteringStatement.java:120)
	at org.apache.cassandra.cql3.CQLTester.schemaChange(CQLTester.java:814)
{code}
It turns out that my machine is configured with {{pt_BR}} locale, which uses comma instead of dot for decimal separator, so the speculative retry option parsing introduced by CASSANDRA-14293, which assumed {{en_US}} locale was not working.

To reproduce on Linux:
{code:none}
export LC_CTYPE=pt_BR.UTF-8
ant test -Dtest.name=""DeleteTest""
ant test -Dtest.name=""SpeculativeRetryParseTest""
{code}",,aleksey,burmanm,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/18 19:30;pauloricardomg;0001-Use-Locale.US-on-PercentileSpeculativeRetryPolicy.to.patch;https://issues.apache.org/jira/secure/attachment/12918444/0001-Use-Locale.US-on-PercentileSpeculativeRetryPolicy.to.patch","10/Apr/18 20:29;pauloricardomg;0002-Use-Locale.US-on-PercentileSpeculativeRetryPolicy.to.patch;https://issues.apache.org/jira/secure/attachment/12918453/0002-Use-Locale.US-on-PercentileSpeculativeRetryPolicy.to.patch","10/Apr/18 20:43;pauloricardomg;0003-Use-Locale.US-on-PercentileSpeculativeRetryPolicy.to.patch;https://issues.apache.org/jira/secure/attachment/12918458/0003-Use-Locale.US-on-PercentileSpeculativeRetryPolicy.to.patch","10/Apr/18 20:47;pauloricardomg;0004-Use-Locale.US-on-PercentileSpeculativeRetryPolicy.to.patch;https://issues.apache.org/jira/secure/attachment/12918460/0004-Use-Locale.US-on-PercentileSpeculativeRetryPolicy.to.patch",,,,,,,,,,4.0,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 17 18:51:47 UTC 2018,,,,,,,,,,,"0|i3sdwf:",9223372036854775807,4.0,,,,,,,,aleksey,,aleksey,,,Low,,,,,,,,,,,,,,,,,,,"10/Apr/18 19:33;pauloricardomg;Attached patch that forces {{US}} locale when generating {{PercentileSpeculativeRetryPolicy}} representation. Mind reviewing [~iamaleksey] or [~mkjellman] ?;;;","10/Apr/18 19:41;aleksey;[~pauloricardomg] Sure. Do you mind going one step further and changing that {{toString()}} to
{code}
return String.format(""%sp"", new DecimalFormat(""#.####"").format(percentile));
{code}
?

Because the previous patch introduced a minor annoying regression, in that 99p for example is being serialized as {{99.00p}} (instead of {{99p}}). And check it in pt_BR locale as well as en_US?;;;","10/Apr/18 20:31;pauloricardomg;{quote}Because the previous patch introduced a minor annoying regression, in that 99p for example is being serialized as 99.00p (instead of 99p). And check it in pt_BR locale as well as en_US?
{quote}
Even if we use a {{DateFormatter}} we still need to specify the US locale to ensure the dot decimal separator (and not comma) is used, so I changed the toString to:
{code:none}
return String.format(""%sp"", new DecimalFormat(""#.####"", new DecimalFormatSymbols(Locale.ENGLISH)).format(percentile));
{code}
I also updated the {{SpeculativeRetryParseTest}} to test round trip parsing with the Brazilian locale which uses comma as the decimal separator. Patch is attached.;;;","10/Apr/18 20:44;pauloricardomg;Oh you guys were faster with CASSANDRA-14352. :) Attached 0003-Use-Locale.US-on-PercentileSpeculativeRetryPolicy.to.patch rebasing on top of that.;;;","10/Apr/18 20:48;pauloricardomg;Attached 0004-Use-Locale.US-on-PercentileSpeculativeRetryPolicy.to.patch with typo fix, sorry for the spam.. (:;;;","13/Apr/18 12:41;aleksey;+1;;;","13/Apr/18 12:51;burmanm;Yep, patch works fine with Finnish locale.;;;","17/Apr/18 18:51;pauloricardomg;Committed as {{f165e72bf19e8d12457b8f569517012628513d24}} to trunk. Thanks for the review!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
data_file_directories config - update documentation in cassandra.yaml,CASSANDRA-14372,13151197,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,n.v.harikrishna,n.v.harikrishna,n.v.harikrishna,09/Apr/18 21:22,15/May/20 08:02,13/Jul/23 08:37,11/Apr/18 04:23,4.0,4.0-alpha1,,,,,Legacy/Documentation and Website,,,,0,,,,"If ""data_file_directories"" configuration is enabled with multiple directories, data is partitioned by token range so that data gets distributed evenly. But the current documentation says that ""Cassandra will spread data evenly across them, subject to the granularity of the configured compaction strategy"". Need to update this comment to reflect the correct behavior.",,jjirsa,n.v.harikrishna,pauloricardomg,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6696,,,,,,,,,,"10/Apr/18 20:30;n.v.harikrishna;14372-trunk.txt;https://issues.apache.org/jira/secure/attachment/12918454/14372-trunk.txt",,,,,,,,,,,,,1.0,n.v.harikrishna,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 11 04:23:18 UTC 2018,,,,,,,,,,,"0|i3sc87:",9223372036854775807,,,,,,,,,jjirsa,,jjirsa,,,Low,,3.2,,,,,,,,,,,,,,,,,"09/Apr/18 21:28;n.v.harikrishna;I can submit a patch for this. Can someone assign this ticket to me?;;;","10/Apr/18 20:30;n.v.harikrishna;Attached patch with the changes. Please review it.;;;","11/Apr/18 04:23;jjirsa;Thanks! Committed as 42827e6a6709c4ba031e0a137a3bab257f88b54f

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commit log replay failure for static columns with collections in clustering keys,CASSANDRA-14365,13150216,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,VincentWhite,VincentWhite,VincentWhite,05/Apr/18 05:05,15/May/20 08:54,13/Jul/23 08:37,10/Mar/20 08:08,2.2.17,3.0.21,3.11.7,4.0,4.0-alpha4,,Legacy/Core,,,,0,,,,"In the old storage engine, static cells with a collection as part of the clustering key fail to validate because a 0 byte collection (like in the cell name of a static cell) isn't valid.

To reproduce:

1.
{code:java}
CREATE TABLE test.x (
    id int,
    id2 frozen<map<text, text>>,
    st int static,
    PRIMARY KEY (id, id2)
);

INSERT INTO test.x (id, st) VALUES (1, 2);
{code}
2.
 Kill the cassandra process

3.
 Restart cassandra to replay the commitlog

Outcome:
{noformat}
ERROR [main] 2018-04-05 04:58:23,741 JVMStabilityInspector.java:99 - Exiting due to error while processing commit log during initialization.
org.apache.cassandra.db.commitlog.CommitLogReplayer$CommitLogReplayException: Unexpected error deserializing mutation; saved to /tmp/mutation3825739904516830950dat.  This may be caused by replaying a mutation against a table with the same name but incompatible schema.  Exception follows: org.apache.cassandra.serializers.MarshalException: Not enough bytes to read a set
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.handleReplayError(CommitLogReplayer.java:638) [main/:na]
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.replayMutation(CommitLogReplayer.java:565) [main/:na]
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.replaySyncSection(CommitLogReplayer.java:517) [main/:na]
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.recover(CommitLogReplayer.java:397) [main/:na]
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.recover(CommitLogReplayer.java:143) [main/:na]
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:181) [main/:na]
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:161) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:284) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:533) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:642) [main/:na]


{noformat}
I haven't investigated if there are other more subtle issues caused by these cells failing to validate other places in the code, but I believe the fix for this is to check for 0 byte length collections and accept them as valid as we do with other types.

I haven't had a chance for any extensive testing but this naive patch seems to have the desired affect. 


||Patch||
|[2.2 PoC|https://github.com/vincewhite/cassandra/commits/zero_length_collection]|
",,jjirsa,KurtG,mck,VincentWhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,VincentWhite,,,,,,,,,,,,Availability -> Unavailable,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 10 08:08:28 UTC 2020,,,,,,,,,,,"0|i3s673:",9223372036854775807,2.1.x,2.2.x,,,,,,,,,mck,,,Normal,,2.1.0,,,https://github.com/apache/cassandra/commit/ae326eed2aa8f9c761fc7a0a872ce8172fde2f0f,,,,,,,,,,,,,,"15/Nov/19 12:28;mck;||branch||circleci||asf jenkins tests||asf jenkins dtests||
|[cassandra-2.2_14365|https://github.com/apache/cassandra/compare/trunk...vincewhite:zero_length_collection]|[circleci|https://circleci.com/workflow-run/d500cc5f-1d87-4beb-815e-9931f8e84d95]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/29//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/29/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/703//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/703]|

[~VincentWhite], can you update the unit test on a cassandra-3.0 based branch as well…
you see can [here|https://github.com/thelastpickle/cassandra/commit/af2ca79b1001150d39a6b97cfadc0ce97f4e99b4] the api the tests use have changed a bit.;;;","28/Nov/19 22:20;mck;With new tests… (test against trunk also needed a rewrite bc {{`TableMetadata.Builder`}}

||branch||circleci||jenkins pipeline||
|[cassandra_2.2_14365|https://github.com/apache/cassandra/compare/cassandra-2.2...thelastpickle:mck/cassandra-2.2_14365]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-2.2_14365]|[!https://builds.apache.org/job/Cassandra-devbranch/40/badge/icon!|https://builds.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/40]|
|[cassandra_3.0_14365|https://github.com/apache/cassandra/compare/cassandra-3.0...thelastpickle:mck/cassandra-3.0_14365]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-3.0_14365]|[!https://builds.apache.org/job/Cassandra-devbranch/41/badge/icon!|https://builds.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/41]|
|[cassandra_3.11_14365|https://github.com/apache/cassandra/compare/cassandra-3.11...thelastpickle:mck/cassandra-3.11_14365]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-3.11_14365]|[!https://builds.apache.org/job/Cassandra-devbranch/42/badge/icon!|https://builds.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/42]|
|[trunk_14365|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/trunk_14365]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Ftrunk_14365]|[!https://builds.apache.org/job/Cassandra-devbranch/43/badge/icon!|https://builds.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/43]|;;;","10/Mar/20 08:08;mck;Committed as ae326eed2aa8f9c761fc7a0a872ce8172fde2f0f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bind to correct local address in 4.0 streaming,CASSANDRA-14362,13149864,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,Lerh Low,Lerh Low,Lerh Low,04/Apr/18 01:44,15/May/20 08:05,13/Jul/23 08:37,17/Apr/18 12:49,4.0,4.0-alpha1,,,,,,,,,0,,,,"I'm not sure if anybody has tried using {{trunk}} and starting an actual EC2 cluster, but with a 3 node setup that works on 3.11, it fails to work on {{trunk}}. I mistakenly stumbled into it while testing my own code changes. 

My setup is as follows:

broadcast_rpc_address: publicIP
broadcast_address: publicIP
listen_address: omitted. Ends up as privateIP. 

Works on 3.11 just fine. 

On {{trunk}} though, it never works. My node is never able to join the cluster:

{code}
Apr 03 05:57:06 ip-10-0-47-122 cassandra[13914]: INFO  [main] 2018-04-03 05:57:05,895 RangeStreamer.java:195 - Bootstrap: range (-128373781239966537,-122439194129870521] exists on 52.88.241.181:7000 for keyspace system_traces
Apr 03 05:57:06 ip-10-0-47-122 cassandra[13914]: INFO  [main] 2018-04-03 05:57:05,895 RangeStreamer.java:195 - Bootstrap: range (6968670424536541270,6973888347502882935] exists on 52.88.241.181:7000 for keyspace system_traces
Apr 03 05:57:42 ip-10-0-47-122 cassandra[13914]: WARN  [GossipTasks:1] 2018-04-03 05:57:42,298 FailureDetector.java:324 - Not marking nodes down due to local pause of 26215173446ns > 5000000000ns
Apr 03 05:57:53 ip-10-0-47-122 cassandra[13914]: WARN  [GossipTasks:1] 2018-04-03 05:57:53,035 FailureDetector.java:324 - Not marking nodes down due to local pause of 10736485907ns > 5000000000ns
Apr 03 05:58:30 ip-10-0-47-122 cassandra[13914]: WARN  [GossipTasks:1] 2018-04-03 05:58:30,790 Gossiper.java:814 - Gossip stage has 28 pending tasks; skipping status check (no nodes will be marked down)
Apr 03 05:58:33 ip-10-0-47-122 cassandra[13914]: WARN  [GossipTasks:1] 2018-04-03 05:58:33,060 Gossiper.java:814 - Gossip stage has 20 pending tasks; skipping status check (no nodes will be marked down)
Apr 03 06:04:33 ip-10-0-47-122 cassandra[13914]: WARN  [GossipTasks:1] 2018-04-03 06:04:33,826 FailureDetector.java:324 - Not marking nodes down due to local pause of 400790432954ns > 5000000000ns
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]: WARN  [GossipTasks:1] 2018-04-03 06:04:49,133 Gossiper.java:814 - Gossip stage has 2 pending tasks; skipping status check (no nodes will be marked down)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]: ERROR [StreamConnectionEstablisher:1] 2018-04-03 06:04:49,138 StreamSession.java:524 - [Stream #d4cd6420-3703-11e8-a6a5-e51ddc10cfe6] Streaming error occurred on session with peer 52.88.241.181:7000
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]: java.io.IOException: failed to connect to 52.88.241.181:7000 (STREAM) for streaming data
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.DefaultConnectionFactory.createConnection(DefaultConnectionFactory.java:98)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.DefaultConnectionFactory.createConnection(DefaultConnectionFactory.java:57)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.async.NettyStreamingMessageSender.createChannel(NettyStreamingMessageSender.java:183)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.async.NettyStreamingMessageSender.setupControlMessageChannel(NettyStreamingMessageSender.java:165)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.async.NettyStreamingMessageSender.sendMessage(NettyStreamingMessageSender.java:222)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.async.NettyStreamingMessageSender.initialize(NettyStreamingMessageSender.java:146)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.StreamSession.start(StreamSession.java:271)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.StreamCoordinator$StreamSessionConnector.run(StreamCoordinator.java:273)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at java.lang.Thread.run(Thread.java:748)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]: Caused by: io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Cannot assign requested address
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.unix.Errors.newIOException(Errors.java:117)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.unix.Socket.bind(Socket.java:266)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.epoll.AbstractEpollStreamChannel.doConnect(AbstractEpollStreamChannel.java:729)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.epoll.EpollSocketChannel.doConnect(EpollSocketChannel.java:184)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.connect(AbstractEpollStreamChannel.java:797)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1274)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:545)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:530)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.DefaultChannelPipeline.connect(DefaultChannelPipeline.java:999)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.AbstractChannel.connect(AbstractChannel.java:260)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.bootstrap.Bootstrap$3.run(Bootstrap.java:254)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:312)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         ... 1 common frames omitted
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]: INFO  [StreamConnectionEstablisher:1] 2018-04-03 06:04:49,140 StreamResultFuture.java:197 - [Stream #d4cd6420-3703-11e8-a6a5-e51ddc10cfe6] Session with 52.88.241.181:7000 is complete
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]: ERROR [StreamConnectionEstablisher:1] 2018-04-03 06:04:49,146 StreamSession.java:524 - [Stream #d4cd6420-3703-11e8-a6a5-e51ddc10cfe6] Streaming error occurred on session with peer 52.88.241.181:7000
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]: java.lang.RuntimeException: stream has been closed, cannot send Prepare SYN (3 requests,  0 files}
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.async.NettyStreamingMessageSender.sendMessage(NettyStreamingMessageSender.java:209)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.StreamSession.onInitializationComplete(StreamSession.java:495)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.StreamSession.start(StreamSession.java:272)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.StreamCoordinator$StreamSessionConnector.run(StreamCoordinator.java:273)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at java.lang.Thread.run(Thread.java:748)
{code}

Remote debugging it, it eventually seems to come to be due to trying to call {{socket.bind}} in {{doConnect}} in {{AbstractEpollStreamChannel}}. Local address is <publicIP>:0. I think port 0 is fine because it just means pick the next ephemeral port available, but there's no way of binding to the public IP. 

I made a commit https://github.com/apache/cassandra/commit/00222ddf0694f1c35c2bdd84fd7407174c3fc57a that seems to have fixed it so I can continue on with my purposes, and that is have {{StreamSession}} use listen address instead of broadcast address which may be public. In the event listen address is public I guess that means if it was able to bind to that interface to begin with then {{socket.bind}} should also work. 

Not sure if this is the right way though, and it seems to have been introduced by the Streaming rework to netty. https://issues.apache.org/jira/browse/CASSANDRA-12229. ",,djoshi,jasobrown,jeromatron,KurtG,Lerh Low,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,Lerh Low,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 19 00:07:38 UTC 2018,,,,,,,,,,,"0|i3s413:",9223372036854775807,4.0,,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"04/Apr/18 01:45;Lerh Low;Any thoughts on this [~jasobrown]...? :);;;","04/Apr/18 01:58;jasobrown;I'll take a look tomorrow, certainly looks like me :). Which snitch are you using?;;;","04/Apr/18 02:01;Lerh Low;I'm using EC2Snitch (mostly was just using bare bones to get a cluster working). If the commit I made seems like the appropriate fix to you (I may be missing something.... :/ ) then I'm happy to create a proper patch for it. 

(And feel free to LMK if you need any more details);;;","04/Apr/18 22:04;jasobrown;So it looks like when working on CASSANDRA-8457/CASSANDRA-12229, I introduced a regression of CASSANDRA-12673. The short of it is we no longer bind the local side of a outbound connection as of CASSANDRA-12673, yet I mistakenly brought that back (see [{{NettyFactory#createOutboundBootstrap()}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/async/NettyFactory.java#L337]).

I believe the most correct fix is just not setting the local address to bind to for outbound connections, as that's bascially what CASSANDRA-12673 does. Removing the local bind should resolve [~Lerh Low]'s error as well as eliminate the regression against CASSANDRA-12673. However, we should also incorporate [~Lerh Low]'s fix as well, to clarify the intent of which address we want to 'reference' (but not actually use) on the local side.
||14362||
|[branch|https://github.com/jasobrown/cassandra/tree/14362]|
|[utests & dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14362]|

tests are running now. 

[~aweisberg] do you mind reviewing?

UPDATE: dtests seem to be pretty unhappy. I'll investigate what's wrong and ping this ticket with updates.;;;","08/Apr/18 11:07;djoshi;[~jasobrown] / [~aweisberg] I think I know what the issue is. [~jasobrown] not only changed the {{OutboundConnectionIdentifier}} to prefer the local address of the node but also removed the binding to a specific local IP address during {{NettyFactory#createOutboundBootstrap}} setup. This had an unintended consequence. When you leave the {{Socket}}'s source IP (local address) unset, the kernel will choose one for you. This normally is not an issue. However, during the dtest run all nodes are local and rely on 127.0.0.x IPs. When the C* nodes try to setup the streaming connections, they get tripped up due to this change.

Say node1 (127.0.0.1) & node2 (127.0.0.2) are started up by the dtest (I know in reality its 3 nodes but for the sake of simplicity I'm limiting it to two nodes). node1 attempts to create an outbound connection for streaming with node2, the outbound socket should actually be {{127.0.0.1:ANY_PORT -> 127.0.0.2:INTERNODE_PORT}}. However, per this patch, we leave the source IP unbound. When the connection is established and node1 sends the {{FirstHandshakeMessage}}, it has a source IP set to set to an arbitrarily chose IP by the kernel - in this case it was {{127.0.0.2}} (when it should actually be {{127.0.0.1}}).

The receiving node (node2) gets this message {{InboundHandshakeHandler#handleStart}}, decodes it fine but when it goes to {{InboundHandshakeHandler#setupStreamingPipeline}}, it creates a {{StreamingInboundHandler}} with remote IP set to {{127.0.0.2}}. This messes up the streaming pipeline because now node2 is waiting on {{127.0.0.2}} (itself) for data. Hence we find it stuck in the bootstrap phase -
{noformat}
""Stream-Deserializer-127.0.0.2:7000-4004ef3b"" #133 daemon prio=5 os_prio=35 tid=0x00007ff58c601ca0 nid=0xdc03 waiting on condition [0x0000700007613000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at java.lang.Thread.sleep(Thread.java:340)
	at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
	at com.google.common.util.concurrent.Uninterruptibles.sleepUninterruptibly(Uninterruptibles.java:285)
	at org.apache.cassandra.streaming.async.StreamingInboundHandler$StreamDeserializingTask.run(StreamingInboundHandler.java:174)
	at java.lang.Thread.run(Thread.java:748)

""main"" #1 prio=5 os_prio=35 tid=0x00007ff58c706000 nid=0x2803 waiting on condition [0x0000700004e5e000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007a11a3f98> (a org.apache.cassandra.streaming.StreamResultFuture)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:497)
	at org.apache.cassandra.service.StorageService.bootstrap(StorageService.java:1519)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:949)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:651)
	- locked <0x00000007a40f80b8> (a org.apache.cassandra.service.StorageService)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:586)
	- locked <0x00000007a40f80b8> (a org.apache.cassandra.service.StorageService)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:367)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:590)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:679)
{noformat}
I also noted this error in the logs which further points to the wrong IP for the peer being used -
{noformat}
ERROR [Stream-Deserializer-127.0.0.2:53830-7b781083] 2018-04-06 13:19:25,863 StreamingInboundHandler.java:210 - [Stream channel: 7b781083] stream operation from 127.0.0.2:53830 failed
java.lang.IllegalArgumentException: Unknown peer requested: 127.0.0.2:7000
        at org.apache.cassandra.streaming.StreamCoordinator.getHostData(StreamCoordinator.java:242)
        at org.apache.cassandra.streaming.StreamCoordinator.getSessionById(StreamCoordinator.java:170)
        at org.apache.cassandra.streaming.StreamResultFuture.getSession(StreamResultFuture.java:237)
        at org.apache.cassandra.streaming.StreamManager.findSession(StreamManager.java:194)
        at org.apache.cassandra.streaming.StreamManager.findSession(StreamManager.java:185)
        at org.apache.cassandra.streaming.messages.IncomingStreamMessage$1.deserialize(IncomingStreamMessage.java:41)
        at org.apache.cassandra.streaming.messages.IncomingStreamMessage$1.deserialize(IncomingStreamMessage.java:36)
        at org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:55)
        at org.apache.cassandra.streaming.async.StreamingInboundHandler$StreamDeserializingTask.run(StreamingInboundHandler.java:177)
        at java.lang.Thread.run(Thread.java:748)
{noformat}
I added a bit of debugging code and turned {{TRACE}} logging on and confirmed that this was indeed the case.

Below is the output of node2's debug.log in the 3 node cluster that dtest run created. node1, 2 & 3 were assigned IPs {{127.0.0.1, 127.0.0.2, 127.0.0.3}} respectively. Note that node2's actual IP is never used as we've left it unbound so it seems the OS kernel prefers the IP that it thinks is closest to the remote IP so channel local & remote as the showing up as the same IP.
{noformat}
03:07 $ tail -F /var/folders/5f/_29kl4f524l7gp__02cxby_c0000gn/T/dtest-wu9pai7a/test/node2/logs/debug.log  | grep handshake
TRACE [MessagingService-NettyOutbound-Thread-4-1] 2018-04-08 03:07:55,360 OutboundHandshakeHandler.java:107 - starting handshake with peer 127.0.0.1:7000, msg = FirstHandshakeMessage - messaging version: 12, mode: MESSAGING, compress: false, channel local = /127.0.0.1:65020 remote = /127.0.0.1:7000
TRACE [MessagingService-NettyOutbound-Thread-4-3] 2018-04-08 03:09:21,427 OutboundHandshakeHandler.java:107 - starting handshake with peer 127.0.0.3:7000, msg = FirstHandshakeMessage - messaging version: 12, mode: MESSAGING, compress: false, channel local = /127.0.0.3:65091 remote = /127.0.0.3:7000
{noformat}
I only modified the trace log line in {{OutboundHandshakeHandler#channelActive}} as below -
{noformat}
        logger.trace(""starting handshake with peer {}, msg = {}, channel local = {} remote = {}"", connectionId.connectionAddress(),
                     msg, ctx.channel().localAddress(), ctx.channel().remoteAddress());
{noformat}

What is unclear to me is why is this an issue now if CASSANDRA-12673 left the local side unbound?;;;","17/Apr/18 12:49;jasobrown;[~Lerh Low]'s patch is correct and fixes a bug I introduced with CASSANDRA-12229. Further, there is something wrong with my handling of CASSANDRA-12673, but I don't want to hold up Lerh or anyone else looking at 4.0 in ec2. Thus, I've decided to commit Lerh's patch and will open a followup ticket to address the local address binding issue.

Committed as sha {{70d95359d2dca1c35f573776d11ed87bb9b4b441}}. Nice find and fix, Lehr.;;;","17/Apr/18 12:53;jasobrown;Opened CASSANDRA-14389 for the local address binding regression. Thanks, [~djoshi3] for also looking at this ticket.;;;","19/Apr/18 00:07;Lerh Low;It's not blocking me because I have that workaround, but thanks for looking into it so quickly, much appreciated :) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CREATE TABLE fails if there is a column called ""default"" with Cassandra 3.11.2",CASSANDRA-14359,13149384,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,adelapena,aklages,aklages,01/Apr/18 21:50,15/May/20 08:06,13/Jul/23 08:37,18/Apr/18 12:58,3.0.17,3.11.3,4.0,4.0-alpha1,,,Legacy/CQL,Legacy/Documentation and Website,,,0,,,,"My project is upgrading from Cassandra 2.1 to 3.11. We have a table whose column name is ""default"". The Cassandra 3.11.2 is rejecting it. I don't see ""default"" as a keyword in the CQL spec. 

To reproduce, try adding the following:
{code:java}
CREATE TABLE simple (
    simplekey text PRIMARY KEY,
    default text // THIS IS REJECTED
);
{code}
I get this error:
{code:java}
SyntaxException: line 3:4 mismatched input 'default' expecting ')' (...    simplekey text PRIMARY KEY,    [default]...)
{code}",This is using Cassandra 3.11.2. This syntax was accepted in 2.1.20.,adelapena,aklages,blerer,jeromatron,marcuse,ronblechman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,adelapena,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 24 09:09:11 UTC 2018,,,,,,,,,,,"0|i3s13j:",9223372036854775807,,,,,,,,,blerer,,blerer,,,Low,,,,,,,,,,,,,,,,,,,"02/Apr/18 21:57;aklages;Even though ""default"" is not in the CQL spec, I was able to create the table if I surrounded default with double-quotes. So ""default"" is an undocumented keyword. So for this JIRA, the documentation just needs to be updated to include ""default"".;;;","06/Apr/18 13:35;adelapena;{{DEFAULT}} keyword was added by CASSANDRA-11424 and included in [{{ReservedKeywords}}|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/cql3/ReservedKeywords.java#L89] by CASSANDRA-14205.

That keyword and some others are indeed missed in the CQL documentation:
* cassandra-3.0: {{IS}}, {{MATERIALIZED}}, {{VIEW}}
* cassandra-3.11 and trunk: {{IS}}, {{CAST}}, {{DEFAULT}}, {{DURATION}}, {{GROUP}}, {{LIKE}}, {{MATERIALIZED}}, {{MBEAN}}, {{MBEANS}}, {{PER}}, {{PARTITION}}, {{UNSET}}, {{VIEW}}

This patch adds them to the {{CQL.textile}}:
||[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...adelapena:14359-3.0]||[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...adelapena:14359-3.11]||[trunk|https://github.com/apache/cassandra/compare/trunk...adelapena:14359-trunk]||;;;","18/Apr/18 07:45;blerer;Thanks for the patch. +1;;;","18/Apr/18 12:58;adelapena;Thanks for reviewing, committed as [9b5ba6ca51e6e35116fbac715cb0e1d3b7eb94f3|https://github.com/apache/cassandra/commit/9b5ba6ca51e6e35116fbac715cb0e1d3b7eb94f3].;;;","24/Apr/18 06:43;marcuse;seems this wasn't merged properly from 3.0 -> 3.11, conflicts in docs/cql3/CQL.textile - I merged 3.0 in to 3.11 with {{-s ours}} could you verify that that is correct?;;;","24/Apr/18 09:09;adelapena;[~krummas] it seems correct to me, thanks for fixing the merge;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partitioned outbound internode TCP connections can occur when nodes restart,CASSANDRA-14358,13149263,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jolynch,jolynch,jolynch,30/Mar/18 23:51,15/May/20 08:05,13/Jul/23 08:37,01/Nov/18 22:37,4.0,4.0-alpha1,,,,,Legacy/Streaming and Messaging,,,,0,4.0-feature-freeze-review-requested,,,"edit summary: This primarily impacts networks with stateful firewalls such as AWS. I'm working on a proper patch for trunk but unfortunately it relies on the Netty refactor in 4.0 so it will be hard to backport to previous versions. A workaround for earlier versions is to set the {{net.ipv4.tcp_retries2}} sysctl to ~5. This can be done with the following:
{code:java}
$ cat /etc/sysctl.d/20-cassandra-tuning.conf
net.ipv4.tcp_retries2=5
$ # Reload all sysctls
$ sysctl --system{code}
Original Bug Report:

I've been trying to debug nodes not being able to see each other during longer (~5 minute+) Cassandra restarts in 3.0.x and 2.1.x which can contribute to {{UnavailableExceptions}} during rolling restarts of 3.0.x and 2.1.x clusters for us. I think I finally have a lead. It appears that prior to trunk (with the awesome Netty refactor) we do not set socket connect timeouts on SSL connections (in 2.1.x, 3.0.x, or 3.11.x) nor do we set {{SO_TIMEOUT}} as far as I can tell on outbound connections either. I believe that this means that we could potentially block forever on {{connect}} or {{recv}} syscalls, and we could block forever on the SSL Handshake as well. I think that the OS will protect us somewhat (and that may be what's causing the eventual timeout) but I think that given the right network conditions our {{OutboundTCPConnection}} threads can just be stuck never making any progress until the OS intervenes.

I have attached some logs of such a network partition during a rolling restart where an old node in the cluster has a completely foobarred {{OutboundTcpConnection}} for ~10 minutes before finally getting a {{java.net.SocketException: Connection timed out (Write failed)}} and immediately successfully reconnecting. I conclude that the old node is the problem because the new node (the one that restarted) is sending ECHOs to the old node, and the old node is sending ECHOs and REQUEST_RESPONSES to the new node's ECHOs, but the new node is never getting the ECHO's. This appears, to me, to indicate that the old node's {{OutboundTcpConnection}} thread is just stuck and can't make any forward progress. By the time we could notice this and slap TRACE logging on, the only thing we see is ~10 minutes later a {{SocketException}} inside {{writeConnected}}'s flush and an immediate recovery. It is interesting to me that the exception happens in {{writeConnected}} and it's a _connection timeout_ (and since we see {{Write failure}} I believe that this can't be a connection reset), because my understanding is that we should have a fully handshaked SSL connection at that point in the code.

Current theory:
 # ""New"" node restarts,  ""Old"" node calls [newSocket|https://github.com/apache/cassandra/blob/6f30677b28dcbf82bcd0a291f3294ddf87dafaac/src/java/org/apache/cassandra/net/OutboundTcpConnection.java#L433]
 # Old node starts [creating a new|https://github.com/apache/cassandra/blob/6f30677b28dcbf82bcd0a291f3294ddf87dafaac/src/java/org/apache/cassandra/net/OutboundTcpConnectionPool.java#L141] SSL socket 
 # SSLSocket calls [createSocket|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/security/SSLFactory.java#L98], which conveniently calls connect with a default timeout of ""forever"". We could hang here forever until the OS kills us.
 # If we continue, we get to [writeConnected|https://github.com/apache/cassandra/blob/6f30677b28dcbf82bcd0a291f3294ddf87dafaac/src/java/org/apache/cassandra/net/OutboundTcpConnection.java#L263] which eventually calls [flush|https://github.com/apache/cassandra/blob/6f30677b28dcbf82bcd0a291f3294ddf87dafaac/src/java/org/apache/cassandra/net/OutboundTcpConnection.java#L341] on the output stream and also can hang forever. I think the probability is especially high when a node is restarting and is overwhelmed with SSL handshakes and such.

I don't fully understand the attached traceback as it appears we are getting a {{Connection Timeout}} from a {{send}} failure (my understanding is you can only get a connection timeout prior to a send), but I think it's reasonable that we have a timeout configuration issue. I'd like to try to make Cassandra robust to networking issues like this via maybe:
 # Change the {{SSLSocket}} {{getSocket}} methods to provide connection timeouts of 2s (equivalent to trunk's [timeout|https://github.com/apache/cassandra/blob/11496039fb18bb45407246602e31740c56d28157/src/java/org/apache/cassandra/net/async/NettyFactory.java#L329])
 # Appropriately set recv timeouts via {{SO_TIMEOUT}}, maybe something like 2 minutes (in old versions via [setSoTimeout|https://docs.oracle.com/javase/8/docs/api/java/net/Socket.html#setSoTimeout-int-], in trunk via [SO_TIMEOUT|http://netty.io/4.0/api/io/netty/channel/ChannelOption.html#SO_TIMEOUT]
 # Since we can't set send timeouts afaik (thanks java) maybe we can have some kind of watchdog that ensures OutboundTcpConnection is making progress in its queue and if it doesn't make any progress for ~30s-1m, forces a disconnect.

If anyone has insight or suggestions, I'd be grateful. I am going to rule out if this is keepalive duration by setting tcp_keepalive_probes to like 1 and maybe tcp_retries2 to like 8 get more information about the state of the tcp connections the next time this happens. It's a very rare bug and when it does happen I only have 10 minutes to jump on the nodes and fix it before it fixes itself so I'll do my best.","Cassandra 2.1.19 (also reproduced on 3.0.15), running with {{internode_encryption: all}} and the EC2 multi region snitch on Linux 4.13 within the same AWS region. Smallest cluster I've seen the problem on is 12 nodes, reproduces more reliably on 40+ and 300 node clusters consistently reproduce on at least one node in the cluster.

So all the connections are SSL and we're connecting on the internal ip addresses (not the public endpoint ones).

Potentially relevant sysctls:
{noformat}
/proc/sys/net/ipv4/tcp_syn_retries = 2
/proc/sys/net/ipv4/tcp_synack_retries = 5
/proc/sys/net/ipv4/tcp_keepalive_time = 7200
/proc/sys/net/ipv4/tcp_keepalive_probes = 9
/proc/sys/net/ipv4/tcp_keepalive_intvl = 75
/proc/sys/net/ipv4/tcp_retries2 = 15
{noformat}",alienth,aweisberg,benedict,dikanggu,djoshi,jasobrown,jasonstack,jay.zhuang,jeromatron,jjirsa,jolynch,khuizhang,kwho,mbyrd,pauloricardomg,rha,snazy,sumanth.pasupuleti,vinaykumarcse,weideng,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14424,,,,,,CASSANDRA-14818,CASSANDRA-14001,,,,,,,,,,,"30/Mar/18 23:44;jolynch;10 Minute Partition.pdf;https://issues.apache.org/jira/secure/attachment/12917074/10+Minute+Partition.pdf",,,,,,,,,,,,,1.0,jolynch,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 06 19:16:36 UTC 2018,,,,,,,,,,,"0|i3s0cn:",9223372036854775807,2.1.19,3.0.15,,,,,,,aweisberg,,aweisberg,,,Normal,,,,,,,,,,,,,,,,,,,"30/Mar/18 23:59;jolynch;It's also worth noting that the non ssl connections have the same problem, it's just unlikely I think that the destination server get's as overloaded and drops a handshake.;;;","02/Apr/18 09:49;djoshi;[~jolynch], I think what you're experiencing is a classic slow consumer issue. The OS will likely buffer the socket but it looks like once this queue is full, you will see it stuck on {{java.net.SocketOutputStream.socketWrite0}}. On Linux, TCP will continue trying to retransmit your packets for roughly 15 minutes before it finally gives up. This is controlled via {{tcp_retries2 (retransmission timeout)}}. You can tune it down to be closer to 100s if that is more desirable to you (See: RFC 1122).

 ;;;","02/Apr/18 17:07;jolynch;[~djoshi3] yea I think that you're probably right, but I think that this might not be a slow consumer so much as a never consumer. Specifically if there is a stateful firewall (e.g. security groups or vpc) in the way, the network could absolutely blackhole packets on a connection that's been reset. My plan for this week is I'm going to try to catch this happening and run the following analysis:
 # Get a netstat view of established connections and their send buffers on both sides
 # Slap a tcpdump on both sides to see if {{RESETS}} from the restarted node are even getting to the old node (VPC might be swallowing them)
 # If #1 or #2 confirm the theory that we're just in a stuck tcp connection, I will try tuning {{tcp_retries2}} down to ~5, setting {{SO_SNDBUF}} or {{wmem_default}} down to a more reasonable number (I think right now it's like 16 megs) to see if that fixes the issue

For what it's worth, Cassandra is _really_ intolerant of these kinds of network partitions. For example if I tell a stateful firewall to just drop all packets on one of Cassandra's internode connections with {{iptables -A OUPTUT -p tcp -d <ip1> --sport <ip1 local port> --dport <ssl port> -j DROP}}, Cassandra continues sending packets out on that {{OutboundTcpConnection}} until the operating system says ""hey that's probably not going to work"" 15 minutes later. Unfortunately we have two TCP connections so I can't just do a simple heartbeat mechanism.;;;","02/Apr/18 17:15;jjirsa;Suspect you'll see a single byte in the sendq on the instance that didnt bounce. 

 ;;;","02/Apr/18 18:14;djoshi;[~jolynch] I understand the problem that you're describing but this is TCP specific and not Cassandra specific. If there is packet loss or slow consumer or no consumer but the connection is established, all applications will see this issue. The problem here is that the thread is stuck on making progress due to Java Sockets implementation specifically the {{write0}} call. In case of Netty or Java NIO, you simply would ignore the socket and continue processing other ""ready"" sockets allowing Cassandra's threads to make progress. I suspect you will not see this issue in trunk. Have you given that a try?;;;","02/Apr/18 18:52;jolynch;[~djoshi3] I agree that the issue is probably a bad TCP level connection, but also I think it's a  bug that Cassandra blocks forever never making any progress and potentially causing an outage for users (until the OS kills the connection); we're supposed to be highly available right ;). In non JVM languages this would be as simple (I think) as setting {{SO_SNDTIMEO}} to a reasonable number like 30s, but I don't see how to do it without JNI in Cassandra 2.1.x/3.0.x/3.11.x. If this is the issue (I think we still need more data), I also think recommending users tune {{tcp_retries2}} to 8, 5 or even 3 would be a reasonable workaround if it's fixed in trunk.

I haven't tried in trunk (tbh we can't even build/deploy trunk right now), but unfortunately I think it would be vulnerable to same kind of issue since afaict {{[NettyFactory|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/async/NettyFactory.java#L358-L377]}} doesn't add any kind of {{[WriteTimeoutHandler|https://netty.io/4.0/api/io/netty/handler/timeout/WriteTimeoutHandler.html]}} or {{[ReadTimeoutHandler|https://netty.io/4.0/api/io/netty/handler/timeout/ReadTimeoutHandler.html]}} which will throw an exception (I believe) if the write/read on the socket doesn't complete in a reasonable amount of time. Admittedly I'm very new to the netty refactor in 4.0 so I might be missing the part where we set a bunch of timeouts somewhere.

I don't want to invest too much time testing trunk until I confirm the type of partition that is actually happening, but if our theory is correct it should be easy to reproduce on trunk with the above iptables rule (if trunk handles it then the tcp connection should get thrown out after ~30s or something reasonable and we re-connect).;;;","02/Apr/18 21:18;aweisberg;We can implement timeouts with a watchdog and wake up the threads by closing the socket. This will work even if they are blocked on the various socket methods.

This sounds like a real implementation hole to me. It means MTTR for any instance where the kernel doesn't gracefully clean up connections is as long as whatever the timeout is.

Even the Netty implementation is vulnerable here because it won't attempt the reconnect for a long time. The thread won't be blocked, but that isn't the issue here the issue is that communication itself is blocked until the faulty connection is replaced.;;;","02/Apr/18 21:33;pauloricardomg;Did you try reproducing this issue with CASSANDRA-9630 in? This has been committed relatively recently and it used to cause hanging connections on 2.1/3.X.;;;","02/Apr/18 22:12;jolynch;[~aweisberg] if the issue is what I think it is (and I definitely need more supporting evidence), I think we can fix it in trunk with a netty {{WriteTimeoutHandler}}/ {{ReadTimeoutHandler}} which will throw away the tcp connection if recv/write take too long. I think it's reasonable to say in earlier versions we are not going to patch it past connection and recv timeouts and just recommend {{/proc/sys/net/ipv4/tcp_retries2 = 5}}. I tested the sysctl and it leads to a much shorter partition with the iptables test, like 20s instead of 10 minutes. I'm testing it out on a large cluster with a rolling restart soon.

[~pauloricardomg] I believe that CASSANDRA-9630 is a separate issue (although +1 to a 2.1.x backport there since it's so simple and 2.1.x is actually still used places) because in this case I believe we have an established connection that is blackholing. I'll double check this next time I run the test and ensure that the tcp connection state is {{ESTABLISHED}}.;;;","03/Apr/18 03:05;djoshi;[~aweisberg] [~jolynch]

The underlying issue is that when a node restarts and begins listening, the peers in the cluster cause a thundering herd overwhelming the node. In 3.0 / 2.1 we accept the incoming connection and read some bytes off the socket on the same thread. This probably causes the connections to build up in the listen queue. Using a threadpool and separating the accept from rest of the code would make this better. 

If the peers kill their connections and restart them with a shorter timeout, the restarting node will get further overwhelmed. I looked at trunk and I don't think we will see the same issue crop up as we use a separate event loop group for accepting connections. Netty internally uses a separate IO thread for the channel pipeline and the request is dispatched using a separate threadpool. Unless you block the Netty accept thread or the IO thread, these symptoms won't surface.;;;","03/Apr/18 13:38;jasobrown;For trunk, we already use {{IdleStateHandler}} [in the outbound pipeline|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/async/OutboundHandshakeHandler.java#L196] for the ""not making progress sending data"" case. Take a look at {{MessageOutHandler#userEventTriggered}}. iirc, {{WriteTimeoutHandler}} / {{ReadTimeoutHandler}} does not fit our use case for messaging; I'd have to look again to page it back in.

Further, [~jolynch], I thought your problem was in pre-4.0? There is a problem (or, at least, non-optimization) with {{MessagingService.SocketThread}} where the single-threaded accept thread handles receiving the first message pf the c* internode messaging protocol, which may require executing the TLS handshake before that can be received (if using internode TLS). Thus a single new establishing connection may block the accept thread for 500-1000 ms for a cross-datacenter connection. Even worse, if you are using the {{Ec2MultiregionSnitch}}, all connections for the local region will be torn down and built anew as we switch from the public IP to the region-local private IP - more traffic on the accept thread. So one slow connection interaction screws the whole accept thread.

I have an internal patch for 3.0 which, when a new socket is accepted, moves all processing (internode protocol handshake, TLS handshake) to a separate thread (via an executor group). So, if one connection runs into a problem, the other incoming connections are not held up. I've been reticent about OSS'ing this late in the 3.0/3.11 branches, even though it's not that clever or invasive. Let me know if you'd be interested in giving it shot and I can post it. If it's useful/helpful we can consider committing it, as well.;;;","03/Apr/18 14:42;aweisberg;A node restart that is graceful (kernel closes connections) is different from power failure or network black holing packets to a host. In the latter case what in Cassandra causes the server to notice a TCP connection is no longer responding and attempt to recreate it so it can connect to the new version of the host?;;;","03/Apr/18 18:00;jolynch;[~djoshi3] We send RSTs on listen overflows via the {{tcp_abort_on_overflow}} sysctl, so I don't think that this is a simple listen overflow. You're absolutely right that nodes coming up can get overwhelmed in 2.1/3.0/3.11, and I think it's really great that trunk fixes that; but I don't think this is the bug I've identified here. In this case a Cassandra node that _didn't_ restart continues trying to send on a dead connection forever until the kernel steps in and cuts off the connection.

 [~jasobrown]
{quote}For trunk, we already use IdleStateHandler in the outbound pipeline for the ""not making progress sending data"" case. Take a look at MessageOutHandler#userEventTriggered. iirc, WriteTimeoutHandler / ReadTimeoutHandler does not fit our use case for messaging; I'd have to look again to page it back in.
{quote}
I am definitely not a netty expert, but I think that {{IdleStateHandler}} needs to be paired with a {{WriteTimeoutHandler}} or {{ReadTimeoutHandler}} in order to effectively identify such blackhole partitions. The app is sending data on the socket, it's just never going anywhere. My understanding is that {{IdleStateHandler}} just sends data if there is none.
{quote}Further, Joseph Lynch, I thought your problem was in pre-4.0? There is a problem (or, at least, non-optimization) with MessagingService.SocketThread where the single-threaded accept thread handles receiving the first message pf the c* internode messaging protocol, which may require executing the TLS handshake before that can be received (if using internode TLS). Thus a single new establishing connection may block the accept thread for 500-1000 ms for a cross-datacenter connection. Even worse, if you are using the Ec2MultiregionSnitch, all connections for the local region will be torn down and built anew as we switch from the public IP to the region-local private IP - more traffic on the accept thread. So one slow connection interaction screws the whole accept thread.
{quote}
Yes, our problem is with pre-4.0, but I'm 99% sure we can mitigate this by setting {{tcp_retries2}} to 3 or 5 to tell the OS to cover over the app bug. If Cassandra trunk is resilient to the (rare) bug I think the sysctl is a reasonable workaround for earlier versions, what do you think? It's also worth noting that this kind of partition probably only occurs frequently in stateful networks that swallow RSTs to unknown flows such as AWS VPC (although as [~aweisberg] points out, power failure could manifest similarly).
{quote}I have an internal patch for 3.0 which, when a new socket is accepted, moves all processing (internode protocol handshake, TLS handshake) to a separate thread (via an executor group). So, if one connection runs into a problem, the other incoming connections are not held up. I've been reticent about OSS'ing this late in the 3.0/3.11 branches, even though it's not that clever or invasive. Let me know if you'd be interested in giving it shot and I can post it. If it's useful/helpful we can consider committing it, as well.
{quote}
I don't think that would fix this particular bug, but it is awesome and fixes other issues. I volunteer to test it and review it if that helps get it merged to 3.0/3.11.;;;","03/Apr/18 18:01;djoshi;[~aweisberg] For cases where you have power failure (or process crashes or even restarts without properly closing its connections), the Socket will be left in a ""half-open"" state. Per TCP's spec, it will retry transmitting the packet a number of times. Each unsuccessful attempt will cause an exponential back off. Ultimately it will give up and send a RST packet. All this while your application will continue writing to the Kernel's socket buffer and will not know that the other end is dead / unresponsive. This is per TCP's design. If you want your applications to timeout faster or detect dead peers quicker you can tune TCP's parameters per your requirements.;;;","04/Apr/18 00:24;jolynch;For what it's worth we've opened an AWS ticket to find out if this is expected behaviour in VPCs (that the network blackholes packets on unknown flows rather than resetting them). I have a feeling that answer will be ""yea that's how stateful firewalls work we can't keep flows forever"", in which case Cassandra (probably) should be resilient to it I think.

[~djoshi3] I think you've got it now. I'm proposing that we tune Cassandra's socket options (opt-in of course like most socket options we set) to tune TCP's parameters per Cassandra's requirement of high availability. In the case of blocking socket the way that I'm familiar with are connect timeouts ([unix|http://man7.org/linux/man-pages/man2/connect.2.html], [java|https://docs.oracle.com/javase/8/docs/api/java/net/Socket.html#connect-java.net.SocketAddress-int-]), receive timeouts ([SO_RCVTIMEO|https://linux.die.net/man/7/socket], [java|https://docs.oracle.com/javase/8/docs/api/java/net/Socket.html#setSoTimeout-int-]) and send timeouts which Java doesn't expose afaict ([SO_SNDTIMEO|https://linux.die.net/man/7/socket]). For {{O_NONBLOCK}} sockets I think typically the timeout is handled at the event loop level (e.g. I think that's what {{WriteTimeoutHandler}} and {{ReadTimeoutHandler}} do in Netty).

I still need to test this thoroughly, but I believe asking users to set {{/proc/sys/net/ipv4/tcp_retries2 = 5}} is a reasonable workaround. I was able to simulate pretty easily the exact error in the production logs by doing the following:
{noformat}
$ sudo lsof -p $(pgrep -f Cassandra) -n | grep OTHER_IP | grep STORAGE_PORT
java    5277 cassandra  64u     IPv4             113860      0t0        TCP LOCAL_IP:38665->OTHER_IP:STORAGE_PORT (ESTABLISHED)
java    5277 cassandra  69u     IPv4             114797      0t0        TCP LOCAL_IP:STORAGE_PORT->OTHER_IP:54875 (ESTABLISHED)

$ sudo iptables -A OUTPUT -p tcp -d OTHER_IP --sport 38665 --dport STORAGE_PORT -j DROP

$ cqlsh
cqlsh> CONSISTENCY ALL
Consistency level set to ALL.
cqlsh> select * from test_ks.test_cf WHERE key = 'key_that_lives_on_OTHER_IP';
OperationTimedOut: errors={}, last_host=<something>
... timeouts happen for next ~15 minutes

$ tail -20 system.log 
TRACE [MessagingService-Outgoing-/OTHER_IP] 2018-04-04 00:13:05,009 OutboundTcpConnection.java:365 - Socket to /OTHER_IP closed
DEBUG [MessagingService-Outgoing-/OTHER_IP] 2018-04-04 00:13:05,010 OutboundTcpConnection.java:303 - error writing to /OTHER_IP
java.net.SocketException: Connection timed out (Write failed)
	at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_152]
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111) ~[na:1.8.0_152]
	at java.net.SocketOutputStream.write(SocketOutputStream.java:155) ~[na:1.8.0_152]
	at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_152]
	at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_152]
	at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:886) ~[na:1.8.0_152]
	at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:857) ~[na:1.8.0_152]
	at sun.security.ssl.AppOutputStream.write(AppOutputStream.java:123) ~[na:1.8.0_152]
	at net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:205) ~[lz4-1.2.0.jar:na]
	at net.jpountz.lz4.LZ4BlockOutputStream.flush(LZ4BlockOutputStream.java:222) ~[lz4-1.2.0.jar:na]
	at org.apache.cassandra.io.util.DataOutputStreamPlus.flush(DataOutputStreamPlus.java:55) ~[cassandra-2.1.19.jar:2.1.19]
	at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:294) [cassandra-2.1.19.jar:2.1.19]
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:222) [cassandra-2.1.19.jar:2.1.19]
DEBUG [MessagingService-Outgoing-/OTHER_IP] 2018-04-04 00:13:49,867 OutboundTcpConnection.java:380 - attempting to connect to /OTHER_IP
INFO  [HANDSHAKE-/OTHER_IP] 2018-04-04 00:13:49,916 OutboundTcpConnection.java:496 - Handshaking version with /OTHER_IP
TRACE [MessagingService-Outgoing-/OTHER_IP] 2018-04-04 00:13:49,960 OutboundTcpConnection.java:453 - Upgrading OutputStream to be compressed

$ cqlsh
cqlsh> CONSISTENCY ALL
Consistency level set to ALL.
cqlsh> select * from test_ks.test_cf WHERE key = 'key_that_lives_on_OTHER_IP';
... results come back and everything is happy

$ sudo lsof -p $(pgrep -f Cassandra) -n | grep OTHER_IP | grep STORAGE_PORT
java    5277 cassandra  134u     IPv4             113860      0t0        TCP LOCAL_IP:33417->OTHER_IP:STORAGE_PORT (ESTABLISHED)
java    5277 cassandra   69u     IPv4             114797      0t0        TCP LOCAL_IP:STORAGE_PORT->OTHER_IP:54875 (ESTABLISHED)

{noformat}
This simulates ""blackholing"" of the {{OutboundTcpConnection}}. With the default linux value of {{/proc/sys/net/ipv4/tcp_retries2 = 15}} this takes ~15 minutes to resolve (the operating system eventually saves Cassandra). With the changed value of ~3, the partition only lasts ~15 seconds. If Cassandra trunk set (configurable) read and write timeouts we could control how long it takes to recover from such a partition rather than relying on the OS.;;;","05/Apr/18 20:19;jolynch;A quick update, AWS has confirmed that this type of half open partition is very much possible when using VPC/security groups as they do their own [connection tracking|https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html#security-group-connection-tracking]. If the sg doesn't know about the flow because the ""timeout"" (which is very non-specific as to how long it is) occurs after close, the sg will blackhole packets. We're following up to see if they could have the more useful behavior of sending resets instead of dropping, but I imagine they'll answer that it's for security reasons.

I'm going to spend time seeing if the proposed workaround of setting {{/proc/sys/net/ipv4/tcp_retries2 = 5 }} fixes the issue satisfactorily, and if so I think that's a reasonable workaround for pre-netty, and post-netty we can use the reproduction steps above to see if Cassandra is now resilient to this kind of half-open partition. [~aweisberg] [~jasobrown] [~djoshi3] what do you guys think?;;;","09/Apr/18 23:15;jolynch;Good news, Linux has support for a [socket option|https://patchwork.ozlabs.org/patch/62889/] since 2.6.37 called {{TCP_USER_TIMEOUT}} which implements [RFC5482|https://tools.ietf.org/html/rfc5482] and does exactly what we want (ensures HA on a kept alive TCP connection). Netty also supports [setting this option|https://github.com/netty/netty/issues/4174] for epoll sockets as of 4.0.31, so we can set that on our internode TCP connections and get really high availability networking without requiring OS tuning on Linux. The {{man tcp}} entry is educational:
{noformat}
       TCP_USER_TIMEOUT (since Linux 2.6.37)
              This  option takes an unsigned int as an argument.  When the value is greater than 0, it specifies the maximum amount of time in milliseconds that trans‐
              mitted data may remain unacknowledged before TCP will forcibly close the corresponding connection and return ETIMEDOUT to the application.  If the option
              value is specified as 0, TCP will to use the system default.

              Increasing  user  timeouts allows a TCP connection to survive extended periods without end-to-end connectivity.  Decreasing user timeouts allows applica‐
              tions to ""fail fast"", if so desired.  Otherwise, failure may take up to 20 minutes with the current system defaults in a normal WAN environment.

              This option can be set during any state of a TCP connection, but is effective only during the synchronized states  of  a  connection  (ESTABLISHED,  FIN-
              WAIT-1, FIN-WAIT-2, CLOSE-WAIT, CLOSING, and LAST-ACK).  Moreover, when used with the TCP keepalive (SO_KEEPALIVE) option, TCP_USER_TIMEOUT will override
              keepalive to determine when to close a connection due to keepalive failure.

              The option has no effect on when TCP retransmits a packet, nor when a keepalive probe is sent.

              This option, like many others, will be inherited by the socket returned by accept(2), if it was set on the listening socket.

              Further details on the user timeout feature can be found in RFC 793 and RFC 5482 (""TCP User Timeout Option"").
{noformat}
I've started working on a [trunk patch|https://github.com/jolynch/cassandra/tree/CASSANDRA-14358] which will fix this on any Linux based deployment via the {{TCP_USER_TIMEOUT}} socket option. 

I've been testing with the reproduction steps I listed above using iptables and without my patch Cassandra waits until the OS kills the connection in ~15-20 minutes, and with my patch it kills it after ~10s.;;;","09/Apr/18 23:27;aweisberg;Nice find! The numbers you picked seem pretty aggressive. I was expecting in the 30-60 second range before giving up. If the network conditions are unreliable but working you might drop connections and then not be able to recreate them quickly due to packet loss. Granted things are probably mostly unavailable anyways in that scenario, but you don't want them looping too tightly trying to recreate connections.

The last question is if this needs to be a hot property or not.;;;","10/Apr/18 20:54;jolynch;Yea, I am interested as to what's the right default for this setting. For normal TCP connections I think it would be reasonable to put this very low (like close to the TCP connect timeout of 2s we use right now), but for SSL ... losing those SSL handshakes is somewhat of a bummer if it's just a temporary switch failure + OSPF convergence or something. What do you think about being conservative (maybe like 30s) for SSL and we'll make it a hot property in addition to the yaml configuration?

I've been testing this option on Linux 4.4 and 4.13 with [a minimal repro|https://gist.github.com/jolynch/90033c2b10ab8280859c8cfe352503cd] and it appears that the option works well if set to a small number (e.g. 5, 10, 20s), but it seems to take about 2x as long for large settings and (I need to do further testing) on Linux 4.4 any setting greater than 30s appears to just default to the system behavior (on 4.13 it is 2x the timeout, but not the system default). So if we set it to 30s we'd get a 60s timeout in most modern Linux's, and if it's not effective we'll just get the system default.;;;","10/Apr/18 23:20;aweisberg;30 seconds (effectively 60) and a hot prop sounds excellent.;;;","04/May/18 03:34;alienth;[~jolynch] One thing slightly interesting here that I found when reproducing. I can confirm that the traffic is retransmitting from the non-restarted node on the blackholed connection, yet the socket is in `CLOSE_WAIT` rather than `ESTABLISHED`. This would indicate that the non-restarted node got the FIN, but AWS blackholed the FIN,ACK. To me that suggests that the flow tracking was only lost in one direction. I saw this both times I was able to reproduce it, so I doubt it's a fluke of timing.;;;","04/May/18 20:40;alienth;Reproduced this behaviour several times now,and in all cases the socket that cassandra is trying to Outbound on has been in stuck in `CLOSE_WAIT`.

I think that suggests that the more common case might be where AWS stops tracking the flow for a FINd connection, and an ACK being dropped somewhere resulting in a node not fully closing the connection?

Note that I'm seeing this on cassandra 3.11.2, which has the patches from CASSANDRA-9630

{{tcp    CLOSE-WAIT 1      322683 10.0.161.40:18254              10.0.109.39:7000                timer:(on,1min54sec,13) uid:115 ino:5837280 sk:1a1 -->}};;;","04/May/18 23:22;alienth;Captured the socket state for before, during, and after the restart, from the POV of a node which the restarted node sees as down:

Before, when nothing has been done:
 {{Netid State Recv-Q Send-Q Local Address:Port Peer Address:Port}}
 {{tcp   ESTAB 0      0      10.0.161.40:59739  10.0.107.88:7000 timer:(keepalive,4min59sec,0) uid:115 ino:5913893 sk:216e <->}}

After 10.0.107.88 has been restarted. Note the {{1}} in in the recv-q:
 {{tcp CLOSE-WAIT 1 0 10.0.161.40:59739 10.0.107.88:7000 timer:(keepalive,4min14sec,0) uid:115 ino:5913893 sk:216e -->}}

When 10.0.107.88 comes back up and 10.0.161.40 tries to respond to the EchoMessage, using the previous socket which has been in CLOSE-WAIT. You can see the outbounds piling up in the send-q:
 {{tcp CLOSE-WAIT 1 36527 10.0.161.40:59739 10.0.107.88:7000 timer:(on,1.932ms,4) uid:115 ino:5913893 sk:216e -->}};;;","05/May/18 02:03;jasobrown;[~alienth] nice detective work. it looks like {{10.0.161.40}} hasn't closed the previous sockets/connections. If you have the logs of {{10.0.161.40}} still handy, can you see if, during and after the bounce, there are log statements about  {{10.0.107.88}} being down or alive (or restarted). Not looking at the code to see the exact log messages, but typically they are emitted by {{Gossiper}}.

Also, how long did it take  {{10.0.107.88}} to bounce? If it went down and came back up fast enough, it's possible the failure detector on {{10.0.161.40}} didn't mark it as down.

How did you terminate {{10.0.107.88}}? A normal shutdown, or {{kill -9}}? ;;;","05/May/18 03:25;alienth;[~jasobrown] {{.88}} was down for about 10 minutes, and was shutdown via `nodetool drain`, followed by stopping the service gracefully.


{{.40}} has no gossiper logs showing {{.88}} going down, until the socket to {{.88}} is killed about 20 minutes later. {{.88}} was shut down at 1602 and came back up at 1612:

{{INFO  [RMI TCP Connection(22)-127.0.0.1] 2018-05-04 16:02:14,055 StorageService.java:1449 - DRAINING: starting drain process}}
{{INFO  [RMI TCP Connection(22)-127.0.0.1] 2018-05-04 16:02:14,057 HintsService.java:220 - Paused hints dispatch}}
{{INFO  [RMI TCP Connection(22)-127.0.0.1] 2018-05-04 16:02:14,058 ThriftServer.java:139 - Stop listening to thrift clients}}
{{INFO  [RMI TCP Connection(22)-127.0.0.1] 2018-05-04 16:02:14,077 Server.java:176 - Stop listening for CQL clients}}
{{INFO  [RMI TCP Connection(22)-127.0.0.1] 2018-05-04 16:02:14,078 Gossiper.java:1540 - Announcing shutdown}}
<snip>
{{INFO  [main] 2018-05-04 16:12:39,746 StorageService.java:1449 - JOINING: Finish joining ring}}

And {{.40}} has no logs of this happening, and seemingly doesn't see it as down until ~20 minutes later:

{{INFO  [GossipStage:1] 2018-05-01 16:20:36,680 Gossiper.java:1034 - InetAddress /10.0.107.88 is now DOWN}}
{{INFO  [HANDSHAKE-/10.0.107.88] 2018-05-01 16:20:36,693 OutboundTcpConnection.java:560 - Handshaking version with /10.0.107.88}}
{{INFO  [HANDSHAKE-/10.0.107.88] 2018-05-01 16:20:36,822 OutboundTcpConnection.java:560 - Handshaking version with /10.0.107.88}}
{{INFO  [GossipStage:1] 2018-05-01 16:20:56,323 Gossiper.java:1053 - Node /10.0.107.88 has restarted, now UP}}
{{INFO  [GossipStage:1] 2018-05-01 16:20:56,325 StorageService.java:2292 - Node /10.0.107.88 state jump to NORMAL}}

That is.. bizarre.;;;","05/May/18 03:28;alienth;One extra thing to note, as expected turning down `tcp_retries2` does greatly alleviate this issue. The `CLOSE-WAIT` socket is nuked by the kernel much more quickly, resulting in the partition resolving quickly.;;;","05/May/18 03:34;alienth;Extra note: At the same second that {{.88}}'s gossiper announced shutdown, there was a newly established socket from {{.40}} to {{.88}}:

{{Fri May 4 16:02:15 PDT 2018}}
 {{tcp ESTAB 0 0 10.0.161.40:59739 10.0.107.88:7000 timer:(keepalive,4min59sec,0) uid:115 ino:5913893 sk:216e <->}}

This is the same socket that would become CLOSE-WAIT only seconds later, and remain that way for the following 18 minutes:

{{Fri May 4 16:02:20 PDT 2018}}
 {{tcp CLOSE-WAIT 1 0 10.0.161.40:59739 10.0.107.88:7000 timer:(keepalive,4min54sec,0) uid:115 ino:5913893 sk:216e -->}};;;","05/May/18 04:39;alienth;Why on earth didn't the failure detector on {{.40}} see anything for 20 minutes? Possibly due to blocking on an Outbound forever?;;;","21/May/18 19:06;jolynch;[~alienth] that is interesting and thank you for digging so deeply! If I understand correctly during a {{drain}} the other servers are responsible for noticing the change and closing their connections within the {{[shutdown_announce_in_ms|https://github.com/apache/cassandra/blob/06b3521acdb21dd3d85902d59146b9d08ad7d752/src/java/org/apache/cassandra/gms/Gossiper.java#L1497]}} period in [response|https://github.com/apache/cassandra/blob/06b3521acdb21dd3d85902d59146b9d08ad7d752/src/java/org/apache/cassandra/gms/GossipShutdownVerbHandler.java#L37] to the {{GOSSIP_SHUTDOWN}} gossip state, and then the {{[markAsShutdown|https://github.com/apache/cassandra/blob/06b3521acdb21dd3d85902d59146b9d08ad7d752/src/java/org/apache/cassandra/gms/Gossiper.java#L363-L373]}} method marks it down and forcibly convicts it. I believe that the TCP connections get closed via the {{StorageService}}'s {{onDead}} method which calls {{[onDead|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L2514]}} which calls {{[MessagingService::reset|https://github.com/apache/cassandra/blob/06b3521acdb21dd3d85902d59146b9d08ad7d752/src/java/org/apache/cassandra/net/MessagingService.java#L505]}} which calls {{[OutboundTcpConnection::closeSocket|https://github.com/apache/cassandra/blob/06b3521acdb21dd3d85902d59146b9d08ad7d752/src/java/org/apache/cassandra/net/OutboundTcpConnectionPool.java#L80], which [enqueues a sentinel|https://github.com/apache/cassandra/blob/06b3521acdb21dd3d85902d59146b9d08ad7d752/src/java/org/apache/cassandra/net/OutboundTcpConnection.java#L210]}} into the backlog and then the {{[OutboundTcpConnection::run|https://github.com/apache/cassandra/blob/06b3521acdb21dd3d85902d59146b9d08ad7d752/src/java/org/apache/cassandra/net/OutboundTcpConnection.java#L253]}} method is actually supposed to close it. The {{drainedMessages}} queue is a local reference though so backlog could get something that was enqueued before the {{CLOSE_SENTINEL}} and after it as well. This seems very racey to me, in particular the reconnection logic might race with the closing logic from what I can tell as we have a 2 second window between when the clients start closing and when the server will actually stop accepting new connections (because it closes the listeners).

Non stateful networks would surface the RST in the {{writeConnected}} method, but AWS is like ""yea that machine isn't allowed to talk to that one"" and just blackholes the RSTs... I wonder if I can reproduce this by increasing that window significantly and just sending lots of traffic.;;;","08/Jun/18 18:18;khuizhang;[~jolynch] currently we have an issue with gossip going one way after a node had sudden loss of storage controller. After the offending node comes back online, all the rest nodes show TCP connections on gossip, but those connections  (in bold) are not seen on the offending node. On offending node, nodetool gossipinfo shows generation 0 for all other nodes and nodetool status show DN for all other nodes. On other nodes, nodetool gossipinfo seems fine but nodetool status shows offending node down. This can be resolved by restarting cassandra on all nodes except the offending node, or wait for 2 hours after crash event (tcp_keepalive is set to 7200s on debian?). I don't know if this is related, but wondering if there is any way to verify (like TRACE logging on org.apache.cassandra.gms and/or org.apache.cassandra.net, or maybe packet capture). So far we can reproduce it at 30% chance.  Thanks in advance. 

node 10.96.105.4
*tcp 0 0 10.96.105.4:7001 10.96.105.6:55629 ESTABLISHED keepalive (729.79/0/0)*
*tcp 0 0 10.96.105.4:39219 10.96.105.6:7001 ESTABLISHED keepalive (783.04/0/0)*
*tcp 0 0 10.96.105.4:7001 10.96.105.6:60007 ESTABLISHED keepalive (729.79/0/0)*
tcp 0 0 10.96.105.4:7001 10.96.105.6:45318 ESTABLISHED keepalive (1471.16/0/0)
node 10.96.105.6
tcp 0 0 10.96.105.6:7001 0.0.0.0:* LISTEN off (0.00/0/0)
tcp 0 0 10.96.105.6:45318 10.96.105.4:7001 ESTABLISHED keepalive (1477.00/0/0);;;","23/Aug/18 01:42;jolynch;[~khuizhang] yea that looks similar with the half open keepalive connections. Did you try the kernel workaround and did it help?

[~aweisberg] [~jasobrown] I've got a mitigation patch so that Cassandra trunk at least heals the half open partitions faster. Please take a look if you have bandwidth for review. While testing the re-connection behavior I ran into CASSANDRA-14503 because the retry future was just getting clobbered by another message, so I couldn't test that we don't keep retrying after just connections are killed (as right now they just retry every message).
||trunk||
|[patch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358]|
|[unit tests|https://circleci.com/gh/jolynch/workflows/cassandra/tree/CASSANDRA-14358]|

This patch makes the timeout configurable for internode connection (2s) and internode tcp user timeout (30s). The timeouts are settable via JMX (and nodetool).

I'm marking this as patch available as I think the operating system workaround is probably ok for previous releases? If it's not just let me know and I can try to figure out how to fix it for those ones as well.

I didn't have any new tests because the only way I'm aware to reproduce this behavior is by using iptables to blackhole traffic. I've been testing with a ccm cluster by trying to block just the small message channel (if gossip is blocked then the failure detector convicts it) and doing a read at ALL:
{noformat}
$ netstat -on | grep 7000 | grep 127.0.0.3:7000 | grep -v ""0 127.0.0.3""
tcp        0      0 127.0.0.1:55604         127.0.0.3:7000          ESTABLISHED keepalive (4093.37/0/0)
tcp        0      0 127.0.0.1:55610         127.0.0.3:7000          ESTABLISHED keepalive (4093.53/0/0)
tcp        0      0 127.0.0.1:58080         127.0.0.3:7000          ESTABLISHED keepalive (6601.96/0/0)

# Try to drop just the small message channel
$ sudo iptables -A OUTPUT -p tcp -d 127.0.0.3 --dport 7000 --sport 58080 -j DROP
{noformat}
Then I just check that we properly reconnect faster than 15 minutes. On an unpatched trunk I can watch the small message channel just sit there probing for 15 retries (the default tcp_retries2 value, node that this netstat is a different run than the previous one) :
{noformat}
netstat -on | grep 7000 | grep 127.0.0.3:7000 | grep -v ""0 127.0.0.3""               
tcp        0      0 127.0.0.1:34532         127.0.0.3:7000          ESTABLISHED keepalive (6823.99/0/0) 
tcp        0      0 127.0.0.1:34564         127.0.0.3:7000          ESTABLISHED keepalive (6840.36/0/0) 
tcp        0    808 127.0.0.1:34544         127.0.0.3:7000          ESTABLISHED probe (108.32/0/10) 
tcp        0      0 127.0.0.1:34554         127.0.0.3:7000          ESTABLISHED keepalive (6840.26/0/0) 
tcp        0      0 127.0.0.1:34540         127.0.0.3:7000          ESTABLISHED keepalive (6831.27/0/0) 
tcp        0      0 127.0.0.1:34534         127.0.0.3:7000          ESTABLISHED keepalive (6828.96/0/0)

{noformat}
And then with my patch it get's killed after like 5 retries instead of waiting the full 15.

If you have ideas for how to unit test it I'm open to suggestions of course.;;;","27/Sep/18 16:23;khuizhang;[~jolynch]: thanks for the answer. I did the kernel workaround and it worked. ;;;","11/Oct/18 20:34;aweisberg;One argument against this approach is that this is Linux specific and maybe we should go with a solution that works on other platforms (timers and external threads timing stuff out), but to an extent that is the tail wagging the dog, and I like how simple this is.

For testing for dead connections one good reason to heartbeat at the application level is that it can detect bugs at the application level that prevent the networking code from processing messages. This won't catch a connection being wedged like that. And once you have gone to the trouble of heartbeating having the kernel check is superfluous.

I think a heartbeat mechanism is a bit much for 4.0 during the code freeze and this is a small enough change I would rather back it out later (or leave it, it's harmless) if we add heartbeating down the road.

* [Move these defaults to Config | https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358#diff-f5e4f8d3a95c98844b371ba1d1e98285R38] and use them there so they can't mismatch. 
* [Can you add these config options to the yaml with these comments, but commented out?|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358#diff-b66584c9ce7b64019b5db5a531deeda1R147]
* It would be nice if the unit of the timeout were in the name in more places. Basically add _ms everywhere you can. I would actually leave nodetool the way it is just to satisfy my OCD since units are not in the name of the other options and we don't want to change them. Although adding synonyms might be nice.

My only concern is whether we should ratchet TCP_USER_TIMEOUT timeouts down as a new default. Not because I have a concrete issue with it, but because I lack the operational experience say yeah this is a good idea in practice.

I really want to give people a good default and not cop out and leave it at 10 minutes where it was just because that is how it was. I brought it up in IRC and maybe some people will chime in.;;;","11/Oct/18 20:50;aweisberg;Talked about it in IRC and people think I am overthinking.

I think 30 seconds should be fine. Each server would only see a few hundred incoming connections at once. A very nice benchmark number to have for C* would be the SSL accept rate. Apparently SSL accept is single threaded right now. One thing to watch out for when measuring that kind of thing is sockets being stuck in TIMED_WAIT causing you to run out of ephemeral ports. Setting ""TCP_TW_REUSE"" fixes that issue for benchmarks.

I don't think you need to do that for this issue though.;;;","25/Oct/18 00:10;jolynch;Great! I pushed a patch incorporating your feedback and am running dtests against it. I am slightly concerned that {{OutboundConnectionParams}} now requires the {{DatabaseDescriptor}} to be loaded, but if tests pass I think we're ok?
||trunk||
|[patch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358]|
|dtests:[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14358.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14358]|

I agree that active application level monitoring is a great idea, but to be honest I think this plus the latency probes we added as part of CASSANDRA-14459 should do a pretty good job of detecting and evicting bad connections. There is of course room for improvement but I think it's actually a pretty good start.;;;","29/Oct/18 22:44;aweisberg;[Why add protocol version here?|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358#diff-1560ed3bf5675f8ec0b1b35198debe15R41]
[The acceptable range of values differs, but the tests are for both are -1|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358#diff-f5e4f8d3a95c98844b371ba1d1e98285R224]. I just want to confirm what the underlying tunables actually support.;;;","29/Oct/18 23:35;jolynch;{quote}[Why add protocol version here?|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358#diff-1560ed3bf5675f8ec0b1b35198debe15R41]
{quote}
Because otherwise the test fails the {{protocolVersion}} check instead of the {{sendBufferSize}} check which the test was (I believe) trying to test. Before this patch I believe that the unit test was testing the wrong thing.
{quote}[The acceptable range of values differs, but the tests are for both are -1|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358#diff-f5e4f8d3a95c98844b371ba1d1e98285R224]. I just want to confirm what the underlying tunables actually support.
{quote}
Correct, the {{TCP_USER_TIMEOUT}} can be set to zero (which will pick up the OS level setting).
{noformat}
       TCP_USER_TIMEOUT (since Linux 2.6.37)
              This option takes an unsigned int as an argument.  When the value is greater than 0, it specifies the maximum amount of time in milliseconds that transmitted data may remain unacknowledged before TCP will forcibly close the corresponding connection and return ETIMEDOUT to the applica‐
              tion.  If the option value is specified as 0, TCP will to use the system default.
{noformat}
The connection timeout setting also [supports zero|https://netty.io/4.0/api/io/netty/channel/ChannelConfig.html#setConnectTimeoutMillis-int-] (meaning disable) but I don't (imo) think that users should ever disable connection timeouts. A user could plausibly set it to the same value as their RPC timeout, or even higher, but I don't think turning it off ever makes sense.;;;","01/Nov/18 04:34;jolynch;I have a branch over on CASSANDRA-14862 which fixes the broken dtests. Is there any more testing or changes you want me to do on this patch?;;;","01/Nov/18 14:58;aweisberg;Can you update CHANGES.txt and NEW.txt and then I will commit?

+1;;;","01/Nov/18 18:50;jolynch;I've added the CHANGES and NEWS updates, rebased and squashed down the changes into [cb82946|https://github.com/apache/cassandra/commit/cb82946b48100b06a342a02093dd3bb2c489e25b]. Tests are re-running on my branch.

||trunk||
|[patch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358]|
|dtests:[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14358.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14358]|;;;","01/Nov/18 18:58;aweisberg;Hey, so feedback on NEWS.txt, linking to the ticket is helpful, but saying there are two new tunables in the YAML is important.

For CHANGES.txt I think we always use the title of the JIRA. On commit I frequently change the title of the JIRA to make it more useful in CHANGES.txt. People should know either what changed, or what problem was fixed, or both.;;;","01/Nov/18 20:36;jolynch;[~aweisberg] Ok I think I've re-factored the jira ticket name and CHANGES entry appropriately (since we're not fixing the underlying problem in this ticket, just mitigating). I've also updated the NEWS entry to list the two tunables.

Rebased patch at [ac83f0de|https://github.com/apache/cassandra/commit/ac83f0def640ab89d0a1c911e82d867f6588de4c].;;;","01/Nov/18 20:41;aweisberg;OK, +1 I'll commit. Just a heads up you can write long beautiful commit messages, but they get replaced with a one line subject of the JIRA and a standard Patch by XYZ; Reviews by ZYX for CASSANDRA1234. We don't deviate from that format for commit messages.;;;","01/Nov/18 21:05;jolynch;Ok, sounds reasonable :-);;;","01/Nov/18 22:37;aweisberg;Committed as [bfbc5274f2b3a5af2cbbe9679f0e78f1066ef638|https://github.com/apache/cassandra/commit/bfbc5274f2b3a5af2cbbe9679f0e78f1066ef638]. Thanks!;;;","02/Nov/18 17:20;jolynch;For those looking for a fix for earlier versions, unfortunately I was not able to determine the root cause of the close issues, but we have a follow up ticket CASSANDRA-14818 for trying to figure that out (if someone can figure that out I think it's worth fixing that).

That being said these partitions are now mitigated in 4.0 with the TCP_USER_TIMEOUT socket option.

A workaround for earlier versions is to set the {{net.ipv4.tcp_retries2}} sysctl to ~5. This can be done with the following:
{code:bash}
$ cat /etc/sysctl.d/20-cassandra-tuning.conf
net.ipv4.tcp_retries2=5
$ # Reload all sysctls
$ sysctl --system{code};;;","05/Nov/18 12:15;benedict;{quote}We don't deviate from that format for commit messages{quote}

Is this true?  I recall our standardising on the last line following the format ""Patch by X,Y,Z; reviewed by A,B,C for CASSANDRA-XXXXX"", and the first line approximating the JIRA title.  I don't recall any proposal to prohibit more information between those two lines?

I certainly put more information in sometimes, and should probably do it more.  I'd hate to explicitly discourage it.;;;","06/Nov/18 19:16;aweisberg;I guess I misinterpreted the scolding I got long ago WRT to commit messages.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
LWTs keep failing in trunk after immutable refactor,CASSANDRA-14356,13149158,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,burmanm,burmanm,burmanm,30/Mar/18 12:52,15/May/20 08:01,13/Jul/23 08:37,18/Jun/18 09:47,4.0,4.0-alpha1,,,,,Feature/Lightweight Transactions,Legacy/Core,,,0,LWT,,,"In the PaxosState, the original assert check is in the form of:

assert promised.update.metadata() == accepted.update.metadata() && accepted.update.metadata() == mostRecentCommit.update.metadata();

However, after the change to make TableMetadata immutable this no longer works as these instances are not necessarily the same (or never). This causes the LWTs to fail although they're still correctly targetting the same table.

From IRC:

<pcmanus> It's a bug alright. Though really, the assertion should be on the metadata ids, cause TableMetadata#equals does more than what we want.
<pcmanus> That is, replacing by .equals() is not ok. That would reject throw on any change to a table metadata, while the spirit of the assumption was to sanity check both update were on the same table.","OpenJDK Runtime Environment (build 1.8.0_161-b14), Cassandra 4.0 commit c22ee2bd451d030e99cfb65be839bbc735a5352f (29.3.2018 14:01)",aleksey,burmanm,jjirsa,mck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/18 16:04;burmanm;CASSANDRA-14356.diff;https://issues.apache.org/jira/secure/attachment/12917039/CASSANDRA-14356.diff",,,,,,,,,,,,,1.0,burmanm,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 18 10:05:24 UTC 2018,,,,,,,,,,,"0|i3rzpb:",9223372036854775807,4.0,,,,,,,,mck,,mck,,,Normal,,,,,,,,,,,,,,,,,,,"17/Jun/18 04:26;mck;This exception is evident in Reaper's builds against Cassandra trunk, ref [travis|https://travis-ci.org/thelastpickle/cassandra-reaper/branches];;;","17/Jun/18 04:40;mck;|| Branch || uTest || aTest || dTest ||
|[trunk_14356|https://github.com/thelastpickle/cassandra/tree/mck/trunk_14356]|[!https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Ftrunk_14356.svg?style=svg!|https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Ftrunk_14356]| [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/35/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/35] | [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/574/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/574] |;;;","17/Jun/18 18:44;jjirsa;cc [~iamaleksey] for visibility.;;;","17/Jun/18 22:50;mck;[~iamaleksey], any objections if I commit this? It makes sense, looks good, to me, and has been tested and verified against Reaper.;;;","18/Jun/18 09:37;aleksey;[~mck] Change LGTM, ship it (:;;;","18/Jun/18 10:05;mck;Committed as 717c108374;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Untracked CDC segment files are not deleted after replay,CASSANDRA-14349,13148721,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,shichao.an,shichao.an,shichao.an,28/Mar/18 22:01,27/Jul/21 19:05,13/Jul/23 08:37,27/Jul/21 19:05,3.11.10,4.0-rc2,,,,,Legacy/Local Write-Read Paths,,,,0,,,,"When CDC is enabled, a hard link to each commit log file will be created in cdc_raw directory. Those commit logs with CDC mutations will also have cdc index files created along with the hard links; these are intended for the consumer to handle and clean them up.

However, if we don't produce any CDC traffic, those hard links in cdc_raw will be never cleaned up (because hard links will still be created, without the index files), whereas the real original commit logs are correctly deleted after replay during process startup. This will results in many untracked hard links in cdc_raw if we restart the cassandra process many times. I am able to use CCM to reproduce it in trunk version which has the CASSANDRA-12148 changes.

This seems a bug in handleReplayedSegment of the commit log segment manager which neglects to take care of CDC commit logs. I will attach a patch here.",,blerer,jay.zhuang,jjirsa,jmckenzie,JoshuaMcKenzie,shichao.an,vinaykumarcse,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,shichao.an,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 27 19:03:12 UTC 2021,,,,,,,,,,,"0|i3rx0n:",9223372036854775807,4.0,,,,,,,,JoshuaMcKenzie,,JoshuaMcKenzie,,,Low,,4.0,,,https://github.com/shichao-an/cassandra/commits/14349-trunk,,,,,,,,,,,,,,"28/Mar/18 22:59;shichao.an;Here's the patch:

||Branch||uTest||
|[14349-trunk|https://github.com/shichao-an/cassandra/commits/14349-trunk]|[!https://circleci.com/gh/shichao-an/cassandra/tree/14349-trunk.svg?style=svg!|https://circleci.com/gh/shichao-an/cassandra/tree/14349-trunk]|
;;;","29/Mar/18 02:50;zznate;[~shichao.an] thanks for the patch. Just to clarify, this is for an edge case where:
 * CDC is enabled, creating hard links
 * CDC has no traffic
 * Cassandra is restarted
 * original hardlinks remain, but new ones are created for current process?

 ;;;","29/Mar/18 03:49;shichao.an;[~zznate] Yes, exactly.;;;","30/Mar/18 21:38;jay.zhuang;Hi [~shichao.an], nice finding. It would be great to have a dTest for that, just restart the node a few time and check if there's any orphaned commitlog in {{cdc_raw}} directory. Any non-active commitlog that doesn't have idx file should be considered orphaned.

cc. [~JoshuaMcKenzie];;;","02/Apr/18 15:31;JoshuaMcKenzie;I'll take review on it. Should be able to get to it this week.;;;","07/Apr/18 00:39;shichao.an;I added a dtest, here's the [patch|https://github.com/shichao-an/cassandra-dtest/commits/master] on GitHub.;;;","07/Apr/18 23:24;JoshuaMcKenzie;+1 to both patches; I'll get the C* side committed tomorrow. Open a PR for those dtest changes and I'll get those merged after the C* side.

 

Thanks!;;;","07/Apr/18 23:44;shichao.an;Thank you [~JoshuaMcKenzie]. Here's the [PR|https://github.com/apache/cassandra-dtest/pull/23] for dtest.;;;","08/Apr/18 15:39;JoshuaMcKenzie;Merged C* side. Don't have merge rights on the dtest repo, so pinged [~philipthompson] about that.;;;","09/Apr/18 00:50;jjirsa;dtest repo is in ASF now, same righths, just check your remote

 ;;;","09/Apr/18 13:52;JoshuaMcKenzie;Yeah, didn't think about it being the whole ""push it, github PR's don't work"" thing. I'll get to it in a bit.;;;","26/Jul/21 08:47;blerer;[~JoshuaMcKenzie] I just wanted to confirm with you that everything has been committed for that ticket and that we can simply close it.;;;","27/Jul/21 19:03;jmckenzie;ack [~blerer]. This was merged in as commit

6edfe7fb50b2d0562282b12b07aba67e95a76940 back in 2018; failed to follow up here.

 

I'll close this out.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix scheduling of speculative retry threshold recalculation,CASSANDRA-14338,13147465,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,aleksey,aleksey,23/Mar/18 13:07,15/May/20 08:01,13/Jul/23 08:37,23/Mar/18 13:59,4.0,4.0-alpha1,,,,,,,,,0,,,,"Realized after committing CASSANDRA-14293 that it's slightly broken: when updating from a static to a dynamic speculative retry policy, the refresher will never be scheduled.

Also realised that the whole thing is a bit dumb. We don't need a latency calculator runnable per {{ColumnFamilyStore}}, we should have one per node, that does in batch refresh thresholds for every table. It's one fewer thing to manage, and allows to get rid of {{isDynamic()}} method in {{SpeculativeRetryPolicy}}, which I thought was clever at the time.",,aleksey,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 23 13:58:59 UTC 2018,,,,,,,,,,,"0|i3rpav:",9223372036854775807,,,,,,,,,samt,,samt,,,Low,,,,,,,,,,,,,,,,,,,"23/Mar/18 13:09;aleksey;4.0 branch: https://github.com/iamaleksey/cassandra/tree/14338-4.0;;;","23/Mar/18 13:43;samt;+1 LGTM.

I would just wrap the calculation in {{CFS::updateSpeculationThreshold}} in a try/catch so that an exception there doesn't cause future executions of the task to be suppressed.;;;","23/Mar/18 13:46;aleksey;Good call. I'd say in 3.0 this also should be fixed, so that a single exception there doesn't stop recalculating percentiles for the affected table.

Thanks for the review. Will commit once Circle is happy.;;;","23/Mar/18 13:58;aleksey;Committed as [8b7e96761f968b346aed08c0c201a8d40d801b19|https://github.com/apache/cassandra/commit/8b7e96761f968b346aed08c0c201a8d40d801b19], thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C* nodetool should report the lowest of the highest CQL protocol version supported by all clients connecting to it,CASSANDRA-14335,13147045,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,djoshi,djoshi,djoshi,22/Mar/18 03:29,15/May/20 08:06,13/Jul/23 08:37,04/May/18 18:50,4.0,4.0-alpha1,,,,,,,,,0,,,,"While upgrading C*, it makes it hard to tell whether any client will be affected if C* is upgraded. C* should internally store the highest protocol version of all clients connecting to it. The lowest supported version will help determining if any client will be adversely affected by the upgrade.",,djoshi,jasobrown,rha,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,djoshi,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 04 21:40:02 UTC 2018,,,,,,,,,,,"0|i3rmpr:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"01/May/18 22:29;djoshi;||trunk||
|[branch|https://github.com/dineshjoshi/cassandra/tree/14335-trunk]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/14335-trunk]|
||;;;","02/May/18 01:35;zznate;[~djoshi3] You have some config changes to CircleCI as part of this branch - was that intentional? Can we put those in a separate ticket if so?;;;","02/May/18 01:54;jasobrown;[~zznate] the circleci changes are just for running the utests/dtests, and not part of the real commit. We make sure to not commit those.;;;","02/May/18 05:39;djoshi;Thank you [~jasobrown]. [~zznate] just so you have more background, I have a paid CircleCI account that supports higher quotas. In order to use it, I need to update the settings. So my patches are typically two commits. You can cherry pick just the patch and ignore the CircleCI commit. If there is a better way to do this, I'm happy to discuss it :);;;","02/May/18 05:41;djoshi;I have fixed some minor issues with the patch that I spotted. I am not 100% certain that this class needs to be thread safe. Thoughts?;;;","02/May/18 13:27;jasobrown;We already store the {{ProtocolVersion}} along with the channel in {{Connection}}. We also store a reference to the {{Connection}} in the {{Channel}}, as a channel attribute. Thus, it seems to me you could just add a method like this to {{Server.ConnectionTracker}} (I changed the return type to make the example simpler):

{code:java}
        public Map<ProtocolVersion, Set<SocketAddress>> getClientsByProtocolVersion()
        {
            Map<ProtocolVersion, Set<SocketAddress>> result = new EnumMap<>(ProtocolVersion.class);
            for (Channel c : allChannels)
            {
                Attribute<Connection> attrConn = c.attr(Connection.attributeKey);
                Connection connection = attrConn.get();
                if (connection != null)
                {
                    ProtocolVersion version = connection.getVersion();
                    SocketAddress addr = c.remoteAddress();
                    result.computeIfAbsent(version, protocolVersion -> new HashSet<>());
                    result.get(version).add(addr);
                }
            }
            return result;
        }
{code}

(Note: {{ConnectionTracker#allChannels}} is a {{DefaultChannelGroup}}, whose {{Iterator()}} method wraps two {{ConcurrentHashMap}} s, so I think you are safe concurrency-wise.) This gives you a snapshot of everything that is currently connected. Is this sufficient, [~djoshi3]?;;;","02/May/18 15:20;djoshi;This will just give me the currently connected clients. When the client connects and drops off we won’t have any idea of what protocol version it used. So, my patch keeps a history of last N Clients and their protocol versions. ;;;","03/May/18 22:35;djoshi;[~jasobrown] I have updated my branch with an implementation that will be threadsafe as well as performant. As part of this change, I am introducing addition flags for {{nodetool clientstats}} command. We can use {{nodetool clientstats --by-protocol}} to give you the list last 100 clients that the C* process has seen for each protocol. I am deduping by IP address so you should see unique IPs. You can clear this list by running {{nodetool clientstats --clear-history}} to clear the history of all clients.

For posterity - [~jasobrown] and I had a conversation about the usefulness of having this information. Although the existing {{nodetool clientstats --all}} is sufficient to tell you what clients are _currently_ connected to the C* process including the protocol version, this information is ephemeral. There are many situations where an operator may miss intermittently connecting clients for example - batch processes. Additionally, we can script around by periodically invoking {{nodetool clientstats --all}} from external scripts but that is sub-optimal for a variety of reasons - you can still miss intermittent clients, you unnecessarily make calls over JMX. Storing this historical information in the C* process means you wont miss intermittently connecting clients and would be overall more efficient that going over JMX all the time.;;;","04/May/18 18:50;jasobrown;+1, and committed with a few minor spacing/naming changes as sha {{a9ec46a613ae5602ced004935c9954638e83e735}};;;","04/May/18 19:58;jasobrown;Ughh, forgot to commit my minor changes locally before pushing up. commited those changes as {{68605cf03bdfecb11cd69c6d5260a773e4e87300}};;;","04/May/18 21:40;djoshi;Thanks [~jasobrown];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix unbounded validation compactions on repair,CASSANDRA-14332,13146727,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,KurtG,KurtG,KurtG,21/Mar/18 04:32,03/Sep/20 11:56,13/Jul/23 08:37,17/Apr/18 21:24,3.0.17,3.11.3,,,,,,,,,0,,,,"After CASSANDRA-13797 it's possible to cause unbounded, simultaneous validation compactions as we no longer wait for validations to finish. Potential fix is to have a sane default for the # of concurrent validation compactions by backporting CASSANDRA-13521 and setting a sane default.",,bdeggleston,jeromatron,KurtG,pauloricardomg,vinaykumarcse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14470,,,,,CASSANDRA-13797,,,,,,,,,CASSANDRA-15902,,,,,,,,,,,,,,,,,,,0.0,KurtG,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 17 21:24:36 UTC 2018,,,,,,,,,,,"0|i3rkrr:",9223372036854775807,,,,,,,,,bdeggleston,,bdeggleston,,,Normal,,,,,,,,,,,,,,,,,,,"13/Apr/18 04:24;KurtG;|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...kgreav:13797-r-3.0]|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:13797-r-3.11]| 
|[utests|https://circleci.com/gh/kgreav/cassandra/152]|[utests|https://circleci.com/gh/kgreav/cassandra/154]|

I think reverting this is the correct solution for 3.0 and 3.x. Changing defaults in minors while in this case is probably fine, it's not good practice. Backporting CASSANDRA-13521 completely is also a bit questionable. 

The original problem from CASSANDRA-13797 of the interrupted exceptions is only very minor, and likely not possible outside of CCM clusters.;;;","13/Apr/18 15:56;bdeggleston;fwiw, the interrupted exception _wasn't_ from a ccm cluster, so this can happen in the wild. That said, occasionally getting interrupted exceptions when repairing inactive tables is a lot less of a problem than unbounded repair sessions.;;;","17/Apr/18 21:24;bdeggleston;committed as {{00e5a3d508eb41944ce01c6cc96ae18cb16dad8c}} thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle repeat open bound from SRP in read repair,CASSANDRA-14330,13146590,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,20/Mar/18 16:27,15/May/20 08:05,13/Jul/23 08:37,23/Mar/18 14:56,3.0.17,3.11.3,4.0,4.0-alpha1,,,Consistency/Coordination,Local/SSTable,,,0,,,,"If there is an open range tombstone in an iterator, a short read protection request for it will include a repeat open bound. Currently, {{DataResolver}} doesn't expect this, and will raise an assertion, timing out the request:
{code:java}
java.lang.AssertionError: Error merging RTs on test.test: merged=null, versions=[Marker EXCL_START_BOUND(0)@0, null], sources={[/127.0.0.1, /127.0.0.2]}, responses:
    /127.0.0.1 => [test.test] key=0 partition_deletion=deletedAt=-9223372036854775808, localDeletion=2147483647 columns=[[] | []]
       Row[info=[ts=1] ]: ck=0 | ,
   /127.0.0.2 => [test.test] key=0 partition_deletion=deletedAt=-9223372036854775808, localDeletion=2147483647 columns=[[] | []]
       Row[info=[ts=-9223372036854775808] del=deletedAt=1, localDeletion=1521572669 ]: ck=0 |
       Row[info=[ts=1] ]: ck=1 | 
{code}
As this is a completely normal/common scenario, we should allow for this, and relax the assertion.

Additionally, the linked branch makes the re-throwing {{AssertionError}} more detailed and more correct: the responses are now printed out in the correct order, respecting {{isReversed}}, the command causing the assertion is now logged, as is {{isReversed}} itself, and local deletion times for RTs.",,aleksey,jjirsa,kohlisankalp,marcuse,samt,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,Correctness -> Transient Incorrect Response,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 23 14:56:26 UTC 2018,,,,,,,,,,,"0|i3rjxj:",9223372036854775807,,,,,,,,,slebresne,,slebresne,,,Normal,,,,,,,,,,,,,,,,,,,"21/Mar/18 15:52;aleksey;Branches for [3.0|https://github.com/iamaleksey/cassandra/tree/14330-3.0], [3.11|https://github.com/iamaleksey/cassandra/tree/14330-3.11], and [4.0|https://github.com/iamaleksey/cassandra/tree/14330-4.0]. CI: [3.0|https://circleci.com/gh/iamaleksey/cassandra/70], [3.11|https://circleci.com/gh/iamaleksey/cassandra/73], [4.0|https://circleci.com/gh/iamaleksey/cassandra/74].

Minimal reproduction dtest [here|https://github.com/iamaleksey/cassandra-dtest/tree/14330].;;;","23/Mar/18 14:29;slebresne;+1, fix lgtm. Nice job tracking that down and on the minimal test.;;;","23/Mar/18 14:56;aleksey;Thanks for the prompt review, [~slebresne]

Committed as [3153c630c499edf3c523d8e5a3db6f1d6c52ad4c|https://github.com/apache/cassandra/commit/3153c630c499edf3c523d8e5a3db6f1d6c52ad4c] to 3.0 and merged upwards. Committed the dtest as [dac3d7535cc120a6615257fb9dd05988e9901dc4|https://github.com/apache/cassandra-dtest/commit/dac3d7535cc120a6615257fb9dd05988e9901dc4].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Java executable check succeeds despite no java on PATH,CASSANDRA-14325,13146284,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,polo-language,polo-language,polo-language,19/Mar/18 16:19,16/Jul/21 13:00,13/Jul/23 08:37,16/Jul/21 13:00,3.0.25,3.11.11,4.0.1,,,,Local/Startup and Shutdown,,,,1,,,,"The check -z $JAVA on line 102 of bin/cassandra currently always succeeds if JAVA_HOME is not set since in this case JAVA gets set directly to 'java'. The error message ""_Unable to find java executable. Check JAVA_HOME and PATH environment variables._"" will never be echoed on a PATH misconfiguration. If java isn't on the PATH the failure will instead occur on line 95 of cassandra-env.sh at the java version check.

It would be better to check consistently for the java executable in one place in bin/cassandra. Also we don't want users to mistakenly think they have a java version problem when they in fact have a PATH problem.

See proposed patch.",,azotcsit,blerer,e.dimitrova,polo-language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/21 10:18;polo-language;3.x.patch;https://issues.apache.org/jira/secure/attachment/13030719/3.x.patch","16/Jul/21 10:18;polo-language;trunk.patch;https://issues.apache.org/jira/secure/attachment/13030720/trunk.patch",,,,,,,,,,,,2.0,polo-language,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,,,Fri Jul 16 13:00:21 UTC 2021,,,,,,,,,,,"0|i3ri1j:",9223372036854775807,,,,,,,,,,,azotcsit,brandon.williams,e.dimitrova,Low,,NA,,,https://github.com/apache/cassandra/commit/75ddf76801a7b603f9da5fcee0dc72bacc5843c6,,,,,,,,,,,,,,"24/Jun/21 21:59;brandon.williams;Looks good to me, +1.;;;","07/Jul/21 18:38;azotcsit;The patch looks good to me, however, I have two small comments:
 1. Even though the patch's logic seems to be working perfectly well, I feel the same can be achieved in a bit more compact way:
{code:java}
diff --git a/bin/cassandra.in.sh b/bin/cassandra.in.sh
index 58b4dd2896..5d13cdae70 100644
--- a/bin/cassandra.in.sh
+++ b/bin/cassandra.in.sh
@@ -95,7 +95,7 @@ if [ -n ""$JAVA_HOME"" ]; then
         fi
     done
 else
-    JAVA=java
+    JAVA=`command -v java 2> /dev/null`
 fi
 
 if [ -z $JAVA ] ; then
{code}
If there is no _java_ executable available then _JAVA_ variable will be empty and _if [ -z $JAVA ]_ condition will match to trigger the error.

2. I can see two more similar scripts and I believe they need to be updated as well:
{code:java}
redhat/cassandra.in.sh
tools/bin/cassandra.in.sh
{code}
 

PS:
I'm not a project committer. I've just chimed in after seeing [~blerer]'s email seeking for reviewers.

 ;;;","08/Jul/21 08:13;blerer;Thanks a lot [~azotcsit]. Any review comment will help us to move faster.;;;","15/Jul/21 15:18;polo-language;That's a nice simplification. I've attached patches for all three files.;;;","15/Jul/21 15:21;brandon.williams;LGTM, +1.;;;","15/Jul/21 15:47;azotcsit;+1 (non-binding);;;","15/Jul/21 15:52;e.dimitrova;+1, thank you. I am also adding [~azotcsit] as a reviewer as he also did a pass and gave valuable feedback.;;;","15/Jul/21 15:56;e.dimitrova;And seems our comments crashed, thanks again :) ;;;","15/Jul/21 16:07;brandon.williams;These seem to only to apply to 4.0 and up.  Were you still interested in fixing this in 3.0 and 3.11?;;;","16/Jul/21 10:19;polo-language;Patches for trunk/4.0 and 3.11/3.0 attached.;;;","16/Jul/21 13:00;brandon.williams;Committed, thank you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra NodeTool clientstats should show SSL Cipher,CASSANDRA-14322,13146007,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,djoshi,djoshi,djoshi,18/Mar/18 01:11,15/May/20 08:02,13/Jul/23 08:37,19/Mar/18 16:55,4.0,4.0-alpha1,,,,,,,,,0,security,,,Currently nodetool prints out some information that does not include the SSL Cipher being used by the client. It would be helpful to add this in for better visibility.,,djoshi,jasobrown,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,djoshi,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 19 17:09:44 UTC 2018,,,,,,,,,,,"0|i3rgc7:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Low,,,,,,,,,,,,,,,,,,,"18/Mar/18 01:13;djoshi;||nodetool-cipher-suite||
|[branch|https://github.com/dineshjoshi/cassandra/tree/nodetool-cipher-suite]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/nodetool-cipher-suite]|
||;;;","19/Mar/18 12:19;jasobrown;two minor points:

- I'm not a fan of printing {{""undefined""}} when the connection is not using ssl. Maybe {{""--""}} is better? 
- As long as we're adding the cipher suite, should we print out the protocol, as well?
;;;","19/Mar/18 16:07;djoshi;Hi [~jasobrown], I have addressed #2 and updated my commit. [~iamaleksey] and I settled on using {{undefined}} for missing driver information. So keep things consistent, I used {{undefined}} here.;;;","19/Mar/18 16:38;djoshi;[~iamaleksey] gave me some feedback over IRC that I have incorporated in the PR. Looks a lot better now.;;;","19/Mar/18 16:54;djoshi;I have rebased my patch on trunk since there were some overlapping changes that would've caused conflicts.;;;","19/Mar/18 16:55;jasobrown;+1. committed as sha {{59814db54375d4002eb11403c72861765d9eb356}}. Thanks!;;;","19/Mar/18 17:09;djoshi;[~jasobrown] thank you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix query pager DEBUG log leak causing hit in paged reads throughput,CASSANDRA-14318,13145729,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,adejanovski,adejanovski,adejanovski,16/Mar/18 14:25,16/Apr/19 09:29,13/Jul/23 08:37,30/Mar/18 15:18,2.2.13,,,,,,,,,,2,lhf,performance,,"Debug logging can involve in many cases (especially very low latency ones) a very important overhead on the read path in 2.2 as we've seen when upgrading clusters from 2.0 to 2.2.

The performance impact was especially noticeable on the client side metrics, where p99 could go up to 10 times higher, while ClientRequest metrics recorded by Cassandra didn't show any overhead.

Below shows latencies recorded on the client side with debug logging on first, and then without it :

!debuglogging.png!  

We generated a flame graph before turning off debug logging that shows the read call stack is dominated by debug logging : 

!flame_graph_snapshot.png!

I've attached the original flame graph for exploration.

Once disabled, the new flame graph shows that the read call stack gets extremely thin, which is further confirmed by client recorded metrics : 

!flame22 nodebug sjk svg.png!

The query pager code has been reworked since 3.0 and it looks like log.debug() calls are gone there, but for 2.2 users and to prevent such issues to appear with default settings, I really think debug logging should be disabled by default.",,adejanovski,arodrime,djoshi,jay.zhuang,jeromatron,jjirsa,jjordan,KurtG,pauloricardomg,tcooke,vinaykumarcse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14326,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/18 16:04;adejanovski;cassandra-2.2-debug.yaml;https://issues.apache.org/jira/secure/attachment/12916415/cassandra-2.2-debug.yaml","16/Mar/18 13:55;adejanovski;debuglogging.png;https://issues.apache.org/jira/secure/attachment/12914871/debuglogging.png","16/Mar/18 14:05;adejanovski;flame22 nodebug sjk svg.png;https://issues.apache.org/jira/secure/attachment/12914868/flame22+nodebug+sjk+svg.png","16/Mar/18 14:05;adejanovski;flame22-nodebug-sjk.svg;https://issues.apache.org/jira/secure/attachment/12914867/flame22-nodebug-sjk.svg","16/Mar/18 13:59;adejanovski;flame22-sjk.svg;https://issues.apache.org/jira/secure/attachment/12914870/flame22-sjk.svg","16/Mar/18 14:00;adejanovski;flame_graph_snapshot.png;https://issues.apache.org/jira/secure/attachment/12914869/flame_graph_snapshot.png",,,,,,,,6.0,adejanovski,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 03 12:45:59 UTC 2018,,,,,,,,,,,"0|i3rem7:",9223372036854775807,2.2.12,,,,,,,,pauloricardomg,,pauloricardomg,,,Normal,,2.2.1,,,,,,,,,,,,,,,,,"16/Mar/18 14:31;adejanovski;Here's the [patch for 2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...thelastpickle:disable-debug-logging-by-default]

It should be mergeable in 3.0/3.11/4.0 without a problem.;;;","16/Mar/18 20:59;vinaykumarcse;+1 on your perf test results. Patch LGTM.;;;","17/Mar/18 18:21;jjordan;CASSANDRA-10241 created that debug.log to be a “production” debug log. If there are things being logged at DEBUG which cause performance issues we should disable those or move them to TRACE, not turn off the debug.log.;;;","17/Mar/18 19:18;adejanovski;Fair enough, I'll go down the read and write path to see if there's debug logging in v3 and above, and switch debug to trace in the query pager in 2.2.;;;","18/Mar/18 19:25;pauloricardomg;Hey Alexander, would you care to re-run the experiments with the query pager DEBUG logs changed to TRACE level to check if debug.log logging overhead will still persist? Thanks!;;;","19/Mar/18 13:22;adejanovski;[~pauloricardomg], thanks for assigning the ticket to me.

Whatever the implementation, I'll run new performance tests and generate flame graphs to verify the impact of the changes, no worries.

I'll wait a bit for a consensus to come out of the discussion on the dev ML before moving on, as there seem to be some conflicting views on what should be done.;;;","19/Mar/18 13:41;jjordan;No matter what is decided we should move those log messages to TRACE.  So I think we can proceed here no matter what.;;;","19/Mar/18 16:10;adejanovski;Fair point. I'll write the patch and run the benchmarks.;;;","19/Mar/18 23:21;jjirsa;If you havent yet started the second run of benchmarks, you may also want to move the debug log statements from ReadCallback.java (one deals with digest mismatch exceptions, forgot what the other is) to trace as well. ;;;","27/Mar/18 16:10;adejanovski;[~jjirsa]: apparently the ReadCallback class already logs at TRACE and not DEBUG on the latest 2.2.

I've created the fix that downgrades debug logging to trace logging in the query pager classes, and here are the results : 

debug on - no fix :
{noformat}
Results:
op rate : 6681 [read_event_1:1109, read_event_2:1119, read_event_3:4452]
partition rate : 6681 [read_event_1:1109, read_event_2:1119, read_event_3:4452]
row rate : 6681 [read_event_1:1109, read_event_2:1119, read_event_3:4452]
latency mean : 19,1 [read_event_1:15,4, read_event_2:15,4, read_event_3:21,0]
latency median : 15,6 [read_event_1:14,2, read_event_2:14,0, read_event_3:16,3]
latency 95th percentile : 39,1 [read_event_1:28,4, read_event_2:28,6, read_event_3:44,2]
latency 99th percentile : 75,6 [read_event_1:52,9, read_event_2:53,6, read_event_3:87,7]
latency 99.9th percentile : 315,7 [read_event_1:101,0, read_event_2:110,1, read_event_3:361,1]
latency max : 609,1 [read_event_1:319,6, read_event_2:315,9, read_event_3:609,1]
Total partitions : 993050 [read_event_1:164882, read_event_2:166381, read_event_3:661787]
Total errors : 0 [read_event_1:0, read_event_2:0, read_event_3:0]
total gc count : 189
total gc mb : 56464
total gc time (s) : 7
avg gc time(ms) : 37
stdev gc time(ms) : 8
Total operation time : 00:02:28{noformat}
 

 

debug off - no fix :
{noformat}
Results:
op rate : 12655 [read_event_1:2141, read_event_2:2093, read_event_3:8422]
partition rate : 12655 [read_event_1:2141, read_event_2:2093, read_event_3:8422]
row rate : 12655 [read_event_1:2141, read_event_2:2093, read_event_3:8422]
latency mean : 10,1 [read_event_1:10,1, read_event_2:10,1, read_event_3:10,1]
latency median : 9,2 [read_event_1:9,2, read_event_2:9,2, read_event_3:9,3]
latency 95th percentile : 15,2 [read_event_1:15,8, read_event_2:15,9, read_event_3:15,7]
latency 99th percentile : 29,3 [read_event_1:44,5, read_event_2:45,1, read_event_3:41,3]
latency 99.9th percentile : 52,7 [read_event_1:67,9, read_event_2:66,9, read_event_3:67,1]
latency max : 268,0 [read_event_1:257,1, read_event_2:263,3, read_event_3:268,0]
Total partitions : 983056 [read_event_1:166311, read_event_2:162570, read_event_3:654175]
Total errors : 0 [read_event_1:0, read_event_2:0, read_event_3:0]
total gc count : 100
total gc mb : 31529
total gc time (s) : 4
avg gc time(ms) : 37
stdev gc time(ms) : 5
Total operation time : 00:01:17{noformat}
 

 

debug on - with fix :
{noformat}
Results:
op rate : 12289 [read_event_1:2058, read_event_2:2051, read_event_3:8181]
partition rate : 12289 [read_event_1:2058, read_event_2:2051, read_event_3:8181]
row rate : 12289 [read_event_1:2058, read_event_2:2051, read_event_3:8181]
latency mean : 10,4 [read_event_1:10,4, read_event_2:10,4, read_event_3:10,4]
latency median : 9,4 [read_event_1:9,4, read_event_2:9,4, read_event_3:9,4]
latency 95th percentile : 16,3 [read_event_1:16,8, read_event_2:17,3, read_event_3:16,2]
latency 99th percentile : 36,6 [read_event_1:44,3, read_event_2:46,6, read_event_3:37,2]
latency 99.9th percentile : 62,2 [read_event_1:78,0, read_event_2:77,1, read_event_3:80,8]
latency max : 251,2 [read_event_1:246,9, read_event_2:249,9, read_event_3:251,2]
Total partitions : 1000000 [read_event_1:167422, read_event_2:166861, read_event_3:665717]
Total errors : 0 [read_event_1:0, read_event_2:0, read_event_3:0]
total gc count : 102
total gc mb : 31843
total gc time (s) : 4
avg gc time(ms) : 38
stdev gc time(ms) : 6
Total operation time : 00:01:21{noformat}
 

 

So we have similar performance with debug logging off and with the fix and debug on.
 The difference in throughput is pretty massive as we roughly get *twice the read throughput* with the fix.

Latencies without the fix and with the fix : 

p95 : 35ms -> 16ms
 p99 : 75ms -> 36ms

I've ran all tests several times, alternating with and without the fix to make sure caches were not making a difference, and results were consistent with what's pasted above.
 It's been running on a single node using an i3.xlarge instance for Cassandra and another i3.large for running cassandra-stress.

 

*One pretty interesting thing to note* is that when I tested with the predefined mode of cassandra-stress, no paging occurred and the performance difference was not noticeable. This is due to the fact that the predefined mode generates COMPACT STORAGE tables, which involve a different read path (apparently). I think anyone performing benchmarks for Cassandra changes should be aware that the predefined mode isn't relevant and that a user defined test should be used (maybe we should create one that would be used as standard benchmark). 
 Here's the one I used : [^cassandra-2.2-debug.yaml]

With the following commands for writing : 

 
{noformat}
/usr/bin/cassandra-stress user profile=/home/ec2-user/cassandra-2.2-debug.yaml n=1000000 'ops(insert=1)' cl=LOCAL_ONE no-warmup -node 172.31.31.42 -mode native cql3 compression=lz4 -rate threads=128{noformat}
 

And for reading : 

 
{noformat}
/usr/bin/cassandra-stress user profile=/home/ec2-user/cassandra-2.2-debug.yaml n=1000000 'ops(read_event_1=1,read_event_2=1,read_event_3=4)' cl=LOCAL_ONE no-warmup -node 172.31.31.42 -mode native cql3 compression=lz4 -rate threads=128{noformat}
 

[~pauloricardomg] : here's the [patch|https://github.com/apache/cassandra/compare/cassandra-2.2...thelastpickle:CASSANDRA-14318] if you're willing to review/commit it, and the unit test results in [CircleCI|https://circleci.com/gh/thelastpickle/cassandra/178].;;;","27/Mar/18 16:57;adejanovski;For the record, the same tests on 3.11.2 didn't show any notable performance difference between debug on and off : 

Cassandra 3.11.2 debug on : 
{noformat}
Results:
Op rate : 18 777 op/s [read_event_1: 3 165 op/s, read_event_2: 3 109 op/s, read_event_3: 12 562 op/s]
Partition rate : 6 215 pk/s [read_event_1: 3 165 pk/s, read_event_2: 3 109 pk/s, read_event_3: 0 pk/s]
Row rate : 6 215 row/s [read_event_1: 3 165 row/s, read_event_2: 3 109 row/s, read_event_3: 0 row/s]
Latency mean : 6,7 ms [read_event_1: 6,7 ms, read_event_2: 6,7 ms, read_event_3: 6,6 ms]
Latency median : 5,0 ms [read_event_1: 5,0 ms, read_event_2: 5,0 ms, read_event_3: 4,9 ms]
Latency 95th percentile : 15,6 ms [read_event_1: 15,5 ms, read_event_2: 15,9 ms, read_event_3: 15,5 ms]
Latency 99th percentile : 43,3 ms [read_event_1: 42,7 ms, read_event_2: 44,2 ms, read_event_3: 43,2 ms]
Latency 99.9th percentile : 82,0 ms [read_event_1: 80,3 ms, read_event_2: 82,4 ms, read_event_3: 82,1 ms]
Latency max : 272,4 ms [read_event_1: 272,4 ms, read_event_2: 268,7 ms, read_event_3: 245,1 ms]
Total partitions : 330 970 [read_event_1: 165 386, read_event_2: 165 584, read_event_3: 0]
Total errors : 0 [read_event_1: 0, read_event_2: 0, read_event_3: 0]
Total GC count : 42
Total GC memory : 13,102 GiB
Total GC time : 1,8 seconds
Avg GC time : 42,4 ms
StdDev GC time : 1,3 ms
Total operation time : 00:00:53{noformat}
 


Cassandra 3.11.2 debug off : 
{noformat}
Results:
Op rate : 18 853 op/s [read_event_1: 3 138 op/s, read_event_2: 3 137 op/s, read_event_3: 12 578 op/s]
Partition rate : 6 275 pk/s [read_event_1: 3 138 pk/s, read_event_2: 3 137 pk/s, read_event_3: 0 pk/s]
Row rate : 6 275 row/s [read_event_1: 3 138 row/s, read_event_2: 3 137 row/s, read_event_3: 0 row/s]
Latency mean : 6,7 ms [read_event_1: 6,7 ms, read_event_2: 6,7 ms, read_event_3: 6,7 ms]
Latency median : 5,0 ms [read_event_1: 5,1 ms, read_event_2: 5,1 ms, read_event_3: 5,0 ms]
Latency 95th percentile : 15,5 ms [read_event_1: 15,5 ms, read_event_2: 15,6 ms, read_event_3: 15,4 ms]
Latency 99th percentile : 39,9 ms [read_event_1: 41,0 ms, read_event_2: 39,6 ms, read_event_3: 39,6 ms]
Latency 99.9th percentile : 73,3 ms [read_event_1: 73,4 ms, read_event_2: 71,6 ms, read_event_3: 73,6 ms]
Latency max : 367,0 ms [read_event_1: 240,5 ms, read_event_2: 250,3 ms, read_event_3: 367,0 ms]
Total partitions : 332 852 [read_event_1: 166 447, read_event_2: 166 405, read_event_3: 0]
Total errors : 0 [read_event_1: 0, read_event_2: 0, read_event_3: 0]
Total GC count : 46
Total GC memory : 14,024 GiB
Total GC time : 2,0 seconds
Avg GC time : 42,7 ms
StdDev GC time : 3,9 ms
Total operation time : 00:00:53{noformat}
The improvement over 2.2 is nice though :)

 ;;;","30/Mar/18 15:18;pauloricardomg;{quote}I think anyone performing benchmarks for Cassandra changes should be aware that the predefined mode isn't relevant and that a user defined test should be used (maybe we should create one that would be used as standard benchmark).
{quote}
Good find! Can you check if this is the case in trunk, and if so maybe open a lhf ticket to change that?
{quote}For the record, the same tests on 3.11.2 didn't show any notable performance difference between debug on and off
{quote}
Nice to know we managed to handle all debug/verbose log leaks there. It will be easier to maintain this after CASSANDRA-14326.
{quote}here's the patch if you're willing to review/commit it, and the unit test results in CircleCI.
{quote}
Thanks for the patch, experiments and analysis! Even though 2.2 is on critical fixes only mode, 50% is a significant performance hit on throughput for this workload, and since the patch is pretty simple I don't see a reason not to commit it.

CI looks good. I added a CHANGES.txt not and committed as {{ac77e5e7742548f7c7c25da3923841f59d4b2713}} to cassandra-2.2 branch.;;;","03/Apr/18 12:45;adejanovski;Thanks for reviewing and merging [~pauloricardomg] !

CASSANDRA-10857 removed compact storage options in trunk and the standard1&counter1 tables are no longer using it : [https://github.com/apache/cassandra/commit/07fbd8ee6042797aaade90357d625ba9d79c31e0#diff-e5d5cb263c5c84c322cd09391af46d7dL141] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ThrottledUnfilteredIterator failed on UnfilteredRowIterator with only partition level info,CASSANDRA-14315,13145298,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasonstack,jasonstack,jasonstack,15/Mar/18 07:57,15/May/20 08:03,13/Jul/23 08:37,20/Mar/18 02:22,4.0,4.0-alpha1,,,,,Feature/Materialized Views,,,,0,,,,"When repairing base table with MV, in order to avoid OOM, Cassandra-13299 added ThrottledUnfilteredIterator to split large partition into small chunks, but it didn't handle partition without unfiltered properly.

{code:title=repro}
// create cell tombstone, range tombstone, partition deletion
createTable(""CREATE TABLE %s (pk int, ck1 int, ck2 int, v1 int, v2 int, PRIMARY KEY (pk, ck1, ck2))"");
// partition deletion
execute(""DELETE FROM %s USING TIMESTAMP 160 WHERE pk=1"");

// flush and generate 1 sstable
ColumnFamilyStore cfs = Keyspace.open(keyspace()).getColumnFamilyStore(currentTable());
cfs.forceBlockingFlush();
cfs.disableAutoCompaction();
cfs.forceMajorCompaction();

assertEquals(1, cfs.getLiveSSTables().size());
SSTableReader reader = cfs.getLiveSSTables().iterator().next();

try (ISSTableScanner scanner = reader.getScanner();
        CloseableIterator<UnfilteredRowIterator> throttled = ThrottledUnfilteredIterator.throttle(scanner, 100))
{
    assertTrue(throttled.hasNext());
    UnfilteredRowIterator iterator = throttled.next();
    assertFalse(throttled.hasNext());
    assertFalse(iterator.hasNext());
    assertEquals(iterator.partitionLevelDeletion().markedForDeleteAt(), 160);
}
{code}",,jasonstack,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/18 10:20;jasonstack;dtest.png;https://issues.apache.org/jira/secure/attachment/12915107/dtest.png","19/Mar/18 10:20;jasonstack;unit test.png;https://issues.apache.org/jira/secure/attachment/12915108/unit+test.png",,,,,,,,,,,,2.0,jasonstack,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 20 03:00:07 UTC 2018,,,,,,,,,,,"0|i3rbyf:",9223372036854775807,,,,,,,,,pauloricardomg,,pauloricardomg,,,Normal,,,,,,,,,,,,,,,,,,,"15/Mar/18 07:58;jasonstack;| [trunk|https://github.com/jasonstack/cassandra/commits/throttle-trunk]|
| [dtest|https://github.com/jasonstack/cassandra-dtest/commits/throttle-mv] |

Changes:
1. when batch size is 0, don't throttle and return itself. (opt out)
2. when the original UnfilteredRowIterator has only partition level info, return itself once.
3. removed the assertion ""UnfilteredPartitionIterator should not contain empty partitions"";;;","20/Mar/18 02:22;pauloricardomg;Good catch! Patch and tests LGTM. Committed as {{5b9e985474e696a83d23e7cf4bedaf360cdb1eaf}} to trunk. Thanks!;;;","20/Mar/18 02:43;pauloricardomg;Committed dtest as {{2c1b986bc82ad29a4db06158043aceaaf473e17c}}.;;;","20/Mar/18 03:00;jasonstack;Thanks for the review!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix argument passing for SSLContext in trunk,CASSANDRA-14314,13145277,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,djoshi,djoshi,djoshi,15/Mar/18 05:09,15/May/20 07:59,13/Jul/23 08:37,22/Mar/18 13:42,4.0,4.0-alpha1,,,,,,,,,0,security,,,Argument passing has a minor bug while creating the SSLContext. Audit and make sure that the client & server SSL contexts are created at appropriate locations.,,djoshi,jasobrown,jeromatron,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,djoshi,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 22 13:42:11 UTC 2018,,,,,,,,,,,"0|i3rbtr:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"15/Mar/18 05:18;djoshi;[~jasobrown] - please review.

||sslfactory||
|[branch|https://github.com/dineshjoshi/cassandra/tree/fix-args-to-sslfactory]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/fix-args-to-sslfactory]|
||;;;","15/Mar/18 18:49;jasobrown;[~djoshi3] and I spoke offline about this, and we uncovered some futher incorrectness with {{SSLFactory.getSslContext()}}. We'll have an updated patch soon.;;;","17/Mar/18 07:14;djoshi;Hi [~jasobrown], I have updated the branch with a bunch of changes. Here's a short rundown of the changes -
 # Removed the {{serverSslContext}} and {{clientSslContext}} {{AtomicReference}}
 # Introduced a new field {{SocketType}} - so now you can create a combination of (PEER, CLIENT) \{{ConnectionType}} and (SERVER, CLIENT) {{SocketType}}
 # Netty SSL contexts are cached in a {{ConcurrentHashMap}}. I haven't currently implemented any strategy to prune or reset this map. My expectation is in the steady state this map should not grow.
 # Hot reloading is updated to use this map.;;;","17/Mar/18 23:38;djoshi;||sslfactory||
|[branch|https://github.com/dineshjoshi/cassandra/tree/fix-args-to-sslfactory]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/fix-args-to-sslfactory]|
||;;;","18/Mar/18 00:22;djoshi;Squashed the commit - b9836dc07560ffa03eb6fb3902a2d96c3ff5715e;;;","21/Mar/18 18:43;jasobrown;I've made a few changes to [~djoshi3]'s branch, which was a great step in the right direction, and pushed up to my repo:
||sslfactory||
|[branch|https://github.com/jasobrown/cassandra/tree/fix-args-to-sslfactory]|
|[utests & dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/fix-args-to-sslfactory]|
 - I renamed {{ConnectionType.CLIENT}} to {{ConnectionType.NATIVE_PROTOCOL}} (and {{ConnectionType.PEER}} to {{ConnectionType.INTERNODE_MESSAGING}}) as it was just confusing to me what 'client' vs 'server' really meant. The change ends up reading very well, especially at call sites.
 - as a petty readability change, I replaced {{SSLFactory.HotReloadableFile.Type}} with {{ConnectionType}}.
 - {{SSLFactory.CacheKey}} - I moved the {{equals}}/{{hashCode}} logic for {{EncryptionOptions}} fields into {{equals}}/{{hashCode}} functions on {{EncryptionOptions}} itself.
 - {{Server}} - {{ConnectionType}} should be {{CLIENT}} (now {{NATIVE_PROTOCOL}}) as this is where we start the server-side of the client-facing native protocol.
 - {{NettyFactory.OutboundInitializer.initChannel}} - {{ConnectionType}} should be {{PEER}} (now {{INTERNODE_MESSAGING}}) as this is where we start the client-side of a internode messaging connection.
 - {{SSLFactory.getSslContext}} - when {{cachedSslContexts.putIfAbsent}} is called, if the value is not null, we should return that value rather than using the instance we just created. That way we reuse the 'winning' context.
 - {{SSLFactory.createNettySslContext}} - I'm not sure we need to check if {{options.enabled}}. Actually, I'm not sure if there's a strong argument for why we should ever not load the keystore. In 3.0, we always loaded the keystore. wdyt?
 - Last, while I can't (or don't want to) rename the yaml properties ({{client_encryption_options}}/{{server_encryption_options}}), I can clean up the code that refers to them. Hence, in DD I've changed {{getServerEncryptionOptions}} to {{getInternodeMessagingEncryptonOptions}} and so on. This make the call sites to get that data more obvious, as well. This change is not a requirement for this patch, but helps clarify the code.;;;","21/Mar/18 20:07;djoshi;[~jasobrown] a few comments -

* {{SSLFactory}} line 327 info message should read - ""SSL certificates have been updated. Reseting the ssl contexts for new connections."". Please drop the word ""peer"".
* {{HotReloadableFile}} we can get rid of {{isServer}} and {{isClient}} methods and associated code as we're not making any distinctions any more.

Rest looks good.;;;","22/Mar/18 13:42;jasobrown;committed, with minor nits addressed, as sha {{11496039fb18bb45407246602e31740c56d28157}}. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh: ssl setting not read from cqlshrc in 3.11,CASSANDRA-14299,13143585,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,spod,tgbeck,tgbeck,08/Mar/18 15:38,16/Apr/19 09:29,13/Jul/23 08:37,17/Apr/18 08:58,3.11.3,,,,,,,,,,0,,,,"With CASSANDRA-10458 an option was added to read the {{--ssl}} flag from cqlshrc, however the commit seems to have been incorrectly merged or the changes were dropped somehow.

Currently adding the following has no effect:
{code:java}
[connection]
ssl = true{code}
When looking at the current tree it's obvious that the flag is not read: [https://github.com/apache/cassandra/blame/cassandra-3.11/bin/cqlsh.py#L2247]

However it should have been added with [https://github.com/apache/cassandra/commit/70649a8d65825144fcdbde136d9b6354ef1fb911]

The values like {{DEFAULT_SSL = False}}  are present, but the {{option_with_default()}} call is missing.

Git blame also shows no change to that line which would have reverted the change.",,jay.zhuang,tgbeck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,spod,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 17 08:58:42 UTC 2018,,,,,,,,,,,"0|i3r1tz:",9223372036854775807,,,,,,,,,jay.zhuang,,jay.zhuang,,,Normal,,,,,,,,,,,,,,,,,,,"09/Mar/18 13:31;spod;Looks like a merge oversight in [6d429cd|https://github.com/apache/cassandra/commit/6d429cd]. Trivial fix is linked, do you mind take a look [~ifesdjeen]?;;;","13/Apr/18 20:19;jay.zhuang;+1;;;","17/Apr/18 08:58;spod;Merged as 845243db93a5add273f6e on 3.11

Thx Jay!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlshlib tests broken on b.a.o,CASSANDRA-14298,13143566,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ptbannister,spod,spod,08/Mar/18 14:32,15/May/20 13:47,13/Jul/23 08:37,15/May/20 13:47,4.0,4.0-alpha1,,,,,Build,Legacy/Testing,,,0,cqlsh,dtest,pull-request-available,"It appears that cqlsh-tests on builds.apache.org on all branches stopped working since we removed nosetests from the system environment. See e.g. [here|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-trunk-cqlsh-tests/458/cython=no,jdk=JDK%201.8%20(latest),label=cassandra/console]. Looks like we either have to make nosetests available again or migrate to pytest as we did with dtests. Giving pytest a quick try resulted in many errors locally, but I haven't inspected them in detail yet. ",,aweisberg,cscotta,djoshi,jasobrown,jay.zhuang,jmckenzie,marcuse,mkjellman,ptbannister,,,,,,,,,,,,,,,,,"dineshjoshi commented on pull request #45: CASSANDRA-14298 Get cqlsh tests passing and temporarily disable tests dependent on cqlshlib
URL: https://github.com/apache/cassandra-dtest/pull/45
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Feb/19 06:43;githubbot;600","dineshjoshi commented on pull request #45: CASSANDRA-14298 Get cqlsh tests passing and temporarily disable tests dependent on cqlshlib
URL: https://github.com/apache/cassandra-dtest/pull/45
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Mar/19 17:45;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,CASSANDRA-10190,,,,,,,,CASSANDRA-9430,,"02/Mar/19 04:50;ptbannister;CASSANDRA-14298-blogposts-and-ratefile-workarounds.txt;https://issues.apache.org/jira/secure/attachment/12960858/CASSANDRA-14298-blogposts-and-ratefile-workarounds.txt","17/Feb/19 04:14;ptbannister;CASSANDRA-14298.txt;https://issues.apache.org/jira/secure/attachment/12958998/CASSANDRA-14298.txt","29/Apr/18 02:59;ptbannister;cqlsh_tests_notes.md;https://issues.apache.org/jira/secure/attachment/12921168/cqlsh_tests_notes.md",,,,,,,,,,,3.0,ptbannister,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 15 13:47:26 UTC 2020,,,,,,,,,,,"0|i3r1pr:",9223372036854775807,,,,,,,,,,,djoshi,spod,,Normal,,4.0,,,,,,,,,,,,,,,,,"13/Apr/18 02:26;ptbannister;[~spodxx@gmail.com], [~krummas], I'm interested in taking on the cqlsh tests if you aren't already working on them. It looks like there's a mix of different things going on. I've started looking into it, I'll post details as they become clear.;;;","13/Apr/18 07:23;spod;Maybe you would also be interested to review CASSANDRA-14299, Patrick?;;;","14/Apr/18 01:18;ptbannister;[~spodxx@gmail.com], thanks for thinking of me, but it looks like somebody else already took care of it.;;;","16/Apr/18 03:14;ptbannister;I'm steadily progressing on this - I've fixed almost everything in cqlsh_tests/cqlsh_tests.py. Findings so far for cqlsh_tests.py are attached in cqlsh_tests_notes.md.;;;","17/Apr/18 02:22;ptbannister;All tests in cqlsh_tests/cqlsh_tests.py pass on trunk, cassandra-3.11, and cassandra-3.0 on Ubuntu 16.04 LTS.;;;","18/Apr/18 20:54;ptbannister;I spent last night reviewing cqlsh_tests/cqlsh_copy_tests.py. There's a lot of similarity between them, but they're numerous. It will take some work to refactor them for the pytest framework.;;;","18/Apr/18 21:28;spod;You do use [nose2pytest|https://github.com/pytest-dev/nose2pytest] for converting the tests to pytest, are you? It should be able to convert most of the code automatically.;;;","20/Apr/18 02:28;ptbannister;For cqlsh_tests/cqlsh_copy_tests.py, It looks like our workaround for importing functions from the cassandra cqlshlib no longer works. Since cqlshlib pointedly does not work in Python 3, this will be an interesting problem to solve. There are 25 tests that depend on functions imported from cqlshlib, which is about a third of the copy tests.;;;","22/Apr/18 21:56;ptbannister;I've considered a few approaches to salvaging the cqlsh copy tests that depend on cassandra cqlshlib's CSV formatting functions. This mostly concerns the function assertCsvResultEqual in cqlsh_tests/cqlsh_copy_tests.py::TestCqlshCopy. There are 25 tests that use it - they get a query result set, call COPY TO to export the results to CSV, call cqlshlib's CSV formatting functions to format the same result set to CSV, and assert that the two CSV files are equal. It's broken right now because cqlshlib isn't compatible with Python 3.

The approach I'm following right now is to add a Python 2.7 compatible script that can import cqlshlib and reuse most of the CSV formatting previously implemented in the cqlsh copy tests. We'll run it with subprocess, pipe pickled query results data to its stdin, and pipe CSV formatted data back out. The new CSV formatting utility would run in a separate virtualenv. The biggest disadvantage of this approach is that it adds Python 2.7 as an additional native dependency.

If this is found to be unacceptable, I think the most realistic alternative would be to rewrite the impacted tests to use different assertions. Most of the impacted tests look like they're fundamentally about edge cases for data types rather than CSV formatting.;;;","23/Apr/18 12:57;spod;""It's broken right now because cqlshlib isn't compatible with Python 3.""

We'll have to address this at some point anyways. Time's probably better spend migrating code to Python 3, instead of creating workarounds to make the tests run with Python 2 again.;;;","23/Apr/18 13:08;jasobrown;bq. Time's probably better spend migrating code to Python 3, instead of creating workarounds to make the tests run with Python 2 again.

I agree with this, only casually observing this ticket. Is it worth bringing up on the dev@ ML? Or should we just make this ticket ""bring all cqlsh-related things up to Python 3?"";;;","24/Apr/18 01:10;ptbannister;I agree that porting C* cqlsh to Python 3 is inevitable. This ticket probably isn't a good reason to do it, but there are other better reasons to do it, such as the pending end of life for Python 2.

[~jasobrown], I suspect you were more asking Stefan than me, but I agree that the question of porting cqlsh to Python 3 should be discussed on the list. It would be worth some discussion on how to do it. For example, do we want to go all the way to Python 3, or would we prefer it to be 2/3 cross-compatible? And, how far back are we going to port - would we go back to 3.0, since we're still supporting it until six months after 4.0 is released?

I suggest that if we're going to start a Python 3 epic, we should do it in a separate ticket, and make this ticket a subticket under it.

Dialing back the scope to just the cqlsh tests - keep in mind that this problem only impacts a third of the cqlsh copy tests. I have everything in cqlsh_tests/cqlsh_tests.py working fine right now, nothing in that set of tests depends on the C* cqlshlib.

However, for the impacted copy tests, we can only completely avoid these ugly workarounds if we port cqlshlib to Python 3 not only in trunk, but also in all other supported versions. Any branch of C* left behind on Python 2.7 will either have to be skipped for the copy tests, or else tested through some kind of alternate approach such as the awful hack I'm working on right now.

 ;;;","24/Apr/18 15:22;spod;{quote}For example, do we want to go all the way to Python 3, or would we prefer it to be 2/3 cross-compatible?
{quote}
Being cross-compatible would be the most convenient option for users I guess. But I haven't really any experience converting Python 2 code in such a way. Would you up to give this a try, Patrick?
{quote}I agree with this, only casually observing this ticket. Is it worth bringing up on the dev@ ML?
{quote}
Sure. Maybe we can bring it up with an actual proposal after checking our options.
{quote}However, for the impacted copy tests, we can only completely avoid these ugly workarounds if we port cqlshlib to Python 3 not only in trunk, but also in all other supported versions.
{quote}
Depending on how invasive those changes will turn out to be, we can discuss patching 3.11, but I don't think that's going to happen for older branches. Anyone running Cassandra 2.x/3.0 should be able to keep using Python 2 until EOL 2020, which should be well past the Cassandra EOL date.
{quote}Any branch of C* left behind on Python 2.7 will either have to be skipped for the copy tests, or else tested through some kind of alternate approach such as the awful hack I'm working on right now.
{quote}
We currently run 0% tests on b.a.o. If we can get all but the copy tests working in a straight forward way, that would be a good start.;;;","24/Apr/18 21:35;ptbannister;If you'd like to set aside the cqlsh copy tests that depend on cqlshlib for now, I can regroup and focus on getting everything else working. I can include a modification to pytest_collection_modifyitems in conftest.py to deselect the impacted tests, similar to what we're doing with the upgrade tests.

I'll investigate the possibility of porting cqlshlib for 2/3 cross compatibility. I personally prefer a complete transition to Python 3, but I suspect the community would be more comfortable with a cross compatible implementation.;;;","25/Apr/18 14:21;spod;If you have a patch to get some of the cqlsh dtests work again with Python 3 and pytest, that would be great. We can get back to the then temporarily disabled dtest leftovers after updating cqlshlib.

As for doing a full Python 3 transition or keeping the code cross compatible, I don't know. Guess it depends on how much work it is and how big the change set is going to be. If we can get away with a few future imports and a couple of changed print statements, we can even consider patching 3.11. But if it's too much work, or to hard to be compatible, we might as well fully migrate trunk to Python 3. But's thats something we should discuss on dev- based on your findings.

 ;;;","29/Apr/18 02:59;ptbannister;I had to do a little extra work to update a few tests for CASSANDRA-13910 and CASSANDRA-13985, but a patch is ready.

It passes on trunk, cassandra-3.11, and cassandra-3.0 on a 64-bit Ubuntu 16.04 LTS platform.

I've attached new notes summarizing the changes.;;;","29/Apr/18 03:01;ptbannister;Patch for cassandra-dtest master branch.;;;","02/May/18 08:42;spod;Nice work! Just found a single change to clarify, before spinning up tests runs.
{quote} 
 ""Many tests were failing because they were asserting that some string was equal to content in stdout or stderr piped from subprocess, which was a bytes""
{quote}
Having both a local {{run_cqlsh}} method returning bytes and ccm's {{node.run_cqlsh}} returning strings, is rather confusing and error prone. Can't we convert the local method to return strings as well, just to keep things consistent and spare us from carefully b' prefixing string literals in our assertions?;;;","03/May/18 01:16;ptbannister;ccm returns strings because it opens its subprocess with universal_newlines=True. Probably we can modify our local run_cqlsh to do the same - I'll look into it.;;;","04/May/18 02:46;ptbannister;Yes! I've modified test_cqlsh/test_cqlsh.py::TestCqlsh::run_cqlsh to open the cqlsh subprocess with universal_newlines=True and that allowed us to remove all bytes and calls to encode() and decode() from functions using our local run_cqlsh. We're still passing on trunk, 3.11, and 3.0.

I posted a new patch, CASSANDRA-14298.txt. I also re-posted the previous patch as CASSANDRA-14298_old.txt, in case it's needed for some reason.;;;","08/May/18 12:14;spod;I was giving your patch a try on builds.apache.org against cassandra-3.0:
[https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/545/#showFailuresLink]

Looks like there's still an encoding issue while piping input to the cqlsh subprocess. Except that, most of the tests seem to pass.;;;","08/May/18 21:33;ptbannister;The encoding issue sounds no good - I'll take another look.;;;","09/May/18 00:40;ptbannister;The encoding problem is environmental. Python subprocesses use whatever is returned by locale.getpreferredencoding(). We can do this on a Debian based platform (such as Ubuntu) by setting LC_CTYPE='C.UTF-8'.

We could simply say that this environment setting is a prerequisite for the cqlsh_tests, but I think we can do one better:

 
{code:java}
class TestCqlsh(Tester):

    @classmethod
    def setUpClass(cls):
        cls._cached_driver_methods = monkeypatch_driver()
        os.environ['LC_CTYPE'] = 'C.UTF-8' # override environment locale setting to prefer UTF-8 encoding

{code}
 

I have to admit I don't know what this will do on Windows, but I've tested it on my Ubuntu environment and it works fine.

I'll issue a new patch with these additions. I'll also re-post the current patch as the ""old"" patch, if you'd prefer to stick with it and just declare that this environment variable setting is a prerequisite to running these tests.

 ;;;","09/May/18 00:52;ptbannister;Running some final checks before posting updated patch.;;;","09/May/18 02:32;ptbannister;Posted a new patch that includes test fixture setup in cqlsh_tests/cqlsh_tests.py to set environment variable LC_CTYPE='C.UTF-8'.;;;","16/May/18 00:49;ptbannister;I've made a lot of progress porting cqlshlib to Python 3. Along the way I've been taking notes on all the areas that I think would require extra effort for cross compatibility with Python 2.

I don't have a complete plan yet, but I have some observations.

In terms of level of effort and complexity, this is not going to be as simple as running 2to3 and then adding a few imports from future and six. However, we won't need to rearchitect the library either. So far I've found that existing classes and functions work with just a few tweaks to their implementation, mostly around IO and strings vs. bytes.

The biggest challenge, regardless of whether we go straight Python 3 or cross-compatible, is going to be adequately testing the result. The cqlshlib unittests and the cqlsh_tests have been useful to help find bugs, but I'm not confident that our tests have enough code coverage to exercise everything. We would need a strategy for more comprehensive testing.

Some specifics:
 * The SaferScanner class in saferscanner.py requires a slightly different implementation in Python 2 vs. Python 3, because of changes in the internals of the re module for regular expressions.
 * copyutil.py, formatting.py, and displaying.py have needed the most work so far, since they have a lot of IO and serialization.
 * The formatter for blobs in formatting.py needs a different implementation in Python 2 vs. Python 3, because of changes in the behavior of binascii.hexlify.

Edited: removed some off-topic discussion of issues encountered while testing.;;;","17/May/18 11:45;spod;The latest changes setting LC_CTYPE explicitly from the python code doesn't seem to do the trick ([CircleCI|https://circleci.com/gh/krummas/cassandra/485#tests/containers/25]). Adding {{LC_CTYPE=""C.UTF-8""}} to the environment instead seems to be cleaner anyways. WDYT [~krummas]?

As for your remarks regard the cqlshlib Python3 migration, maybe it's a good time to move this discussion to the -dev mailing list and discuss how we move on with the testing code in general?;;;","17/May/18 13:04;marcuse;bq. Adding LC_CTYPE=""C.UTF-8"" to the environment instead seems to be cleaner anyways.
yeah, sounds good, and it almost fixes the cqlsh tests: https://circleci.com/gh/krummas/cassandra/489 (running on this: https://github.com/krummas/cassandra/commits/CASSANDRA-14298);;;","18/May/18 00:03;ptbannister;Too bad the programmatic environment setting didn't work! It must be environmental - I'll have to start testing my patches on multiple flavors of Linux to help catch these kind of issues.

As for taking the cqlshlib porting question to the dev list, I agree that now is a good time. Did you plan for you or Marcus to write the email, or would you like me to write it?;;;","19/May/18 00:59;ptbannister;Regarding my concerns about code coverage - once I get the cqlshlib unit tests and dtests working, I'll experiment with [coverage.py|https://coverage.readthedocs.io/en/coverage-4.5.1/] and see if I can get a useful analysis of our code coverage. This would help us plan where to spend effort doing extra testing.;;;","21/May/18 22:26;ptbannister;I think I need to retract my recommendation to use LC_CTYPE=C.UTF-8. I learned this weekend that the C.UTF-8 locale is somewhat specific to Debian. (It's also available on more recent versions of Fedora, as an optional add-on.)

I recommended it initially because it's more internationalization friendly than picking a single language such as en_US.UTF-8. Unfortunately, since it's specific to the Debian family, I think that makes it a poor choice for testing.

For the lack of a better solution, I recommend we use LC_CTYPE=en_US.UTF-8.

Also - I'm working on standing up a RHEL 7.5 instance on AWS to test my work on a different environment, to make sure there aren't more hidden environmental dependencies like this.

Separately, as an update on the cqlshlib porting work: my forks of cassandra and cassandra-dtest have cqlshlib3 branches with cqlshlib ported to straight Python 3, with all cqlshlib unittests and all dtest cqlsh_tests passing, except for test_describe (in test_cqlsh_output.py in the cqlshlib unit tests) and test_unusual_dates (in cqlsh_tests.py in the dtests). I still want to try to measure coverage (not sure how that's going to work with the dtests but it should be doable with the unittests), and I definitely want to test these on RHEL or some other non-Debian environment; I'll continue with that work this week.

 ;;;","24/May/18 02:49;ptbannister;I switched to CentOS instead of RHEL, since RHEL requires a license to get access to the packages for many of the C* dependencies.

Things look okay for the dtests on CentOS for these modifications using LC_CTYPE=en_US.utf8. That includes the cqlshlib port to Pyfhon 3 too.

There are some extensive failures for the cqlshlib unittests in CentOS. cqlshlib/test/run_cqlsh.py::ProcRunner::read_until() can't handle ANSI control characters for colors in the prompt. I was able to modify the regular expression to handle the escape sequences, and with that change, the tests pass. But I won't be happy with that solution until I figure out why these escape characters are showing up in CentOS but not in my usual Ubuntu environment. The inputrc and bashrc files are a bit different, but I haven't figured out yet which difference, if any, is making the CentOS cqlsh prompt include colors.;;;","24/May/18 19:54;ptbannister;[~spodxx@gmail.com], I've been cluttering up this ticket with lots of discussion of testing the cqlshlib port to Python 3, but I think I'd like to move that to a new ticket and try to close up this one according to our original plan: deselect tests that depend on cqlshlib and fix everything else.

I'm testing a new CASSANDRA-14298 patch that will try to use LC_CTYPE=en_US.UTF-8. If it works on Ubuntu 16.04 LTS and CentOS, I'll submit it. Does all that sound good to you?;;;","24/May/18 23:24;ptbannister;Testing modifications that should work on Red Hat variants in addition to Ubuntu.;;;","25/May/18 03:00;ptbannister;Please see attachment CASSSANDRA-14298.txt for a new patch to cassandra-dtest that passes on trunk, cassandra-3.11, and cassandra-3.0 on CentOS 7.5 and Ubuntu 16.04 LTS on AWS t2.large instances.

In this patch, the setUpClass() function for cqlsh_tests.py::TestCqlsh and cqlsh_copy_tests.py::TestCqlshCopy checks the system preferred encoding with locale.getpreferredencoding(), and if the result isn't UTF-8, it sets LC_CTYPE=en_US.utf8. This locale is more widely available.

On systems where en_US.utf8 isn't available, the tests will fail unless some other UTF-8 locale is set.

I've noticed that the bulk round trip tests in cqlsh_tests/cqlsh_copy_tests::TestCqlshCopy are flaky. When they've failed, they've consistently passed when rerun.;;;","25/May/18 17:09;spod;I'm going to continue testing on builds.apache.org next week. Although I think I'll just add LC_ALL=en_US.UTF-8 to the wrapper script, so we don't have to change the environment programmatically from python.;;;","25/May/18 20:02;ptbannister;OK - I left up the ""old"" patch that didn't change the environment variable, if you prefer that approach.

I'm going to move any further comments of porting cqlshlib to CASSANDRA-10190. I also plan to email the dev list about it after I put together some kind of code coverage analysis.;;;","27/May/18 02:14;ptbannister;I just noticed [this discussion of this ticket|https://wilderness.apache.org/channels/?f=cassandra-dev/2018-05-17] on the #cassandra-dev IRC channel. Would you like me to reissue this patch with the files renamed to test_cqlsh.py and test_cqlsh_copy.py?;;;","28/May/18 12:59;spod;I've now [pushed a commit|https://github.com/apache/cassandra-builds/commit/8f796c66896f8c889c8d654a90b35d3b7d03efb4] that adds the LC_CTYPE variable to the dtest wrapper script.

Tests have also been restarted to get the results before and after your patch:
 * [3.0|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-3.0-dtest/]
 * [3.11|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-3.11-dtest/]
 * [trunk|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-trunk-dtest/]

[~ptbannister], yes, please rename the files so they will get picked up by pytest when running a directory.

As you're probably aware, there have been code freeze dates floating around for 4.0. Usually before releases starting to happen, we also try to concentrate more on getting tests to successfully pass. And there are plenty of dtests issues at the moment. But as this is still a bit ahead, it's not a top priority for me right now, so I'm not always able to catch up and respond as quickly as I should here. Sorry about that.

As for porting cqlshlib to Python 3, this is probably a good time to clarify in which direction we're heading. But there needs to be broader support among devs, if we decide to support Python 3 in one way or another. Bringing this to the dev mailing list would be the right thing to do at this point. Please feel free to reach out to the ML with any suggestions on that if you like. Thanks!;;;","28/May/18 23:25;ptbannister;[~spodxx@gmail.com], no worries about having to multitask, I appreciate the significant time you've put into working with me on this ticket.

I'll update the patch to rename the files.

I'll email the dev list about porting as soon as I finish coverage analysis (working actively on that today).;;;","30/May/18 01:35;ptbannister;[~spodxx@gmail.com] - I attached a new patch (CASSANDRA-14298.txt) with the files cqlsh_tests.py and cqlsh_copy_tests.py renamed to test_cqlsh.py and test_cqlsh_copy.py, respectively.

The patch still has a check in the setUp fixture that adds LC_CTYPE=en_US.utf8 to the environment if locale.getpreferredencoding() is not UTF-8. If you feel strongly that we should not do this, I'd be willing to remove it, but I recommend we keep it. For people who run outside of the cassandra-builds dtest wrapper environment, this may prevent some configuration headaches during testing.;;;","03/Aug/18 08:46;spod;I've been able to get a dtest run to finish, after getting back to this issue.

https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/597/testReport/

Looks like there are still encoding issues left and {{TestCqlsh.setUpClass}} isn't executed at all without an {{@pytest.fixture}} annotation. Even when it does, it doesn't fix the error for me locally.;;;","04/Aug/18 01:55;ptbannister;[~spodxx@gmail.com], great to hear from you again. I'll start looking into this.

I've been on a break for a couple of weeks, and coming back to it now, I can't get ccm to start - I get the error ""[node1 ERROR] b'Error: Could not find or load main class org.apache.cassandra.service.CassandraDaemon'."" Did we add a new dependency or something?;;;","15/Aug/18 02:07;ptbannister;[~spodxx@gmail.com], I built a CentOS system, checked out your copy of CASSANDRA-14298, and ran some of the tests that failed on build 597. I have LANG=en_US.utf8 and nothing set for LC_ALL or LC_CTYPE. Almost all of the Unicode related tests passed:
 * test_eat_glass
 * test_source_glass
 * test_unicode_syntax_error
 * test_with_empty_values
 * test_copy_to

I have two suggestions:
 * Please check the output of locale -a on your test system and confirm that you have an en_US.utf8 locale generated.
 * If locale -a lists en_US.utf8 but not en_US.UTF-8, please try setting the environment variable LANG=en_US.utf8.

Alternatively - is it possible for me to duplicate your Jenkins build and do that troubleshooting myself? I haven't set up on Jenkins yet, but I'd be willing to do it to work more efficiently on this ticket and eat up a little less of your time.

Other failures...test_unicode_invalid_request_error fails for me too, but it looks like a superficial problem. test_materialized_view looks like a more substantive issue. I'll check into both.;;;","23/Aug/18 11:48;ptbannister;[~spodxx@gmail.com], I'm working on duplicating your test environment to troubleshoot the problems you mentioned. Those problems don't arise on my simpler Ubuntu and Centos environments, so I suspect it's something related to setting up the cassandra-builds dtest environment.

Is it possible to run the test script (```/cassandra-builds/docker/jenkins/jenkinscommand.sh apache trunk https://github.com/spodkowinski/cassandra-dtest.git CASSANDRA-14298 https://git.apache.org/cassandra-builds.git master```) outside of builds.apache.org or some other specifically configured Jenkins server?;;;","24/Aug/18 00:35;ptbannister;[~spodxx@gmail.com], I diagnosed the cause of the Unicode related failures. You are running the dtests in the Docker image kjellman/cassandra-test:0.4.4. That image does not have a locale configured, other than C.UTF-8.

I recommend we resolve this by adding the following RUN command to the Dockerfile and rebuilding the image:
{code:java}
RUN sudo apt-get -q -y install locales && sudo locale-gen en_US.UTF-8 && sudo apt-get -q -y remove locales
{code}
I still need to look into the other failures you encountered, but this should at least clear up the Unicode problems.;;;","24/Aug/18 08:43;spod;Thanks for figuring this out, [~ptbannister]! I'll look into rebuilding the docker image with the described steps.;;;","25/Aug/18 02:27;ptbannister;Posted a new patch (CASSANDRA-14298.txt) that fixes the failures in test_unicode_invalid_request_error and test_materialized_view.

test_pycodestyle_compliance is still failing, because it's complaining about Python style in cqlsh and cqlshlib, which is out of scope for this ticket.;;;","30/Aug/18 15:00;mkjellman;[~krummas] i’m donating the cassandra-test repository along with the Dockerfile to the ASF. test away ;);;;","10/Sep/18 10:14;spod;Thanks [~mkjellman]! Can you please attach your Dockerfile to CASSANDRA-14713? I'll then try to get rid of the ADDed resource dependencies.;;;","12/Sep/18 23:14;ptbannister;I attached [https://github.com/mkjellman/cassandra-test-docker/blob/master/Dockerfile] to CASSANDRA-14713.;;;","11/Oct/18 11:48;spod;My idea on how to integrate the cql tests are now basically to add them as an extra build to the CircleCI work-flow and builds.apache.org. Builds run on b.a.o are already constantly running into timeout issues and adding cqlsh tests will make this worse. Also cqlsh related regressions should be more easily to predict than others, so we should leave the decision to the dev to run or not to run them.

Here's what needs to be done next:
 * CASSANDRA-14713 - any Python version and environment issues should be easier to address, once the docker image has been update
 * CASSANDRA-14806 - improved CircleCI work-flow will introduce ad-hoc jobs than needs to be manually triggered by the developer
 * CASSANDRA-14298 - add new cqlsh build to work-flow, see [WIP-14298|https://github.com/spodkowinski/cassandra/tree/WIP-14298] (requires dtest update in [WIP-14298|https://github.com/spodkowinski/cassandra-dtest/tree/WIP-14298])

I've tested running the cqlsh tests using the free CircleCI settings once and [results|https://circleci.com/gh/spodkowinski/cassandra/563#tests/containers/0] indicate that the resources (4GB mem) are probably not enough to make the tests pass reliably. It would be nice if we could make the tests fit into the free tier settings, as I wouldn't expect that we need complex virtual testing topology, for the sake of testing cqlsh aspects.;;;","17/Feb/19 02:29;ptbannister;I'm finalizing a new patch.;;;","17/Feb/19 04:18;ptbannister;Attachment CASSANDRA-14298.txt is a new patch for the head of cassandra-dtest master (currently commit e6f58cb33f7a09f273c5990d5d21c7b529ba80bf).

As previously discussed on this ticket, this patch deselects 27 tests in test_cqlsh_copy.py that depend on importing the Python 2.7 cqlshlib into the Python 3 dtests, and gets the remaining tests to pass.

I've gotten passing tests with this patch on AWS t2.large instances running Ubuntu and CentOS 7 with trunk, cassandra-3.11, and cassandra-3.0. The environment must have the en_US.UTF-8 locale generated, with the environment variable LC_CTYPE=en_US.UTF-8.;;;","22/Feb/19 06:45;djoshi;Thank you for the patch, [~ptbannister]. I have imported your patch into a git branch and rebased it on the current master to make it easier for review. I have opened this PR for review: https://github.com/apache/cassandra-dtest/pull/45/files I will over over it and add my comments. [~spodxx@gmail.com] please feel free to review as well.;;;","26/Feb/19 07:08;djoshi;[~ptbannister] I have gone over the changes and they look reasonable so I ran the dtests against trunk and few cqlsh tests have failed. Take a look [here|https://circleci.com/workflow-run/6426fd6f-0e35-4c0b-ba16-aeb3fb9a4247]. Could you please look into those failures? I believe a couple might be because the cluster failed to start. However others seem to be genuine. ;;;","28/Feb/19 03:54;ptbannister;Here's what I think I see:
 * In container #26, test_cqlsh.py passes
 * In container #28, test_cqlsh_copy.py test_bulk_round_trip_blogposts fails because some of the nodes of the ccm cluster wouldn't start
 * In container #29, test_cqlsh_copy.py test_bulk_round_trip_blogposts_with_max_connections fails because the ccm cluster has problems with its virtual network
 * In container #68, test_cqlsh_copy.py test_round_trip_with_rate_file fails because expected and observed data don't match. This may be another environmental resource issue - I think we get trouble when the environment is too slammed to get data through at the desired rate

These tests pass for me when I run the cqlsh copy tests in isolation. I'm thinking about breaking the resource intensive bulk round trip tests in the cqlsh copy tests into a separate script, and marking them as resource intensive. What options does our test infrastructure offer for dealing with resource intensive tests? I know [~spodxx@gmail.com] had some long term plans to improve the infrax to deal with this, but maybe we have some options right now that we could exercise by restructuring the cqlsh tests a bit.;;;","28/Feb/19 18:27;djoshi;[~ptbannister] I realized the CircleCI wasn't executing the tests as intended. I made some minor modifications and reran only the cqlsh tests. There were 5 failures out of 99. Here are teh failures -

# test_colons_in_string_literals - cqlsh_tests.test_cqlsh.TestCqlshSmoke - test teardown
# test_uuid - cqlsh_tests.test_cqlsh.TestCqlshSmoke - test teardown
# test_bulk_round_trip_blogposts - cqlsh_tests.test_cqlsh_copy.TestCqlshCopy - assertion error
# test_bulk_round_trip_blogposts_with_max_connections - assertion error
# test_round_trip_with_rate_file - assertion error

We should definitely look into the assertions first. Individual test function is being run in a 8 Core / 16GB container so I don't think they should be resource constrained.;;;","01/Mar/19 01:39;djoshi;I was also able to run these tests locally and they ran fine on my Mac so this issue has to be related to CircleCI environment.;;;","01/Mar/19 14:20;ptbannister;The failures in test_bulk_round_trip_blogposts* are happening when we do a SELECT COUNT statement by executing a Cassandra driver SimpleStatement with ConsistencyLevel.ALL. It can't achieve the desired consistency level for some reason. I have some ideas for how to work around the problem - either a retry strategy, or else I'll execute that statement with a ccm run_cqlsh call instead.;;;","02/Mar/19 02:22;ptbannister;I've posted a new patch with some possible workarounds for the test_bulk_round_trip_blogposts* and test_round_trip_with_ratefile failures. For the blogposts tests, I added a retry policy to the select count statement. For the ratefile test, I increased the number of rows written with stress, to make sure the stress tool writes for at least one second.

I can't reproduce any of these failures on the environments I've tried, even running the tests inside the same Docker container used by CircleCI. But hopefully these workarounds will address these problems.

I'm not sure what's going on with the failures in the test teardowns.;;;","04/Mar/19 23:23;djoshi;The tear down failures have been fixed (https://github.com/riptano/ccm/pull/694). I have verified the fix works for us as well. I will look into the changes you made.;;;","06/Mar/19 06:01;djoshi;[~ptbannister] I didn't have a whole lot of time to work on it. I imported your updates and modified the assertion as it was still failing. The useful check was to ensure that the number of rows we imported are equal to what we expect. The tests pass without vnodes. However, when I run them with vnodes one test fails - https://circleci.com/gh/dineshjoshi/cassandra/1033#tests/containers/10 Could you debug this further?;;;","09/Mar/19 02:12;djoshi;So [~ptbannister] and I went back and forth on the tests. He finally fixed all issues. We got the cqlsh tests green – https://circleci.com/workflow-run/d782f4e9-63dd-4f8f-b3e3-d008a1e4d5da I am going to run a full test suite (Passed - https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/test-trunk). If everything looks good, [~spodxx@gmail.com] could you please look over it once?;;;","10/Mar/19 17:07;spod;I'll have a look next week. I also hope we'll be able to commit  CASSANDRA-14806 by then and we can break this out as separate build/test target.;;;","10/Mar/19 20:20;djoshi;[~spodxx@gmail.com] do we really need to break this out? I think they should just run as part of the dtests. This is a minor issue and we can work it out later. We'd like to get this reviewed and committed so these tests start running and we can work on CASSANDRA-10190;;;","11/Mar/19 07:27;spod;I was hopping we'd be able to run the separate build with low resources settings, so Patrick and others would be able to run tests own their own in CircleCI. Also can you link the branches you want me to look at?;;;","11/Mar/19 20:43;djoshi;[~spodxx@gmail.com] please take a look at this PR - https://github.com/apache/cassandra-dtest/pull/45/files

I also ran these tests with low memory settings that come with the free circleci tier and I don't think they work. ;;;","12/Mar/19 18:43;spod;We should be fine merging the dtest changes (sans ""Hack debugging"") if tests are passing and do not take a huge amount of time, which doesn't seem to be the case.;;;","12/Mar/19 19:01;djoshi;Thanks, [~spodxx@gmail.com] for taking a look. We'll hold off making any changes to circleci. They were the only ""hack debugging"" we did :). All the tests pass. I will squash and merge it.;;;","12/Mar/19 20:57;spod;You to be clear, I was referring to [d724c125|https://github.com/apache/cassandra-dtest/pull/45/commits/d724c1257ab4cf2428a62ff8a77d9d1265b03cba] as ""Hack debugging"".;;;","12/Mar/19 21:01;djoshi;Yep, that is already cleaned up :);;;","13/Mar/19 23:53;djoshi;Committed as 87afb85f59bdc94a7909bf02575819b6812efaf2 to cassandra-dtest repo. Thanks [~ptbannister] and [~spodxx@gmail.com];;;","15/May/20 13:47;jmckenzie;Somehow is resolved w/resolution unresolved. Re-opening to fix.;;;"
Startup checker should wait for count rather than percentage,CASSANDRA-14297,13143384,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jolynch,jolynch,jolynch,07/Mar/18 22:52,15/May/20 07:59,13/Jul/23 08:37,12/Nov/18 17:47,4.0,4.0-alpha1,,,,,Local/Startup and Shutdown,,,,1,pull-request-available,,,"As I commented in CASSANDRA-13993, the current wait for functionality is a great step in the right direction, but I don't think that the current setting (70% of nodes in the cluster) is the right configuration option. First I think this because 70% will not protect against errors as if you wait for 70% of the cluster you could still very easily have {{UnavailableException}} or {{ReadTimeoutException}} exceptions. This is because if you have even two nodes down in different racks in a Cassandra cluster these exceptions are possible (or with the default {{num_tokens}} setting of 256 it is basically guaranteed). Second I think this option is not easy for operators to set, the only setting I could think of that would ""just work"" is 100%.

I proposed in that ticket instead of having `block_for_peers_percentage` defaulting to 70%, we instead have `block_for_peers` as a count of nodes that are allowed to be down before the starting node makes itself available as a coordinator. Of course, we would still have the timeout to limit startup time and deal with really extreme situations (whole datacenters down etc).

I started working on a patch for this change [on github|https://github.com/jasobrown/cassandra/compare/13993...jolynch:13993], and am happy to finish it up with unit tests and such if someone can review/commit it (maybe [~aweisberg]?).

I think the short version of my proposal is we replace:
{noformat}
block_for_peers_percentage: <percentage needed up, defaults to 70%>
{noformat}

with either
{noformat}
block_for_peers: <number that can be down, defaults to 1>
{noformat}

or, if we want to do even better imo and enable advanced operators to finely tune this behavior (while still having good defaults that work for almost everyone):
{noformat}
block_for_peers_local_dc:  <number that can be down, defaults to 1>
block_for_peers_each_dc: <number that can be down, defaults to sys.maxint>
block_for_peers_all_dcs: <number that can be down, defaults to sys.maxint>
{noformat}

For example if an operator knows that they must be available at {{LOCAL_QUORUM}} they would set {{block_for_peers_local_dc=1}}, if they use {{EACH_QUOURM}} they would set {{block_for_peers_local_dc=1}}, if they use {{QUORUM}} (RF=3, dcs=2) they would set {{block_for_peers_all_dcs=2}}. Naturally everything would of course have a timeout to prevent startup taking too long.
",,aweisberg,dikanggu,githubbot,jasonstack,jay.zhuang,jeromatron,jolynch,KurtG,mbyrd,rha,sumanth.pasupuleti,vinaykumarcse,,,,,,,,,,,,,,"Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225709488
  
    --- Diff: src/java/org/apache/cassandra/config/Config.java ---
    @@ -376,9 +376,31 @@
     
         public String full_query_log_dir = null;
     
    -    // parameters to adjust how much to delay startup until a certain amount of the cluster is connect to and marked alive
    -    public int block_for_peers_percentage = 70;
    +    /**
    +     * When a node first starts up it intially thinks all other peers are DOWN, and then as the initial gossip
    +     * broadcast messages comes back nodes transition to UP. These options configure how many nodes can remain in
    +     * DOWN state before we make this node available as a coordinator, as well as an overall timeout on this process
    +     * to ensure that startup is not delayed too much.
    +     *
    +     * The defaults are tuned for LOCAL_ONE consistency levels with RF=3, and have natural settings for other CLs:
    +     *
    +     *     Consistency Level | local_dc     | all_dcs
    +     *     --------------------------------------------------------
    +     *     LOCAL_ONE         | default (2)  | default (any)
    +     *     LOCAL_QUORUM      | 1            | default (any)
    +     *     ONE               | any          | RF - 1
    +     *     QUORUM            | any          | (RF / 2) - 1
    +     *     ALL               | default      | 0
    +     *
    +     * A concrete example with QUORUM would be if you have 3 replicas in 2 datacenters, then you would set
    +     * block_for_peers_all_dcs to (6 / 2) - 1 = 2 because that guarantees that at most 2 hosts in all datacenters
    +     * are down when you start taking client traffic, which should satistfy QUORUM for all RF=6 QUORUM queries.
    +     */
    +    public int block_for_peers_local_dc = 2;
    --- End diff --
    
    If the goal is to not return unavailable shouldn't the default really be 1 since LOCAL_QUORUM and QUORUM are more common? I feel like if this doesn't do what people want with the defaults it makes it a lot less useful?
    
    All this does is add 10 seconds to startup which seems really minor. A part of me even wonders why fuss with any of this if we are just going to wait 10 seconds? Just wait until everything is up and if it doesn't happen in 10 seconds continue to do what we were going to do anyways?
    
    The connection priming stuff I get. That is good and I'm glad we added that.
    
    The name also doesn't really make sense anymore since this is not the number we are blocking for it's the number we aren't blocking for.
;16/Oct/18 22:15;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225725532
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -213,4 +261,11 @@ boolean incrementAndCheck(InetAddressAndPort address)
                 return acks.computeIfAbsent(address, addr -> new AtomicInteger(0)).incrementAndGet() == threshold;
             }
         }
    +
    +    private String fmtBlocker(Integer size, Integer count)
    --- End diff --
    
    Is this a more complicated Math.max?
;16/Oct/18 22:15;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225724502
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -17,13 +17,16 @@
      */
     package org.apache.cassandra.net;
     
    +import java.util.Collections;
     import java.util.HashSet;
     import java.util.Map;
     import java.util.Set;
     import java.util.concurrent.ConcurrentHashMap;
     import java.util.concurrent.CountDownLatch;
     import java.util.concurrent.TimeUnit;
     import java.util.concurrent.atomic.AtomicInteger;
    +import java.util.function.Function;
    +import java.util.stream.Collectors;
     
     import com.google.common.annotations.VisibleForTesting;
     import com.google.common.collect.Sets;
    --- End diff --
    
    Unused import
;16/Oct/18 22:15;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225721508
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -73,56 +80,84 @@ public static StartupClusterConnectivityChecker create(int targetPercent, int ti
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenter)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    -
    -        if (peers.isEmpty())
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        if (peers.size() == 1 && peers.contains(localAddress))
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        Set<InetAddressAndPort> myLocalPeers = peers.stream()
    +                                                    .collect(Collectors.groupingBy(getDatacenter, toSet()))
    +                                                    .getOrDefault(getDatacenter.apply(FBUtilities.getBroadcastAddressAndPort()),
    +                                                                  Collections.emptySet());
    +
    +
    +        logger.info(""choosing to block until no more than {}/{} local and no more than {}/{} global peers are still DOWN; max time to wait = {} seconds"",
    --- End diff --
    
    This logs the local node as a peer since you aren't removing it. Seems like it's not a desired side effect.
;16/Oct/18 22:15;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225718303
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -39,7 +42,9 @@
     import org.apache.cassandra.locator.InetAddressAndPort;
     import org.apache.cassandra.net.async.OutboundConnectionIdentifier.ConnectionType;
     import org.apache.cassandra.utils.FBUtilities;
    +import org.jboss.byteman.synchronization.CountDown;
    --- End diff --
    
    Unused import
;16/Oct/18 22:15;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225719987
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -73,56 +80,84 @@ public static StartupClusterConnectivityChecker create(int targetPercent, int ti
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenter)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null)
    --- End diff --
    
    This change appears to remove the ability to disable this? It's an interesting change because it means we will always prime the connections which sounds kind of useful.
    
    The most you can do is crank down the timeout or set the number of hosts you won't wait for very high which has a similar impact.
;16/Oct/18 22:15;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225720782
  
    --- Diff: src/java/org/apache/cassandra/config/Config.java ---
    @@ -376,9 +376,31 @@
     
         public String full_query_log_dir = null;
     
    -    // parameters to adjust how much to delay startup until a certain amount of the cluster is connect to and marked alive
    -    public int block_for_peers_percentage = 70;
    +    /**
    +     * When a node first starts up it intially thinks all other peers are DOWN, and then as the initial gossip
    +     * broadcast messages comes back nodes transition to UP. These options configure how many nodes can remain in
    +     * DOWN state before we make this node available as a coordinator, as well as an overall timeout on this process
    +     * to ensure that startup is not delayed too much.
    +     *
    +     * The defaults are tuned for LOCAL_ONE consistency levels with RF=3, and have natural settings for other CLs:
    +     *
    +     *     Consistency Level | local_dc     | all_dcs
    +     *     --------------------------------------------------------
    +     *     LOCAL_ONE         | default (2)  | default (any)
    +     *     LOCAL_QUORUM      | 1            | default (any)
    +     *     ONE               | any          | RF - 1
    +     *     QUORUM            | any          | (RF / 2) - 1
    +     *     ALL               | default      | 0
    +     *
    +     * A concrete example with QUORUM would be if you have 3 replicas in 2 datacenters, then you would set
    +     * block_for_peers_all_dcs to (6 / 2) - 1 = 2 because that guarantees that at most 2 hosts in all datacenters
    +     * are down when you start taking client traffic, which should satistfy QUORUM for all RF=6 QUORUM queries.
    +     */
    +    public int block_for_peers_local_dc = 2;
    +    public int block_for_peers_all_dcs = Integer.MAX_VALUE;
    --- End diff --
    
    QUORUM is at least a little common? Seems like defaulting to no waiting for remote DCs might not be the right default? It's a question of which do people dislike more? Getting unavailables when a node restarts or potentially waiting extra time on startup?
;16/Oct/18 22:15;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225732037
  
    --- Diff: src/java/org/apache/cassandra/config/Config.java ---
    @@ -376,9 +376,31 @@
     
         public String full_query_log_dir = null;
     
    -    // parameters to adjust how much to delay startup until a certain amount of the cluster is connect to and marked alive
    -    public int block_for_peers_percentage = 70;
    +    /**
    +     * When a node first starts up it intially thinks all other peers are DOWN, and then as the initial gossip
    +     * broadcast messages comes back nodes transition to UP. These options configure how many nodes can remain in
    +     * DOWN state before we make this node available as a coordinator, as well as an overall timeout on this process
    +     * to ensure that startup is not delayed too much.
    +     *
    +     * The defaults are tuned for LOCAL_ONE consistency levels with RF=3, and have natural settings for other CLs:
    +     *
    +     *     Consistency Level | local_dc     | all_dcs
    +     *     --------------------------------------------------------
    +     *     LOCAL_ONE         | default (2)  | default (any)
    +     *     LOCAL_QUORUM      | 1            | default (any)
    +     *     ONE               | any          | RF - 1
    +     *     QUORUM            | any          | (RF / 2) - 1
    +     *     ALL               | default      | 0
    +     *
    +     * A concrete example with QUORUM would be if you have 3 replicas in 2 datacenters, then you would set
    +     * block_for_peers_all_dcs to (6 / 2) - 1 = 2 because that guarantees that at most 2 hosts in all datacenters
    +     * are down when you start taking client traffic, which should satistfy QUORUM for all RF=6 QUORUM queries.
    +     */
    +    public int block_for_peers_local_dc = 2;
    --- End diff --
    
    > If the goal is to not return unavailable shouldn't the default really be 1 since LOCAL_QUORUM and QUORUM are more common? I feel like if this doesn't do what people want with the defaults it makes it a lot less useful?
    
    I thought that defaulting to handling the case that the drivers default to `LOCAL_ONE` was most sensible, although I can make it `1` if you prefer and say the defaults are for `LOCAL_QUORUM` (I don't have strong opinions other than we shouldn't be waiting by default on remote DCs).
    
    > All this does is add 10 seconds to startup which seems really minor. A part of me even wonders why fuss with any of this if we are just going to wait 10 seconds? Just wait until everything is up and if it doesn't happen in 10 seconds continue to do what we were going to do anyways?
    
    In practice you wait way less than 10 seconds (e.g. on the test 200 node 4.0 cluster we handshake with the whole local DC in about ... 500ms). I personally think that most users would rather their database wait O(minutes) than throw errors in the general case, but from the earlier conversations in CASSANDRA-13993 it seemed like folks were hesitant to wait so long on startup e.g. when doing ccm clusters or the such.
    
    > The name also doesn't really make sense anymore since this is not the number we are blocking for it's the number we aren't blocking for.
    
    How about ... `startup_max_down_local_dc_peers` and `startup_max_down_peers`?
    
    

;16/Oct/18 22:44;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225732092
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -39,7 +42,9 @@
     import org.apache.cassandra.locator.InetAddressAndPort;
     import org.apache.cassandra.net.async.OutboundConnectionIdentifier.ConnectionType;
     import org.apache.cassandra.utils.FBUtilities;
    +import org.jboss.byteman.synchronization.CountDown;
    --- End diff --
    
    ack
;16/Oct/18 22:44;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225732818
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -73,56 +80,84 @@ public static StartupClusterConnectivityChecker create(int targetPercent, int ti
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenter)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null)
    --- End diff --
    
    Yea that was my idea, that we should always prime the connections but we can just not wait (if you set the local dc option to a really large number then it basically primes and immediately returns).
;16/Oct/18 22:47;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225733894
  
    --- Diff: src/java/org/apache/cassandra/config/Config.java ---
    @@ -376,9 +376,31 @@
     
         public String full_query_log_dir = null;
     
    -    // parameters to adjust how much to delay startup until a certain amount of the cluster is connect to and marked alive
    -    public int block_for_peers_percentage = 70;
    +    /**
    +     * When a node first starts up it intially thinks all other peers are DOWN, and then as the initial gossip
    +     * broadcast messages comes back nodes transition to UP. These options configure how many nodes can remain in
    +     * DOWN state before we make this node available as a coordinator, as well as an overall timeout on this process
    +     * to ensure that startup is not delayed too much.
    +     *
    +     * The defaults are tuned for LOCAL_ONE consistency levels with RF=3, and have natural settings for other CLs:
    +     *
    +     *     Consistency Level | local_dc     | all_dcs
    +     *     --------------------------------------------------------
    +     *     LOCAL_ONE         | default (2)  | default (any)
    +     *     LOCAL_QUORUM      | 1            | default (any)
    +     *     ONE               | any          | RF - 1
    +     *     QUORUM            | any          | (RF / 2) - 1
    +     *     ALL               | default      | 0
    +     *
    +     * A concrete example with QUORUM would be if you have 3 replicas in 2 datacenters, then you would set
    +     * block_for_peers_all_dcs to (6 / 2) - 1 = 2 because that guarantees that at most 2 hosts in all datacenters
    +     * are down when you start taking client traffic, which should satistfy QUORUM for all RF=6 QUORUM queries.
    +     */
    +    public int block_for_peers_local_dc = 2;
    +    public int block_for_peers_all_dcs = Integer.MAX_VALUE;
    --- End diff --
    
    I agree and 10s isn't that big a deal (imo) for users that don't want to wait for remote dcs, but I was trying to compromise between the old settings (70% == could literally have a whole DC down with 3 DCs) which appear to be motivated from a perspective of ""don't block my startup"" and what I personally think that we shouldn't be saying we're ready to coordinate until we're actually ready to coordinate.
    
    I am happy to put whatever default gets this merged ;-) We'll probably internally be setting the local setting to `1` (so `LOCAL_QUORUM`) and the remote to a really large number. But that's just our perspective...
;16/Oct/18 22:53;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225734481
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -73,56 +80,84 @@ public static StartupClusterConnectivityChecker create(int targetPercent, int ti
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenter)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    -
    -        if (peers.isEmpty())
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        if (peers.size() == 1 && peers.contains(localAddress))
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        Set<InetAddressAndPort> myLocalPeers = peers.stream()
    +                                                    .collect(Collectors.groupingBy(getDatacenter, toSet()))
    +                                                    .getOrDefault(getDatacenter.apply(FBUtilities.getBroadcastAddressAndPort()),
    +                                                                  Collections.emptySet());
    +
    +
    +        logger.info(""choosing to block until no more than {}/{} local and no more than {}/{} global peers are still DOWN; max time to wait = {} seconds"",
    --- End diff --
    
    Yea ... I was debating if this should be here or down below 122 where we remove it. I was thinking that users would want to see their cluster size in the denominator (e.g. `choosing to block until no more than 1/200` nodes) even if strictly speaking we just immediately succeed on the local host.
    
    I don't have strong preferences, I'll take the local node out of the count.
;16/Oct/18 22:56;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225734572
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -17,13 +17,16 @@
      */
     package org.apache.cassandra.net;
     
    +import java.util.Collections;
     import java.util.HashSet;
     import java.util.Map;
     import java.util.Set;
     import java.util.concurrent.ConcurrentHashMap;
     import java.util.concurrent.CountDownLatch;
     import java.util.concurrent.TimeUnit;
     import java.util.concurrent.atomic.AtomicInteger;
    +import java.util.function.Function;
    +import java.util.stream.Collectors;
     
     import com.google.common.annotations.VisibleForTesting;
     import com.google.common.collect.Sets;
    --- End diff --
    
    ack
;16/Oct/18 22:56;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225734880
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -213,4 +261,11 @@ boolean incrementAndCheck(InetAddressAndPort address)
                 return acks.computeIfAbsent(address, addr -> new AtomicInteger(0)).incrementAndGet() == threshold;
             }
         }
    +
    +    private String fmtBlocker(Integer size, Integer count)
    --- End diff --
    
    Heh, yea it is I just thought that it was slightly clearer what it was doing (so we don't print `Integer.MAX_VALUE` to the log. I'll change it to `Math.max` instead.
;16/Oct/18 22:58;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225735927
  
    --- Diff: src/java/org/apache/cassandra/config/Config.java ---
    @@ -376,9 +376,31 @@
     
         public String full_query_log_dir = null;
     
    -    // parameters to adjust how much to delay startup until a certain amount of the cluster is connect to and marked alive
    -    public int block_for_peers_percentage = 70;
    +    /**
    +     * When a node first starts up it intially thinks all other peers are DOWN, and then as the initial gossip
    +     * broadcast messages comes back nodes transition to UP. These options configure how many nodes can remain in
    +     * DOWN state before we make this node available as a coordinator, as well as an overall timeout on this process
    +     * to ensure that startup is not delayed too much.
    +     *
    +     * The defaults are tuned for LOCAL_ONE consistency levels with RF=3, and have natural settings for other CLs:
    +     *
    +     *     Consistency Level | local_dc     | all_dcs
    +     *     --------------------------------------------------------
    +     *     LOCAL_ONE         | default (2)  | default (any)
    +     *     LOCAL_QUORUM      | 1            | default (any)
    +     *     ONE               | any          | RF - 1
    +     *     QUORUM            | any          | (RF / 2) - 1
    +     *     ALL               | default      | 0
    +     *
    +     * A concrete example with QUORUM would be if you have 3 replicas in 2 datacenters, then you would set
    +     * block_for_peers_all_dcs to (6 / 2) - 1 = 2 because that guarantees that at most 2 hosts in all datacenters
    +     * are down when you start taking client traffic, which should satistfy QUORUM for all RF=6 QUORUM queries.
    +     */
    +    public int block_for_peers_local_dc = 2;
    +    public int block_for_peers_all_dcs = Integer.MAX_VALUE;
    --- End diff --
    
    This one of those things where I want to ask around and see if we can get away with just waiting 10 seconds if a node is down.
    
    Seems like the solution for CCM is to just have CCM set the config it wants. I'm also wondering if CCM actually waits the 10 seconds anyways just because it starts one node at a time and waits for a specific log message a lot of the time and I'm not sure if that message is before or after the connectivity checker.
    
    I have to go take a look at how CCM works. 
;16/Oct/18 23:04;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r228386047
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -73,56 +80,84 @@ public static StartupClusterConnectivityChecker create(int targetPercent, int ti
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenter)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null)
    --- End diff --
    
    In the latest version I added the ability to disable this by setting a negative timeout. I guess it's probably a good idea to have an opt out option since we're post freeze.
;26/Oct/18 01:52;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r228386128
  
    --- Diff: src/java/org/apache/cassandra/config/Config.java ---
    @@ -376,9 +376,31 @@
     
         public String full_query_log_dir = null;
     
    -    // parameters to adjust how much to delay startup until a certain amount of the cluster is connect to and marked alive
    -    public int block_for_peers_percentage = 70;
    +    /**
    +     * When a node first starts up it intially thinks all other peers are DOWN, and then as the initial gossip
    +     * broadcast messages comes back nodes transition to UP. These options configure how many nodes can remain in
    +     * DOWN state before we make this node available as a coordinator, as well as an overall timeout on this process
    +     * to ensure that startup is not delayed too much.
    +     *
    +     * The defaults are tuned for LOCAL_ONE consistency levels with RF=3, and have natural settings for other CLs:
    +     *
    +     *     Consistency Level | local_dc     | all_dcs
    +     *     --------------------------------------------------------
    +     *     LOCAL_ONE         | default (2)  | default (any)
    +     *     LOCAL_QUORUM      | 1            | default (any)
    +     *     ONE               | any          | RF - 1
    +     *     QUORUM            | any          | (RF / 2) - 1
    +     *     ALL               | default      | 0
    +     *
    +     * A concrete example with QUORUM would be if you have 3 replicas in 2 datacenters, then you would set
    +     * block_for_peers_all_dcs to (6 / 2) - 1 = 2 because that guarantees that at most 2 hosts in all datacenters
    +     * are down when you start taking client traffic, which should satistfy QUORUM for all RF=6 QUORUM queries.
    +     */
    +    public int block_for_peers_local_dc = 2;
    +    public int block_for_peers_all_dcs = Integer.MAX_VALUE;
    --- End diff --
    
    I played around with CCM and it appears to work fine (starting a cluster, stopping it, starting and stopping nodes etc). I think we should be good on this front.
;26/Oct/18 01:53;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r228386194
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -73,56 +80,84 @@ public static StartupClusterConnectivityChecker create(int targetPercent, int ti
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenter)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    -
    -        if (peers.isEmpty())
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        if (peers.size() == 1 && peers.contains(localAddress))
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        Set<InetAddressAndPort> myLocalPeers = peers.stream()
    +                                                    .collect(Collectors.groupingBy(getDatacenter, toSet()))
    +                                                    .getOrDefault(getDatacenter.apply(FBUtilities.getBroadcastAddressAndPort()),
    +                                                                  Collections.emptySet());
    +
    +
    +        logger.info(""choosing to block until no more than {}/{} local and no more than {}/{} global peers are still DOWN; max time to wait = {} seconds"",
    --- End diff --
    
    I changed it to remove the local peer like before :-)
;26/Oct/18 01:53;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r228386285
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -213,4 +261,11 @@ boolean incrementAndCheck(InetAddressAndPort address)
                 return acks.computeIfAbsent(address, addr -> new AtomicInteger(0)).incrementAndGet() == threshold;
             }
         }
    +
    +    private String fmtBlocker(Integer size, Integer count)
    --- End diff --
    
    I killed it.
;26/Oct/18 01:54;githubbot;600","Github user jolynch commented on the issue:

    https://github.com/apache/cassandra/pull/212
  
    Ok, I've rebased and incorporated the feedback from IRC/jira as well. Please let me know if there are further changes I need to make. 
;26/Oct/18 01:55;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229106050
  
    --- Diff: src/java/org/apache/cassandra/config/Config.java ---
    @@ -380,9 +380,23 @@
         public RepairCommandPoolFullStrategy repair_command_pool_full_strategy = RepairCommandPoolFullStrategy.queue;
         public int repair_command_pool_size = concurrent_validations;
     
    -    // parameters to adjust how much to delay startup until a certain amount of the cluster is connect to and marked alive
    -    public int block_for_peers_percentage = 70;
    +    /**
    +     * When a node first starts up it intially considers all other peers as DOWN, and then as the initial gossip
    +     * broadcast messages comes back nodes transition to UP. These options configure how long we wait for peers to
    +     * connect before we make this node available as a coordinator. Furthermore, if this feature is enabled
    +     * (timeout >= 0) Cassandra initiates the non gossip channel internode connections on startup as well and waits
    --- End diff --
    
    This part of the description doesn't match the description for block_for_peers_timeot_in_secs. It sounds like now it differentiates between disabled, and prime the connections.
;29/Oct/18 21:31;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229107287
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    --- End diff --
    
    Should this be a warning? They have pretty clearly asked for it?
;29/Oct/18 21:35;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229110247
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    --- End diff --
    
    Could this be a set multimap?
;29/Oct/18 22:30;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229113284
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    +                                                              .collect(Collectors.groupingBy(getDatacenter,
    +                                                                                             Collectors.toSet()));
    +
    +        if (!blockForRemoteDcs)
    +        {
    +            peersByDc.keySet().retainAll(Collections.singleton(localDc));
    +            logger.info(""Blocking coordination until only a single peer is DOWN in the local datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
    +        else
    +        {
    +            logger.info(""Blocking coordination until only a single peer is DOWN in each datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
     
             AckMap acks = new AckMap(3);
    -        int target = (int) ((targetPercent / 100.0) * peers.size());
    -        CountDownLatch latch = new CountDownLatch(target);
    +        Map<String, CountDownLatch> latchMap = new HashMap<>(peersByDc.size());
    +        for (Map.Entry<String, Set<InetAddressAndPort>> entry: peersByDc.entrySet())
    +        {
    +            latchMap.put(entry.getKey(), new CountDownLatch(Math.max(entry.getValue().size() - 1, 0)));
    +        }
    +
    +        long startNanos = System.nanoTime();
     
             // set up a listener to react to new nodes becoming alive (in gossip), and account for all the nodes that are already alive
    -        Set<InetAddressAndPort> alivePeers = Sets.newSetFromMap(new ConcurrentHashMap<>());
    -        AliveListener listener = new AliveListener(alivePeers, latch, acks);
    +        Set<InetAddressAndPort> alivePeers = Collections.newSetFromMap(new ConcurrentHashMap<>());
    +        AliveListener listener = new AliveListener(alivePeers, latchMap, acks, getDatacenter);
             Gossiper.instance.register(listener);
     
             // send out a ping message to open up the non-gossip connections
    -        sendPingMessages(peers, latch, acks);
    +        sendPingMessages(peers, latchMap, acks, getDatacenter);
     
             for (InetAddressAndPort peer : peers)
    +        {
                 if (Gossiper.instance.isAlive(peer) && alivePeers.add(peer) && acks.incrementAndCheck(peer))
    -                latch.countDown();
    +            {
    +                String datacenter = getDatacenter.apply(peer);
    +                if (latchMap.containsKey(datacenter))
    +                    latchMap.get(datacenter).countDown();
    +            }
    +        }
    +
    +        boolean succeeded = Uninterruptibles.awaitUninterruptibly(latchMap.get(localDc), timeoutNanos, TimeUnit.NANOSECONDS);
    +        for (String datacenter: latchMap.keySet())
    +        {
    +            if (datacenter.equals(localDc))
    --- End diff --
    
    Then you don't need this exception?
;29/Oct/18 22:30;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229113240
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    +                                                              .collect(Collectors.groupingBy(getDatacenter,
    +                                                                                             Collectors.toSet()));
    +
    +        if (!blockForRemoteDcs)
    +        {
    +            peersByDc.keySet().retainAll(Collections.singleton(localDc));
    +            logger.info(""Blocking coordination until only a single peer is DOWN in the local datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
    +        else
    +        {
    +            logger.info(""Blocking coordination until only a single peer is DOWN in each datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
     
             AckMap acks = new AckMap(3);
    -        int target = (int) ((targetPercent / 100.0) * peers.size());
    -        CountDownLatch latch = new CountDownLatch(target);
    +        Map<String, CountDownLatch> latchMap = new HashMap<>(peersByDc.size());
    +        for (Map.Entry<String, Set<InetAddressAndPort>> entry: peersByDc.entrySet())
    +        {
    +            latchMap.put(entry.getKey(), new CountDownLatch(Math.max(entry.getValue().size() - 1, 0)));
    +        }
    +
    +        long startNanos = System.nanoTime();
     
             // set up a listener to react to new nodes becoming alive (in gossip), and account for all the nodes that are already alive
    -        Set<InetAddressAndPort> alivePeers = Sets.newSetFromMap(new ConcurrentHashMap<>());
    -        AliveListener listener = new AliveListener(alivePeers, latch, acks);
    +        Set<InetAddressAndPort> alivePeers = Collections.newSetFromMap(new ConcurrentHashMap<>());
    +        AliveListener listener = new AliveListener(alivePeers, latchMap, acks, getDatacenter);
             Gossiper.instance.register(listener);
     
             // send out a ping message to open up the non-gossip connections
    -        sendPingMessages(peers, latch, acks);
    +        sendPingMessages(peers, latchMap, acks, getDatacenter);
     
             for (InetAddressAndPort peer : peers)
    +        {
                 if (Gossiper.instance.isAlive(peer) && alivePeers.add(peer) && acks.incrementAndCheck(peer))
    -                latch.countDown();
    +            {
    +                String datacenter = getDatacenter.apply(peer);
    +                if (latchMap.containsKey(datacenter))
    +                    latchMap.get(datacenter).countDown();
    +            }
    +        }
    +
    +        boolean succeeded = Uninterruptibles.awaitUninterruptibly(latchMap.get(localDc), timeoutNanos, TimeUnit.NANOSECONDS);
    --- End diff --
    
    Why do you need the loop and this? Since you removed other DCs from peersByDC shouldn't there already be only the local DC in the latch map?
;29/Oct/18 22:30;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229113469
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    +                                                              .collect(Collectors.groupingBy(getDatacenter,
    +                                                                                             Collectors.toSet()));
    +
    +        if (!blockForRemoteDcs)
    +        {
    +            peersByDc.keySet().retainAll(Collections.singleton(localDc));
    +            logger.info(""Blocking coordination until only a single peer is DOWN in the local datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
    +        else
    +        {
    +            logger.info(""Blocking coordination until only a single peer is DOWN in each datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
     
             AckMap acks = new AckMap(3);
    -        int target = (int) ((targetPercent / 100.0) * peers.size());
    -        CountDownLatch latch = new CountDownLatch(target);
    +        Map<String, CountDownLatch> latchMap = new HashMap<>(peersByDc.size());
    --- End diff --
    
    Another candidate for datacenterToY. Not quite sure of the perfect name for Y. I am fine with using dc if you want shorter.
;29/Oct/18 22:30;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229114775
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    +                                                              .collect(Collectors.groupingBy(getDatacenter,
    +                                                                                             Collectors.toSet()));
    +
    +        if (!blockForRemoteDcs)
    +        {
    +            peersByDc.keySet().retainAll(Collections.singleton(localDc));
    --- End diff --
    
    This important bit of behavior is tucked away here and I only know about it because I went looking. Can you add a mostly redundant comment mentioning that you remove all but the local DC so that we only wait on nodes in the local DC?
;29/Oct/18 22:30;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229120761
  
    --- Diff: test/unit/org/apache/cassandra/net/StartupClusterConnectivityCheckerTest.java ---
    @@ -36,13 +36,34 @@
     import org.apache.cassandra.gms.Gossiper;
     import org.apache.cassandra.gms.HeartBeatState;
     import org.apache.cassandra.locator.InetAddressAndPort;
    +import org.apache.cassandra.utils.FBUtilities;
     
     import static org.apache.cassandra.net.async.OutboundConnectionIdentifier.ConnectionType.SMALL_MESSAGE;
     
     public class StartupClusterConnectivityCheckerTest
     {
    -    private StartupClusterConnectivityChecker connectivityChecker;
    +    private StartupClusterConnectivityChecker localQuorumConnectivityChecker;
    +    private StartupClusterConnectivityChecker globalQuorumConnectivityChecker;
    +    private StartupClusterConnectivityChecker noopChecker;
    +    private StartupClusterConnectivityChecker zeroWaitChecker;
    +
    +    private static final int NUM_PER_DC = 6;
         private Set<InetAddressAndPort> peers;
    +    private Set<InetAddressAndPort> peersA;
    +    private Set<InetAddressAndPort> peersAMinusLocal;
    +    private Set<InetAddressAndPort> peersB;
    +    private Set<InetAddressAndPort> peersC;
    +
    +    private String getDatacenter(InetAddressAndPort endpoint)
    +    {
    +        if (peersA.contains(endpoint))
    +            return ""datacenterA"";
    +        if (peersB.contains(endpoint))
    +            return ""datacenterB"";
    +        else if (peersC.contains(endpoint))
    +            return ""datacenterC"";
    +        return ""NA"";
    --- End diff --
    
    Should probably return null
;29/Oct/18 22:30;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229120810
  
    --- Diff: test/unit/org/apache/cassandra/net/StartupClusterConnectivityCheckerTest.java ---
    @@ -69,33 +113,102 @@ public void tearDown()
         @Test
         public void execute_HappyPath()
         {
    -        Sink sink = new Sink(true, true);
    +        Sink sink = new Sink(true, true, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertTrue(connectivityChecker.execute(peers));
    +        Assert.assertTrue(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
             checkAllConnectionTypesSeen(sink);
         }
     
         @Test
         public void execute_NotAlive()
         {
    -        Sink sink = new Sink(false, true);
    +        Sink sink = new Sink(false, true, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertFalse(connectivityChecker.execute(peers));
    +        Assert.assertFalse(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
             checkAllConnectionTypesSeen(sink);
         }
     
         @Test
         public void execute_NoConnectionsAcks()
         {
    -        Sink sink = new Sink(true, false);
    +        Sink sink = new Sink(true, false, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertFalse(connectivityChecker.execute(peers));
    +        Assert.assertFalse(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
    +    }
    +
    +    @Test
    +    public void execute_LocalQuorum()
    +    {
    +        // local peer plus 3 peers from same dc shouldn't pass (4/6)
    +        Set<InetAddressAndPort> available = new HashSet<>();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC - 3);
    +        checkAvailable(localQuorumConnectivityChecker, available, false, true);
    +
    +        // local peer plus 4 peers from same dc should pass (5/6)
    +        available.clear();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC - 2);
    +        checkAvailable(localQuorumConnectivityChecker, available, true, true);
    +    }
    +
    +    @Test
    +    public void execute_GlobalQuorum()
    +    {
    +        // local dc passing shouldn'nt pass globally with two hosts down in datacenterB
    --- End diff --
    
    typo
;29/Oct/18 22:30;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229121401
  
    --- Diff: test/unit/org/apache/cassandra/net/StartupClusterConnectivityCheckerTest.java ---
    @@ -69,33 +113,102 @@ public void tearDown()
         @Test
         public void execute_HappyPath()
         {
    -        Sink sink = new Sink(true, true);
    +        Sink sink = new Sink(true, true, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertTrue(connectivityChecker.execute(peers));
    +        Assert.assertTrue(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
             checkAllConnectionTypesSeen(sink);
         }
     
         @Test
         public void execute_NotAlive()
         {
    -        Sink sink = new Sink(false, true);
    +        Sink sink = new Sink(false, true, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertFalse(connectivityChecker.execute(peers));
    +        Assert.assertFalse(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
             checkAllConnectionTypesSeen(sink);
         }
     
         @Test
         public void execute_NoConnectionsAcks()
         {
    -        Sink sink = new Sink(true, false);
    +        Sink sink = new Sink(true, false, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertFalse(connectivityChecker.execute(peers));
    +        Assert.assertFalse(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
    +    }
    +
    +    @Test
    +    public void execute_LocalQuorum()
    +    {
    +        // local peer plus 3 peers from same dc shouldn't pass (4/6)
    +        Set<InetAddressAndPort> available = new HashSet<>();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC - 3);
    +        checkAvailable(localQuorumConnectivityChecker, available, false, true);
    +
    +        // local peer plus 4 peers from same dc should pass (5/6)
    +        available.clear();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC - 2);
    +        checkAvailable(localQuorumConnectivityChecker, available, true, true);
    +    }
    +
    +    @Test
    +    public void execute_GlobalQuorum()
    +    {
    +        // local dc passing shouldn'nt pass globally with two hosts down in datacenterB
    +        Set<InetAddressAndPort> available = new HashSet<>();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC - 2);
    +        copyCount(peersB, available, NUM_PER_DC - 2);
    +        copyCount(peersC, available, NUM_PER_DC - 1);
    +        checkAvailable(globalQuorumConnectivityChecker, available, false, true);
    +
    +        available.clear();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC -2);
    +        copyCount(peersB, available, NUM_PER_DC - 1);
    +        copyCount(peersC, available, NUM_PER_DC - 1);
    +        checkAvailable(globalQuorumConnectivityChecker, available, true, true);
    +    }
    +
    +    @Test
    +    public void execute_Noop()
    +    {
    +        checkAvailable(noopChecker, new HashSet<>(), true, false);
    +    }
    +
    +    @Test
    +    public void execute_ZeroWait()
    +    {
    +        checkAvailable(zeroWaitChecker, new HashSet<>(), false, false);
    --- End diff --
    
    Maybe add a test that does zero wait, but also checks the connections are made?
;29/Oct/18 22:30;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229114904
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    +                                                              .collect(Collectors.groupingBy(getDatacenter,
    +                                                                                             Collectors.toSet()));
    +
    +        if (!blockForRemoteDcs)
    +        {
    +            peersByDc.keySet().retainAll(Collections.singleton(localDc));
    +            logger.info(""Blocking coordination until only a single peer is DOWN in the local datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
    +        else
    +        {
    +            logger.info(""Blocking coordination until only a single peer is DOWN in each datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
     
             AckMap acks = new AckMap(3);
    -        int target = (int) ((targetPercent / 100.0) * peers.size());
    -        CountDownLatch latch = new CountDownLatch(target);
    +        Map<String, CountDownLatch> latchMap = new HashMap<>(peersByDc.size());
    +        for (Map.Entry<String, Set<InetAddressAndPort>> entry: peersByDc.entrySet())
    +        {
    +            latchMap.put(entry.getKey(), new CountDownLatch(Math.max(entry.getValue().size() - 1, 0)));
    +        }
    +
    +        long startNanos = System.nanoTime();
     
             // set up a listener to react to new nodes becoming alive (in gossip), and account for all the nodes that are already alive
    -        Set<InetAddressAndPort> alivePeers = Sets.newSetFromMap(new ConcurrentHashMap<>());
    -        AliveListener listener = new AliveListener(alivePeers, latch, acks);
    +        Set<InetAddressAndPort> alivePeers = Collections.newSetFromMap(new ConcurrentHashMap<>());
    +        AliveListener listener = new AliveListener(alivePeers, latchMap, acks, getDatacenter);
             Gossiper.instance.register(listener);
     
             // send out a ping message to open up the non-gossip connections
    -        sendPingMessages(peers, latch, acks);
    +        sendPingMessages(peers, latchMap, acks, getDatacenter);
    --- End diff --
    
    Comment that pings are sent to all peers even if latchMap doesn't contain a latch for them.
;29/Oct/18 22:30;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229109638
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    --- End diff --
    
    Bikeshedding, but datacenterMap is not very descriptive. What it is is a peerToDatacenter. You can also omit mpa it's kind of redundant with XtoY which automatically implies it's a map relationship. Multimaps are handles by making Y plural. I really think it's worth using better naming here.
    
    Also pulling out getDataCenter is a few characters more succinct, but I would rather see XtoY::get repeated a few times. If it's not referring to the source then the name also needs to be more representative like getDatacenterFromPeer which is basically just another way of saying peerToDatacenter::get.
;29/Oct/18 22:30;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231680735
  
    --- Diff: src/java/org/apache/cassandra/config/Config.java ---
    @@ -380,9 +380,23 @@
         public RepairCommandPoolFullStrategy repair_command_pool_full_strategy = RepairCommandPoolFullStrategy.queue;
         public int repair_command_pool_size = concurrent_validations;
     
    -    // parameters to adjust how much to delay startup until a certain amount of the cluster is connect to and marked alive
    -    public int block_for_peers_percentage = 70;
    +    /**
    +     * When a node first starts up it intially considers all other peers as DOWN, and then as the initial gossip
    +     * broadcast messages comes back nodes transition to UP. These options configure how long we wait for peers to
    +     * connect before we make this node available as a coordinator. Furthermore, if this feature is enabled
    +     * (timeout >= 0) Cassandra initiates the non gossip channel internode connections on startup as well and waits
    --- End diff --
    
    Ok, I reworded it. I think that having an escape hatch makes sense, what do you think? 
;07/Nov/18 21:20;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231681120
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    --- End diff --
    
    I thought it should be for the same reason we warn them if they set the timeout to more than 100, but I don't care too much. I removed it.
;07/Nov/18 21:21;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231688756
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    --- End diff --
    
    Sure thing, makes sense and I've made both refactors.
;07/Nov/18 21:46;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231688792
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    --- End diff --
    
    Sure, done.
;07/Nov/18 21:46;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231691984
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    +                                                              .collect(Collectors.groupingBy(getDatacenter,
    +                                                                                             Collectors.toSet()));
    +
    +        if (!blockForRemoteDcs)
    +        {
    +            peersByDc.keySet().retainAll(Collections.singleton(localDc));
    +            logger.info(""Blocking coordination until only a single peer is DOWN in the local datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
    +        else
    +        {
    +            logger.info(""Blocking coordination until only a single peer is DOWN in each datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
     
             AckMap acks = new AckMap(3);
    -        int target = (int) ((targetPercent / 100.0) * peers.size());
    -        CountDownLatch latch = new CountDownLatch(target);
    +        Map<String, CountDownLatch> latchMap = new HashMap<>(peersByDc.size());
    +        for (Map.Entry<String, Set<InetAddressAndPort>> entry: peersByDc.entrySet())
    +        {
    +            latchMap.put(entry.getKey(), new CountDownLatch(Math.max(entry.getValue().size() - 1, 0)));
    +        }
    +
    +        long startNanos = System.nanoTime();
     
             // set up a listener to react to new nodes becoming alive (in gossip), and account for all the nodes that are already alive
    -        Set<InetAddressAndPort> alivePeers = Sets.newSetFromMap(new ConcurrentHashMap<>());
    -        AliveListener listener = new AliveListener(alivePeers, latch, acks);
    +        Set<InetAddressAndPort> alivePeers = Collections.newSetFromMap(new ConcurrentHashMap<>());
    +        AliveListener listener = new AliveListener(alivePeers, latchMap, acks, getDatacenter);
             Gossiper.instance.register(listener);
     
             // send out a ping message to open up the non-gossip connections
    -        sendPingMessages(peers, latch, acks);
    +        sendPingMessages(peers, latchMap, acks, getDatacenter);
     
             for (InetAddressAndPort peer : peers)
    +        {
                 if (Gossiper.instance.isAlive(peer) && alivePeers.add(peer) && acks.incrementAndCheck(peer))
    -                latch.countDown();
    +            {
    +                String datacenter = getDatacenter.apply(peer);
    +                if (latchMap.containsKey(datacenter))
    +                    latchMap.get(datacenter).countDown();
    +            }
    +        }
    +
    +        boolean succeeded = Uninterruptibles.awaitUninterruptibly(latchMap.get(localDc), timeoutNanos, TimeUnit.NANOSECONDS);
    --- End diff --
    
    I think the loop is requires because this main method can race with the callbacks that are triggered as part of {{sendPingMessages}}. There is no point in the {{containsKey}} though so I just refactored back to what was there before.
;07/Nov/18 21:55;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231693230
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    +                                                              .collect(Collectors.groupingBy(getDatacenter,
    +                                                                                             Collectors.toSet()));
    +
    +        if (!blockForRemoteDcs)
    +        {
    +            peersByDc.keySet().retainAll(Collections.singleton(localDc));
    +            logger.info(""Blocking coordination until only a single peer is DOWN in the local datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
    +        else
    +        {
    +            logger.info(""Blocking coordination until only a single peer is DOWN in each datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
     
             AckMap acks = new AckMap(3);
    -        int target = (int) ((targetPercent / 100.0) * peers.size());
    -        CountDownLatch latch = new CountDownLatch(target);
    +        Map<String, CountDownLatch> latchMap = new HashMap<>(peersByDc.size());
    +        for (Map.Entry<String, Set<InetAddressAndPort>> entry: peersByDc.entrySet())
    +        {
    +            latchMap.put(entry.getKey(), new CountDownLatch(Math.max(entry.getValue().size() - 1, 0)));
    +        }
    +
    +        long startNanos = System.nanoTime();
     
             // set up a listener to react to new nodes becoming alive (in gossip), and account for all the nodes that are already alive
    -        Set<InetAddressAndPort> alivePeers = Sets.newSetFromMap(new ConcurrentHashMap<>());
    -        AliveListener listener = new AliveListener(alivePeers, latch, acks);
    +        Set<InetAddressAndPort> alivePeers = Collections.newSetFromMap(new ConcurrentHashMap<>());
    +        AliveListener listener = new AliveListener(alivePeers, latchMap, acks, getDatacenter);
             Gossiper.instance.register(listener);
     
             // send out a ping message to open up the non-gossip connections
    -        sendPingMessages(peers, latch, acks);
    +        sendPingMessages(peers, latchMap, acks, getDatacenter);
     
             for (InetAddressAndPort peer : peers)
    +        {
                 if (Gossiper.instance.isAlive(peer) && alivePeers.add(peer) && acks.incrementAndCheck(peer))
    -                latch.countDown();
    +            {
    +                String datacenter = getDatacenter.apply(peer);
    +                if (latchMap.containsKey(datacenter))
    +                    latchMap.get(datacenter).countDown();
    +            }
    +        }
    +
    +        boolean succeeded = Uninterruptibles.awaitUninterruptibly(latchMap.get(localDc), timeoutNanos, TimeUnit.NANOSECONDS);
    +        for (String datacenter: latchMap.keySet())
    +        {
    +            if (datacenter.equals(localDc))
    --- End diff --
    
    Makes sense, removed
;07/Nov/18 21:58;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231695413
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    +                                                              .collect(Collectors.groupingBy(getDatacenter,
    +                                                                                             Collectors.toSet()));
    +
    +        if (!blockForRemoteDcs)
    +        {
    +            peersByDc.keySet().retainAll(Collections.singleton(localDc));
    +            logger.info(""Blocking coordination until only a single peer is DOWN in the local datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
    +        else
    +        {
    +            logger.info(""Blocking coordination until only a single peer is DOWN in each datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
     
             AckMap acks = new AckMap(3);
    -        int target = (int) ((targetPercent / 100.0) * peers.size());
    -        CountDownLatch latch = new CountDownLatch(target);
    +        Map<String, CountDownLatch> latchMap = new HashMap<>(peersByDc.size());
    --- End diff --
    
    I called it `dcToRemainingPeers` to try to convey what it was counting down. Let me know if that's not clear and I'll change it.
;07/Nov/18 22:05;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231695792
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    +                                                              .collect(Collectors.groupingBy(getDatacenter,
    +                                                                                             Collectors.toSet()));
    +
    +        if (!blockForRemoteDcs)
    +        {
    +            peersByDc.keySet().retainAll(Collections.singleton(localDc));
    --- End diff --
    
    Ack, added the comment.
;07/Nov/18 22:06;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231696207
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    +                                                              .collect(Collectors.groupingBy(getDatacenter,
    +                                                                                             Collectors.toSet()));
    +
    +        if (!blockForRemoteDcs)
    +        {
    +            peersByDc.keySet().retainAll(Collections.singleton(localDc));
    +            logger.info(""Blocking coordination until only a single peer is DOWN in the local datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
    +        else
    +        {
    +            logger.info(""Blocking coordination until only a single peer is DOWN in each datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
     
             AckMap acks = new AckMap(3);
    -        int target = (int) ((targetPercent / 100.0) * peers.size());
    -        CountDownLatch latch = new CountDownLatch(target);
    +        Map<String, CountDownLatch> latchMap = new HashMap<>(peersByDc.size());
    +        for (Map.Entry<String, Set<InetAddressAndPort>> entry: peersByDc.entrySet())
    +        {
    +            latchMap.put(entry.getKey(), new CountDownLatch(Math.max(entry.getValue().size() - 1, 0)));
    +        }
    +
    +        long startNanos = System.nanoTime();
     
             // set up a listener to react to new nodes becoming alive (in gossip), and account for all the nodes that are already alive
    -        Set<InetAddressAndPort> alivePeers = Sets.newSetFromMap(new ConcurrentHashMap<>());
    -        AliveListener listener = new AliveListener(alivePeers, latch, acks);
    +        Set<InetAddressAndPort> alivePeers = Collections.newSetFromMap(new ConcurrentHashMap<>());
    +        AliveListener listener = new AliveListener(alivePeers, latchMap, acks, getDatacenter);
             Gossiper.instance.register(listener);
     
             // send out a ping message to open up the non-gossip connections
    -        sendPingMessages(peers, latch, acks);
    +        sendPingMessages(peers, latchMap, acks, getDatacenter);
    --- End diff --
    
    Done.
;07/Nov/18 22:07;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231696993
  
    --- Diff: test/unit/org/apache/cassandra/net/StartupClusterConnectivityCheckerTest.java ---
    @@ -36,13 +36,34 @@
     import org.apache.cassandra.gms.Gossiper;
     import org.apache.cassandra.gms.HeartBeatState;
     import org.apache.cassandra.locator.InetAddressAndPort;
    +import org.apache.cassandra.utils.FBUtilities;
     
     import static org.apache.cassandra.net.async.OutboundConnectionIdentifier.ConnectionType.SMALL_MESSAGE;
     
     public class StartupClusterConnectivityCheckerTest
     {
    -    private StartupClusterConnectivityChecker connectivityChecker;
    +    private StartupClusterConnectivityChecker localQuorumConnectivityChecker;
    +    private StartupClusterConnectivityChecker globalQuorumConnectivityChecker;
    +    private StartupClusterConnectivityChecker noopChecker;
    +    private StartupClusterConnectivityChecker zeroWaitChecker;
    +
    +    private static final int NUM_PER_DC = 6;
         private Set<InetAddressAndPort> peers;
    +    private Set<InetAddressAndPort> peersA;
    +    private Set<InetAddressAndPort> peersAMinusLocal;
    +    private Set<InetAddressAndPort> peersB;
    +    private Set<InetAddressAndPort> peersC;
    +
    +    private String getDatacenter(InetAddressAndPort endpoint)
    +    {
    +        if (peersA.contains(endpoint))
    +            return ""datacenterA"";
    +        if (peersB.contains(endpoint))
    +            return ""datacenterB"";
    +        else if (peersC.contains(endpoint))
    +            return ""datacenterC"";
    +        return ""NA"";
    --- End diff --
    
    Right, so it'll fail fast, done.
;07/Nov/18 22:09;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231697341
  
    --- Diff: test/unit/org/apache/cassandra/net/StartupClusterConnectivityCheckerTest.java ---
    @@ -69,33 +113,102 @@ public void tearDown()
         @Test
         public void execute_HappyPath()
         {
    -        Sink sink = new Sink(true, true);
    +        Sink sink = new Sink(true, true, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertTrue(connectivityChecker.execute(peers));
    +        Assert.assertTrue(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
             checkAllConnectionTypesSeen(sink);
         }
     
         @Test
         public void execute_NotAlive()
         {
    -        Sink sink = new Sink(false, true);
    +        Sink sink = new Sink(false, true, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertFalse(connectivityChecker.execute(peers));
    +        Assert.assertFalse(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
             checkAllConnectionTypesSeen(sink);
         }
     
         @Test
         public void execute_NoConnectionsAcks()
         {
    -        Sink sink = new Sink(true, false);
    +        Sink sink = new Sink(true, false, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertFalse(connectivityChecker.execute(peers));
    +        Assert.assertFalse(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
    +    }
    +
    +    @Test
    +    public void execute_LocalQuorum()
    +    {
    +        // local peer plus 3 peers from same dc shouldn't pass (4/6)
    +        Set<InetAddressAndPort> available = new HashSet<>();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC - 3);
    +        checkAvailable(localQuorumConnectivityChecker, available, false, true);
    +
    +        // local peer plus 4 peers from same dc should pass (5/6)
    +        available.clear();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC - 2);
    +        checkAvailable(localQuorumConnectivityChecker, available, true, true);
    +    }
    +
    +    @Test
    +    public void execute_GlobalQuorum()
    +    {
    +        // local dc passing shouldn'nt pass globally with two hosts down in datacenterB
    --- End diff --
    
    Ack, fixed.
;07/Nov/18 22:10;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231705256
  
    --- Diff: test/unit/org/apache/cassandra/net/StartupClusterConnectivityCheckerTest.java ---
    @@ -69,33 +113,102 @@ public void tearDown()
         @Test
         public void execute_HappyPath()
         {
    -        Sink sink = new Sink(true, true);
    +        Sink sink = new Sink(true, true, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertTrue(connectivityChecker.execute(peers));
    +        Assert.assertTrue(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
             checkAllConnectionTypesSeen(sink);
         }
     
         @Test
         public void execute_NotAlive()
         {
    -        Sink sink = new Sink(false, true);
    +        Sink sink = new Sink(false, true, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertFalse(connectivityChecker.execute(peers));
    +        Assert.assertFalse(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
             checkAllConnectionTypesSeen(sink);
         }
     
         @Test
         public void execute_NoConnectionsAcks()
         {
    -        Sink sink = new Sink(true, false);
    +        Sink sink = new Sink(true, false, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertFalse(connectivityChecker.execute(peers));
    +        Assert.assertFalse(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
    +    }
    +
    +    @Test
    +    public void execute_LocalQuorum()
    +    {
    +        // local peer plus 3 peers from same dc shouldn't pass (4/6)
    +        Set<InetAddressAndPort> available = new HashSet<>();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC - 3);
    +        checkAvailable(localQuorumConnectivityChecker, available, false, true);
    +
    +        // local peer plus 4 peers from same dc should pass (5/6)
    +        available.clear();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC - 2);
    +        checkAvailable(localQuorumConnectivityChecker, available, true, true);
    +    }
    +
    +    @Test
    +    public void execute_GlobalQuorum()
    +    {
    +        // local dc passing shouldn'nt pass globally with two hosts down in datacenterB
    +        Set<InetAddressAndPort> available = new HashSet<>();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC - 2);
    +        copyCount(peersB, available, NUM_PER_DC - 2);
    +        copyCount(peersC, available, NUM_PER_DC - 1);
    +        checkAvailable(globalQuorumConnectivityChecker, available, false, true);
    +
    +        available.clear();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC -2);
    +        copyCount(peersB, available, NUM_PER_DC - 1);
    +        copyCount(peersC, available, NUM_PER_DC - 1);
    +        checkAvailable(globalQuorumConnectivityChecker, available, true, true);
    +    }
    +
    +    @Test
    +    public void execute_Noop()
    +    {
    +        checkAvailable(noopChecker, new HashSet<>(), true, false);
    +    }
    +
    +    @Test
    +    public void execute_ZeroWait()
    +    {
    +        checkAvailable(zeroWaitChecker, new HashSet<>(), false, false);
    --- End diff --
    
    Makes sense, I refactored this test to look at that instead of just returning immediately.
;07/Nov/18 22:37;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231711085
  
    --- Diff: src/java/org/apache/cassandra/config/Config.java ---
    @@ -376,9 +376,31 @@
     
         public String full_query_log_dir = null;
     
    -    // parameters to adjust how much to delay startup until a certain amount of the cluster is connect to and marked alive
    -    public int block_for_peers_percentage = 70;
    +    /**
    +     * When a node first starts up it intially thinks all other peers are DOWN, and then as the initial gossip
    +     * broadcast messages comes back nodes transition to UP. These options configure how many nodes can remain in
    +     * DOWN state before we make this node available as a coordinator, as well as an overall timeout on this process
    +     * to ensure that startup is not delayed too much.
    +     *
    +     * The defaults are tuned for LOCAL_ONE consistency levels with RF=3, and have natural settings for other CLs:
    +     *
    +     *     Consistency Level | local_dc     | all_dcs
    +     *     --------------------------------------------------------
    +     *     LOCAL_ONE         | default (2)  | default (any)
    +     *     LOCAL_QUORUM      | 1            | default (any)
    +     *     ONE               | any          | RF - 1
    +     *     QUORUM            | any          | (RF / 2) - 1
    +     *     ALL               | default      | 0
    +     *
    +     * A concrete example with QUORUM would be if you have 3 replicas in 2 datacenters, then you would set
    +     * block_for_peers_all_dcs to (6 / 2) - 1 = 2 because that guarantees that at most 2 hosts in all datacenters
    +     * are down when you start taking client traffic, which should satistfy QUORUM for all RF=6 QUORUM queries.
    +     */
    +    public int block_for_peers_local_dc = 2;
    --- End diff --
    
    We ended up going the direction of a boolean.
;07/Nov/18 23:00;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r232731480
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    --- End diff --
    
    The reason I thought it was strange was because -1 was a valid config option.
;12/Nov/18 16:43;githubbot;600","Github user jolynch closed the pull request at:

    https://github.com/apache/cassandra/pull/212
;12/Nov/18 23:27;githubbot;600","Github user jolynch commented on the issue:

    https://github.com/apache/cassandra/pull/212
  
    Closing as this was committed in https://github.com/apache/cassandra/commit/801cb70ee811c956e987718a00695638d5bec1b6
;12/Nov/18 23:27;githubbot;600",,0,25200,,,0,25200,,,,,,,,,,,,,,,CASSANDRA-14001,CASSANDRA-13993,,,,,,,,,,,,,,,,,,,,,,,,0.0,jolynch,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 12 18:45:42 UTC 2018,,,,,,,,,,,"0|i3r0lb:",9223372036854775807,,,,,,,,,aweisberg,,aweisberg,,,Low,,,,,,,,,,,,,,,,,,,"27/Mar/18 01:01;githubbot;GitHub user jolynch opened a pull request:

    https://github.com/apache/cassandra/pull/212

    Rework the wait for healthy logic to count down nodes for CASSANDRA-14297

    This improves on the wait for healthy work from CASSANDRA-13993 to
    solve CASSANDRA-14297. It allows the cluster owners to fine tune the
    wait for behaviour to ensure availability of their application during
    rolling restarts. The defaults are to wait for all but one local DC host
    to be alive and up, and not care about remote DC hosts.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jolynch/cassandra CASSANDRA-14297

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/212.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #212
    
----
commit 74373297ae80b44938de5c04078b391f04d63b79
Author: Joseph Lynch <joe.e.lynch@...>
Date:   2018-02-21T05:12:21Z

    Rework the wait for healthy logic to count down nodes
    
    This improves on the wait for healthy work from CASSANDRA-13993 to
    solve CASSANDRA-14297. It allows the cluster owners to fine tune the
    wait for behaviour to ensure availability of their application during
    rolling restarts. The defaults are to wait for all but one local DC host
    to be alive and up, and not care about remote DC hosts.

----
;;;","27/Mar/18 01:05;jolynch;First implementation up on github along with a lot of unit tests. I'll start doing some more e2e testing using ccm just to make sure all the edge cases are covered but if someone ([~aweisberg] or [~jasobrown] perhaps) wants to review that would be excellent.
||trunk||
|[pull request|https://github.com/apache/cassandra/pull/212]|
|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14297.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14297] |

 ;;;","23/May/18 16:01;sumanth.pasupuleti;LGTM. +1;;;","26/May/18 00:17;jolynch;Looks like CASSANDRA-14447 refactoring has given me a nasty merge conflict. I'll work on rebasing the patchset but if a someone has time to give me quick feedback on if this idea is mergeable I'd appreciate it before investing more time into it. I do think this change makes the connectivity checker feature much more useful for operators trying to restart their databases without dropping traffic.;;;","24/Aug/18 00:46;jolynch;I updated the patch to fix the merge conflicts, and reduced it two just two options to make life easier (the default is tuned to wait for all but 2 local DC nodes and not care about non local DC == local_quorum).

This is ready for review. I hope we get it in before 4.0 because the user interface of a percentage is something users can't reliably set (compared to the count where there are correct answers for each use case).;;;","26/Sep/18 17:18;jolynch;I'm changing this to a bug since I think the current user interface is not possible for users to correctly configure and I hope we don't ship 4.0 with the percentage option instead of a count. If someone thinks that there are plausible settings of the existing configuration options users can use we can change this back to an improvement.;;;","25/Oct/18 00:16;jolynch;Alright, per the discussion on [IRC|https://wilderness.apache.org/channels/?f=cassandra-dev/2018-10-17#1539793033] with Ariel and Jason, we've decided that instead of counts we should always wait for all but a single local DC node and replace the percentage option with:
{noformat}
block_for_remote_dcs: <boolean, default: false>
{noformat}
The startup connectivity checker will wait for all but a single node in the local datacenter, and if you want to block startup on every datacenter having only a single node down you can set this to true.

The timeout will be the fallback for when multiple nodes are down in a local DC.;;;","26/Oct/18 01:59;jolynch;Ok I've uploaded a patch to my branch that does what was asked in IRC I believe. Let me know if it looks good and I can run dtests and such against it.;;;","01/Nov/18 14:51;aweisberg;I left some comments on the PR. Looks good.;;;","07/Nov/18 23:01;jolynch;[~aweisberg] Awesome I just rebased and merged in your suggestions:

||trunk||
|[fdd8173f|https://github.com/apache/cassandra/pull/212/commits/fdd8173f]|
|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14297.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14297] |

Dtests are running.;;;","12/Nov/18 17:27;aweisberg;+1 🚢 it.

There is one unused import in [StartupClusterConnectivityCheckerTest.java|https://github.com/apache/cassandra/pull/212/files#diff-c74adeeae072ee4af35c12a157cd7d61L26] I'll fix on commit.;;;","12/Nov/18 17:47;aweisberg;Committed as [801cb70ee811c956e987718a00695638d5bec1b6|https://github.com/apache/cassandra/commit/801cb70ee811c956e987718a00695638d5bec1b6] thanks!

I also added a NEWS.txt and CHANGES.txt entries. I also added Patchy by XYZ; Reviewed by XYZ for CASSANDRA-14297 to the commit message.;;;","12/Nov/18 18:45;jolynch;Sweet, thanks! Yea I was holding off on adding the NEWs/CHANGES entries until you marked it ready to commit. In the future I'll include it with the dtest run.

Thanks for all the great feedback, I think this feature is much more valuable now to users.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix batch commitlog sync regression,CASSANDRA-14292,13142731,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasonstack,jasonstack,jasonstack,06/Mar/18 01:55,15/May/20 08:04,13/Jul/23 08:37,07/Mar/18 12:12,3.0.17,3.11.3,4.0,4.0-alpha1,,,,,,,0,,,,"Prior to CASSANDRA-13987, in batch commitlog mode, commitlog will be synced to disk right after mutation comes.
 * haveWork semaphore is released in BatchCommitLogService.maybeWaitForSync
 * AbstractCommitlogService will continue and sync to disk

After C-13987, it makes a branch for chain maker flush more frequently in periodic mode. To make sure in batch mode CL still flushes immediately, it added {{syncRequested}} flag.
 Unfortunately, in 3.0 branch, this flag is not being set to true when mutation is waiting.

So in AbstractCommitlogService, it will not execute the CL sync branch until it reaches sync window(2ms)..
{code:java|title=AbstractCommitLogService.java}
if (lastSyncedAt + syncIntervalMillis <= pollStarted || shutdown || syncRequested)
{
    // in this branch, we want to flush the commit log to disk
    syncRequested = false;
    commitLog.sync(shutdown, true);
    lastSyncedAt = pollStarted;
    syncComplete.signalAll();
}
else
{
    // in this branch, just update the commit log sync headers
    commitLog.sync(false, false);
}
{code}",,aweisberg,jasobrown,jasonstack,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14108,,,CASSANDRA-13987,,,,,,,,,,,,"07/Mar/18 01:27;jasonstack;14292-3.0-dtest.png;https://issues.apache.org/jira/secure/attachment/12913305/14292-3.0-dtest.png","07/Mar/18 01:27;jasonstack;14292-3.0-unittest.png;https://issues.apache.org/jira/secure/attachment/12913306/14292-3.0-unittest.png",,,,,,,,,,,,2.0,jasonstack,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 08 10:25:53 UTC 2018,,,,,,,,,,,"0|i3qwk7:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"06/Mar/18 02:02;jasonstack;|[3.0|https://github.com/jasonstack/cassandra/commits/CASSANDRA-14292-3.0]| unit running |dtest running|

Changes:

1. Set {{syncRequested}} to true in {{BatchCommitLogService.maybeWaitForSync, so batch CL will sync immediately when mutation comes}}

2. Added unit test to verify batch CL sync and shutdown immediately;;;","06/Mar/18 03:48;jasobrown;Please take a look to see if CASSANDRA-14194 (committed today) helps. I'll read this more thoroughly, as well.;;;","06/Mar/18 05:05;jasonstack;CASSANDRA-14194 was solving race condition with multiple mutations. This ticket is to solve performance for single mutation.

In 3.0 branch with 13987, each mutation will have to wait for full batch-sync-window(2ms) for commitlog to sync.;;;","06/Mar/18 16:21;aweisberg;[~jasobrown] looks like it wasn't registering to receive notification of the sync before requesting the sync. It's another way for {{syncRequested}} to get clobbered.

I'm +1 on this change. Is it a 3.11 bug as well?;;;","06/Mar/18 16:24;jasonstack;It’s 3.0.16 only..;;;","07/Mar/18 00:19;jasobrown;[~jasonstack] Yeah, you are correct; I missed this case. This is the problem when working on the same patch across four branches; I'm sure I had something like this at one point in a 3.0 branch .... <sigh>

I'm +1 on the patch, as well. Thanks for the unit test - I like it enough that I'll forward port it to 3.11 and trunk when I commit.;;;","07/Mar/18 12:12;jasobrown;committed as sha {{00c90c16e99fdfb153cb418f0e3486b62183986e}}. Thanks!;;;","08/Mar/18 10:25;jasonstack;Thanks for the review.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexOutOfBoundsException with SELECT JSON using IN and ORDER BY,CASSANDRA-14286,13142178,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,fcofdezc,accek-invinets,accek-invinets,02/Mar/18 19:26,15/May/20 08:01,13/Jul/23 08:37,17/Apr/18 10:45,2.2.13,3.0.17,3.11.3,4.0,4.0-alpha1,,Legacy/CQL,,,,0,,,,"When running the following code:

{code}
public class CassandraJsonOrderingBug {
    public static void main(String[] args) {
        Session session = CassandraFactory.getSession();

        session.execute(""CREATE TABLE thebug ( PRIMARY KEY (a, b), a INT, b INT)"");
        try {
            session.execute(""INSERT INTO thebug (a, b) VALUES (20, 30)"");
            session.execute(""INSERT INTO thebug (a, b) VALUES (100, 200)"");
            Statement statement = new SimpleStatement(""SELECT JSON a, b FROM thebug WHERE a IN (20, 100) ORDER BY b"");
            statement.setFetchSize(Integer.MAX_VALUE);
            for (Row w: session.execute(statement)) {
                System.out.println(w.toString());
            }
        } finally {
            session.execute(""DROP TABLE thebug"");
        }
    }
}
{code}

The following exception is thrown server-side:

{noformat}
java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
	at java.util.Collections$SingletonList.get(Collections.java:4815) ~[na:1.8.0_151]
	at org.apache.cassandra.cql3.statements.SelectStatement$SingleColumnComparator.compare(SelectStatement.java:1297) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.cql3.statements.SelectStatement$SingleColumnComparator.compare(SelectStatement.java:1284) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355) ~[na:1.8.0_151]
	at java.util.TimSort.sort(TimSort.java:220) ~[na:1.8.0_151]
	at java.util.Arrays.sort(Arrays.java:1512) ~[na:1.8.0_151]
	at java.util.ArrayList.sort(ArrayList.java:1460) ~[na:1.8.0_151]
	at java.util.Collections.sort(Collections.java:175) ~[na:1.8.0_151]
{noformat}

(full traceback attached)

The accessed index is the index of the sorted column in the SELECT JSON fields list.
Similarly, if the select clause is changed to

SELECT JSON b, a FROM thebug WHERE a IN (20, 100) ORDER BY b

then the query finishes, but the output is sorted incorrectly (by textual JSON representation):

{noformat}
Row[{""b"": 200, ""a"": 100}]
Row[{""b"": 30, ""a"": 20}]
{noformat}",Kubernetes cluster using cassandra:3.11.1 Docker image.,accek-invinets,blerer,fcofdezc,jeromatron,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/18 19:23;accek-invinets;orderbug-traceback.txt;https://issues.apache.org/jira/secure/attachment/12912816/orderbug-traceback.txt",,,,,,,,,,,,,1.0,fcofdezc,,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 17 10:45:04 UTC 2018,,,,,,,,,,,"0|i3qt53:",9223372036854775807,,,,,,,,,blerer,,blerer,,,Normal,,2.2.0,,,,,,,,,,,,,,,,,"22/Mar/18 10:21;fcofdezc;The problem was that columns were serialized as json and then discarded, while {{OrderingComparator}} expected those columns to be present. With this patch, all columns are present and only the json column is sent back to clients. It uses the same mechanism as the regular ordering of non-selected columns.

[Patch 2.2|https://github.com/apache/cassandra/compare/trunk...blerer:14286-2.2-review] 
[Patch 3.0|https://github.com/apache/cassandra/compare/trunk...fcofdez:14286-3.0?expand=1] 
[Patch 3.11|https://github.com/apache/cassandra/compare/trunk...fcofdez:14286-3.11?expand=1] 
[Patch trunk|https://github.com/apache/cassandra/compare/trunk...fcofdez:14286-trunk-2?expand=1] ;;;","04/Apr/18 15:17;blerer;Patches and test results look fine. Thanks.;;;","05/Apr/18 15:45;blerer;I just noticed while merging a problem with the trunk patch. In trunk {{Selection}} is immutable but the patch is breaking that contract.

Sorry, for noticing that problem only now.;;;","09/Apr/18 11:15;fcofdezc;I've updated the trunk patch with the suggested approach, let me know if there is something that should be changed. Thanks for the review.;;;","17/Apr/18 08:33;blerer;I looked at the trunk patch. For queries with selection clauses having some processing like: {{SELECT JSON a, CAST(b AS FLOAT) FROM %s WHERE a IN (20, 100) ORDER BY b}}. The query will fail with an {{IndexOutOfBoundException}} during the {{Selection}} initialization.

I looked into the problem and pushed a correction to the patch [here|https://github.com/apache/cassandra/compare/trunk...blerer:14286-trunk-review]. CI results look good.

[~fcofdezc] Could you have a look at my changes and tell me if they make sense to you?;;;","17/Apr/18 09:14;fcofdezc;I see the problem now, thanks for pointing out. The code seems less complicated with your approach, so +1. Only one nit: there is a {{println}} on {{Selection.java}}.;;;","17/Apr/18 10:45;blerer;Committed into 2.2 at 594cde7937de6f848998bac35d35591f8a18aa10 and merged into 3.0, 3.11 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Comma at the end of the end of the seed list is interpretated as localhost,CASSANDRA-14285,13142066,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,nicolas.guyomar,usernkey,usernkey,02/Mar/18 10:43,15/May/20 08:01,13/Jul/23 08:37,02/Mar/18 22:33,4.0,4.0-alpha1,,,,,Local/Config,,,,0,,,,"Seeds: '10.1.20.10,10.1.21.10,10.1.22.10,'  cause a flood of the debug log with messages like this one.

DEBUG [MessagingService-Outgoing-localhost/127.0.0.1-Gossip] 2018-02-28 15:53:57,314 OutboundTcpConnection.java:545 - Unable to connect to localhost/[127.0.0.1|http://127.0.0.1/]

This code provide by Nicolas Guyomar provide the reason of the issue.

In SImpleSeedProvider : 
 
String[] hosts = ""10.1.20.10,10.1.21.10,10.1.22.10,"".split("","", -1);
List<InetAddress> seeds = new ArrayList<InetAddress>(hosts.length);
for (String host : hosts)
{
System.out.println(InetAddress.getByName(host.trim()));
}
 
output : 
/[10.1.20.10|http://10.1.20.10/]
/[10.1.21.10|http://10.1.21.10/]
/[10.1.22.10|http://10.1.22.10/]
localhost/[127.0.0.1|http://127.0.0.1/]",,jasobrown,jeromatron,jwest,KurtG,nicolas.guyomar,usernkey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,nicolas.guyomar,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 06 01:23:23 UTC 2018,,,,,,,,,,,"0|i3qsgf:",9223372036854775807,3.11.1,,,,,,,,jwest,,jwest,,,Low,,,,,,,,,,,,,,,,,,,"02/Mar/18 11:08;nicolas.guyomar;Could this one be added to Cassandra ? [https://github.com/nicolasguyomar/cassandra/commit/0cb5cb565c84a49ed08b216bb2a0e75ec1d2fa2c]

First small contribution;;;","02/Mar/18 20:33;jwest;+1. As an added bonus it also fixes the case where there is a double comma in the string. Don't see an interpretation of the config where the operator intended to include localhost as a seed so I think the ""breaking"" change is a good one (i.e no one should be expecting this behavior). Since its breaking, I guess we can ignore back porting to 3.11 – the patch doesn't apply clean either. 

I wanted to ask you to add a test but it looks like that will be a pain given how {{SimpleSeedProvider}} is currently implemented.

Branch: [https://github.com/jrwest/cassandra/tree/14285-trunk]

utests: [https://circleci.com/gh/jrwest/cassandra/37]

 ;;;","02/Mar/18 22:33;jasobrown;committed as sha {{2dbbbc57dbb70886417c888e2bf8e01da78ff6ee}}. Thanks, all!;;;","06/Sep/18 01:23;KurtG;IDK how many people were doing it, or if we do it elsewhere in ccm/our testing, but this also means you can't have an empty seed list - which used to resolve to localhost. Honestly, that's probably less surprising, especially considering it's not documented at the moment, but thought I'd mention it here because it surprised me.

FTR, if you're using {{ccm add}}, you now _have_ to specify {{-s}} for at least one node. If people think this is annoying or find other issues we can easily add to this patch to handle that case.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Chunk checksum test needs to occur before uncompress to avoid JVM crash,CASSANDRA-14284,13141910,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,blerer,giltene,giltene,01/Mar/18 21:12,15/May/20 08:05,13/Jul/23 08:37,10/Apr/18 12:15,2.1.21,2.2.13,3.0.17,3.11.3,4.0,4.0-alpha1,Legacy/Core,,,,0,,,,"While checksums are (generally) performed on compressed data, the checksum test when reading is currently (in all variants of C* 2.x, 3.x I've looked at) done [on the compressed data] only after the uncompress operation has completed. 

The issue here is that LZ4_decompress_fast (as documented in e.g. [https://github.com/lz4/lz4/blob/dev/lib/lz4.h#L214)] can result in memory overruns when provided with malformed source data. This in turn can (and does, e.g. in CASSANDRA-13757) lead to JVM crashes during the uncompress of corrupted chunks. The checksum operation would obviously detect the issue, but we'd never get to it if the JVM crashes first.

Moving the checksum test of the compressed data to before the uncompress operation (in cases where the checksum is done on compressed data) will resolve this issue.

-----------------------------

The check-only-after-doing-the-decompress logic appears to be in all current releases.

Here are some samples at different evolution points :

3.11.2:

[https://github.com/apache/cassandra/blob/cassandra-3.11.2/src/java/org/apache/cassandra/io/util/CompressedChunkReader.java#L146]

https://github.com/apache/cassandra/blob/cassandra-3.11.2/src/java/org/apache/cassandra/io/util/CompressedChunkReader.java#L207

 

3.5:

 [https://github.com/apache/cassandra/blob/cassandra-3.5/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java#L135]

[https://github.com/apache/cassandra/blob/cassandra-3.5/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java#L196]

2.1.17:

 [https://github.com/apache/cassandra/blob/cassandra-2.1.17/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java#L122]","The check-only-after-doing-the-decompress logic appears to be in all current releases.

Here are some samples at different evolution points :

3.11.2:

[https://github.com/apache/cassandra/blob/cassandra-3.11.2/src/java/org/apache/cassandra/io/util/CompressedChunkReader.java#L146]

https://github.com/apache/cassandra/blob/cassandra-3.11.2/src/java/org/apache/cassandra/io/util/CompressedChunkReader.java#L207

 

3.5:

 [https://github.com/apache/cassandra/blob/cassandra-3.5/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java#L135]

[https://github.com/apache/cassandra/blob/cassandra-3.5/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java#L196]

2.1.17:

 [https://github.com/apache/cassandra/blob/cassandra-2.1.17/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java#L122]

 ",blambov,blerer,eparusel,giltene,jeromatron,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14283,CASSANDRA-13757,CASSANDRA-13757,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,blerer,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 10 12:15:56 UTC 2018,,,,,,,,,,,"0|i3qrhr:",9223372036854775807,,,,,,,,,blambov,,blambov,,,Normal,,,,,,,,,,,,,,,,,,,"28/Mar/18 12:50;blerer;||Branches|[2.1|https://github.com/apache/cassandra/compare/cassandra-2.1...blerer:14284-2.1]|[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...blerer:14284-2.2]|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...blerer:14284-3.0]|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...blerer:14284-3.11]|[trunk|https://github.com/apache/cassandra/compare/trunk...blerer:14284-trunk]|

 I ran the unit tests and the dtests on our internal CI and the failing tests seems unrelated.;;;","28/Mar/18 13:29;blambov;The patches before 3.11 look good, but I'm not very happy with the separate buffer for the checksum in 3.11+. In fact, since we are doing the checksum immediately after the read now, why not include the checksum read with the compressed content? I.e. reserve four extra bytes at the end of the buffer, choose if checksum will be done before the read and adjust its length if so, then proceed like in the memmapped case.

Alternatively, you could use the first bytes of the uncompressed buffer.

The trunk patch seems to contain at least one odd character and the diff isn't displayed properly. Could you fix the line endings and upload it again?;;;","28/Mar/18 15:40;giltene;The patch for 2.1 has an issue, I think: 2.1 (unlike the later versions) seems to support checksumming o either the compressed or uncompressed data (depending on what metadata.hasPostCompressionAdlerChecksums indicates). Only the checksum test of the compressed data can be moved to before the uncompress. The checksum in the uncompressed case has to remain after the uncompress.;;;","29/Mar/18 12:55;blerer;[~blambov], [~giltene] Thanks for the reviews. I completely missed the pre-compression checksum logic in 2.1.

I force pushed some new patches for [2.1|https://github.com/apache/cassandra/compare/cassandra-2.1...blerer:14284-2.1] , [3.1|https://github.com/apache/cassandra/compare/cassandra-3.11...blerer:14284-3.11], and [trunk|https://github.com/apache/cassandra/compare/trunk...blerer:14284-trunk] that address the different problems.;;;","29/Mar/18 13:13;blambov;Shouldn't [this|https://github.com/apache/cassandra/compare/cassandra-2.1...blerer:14284-2.1#diff-67366d25fce65930f46564d6ea8e0f40R139] be using {{bytes}} instead of {{compressed.array()}}?;;;","29/Mar/18 14:34;blambov;The rest looks fine with one nit, which you can feel free to ignore: you could now extract the crc checking logic to a common method for the normal and memmapped case (in trunk, the normal+uncompressed case has to remain separate, though).;;;","10/Apr/18 12:15;blerer;Committed into 2.1 at 34a1d5da58fb8edcad39633084541bb4162f5ede and merged into 2.2, 3.0, 3.11 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use zero as default score in DynamicEndpointSnitch,CASSANDRA-14252,13140065,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,dikanggu,dikanggu,dikanggu,21/Feb/18 23:46,15/May/20 08:07,13/Jul/23 08:37,20/Mar/18 22:19,4.0,4.0-alpha1,,,,,Legacy/Coordination,,,,0,,,,"The problem I want to solve is that I found in our deployment, one slow but alive data node can slow down the whole cluster, even caused timeout of our requests. 

We are using DynamicEndpointSnitch, with badness_threshold 0.1. I expect the DynamicEndpointSnitch switch to sortByProximityWithScore, if local data node latency is too high.

I added some debug log, and figured out that in a lot of cases, the score from remote data node was not populated, so the fallback to sortByProximityWithScore never happened. That's why a single slow data node, can cause huge problems to the whole cluster.

In this jira, I'd like to use zero as default score, so that we will get a chance to try remote data node, if local one is slow. 

I tested it in our test cluster, it improved the client latency in single slow data node case significantly.  

I flag this as a Bug, because it caused problems to our use cases multiple times.

 ==== logs ===

_2018-02-21_23:08:57.54145 WARN 23:08:57 [RPC-Thread:978]: sortByProximityWithBadness: after sorting by proximity, addresses order change to [ip1, ip2], with scores [1.0]_
 _2018-02-21_23:08:57.54319 WARN 23:08:57 [RPC-Thread:967]: sortByProximityWithBadness: after sorting by proximity, addresses order change to [ip1, ip2], with scores [0.0]_
 _2018-02-21_23:08:57.55111 WARN 23:08:57 [RPC-Thread:453]: sortByProximityWithBadness: after sorting by proximity, addresses order change to [ip1, ip2], with scores [1.0]_
 _2018-02-21_23:08:57.55687 WARN 23:08:57 [RPC-Thread:753]: sortByProximityWithBadness: after sorting by proximity, addresses order change to [ip1, ip2], with scores [1.0]_

 

 

 ",,aleksey,blerer,christopher.lambert,dikanggu,jay.zhuang,jeromatron,KurtG,rha,szhou,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14555,,,,,,,,,,"19/Mar/18 17:17;jay.zhuang;IMG_3180.jpg;https://issues.apache.org/jira/secure/attachment/12915165/IMG_3180.jpg",,,,,,,,,,,,,1.0,dikanggu,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 05 17:11:39 UTC 2018,,,,,,,,,,,"0|i3qg5r:",9223372036854775807,,,,,,,,,jay.zhuang,,jay.zhuang,,,Normal,,,,,,,,,,,,,,,,,,,"21/Feb/18 23:57;dikanggu;|[3.0 | https://github.com/DikangGu/cassandra/commit/163ad12c7c84cfd44932b814ce61c7a0145e9bcd]|[3.11 | https://github.com/DikangGu/cassandra/commit/980a7c36d410448b3714111b541193b68c952d34]|[trunk | https://github.com/DikangGu/cassandra/commit/fd00c51321f6252294b066a758bfaaa5ba810ea9]|[unit test | https://circleci.com/gh/DikangGu/cassandra/20]|;;;","26/Feb/18 20:59;szhou;This is an interesting change but I'm not sure it fixes all problems.

The code that you changed was introduced in CASSANDRA-13074, which also claims to fix slow node issue, by totally ignoring nodes that we don't have a score, no matter it's a node in local or remote data center. Now with your fix, we still give these (remote) nodes a try by assigning an artificially low score. However, isn't 0 the lowest score that could result in these slow/unresponsive remote nodes being picked up before other remote nodes that have normal scores (such as 1.0)?

Btw, badness_threshold=0.1 may be too conservative. We also disabled IO factor when calculating the scores through -Dcassandra.ignore_dynamic_snitch_severity=true. See CASSANDRA-11738 for details.;;;","27/Feb/18 01:33;dikanggu;[~szhou], 1, We will need to populate the scores for all replicas first, which should be finished within seconds. After that, a local and healthy node will always have lower score than remotes nodes, then we can stick to local node, and only switch to remote node if local node is bad, right?  2. Yeah, we back-ported that patch and disabled the IO factor for severity already.;;;","27/Feb/18 06:14;szhou;For nodes in the same remote data center, if we don't have score for one node because there is no read response yet and we set an artificially low score 0 for it, does it mean this node will be picked with higher probability than other nodes that have scores?;;;","27/Feb/18 19:00;dikanggu;[~szhou], Yes, it's the warm up phase. We have to know the distance/latency differences between different replicas, otherwise we will have no way to fall back to remote replicas. One idea to limit unnecessary requests to remote replica is to only fall back when local node is really bad. Something like this:

if ({color:red}subsnitchScore > 0.5{color} && subsnitchScore > (sortedScoreIterator.next() * (1.0 + dynamicBadnessThreshold)))
            {
                sortByProximityWithScore(address, addresses);
                return;
            }

of course, the param 0.5 can be tunable.
;;;","28/Feb/18 19:52;szhou;I think I haven't made it clear enough or I misunderstand your fix. Let's say you want to use nodes in remote data center because of whatever issue with local datacenter, my understanding is that:
- Either we don't use a node from remote data center that doesn't have score yet, because the reason could be that it's totally unresponsive to previous read requests but still responds to for example, gossip message, thus this node hasn't been marked as down. (A node doesn't have score may also because it hasn't received read request from the remote coordinator node yet, or all the scores got reset after dynamicResetInterval, both of which are less of a problem)
- Or maybe we can use it but it should be picked with lower probability than other nodes in the same remote data center.

Now you assign a low score of 0 to a node that doesn't have score yet. This means it will be picked with higher probability. If that node truly has problem (unresponsive to read requests), then your fix will cause higher latency. Having said that, I don't mind setting the node score as the highest one among all node scores from the same data center.;;;","28/Feb/18 20:07;dikanggu;[~szhou], hmm, it's might be easier to explain offline. Anyway, what I'm trying to do is to set a default score which is 0, to any node in the cluster. Then, as Cassandra dispatch the requests, it will get the correct latency among different nodes, and have correct score for each one. The node is in local or remote does not matter a lot actually. Just in the function `sortByProximityWithBadness`, we will consider local node first. If local node is slow but alive, we can fall back to remote node. Without a default score, we will NEVER have a chance to talk to a remote node, as long as local node is alive. ;;;","01/Mar/18 22:22;szhou;Talked with [~dikanggu] offline. Previously I thought that timeout wouldn't be counted as part of latency score. Actually it is, so setting replica score as 0 by default is less of a problem but only exposes a small vulnerability window:
Say if you have multiple replicas in a remote data center and you don't have score for one of them, thus it will be assigned score 0. This might cause traffic burst on this replica, for a short period of time and most of time it won't even be noticed.

This can be mitigated by assigning a larger score (such as maximum score of all the replicas) to the replica with null score. I'd defer this decision to [~dikanggu]. Otherwise the patch looks good to me.;;;","01/Mar/18 23:26;dikanggu;Thanks [~szhou]!  [~tjake], [~jasobrown], do you want to take a look as well? ;;;","19/Mar/18 17:36;jay.zhuang;Hi [~dikanggu], would you please help me to understand the scenario?
Assume there're 3 nodes: coordinator node, a degraded node, and a healthy node:

!IMG_3180.jpg!

When the issue happens, the coordinator node doesn't have the score for either degraded node nor healthy node, so it follows subsnitch ordering and always talk to the degraded node, is that right? Or are the coordinator node and the degraded node the same node?;;;","19/Mar/18 18:27;dikanggu;[~jay.zhuang], the scenario is:
 # Coordinator and node A is in DC1, node B is another replica of node A and in DC2, we use DynamicEndpointSnitch and NetworkTopologyStrategy.
 # In normal situation, coordinator only talks to node A, it has correct score of node A. Coordinator never talks to node B, and do not have score for node B.
 # Then node A is degraded, it is slow but still alive. Coordinator set the score of node A to be very high, like 1.
 # But still, Coordinator do not have score for node B, which makes it never has the chance to talk to node B, which is a healthy of the replica in a different region.

 

My patch is provide a default score for node B, so coordinator will have chance to talk to node B at least once, to get the correct latency number between coordinator and node B, and can use it to decide whether to switch from node A to node B, if necessary.;;;","19/Mar/18 21:59;jay.zhuang;Should ""Speculative Retry"" help in this situation?;;;","19/Mar/18 22:22;dikanggu;Could be. We had serious problems with speculative retry before, which overloaded hot replicas, so we turned it off in our production clusters. 

Still I feel it's bad for dynamic endpoint snitch can not fallback due to lack of a default score. ;;;","20/Mar/18 20:15;jay.zhuang;+1

Good catch. I created a dtest to reproduce the problem: [14252|https://github.com/cooldoger/cassandra-dtest/tree/14252]
Also when comparing 2 versions, the existing code uses {{0.0}} as default value: [{{DynamicEndpointSnitch.java:267}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/locator/DynamicEndpointSnitch.java#L267];;;","20/Mar/18 22:19;dikanggu;Thanks  [~jay.zhuang], committed as *f109f200a3a7f6002d7e1f6cc67e9ef5bf5cb2df*

;;;","20/Mar/18 22:34;jay.zhuang;Hi [~dikanggu], instead of committing the change to each branch separately, I think it would be better to merge up to later branches:
https://cassandra.apache.org/doc/latest/development/how_to_commit.html;;;","20/Mar/18 23:35;dikanggu;[~jay.zhuang], sure, almost forgot that wiki.;;;","21/Mar/18 09:20;blerer;[~dikanggu] If you do not merge the branches. The person behind you will be the one having to do it. When you do not expect it, it can cause you some problems.

Regarding the CHANGES.txt. You should only add the entry for the first branch that you modified and make sure that it appears in the {{merged from}} part of the other branches.;;;","21/Mar/18 17:18;szhou;[~dikanggu] FYI I had a [fix|https://issues.apache.org/jira/browse/CASSANDRA-13261] for ""overloading"" issue long time before. Not sure if it's the issue that you had.
;;;","23/Mar/18 14:51;aleksey;I fixed the incorrect CHANGES.txt. Please follow the agreed upon process when committing things, or ask around if you aren't sure how to do it next time. Thanks.;;;","23/Mar/18 16:00;dikanggu;sure, will do.;;;","05/Jul/18 17:11;jay.zhuang;The change is reverted from 3.0 and 3.11 branches. More details: CASSANDRA-14555;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
View replica is not written to pending endpoint when base replica is also view replica,CASSANDRA-14251,13140056,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,pauloricardomg,pauloricardomg,pauloricardomg,21/Feb/18 22:52,15/May/20 08:04,13/Jul/23 08:37,06/Mar/18 14:37,3.0.17,3.11.3,4.0,4.0-alpha1,,,Feature/Materialized Views,,,,0,,,,"From the [dev list|https://www.mail-archive.com/dev@cassandra.apache.org/msg12084.html]:

bq. There's an optimization that when we're lucky enough that the paired view replica is the same as this base replica, mutateMV doesn't use the normal view-mutation-sending code (wrapViewBatchResponseHandler) and just writes the mutation locally. In particular, in this case we do NOT write to the pending node (unless I'm missing something). But, sometimes all replicas will be paired with themselves - this can happen for example when number of nodes is equal to RF, or when the base and view table have the same partition keys (but different clustering keys). In this case, it seems the pending node will not be written at all...

This was a regression from CASSANDRA-13069 and the original behavior should be restored.",,christianmovi,jasonstack,jeromatron,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13069,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,pauloricardomg,,,,,,,,,,,,Correctness -> Recoverable Corruption / Loss,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 06 14:37:36 UTC 2018,,,,,,,,,,,"0|i3qg3r:",9223372036854775807,3.0.15,3.11.1,,,,,,,jasonstack,,jasonstack,,,Normal,,3.0.15,,,,,,,,,,,,,,,,,"21/Feb/18 23:21;pauloricardomg;Added regression [dtest|https://github.com/pauloricardomg/cassandra-dtest/commit/dbf6eba59da67edaa50930046fc440430fe61d2d] with different RFs and [restored|https://github.com/pauloricardomg/cassandra/commit/1a06c97a7896621f346b806e6369a820b75fdd75] correct behavior along with a NEWS.txt asking MV users potentially affected by this to run repair on the view to ensure all replicas are up to date.

||3.0||3.11||trunk||dtest||
|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-14251]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...pauloricardomg:3.11-14251]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14251]|[branch|https://github.com/apache/cassandra-dtest/compare/master...pauloricardomg:14251]|

Will run internal CI and post results here when ready.;;;","05/Mar/18 06:56;jasonstack;This patch LGTM,  thanks for the fix.;;;","06/Mar/18 14:37;pauloricardomg;Updated {{NEWS.TXT}} wording to state that potentially affected users must run repair in the base table and subsequently on the views, after [mailing list discussion|https://www.mail-archive.com/dev@cassandra.apache.org/msg12128.html].

Committed fix as {{c67338989f17257d3be95212ca6ecb4b83009326}} to cassandra-3.0 and merged up to cassandra-3.11 and trunk and dtest as {{8fa87f63dec7dd636473b620071d264893a19df8}}. Thanks for the review [~jasonstack].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SELECT JSON prints null on empty strings,CASSANDRA-14245,13139842,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,fcofdezc,nob13,nob13,21/Feb/18 09:11,15/May/20 08:03,13/Jul/23 08:37,29/Mar/18 06:57,3.11.3,4.0,4.0-alpha1,,,,Legacy/CQL,,,,0,,,,"SELECT JSON reports an empty string as null.

 

Example:
{code:java}
cqlsh:unittest> create table test(id INT, name TEXT, PRIMARY KEY(id));
cqlsh:unittest> insert into test (id, name) VALUES (1, 'Foo');
cqlsh:unittest> insert into test (id, name) VALUES (2, '');
cqlsh:unittest> insert into test (id, name) VALUES (3, null);

cqlsh:unittest> select * from test;

id | name
----+------
  1 |  Foo
  2 |     
  3 | null

(3 rows)

cqlsh:unittest> select JSON * from test;

[json]
--------------------------
{""id"": 1, ""name"": ""Foo""}
{""id"": 2, ""name"": null}
{""id"": 3, ""name"": null}

(3 rows){code}
 

This even happens, if the string is part of the Primary Key, which makes the generated string not insertable.

 
{code:java}
cqlsh:unittest> create table test2 (id INT, name TEXT, age INT, PRIMARY KEY(id, name));
cqlsh:unittest> insert into test2 (id, name, age) VALUES (1, '', 42);
cqlsh:unittest> select JSON * from test2;

[json]
------------------------------------
{""id"": 1, ""name"": null, ""age"": 42}

(1 rows)

cqlsh:unittest> insert into test2 JSON '{""id"": 1, ""name"": null, ""age"": 42}';
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Invalid null value in condition for column name""{code}
 

On an older version of Cassandra (3.0.8) does not have this problem.","Cassandra 3.11.2, Ubuntu 16.04 LTS

 ",blerer,BrunoZ,fcofdezc,nob13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,fcofdezc,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 19 08:38:54 UTC 2018,,,,,,,,,,,"0|i3qesn:",9223372036854775807,,,,,,,,,blerer,,blerer,,,Normal,,3.11.0,,,,,,,,,,,,,,,,,"22/Mar/18 10:54;fcofdezc;The problem is that the json serializer writes empty buffers as null instead of """".

[Patch|https://github.com/apache/cassandra/compare/cassandra-3.11...fcofdez:CASSANDRA-14245-3.11?expand=1];;;","28/Mar/18 12:58;blerer;The patch and the unit tests results look good. Thanks.;;;","29/Mar/18 06:57;blerer;Committed into 3.11 at 5cbe08b6a84cfa51ffd952a7c997b9a5f5e46e92 and merged into trunk.;;;","29/Mar/18 07:18;nob13;Thanks for fixing.

 ;;;","19/Sep/18 08:23;BrunoZ;Thanks but when you import such exported using CQL.json(String), you have : 

Error decoding JSON value for value: Value '' is not a valid blob representation: String representation of blob is missing 0x prefix: 
 com.datastax.driver.core.exceptions.InvalidQueryException: Error decoding JSON value for value: Value '' is not a valid blob representation: String representation of blob is missing 0x prefix: 
 at com.datastax.driver.core.Responses$Error.asException(Responses.java:148) ~C[cassandra-driver-core-3.2.0.jar:?]

Can you Reopen this ticket ?;;;","19/Sep/18 08:38;blerer;[~BrunoZ] it is better if you open another ticket as this patch as already been released in the version 3.11.3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlshrc.sample uses incorrect option for time formatting,CASSANDRA-14243,13139644,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,alexott,alexott,alexott,20/Feb/18 12:21,15/May/20 08:05,13/Jul/23 08:37,02/Mar/18 07:17,4.0,4.0-alpha1,,,,,Legacy/Tools,,,,0,lhf,,,"Sample cqlshrc file in conf/cqlshrc.sample uses incorrect option (datetimeformat) instead of correct one (time_format).

The datetimeformat is the sub-option of the COPY FROM command, not cqlsh itself.

 ",,alexott,githubbot,jeromatron,jjirsa,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,alexott,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 02 07:17:20 UTC 2018,,,,,,,,,,,"0|i3qdkn:",9223372036854775807,,,,,,,,,jjirsa,,jjirsa,,,Low,,,,,,,,,,,,,,,,,,,"20/Feb/18 12:24;githubbot;GitHub user alexott opened a pull request:

    https://github.com/apache/cassandra/pull/197

    fix for CASSANDRA-14243

    `datetimeformat` option that was used in the `conf/cqlshrc.sample` file is sub-option of
    the `COPY FROM` command, not of the `cqlsh` itself.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/alexott/cassandra CASSANDRA-14243

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/197.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #197
    
----
commit 53df3c118f4bb73494020b8414401540121a4c5d
Author: Alex Ott <alexott@...>
Date:   2018-02-20T12:23:21Z

    fix for CASSANDRA-14243
    
    `datetimeformat` option that was used in the `conf/cqlshrc.sample` file is sub-option of
    the `COPY FROM` command, not of the `cqlsh` itself.

----
;;;","02/Mar/18 07:17;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/cassandra/pull/197
;;;","02/Mar/18 07:17;jjirsa;Thanks for the patch [~alexott] - committed as {{73fa27cc6ae35a16ab8b828f9872ce59063aa83d}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Indexed static column returns inconsistent results,CASSANDRA-14242,13139561,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,adelapena,ross.black,ross.black,20/Feb/18 03:25,15/May/20 08:54,13/Jul/23 08:37,27/Apr/20 15:46,3.0.21,3.11.7,4.0,4.0-beta1,,,Feature/2i Index,,,,0,,,,"I am using Cassandra 3.11.2, and the Java driver 3.4.0

I have a table that has a static column, where the static column has a secondary index.
When querying the table I get incomplete or duplicated results, depending on the fetch size.

e.g.
{code:java}
CREATE KEYSPACE hack WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
CREATE TABLE hack.stuff (id int, kind text, chunk int static, val1 int, PRIMARY KEY (id, kind));
CREATE INDEX stuff_chunk_index ON hack.stuff (chunk);{code}

-- repeat with thousands of values for id =>
{code:java}
  INSERT INTO hack.stuff (id, chunk, kind, val1 ) VALUES (${id}, 777, 'A', 123);{code}


Querying from Java:
{code:java}
    final SimpleStatement statement = new SimpleStatement(""SELECT id, kind, val1 FROM hack.stuff WHERE chunk = "" + chunk); 
    statement.setFetchSize(fetchSize);
    statement.setConsistencyLevel(ConsistencyLevel.ALL);
    final ResultSet resultSet = connection.getSession().execute(statement);
    for (Row row : resultSet) {
        final int id = row.getInt(""id"");
    }{code}

*The number of results returned depends on the fetch-size.*

e.g. For 30k values inserted, I get the following:
||fetch-size||result-size||
|40000|30000|
|20000|30001|
|5000|30006|
|100|30303|



In production, I have a much larger table where the correct result size for a specific chunk is 20019, but some fetch sizes will return _significantly fewer_ results.
||fetch-size||result-size|| ||
|25000|20019| |
|5000|9999|*<== this one is has far fewer results*|
|5001|20026| |

(so far been unable to reproduce this with the simpler test table)

Thanks,
Ross","Cassandra 3.11.2

Java driver 3.4.0

Ubuntu - 4.4.0-112-generic",adelapena,blerer,jasonstack,jeromatron,ross.black,,,,,,,,,,,,,,,,,,,,,"adelapena opened a new pull request #549:
URL: https://github.com/apache/cassandra/pull/549


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Apr/20 12:26;githubbot;600","adelapena opened a new pull request #550:
URL: https://github.com/apache/cassandra/pull/550


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Apr/20 12:26;githubbot;600","adelapena opened a new pull request #551:
URL: https://github.com/apache/cassandra/pull/551


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Apr/20 12:27;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,adelapena,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,,Mon Apr 27 15:42:50 UTC 2020,,,,,,,,,,,"0|i3qd27:",9223372036854775807,,,,,,,,,blerer,,blerer,,,Normal,,3.0.0,,,https://ci-cassandra.apache.org/job/Cassandra-devbranch-test/61/,,,,,,,,,New unit tests,,,,,"08/Mar/18 18:56;adelapena;Thanks for reporting. I've reproduced the first case in 3.11 and trunk branches with [this unit test|https://github.com/apache/cassandra/compare/cassandra-3.11...adelapena:14242-3.11]:

{code:java}
@Test
public void pagingWithIndexOnStaticColumn() throws Throwable
{
    createTable(""CREATE TABLE %s (k int, c int, s int static, PRIMARY KEY (k, c));"");
    execute(""CREATE INDEX ON %s (s);"");

    int numRows = 100;
    for (int id = 0; id < numRows; id++)
    {
        execute(""INSERT INTO %s (k, c, s) VALUES (?, 1, 2);"", id);
    }

    String query = String.format(""SELECT * FROM %s.%s WHERE s=2"", KEYSPACE, currentTable());
    SimpleStatement stmt = new SimpleStatement(query);

    try (Session session = sessionNet())
    {
        assertEquals(numRows, session.execute(stmt.setFetchSize(1000)).all().size());
        assertEquals(numRows, session.execute(stmt.setFetchSize(100)).all().size());
        assertEquals(numRows, session.execute(stmt.setFetchSize(10)).all().size());
        assertEquals(numRows, session.execute(stmt.setFetchSize(1)).all().size());
    }
}
{code}
Still trying to reproduce the case returning fewer results.;;;","28/Mar/18 12:17;adelapena;This patch should solve the problem:
||[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...adelapena:14242-3.0]||[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...adelapena:14242-3.11]||[trunk|https://github.com/apache/cassandra/compare/trunk...adelapena:14242-trunk]||
It also fixes this infinite loop problem when querying 2i with a page size of one:
{code}
CREATE TABLE t(k int PRIMARY KEY, v int);
CREATE INDEX on t(v);
INSERT INTO t (k, v) VALUES (1, 10);
PAGING 1
SELECT * FROM t WHERE v = 10 AND k = 1; -- Infinite loop!
{code}
For 3.11 and trunk, where it's possible to create 2i on static columns since CASSANDRA-8103, the patch allows selecting only static columns, for example:
{code}
CREATE TABLE t(k int, c int, s int STATIC, PRIMARY KEY(k, c));
CREATE INDEX on t(s);

INSERT INTO t (k, s) VALUES (1, 100);
INSERT INTO t (k, c) VALUES (1, 10);
INSERT INTO t (k, c) VALUES (1, 20);

SELECT DISTINCT s FROM t WHERE s = 100 AND k = 1; -- Previously it would have thrown a ""Queries using 2ndary indexes don't support selecting only static columns"" error
{code}

[~blerer]/[~ifesdjeen] could you please review?;;;","21/Apr/20 15:20;adelapena;I think we should try to include this in 4.0.

I've rebased the patch, created PRs and run the new CIs:
||PR||Branch||CircleCI||
|[549|https://github.com/apache/cassandra/pull/549]|[14242-3.0|https://github.com/adelapena/cassandra/tree/14242-3.0]|[3.0|https://circleci.com/workflow-run/fbe54caa-31c7-4d97-b97d-34948431d830]|
|[550|https://github.com/apache/cassandra/pull/550]|[14242-3.11|https://github.com/adelapena/cassandra/tree/14242-3.11]|[3.11|https://circleci.com/workflow-run/0918ffcc-3ce2-4623-bfd4-94c6859ed05a]|
|[551|https://github.com/apache/cassandra/pull/551]|[14242-trunk|https://github.com/adelapena/cassandra/tree/14242-trunk]|[trunk|https://circleci.com/workflow-run/f7018c60-fe84-4cb0-b2b5-6944957ee182]|

[~blerer] do you think you'll have cycles to review?
;;;","21/Apr/20 15:57;blerer;I will find some time for it. :-)
;;;","24/Apr/20 12:36;blerer;The patch looks good to me.;;;","27/Apr/20 15:42;adelapena;Thanks for the review. Committed to 3.0 as [6cd2d07d9ae00fafa460fa1613264c43a283e24d|https://github.com/apache/cassandra/commit/6cd2d07d9ae00fafa460fa1613264c43a283e24d] and merged to [cassandra-3.11|https://github.com/apache/cassandra/commit/377ceb675c9c64afcfb3decbd4659f02b6f584a5] and [trunk|https://github.com/apache/cassandra/commit/78de6aca166e10524abf7f362efeb65751b78170].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Unittest: org.apache.cassandra.db.compaction.BlacklistingCompactionsTest,CASSANDRA-14238,13138972,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,marcuse,jay.zhuang,jay.zhuang,16/Feb/18 04:54,16/Jun/20 21:55,13/Jul/23 08:37,07/Jun/18 15:46,2.2.13,,,,,,Legacy/Testing,,,,0,testing,,,"The unittest is flaky
{noformat}
    [junit] Testcase: testBlacklistingWithSizeTieredCompactionStrategy(org.apache.cassandra.db.compaction.BlacklistingCompactionsTest): FAILED
    [junit] expected:<8> but was:<25>
    [junit] junit.framework.AssertionFailedError: expected:<8> but was:<25>
    [junit]     at org.apache.cassandra.db.compaction.BlacklistingCompactionsTest.testBlacklisting(BlacklistingCompactionsTest.java:170)
    [junit]     at org.apache.cassandra.db.compaction.BlacklistingCompactionsTest.testBlacklistingWithSizeTieredCompactionStrategy(BlacklistingCompactionsTest.java:71)
{noformat}",,djoshi,jay.zhuang,maedhroz,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15879,,CASSANDRA-14888,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 16 21:55:32 UTC 2020,,,,,,,,,,,"0|i3q9fb:",9223372036854775807,2.1.x,,,,,,,,jay.zhuang,,jay.zhuang,,,Low,,,,,,,,,,,,,,,,,,,"31/May/18 14:32;marcuse;this only fails on 2.2, and most often because the DeletionTime is initialized with a negative local deletion time due to the randomness when corrupting the files. Can't reproduce on 3.0+

patch https://github.com/krummas/cassandra/commits/marcuse/14238 which hard codes the random seed to something I've tested works (ran the test in a loop for 10+ minutes)
test run on circle: https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F14238;;;","07/Jun/18 01:45;jay.zhuang;+1

Thanks [~krummas] for the fix.

Seems the following positions will cause an assertion exception and fail the test:
{{2, 3, 114, 115}}.
[test code|https://github.com/cooldoger/cassandra/commit/32e01cf4d144625eeec207708d90242f195e21b9]. Command to test: 
{noformat}
$ for i in {0..180}; do echo == $i; ant test -Dtest.name=BlacklistingCompactionsTest -DTest.BadPos=$i >> /dev/null 2>&1 && echo ""okay"" || echo ""failed""; done
{noformat}

;;;","07/Jun/18 15:46;marcuse;committed as {{fc7a69b65399597a2bf9c6025f035f8fe26724c7}} to 2.2 and merged up with -s ours, thanks!;;;","16/Jun/20 18:07;djoshi;trunk has a similar failure: https://circleci.com/gh/dineshjoshi/cassandra/2516#tests/containers/36 [~marcuse] can you please confirm?;;;","16/Jun/20 18:10;maedhroz;[~marcuse] We've been able [to reproduce this|https://app.circleci.com/pipelines/github/dineshjoshi/cassandra/47/workflows/de5f7cdb-06b6-4869-9d19-81a145e79f3f/jobs/2516/tests] in a branch based on the current trunk while working on CASSANDRA-14888.

CC [~djoshi];;;","16/Jun/20 18:30;marcuse;[~maedhroz]/[~djoshi] could you open a new jira?;;;","16/Jun/20 21:55;maedhroz;Just created CASSANDRA-15879. Will put up the patch shortly...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReadCommandTest::testCountWithNoDeletedRow broken,CASSANDRA-14234,13138387,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,KurtG,KurtG,KurtG,14/Feb/18 01:04,15/May/20 08:00,13/Jul/23 08:37,14/Feb/18 13:27,3.11.2,4.0,4.0-alpha1,,,,,,,,0,,,,"{code}junit.framework.AssertionFailedError: expected:<1> but was:<5>
	at org.apache.cassandra.db.ReadCommandTest.testCountWithNoDeletedRow(ReadCommandTest.java:336){code}",,jasobrown,KurtG,mshuler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-8527,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,KurtG,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 14 13:27:32 UTC 2018,,,,,,,,,,,"0|i3q5tj:",9223372036854775807,3.11.x,,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"14/Feb/18 01:09;KurtG;[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:14234-3.11];;;","14/Feb/18 02:39;jasobrown;using {{git bisect}}, it looks like the commit that broke the unit test is {{9d649d69a56a91fcb06a3582b22606f0fe361f49}}, introduced in CASSANDRA-8527. ;;;","14/Feb/18 02:54;jasobrown;patch looks clean/compiles/test succeeds on 3.11, but trunk fails to apply cleanly. [~KurtG] can you take a look at that, as well?;;;","14/Feb/18 03:18;jasobrown;3.11 utest run [looks good|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14234-3.11].;;;","14/Feb/18 05:42;KurtG;[trunk|https://github.com/apache/cassandra/compare/trunk...kgreav:14234-trunk]
Sorry 'bout that. Not sure why I naively expected it to merge up cleanly.;;;","14/Feb/18 13:27;jasobrown;+1. committed as sha {{1d506f9d09c880ff2b2693e3e27fa58c02ecf398}}

Thanks for the quick work on this, [~KurtG]. I appreciate it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool tablestats/cfstats output has inconsistent formatting for latency,CASSANDRA-14233,13138357,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,sproberts92,sproberts92,sproberts92,13/Feb/18 22:47,15/May/20 08:00,13/Jul/23 08:37,14/Feb/18 13:04,3.11.2,4.0,4.0-alpha1,,,,,,,,0,,,,"Latencies are reported at keyspace level with `ms.` and at table level with `ms`.

There should be no trailing `.` as it is not a sentence and `.` is not part of the abbreviation.

This is also present in 2.x with `nodetool cfstats`.",,cnlwsu,githubbot,jasobrown,sproberts92,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,sproberts92,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 14 23:36:22 UTC 2018,,,,,,,,,,,"0|i3q5mv:",9223372036854775807,3.11.1,,,,,,,,cnlwsu,,cnlwsu,,,Low,,2.0.0,,,,,,,,,,,,,,,,,"13/Feb/18 22:57;sproberts92;Very simple to fix:

-[https://github.com/sproberts92/cassandra/commit/0eeee5ccb4851bf2ab1f35237ec901e1426a9bda]-

but I am confused about which branch to commit to - docs state start with the oldest branch? As the file has been moved around a lot and names changed, I'm not sure if this will propagate to 3.x cleanly.;;;","14/Feb/18 00:08;cnlwsu;Can you just make it for trunk? 2.0 isnt taking changes anymore and 2.1 is pretty much reserved to data loss bugs.;;;","14/Feb/18 06:54;githubbot;GitHub user sproberts92 opened a pull request:

    https://github.com/apache/cassandra/pull/195

    Resolve nodetool formatting in CASSANDRA-14233

    Remove trailing ""."" from latency reports at keyspace level.
    
    For [CASSANDRA-14233](https://issues.apache.org/jira/browse/CASSANDRA-14233).

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/sproberts92/cassandra CASSANDRA-14233

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/195.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #195
    
----
commit ae6e74ac0f31161be10481f2f4c76cebfe45d7b0
Author: Samuel Roberts <sproberts92@...>
Date:   2018-02-14T06:44:50Z

    Resolve nodetool formatting in CASSANDRA-14233
    
    Remove trailing ""."" from latency reports at keyspace level.

----
;;;","14/Feb/18 08:04;sproberts92;Ok, I've done it against trunk.

Also, Gary Stewart from NL wants the world to know that he was the one who noticed the inconstancy, so let that be recorded here for posterity.;;;","14/Feb/18 13:04;jasobrown;+1. committed as sha {{d10e6ac606c6b484c75bb850de7a754b75ad5eca}}.

Thanks for the patch!;;;","06/Mar/18 09:44;githubbot;Github user spodkowinski commented on the issue:

    https://github.com/apache/cassandra/pull/195
  
    Please close the PR as the patch already has been merged in d10e6ac606c6b484c75bb850de7a754b75ad5eca
;;;","14/Mar/18 23:36;githubbot;Github user sproberts92 closed the pull request at:

    https://github.com/apache/cassandra/pull/195
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ViewComplexTest broken in trunk,CASSANDRA-14230,13138050,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,KurtG,KurtG,KurtG,13/Feb/18 03:08,16/Apr/19 09:29,13/Jul/23 08:37,14/Feb/18 00:00,,,,,,,,,,,0,,,,"testUpdateWithColumnTimestampBiggerThanPkWithFlush
testUpdateWithColumnTimestampBiggerThanPkWithoutFlush
testUpdateColumnNotInViewWithFlush
testUpdateColumnNotInViewWithoutFlush

All fail with:
{code}
    [junit] junit.framework.AssertionFailedError: Expected error message to contain 'Cannot drop column v2 on base table with materialized views', but got 'Cannot drop column v2 on base table table_20 with materialized views.'
{code}
Because the error message changed.",,jasobrown,KurtG,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14219,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,KurtG,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 14 00:00:52 UTC 2018,,,,,,,,,,,"0|i3q3qn:",9223372036854775807,,,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"13/Feb/18 03:51;KurtG;|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:14230-3.11]|[trunk|https://github.com/apache/cassandra/compare/trunk...kgreav:14230-trunk]|;;;","14/Feb/18 00:00;jasobrown;Thanks for the patch, [~KurtG], but I think we beat you to it ;);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extend maximum expiration date,CASSANDRA-14227,13137703,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,bereng,pauloricardomg,pauloricardomg,11/Feb/18 13:44,13/Jun/23 12:35,13/Jul/23 08:37,05/Jun/23 05:23,5.0,,,,,,Legacy/Local Write-Read Paths,,,,7,,,,"The maximum expiration timestamp that can be represented by the storage engine is
2038-01-19T03:14:06+00:00 due to the encoding of {{localExpirationTime}} as an int32.

On CASSANDRA-14092 we added an overflow policy which rejects requests with expiration above the maximum date as a temporary measure, but we should remove this limitation by updating the storage engine to support at least the maximum allowed TTL of 20 years.",,adelapena,bcicchi,benedict,bereng,blerer,e.dimitrova,eanujwa,eperott,erickramirezau,jasonstack,jeromatron,KurtG,laxmikant99,manmagic3,mshuler,pauloricardomg,shaurya10000,sivann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14092,,,,,,,,,,CASSANDRA-18593,,,CASSANDRA-14178,CASSANDRA-18301,,,,,,CASSANDRA-18564,,,"22/Mar/23 07:52;bereng;C14227 Perf check 2023.03.21.pdf;https://issues.apache.org/jira/secure/attachment/13056567/C14227+Perf+check+2023.03.21.pdf","26/May/23 09:51;bereng;C14227 Perf check 2023.05.26.pdf;https://issues.apache.org/jira/secure/attachment/13058571/C14227+Perf+check+2023.05.26.pdf","18/Oct/22 08:44;bereng;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13051065/screenshot-1.png","18/Oct/22 08:44;bereng;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13051066/screenshot-2.png","26/Jan/23 05:40;bereng;screenshot-3.png;https://issues.apache.org/jira/secure/attachment/13054811/screenshot-3.png","26/Jan/23 05:43;bereng;screenshot-4.png;https://issues.apache.org/jira/secure/attachment/13054812/screenshot-4.png","03/Feb/23 07:37;bereng;unnamed-1.png;https://issues.apache.org/jira/secure/attachment/13055091/unnamed-1.png",,,,,,,7.0,bereng,,,,,,,,,,,,,,,,,,,,Challenging,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 05 05:27:18 UTC 2023,,,,,,,,,,,"0|i3q1lj:",9223372036854775807,,,,,,,,,,,adelapena,,,Critical,,5.0,,,https://github.com/apache/cassandra/commit/1adbea5a068287f42f2421e558f4c404c69aea74 https://github.com/apache/cassandra-dtest/commit/58820de92eef140991a3e45f68f9152ae2fbc490,,,,,,,,,See PR,,,,,"11/Feb/18 13:46;pauloricardomg;[~VincentWhite] feel free to add your PoC patch from CASSANDRA-14092. :);;;","05/Apr/18 20:30;pauloricardomg;It would be great to get this in for 4.0. The simplest approach would be to change the {{localDeletionTime}} representation to {{long}} as proposed by [~VincentWhite], but we need to verify the impact of this on heap for TTL and non-TTL workloads.

Alternatives include using a unsigned int32 to represent {{localDeletionTime}} and/or using a later start epoch (maybe 2000 or so) - but this would require some considerate work to maintain backward compatibility and would difficult interoperability with other systems - even though it's an internal structure.

[~VincentWhite] would you be willing to work on this and maybe perform some stress tests to check impact on heap and GC of changing the {{localDeletionTime}} representation from {{int}} to {{long}} ?;;;","04/Jun/19 05:49;laxmikant99;[~pauloricardomg] Upgrade Sstable change to resume the original ttl should be in scope of this bug or should we open another jira ticket for this?
Note: This change is important for those users who are currently using CAP or CAP_NOWARN to avoid request rejection but want to resume to originally  set TTL when they upgrade to new cassandra version with fix.;;;","18/Jul/19 10:33;laxmikant99;Resuming the {{localDeletionTime}}  value as discussed [here|https://jira.apache.org/jira/browse/CASSANDRA-14092?focusedCommentId=16341749&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16341749]  after an update operation on a row with TTL does not look feasible. Please see the below test case:

*Step 1:*

insert into tab3(id,key,value) VALUES('id2', 'key1', 2) USING TTL 630720000; (20 years).
{code:java}
sstabletojson: 
 {
 ""partition"" : {
 ""key"" : [ ""id2"" ],
 ""position"" : 33
 },
 ""rows"" : [
 {
 ""type"" : ""row"",
 ""position"" : 75,
 ""clustering"" : [ ""key1"" ],
 ""liveness_info"" : { ""tstamp"" : ""2019-07-18T07:03:22.198Z"", ""ttl"" : 630720000, ""expires_at"" : ""2038-01-19T03:14:06Z"", ""expired"" : false },
 ""cells"" : [
 { ""name"" : ""value"", ""value"" : 2 }
 ]
 }
 ]
 }{code}
*Step 2:* 
 1. select ttl(value) from tab3 where id='id2';

*This results 584043668* *and not 630720000 !* and then updating the row with fetched ttl loses the original ttl value (20 years) so resuming {{localDeletionTime}}  based on ttl the value will not be feasible if someone chooses CAP or CAP_NOWARN option as in sstable, TTL value is no more 20 years (630720000).

2. insert into tab3(id,key,value) VALUES('id2', 'key1', 3) USING TTL 584043668;
{code:java}
sstabletojson: 
 
 {
 ""partition"" : {
 ""key"" : [ ""id2"" ],
 ""position"" : 33
 },
 ""rows"" : [
 {
 ""type"" : ""row"",
 ""position"" : 75,
 ""clustering"" : [ ""key1"" ],
 ""liveness_info"" : { ""tstamp"" : ""2019-07-18T08:54:27.921Z"", ""ttl"" : 584043668, ""expires_at"" : ""2038-01-19T03:14:06Z"", ""expired"" : false },
 ""cells"" : [
 { ""name"" : ""value"", ""value"" : 3 }
 ]
 }
 ]
 }{code}
 ;;;","23/Jul/19 13:23;pauloricardomg;{quote}This results 584043668 and not 630720000 ! and then updating the row with fetched ttl loses the original ttl value (20 years) so resuming localDeletionTime based on ttl the value will not be feasible if someone chooses CAP or CAP_NOWARN option as in sstable, TTL value is no more 20 years (630720000).
{quote}
The fact that {{SELECT TTL(*)}} does not return the original TTL does not mean we cannot resume the original TTL once this is fixed. Data will need to be rewritten via ""upgradesstables"" to restore the original TTL once this limitation is fixed. If you are updating the row with fetched TTL you are overwriting the original value, so it's expected behavior that you lose the original TTL.

If you are willing to take a stab at fixing this, I think the simplest approach is to change the {{localDeletionTime}} field from integer to long.;;;","25/Jul/19 05:23;laxmikant99;[~pauloricardomg] I believe there are many use-cases where users maintain the TTL of a row while updating it. Hence in all such cases original TTL will be lost. So, is it worth giving TTL recalculation feature in upgradesstable which won't be applicable to all users ? 

I agree that a permanent fix, changing {{localDeletionTime}} field from integer to long is simplest way to go but this change will be big in terms changes in number of classes and we need to carefully analyse the impact as well.;;;","12/Jun/20 15:08;mshuler;Set this Jira to Severity = Critical and Complexity = Challenging to reflect the state of this problem, as time goes on, as well as the nature of the fix.;;;","29/Nov/21 10:18;sivann;This is a critical issue, that severely limits TTL functionality; the CAP_NOWARN workaround of CASSANDRA-14092 is already limiting our data lifecycle.  I'm not sure why this is high-severity issue was not planned for 4.0, what's missing?;;;","11/Apr/22 18:55;mshuler;Removed assignee. I only found a few random state changes by the user in Jira, so this assignment also looked random. If this was incorrect, please reassign!;;;","29/Sep/22 07:26;bereng;Hi,

moving to this to review. Some comments:
- Max TTL is now 68y and there is a new default NONE overflow policy, the other ones are kept for backwards compatibility.
- All tests pass but for one upgrade rolling failing 10% of times. I am investigating it but given it is taking a lot of time and I can't repro locally I prefer to start with the review already.
- The code has a few 'TODO 14227' comments which are points I would like to discuss with the reviewer such as {{StreamingTombstoneHistogramBuilder#mergeNearestPoints()}}
- Most files only change int to longs, think about sentinel values while reviewing and possible side-effects
- The PR has a few self-contained commits for an easier review.

;;;","18/Oct/22 08:51;bereng;Hi,

I have been asked to run some profiling to see the impact 'longs' would have. I have tested on a single node doing inserts with a TTL + selects against that for 4m at around 100Kops/s after a warm up run. The results are virtual identical to both trunk and 14227 both at jfr and stress tool perf reporting

Trunk:

 !screenshot-1.png! 

14227

 !screenshot-2.png! 

If anything 14227's number are slightly better but probably just test env noise. GC pauses, total times, latencies, etc are all identical as well. The test CQL was:

{noformat}
CQL|INSERT INTO test.test (id, type, text) VALUES ($RANDOM_20000, $RANDOM_10, 'TTL Profiling') USING TTL $RANDOM_500000
CQL|SELECT id, type, text FROM test.test WHERE id=$RANDOM_20000
{noformat}

where RANDOM_X means a random number up to X. Here we can see inserts, inserts colliding and selects. I will be happy to repeat the test if anybody has any suggestions. I am not posting the jfr for security reasons as the env vars section contains sensitive data (call me paranoid) but I can share them with known people on request. 

EDIT: I have been asked to confirm both runs are under {{memtable_allocation_type: heap_buffers}} so it is on heap.;;;","21/Nov/22 07:54;bereng;After some further perf testing we've found out there is a 3% disk size increase hit we've like to avoid as discussed in the [ML|https://lists.apache.org/thread/fyf2d9jlrsor3hmz46qn282o5goooowd]. We're going to experiment and POC a bit a uint encoding to get rid of that and any extra flushes as well.;;;","12/Jan/23 10:04;bereng;Update: The uint approach is giving good results in the POC on disk size, memtable, etc. I need to rebase, put capping back and revisit all perf.;;;","26/Jan/23 05:45;bereng;Uint POC seems to be OK as well on the latency side. There are latencies on 2 completely different tests with even 2 diff test tooling:

10 averaged runs latency
[^screenshot-3.png]

!screenshot-3.png!

Averaged latency on a 1h test
[^screenshot-4.png]

!screenshot-4.png!;;;","03/Feb/23 07:45;bereng;Putting this back for a first pass review with the new Uint approach. I have done a final round of perf testing:
 * Perf testing revolved around 10M rows and runs with random/fixed TTL for periods of 2m, 5m and 10m and averaging 10 runs. The exception being the much longer latency test.
 * Longer tests tend to give less noisy results. 14227 vs trunk may randomly appear one slightly better than the other which I attribute to env noise and lack of dedicated perf testing HW.
 * Latency seems to be aligned as per comment above
 * JFRs available on request
 * Disk size, flushes etc seem to be aligned. Here is some table stats where sometimes 14227 is slightly better and sometimes the other way around  !unnamed-1.png!

The PR has several commits to make it easier to review. Some refactoring of an earlier commit might be found in a later one but nothing too big and quite straightforward. I have left a bunch of 14227 TODO comments to bring the attention of the reviewer just for feedback. CI can be found in the PR, yet repeat tests haven't been run yet due to the size. We can do it at a later time once we're happy with the approach

The only thing to note is the addition of the NONE policy where it's later removed. That is the only bit it can't be easily collapsed, apologies in advance.;;;","14/Feb/23 11:23;benedict;[~jlewandowski] helpfully pointed out we should consider downgrade for this patch. It looks like we have an option to CAP to 2038, and if we default to this on upgrade I think a downgrade should be smooth. I'm not certain we even need to change the sstable version? Though I haven't looked closely.;;;","15/Feb/23 06:51;bereng;Seems like downgradability, at the time of writing, is still being discussed iiuc.

Now we're also writing an int, with the uint approach, and the sstable version change is to signal a negative int being 'undefined data' vs 'a valid uint'. Regardless of the downgradability discussion _I think_ (not tested) an sstable scrub would suffice as scrub will either REJECT or CAP those entries. That is pending the downgradibility discussion detailed requirements: forward compatibility?, flag?, scrub?

Sthg to take into account though, thx for bringing it up.;;;","15/Feb/23 08:28;benedict;Downgradeability isn't under continuing discussion at present, no, but you are welcome to raise it for discussion.

Either way, if it makes it simpler: this patch can easily avoid breaking downgrade, so my binding review feedback is that it *must* not.;;;","20/Feb/23 15:00;blerer;{quote} It looks like we have an option to CAP to 2038, and if we default to this on upgrade I think a downgrade should be smooth.{quote}
[~benedict] Could you elaborate a bit what you mean? I am not sure that I am understanding what you have in mind. ;;;","20/Feb/23 15:20;benedict;By default we should reject TTLs past 2038, and while this setting is in place we should continue to write \-nc\- format sstables. Once the operator configures longer TTLs, we can write \-oa\- format sstables.;;;","21/Feb/23 13:27;blerer;I think, that I am misunderstanding what we call downgradability. What you propose is some kind of feature flag that delay the problem, no? If a problem occurs once we switch to the new sstable format then we cannot downgrade anymore.;;;","21/Feb/23 13:42;benedict;This is the canonical example that was given in the original thread more than a year ago, i.e. that a cluster should not write files that are incompatible with the version we are upgrading from until the operator agrees the upgrade has been successful, yes. If there are performance or other regressions encountered during an upgrade, recovery should be as simple as restarting with the prior version (until the operator decides the new version is performing adequately). This minimises risks, without fully eliminating them. 

;;;","23/Feb/23 19:28;benedict;I decided to have a quick look at adding support for dynamically determining the output format based on whether the TTL data requires it, and I noticed that EncodingStats and EncodingStats.Collector likely treat TTL incorrectly, using simple min/max on int. This might not cause any bugs, but it might, and we should fix it - we should probably consider what additional testing we might need to catch this kind of error elsewhere.;;;","22/Mar/23 07:53;bereng;Perf check report post review and big rebase  [^C14227 Perf check 2023.03.21.pdf] LGTM;;;","10/May/23 05:00;bereng;[~benedict] a feature flag with upgrade tests etc has been added. I have provided links to the key bits of code [here|https://github.com/apache/cassandra/pull/1891#issuecomment-1541352411]. If we could +1 I would start getting ready for the merge :-);;;","17/May/23 16:56;adelapena;This looks good to me. I have left some final minor nits on the ticket.

If [~benedict] agrees with the approach for compatibility we can probably rebase and solve [those naming details|https://github.com/apache/cassandra/pull/1891/files#r1188414272] on the new CircleCI jobs.;;;","18/May/23 05:14;bereng;^The diff to the commit a few days ago is that the property has been moved to yaml as suggested by [~benedict];;;","22/May/23 05:00;bereng;Hi [~benedict] we're a bit blocked here, if you could find a gap it should be a quick review for the FF having been moved to yaml and it would be highly appreciated. Thx.;;;","22/May/23 09:02;benedict;Sorry, the downside of lots of Jira traffic (incl from GitHub comments) is that I don't check the email notifications for a high traffic ticket.

I won't have time to look at the code soon, but I trust you to have addressed my concerns given what you describe above. Feel free to proceed.;;;","22/May/23 09:51;bereng;Thx, it should be pretty non-controversial and we can always tweak the name etc.;;;","26/May/23 09:53;bereng;Latest perf check after rebases etc LGTM [^C14227 Perf check 2023.05.26.pdf]

The merge is coming...;;;","30/May/23 14:37;adelapena;Looks good to me after rebase, and CI also looks good. +1 from me. The few remaining, trivial suggestions about comments can be addressed on commit.;;;","30/May/23 14:42;bereng;[~benedict] Everything looks good as far as we can see. Do you think you could find a gap see if we can +1 and merge :-) You're probably most interested in the yaml feature flag commit [here|https://github.com/apache/cassandra/pull/1891/commits/159eabbc7a0dec932864447ecffbaae5ffe66c4c];;;","05/Jun/23 05:27;bereng;Merged. Thx everyone for all the help!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix comparison of address and port for repair and messages,CASSANDRA-14225,13137315,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius,dbrosius,09/Feb/18 01:14,15/May/20 08:05,13/Jul/23 08:37,13/Feb/18 23:24,4.0,4.0-alpha1,,,,,Legacy/Core,,,,0,,,,"compare both host and port, not just host",,aweisberg,dbrosius,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/18 01:16;dbrosius;14225.txt;https://issues.apache.org/jira/secure/attachment/12909861/14225.txt",,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,,,Tue Feb 13 23:24:13 UTC 2018,,,,,,,,,,,"0|i3pz7b:",9223372036854775807,,,,,,,,,aweisberg,,aweisberg,,,Low,,,,,,,,,,,,,,,,,,,"09/Feb/18 01:17;dbrosius;mind reviewing?;;;","09/Feb/18 17:30;aweisberg;Did you want me as the reviewer or assignee?

{{FBUtilities.getBroadcastNativeAddressAndPort()}} is for the port used by the CQL protocol. Maybe we should javadoc that. You want {{FBUtilities.getBroadcastAddressAndPort()}} for the storage port. There is a missing newline between the new import and the start of the class comments.

In RepairRunnable the diff introduces a tab when constructing the InetAddressAndPort.

Otherwise +1.;;;","10/Feb/18 04:34;aweisberg;I also realized you probably can't assume that source_port is in the row. You will need to check if it is there and supply a default value if it isn't. I think it's the storage port you use in this case. I need to check.;;;","13/Feb/18 17:33;aweisberg;Testing in CircleCI https://circleci.com/gh/aweisberg/cassandra/tree/cassandra-14225
https://github.com/apache/cassandra/compare/trunk...aweisberg:cassandra-14225?expand=1;;;","13/Feb/18 23:24;aweisberg;+1 Committed as [834f2a6ecdb8974839762bf4e9c5fed32163f9c8|https://github.com/apache/cassandra/commit/834f2a6ecdb8974839762bf4e9c5fed32163f9c8] thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change to AlterTableStatement logging breaks MView tests,CASSANDRA-14219,13136837,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,djoshi,jasobrown,jasobrown,07/Feb/18 12:41,15/May/20 08:02,13/Jul/23 08:37,13/Feb/18 23:42,3.0.16,3.11.2,4.0,4.0-alpha1,,,,,,,0,,,,"looks like [~dbrosius]'s ninja commit {{7df36056b12a13b60097b7a9a4f8155a1d02ff62}} to improve the logging of {{AlterTableStatement}} breaks some MView tests that check the exception message. I see about six failed tests from {{ViewComplexTest}} that have messages similar to this:
{noformat}
junit.framework.AssertionFailedError: Expected error message to contain 'Cannot drop column m on base table with materialized views', but got 'Cannot drop column m on base table table_6 with materialized views.'{noformat}",,djoshi,jasobrown,KurtG,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14230,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,djoshi,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 14 00:07:58 UTC 2018,,,,,,,,,,,"0|i3pw93:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"09/Feb/18 23:59;djoshi;||14219||
|[branch|https://github.com/dineshjoshi/cassandra/tree/CASSANDRA-14219]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/CASSANDRA-14219]|
||;;;","13/Feb/18 23:26;KurtG;Missed this ticket and accidentally created a duplicate and did the same patch yesterday CASSANDRA-14230.... It's also broken on trunk FTR.

 

In regards to patch, you can just use currentTable() to get the CF name.;;;","13/Feb/18 23:42;jasobrown;+1.Thanks, [~djoshi3]. Committed as sha {{890f319142ddd3cf2692ff45ff28e71001365e96}}

For posterity here's my ports to 3.0/3.11/trunk and the test runs; {{ViewComplexTest}} no longer fails.

||3.0||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/3.0.16-candidate]|[branch|https://github.com/jasobrown/cassandra/tree/3.11.2-candidate]|[branch|https://github.com/jasobrown/cassandra/tree/14291-trunk]|
|[utests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/3.0.16-candidate]|[utests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/3.11.2-candidate]|[utests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14291-trunk]|
||
;;;","14/Feb/18 00:07;djoshi;[~KurtG] I originally used \{{currentTable()}} but then followed the pattern you had in other commits.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
node map does not handle InetAddressAndPort correctly.,CASSANDRA-14216,13136464,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius,dbrosius,06/Feb/18 04:04,15/May/20 08:03,13/Jul/23 08:37,07/Feb/18 16:49,4.0,4.0-alpha1,,,,,Legacy/Core,,,,0,,,,"Collection of node information in nodeMap does not use the correct types for accessing data. Since these maps are keyed by Strings, they are not metatype-safe, and so i can't be certain what data was meant to be in them. I'm assuming it was meant that host and port information should be used, but perhaps it's just host.

 

I have created a patch assuming it's host and port info.",,aweisberg,dbrosius,jasobrown,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/18 04:05;dbrosius;14216.txt;https://issues.apache.org/jira/secure/attachment/12909360/14216.txt",,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,,,Wed Feb 07 16:49:21 UTC 2018,,,,,,,,,,,"0|i3ptyn:",9223372036854775807,,,,,,,,,aweisberg,,aweisberg,,,Low,,,,,,,,,,,,,,,,,,,"06/Feb/18 11:45;jasobrown;/cc [~aweisberg];;;","07/Feb/18 16:03;aweisberg;You are correct it's supposed to be the host and port string and we also do need to unwrap the address from the InetAddressAndPort.
+1

Thanks for spotting this. Are you finding these things with manual testing, or a linter or some other static analysis tool?;;;","07/Feb/18 16:49;dbrosius;pushed to trunk as d0b34d383c20f5add8b8d7d454b4460aace0c939

 

findbugs/fb-contrib;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra does not respect hint window for CAS,CASSANDRA-14215,13136453,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,KurtG,arijit91,arijit91,06/Feb/18 02:38,15/May/20 08:05,13/Jul/23 08:37,15/Mar/18 14:27,3.0.17,3.11.3,4.0,4.0-alpha1,,,Consistency/Hints,Feature/Lightweight Transactions,Legacy/Streaming and Messaging,,0,LWT,,,"On Cassandra 3.0.9, it was observed that Cassandra continues to write hints even though a node remains down (and does not come up) for longer than the default 3 hour window.

 

After doing ""nodetool setlogginglevel org.apache.cassandra TRACE"", we see the following log line in cassandra (debug) logs:
 StorageProxy.java:2625 - Adding hints for [/10.0.100.84]

 

One possible code path seems to be:

cas -> commitPaxos(proposal, consistencyForCommit, true); -> submitHint (in StorageProxy.java)

 

The ""true"" parameter above explicitly states that a hint should be recorded and ignores the time window calculation performed by the shouldHint method invoked in other code paths. Is there a reason for this behavior?

 

Edit: There are actually two stacks that seem to be producing hints, the ""cas"" and ""syncWriteBatchedMutations"" methods. I have posted them below.

 

A third issue seems to be that Cassandra seems to reset the timer which counts how long a node has been down after a restart. Thus if Cassandra is restarted on a good node, it continues to accumulate hints for a down node over the next three hours.

 

{code:java}
WARN [SharedPool-Worker-14] 2018-02-06 22:15:51,136 StorageProxy.java:2636 - Adding hints for [/10.0.100.84] with stack trace: java.lang.Throwable: at org.apache.cassandra.service.StorageProxy.stackTrace(StorageProxy.java:2608) at org.apache.cassandra.service.StorageProxy.submitHint(StorageProxy.java:2617) at org.apache.cassandra.service.StorageProxy.submitHint(StorageProxy.java:2603) at org.apache.cassandra.service.StorageProxy.commitPaxos(StorageProxy.java:540) at org.apache.cassandra.service.StorageProxy.cas(StorageProxy.java:282) at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithCondition(ModificationStatement.java:432) at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:407) at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:206) at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:237) at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:222) at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:115) at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:513) at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:407) at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) at java.lang.Thread.run(Thread.java:748) WARN
{code}

{code:java}
[SharedPool-Worker-8] 2018-02-06 22:15:51,153 StorageProxy.java:2636 - Adding hints for [/10.0.100.84] with stack trace: java.lang.Throwable: at org.apache.cassandra.service.StorageProxy.stackTrace(StorageProxy.java:2608) at org.apache.cassandra.service.StorageProxy.submitHint(StorageProxy.java:2617) at org.apache.cassandra.service.StorageProxy.sendToHintedEndpoints(StorageProxy.java:1247) at org.apache.cassandra.service.StorageProxy.syncWriteBatchedMutations(StorageProxy.java:1014) at org.apache.cassandra.service.StorageProxy.mutateAtomically(StorageProxy.java:899) at org.apache.cassandra.service.StorageProxy.mutateWithTriggers(StorageProxy.java:834) at org.apache.cassandra.cql3.statements.BatchStatement.executeWithoutConditions(BatchStatement.java:365) at org.apache.cassandra.cql3.statements.BatchStatement.execute(BatchStatement.java:343) at org.apache.cassandra.cql3.statements.BatchStatement.execute(BatchStatement.java:329) at org.apache.cassandra.cql3.statements.BatchStatement.execute(BatchStatement.java:324) at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:206) at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:237) at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:222) at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:115) at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:513) at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:407) at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) at java.lang.Thread.run(Thread.java:748)
{code}

 

 ",,aleksey,arijit91,jeromatron,jjirsa,jjordan,KurtG,rha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,KurtG,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 15 14:26:19 UTC 2018,,,,,,,,,,,"0|i3ptw7:",9223372036854775807,3.0.16,3.11.2,,,,,,,aleksey,,aleksey,,,Normal,,3.0.0,,,,,,,,,,,,,,,,,"06/Feb/18 23:19;jjirsa;Fairly sure this is working as intended; we should write hints beyond the hint window, we just won't replay hints beyond the hint window (so if a host is down for 4 hours, and hint lifetime is 3 hours, we'll lose the first hour of hints, but still replay the last 3).

Check out {{Hint.isLive()}} for more info, and let me know if we can close this if you agree.



;;;","06/Feb/18 23:21;arijit91;[~jjirsa] Thanks for the response. What's the reason for writing hints beyond the hint window if there's no chance that they would be replayed? On our production cluster, this causes downtime as the Cassandra partition fills up quickly with hints when a node goes down.;;;","06/Feb/18 23:29;arijit91;[~jjirsa] Is Hint.isLive a bit orthogonal to this, it seems to be dealing with gc grace period?

Also hints *are not written* in some code paths once the node is observed to be down for more than the window. Also the documentation around hints explicitly says that hints will not *be generated* when a node is down or longer than the window.

Finally, check out the bit around the node down timer being reset on Cassandra restarts. This does not matter if we are ok with generating hints beyond the max hint window, but I really don't think that we should be...

 ;;;","06/Feb/18 23:30;jjirsa;We don't know when the node is going to come back up, and we don't know what you as a user expect to happen.

We guarantee we won't replay a hint past the time you specify, but as I explained, if your limit is N hours, and you're down for N + 1 hour, we can still replay N hours of data via hints, making you repair only 1 hour instead of N. For many users, this is preferable to replaying nothing.

There is a {{deleteAllHintsForEndpoint}} JMX target that would let you purge hints (manually), but it does perhaps seem like a missing feature that we don't more aggressively clean up hints that are expired. [~iamaleksey] any thoughts here?

Edit: reading your last comment, I may be wrong (it happens a lot, so maybe I shouldn't be surprised). Aleksey will know for sure. 
;;;","07/Feb/18 23:44;KurtG;My understanding was that we'd keep hints for the first N hours, and then we'd stop storing them. We'd always replay whatever hints are stored and delete them after replaying. 

Code in {{org.apache.cassandra.service.StorageProxy#shouldHint}} seems to imply the above
{code:java}
boolean hintWindowExpired = Gossiper.instance.getEndpointDowntime(ep) > DatabaseDescriptor.getMaxHintWindow();
if (hintWindowExpired)
{
    HintsService.instance.metrics.incrPastWindow(ep);
    Tracing.trace(""Not hinting {} which has been down {} ms"", ep, Gossiper.instance.getEndpointDowntime(ep));
}
return !hintWindowExpired;
{code}
This sounds like an issue to me, pretty sure we shouldn't be storing anything that's past the hint window.

bq. Finally, check out the bit around the node down timer being reset on Cassandra restarts. This does not matter if we are ok with generating hints beyond the max hint window, but I really don't think that we should be...
 This is also not really intended. As checks are currently only performed on how long the node has been down if it comes up after 3 hours then goes back down straight away we'll effectively store 6 hours of hints for the node. It's probably reasonable to only store a maximum of {{max_hint_window_in_ms}}. We might be able to get away with just looking at the timestamp of the earliest hint for the node and using that if it's prior to the current downtime.

I'll have a look at these in the next few days.
{quote}There is a deleteAllHintsForEndpoint JMX target that would let you purge hints (manually), but it does perhaps seem like a missing feature that we don't more aggressively clean up hints that are expired.
{quote}
[~VincentWhite] was looking at this recently while trying to solve a problem where hint replaying would get stuck and continuously replay a single hint if it happened to fail on the receiving side continuously (usually due to receiving node being overloaded and never acknowledging the hint in time). I recall that he also had problems because we have no service to clean up hints in this case. He's on leave this week but would probably have some input around this.;;;","08/Feb/18 00:20;arijit91;Thanks [~KurtG]. Yes that's the main issue, we're storing hints beyond the hint window.

About the Cassandra restarts, I meant restarts on the *live node* and not on the *dead node* also end up resetting the timer. Though the point you mentioned is valid as well.;;;","08/Feb/18 01:13;KurtG;Huh yeah, that makes sense. I missed that part but definitely also an issue. I've already started working on the patch so I'll take the ticket.;;;","08/Feb/18 12:24;aleksey;It could be the case that LWT write path is not respecting hint window. If it is, I'm pretty sure it's *not* intended.

Everything else here I *think* works as intended - behaved like this since the very beginning of hints. Whether or not that intention was ever optimal is a different matter, and it's not immediately obvious to me it it was or wasn't.;;;","24/Feb/18 20:44;arijit91;Just to confirm [~KurtG], will the fix that you are working on also make it to the 3.0 branch?;;;","26/Feb/18 00:58;KurtG;[~arijit91] Will have 2 separate patches up at some point today. One for CAS problem which will be for >=3.0, and another for the hint window problem which will be just targeted at trunk. Don't really want to change hint window behaviour in any minors.

As far as I can tell the batch statements are adhering to {{StorageProxy::shouldHint()}} so no fix necessary there.;;;","08/Mar/18 09:04;KurtG;So I know I said today... but then I realised there were some issues with the tests I'd written and instead got distracted by another ticket.

I've got two patches, one for the CAS problem (which should go to 3.0, 3.11, and trunk), and a patch for the non-persistent hint window problem which is aimed only at trunk. 

CAS patches:
|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...kgreav:14215-3.0]|
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:14215-3.11]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...kgreav:14215-trunk-1]|

Hint window patch:
|[trunk|https://github.com/apache/cassandra/compare/trunk...kgreav:14215-trunk-2]|

There's a dtest for each patch, at the moment both are in a single branch.
|[dtest|https://github.com/apache/cassandra-dtest/compare/master...kgreav:14215]|
;;;","09/Mar/18 13:19;aleksey;The CAS patch looks good to me, and I agree it's fine for all of 3.0, 3.11, and trunk.

I'm not yet sure about the change to the semantic of hint window, however. And either way, we should be committing both in the same ticket.

Can you please split it out into a separate JIRA? In that case I'll +1 the CAS patch, and we can keep discussion about the window change and review it separately.

Cheers.;;;","13/Mar/18 03:33;KurtG;Thanks [~iamaleksey].

Created CASSANDRA-14309 for hint window patch.

Branches are the same for C*

|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...kgreav:14215-3.0]|
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:14215-3.11]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...kgreav:14215-trunk-1]| 

I've split out the dtest for CAS.
|[dtest|https://github.com/apache/cassandra-dtest/compare/master...kgreav:14215-2]|;;;","15/Mar/18 14:26;aleksey;Committed to 3.0 as [fdc61cb6a74163402181d133519462f0ab1b504d|https://github.com/apache/cassandra/commit/fdc61cb6a74163402181d133519462f0ab1b504d] and merged into 3.11 and trunk. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
group by select queries query results differ when using select * vs select fields,CASSANDRA-14209,13135435,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,blerer,dmodha,dmodha,01/Feb/18 14:18,16/Apr/19 09:29,13/Jul/23 08:37,02/Mar/18 10:48,3.11.3,,,,,,,,,,0,,,,"{{I get two different out with these 2 queries.  The only difference between the 2 queries is that one does ‘select *’ and other does ‘select specific fields’ without any aggregate functions.}}

{{I am using Apache Cassandra 3.10.}}


{{Consistency level set to LOCAL_QUORUM.}}
{{cassandra@cqlsh> select * from wp.position where account_id = 'user_1';}}

{{ account_id | security_id | counter | avg_exec_price | pending_quantity | quantity | transaction_id | update_time}}
{{------------+-------------+---------+----------------+------------------+----------+----------------+---------------------------------}}
{{ user_1 | AMZN | 2 | 1239.2 | 0 | 1011 | null | 2018-01-25 17:18:07.158000+0000}}
{{ user_1 | AMZN | 1 | 1239.2 | 0 | 1010 | null | 2018-01-25 17:18:07.158000+0000}}

{{(2 rows)}}
{{cassandra@cqlsh> select * from wp.position where account_id = 'user_1' group by security_id;}}

{{ account_id | security_id | counter | avg_exec_price | pending_quantity | quantity | transaction_id | update_time}}
{{------------+-------------+---------+----------------+------------------+----------+----------------+---------------------------------}}
{{ user_1 | AMZN | 1 | 1239.2 | 0 | 1010 | null | 2018-01-25 17:18:07.158000+0000}}

{{(1 rows)}}
{{cassandra@cqlsh> select account_id,security_id, counter, avg_exec_price,quantity, update_time from wp.position where account_id = 'user_1' group by security_id ;}}

{{ account_id | security_id | counter | avg_exec_price | quantity | update_time}}
{{------------+-------------+---------+----------------+----------+---------------------------------}}
{{ user_1 | AMZN | 2 | 1239.2 | 1011 | 2018-01-25 17:18:07.158000+0000}}

{{(1 rows)}}


{{Table Description:}}
{{CREATE TABLE wp.position (}}
{{ account_id text,}}
{{ security_id text,}}
{{ counter bigint,}}
{{ avg_exec_price double,}}
{{ pending_quantity double,}}
{{ quantity double,}}
{{ transaction_id uuid,}}
{{ update_time timestamp,}}
{{ PRIMARY KEY (account_id, security_id, counter)}}
{{) WITH CLUSTERING ORDER BY (security_id ASC, counter DESC)}}{{ }}",,adelapena,blerer,dmodha,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/18 14:12;dmodha;Re group by select queries.txt;https://issues.apache.org/jira/secure/attachment/12908815/Re+group+by+select+queries.txt",,,,,,,,,,,,,1.0,blerer,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 02 10:48:53 UTC 2018,,,,,,,,,,,"0|i3pnmf:",9223372036854775807,,,,,,,,,adelapena,,adelapena,,,Low,,,,,,,,,,,,,,,,,,,"14/Feb/18 09:58;blerer;The problem is that the {{Selection}} used for building the result set is not the good one for wildcard queries with {{GROUP BY}}.  

I pushed some patches for [3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...blerer:14209-3.11] and [trunk|https://github.com/apache/cassandra/compare/trunk...blerer:14209-trunk] to fix that problem and add some testing for it.

[~adelapena] Could you review?;;;","14/Feb/18 13:12;adelapena;The patch with the thorough tests looks good to me, +1;;;","02/Mar/18 10:48;blerer;Committed in 3.11 at 515f07b5ac75b15015401e89c379b29c788ba5a3 and merged into trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReservedKeywords class is missing some reserved CQL keywords,CASSANDRA-14205,13135087,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,adelapena,adelapena,adelapena,31/Jan/18 13:52,15/May/20 08:02,13/Jul/23 08:37,02/Mar/18 10:49,3.11.2,4.0,4.0-alpha1,,,,Legacy/CQL,,,,0,,,,"The CQL keywords {{DEFAULT}}, {{UNSET}}, {{MBEAN}} and {{MBEANS}} (introduced by CASSANDRA-11424 and CASSANDRA-10091) are neither considered [unreserved keywords|https://github.com/apache/cassandra/blob/trunk/src/antlr/Parser.g#L1788-L1846] by the ANTLR parser, nor included in the [{{ReservedKeywords}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/cql3/ReservedKeywords.java] class.

The current parser behaviour is considering them as reserved keywords, in the sense that they can't be used as keyspace/table/column names, which seems right:
{code:java}
cassandra@cqlsh> CREATE KEYSPACE unset WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
SyntaxException: line 1:16 no viable alternative at input 'unset' (CREATE KEYSPACE [unset]...)
{code}
I think we should keep considering these keywords as reserved and add them to {{ReservedKeywords}} class.",,adelapena,blerer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,adelapena,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 01 20:32:56 UTC 2018,,,,,,,,,,,"0|i3plh3:",9223372036854775807,,,,,,,,,blerer,,blerer,,,Normal,,,,,,,,,,,,,,,,,,,"31/Jan/18 13:59;adelapena;Here is the patch adding the missed reserved keywords:

||[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...adelapena:14205-3.11]||[trunk|https://github.com/apache/cassandra/compare/trunk...adelapena:14205-trunk]||;;;","01/Feb/18 09:31;blerer;Thanks for the patch +1.;;;","01/Feb/18 20:32;adelapena;Thanks for the review!

Committed to cassandra-3.11 as [6b00767427706124e016e4f471c2266899387163|https://github.com/apache/cassandra/commit/6b00767427706124e016e4f471c2266899387163] and merged to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unrepaired SSTables from garbage collection when only_purge_repaired_tombstones is true to avoid AssertionError in nodetool garbagecollect,CASSANDRA-14204,13135007,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,smiklosovic,VincentWhite,VincentWhite,31/Jan/18 07:54,04/Jul/23 12:06,13/Jul/23 08:37,04/Jul/23 12:06,3.11.16,4.0.11,4.1.3,5.0,,,Local/Compaction,,,,0,,,,"When manually running a garbage collection compaction across a table with unrepaired sstables and only_purge_repaired_tombstones set to true an assertion error is thrown. This is because the unrepaired sstables aren't being removed from the transaction as they are filtered out in filterSSTables().
||3.11||trunk||
|[branch|https://github.com/vincewhite/cassandra/commit/e13c822736edd3df3403c02e8ef90816f158cde2]|[branch|https://github.com/vincewhite/cassandra/commit/cc8828576404e72504d9b334be85f84c90e77aa7]|

The stacktrace:
{noformat}
-- StackTrace --
java.lang.AssertionError
	at org.apache.cassandra.db.compaction.CompactionManager.parallelAllSSTableOperation(CompactionManager.java:339)
	at org.apache.cassandra.db.compaction.CompactionManager.performGarbageCollection(CompactionManager.java:476)
	at org.apache.cassandra.db.ColumnFamilyStore.garbageCollect(ColumnFamilyStore.java:1579)
	at org.apache.cassandra.service.StorageService.garbageCollect(StorageService.java:3069)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1468)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:76)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1309)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1401)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:829)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:357)
	at sun.rmi.transport.Transport$1.run(Transport.java:200)
	at sun.rmi.transport.Transport$1.run(Transport.java:197)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:196)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:568)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:826)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:683)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:682)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


{noformat}",,jeromatron,KurtG,smiklosovic,VincentWhite,weideng,,,,,,,,,,,,,,,,,,,,,"smiklosovic opened a new pull request, #2423:
URL: https://github.com/apache/cassandra/pull/2423

   Thanks for sending a pull request! Here are some tips if you're new here:
    
    * Ensure you have added or run the [appropriate tests](https://cassandra.apache.org/_/development/testing.html) for your PR.
    * Be sure to keep the PR description updated to reflect all changes.
    * Write your PR title to summarize what this PR proposes.
    * If possible, provide a concise example to reproduce the issue for a faster review.
    * Read our [contributor guidelines](https://cassandra.apache.org/_/development/index.html)
    * If you're making a documentation change, see our [guide to documentation contribution](https://cassandra.apache.org/_/development/documentation.html)
    
   Commit messages should follow the following format:
   
   ```
   <One sentence description, usually Jira title or CHANGES.txt summary>
   
   <Optional lengthier description (context on patch)>
   
   patch by <Authors>; reviewed by <Reviewers> for CASSANDRA-#####
   
   Co-authored-by: Name1 <email1>
   Co-authored-by: Name2 <email2>
   
   ```
   
   The [Cassandra Jira](https://issues.apache.org/jira/projects/CASSANDRA/issues/)
   
   


;20/Jun/23 07:14;githubbot;600","smiklosovic opened a new pull request, #2424:
URL: https://github.com/apache/cassandra/pull/2424

   Thanks for sending a pull request! Here are some tips if you're new here:
    
    * Ensure you have added or run the [appropriate tests](https://cassandra.apache.org/_/development/testing.html) for your PR.
    * Be sure to keep the PR description updated to reflect all changes.
    * Write your PR title to summarize what this PR proposes.
    * If possible, provide a concise example to reproduce the issue for a faster review.
    * Read our [contributor guidelines](https://cassandra.apache.org/_/development/index.html)
    * If you're making a documentation change, see our [guide to documentation contribution](https://cassandra.apache.org/_/development/documentation.html)
    
   Commit messages should follow the following format:
   
   ```
   <One sentence description, usually Jira title or CHANGES.txt summary>
   
   <Optional lengthier description (context on patch)>
   
   patch by <Authors>; reviewed by <Reviewers> for CASSANDRA-#####
   
   Co-authored-by: Name1 <email1>
   Co-authored-by: Name2 <email2>
   
   ```
   
   The [Cassandra Jira](https://issues.apache.org/jira/projects/CASSANDRA/issues/)
   
   


;20/Jun/23 07:16;githubbot;600","smiklosovic opened a new pull request, #2425:
URL: https://github.com/apache/cassandra/pull/2425

   Thanks for sending a pull request! Here are some tips if you're new here:
    
    * Ensure you have added or run the [appropriate tests](https://cassandra.apache.org/_/development/testing.html) for your PR.
    * Be sure to keep the PR description updated to reflect all changes.
    * Write your PR title to summarize what this PR proposes.
    * If possible, provide a concise example to reproduce the issue for a faster review.
    * Read our [contributor guidelines](https://cassandra.apache.org/_/development/index.html)
    * If you're making a documentation change, see our [guide to documentation contribution](https://cassandra.apache.org/_/development/documentation.html)
    
   Commit messages should follow the following format:
   
   ```
   <One sentence description, usually Jira title or CHANGES.txt summary>
   
   <Optional lengthier description (context on patch)>
   
   patch by <Authors>; reviewed by <Reviewers> for CASSANDRA-#####
   
   Co-authored-by: Name1 <email1>
   Co-authored-by: Name2 <email2>
   
   ```
   
   The [Cassandra Jira](https://issues.apache.org/jira/projects/CASSANDRA/issues/)
   
   


;20/Jun/23 07:17;githubbot;600","smiklosovic opened a new pull request, #2426:
URL: https://github.com/apache/cassandra/pull/2426

   Thanks for sending a pull request! Here are some tips if you're new here:
    
    * Ensure you have added or run the [appropriate tests](https://cassandra.apache.org/_/development/testing.html) for your PR.
    * Be sure to keep the PR description updated to reflect all changes.
    * Write your PR title to summarize what this PR proposes.
    * If possible, provide a concise example to reproduce the issue for a faster review.
    * Read our [contributor guidelines](https://cassandra.apache.org/_/development/index.html)
    * If you're making a documentation change, see our [guide to documentation contribution](https://cassandra.apache.org/_/development/documentation.html)
    
   Commit messages should follow the following format:
   
   ```
   <One sentence description, usually Jira title or CHANGES.txt summary>
   
   <Optional lengthier description (context on patch)>
   
   patch by <Authors>; reviewed by <Reviewers> for CASSANDRA-#####
   
   Co-authored-by: Name1 <email1>
   Co-authored-by: Name2 <email2>
   
   ```
   
   The [Cassandra Jira](https://issues.apache.org/jira/projects/CASSANDRA/issues/)
   
   


;20/Jun/23 07:18;githubbot;600","blambov commented on code in PR #2423:
URL: https://github.com/apache/cassandra/pull/2423#discussion_r1244919074


##########
src/java/org/apache/cassandra/db/compaction/CompactionManager.java:
##########
@@ -680,12 +680,28 @@ public AllSSTableOpStatus performGarbageCollection(final ColumnFamilyStore cfSto
             @Override
             public Iterable<SSTableReader> filterSSTables(LifecycleTransaction transaction)
             {
-                Iterable<SSTableReader> originals = transaction.originals();
+                List<SSTableReader> sstables = Lists.newArrayList(transaction.originals());
+                Iterator<SSTableReader> iter = sstables.iterator();
                 if (cfStore.getCompactionStrategyManager().onlyPurgeRepairedTombstones())
-                    originals = Iterables.filter(originals, SSTableReader::isRepaired);
-                List<SSTableReader> sortedSSTables = Lists.newArrayList(originals);
-                Collections.sort(sortedSSTables, SSTableReader.maxTimestampAscending);
-                return sortedSSTables;
+                {
+                    while (iter.hasNext())
+                    {
+                        SSTableReader sstable = iter.next();
+                        if (!sstable.isRepaired())
+                        {
+                            try
+                            {
+                                transaction.cancel(sstable);
+                            }
+                            finally
+                            {
+                                iter.remove();

Review Comment:
   Removal in the interior of the array list makes this an O(n^2) operation, which can be a problem when the number of sstables can grow proportionally with data size (e.g. with LCS and UCS).
   I'd turn this into an iteration over the source, deciding to add to an initially empty array or cancel.



;28/Jun/23 09:44;githubbot;600","smiklosovic commented on PR #2423:
URL: https://github.com/apache/cassandra/pull/2423#issuecomment-1612664355

   @blambov would  you mind to take a look again, please?


;29/Jun/23 08:58;githubbot;600","blambov commented on code in PR #2423:
URL: https://github.com/apache/cassandra/pull/2423#discussion_r1246365989


##########
src/java/org/apache/cassandra/db/compaction/CompactionManager.java:
##########
@@ -680,12 +680,25 @@ public AllSSTableOpStatus performGarbageCollection(final ColumnFamilyStore cfSto
             @Override
             public Iterable<SSTableReader> filterSSTables(LifecycleTransaction transaction)
             {
-                Iterable<SSTableReader> originals = transaction.originals();
+                List<SSTableReader> filteredSSTables = new ArrayList<>();
                 if (cfStore.getCompactionStrategyManager().onlyPurgeRepairedTombstones())
-                    originals = Iterables.filter(originals, SSTableReader::isRepaired);
-                List<SSTableReader> sortedSSTables = Lists.newArrayList(originals);
-                Collections.sort(sortedSSTables, SSTableReader.maxTimestampAscending);
-                return sortedSSTables;
+                    for (SSTableReader sstable : transaction.originals())

Review Comment:
   Nit: Multi-line cases are usually enclosed in {}. In this case there's some potential for confusion about the else case on 697, thus at least the for cycle needs to be in a block.



;03/Jul/23 11:12;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,4200,,,0,4200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,smiklosovic,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 04 11:39:15 UTC 2023,,,,,,,,,,,"0|i3pkzj:",9223372036854775807,,,,,,,,,,,blambov,,,Low,,3.10,,,https://github.com/apache/cassandra/commit/028ea02a4a877d115c748d3e179c6afb2328e430,,,,,,,,,CI,,,,,"06/Mar/18 01:53;KurtG;patch LGTM but could probably have a test that will ensure we never include unrepaired sstables when onlyPurgeRepairedTombstones is true.;;;","13/Jul/18 11:19;spod;So the patched implementation pretty much follows {{performSSTableRewrite()}}, which looks like the correct way to handle this to me. We could modify {{GcCompactionTest}} a bit to make some of the effects more testable, see [ebd7de7|https://github.com/spodkowinski/cassandra/commit/ebd7de758b48a6f924d60eeecbc615c355c87257].;;;","20/Jun/23 07:18;smiklosovic;PRs

trunk https://github.com/apache/cassandra/pull/2423
4.1 https://github.com/apache/cassandra/pull/2424
4.0 https://github.com/apache/cassandra/pull/2425
3.11 https://github.com/apache/cassandra/pull/2426

builds

trunk 
j11 pre-commit https://app.circleci.com/pipelines/github/instaclustr/cassandra/2482/workflows/b8690c93-121b-4ed6-aed7-6e742285ce13
j8 pre-commit https://app.circleci.com/pipelines/github/instaclustr/cassandra/2482/workflows/20e45719-36cb-4010-95b5-89c5519e91d3 
4.1
j11 pre-commit https://app.circleci.com/pipelines/github/instaclustr/cassandra/2484/workflows/bfa530d2-1ab9-4ce3-9f08-59d41f0bbcac
j8 pre-commit https://app.circleci.com/pipelines/github/instaclustr/cassandra/2497/workflows/2eb2a057-8b1a-44ad-af12-72c34459b551
4.0 
j11 pre-commit  https://app.circleci.com/pipelines/github/instaclustr/cassandra/2485/workflows/cb68a9f9-8e52-4d45-843d-24e626ca0402
j8 pre-commit https://app.circleci.com/pipelines/github/instaclustr/cassandra/2485/workflows/fbf627a1-ed93-42c9-99c8-524e22536891
3.11 https://app.circleci.com/pipelines/github/instaclustr/cassandra/2486/workflows/c32e4dc3-e680-4ceb-8218-3bd71c5a5bff;;;","26/Jun/23 12:43;smiklosovic;[~jjirsa] would  you mind to take a look please? This should be quite straightforward.;;;","27/Jun/23 16:06;smiklosovic;[~blambov] would you mind to take a look? I contacted Jeff and he is not working on Cassandra actively at the moment.;;;","29/Jun/23 08:59;smiklosovic;Thank you [~blambov] for review, would you mind to take a look again? https://github.com/apache/cassandra/pull/2423;;;","04/Jul/23 11:39;smiklosovic;Branimir +1ed on the PR. Builds are here

3.11 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2631/workflows/812d7b99-1da3-4cc1-b15e-ac5cb07c8f4e]
4.0 j11 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2632/workflows/85890046-207a-4508-8c76-fb3e06b76c48]
4.0 j8 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2632/workflows/ae1a70e2-65ff-46e9-a5d1-b885fa372aef]
4.1 j11 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2633/workflows/9041d37f-a5a0-4346-81ec-0bdcd2189b36]
4.1 j8 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2633/workflows/d55d764d-1661-4b2c-9b2b-964f30775600]
trunk j8 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2637/workflows/24a86898-bcb3-47d1-9631-a894aac0f12a]
trunk j8 [https://app.circleci.com/pipelines/github/instaclustr/cassandra/2637/workflows/e84fc541-abe8-43ca-9b4b-d38109fe374b]

[https://app.circleci.com/pipelines/github/instaclustr/cassandra/2638/workflows/c84de8f7-8e9f-4f41-82d4-7ad3c5892859]
[https://app.circleci.com/pipelines/github/instaclustr/cassandra/2638/workflows/38a73d2f-a21d-4bd3-abfc-12f79425ffb3]

I am going to merge this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
replace_address_test:TestReplaceAddress.test_multi_dc_replace_with_rf1 fails without vnodes,CASSANDRA-14196,13134151,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,k-rus,aweisberg,aweisberg,27/Jan/18 00:02,27/May/22 19:25,13/Jul/23 08:37,15/Sep/21 19:02,3.0.26,3.11.12,4.0.2,4.1,4.1-alpha1,,Legacy/Core,Legacy/Testing,,,0,,,,"Skipping it for now, but it probably should pass without vnodes.
https://circleci.com/gh/aweisberg/cassandra/871#tests/containers/15",,aweisberg,e.dimitrova,k-rus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,k-rus,,,,,,,,,,,,Correctness -> Test Failure,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 15 19:02:07 UTC 2021,,,,,,,,,,,"0|i3pfpr:",9223372036854775807,,,,,,,,,,,brandon.williams,e.dimitrova,,Normal,,,,,https://github.com/apache/cassandra-dtest/commit/f82f102bc1b401ce97256032a8e7ce0c4097d97c,,,,,,,,,"[PR|https://github.com/apache/cassandra-dtest/pull/159]

[Java 11 CircleCI build|https://app.circleci.com/pipelines/github/k-rus/cassandra/14/workflows/3af46462-d162-4997-a49e-1ca10cd2392b] with dtests and repeated this test is succeeded
[Java 8 CircleCI build|https://app.circleci.com/pipelines/github/k-rus/cassandra/14/workflows/6f252437-7829-43a2-95d6-48d11531a26e] with dtests and repeated this test is succeeded",,k-rus,,,"08/Sep/21 09:20;k-rus;I have done some local testing and suspect that this is not an issue any more. It looks like that the marks of vnode and flaky can be removed.;;;","09/Sep/21 09:22;k-rus;[~aweisberg] if you don't mind, I will assign the ticket to me and will run this test without vnodes on CircleCI to see if it is not the issue any more. Let me know if you prefer to keep this ticket on you, or you have comments to add to the ticket. Thank you.;;;","09/Sep/21 14:14;e.dimitrova;Hey [~k-rus], thank you for picking this one!

As you mentioned CircleCI and flakiness, just wanted to bring your attention to the point we have multiplexer jobs in CircleCI now that can be used to run a test in a loop and ensure no flakiness. Excuse me if you already knew about it and I am just making noise. ;) ;;;","09/Sep/21 14:28;k-rus;[~e.dimitrova] Thank you for brining it. I heard about it but haven't tried. So I am not fully sure how it works. If it will be enough just to enabled repeated_dtest in CI or do I need to do additional adjustments for the number of runs.;;;","09/Sep/21 14:51;e.dimitrova;Example usages can be found [here|https://github.com/apache/cassandra/blob/trunk/.circleci/config-2_1.yml#L39-L91]

About number of repetitions - I would say we normally use our own judgement based on experience.

For more complicated tests I normally raise the repetitions. You can also set the job to [stop|https://github.com/apache/cassandra/blob/trunk/.circleci/config-2_1.yml#L70] in case of failure if you want to.  

Please feel free to ping me if you are unsure or have issues to run the job. Thank you;;;","10/Sep/21 10:52;k-rus;[PR|https://github.com/apache/cassandra-dtest/pull/159]

[Java 11 CircleCI build|https://app.circleci.com/pipelines/github/k-rus/cassandra/14/workflows/3af46462-d162-4997-a49e-1ca10cd2392b] with dtests and repeated this test is succeeded
 [Java 8 CircleCI build|https://app.circleci.com/pipelines/github/k-rus/cassandra/14/workflows/6f252437-7829-43a2-95d6-48d11531a26e] with dtests and repeated this test is succeeded;;;","10/Sep/21 14:31;e.dimitrova;Thanks [~k-rus].

The patch seems fine and the test seems stable on trunk with no vnodes.

In regards to testing, I am thinking of two things here:

1) The flaky mark is for running with vnodes if I understand correctly. I think you checked it only without vnodes.

2) I think this test is ran also with other cassandra versions so if we do any modifications we need to be sure that it works fine with all affected branches. ;;;","13/Sep/21 06:27;k-rus;Thank you. I will test flakiness with vnodes and different versions. Am I correct that I need for each case create new commit for CircleCI?;;;","13/Sep/21 14:58;e.dimitrova;{quote}Thank you. I will test flakiness with vnodes and different versions
{quote}
Thank you :) 
{quote}Am I correct that I need for each case create new commit for CircleCI?
{quote}
That's correct. By default you test with no vnodes, to change that you need to apply the change [here.|https://github.com/k-rus/cassandra/blob/c862a8fc5dfc33a72dc709ee5b4c43c8297b44f3/.circleci/config-2_1.yml#L66]

 ;;;","14/Sep/21 09:04;k-rus;{quote}
[Java 11 CircleCI build|https://app.circleci.com/pipelines/github/k-rus/cassandra/14/workflows/3af46462-d162-4997-a49e-1ca10cd2392b] with dtests and repeated this test is succeeded
 [Java 8 CircleCI build|https://app.circleci.com/pipelines/github/k-rus/cassandra/14/workflows/6f252437-7829-43a2-95d6-48d11531a26e] with dtests and repeated this test is succeeded
{quote}
[Java 11 CircleCI build|https://app.circleci.com/pipelines/github/k-rus/cassandra/18/workflows/0cb193f3-ffe8-41c1-a376-43c91634579e] with vnodes and repeated test succeeded on trunk (with unrelated failures of flaky test)
[Java 8 CircleCI build|https://app.circleci.com/pipelines/github/k-rus/cassandra/18/workflows/ba5d01d1-70a7-4688-b599-da5c053ea749] with vnodes and repeated test succeeded on trunk (with unrelated failures of flaky test)
[Cassandra 3.0 CircleCI build|https://app.circleci.com/pipelines/github/k-rus/cassandra/19/workflows/06efdef7-143c-4382-a47e-6c9e991673d5] without vnodes succeeded.
[Cassandra 3.0 CircleCI build|https://app.circleci.com/pipelines/github/k-rus/cassandra/20/workflows/7ed066be-1868-4fa6-9dcc-6cf73e9bcbab] with vnodes succeeded.
[3.11 no-vnodes|https://app.circleci.com/pipelines/github/k-rus/cassandra/21/workflows/cf927709-a17c-4f05-9935-0dba59dd8bd4]
[3.11 vnodes|https://app.circleci.com/pipelines/github/k-rus/cassandra/22/workflows/e03020d5-d68c-4193-b27c-a1b7b6136bb5]
[4.0 j11 no-vnodes|https://app.circleci.com/pipelines/github/k-rus/cassandra/23/workflows/8053e198-656b-45bd-b032-b865a217fd5d]
[4.0 j8 no-vnodes|https://app.circleci.com/pipelines/github/k-rus/cassandra/23/workflows/51366990-7764-4b7e-9fcb-28164d2b16a0]
[4.0 j11 vnodes|https://app.circleci.com/pipelines/github/k-rus/cassandra/24/workflows/79d34c10-f382-4057-987b-b887304c84dd]
[4.0 j8 vnodes|https://app.circleci.com/pipelines/github/k-rus/cassandra/24/workflows/bd2eed0a-3c60-4bc3-9c12-ce6665c72ccc];;;","14/Sep/21 12:33;k-rus;[~e.dimitrova] I run builds on trunk and on Cassandra 3.0 branches with and without vnodes. Is it enough? Or do you expect me to run on 3.11 and 4.0 branches?;;;","14/Sep/21 14:42;e.dimitrova;We need to run the tests at least once per affected branch or I would probably do it just a few times on the other branches, or at least 3.11 more than one test run. Depends on how much the code in that area diverged between the different versions. 

Also, we need second reviewer, [~brandon.williams], do you mind to look at this too?;;;","14/Sep/21 14:48;brandon.williams;We should test all the branches to be proactive, imo.  With 4.0 in the unicorn green state right now, I'd hate to discover a gotcha after committing.  I'm happy to review.;;;","14/Sep/21 14:54;k-rus;[~e.dimitrova] [~brandon.williams] I am working on to push branches with commits to CircleCI for 3.11 and 4.0 with and without vnodes.;;;","15/Sep/21 14:07;k-rus;[~e.dimitrova][~brandon.williams] to my understanding the test, which I enabled, was successfully executed as repeated on all Cassandra branches with and without vnodes.;;;","15/Sep/21 14:54;brandon.williams;I may have missed it, but I don't see a 4.0 w/vnodes run?;;;","15/Sep/21 15:32;e.dimitrova;It seems those were added in the latest edit of this comment.

https://issues.apache.org/jira/browse/CASSANDRA-14196?focusedCommentId=17414821&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17414821

The results look good to me. +1 Thank you!;;;","15/Sep/21 15:35;brandon.williams;ah, my comment preceded that.  +1;;;","15/Sep/21 18:49;e.dimitrova;Thanks, I will commit it soon ;;;","15/Sep/21 19:02;e.dimitrova;Committed, thanks!

To https://github.com/apache/cassandra-dtest.git

   6f3a4cb3..f82f102b  trunk -> trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLogSegmentManagerCDCTest is flaky,CASSANDRA-14195,13134127,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jay.zhuang,aweisberg,aweisberg,26/Jan/18 22:19,15/May/20 08:02,13/Jul/23 08:37,30/Jan/18 15:10,4.0,4.0-alpha1,,,,,Legacy/Testing,,,,0,,,,"This fails fairly reliably in CircleCI and in a few minutes if you run it in a loop on a MacOS laptop.

I see two failures.
{noformat}
    [junit] Testcase: testRetainLinkOnDiscardCDC(org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest):	Caused an ERROR
    [junit] Rejecting mutation to keyspace cql_test_keyspace. Free up space in build/test/cassandra/cdc_raw:0 by processing CDC logs.
    [junit] org.apache.cassandra.exceptions.CDCWriteException: Rejecting mutation to keyspace cql_test_keyspace. Free up space in build/test/cassandra/cdc_raw:0 by processing CDC logs.
    [junit] 	at org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDC.throwIfForbidden(CommitLogSegmentManagerCDC.java:136)
    [junit] 	at org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDC.allocate(CommitLogSegmentManagerCDC.java:108)
    [junit] 	at org.apache.cassandra.db.commitlog.CommitLog.add(CommitLog.java:272)
    [junit] 	at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:604)
    [junit] 	at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:481)
    [junit] 	at org.apache.cassandra.db.Mutation.apply(Mutation.java:191)
    [junit] 	at org.apache.cassandra.db.Mutation.apply(Mutation.java:196)
    [junit] 	at org.apache.cassandra.db.Mutation.apply(Mutation.java:205)
    [junit] 	at org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest.testRetainLinkOnDiscardCDC(CommitLogSegmentManagerCDCTest.java:256)
{noformat}

and

{noformat}
    [junit] Testcase: testCompletedFlag(org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest):	FAILED
    [junit] Index file not written: build/test/cassandra/cdc_raw:0/CommitLog-7-1517005121474_cdc.idx
    [junit] junit.framework.AssertionFailedError: Index file not written: build/test/cassandra/cdc_raw:0/CommitLog-7-1517005121474_cdc.idx
    [junit] 	at org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest.testCompletedFlag(CommitLogSegmentManagerCDCTest.java:210)
{noformat}",,aweisberg,jasobrown,jay.zhuang,JoshuaMcKenzie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14141,CASSANDRA-12148,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 30 15:10:00 UTC 2018,,,,,,,,,,,"0|i3pfkf:",9223372036854775807,,,,,,,,,JoshuaMcKenzie,,JoshuaMcKenzie,,,Low,,,,,,,,,,,,,,,,,,,"29/Jan/18 00:12;jay.zhuang;The problem is because [{{testCompletedFlag()}}|https://github.com/apache/cassandra/commit/e9da85723a8dd40872c4bca087a03b655bd2cacb#diff-7bfedffd12a11d61fa013c6a5894c102R191] and [{{testReplayLogic()}}|https://github.com/apache/cassandra/commit/e9da85723a8dd40872c4bca087a03b655bd2cacb#diff-7bfedffd12a11d61fa013c6a5894c102R279] don't reset the {{CDCSpaceInMB}} after running, which impacts the other tests.
Here is the fix, please review:
| Branch | uTest |
| [14195|https://github.com/cooldoger/cassandra/tree/14195] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14195.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14195] |;;;","29/Jan/18 17:19;JoshuaMcKenzie;+1. Good catch.;;;","29/Jan/18 17:44;aweisberg;Thanks that did seem to fix it.;;;","30/Jan/18 11:46;jasobrown;Did any one commit yet? :);;;","30/Jan/18 14:09;JoshuaMcKenzie;Not yet; env is super rusty. I should be able to get to it today.

 

Thankfully it's in a bit of the code that's not changing frequently so I can get away with being slow. =/

 

Sorry about that [~jay.zhuang]!;;;","30/Jan/18 15:10;JoshuaMcKenzie;[Committed|https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commit;h=69db2359ee0889cb4a57aec179b9821ff442d26b]. Thanks Jay!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Chain commit log marker potential performance regression in batch commit mode,CASSANDRA-14194,13134074,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasobrown,aweisberg,aweisberg,26/Jan/18 18:37,15/May/20 08:06,13/Jul/23 08:37,05/Mar/18 22:39,3.0.17,3.11.3,4.0,4.0-alpha1,,,Legacy/Core,Legacy/Testing,,,0,,,,"CASSANDRA-13987 modified how the commit log sync thread works. I noticed that cql3.ViewTest and ViewBuilderTaskTest have been timing out, but only in CircleCI. When I jstack in CircleCI what I see is that the commit log writer thread is parked and the threads trying to append to the commit log are blocked waiting on it.

I tested the commit before 13987 and it passed in CircleCI and then I tested with 13987 and it timed out. I suspect this may be a general performance regression and not just a CircleCI issue.

And this is with write barriers disabled (sync thread doesn't actually sync) so it wasn't blocked in the filesystem.",,aweisberg,dikanggu,jasobrown,jasonstack,jay.zhuang,jjirsa,mshuler,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13987,,,,,,,,,,,,,,,"26/Jan/18 19:42;aweisberg;jstack.txt;https://issues.apache.org/jira/secure/attachment/12907918/jstack.txt",,,,,,,,,,,,,1.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 05 22:39:31 UTC 2018,,,,,,,,,,,"0|i3pf8n:",9223372036854775807,,,,,,,,,aweisberg,,aweisberg,,,Normal,,,,,,,,,,,,,,,,,,,"26/Jan/18 18:52;jasobrown;[~aweisberg] any chance you might still have the jstack output (and can attach it here)?;;;","27/Jan/18 03:50;jjirsa; A release vote imminent; if we can’t get this reviewed and committed before that vote, the cause of the regression should be reverted until it can be fixed in 3.0.17/3.11.3

 

cc [~mshuler] ;;;","27/Jan/18 14:22;aweisberg;We would need to benchmark to establish definitively it's a performance regression. Right now I just suspect it from the fact the tests take longer in CircleCI.;;;","27/Jan/18 16:20;jasobrown;I'm working on this next (i.e. Monday morning), so depending on what the definition of what `imminent` is, we can decide if we need to revert.

That would be a shame, since we do have definable data loss problems without that patch.;;;","27/Jan/18 17:24;jasobrown;[~aweisberg] don't forget, there was a follow ticket (CASSANDRA-14108) where we cleaned up a few things. I suspect that's the ticket that would have introduced the regression, if any.

That being said, running {{ViewTest}} on my laptop at the following SHAs all completed within the same amount of time (on the 3.0 branch):
 * {{eb05025c0a768241ea61fd86db9a88cfd8f6e93e}} - commit before CASSANDRA-13987
 * {{05cb556f90dbd1929a180254809e05620265419b}} - CASSANDRA-13987
* {{010c477e4c1b8b452cc0fa33b3fdb6c286e4037d}} - current head of cassandra-3.0 (CASSANDRA-14181)

Now, this doesn't discount [~aweisberg]'s discovery here. Ariel, which branch was this on? 

I'll also do some more runs on circleci to compare... ;;;","27/Jan/18 18:14;jasobrown;I've run the head of cassandra-3.11 (currently sha {{b8c12fba064fb0b6a3b6306b2670497434471920}}), and it completed in the same time that cassandra-3.0 did (no timeout). I ran trunk (sha {{6fc3699adb4f609f78a40888af9797bb205216b0}}), and it *did* timeout.

I'm running 3.0 and 3.11 in circleci to see if they timeout. If not, then I think the regression is on trunk alone, and I'll focus my effort there. Obviously, if the source of the source looks like it'll affect earlier versions, i'll dig into that,as well.;;;","07/Feb/18 12:59;jasobrown;OK, so I've bisected both 3.0 and 3.11 locally, and I can't get them to timeout (or fail in other ways, outside of CASSANDRA-14219). I've also run them in circleci, and they do not timeout. Thus, to quote myself, I'm focusing on trunk for now. (Note: I did bisect trunk, and my initial CASSANDRA-13987 patch introduces the timeout);;;","07/Feb/18 13:26;jasobrown;[~jjirsa] I'm pretty sure this regression is related to the intersection of the group commit log patch (CASSANDRA-13530) and CASSANDRA-13987, thus only affecting trunk. Doesn't affect 3.0/3.11, so votes on those can proceed when they are ready.;;;","02/Mar/18 22:45;jasobrown;OK, think I'm at the bottom of this. While trunk is the only branch that is flat-out failing the unit tests due to a problem (see below), 3.0 and 3.11 utests are performing about 5-10% slower (when using batch commit log mode), but that 5-10% isn't enough to cause the test to timeout.
||3.0||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/14194-v2-3.0]|[branch|https://github.com/jasobrown/cassandra/tree/14194-v2-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/14194-v2-trunk]|
|[utests & dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14194-v2-3.0]|[utests & dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14194-v2-3.11]|[utests & dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14194-v2-trunk]|

This patch contains a set of small, subtle changes (the same across all three branches):

1) The main fix - There is a race in {{AbstractCommitLogService}} where the {{SyncRunnable}} thread where once we determine to call sync to disk (not just update the headers), we set {{syncRequested}} to false *after* calling {{commitLog.sync(true)}}. This allows us to have a race where {{commitLog.sync(true)}} begins it's marking work, but another mutation comes in, calls {{#requestExtraSync}} (which sets {{syncRequested}} to true, and calls unpark), and blocks in {{Allocation#awaitDiskSync()}}. If no other mutations come in (like in a unit test), {{AbstractCommitLogService}} will not {{commitLog.sync(true)}} (and {{CommitLogSegment#syncComplete}} will not release the blocked mutation) until the syncIntervalNanos is met - which, due to the introduction of the GroupCommitLog, is now hardcoded at 1000ms. By moving the {{syncRequested = false}} *before* the call to {{commitLog.sync(true)}} we eliminate the race.

2) related to #1, right before we choose to park the thread in {{SyncRunnable}}, we can check if {{syncRequested}} is true, and if so, avoid parking. This is a race between the sync thread completing the sync, but before it sleeps a new mutation calls {{#requestExtraSync}} (which unparks), and that mutation then gets stuck when the sync thread does park. If another mutation comes along it will unpark the sync thread, then both mutations can proceed; otherwise, the first mutation is blocked. This should only affect things like unit tests and *very* underutilized clusters. Note: I believe we've had this race since CASSANDRA-10202, but I'm not completely sure.

3) Fix a correctness problem, which, fortunately, has no practical implications - At the end of {{CommitLogSegment#sync()}} we always call {{syncComplete.signalAll()}}. {{syncComplete.signalAll()}} should only be called when we actually msync. Thus, if {{#sync()}} is called with {{flush}} = false, this could be improperly signalling any waiting threads. Luckily, {{AbstractCommitLogService}} should never be calling {{commitLog#sync(false)}} when in batch mode, so there is no improper signalling. However, for soundness of code, I'm moving the {{syncComplete.signalAll()}} call into the {{if(flush || close)}} block in {{CommitLogSegment#sync()}}.

4) {{AbstractCommitLogService#thread}} is now marked volatile. This field is not defined until the {{start()}} method (not the constructor), so we're not guaranteed visibility when the {{#requestExtraSync()}} and {{#awaitTermination()}} methods run. (NOTE: this field was final and defined in the ctor until CASSANDRA-8308, introduced c* 2.2) Admittedly, I'm not thrilled with putting yet another volatile on the commit log path (only in batch mode, in {{#requestExtraSync()}}), but ... correctness? Also, reading a volatile variable isn't much more expensive vs. a regulat LOAD instruction. wdyt? This one arguably could be backported to 2.2+, but that ticket was committed 3.5 years ago we have haven't heard anything, so maybe just 3.0+?;;;","05/Mar/18 18:24;aweisberg;RE #1 I think this is a correct fix. Totally makes sense that clobbering the more work indicator after already starting to collect the next batch of work is going to cause this issue.

RE #2 This won't happen because LockSupport.unpark() will cause it to wake immediately I believe? https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/locks/LockSupport.html#unpark(java.lang.Thread)

RE #3  looks good.

RE #4 should be fine. It's only read when doing hideously expensive operations that dwarf the cost of a volatile read anyways.

+1;;;","05/Mar/18 20:00;jasobrown;bq.  #2 This won't happen because LockSupport.unpark() will cause it to wake immediately I believe?

Reading the docs you linked, this sounds correct. I will remove that change on commit. (thanks! this was a good pointer);;;","05/Mar/18 22:39;jasobrown;committed as sha {{85fafd0c134cae5aa84133ad54d67f2dba28c953}}. Thanks for finding and reviewing, [~aweisberg]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sub-range selection for non-frozen collections should return null instead of empty,CASSANDRA-14182,13132670,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,blerer,blerer,blerer,22/Jan/18 12:29,16/Apr/19 09:29,13/Jul/23 08:37,24/Jan/18 15:12,,,,,,,,,,,0,,,,"For non frozen collections, Cassandra cannot differentiate an empty collection from a {{null}} one. Due to that Cassandra returns always {{null}} for non-frozen empty collection.

When selecting a sub range from a non-frozen collection, if the range does not contains any data an empty collection will be returned. It is counter intuitive and a {{null}} value should be returned instead.
 
{code:sql}
CREATE TABLE IF NOT EXISTS t (k int PRIMARY KEY, v set<int>);

INSERT INTO t (k, v) VALUES (1, {});
SELECT v FROM t; -- null
SELECT v[1] FROM t; -- null
SELECT v[1..] FROM t; -- null

INSERT INTO t (k, v) VALUES (1, {0});
SELECT v FROM t; -- {0}
SELECT v[1] FROM t; -- null
SELECT v[1..] FROM t; -- {}
{code}",,adelapena,blerer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,blerer,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jan 24 15:12:00 UTC 2018,,,,,,,,,,,"0|i3p6lr:",9223372036854775807,,,,,,,,,adelapena,,adelapena,,,Low,,,,,,,,,,,,,,,,,,,"22/Jan/18 13:32;blerer;I pushed a patch for that problem [here|https://github.com/apache/cassandra/compare/trunk...blerer:14182-trunk].

[~adelapena] could you review?;;;","22/Jan/18 13:45;adelapena;Sure thing!;;;","22/Jan/18 15:59;adelapena;Nice patch, +1 assuming CI looks good.

Just a trivial detail that can be addressed during commit: can you change [this {{row(null)}}|https://github.com/blerer/cassandra/blob/bc0d444c18a963465e8758cbb3759030a6442932/test/unit/org/apache/cassandra/cql3/validation/entities/CollectionsTest.java#L1818] by {{row((Map) null)}} to prevent the build warning, as it is done [here|https://github.com/blerer/cassandra/blob/bc0d444c18a963465e8758cbb3759030a6442932/test/unit/org/apache/cassandra/cql3/validation/entities/CollectionsTest.java#L1874]?;;;","24/Jan/18 14:27;blerer;Thanks for the review. CI looks good.;;;","24/Jan/18 15:12;blerer;Committed into trunk at 4de7a65ed9f3c97658a80dd64032ad6e82e9d58b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RPM package has too many executable files,CASSANDRA-14181,13132524,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,troels@arvin.dk,troels@arvin.dk,troels@arvin.dk,21/Jan/18 17:18,15/May/20 08:00,13/Jul/23 08:37,22/Jan/18 13:39,2.1.20,2.2.12,3.0.16,3.11.2,4.0,4.0-alpha1,Packaging,,,,0,,,,"When installing using the RPM files:
In /etc/cassandra/conf, the files should not be execuable, as they are either
 * properties-like files
 * readme-like files, or
 * files to be sourced by shell scripts

I'm adding a patch (cassandra-permissions-fix.patch) to the cassandra.spec file which fixes this.",,troels@arvin.dk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/18 17:17;troels@arvin.dk;cassandra-permissions-fix.patch;https://issues.apache.org/jira/secure/attachment/12907014/cassandra-permissions-fix.patch",,,,,,,,,,,,,1.0,troels@arvin.dk,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,,,Mon Jan 22 13:39:34 UTC 2018,,,,,,,,,,,"0|i3p5pb:",9223372036854775807,,,,,,,,,spod,,spod,,,Low,,,,,,,,,,,,,,,,,,,"22/Jan/18 13:39;spod;Again, thanks for the patch!

I've tested it locally on Fedora 26 and the affected files are now installed and owned by root using 644 permissions, just as expected.

Committed as 5ba9e6da94ed74c11f6ea37199dbe8a501859e7c to 2.1 upwards.

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra.spec needs to require ant-junit,CASSANDRA-14180,13132515,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,troels@arvin.dk,troels@arvin.dk,troels@arvin.dk,21/Jan/18 15:41,15/May/20 08:02,13/Jul/23 08:37,22/Jan/18 12:46,3.0.16,3.11.2,4.0,4.0-alpha1,,,Packaging,,,,0,,,,"I tried rebuilding cassandra-3.11.1-1.src.rpm on a Centos 7 host which had ant installed, but not the ""ant-junit"" package; that failed with a somewhat cryptic error message. It turnout out I needed to have the ""ant-junit"" package installed, as well. So for the cassandra.spec file, I suggest that the following line be added just below the existing BuildRequires line:

{{BuildRequires: ant-junit >= 1.9}}",,troels@arvin.dk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,troels@arvin.dk,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jan 22 12:46:37 UTC 2018,,,,,,,,,,,"0|i3p5nb:",9223372036854775807,,,,,,,,,spod,,spod,,,Low,,,,,,,,,,,,,,,,,,,"22/Jan/18 09:48;spod;I think you're right. But how did you manage to build the source package at all? Mine was missing a version and revision macro declaration before rpmbuild was able to finish successfully.

We use to create these values dynamically and [pass them|https://github.com/apache/cassandra-builds/blob/master/docker/build-rpms.sh#L69] as {{rpmbuild --define=""version ..""}} during our release process. Maybe we should turn that into a string substitution in the spec instead.;;;","22/Jan/18 10:28;troels@arvin.dk;I ran the following:
rpmbuild -bb --define ""version 3.11.1"" --define ""revision 4"" cassandra.spec

(And for it to compile, I also had to add a patch to the spec file, due to the recent JMX incompatibilities described in case CASSANDRA-14173).

It would certainly be nice for those building RPMs, if the version and revision values were part of the spec file itself.;;;","22/Jan/18 12:46;spod;Thanks for reporting!

Committed as 0628520a9be69bb42a0ba73859888a5a8af83b27 to cassandra-3.0 and merged upwards.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect documentation about CASSANDRA_INCLUDE priority,CASSANDRA-14175,13132116,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jasobrown,moneytoo,moneytoo,19/Jan/18 08:24,15/May/20 08:02,13/Jul/23 08:37,19/Jan/18 13:17,3.0.16,3.11.2,4.0,4.0-alpha1,,,Legacy/Documentation and Website,,,,1,,,,"In _bin/cassandra_ the comments say:
{quote}The lowest priority search path is the same directory as the startup script...
{quote}
However the ""same directory"" currently has the *highest* priority:
{code:java}
    # Locations (in order) to use when searching for an include file.
    for include in ""`dirname ""$0""`/cassandra.in.sh"" \
                   ""$HOME/.cassandra.in.sh"" \
                   /usr/share/cassandra/cassandra.in.sh \
                   /usr/local/share/cassandra/cassandra.in.sh \
                   /opt/cassandra/cassandra.in.sh; do
        if [ -r ""$include"" ]; then
            . ""$include""
            break
        fi
    done
{code}
It looks like around the release of v 2.0.0 the order was changed but the comments stayed the same.",,jasobrown,moneytoo,shinigami,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jan 19 13:17:21 UTC 2018,,,,,,,,,,,"0|i3p3r3:",9223372036854775807,3.11.1,,,,,,,,,,,,,Low,,2.0.0,,,,,,,,,,,,,,,,,"19/Jan/18 13:17;jasobrown;lol - nice find ;) Ninja-committed as sha {{1cb91eaaaad8169a7b680f1f6ab6b1418ce56e61}}

Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDK 8u161 breaks JMX integration,CASSANDRA-14173,13131823,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,samt,samt,samt,18/Jan/18 09:14,14/Sep/22 20:53,13/Jul/23 08:37,02/Feb/18 13:00,3.11.2,4.0,4.0-alpha1,,,,Observability/JMX,,,,11,,,,"{\{org.apache.cassandra.utils.JMXServerUtils}} which is used to programatically configure the JMX server and RMI registry (CASSANDRA-2967, CASSANDRA-10091) depends on some JDK internal classes/interfaces. A change to one of these, introduced in Oracle JDK 1.8.0_162 is incompatible, which means we cannot build using that JDK version. Upgrading the JVM on a node running 3.6+ will result in Cassandra being unable to start.
{noformat}
ERROR [main] 2018-01-18 07:33:18,804 CassandraDaemon.java:706 - Exception encountered during startup
java.lang.AbstractMethodError: org.apache.cassandra.utils.JMXServerUtils$Exporter.exportObject(Ljava/rmi/Remote;ILjava/rmi/server/RMIClientSocketFactory;Ljava/rmi/server/RMIServerSocketFactory;Lsun/misc/ObjectInputFilter;)Ljava/rmi/Remote;
        at javax.management.remote.rmi.RMIJRMPServerImpl.export(RMIJRMPServerImpl.java:150) ~[na:1.8.0_162]
        at javax.management.remote.rmi.RMIJRMPServerImpl.export(RMIJRMPServerImpl.java:135) ~[na:1.8.0_162]
        at javax.management.remote.rmi.RMIConnectorServer.start(RMIConnectorServer.java:405) ~[na:1.8.0_162]
        at org.apache.cassandra.utils.JMXServerUtils.createJMXServer(JMXServerUtils.java:104) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.service.CassandraDaemon.maybeInitJmx(CassandraDaemon.java:143) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:188) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:600) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:689) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]{noformat}

This is also a problem for CASSANDRA-9608, as the internals are completely re-organised in JDK9, so a more stable solution that can be applied to both JDK8 & JDK9 is required.",,amichai,eribeiro,fattahsafa,gnuphie,hkroger,jasobrown,jay.zhuang,KurtG,mshuler,rha,ricbartm,samt,sayap,schlosna,seblm,Thraxas315,tmoore,tommy_s,troels@arvin.dk,tsachev,tsteinmaurer,vovodroid,voytek.jarnot,yogeshkumar.more@gmail.com,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14189,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 06 10:21:27 UTC 2018,,,,,,,,,,,"0|i3p1y7:",9223372036854775807,3.10,3.11.0,3.11.1,3.6,3.7,3.8,3.9,,,,,,,Critical,,,,,https://github.com/apache/cassandra/commit/28ee665b3c0c9238b61a871064f024d54cddcc79,,,,,,,,,,,,,,"18/Jan/18 09:22;spod;This seems to be caused by [JDK-8159377|http://www.oracle.com/technetwork/java/javase/8u161-relnotes-4021379.html#JDK-8159377] ([cfdf57b094ce|http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/rev/cfdf57b094ce]) in 8u161.;;;","20/Jan/18 18:07;eribeiro;Hi guys,

I have uploaded a patch file based on cassandra-3.x branch. Please, see if it helps to solve the issue. 

Best regards,

Ed;;;","20/Jan/18 18:28;mshuler;Build fails on JDK 1.8u152 - this patch would appear to *require* >= 1.8u161?
{noformat}
(trunk *)mshuler@hana:~/git/cassandra$ git diff
diff --git a/src/java/org/apache/cassandra/utils/JMXServerUtils.java b/src/java/org/apache/cassandra/utils/JMXServerUtils.java
index e78ed01746..072f237049 100644
--- a/src/java/org/apache/cassandra/utils/JMXServerUtils.java
+++ b/src/java/org/apache/cassandra/utils/JMXServerUtils.java
@@ -46,6 +46,7 @@ import org.slf4j.LoggerFactory;
import com.sun.jmx.remote.internal.RMIExporter;
import com.sun.jmx.remote.security.JMXPluggableAuthenticator;
import org.apache.cassandra.auth.jmx.AuthenticationProxy;
+import sun.misc.ObjectInputFilter;
import sun.rmi.registry.RegistryImpl;
import sun.rmi.server.UnicastServerRef2;

@@ -308,10 +309,10 @@ public class JMXServerUtils
// to our custom Registry too.
private Remote connectorServer;

- public Remote exportObject(Remote obj, int port, RMIClientSocketFactory csf, RMIServerSocketFactory ssf)
+ public Remote exportObject(Remote obj, int port, RMIClientSocketFactory csf, RMIServerSocketFactory ssf, ObjectInputFilter filter)
throws RemoteException
{
- Remote remote = new UnicastServerRef2(port, csf, ssf).exportObject(obj, null, true);
+ Remote remote = new UnicastServerRef2(port, csf, ssf, filter).exportObject(obj, null, true);
// Keep a reference to the first object exported, the JMXConnectorServer
if (connectorServer == null)
connectorServer = remote;
(trunk *)mshuler@hana:~/git/cassandra$ java -version
java version ""1.8.0_152""
Java(TM) SE Runtime Environment (build 1.8.0_152-b16)
Java HotSpot(TM) 64-Bit Server VM (build 25.152-b16, mixed mode)
(trunk *)mshuler@hana:~/git/cassandra$ ant
Buildfile: /home/mshuler/git/cassandra/build.xml
Trying to override old definition of task junit

init:
[mkdir] Created dir: /home/mshuler/git/cassandra/build/classes/main
[mkdir] Created dir: /home/mshuler/git/cassandra/build/test/lib
[mkdir] Created dir: /home/mshuler/git/cassandra/build/test/classes
[mkdir] Created dir: /home/mshuler/git/cassandra/build/test/stress-classes
[mkdir] Created dir: /home/mshuler/git/cassandra/src/gen-java
[mkdir] Created dir: /home/mshuler/git/cassandra/build/lib
[mkdir] Created dir: /home/mshuler/git/cassandra/build/jacoco
[mkdir] Created dir: /home/mshuler/git/cassandra/build/jacoco/partials

maven-ant-tasks-localrepo:
[copy] Copying 1 file to /home/mshuler/git/cassandra/build

maven-ant-tasks-download:

maven-ant-tasks-init:

maven-declare-dependencies:

maven-ant-tasks-retrieve-build:
[artifact:dependencies] Building ant file: /home/mshuler/git/cassandra/build/build-dependencies.xml
[artifact:dependencies] Building ant file: /home/mshuler/git/cassandra/build/build-dependencies-sources.xml
[copy] Copying 61 files to /home/mshuler/git/cassandra/build/lib/jars
[copy] Copying 15 files to /home/mshuler/git/cassandra/build/lib/sources
[copy] Copying 9 files to /home/mshuler/git/cassandra/build/lib/jars
[unzip] Expanding: /home/mshuler/git/cassandra/build/lib/jars/org.jacoco.agent-0.7.5.201505241946.jar into /home/mshuler/git/cassandra/build/lib/jars

check-gen-cql3-grammar:

gen-cql3-grammar:
[echo] Building Grammar /home/mshuler/git/cassandra/src/antlr/Cql.g ...

generate-cql-html:

generate-jflex-java:
[jflex] Generated: StandardTokenizerImpl.java

build-project:
[echo] apache-cassandra: /home/mshuler/git/cassandra/build.xml
[javac] Compiling 1554 source files to /home/mshuler/git/cassandra/build/classes/main
[javac] Note: Processing compiler hints annotations
[javac] Note: Processing compiler hints annotations
[javac] Note: Writing compiler command file at META-INF/hotspot_compiler
[javac] Note: Done processing compiler hints annotations
[javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/utils/JMXServerUtils.java:305: error: Exporter is not abstract and does not override abstract method exportObject(Remote,int,RMIClientSocketFactory,RMIServerSocketFactory) in RMIExporter
[javac] private static class Exporter implements RMIExporter
[javac] ^
[javac] Note: Some input files use or override a deprecated API.
[javac] Note: Recompile with -Xlint:deprecation for details.
[javac] Note: Some input files use unchecked or unsafe operations.
[javac] Note: Recompile with -Xlint:unchecked for details.
[javac] 1 error

BUILD FAILED
/home/mshuler/git/cassandra/build.xml:762: Compile failed; see the compiler error output for details.

Total time: 58 seconds{noformat};;;","21/Jan/18 03:17;eribeiro;Yes [~mshuler], the first cut of this patch *required* >= 1.8u161 :( 

OTOH, I have put together another version (see further attachments above) that compiles on both 152 and 161. *_But_* *_I still need to verify if it doesn't break Cassandra_ _at runtime, though!_*

(heh, changing jdk back and forth messed up my IDE so it dragged me down)

AFAIK, 161 build has both the legacy and the new constructor (the one with _ObjectInputFilter_), so it should work as intended, but I am not sure about 152 yet. _Please_, let me know what you think, thanks.

*update:* I have uploaded a couple of patches following the pattern cassandra-14173.<versions>.patch that apply to a single branch or a range of branches. 

 ;;;","22/Jan/18 07:13;fattahsafa;161 has the same issue. I had to downgrade to 152 to get it working again.;;;","26/Jan/18 18:55;samt;In the linked branch I've removed the dependencies on internal classes completely, so this works both with u161 and earlier JDKs. The branch is against 3.11 and applies trivially to trunk.

I've run the relevant dtests locally & they seem fine as well as manually testing with jconsole & nodetool using:
 * local / remote only connections
 * SSL both with & without client auth
 * internal C* authentication & authorization
 * default JMX authn/z

||branch||testall||dtest||
|[14173-3.11|https://github.com/beobal/cassandra/tree/14173-3.11]|[testall|https://circleci.com/gh/beobal/cassandra/tree/14173-3.11]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/476/]|

 ;;;","26/Jan/18 19:09;eribeiro;Woot! Great job, [~beobal]! :) I have removed my hacky-patches from upload area not to pollute this issue.;;;","30/Jan/18 11:22;jasobrown;+1. I think we should commit this to 3.0, as well. wdyt?;;;","30/Jan/18 11:35;samt;It only affects versions above 3.6 ;;;","30/Jan/18 18:49;jasobrown;bq. It only affects versions above 3.6

wfm - go ahead and commit to 3.11 and trunk.;;;","31/Jan/18 10:59;samt;thanks, the patch actually caused a new (harmless) compiler warning, so the CI failed. I've added an annotation to suppress & will commit once the followup CI run completes.;;;","02/Feb/18 00:13;yogeshkumar.more@gmail.com;Hi Sam,

Thanks for the fix.

May i know when it is planned for release?

We are under some pressure to upgrade java version to u162. 

Thanks,

Yogesh.

 ;;;","02/Feb/18 08:56;samt;We currently have just 2 issues blocking the 3.11.2 release; this and CASSANDRA-14092. Both should be committed soon, at which point a release vote will be called. tl;dr I would probably expect a release next week.;;;","02/Feb/18 11:52;samt;I ran into a few issues with CI partly due to the recent dtest changes from CASSANDRA-14134. The master branch of dtests is currently incompatible with non-trunk C* builds (though that's being worked on in CASSANDRA-14206) and running the pre-14134 dtests via builds.apache.org is proving problematic, especially so because of the multi-hour runtime. I've managed to get a run of the latest dtests patched with [~aweisberg]'s 14206 branch through CircleCI here: https://circleci.com/workflow-run/48e1f33f-05a6-4f63-b988-01cc09f03f5e

There are only a handful of failures, none of which are JMX related so I'm going to commit this now. I appreciate this is not ideal, but I think it's the most pragmatic option right now.;;;","02/Feb/18 13:00;samt;I should add that the CI for trunk version looks reasonable with only pre-existing failures: https://circleci.com/workflow-run/b656ab4a-2249-4b49-8cd2-25edd8f2955a;;;","02/Feb/18 13:00;samt;Committed to 3.11 & trunk in 28ee665b3c0c9238b61a871064f024d54cddcc79;;;","06/Feb/18 10:21;tsteinmaurer;[~beobal]: locally built Cassandra 3.11 from source including the fix and deployed in our loadtest environment. Starts up fine now with 8u162. Thanks a lot!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Loss of digits when doing CAST from varint/bigint to decimal,CASSANDRA-14170,13131535,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,blerer,danfiala,danfiala,17/Jan/18 10:22,15/May/20 08:00,13/Jul/23 08:37,14/Mar/18 09:34,3.11.3,4.0,4.0-alpha1,,,,Legacy/CQL,,,,0,,,,"Cast functions from numeric types to decimal type are implemented as conversion to double first and then from double to decimal: [https://github.com/apache/cassandra/compare/trunk...blerer:10310-3.0#diff-6aa4a8f76df6c30c5bb4026b8c9251eeR80].

This can cause loss of digits for big values stored in varint or bigint. It is probably unexpected because decimal can store such values precisely.

Examples:

{{cqlsh> CREATE TABLE cast_bigint_test(k int PRIMARY KEY, bigint_clmn bigint);}}
 {{cqlsh> INSERT INTO cast_bigint_test(k, decimal_clmn) VALUES(2, 9223372036854775807);}}
 {{cqlsh> SELECT CAST(bigint_clmn AS decimal) FROM cast_bigint_test;}}
 {{cast(bigint_clmn as decimal)}}
 {{------------------------------}}
 {{9.223372036854776E+18}}
 {{(1 rows)}}

{{cqlsh> CREATE TABLE cast_varint_test (k int PRIMARY KEY, varint_clmn varint);}}
 {{cqlsh> INSERT INTO cast_varint_test(k, varint_clmn) values(2, 1234567890123456789);}}
 {{cqlsh> SELECT CAST(varint_clmn AS decimal) FROM cast_varint_test;}}
 {{cast(varint_clmn as decimal)}}
 {{------------------------------}}
1.23456789012345677E+18
 {{(1 rows)}}

 ",Tested with Cassandra 3.11.1 but this issue is present since the implementation of cast functions.,adelapena,blerer,danfiala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,blerer,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 14 09:34:02 UTC 2018,,,,,,,,,,,"0|i3p073:",9223372036854775807,,,,,,,,,adelapena,,adelapena,,,Low,,,,,,,,,,,,,,,,,,,"15/Feb/18 09:29;blerer;[~danfiala] Thanks for reporting the problem.

I pushed a branch [here|https://github.com/apache/cassandra/compare/cassandra-3.11...blerer:14170-3.11] to fix the problem and add some extra tests.

The patch can be merge without trouble into trunk and the Unit tests runs fine on both branches. We do not have DTests for the {{CAST}} operations.

[~adelapena] Could you review?;;;","05/Mar/18 12:46;adelapena;Overall the patch looks good to me. However, the new test [{{testNoLossOfPrecisionForCastToDecimal}}|https://github.com/blerer/cassandra/blob/4b1f5804837ae3164bb183fcb62077683621651a/test/unit/org/apache/cassandra/cql3/functions/CastFctsTest.java#L205-L212] misses the {{@Test}} annotation. Indeed, it fails due to [a column names mismatch at the insert statement|https://github.com/blerer/cassandra/blob/4b1f5804837ae3164bb183fcb62077683621651a/test/unit/org/apache/cassandra/cql3/functions/CastFctsTest.java#L208]. I think it should be [this way|https://github.com/adelapena/cassandra/commit/be44415a3f61e13720889cfd44482ee1142e8c1d]. If it's ok for you it can be addressed during commit.

Nitpick: there is a misssed blank line in {{getDecimalConversionFunction}} JavaDoc, [here|https://github.com/adelapena/cassandra/commit/86f8121f95adb60eef85edbcaf9fb3917ef5207f].;;;","14/Mar/18 09:33;blerer;Thanks for picking up my mistakes :-). I was far to quick on that one. ;;;","14/Mar/18 09:34;blerer;Committed into 3.11 at 3fa7c0894449b5c6033a6c4f47ec3292d07268b8 and merged into trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trivial intellij junit run fix,CASSANDRA-14169,13131122,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,16/Jan/18 02:28,15/May/20 08:00,13/Jul/23 08:37,14/Feb/18 15:35,4.0,4.0-alpha1,,,,,,,,,0,,,,"Unable to run {{[LegacySSTableTest|https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java#L63]}} in the Intellij, because the {{[legacy-sstable-root|https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java#L96]}} is not defined.",,jay.zhuang,VincentWhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jay.zhuang,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 14 15:35:43 UTC 2018,,,,,,,,,,,"0|i3oxnr:",9223372036854775807,,,,,,,,,spod,,spod,,,Low,,,,,,,,,,,,,,,,,,,"16/Jan/18 02:39;jay.zhuang;| Branch | uTest |
| [14169|https://github.com/cooldoger/cassandra/tree/14169] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14169.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14169] |;;;","16/Jan/18 03:59;VincentWhite;I wonder if we should include the rest of the parameters that are normally included by build.xml e.g. 

{code:java}
 <jvmarg value=""-Dmigration-sstable-root=${test.data}/migration-sstables""/>
 <jvmarg value=""-Dcassandra.ring_delay_ms=1000""/>
 <jvmarg value=""-Dcassandra.tolerate_sstable_size=true""/>
 <jvmarg value=""-Dcassandra.skip_sync=true"" />{code}


I don't know if it should be its own ticket, but I also noticed this the exception message isn't particularly helpful since it outputs the wrong variable

{code: title=org.apache.cassandra.io.sstable.LegacySSTableTest#defineSchema | java}
        String scp = System.getProperty(LEGACY_SSTABLE_PROP);
        Assert.assertNotNull(""System property "" + LEGACY_SSTABLE_ROOT + "" not set"", scp);
{code}

I believe it is meant to be:

{code: title=org.apache.cassandra.io.sstable.LegacySSTableTest#defineSchema | java}
        String scp = System.getProperty(LEGACY_SSTABLE_PROP);
        Assert.assertNotNull(""System property "" + LEGACY_SSTABLE_PROP + "" not set"", scp);
{code};;;","16/Jan/18 06:34;jay.zhuang;Thanks [~VincentWhite] for the review.

bq. <jvmarg value=""-Dmigration-sstable-root=${test.data}/migration-sstables""/>}}
The migration-sstables is removed in CASSANDRA-12716, removing them in the {{build.xml}}.

bq. <jvmarg value=""-Dcassandra.ring_delay_ms=1000""/>
It wouldn't impact the test, but nice to have. added.

bq. <jvmarg value=""-Dcassandra.tolerate_sstable_size=true""/>
It's just to print warning message: [LeveledCompactionStrategy.java:69|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/LeveledCompactionStrategy.java#L69]. I think it's good to have them in IDE debug.

bq. <jvmarg value=""-Dcassandra.skip_sync=true"" />
It's added in CASSANDRA-9403. would be nice to have. added.


Also for {{LEGACY_SSTABLE_ROOT}} -> {{LEGACY_SSTABLE_PROP}}, changed.

The patch is updated.

;;;","14/Feb/18 15:35;spod;Thanks Jay!
Merged as 7a424bc2a79dce98817054057dd3a479b202f09e to trunk.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexOutOfBoundsException when selecting column counter and consistency quorum,CASSANDRA-14167,13130995,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,fcofdezc,tristar,tristar,15/Jan/18 11:15,16/Apr/19 09:29,13/Jul/23 08:37,04/Jul/18 10:14,3.0.17,3.11.3,,,,,Legacy/Coordination,,,,0,,,,"This morning I upgraded my cluster from 3.11.0 to 3.11.1 and it appears when I perform a query on a counter specifying the column name cassandra throws the following exception:
{code:java}
WARN [ReadStage-1] 2018-01-15 10:58:30,121 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-1,5,main]: {}
java.lang.IndexOutOfBoundsException: null
java.nio.Buffer.checkIndex(Buffer.java:546) ~[na:1.8.0_144]
java.nio.HeapByteBuffer.getShort(HeapByteBuffer.java:314) ~[na:1.8.0_144]
org.apache.cassandra.db.context.CounterContext.headerLength(CounterContext.java:173) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.context.CounterContext.updateDigest(CounterContext.java:696) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.rows.AbstractCell.digest(AbstractCell.java:126) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.rows.AbstractRow.digest(AbstractRow.java:73) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.rows.UnfilteredRowIterators.digest(UnfilteredRowIterators.java:181) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.partitions.UnfilteredPartitionIterators.digest(UnfilteredPartitionIterators.java:263) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.ReadResponse.makeDigest(ReadResponse.java:120) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.ReadResponse.createDigestResponse(ReadResponse.java:87) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:345) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:50) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:66) ~[apache-cassandra-3.11.1.jar:3.11.1]
java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_144]
org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.1.jar:3.11.1]
java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
{code}

Query works completely find on consistency level ONE but not on QUORUM. 
Is this possibly related to CASSANDRA-11726?","Cassandra 3.11.1

Ubuntu 14-04",dmody,fcofdezc,gzeska,jwartnic,sinorga,slebresne,tristar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,fcofdezc,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 04 10:14:47 UTC 2018,,,,,,,,,,,"0|i3owvj:",9223372036854775807,,,,,,,,,slebresne,,slebresne,,,Normal,,,,,,,,,,,,,,,,,,,"20/Feb/18 15:22;dmody;I am facing the similar issue after upgrading from 3.0.15 to 3.11.1 .. 
The system.log file - has a bunch of these warnings- 
WARN  [ReadStage-15] 2018-02-20 14:39:49,929 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-15,5,main]: {}
java.lang.IndexOutOfBoundsException: null
        at java.nio.Buffer.checkIndex(Buffer.java:546) ~[na:1.8.0_141]
        at java.nio.HeapByteBuffer.getShort(HeapByteBuffer.java:314) ~[na:1.8.0_141]
        at org.apache.cassandra.db.context.CounterContext.headerLength(CounterContext.java:173) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.db.context.CounterContext.updateDigest(CounterContext.java:696) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.db.rows.AbstractCell.digest(AbstractCell.java:126) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.db.rows.AbstractRow.digest(AbstractRow.java:73) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.db.rows.UnfilteredRowIterators.digest(UnfilteredRowIterators.java:181) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators.digest(UnfilteredPartitionIterators.java:263) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.db.ReadResponse.makeDigest(ReadResponse.java:120) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.db.ReadResponse.createDigestResponse(ReadResponse.java:87) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:345) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:50) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:66) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_141]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.1.jar:3.11.1]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_141]

Can someone please advise on how to resolve this? 
This is causing application failure to write data (it uses consistency level - local_quoram for both reads and writes). ;;;","21/Feb/18 15:07;dmody;[~slebresne] [~iamaleksey] - Hi Aleksey and Sylvain - I saw https://issues.apache.org/jira/browse/CASSANDRA-11726 which is similar to the issue I'm facing - can you please let me know on how can I fix this? Should I downgrade to 3.11.0 from 3.11.1 - would that help in fixing this issue? ;;;","02/Mar/18 16:31;jwartnic;I am having the exact same problem. Upgraded to Cassandra 3.11.1. I have a reproducible case with a record.

I even tried copying the data out to a flat-file, truncating the table, and re-loading it back in. Then flushed the memtables and re-queried a single, random, record which fails with consistency LOCAL_QUORUM but works fine with consistency ONE OR works fine if you don't specify the column name (e.g. SELECT *). 

One workaround that worked for us - as this is NOT a production environment yet, was to set the RF=1 on the keyspaces that contained the ""counter"" tables. By doing that the CF of QUORUM equates to 1, which then fixes the issue (no read repairs or validations required). Obviously if this was production, I would have some concerns regarding data availability, but for now, this is a workaround that may work for someone else.

 ;;;","11/Apr/18 06:39;sinorga;Hi, I got the same warn log with Cassandra 3.11.1, it also comes with another error log which seems to be related.
{code:java}
ERROR [ReadRepairStage:112269] 2018-04-11 05:44:46,563 CassandraDaemon.java:228 - Exception in thread Thread[ReadRepairStage:112269,5,main]
java.lang.IndexOutOfBoundsException: null
	at java.nio.Buffer.checkIndex(Buffer.java:546) ~[na:1.8.0_151]
	at java.nio.HeapByteBuffer.getShort(HeapByteBuffer.java:314) ~[na:1.8.0_151]
	at org.apache.cassandra.db.context.CounterContext.headerLength(CounterContext.java:173) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.context.CounterContext.updateDigest(CounterContext.java:696) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.rows.AbstractCell.digest(AbstractCell.java:126) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.rows.AbstractRow.digest(AbstractRow.java:73) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.rows.UnfilteredRowIterators.digest(UnfilteredRowIterators.java:181) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators.digest(UnfilteredPartitionIterators.java:263) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.ReadResponse.makeDigest(ReadResponse.java:120) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.ReadResponse$DataResponse.digest(ReadResponse.java:225) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.service.DigestResolver.compareResponses(DigestResolver.java:87) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.service.ReadCallback$AsyncRepairRunner.run(ReadCallback.java:233) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_151]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_151]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_151]
{code}
The environment Info:
 * Cluster nodes: 3
 * RF: 3
 * Some counter tables
 * Consistency Level

 * 
 ** Read: ONE
 ** Write: LOCAL_QUORUM

The problem is I can't find the exact query command to reproduce this issue so far. And I didn't get any error from the application level.
Is there any way to locate the query? Could It be triggered Cassandra internal jobs or write action?

Any suggestion is appreciated.

 ;;;","22/Apr/18 11:11;fcofdezc;The issue seems to be related to an optimization (CASSANDRA-10657) making counter cell empty. 
Here you can find a [patch|https://github.com/apache/cassandra/compare/cassandra-3.11...fcofdez:CASSANDRA-14167?expand=1] that solves the issue.;;;","23/Apr/18 08:06;slebresne;+1 on that patch. As mentioned by [~dhawalmody1] above, it is very similar to CASSANDRA-11726 (and indeed ultimately due to proper handling of CASSANDRA-10657).

[~fcofdezc] Would you have time to quickly setup CircleCI (http://cassandra.apache.org/doc/latest/development/testing.html#circleci) and run it on this? I can take care of it though if you prefer.;;;","23/Apr/18 09:26;fcofdezc;[Here|https://circleci.com/gh/fcofdez/cassandra/8] it is. If you need something else, just tell me. Thanks for the review!;;;","02/Jul/18 07:11;gzeska;[~slebresne] [~fcofdezc] what is the status of this issue ? It is going to be fixed with next release ? ;;;","04/Jul/18 10:14;slebresne;Sorry, it slipped to the cracks. CI was clean so committed. As the vote for 3.0.17 and 3.11.3 got -1ed yesterday, this should make the next releases, whenever those happen.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstabledump tries to delete a file,CASSANDRA-14166,13130544,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,KurtG,Python_Max,Python_Max,12/Jan/18 15:04,15/May/20 08:01,13/Jul/23 08:37,08/Mar/18 01:50,3.0.17,3.11.3,4.0,4.0-alpha1,,,Legacy/Tools,,,,0,,,,"Directory /var/lib/cassandra/data/<keyspace>/<table> has cassandra:cassandra owner.
An error happens when sstabledump executed on file in that directory by regular user:


{code:java}
$ sstabledump mc-56801-big-Data.db
Exception in thread ""main"" FSWriteError in /var/lib/cassandra/data/<keyspace>/<table>/mc-56801-big-Summary.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:142)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:159)
        at org.apache.cassandra.io.sstable.format.SSTableReader.saveSummary(SSTableReader.java:935)
        at org.apache.cassandra.io.sstable.format.SSTableReader.saveSummary(SSTableReader.java:920)
        at org.apache.cassandra.io.sstable.format.SSTableReader.load(SSTableReader.java:788)
        at org.apache.cassandra.io.sstable.format.SSTableReader.load(SSTableReader.java:731)
        at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:516)
        at org.apache.cassandra.io.sstable.format.SSTableReader.openNoValidation(SSTableReader.java:396)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:191)
Caused by: java.nio.file.AccessDeniedException: /var/lib/cassandra/data/<keyspace>/<table>/mc-56801-big-Summary.db
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
        at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
        at java.nio.file.Files.delete(Files.java:1126)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:136)
        ... 8 more

{code}

I have changed bloom_filter_fp_chance for that table couple months ago, so I believe that's the reason why SSTableReader wants to recreate summary file. But when used in sstabledump it should not try to modify / delete any files.",,jeromatron,jjirsa,KurtG,Python_Max,rha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-11163,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,KurtG,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jan 12 16:57:51 UTC 2018,,,,,,,,,,,"0|i3ouuv:",9223372036854775807,3.11.1,,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"12/Jan/18 16:57;jjirsa;Somewhat related to CASSANDRA-11163 , but {{sstabledump}} especially really shouldn't be making any changes.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backport 7950 (Output of nodetool compactionstats and compactionhistory does not work well with long keyspace and column family names),CASSANDRA-14162,13130029,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stefan.miklosovic,KurtG,KurtG,11/Jan/18 01:06,13/Jul/21 14:56,13/Jul/23 08:37,13/Jul/21 14:56,3.0.25,,,,,,Legacy/Tools,,,,0,,,,Colleagues have had issues with output of listsnapshots/compactionstats because of things with really long names. Mostly cosmetic but I see no reason we shouldn't backport CASSANDRA-7950 to 3.0. It's practically a bugfix. I've attached a patch and a bunch of images to show the relevant commands working as intended after applying the patch.,,aleksey,e.dimitrova,jay.zhuang,jeromatron,jjirsa,jjordan,KurtG,stefan.miklosovic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/18 01:08;KurtG;14162-3.0.patch;https://issues.apache.org/jira/secure/attachment/12905584/14162-3.0.patch","11/Jan/18 01:08;KurtG;Screenshot from 2018-01-11 01-02-02.png;https://issues.apache.org/jira/secure/attachment/12905581/Screenshot+from+2018-01-11+01-02-02.png","11/Jan/18 01:08;KurtG;Screenshot from 2018-01-11 01-02-46.png;https://issues.apache.org/jira/secure/attachment/12905582/Screenshot+from+2018-01-11+01-02-46.png","11/Jan/18 01:08;KurtG;Screenshot from 2018-01-11 01-02-51.png;https://issues.apache.org/jira/secure/attachment/12905583/Screenshot+from+2018-01-11+01-02-51.png",,,,,,,,,,4.0,stefan.miklosovic,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 12 16:35:26 UTC 2021,,,,,,,,,,,"0|i3orr3:",9223372036854775807,,,,,,,,,,,e.dimitrova,,,Low,,NA,,,https://github.com/apache/cassandra/commit/15c22fa2f1e1ec04efb76646fd0ed5ceaff889cf,,,,,,,,,tested manually / visually,,,,,"11/Jan/18 07:21;jjirsa;[~iamaleksey] / [~slebresne] / [~jjordan] - The only argument against such a backport seems like ""it may break someone's tool parsing"" (if someone's relying on some specifics of the old, awkward nodetool output). Is that something we typically protect against in minor versions?

;;;","11/Jan/18 11:32;aleksey;Sometimes we do, but not consistently so. Personally, I'm ok with porting this to 3.0.;;;","11/Jan/18 11:55;jjordan;We do try not to change tool output that would break tooling. I think it would be fine to change the spacing, to accommodate long names as long as we don’t change the order or number of columns.;;;","14/Mar/18 04:53;KurtG;Anyone feel like reviewing this? I put it in a GH branch for easier reviewing.

[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...kgreav:14162-3.0];;;","02/Jul/21 16:32;brandon.williams;Needs a little rebase.;;;","07/Jul/21 19:30;stefan.miklosovic;[~brandon.williams] would you mind to review please?

 

[https://github.com/apache/cassandra/pull/1103/files]

 
{code:java}
[21:28:09 ~]$ docker-shell-master-nodetool compactionstats -H
pending tasks: 3
id                                   compaction type keyspace  table     completed total     unit  progress
54a026b0-df58-11eb-a591-43fc063264b7 Compaction      keyspace1 standard1 244.39 MB 709.18 MB bytes 34.46%  
1b9c5b40-df58-11eb-a591-43fc063264b7 Compaction      keyspace1 standard1 340.16 MB 710.4 MB  bytes 47.88%  
Active compaction remaining time :   0h13m55s
[21:28:12 ~]$ docker-shell-master-nodetool compactionhistory
Compaction History: 
id                                   keyspace_name columnfamily_name              compacted_at            bytes_in bytes_out rows_merged     
dddae0b0-df57-11eb-a591-43fc063264b7 system_schema tables                         2021-07-07T21:16:44.219 3396     2716      {1:7, 4:1}      
ddd0a780-df57-11eb-a591-43fc063264b7 system_schema columns                        2021-07-07T21:16:44.152 6300     5798      {1:7, 4:1}      
dd206a50-df57-11eb-a591-43fc063264b7 system_schema keyspaces                      2021-07-07T21:16:42.997 669      381       {1:7, 4:1}      
ce15b510-df57-11eb-a591-43fc063264b7 system        sstable_activity               2021-07-07T21:16:17.761 1492     354       {1:62, 2:6}     
cdd8fa30-df57-11eb-a591-43fc063264b7 system        local                          2021-07-07T21:16:17.363 5378     5160      {4:1}           
cdcec100-df57-11eb-a591-43fc063264b7 system        size_estimates                 2021-07-07T21:16:17.296 46128    36268     {1:4, 2:1, 3:1} 
cdaf5220-df57-11eb-a591-43fc063264b7 system_auth   resource_role_permissons_index 2021-07-07T21:16:17.090 611      304       {1:4, 2:4, 3:3} 
cd9672f0-df57-11eb-a591-43fc063264b7 system_auth   role_permissions               2021-07-07T21:16:16.927 1013     522       {1:1, 4:1}      
cbe3b9e0-df57-11eb-a591-43fc063264b7 system_schema views                          2021-07-07T21:16:14.078 153      99        {1:3, 2:1}   
[21:49:44 ~]$ docker-shell-master-nodetool listsnapshots
Snapshot Details: 
Snapshot name Keyspace name      Column family name             True size Size on disk
1625686746556 system_distributed parent_repair_history          0 bytes   13 bytes    
1625686746556 system_distributed repair_history                 0 bytes   13 bytes    
1625686746556 keyspace1          counter1                       0 bytes   847 bytes   
1625686746556 keyspace1          standard1                      1.5 GB    2.31 GB     
1625686746556 test               test                           0 bytes   872 bytes   
1625686746556 system_auth        roles                          0 bytes   5.14 KB     
1625686746556 system_auth        role_members                   0 bytes   13 bytes    
1625686746556 system_auth        resource_role_permissons_index 0 bytes   15.6 KB     
1625686746556 system_auth        role_permissions               0 bytes   15.75 KB    
1625686746556 system_traces      sessions                       0 bytes   13 bytes    
1625686746556 system_traces      events                         0 bytes   13 bytes    

Total TrueDiskSpaceUsed: 1.5 GB   

{code}
 

 ;;;","07/Jul/21 19:36;stefan.miklosovic;https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/902/;;;","08/Jul/21 19:34;e.dimitrova;I can help with a review.

I just checked the build and it seems there are cqlshlib tests failing which I don't find failing in Jenkins builds.

Any ideas?;;;","09/Jul/21 13:40;stefan.miklosovic;The backport contains yet another ""feature"" which is sorting of the output.

This sorting was firstly introduced in 3.2 in https://issues.apache.org/jira/browse/CASSANDRA-10464 and since then it is sorted one way or the other but together with [~e.dimitrova] (thanks for spotting this) we actually think that it should not be the part of the backport as that would be the introduction of behaviour which would not address the original issue this ticket is trying to solve.

Unless we agree that we may introduce sorting (either doing it directly in this ticket or creating new one just for 3.0.25), we drift towards removing sorting to be strictly just about solving the long names.

However, I doubt that the introduction of sorting would be ""bad"". It only improve the user experience and I can not see any reason why a user would be suddenly ""confused and angry"" that we started to sort it for him since 3.0.25.

The commit which deletes sorting is here: https://github.com/apache/cassandra/pull/1103/commits/415a029657fe9a3873cdd256d43fe2e9ca8be384;;;","12/Jul/21 16:19;brandon.williams;It seems to me the only reason not to port the sorting would be to preserve old behavior, but we're already not doing that in this patch.  We're better off having the output match in all versions rather than having this one still be the odd ball.;;;","12/Jul/21 16:35;e.dimitrova;As it is only sorted representation, probably it is fine but normally we don't backport new behaviors. I think that was the reason also to discuss initially whether this patch should be really backported at all. 

Cassandra has that behavior of having pain in the leg when you slap her in the face even after smallest changes, so I felt it's worth mentioning that change before any commit. Thank you both;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[DTEST] [TRUNK] repair_test.py::test_dead_coordinator is flaky due to JMX connection error from nodetool,CASSANDRA-14158,13129769,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,mkjellman,mkjellman,10/Jan/18 07:16,16/Apr/19 09:29,13/Jul/23 08:37,30/Jan/18 08:02,,,,,,,Test/dtest/python,,,,0,,,,"repair_test.py::test_dead_coordinator is flaky due to occasionally failing when a JMX connection error is propagated by nodetool.

the test has failed 4+ times for the same reason.

latest failure can be found in the artifacts for the following circleci run:
[https://circleci.com/gh/mkjellman/cassandra/538]

I *think* that this might be expected behavior for this test and we just need to catch any ToolError exceptions thrown and only fail if included stack is for any error other than ""JMX connection closed.""

{code}
stderr: error: [2018-01-10 07:07:55,178] JMX connection closed. You should check server log for repair status of keyspace system_traces(Subsequent keyspaces are not going to be repaired).
-- StackTrace --
java.io.IOException: [2018-01-10 07:07:55,178] JMX connection closed. You should check server log for repair status of keyspace system_traces(Subsequent keyspaces are not going to be repaired).
	at org.apache.cassandra.tools.RepairRunner.handleConnectionFailed(RepairRunner.java:104)
	at org.apache.cassandra.utils.progress.jmx.JMXNotificationProgressListener.handleNotification(JMXNotificationProgressListener.java:86)
	at javax.management.NotificationBroadcasterSupport.handleNotification(NotificationBroadcasterSupport.java:275)
	at javax.management.NotificationBroadcasterSupport$SendNotifJob.run(NotificationBroadcasterSupport.java:352)
	at javax.management.NotificationBroadcasterSupport$1.execute(NotificationBroadcasterSupport.java:337)
	at javax.management.NotificationBroadcasterSupport.sendNotification(NotificationBroadcasterSupport.java:248)
	at javax.management.remote.rmi.RMIConnector.sendNotification(RMIConnector.java:441)
	at javax.management.remote.rmi.RMIConnector.access$1200(RMIConnector.java:121)
	at javax.management.remote.rmi.RMIConnector$RMIClientCommunicatorAdmin.gotIOException(RMIConnector.java:1531)
	at javax.management.remote.rmi.RMIConnector$RMINotifClient.fetchNotifs(RMIConnector.java:1352)
	at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.fetchOneNotif(ClientNotifForwarder.java:655)
	at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.fetchNotifs(ClientNotifForwarder.java:607)
	at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.doRun(ClientNotifForwarder.java:471)
	at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.run(ClientNotifForwarder.java:452)
	at com.sun.jmx.remote.internal.ClientNotifForwarder$LinearExecutor$1.run(ClientNotifForwarder.java:108)
{code}",,githubbot,marcuse,mkjellman,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 30 08:02:39 UTC 2018,,,,,,,,,,,"0|i3oq5j:",9223372036854775807,,,,,,,,,samt,,samt,,,Normal,,,,,,,,,,,,,,,,,,,"15/Jan/18 09:46;githubbot;GitHub user krummas opened a pull request:

    https://github.com/apache/cassandra-dtest/pull/16

    catch and ignore ToolError

    CASSANDRA-14158

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/krummas/cassandra-dtest marcuse/14158

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra-dtest/pull/16.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #16
    
----
commit 08fb197b1097dfc9de16ccded05df15c8ea5a103
Author: Marcus Eriksson <marcuse@...>
Date:   2018-01-15T09:45:24Z

    catch and ignore ToolError

----
;;;","15/Jan/18 09:46;marcuse;https://github.com/apache/cassandra-dtest/pull/16;;;","29/Jan/18 16:35;samt;+1 LGTM;;;","30/Jan/18 08:02;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/cassandra-dtest/pull/16
;;;","30/Jan/18 08:02;marcuse;committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[DTEST] [TRUNK] test_tracing_does_not_interfere_with_digest_calculation - cql_tracing_test.TestCqlTracing failed once : AssertionError: assert 0 == 1,CASSANDRA-14157,13129760,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aholmber,mkjellman,mkjellman,10/Jan/18 06:24,03/Jan/21 17:02,13/Jul/23 08:37,08/Oct/20 13:24,4.0,4.0-beta3,,,,,Test/dtest/python,,,,0,dtest,,,"test_tracing_does_not_interfere_with_digest_calculation - cql_tracing_test.TestCqlTracing failed it's assertion once today in a circleci run. the dtests were running against trunk.

Although it has failed once so far, a quick read of the comments in the test seems to indicate that the assertion failing this way might mean that CASSANDRA-13964 didn't fully fix the issue.

{code:python}
if jmx.has_mbean(rr_count):
                # expect 0 digest mismatches
>               assert 0 == jmx.read_attribute(rr_count, 'Count')
E               AssertionError: assert 0 == 1
E                +  where 1 = <bound method JolokiaAgent.read_attribute of <tools.jmxutils.JolokiaAgent object at 0x7f62d4156898>>('org.apache.cassandra.metrics:type=ReadRepair,name=RepairedBlocking', 'Count')
E                +    where <bound method JolokiaAgent.read_attribute of <tools.jmxutils.JolokiaAgent object at 0x7f62d4156898>> = <tools.jmxutils.JolokiaAgent object at 0x7f62d4156898>.read_attribute
{code}",,aholmber,blerer,e.dimitrova,mkjellman,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aholmber,,,,,,,,,,,,,,,,,,,,Normal,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 08 13:24:47 UTC 2020,,,,,,,,,,,"0|i3oq3j:",9223372036854775807,,,,,,,,,,,blerer,,,Normal,,3.0.0,,,relocate flaky repair/digest test to avoid cqlsh trace queries ,,,,,,,,,"Test relocated.
No docs required.",,,,,"24/Sep/20 13:22;e.dimitrova;Hi [~samt], do you plan to work on this ticket?

I saw this failure again today:

https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/349/workflows/04bccc52-4e3e-41e2-9c04-93501ea4ce77/jobs/2116;;;","24/Sep/20 14:21;samt;Hey [~e.dimitrova], I'm afraid I don't have capacity to pick it up right now, so feel free to take it if you like.;;;","24/Sep/20 14:25;e.dimitrova;Thanks for confirming [~samt] . I left it unassigned In case someone is interested, otherwise, probably I will try to look into it next week. ;;;","07/Oct/20 20:20;aholmber;Running on a resource-constrained VM reproduces almost 100% for me.

I believe this is just a problem with the test. It looks like the read repairs are valid, occasionally being triggered by queries ""under the hood"" in cqlsh.

The test rightly expects the [insert and subsequent select|https://github.com/apache/cassandra-dtest/blob/b117565b8f0096a3ed2af05fdec6e014a05788a1/cql_tracing_test.py#L80-L98] at CL.ALL to produce no read repairs. However, running with trace in cqlsh [causes additional queries getting the trace data|https://github.com/apache/cassandra/blob/83e1e9e45193322f18f57aa7cc4ad31d9d5a152d/bin/cqlsh.py#L1044]. They will be run at the session level of CL.ALL, but the data is written at CL.ONE. Therefore these queries can cause RR.

Rather than try to work around this in-place, my suggested change relocates the test out of cql_tracing_tests and uses the driver directly (cqlsh is not the thing we're testing here).

[patch|https://github.com/apache/cassandra-dtest/compare/master...aholmberg:CASSANDRA-14157?expand=1]
[ci|https://app.circleci.com/pipelines/github/aholmberg/cassandra?branch=CASSANDRA-14157] (note that another read_repair_test is failing on trunk for a different reason CASSANDRA-16148);;;","08/Oct/20 13:10;blerer;The patch looks good to me.;;;","08/Oct/20 13:24;blerer;Committed into master at 4b164488735fb13e31b21d7786acb7b477473e2e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[DTEST] [TRUNK] TestTopology.movement_test is flaky; fails assert ""values not within 16.00% of the max: (851.41, 713.26)""",CASSANDRA-14156,13129753,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,mkjellman,mkjellman,10/Jan/18 04:39,16/Apr/19 09:29,13/Jul/23 08:37,27/Jul/18 07:44,,,,,,,Test/dtest/python,,,,0,dtest,,,"DTest* TestTopology.test_movement* is flaky. All of the testing so far (and thus all of the current known observed failures) have been when running against trunk. When the test fails, it always due to the assert_almost_equal assert.

{code}
AssertionError: values not within 16.00% of the max: (851.41, 713.26) ()
{code}

The following CircleCI runs are 2 examples with dtests runs that failed due to this test failing it's assert:
[https://circleci.com/gh/mkjellman/cassandra/487]
[https://circleci.com/gh/mkjellman/cassandra/526]

*p.s.* assert_almost_equal has a comment ""@params error Optional margin of error. Default 0.16"". I don't see any obvious notes for why the default is this magical 16% number. It looks like it was committed as part of a big bulk commit by Sean McCarthy (who I can't find on JIRA). If anyone has any history on the magic 16% allowed delta please share!
",,jasobrown,marcuse,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 27 07:44:22 UTC 2018,,,,,,,,,,,"0|i3oq1z:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Normal,,,,,,,,,,,,,,,,,,,"10/Jan/18 06:08;mkjellman;Failed again just now:
https://circleci.com/gh/mkjellman/cassandra/532

{code}
AssertionError: values not within 16.00% of the max: (870.33, 713.36) ()
{code};;;","12/Jan/18 15:32;marcuse;in 3.11 and trunk we need to run {{relocatesstables}} to move tokens to their correct places, then we need to run a major compaction to make sure that there are no overlapping tokens

https://github.com/krummas/cassandra-dtest/commits/marcuse/14156;;;","18/Jan/18 13:40;marcuse;[~jasobrown] could you review?;;;","26/Jul/18 17:25;jasobrown;I was able to repro the flakey fail after 17 runs on my laptop. With [~krummas]'s patch, it ran 50 times without fail.

The [debug statement|https://github.com/krummas/cassandra-dtest/commit/42e9125c189f61dd31a2fc0a157b95e6ecea4cf1#diff-376c5cd8425c9b4b078f87a20db738b9R300] in the patch should be removed as it fails hard with the following error:

{noformat}
test_movement failed and was not selected for rerun.
	<class 'NameError'>
	name 'debug' is not defined
	[<TracebackEntry /opt/orig/1/opt/dev/cassandra-dtest/topology_test.py:300>]
{noformat}
 
As it's not essential, you can probably remove it. Otherwise, +1.;;;","27/Jul/18 07:44;marcuse;committed as {{2548ec6e664bf118943ea07d070c5bb863f90426}}, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`ant javadoc` task broken due to UTF-8 characters in multiple source files,CASSANDRA-14154,13129255,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jgrassler,jgrassler,jgrassler,08/Jan/18 14:29,15/May/20 08:00,13/Jul/23 08:37,10/Jan/18 13:18,3.0.16,3.11.2,4.0,4.0-alpha1,,,Build,,,,0,,,,"Several source files contain UTF-8 characters in String literals. When building the {{javadoc}} target with ant ({{ant javadoc}}), these will trip up javadoc, which defaults to ASCII encoding. See the {{build.log}} for what I did and the resulting output.

I created a patch that will fix the problem ({{javadoc-encoding.patch}}), which is attached.

I encountered this problem in 3.11.1, but I haven't checked whether other versions are affected as well.","Built on OpenSUSE Tumbleweed. I used the following java packages when building:

{quote}
titan% rpm -qa | grep java
javapackages-local-4.7.0+git20170331.ef4057e7-1.5.x86_64
java-1_8_0-openjdk-devel-1.8.0.151-1.3.x86_64
python3-javapackages-4.7.0+git20170331.ef4057e7-1.5.x86_64
java-1_8_0-openjdk-headless-1.8.0.151-1.3.x86_64
timezone-java-2017c-1.3.noarch
java-1_8_0-openjdk-1.8.0.151-1.3.x86_64
libjavascriptcoregtk-1_0-0-2.4.11-7.6.x86_64
libjavascriptcoregtk-4_0-18-2.18.4-1.1.x86_64
javapackages-tools-4.7.0+git20170331.ef4057e7-1.5.x86_64
{quote}",jasobrown,jgrassler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/18 14:26;jgrassler;build.log;https://issues.apache.org/jira/secure/attachment/12905094/build.log","08/Jan/18 14:27;jgrassler;javadoc-encoding.patch;https://issues.apache.org/jira/secure/attachment/12905093/javadoc-encoding.patch",,,,,,,,,,,,2.0,jgrassler,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,,,Wed Jan 10 13:18:53 UTC 2018,,,,,,,,,,,"0|i3omzj:",9223372036854775807,,,,,,,,,jasobrown,,jasobrown,,,Low,,3.11.1,,,,,,,,,,,,,,,,,"10/Jan/18 13:18;jasobrown;Nice find. +1 and committed as sha {{fde05f4f1b4ad814acf79bed61500aaf2ebe39d6}}.

Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_no_base_column_in_view_pk_complex_timestamp_with_flush - materialized_views_test.TestMaterializedViews frequently fails in CI,CASSANDRA-14148,13128702,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,mkjellman,mkjellman,05/Jan/18 05:41,16/Apr/19 09:29,13/Jul/23 08:37,10/Apr/18 12:11,,,,,,,Legacy/Testing,,,,0,,,,"test_no_base_column_in_view_pk_complex_timestamp_with_flush - materialized_views_test.TestMaterializedViews frequently fails in CI

self = <materialized_views_test.TestMaterializedViews object at 0x7f849b25cf60>

    @since('3.0')
    def test_no_base_column_in_view_pk_complex_timestamp_with_flush(self):
>       self._test_no_base_column_in_view_pk_complex_timestamp(flush=True)

materialized_views_test.py:970: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
materialized_views_test.py:1066: in _test_no_base_column_in_view_pk_complex_timestamp
    assert_one(session, ""SELECT * FROM t"", [1, 1, None, None, None, 1])
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

session = <cassandra.cluster.Session object at 0x7f849b379390>
query = 'SELECT * FROM t', expected = [1, 1, None, None, None, 1], cl = None

    def assert_one(session, query, expected, cl=None):
        """"""
        Assert query returns one row.
        @param session Session to use
        @param query Query to run
        @param expected Expected results from query
        @param cl Optional Consistency Level setting. Default ONE
    
        Examples:
        assert_one(session, ""LIST USERS"", ['cassandra', True])
        assert_one(session, query, [0, 0])
        """"""
        simple_query = SimpleStatement(query, consistency_level=cl)
        res = session.execute(simple_query)
        list_res = _rows_to_list(res)
>       assert list_res == [expected], ""Expected {} from {}, but got {}"".format([expected], query, list_res)
E       AssertionError: Expected [[1, 1, None, None, None, 1]] from SELECT * FROM t, but got []",,jasonstack,marcuse,mkjellman,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 10 12:11:25 UTC 2018,,,,,,,,,,,"0|i3ojmf:",9223372036854775807,,,,,,,,,pauloricardomg,,pauloricardomg,,,Normal,,,,,,,,,,,,,,,,,,,"18/Jan/18 15:30;marcuse;so, seems self.update_view(...) can take more than 10s in some cases - this means the ttl expires and the query does not return any data.

https://github.com/krummas/cassandra-dtest/commits/marcuse/14148;;;","06/Apr/18 13:41;pauloricardomg;sorry for the delay. fix LGTM, executed multiplexer run and this seems to fix flakiness. Marking as ready to commit, thanks!;;;","10/Apr/18 12:11;marcuse;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[DTEST] cdc_test::TestCDC::test_insertion_and_commitlog_behavior_after_reaching_cdc_total_space assertion always fails (Extra items in the left set),CASSANDRA-14146,13128387,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,mkjellman,mkjellman,04/Jan/18 02:08,16/Apr/19 09:29,13/Jul/23 08:37,08/Jun/18 22:19,,,,,,,Test/dtest/python,,,,0,,,,"Dtest cdc_test::TestCDC::test_insertion_and_commitlog_behavior_after_reaching_cdc_total_space always fails on an assertion.

the assert is the final step of the test and it checks that pre_non_cdc_write_cdc_raw_segments == _get_cdc_raw_files(node.get_path())

This fails 100% of the time locally, 100% of the time on circleci executed under pytest, and 100% of the time for the past 40 test runs on ASF Jenkins runs against trunk.

This is the only test failure (excluding flaky one-off failures) remaining on the pytest dtest branch. I'm going to annotate the test with a skip marker (including a reason reference to this JIRA)... when it's fixed we should also remove the skip annotation from the test.

{code}
>       assert pre_non_cdc_write_cdc_raw_segments == _get_cdc_raw_files(node.get_path())
E       AssertionError: assert {'/tmp/dtest-...169.log', ...} == {'/tmp/dtest-v...169.log', ...}
E         Extra items in the left set:
E         '/tmp/dtest-vrn4k8ov/test/node1/cdc_raw/CommitLog-7-1515030005097.log'
E         '/tmp/dtest-vrn4k8ov/test/node1/cdc_raw/CommitLog-7-1515030005098.log'
E         Extra items in the right set:
E         '/tmp/dtest-vrn4k8ov/test/node1/cdc_raw/CommitLog-7-1515030005099.log'
E         '/tmp/dtest-vrn4k8ov/test/node1/cdc_raw/CommitLog-7-1515030005100.log'
E         Use -v to get the full diff
{code}",,jay.zhuang,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Jan 06 19:25:39 UTC 2018,,,,,,,,,,,"0|i3ohof:",9223372036854775807,,,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"06/Jan/18 19:25;jay.zhuang;https://github.com/apache/cassandra-dtest/pull/6 fixes the issue.
cc [~JoshuaMcKenzie];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
