Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Outward issue link (Cloners),Inward issue link (Completes),Outward issue link (Completes),Outward issue link (Completes),Outward issue link (Completes),Outward issue link (Completes),Inward issue link (Container),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Supercedes),Inward issue link (Testing),Inward issue link (Testing),Outward issue link (Testing),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Job may be stuck in upgrade loop when last-state fallback is disabled and deployment is missing,FLINK-30528,13515855,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,28/Dec/22 14:58,01/Jan/23 16:17,13/Jul/23 08:13,01/Jan/23 16:17,kubernetes-operator-1.3.0,,,,,,,kubernetes-operator-1.3.1,kubernetes-operator-1.4.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"When last-state upgrade fallback is disabled (or we are switching flink versions) and the JM deployment is missing for some reason, we get stuck in the upgrade loop as the spec change /upgrade logic always takes precedence over the JM deployment recovery logic.

In this cases the JM deployment need to be recovered first so savepoint upgrade can be executed.",,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 01 16:17:22 UTC 2023,,,,,,,,,,"0|z1ef8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jan/23 16:17;gyfora;release-1.3 c4e691696086358dfa981e35d9a799e7f31b1784
main a1842d4c0170feb008293963ec51c0343f42771d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Last-state suspend followed by flinkVersion change may lead to state loss,FLINK-30527,13515854,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,28/Dec/22 14:56,01/Jan/23 16:16,13/Jul/23 08:13,01/Jan/23 16:16,kubernetes-operator-1.2.0,kubernetes-operator-1.3.0,,,,,,kubernetes-operator-1.3.1,kubernetes-operator-1.4.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"We do not check flink version changes on recovery, so if the user suspended the job using last-state mode (or that was the only mode available) and then subsequently the flinkVersion is changed to a non-HA compatible version, the job would be restored using last-state and state would be lost.

In these cases we should set an error in the Flink resource instructing the user that changing version is not allowed at that point.",,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 01 16:16:52 UTC 2023,,,,,,,,,,"0|z1ef8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jan/23 16:16;gyfora;main 0dd9e9c257b1e4d7659d8c2cb367a35e7736c7a1
release-1.3 7f65de716cb7a2d6ac158cd01f1ee2923b634795;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot open jobmanager configuration web page,FLINK-30525,13515839,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huwh,huwh,huwh,28/Dec/22 12:37,13/Jan/23 15:08,13/Jul/23 08:13,13/Jan/23 15:08,1.16.1,,,,,,,1.16.1,1.17.0,,,,Runtime / Web Frontend,,,,,,,0,pull-request-available,,,,"we remove the environments in rest api in https://issues.apache.org/jira/browse/FLINK-30116.
The jobmanager.configuration web page will throw ""TypeError: Cannot read properties of undefined (reading 'length')"" 

the environment in jobmanager.configuration web page should be delete too.
 !image-2022-12-28-20-37-00-825.png! 
 !image-2022-12-28-20-37-05-551.png! ",,begginghard,huwh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Dec/22 12:37;huwh;image-2022-12-28-20-37-00-825.png;https://issues.apache.org/jira/secure/attachment/13054194/image-2022-12-28-20-37-00-825.png","28/Dec/22 12:37;huwh;image-2022-12-28-20-37-05-551.png;https://issues.apache.org/jira/secure/attachment/13054193/image-2022-12-28-20-37-05-551.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 13 15:08:26 UTC 2023,,,,,,,,,,"0|z1ef5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Dec/22 12:42;huwh;[~chesnay] looking forward to your opinions on it. ;;;","09/Jan/23 07:52;huwh;[~renqs] Can you take a look at this? This will affect the release of 1.17;;;","13/Jan/23 15:08;chesnay;master: 560b4612735a2b9cd3b5db88adf5cb223e85535b
1.16: 8283dfcf69f36398db0aa1767c8a0d1ad78364a7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`SHOW TBLPROPERTIES` can't read properties of table in Spark3,FLINK-30522,13515823,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,28/Dec/22 10:58,04/Jan/23 01:45,13/Jul/23 08:13,04/Jan/23 01:45,table-store-0.4.0,,,,,,,table-store-0.3.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,,,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Dec/22 11:00;yzl;1.png;https://issues.apache.org/jira/secure/attachment/13054189/1.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 04 01:45:51 UTC 2023,,,,,,,,,,"0|z1ef1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/23 01:45;lzljs3620320;master: 545617a6fcf7eefd0a3f19d7adac431074891367
release-0.3: eeb09ca128553d77e9f8886b335e6048cd498179
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[flink-operator] Kubernetes HA Service not working with standalone mode,FLINK-30518,13515810,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,usamj,tbnguyen1407,tbnguyen1407,28/Dec/22 08:43,18/Jun/23 15:29,13/Jul/23 08:13,03/Feb/23 13:39,kubernetes-operator-1.3.0,,,,,,,kubernetes-operator-1.4.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"-Since flink-conf.yaml is mounted as read-only configmap, the /docker-entrypoint.sh script is not able to inject correct Pod IP to `jobmanager.rpc.address`. This leads to same address (e.g flink.ns-ext) being set for all Job Manager pods. This causes:-
Setting up FlinkDeployment in Standalone mode with Kubernetes HA Service. Problems:

(1) flink-cluster-config-map always contains wrong address for all 3 component leaders (see screenshot, should be pod IP instead of clusterIP service name)

(2) Accessing Web UI when jobmanager.replicas > 1 is not possible with error
{code:java}
{""errors"":[""Service temporarily unavailable due to an ongoing leader election. Please refresh.""]} {code}
 

~ flinkdeployment.yaml ~
{code:java}
spec:
  mode: standalone
  flinkConfiguration:
    high-availability: kubernetes
    high-availability.storageDir: ""file:///opt/flink/storage""
    ...
  jobManager:
    replicas: 3
  ... {code}",,bgeng777,gyfora,rmetzger,tbnguyen1407,usamj,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31187,,,,,,"28/Dec/22 08:40;tbnguyen1407;flink-configmap.png;https://issues.apache.org/jira/secure/attachment/13054176/flink-configmap.png","28/Dec/22 14:51;bgeng777;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13054196/screenshot-1.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 03 13:39:13 UTC 2023,,,,,,,,,,"0|z1eeyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Dec/22 09:04;gyfora;This may be related to:

https://issues.apache.org/jira/browse/FLINK-28554
https://issues.apache.org/jira/browse/FLINK-30329;;;","28/Dec/22 09:39;tbnguyen1407;[~gyfora] I checked. Both are not related. I think this is more about the design of having flink-conf mounted as configmap, which is always read-only.

 

A possible solution is for the Operator to mount flink-conf.yaml elsewhere and copy it to /opt/flink/conf/ to make it writable.;;;","28/Dec/22 14:07;gyfora;You are right [~tbnguyen1407] , those issues are not related, my mistake.

However the operator does not control how the flink-conf for jobs are mounted. We simply use the Flink clients to deploy the cluster, which mounts the configmap as part of the deployment. So this is not something that we can fix in the operator. 

cc [~bgeng777] [~yangwang166] 

I don't remember this being a problem earlier. Is it possible that the Kubernetes HA with native mode doesnt support multiple JM replicas?;;;","28/Dec/22 14:56;bgeng777;I think [~gyfora] is right that this is not a problem before.
I just tried an older version(1.1.0) of the operator with setting replicas to 2 for jobManager and it works fine on running jobs and accessing web ui. 
I checked my configmap(`basic-checkpoint-ha-example-cluster-config-map`) and it seems that leader.dispatcher is set correctly.
 !screenshot-1.png! 

I will try to reproduce this problem with the image of the operator based on the latest main branch as the [PR|https://github.com/apache/flink-kubernetes-operator/pull/494] to remove read-only limt is merged.;;;","28/Dec/22 15:00;gyfora;[~bgeng777] I think this is not related to any operator version as the other issue you are referring to is about the operators own flink-conf yaml, not about the deployed clusters. So this should work in all operator versions the same way.;;;","28/Dec/22 15:19;bgeng777;[~gyfora] I see. Thanks for the information. I misunderstood the problem somehow :(
But I just tried 1.3.0 operator and 1.16 flink to run basic-checkpoint-ha-example with setting replicas of JM to 3. It works fine as well.
[~tbnguyen1407] would you mind sharing the full deployment yaml for this problem?       
 ;;;","28/Dec/22 15:40;tbnguyen1407;[~bgeng777] Thank you. I found the problem when comparing with the basic-checkpoint-ha-example. I was using deployment mode ""standalone"". Switching to ""native"" the configmap is correctly populated.

My understanding about the difference between the 2 modes is how TaskManager pods are spawned. Not sure if using Kubernetes HA service in ""standalone"" mode is supported or not.

Notes: Our current helm chart sets up ""standalone"" cluster (JM and TM are predefined deployments) without the Operator and is using Kubernetes HA Service so I think Operator also should have support for it.;;;","28/Dec/22 15:54;gyfora;Strange, the standalone and native integration mode should work in the same way with regards to creating and mounting configmaps as they use the same decorators.;;;","28/Dec/22 16:01;tbnguyen1407;[~gyfora] I think my assumption is wrong about the flink-conf configmap as even with native mode the rpc.address is not set in flink-conf.yaml (file is read-only) but the advertised address is correct in cluster configmap. May be due to something else. You can try with the basic-checkpoint-ha-example and switch the mode to ""standalone"".;;;","28/Dec/22 16:07;gyfora;But can you clarify what doesn't work with multiple replicas? Because with 1 replica it should work.
With 2 replicas?
Does the job not start? Cannot fail over?;;;","28/Dec/22 16:10;tbnguyen1407;[~gyfora] I did not test with jobs yet. First problem is not able to access the Web UI (either ingress or clusterIP service) when there is more than 1 replica.

Also with wrong advertised addresses in cluster configmap, I think it will definitely affect leader election.;;;","28/Dec/22 16:22;gyfora;Please test this with replicas 1 and 2 and see what happens whether jobs can start etc. So we know what the exact problem is :) 
We have e2e tests set up with 1 replica so I am pretty sure that works. (also thats the most common setup);;;","12/Jan/23 09:40;wangyang0918;Really sorry for the late response. For native K8s implementation with HA enabled, we always override the jobmanager.rpc.address to pod IP. So for standalone mode with HA, we also need to do this in the operator.

You could find the similar logic in the example yaml for standalone mode.
{code:java}
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        # The following args overwrite the value of jobmanager.rpc.address configured in the configuration config map to POD_IP.
        args: [""jobmanager"", ""$(POD_IP)""] {code};;;","12/Jan/23 09:44;gyfora;[~usamj] could you please take a look?;;;","30/Jan/23 09:08;usamj;Yeah I'm looking into a fix;;;","03/Feb/23 13:39;gyfora;merged to main 87eb5c6f1855ef8fc1400039ce2517fc718f2f01;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL state TTL has no effect when using Interval Join,FLINK-30512,13515738,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,wangkang,wangkang,27/Dec/22 11:58,06/Jan/23 03:43,13/Jul/23 08:13,03/Jan/23 09:21,1.15.1,1.16.1,,,,,,,,,,,Runtime / State Backends,Table SQL / Runtime,,,,,,0,,,,,"Take the following join SQL program as an example:
{code:java}
SET 'table.exec.state.ttl' = '900000 ms';
select 
...
from kafka_source_dwdexpose as t1
left join kafka_source_expose_attr_click t3 
ON t1.mid = t3.mid and t1.sr = t3.sr 
and t1.time_local = t3.time_local 
and t1.log_ltz BETWEEN t3.log_ltz - INTERVAL '2' MINUTE  AND t3.log_ltz + INTERVAL '2' MINUTE {code}
!flink1.16.png|width=906,height=278!

the state size is getting bigger and bigger.

we also test the same sql with flink sql 1.13,the state size is stable.",,begginghard,wangkang,Yanfei Lei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Dec/22 11:52;wangkang;flink1.16.png;https://issues.apache.org/jira/secure/attachment/13054144/flink1.16.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 06 03:43:09 UTC 2023,,,,,,,,,,"0|z1eeiw:",9223372036854775807,"add param : set table.exec.source.idle-timeout=120s; the issue fixed.",,,,,,,,,,,,,,,,,,,"03/Jan/23 09:21;wangkang;add param : set table.exec.source.idle-timeout=120s; the issue fixed.;;;","06/Jan/23 03:43;Yanfei Lei;Thanks for reporting this issue and sharing the solution.

From your screenshot, the job ran for 45min, your ttl was set to 15min and the checkpoint interval is 5min. The full checkpoint size of chk-14 and chk-18 is smaller than the previous one, so I think TTL is in effect. 

> the state size is getting bigger and bigger.
I guess it is because the TPS of your job is relatively high during this period, new states grow faster than old states expire.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CliClientITCase.testSqlStatements failed with output not matched with expected,FLINK-30508,13515706,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,renqs,renqs,27/Dec/22 07:27,14/Feb/23 02:06,13/Jul/23 08:13,09/Feb/23 03:30,1.16.0,1.17.0,,,,,,1.16.2,1.17.0,,,,Table SQL / Client,,,,,,,0,test-stability,,,,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44246&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=14992,,fsk119,leonard,lsy,luoyuxia,mapohl,renqs,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30640,,,,,,FLINK-30640,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 14 02:06:25 UTC 2023,,,,,,,,,,"0|z1eebs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/22 07:27;renqs;[~fsk119] any idea on this one?;;;","27/Dec/22 07:30;renqs;Other instances not linked correctly: 

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43772&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=14992]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43917&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=14190]

 ;;;","04/Jan/23 12:16;fsk119;Thanks for reporting this issue. It seems the job has been cleaned up before the client fetch results... Pleaes take a look at the following logs

 
{code:java}
04:10:04,314 [flink-akka.actor.default-dispatcher-13] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job 'collect' (17922fda0de3fe8d5d4d82f496fd8480).
04:10:04,317 [flink-akka.actor.default-dispatcher-12] ERROR org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler [] - Unhandled exception.
org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (17922fda0de3fe8d5d4d82f496fd8480)
    at org.apache.flink.runtime.dispatcher.Dispatcher.getJobMasterGateway(Dispatcher.java:1212) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.runtime.dispatcher.Dispatcher.performOperationOnJobMasterGateway(Dispatcher.java:1227) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.runtime.dispatcher.Dispatcher.deliverCoordinationRequestToCoordinator(Dispatcher.java:985) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[?:?]
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[?:?]
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[?:?]
    at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[?:?]
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[?:?]
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]
    at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[scala-library-2.12.7.jar:?]
    at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[scala-library-2.12.7.jar:?]
    at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[scala-library-2.12.7.jar:?]
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[scala-library-2.12.7.jar:?]
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[scala-library-2.12.7.jar:?]
    at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]
    at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]
    at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) ~[?:?]
    at akka.actor.ActorCell.invoke(ActorCell.scala:548) ~[?:?]
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]
    at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]
    at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
    at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
04:10:04,325 [flink-akka.actor.default-dispatcher-13] INFO  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down
04:10:04,325 [flink-akka.actor.default-dispatcher-13] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Disconnect TaskExecutor f8383d25-f1de-44e9-8c1c-98a7465bc349 because: Stopping JobMaster for job 'collect' (17922fda0de3fe8d5d4d82f496fd8480).
04:10:04,325 [flink-akka.actor.default-dispatcher-13] INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [037b6db35609cc3bef4112cb2b2c2301].
04:10:04,325 [flink-akka.actor.default-dispatcher-13] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection f91ca618e2bf66e6cd04e4537ba14346: Stopping JobMaster for job 'collect' (17922fda0de3fe8d5d4d82f496fd8480).
04:10:04,325 [flink-akka.actor.default-dispatcher-13] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:3, state:ACTIVE, resource profile: ResourceProfile{taskHeapMemory=256.000gb (274877906944 bytes), taskOffHeapMemory=256.000gb (274877906944 bytes), managedMemory=20.000mb (20971520 bytes), networkMemory=16.000mb (16777216 bytes)}, allocationId: 037b6db35609cc3bef4112cb2b2c2301, jobId: 17922fda0de3fe8d5d4d82f496fd8480).
04:10:04,326 [flink-akka.actor.default-dispatcher-13] INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job 17922fda0de3fe8d5d4d82f496fd8480 from job leader monitoring.
04:10:04,326 [flink-akka.actor.default-dispatcher-13] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job 17922fda0de3fe8d5d4d82f496fd8480.
04:10:04,331 [flink-akka.actor.default-dispatcher-13] INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager 9234f8e5984b2c41461d696253fe4725@akka://flink/user/rpc/jobmanager_10 for job 17922fda0de3fe8d5d4d82f496fd8480 from the resource manager.
04:10:04,334 [          Thread-164] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Closing SourceCoordinator for source Source: foo[57].
04:10:04,335 [          Thread-164] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source coordinator for source Source: foo[57] closed.
04:10:04,336 [          Thread-157] WARN  org.apache.flink.streaming.api.operators.collect.CollectResultFetcher [] - An exception occurred when fetching query results
java.util.concurrent.ExecutionException: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (17922fda0de3fe8d5d4d82f496fd8480)
    at org.apache.flink.runtime.dispatcher.Dispatcher.getJobMasterGateway(Dispatcher.java:1212)
    at org.apache.flink.runtime.dispatcher.Dispatcher.performOperationOnJobMasterGateway(Dispatcher.java:1227)
    at org.apache.flink.runtime.dispatcher.Dispatcher.deliverCoordinationRequestToCoordinator(Dispatcher.java:985)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
    at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
    at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
    at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
    at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at akka.actor.Actor.aroundReceive(Actor.scala:537)
    at akka.actor.Actor.aroundReceive$(Actor.scala:535)
    at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
    at akka.actor.ActorCell.invoke(ActorCell.scala:548)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
    at akka.dispatch.Mailbox.run(Mailbox.scala:231)
    at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
    at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)End of exception on server side>]
    at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_292]
    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) ~[?:1.8.0_292]
    at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sendRequest(CollectResultFetcher.java:170) ~[flink-streaming-java-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:129) [flink-streaming-java-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106) [flink-streaming-java-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80) [flink-streaming-java-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222) [flink-table-planner_2.12-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.gateway.local.result.CollectResultBase$ResultRetrievalThread.run(CollectResultBase.java:75) [classes/:?]
Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (17922fda0de3fe8d5d4d82f496fd8480)
    at org.apache.flink.runtime.dispatcher.Dispatcher.getJobMasterGateway(Dispatcher.java:1212)
    at org.apache.flink.runtime.dispatcher.Dispatcher.performOperationOnJobMasterGateway(Dispatcher.java:1227)
    at org.apache.flink.runtime.dispatcher.Dispatcher.deliverCoordinationRequestToCoordinator(Dispatcher.java:985)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
    at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
    at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
    at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
    at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at akka.actor.Actor.aroundReceive(Actor.scala:537)
    at akka.actor.Actor.aroundReceive$(Actor.scala:535)
    at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
    at akka.actor.ActorCell.invoke(ActorCell.scala:548)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
    at akka.dispatch.Mailbox.run(Mailbox.scala:231)
    at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
    at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)End of exception on server side>]
    at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:534) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:514) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966) ~[?:1.8.0_292]
    at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940) ~[?:1.8.0_292]
    at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) ~[?:1.8.0_292]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_292]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_292]
    at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292]
04:10:04,437 [          Thread-157] INFO  org.apache.flink.configuration.Configuration                 [] - Config uses fallback configuration key 'jobmanager.rpc.address' instead of key 'rest.address'
04:10:04,449 [          Thread-157] INFO  org.apache.flink.configuration.Configuration                 [] - Config uses fallback configuration key 'jobmanager.rpc.address' instead of key 'rest.address'
04:10:04,467 [                main] INFO  org.apache.flink.table.resource.ResourceManager              [] - Added jar resource [file:/tmp/junit3065787421766710063/test-jar/test-classloader-udf.jar] to class path.
04:10:04,467 [                main] INFO  org.apache.flink.table.resource.ResourceManager              [] - Register resource [/tmp/junit3065787421766710063/test-jar/test-classloader-udf.jar] successfully.
04:10:04,498 [                main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore               [] - 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
04:10:04,499 [                main] INFO  org.apache.hadoop.hive.metastore.ObjectStore                 [] - ObjectStore, initialize called
04:10:04,504 [                main] INFO  org.apache.hadoop.hive.metastore.MetaStoreDirectSql          [] - Using direct SQL, underlying DB is DERBY
04:10:04,504 [                main] INFO  org.apache.hadoop.hive.metastore.ObjectStore                 [] - Initialized ObjectStore
04:10:04,600 [                main] INFO  org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner [] - Starting generating logical plan
04:10:04,601 [                main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore               [] - 0: get_function: default.hive_add_one
04:10:04,601 [                main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit         [] - ugi=vsts_azpcontainer    ip=unknown-ip-addr    cmd=get_function: default.hive_add_one    
04:10:04,613 [                main] INFO  org.apache.flink.connectors.hive.HiveTableFactory            [] - Successfully loaded Hive udf 'hive_add_one' with class 'HiveAddOneFunc'
04:10:04,613 [                main] INFO  org.apache.flink.connectors.hive.HiveTableFactory            [] - Transforming Hive function 'hive_add_one' into a HiveSimpleUDF
04:10:04,615 [                main] INFO  org.apache.flink.table.functions.hive.HiveSimpleUDF          [] - Creating HiveSimpleUDF from 'HiveAddOneFunc'
04:10:04,616 [                main] INFO  org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer [] - Completed phase 1 of Semantic Analysis
04:10:04,616 [                main] INFO  org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer [] - Get metadata for source tables
04:10:04,616 [                main] INFO  org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer [] - Get metadata for subqueries
04:10:04,616 [                main] INFO  org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer [] - Get metadata for destination tables
04:10:04,616 [                main] INFO  org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer [] - Completed getting MetaData in Semantic Analysis
04:10:04,617 [                main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore               [] - 0: get_function: default.hive_add_one
04:10:04,617 [                main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit         [] - ugi=vsts_azpcontainer    ip=unknown-ip-addr    cmd=get_function: default.hive_add_one {code};;;","09/Jan/23 07:55;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44552&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=13186;;;","09/Jan/23 10:37;mapohl;[~fsk119] will you be able to look into it a come up with a fix for this test stability? May I assign this task to you?;;;","10/Jan/23 10:53;mapohl;This is a 1.16 build:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44635&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=15012;;;","16/Jan/23 07:27;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44824&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=15722;;;","16/Jan/23 09:50;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44856&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=15204;;;","17/Jan/23 07:14;fsk119;Sorry for the late response. I will take a look.;;;","23/Jan/23 08:29;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45148&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=13534;;;","06/Feb/23 07:15;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45707&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=14051;;;","06/Feb/23 07:15;mapohl;Any updates on that one [~fsk119]?;;;","06/Feb/23 08:08;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45727&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=13572;;;","07/Feb/23 11:50;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45828&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=46497;;;","08/Feb/23 06:22;fsk119;I read recently failed tests are related to the CTAS syntax. It seems the query doesn't wait for the last INSERT INTO statement to finish. Could you share some thoughts about Hive behavior [~lsy] [~luoyuxia];;;","08/Feb/23 06:34;luoyuxia;Hi, the reason maybe ctas statement write data to files and rename files to commit {_}asynchronously{_}, then the following statemt try to read the files written, but the ctas statement may rename these files at same time, so the exception ""can not access file"" as them have been renamed.

[~lsy] Could you please help fix as it's introuced by your changes?;;;","08/Feb/23 09:21;lsy;I have open a PR in another issue.;;;","09/Feb/23 06:20;mapohl;[~lsy] With ""another issue"" you mean FLINK-30640?;;;","10/Feb/23 02:33;fsk119;[~mapohl] you are right. But we haven't cherry pick the fix to the 1.16 yet.;;;","10/Feb/23 06:40;mapohl;Then, the Jira issue management is a bit confusing here: Either, we open this one again to do the 1.16 backport here or (the one I would suggest doing) open FLINK-30640 again, add 1.16.1 as an affected version and provide the backport for it;;;","13/Feb/23 14:58;mapohl;[~fsk119] can you give a bit of guidance here? It looks like this one is fixed by FLINK-30640. But there is no 1.16 backport provided, yet.;;;","14/Feb/23 02:06;fsk119;Sorry for the late response. I have cherry-picked the commit to the 1.16 right now.  Because this is only a test issue, I think either way is fine. I have added 1.16.2 to the fixed versions.

The merged commit in 1.16 is 0994832be8a121273be423de05203390b7bdece4

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix UnsupportedFileSystemSchemeException when writing Table Store on OSS with other engines,FLINK-30504,13515484,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,26/Dec/22 09:06,27/Dec/22 02:14,13/Jul/23 08:13,27/Dec/22 02:14,table-store-0.3.0,table-store-0.4.0,,,,,,table-store-0.3.0,table-store-0.4.0,,,,Table Store,,,,,,,0,pull-request-available,,,,"Currently when writing Table Store tables on OSS with other engines (for example Spark), the following exception will occur.

{code}
22/12/23 17:54:12 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3) (core-1-1.c-c9f1b761c8946269.cn-huhehaote.emr.aliyuncs.com executor 2): java.lang.RuntimeException: Failed to find latest snapshot id
  at org.apache.flink.table.store.file.utils.SnapshotManager.latestSnapshotId(SnapshotManager.java:81)
  at org.apache.flink.table.store.file.operation.AbstractFileStoreWrite.scanExistingFileMetas(AbstractFileStoreWrite.java:87)
  at org.apache.flink.table.store.file.operation.KeyValueFileStoreWrite.createWriter(KeyValueFileStoreWrite.java:113)
  at org.apache.flink.table.store.file.operation.AbstractFileStoreWrite.createWriter(AbstractFileStoreWrite.java:227)
  at org.apache.flink.table.store.file.operation.AbstractFileStoreWrite.lambda$getWriter$1(AbstractFileStoreWrite.java:217)
  at java.util.HashMap.computeIfAbsent(HashMap.java:1128)
  at org.apache.flink.table.store.file.operation.AbstractFileStoreWrite.getWriter(AbstractFileStoreWrite.java:217)
  at org.apache.flink.table.store.file.operation.AbstractFileStoreWrite.write(AbstractFileStoreWrite.java:106)
  at org.apache.flink.table.store.table.sink.TableWriteImpl.write(TableWriteImpl.java:63)
  at org.apache.flink.table.store.spark.SparkWrite$WriteRecords.call(SparkWrite.java:124)
  at org.apache.flink.table.store.spark.SparkWrite$WriteRecords.call(SparkWrite.java:105)
  at org.apache.spark.api.java.JavaPairRDD$.$anonfun$toScalaFunction$1(JavaPairRDD.scala:1070)
  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)
  at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
  at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
  at scala.collection.Iterator.foreach(Iterator.scala:943)
  at scala.collection.Iterator.foreach$(Iterator.scala:943)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  at scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:237)
  at scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:220)
  at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1431)
  at org.apache.spark.rdd.RDD.$anonfun$reduce$2(RDD.scala:1097)
  at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  at org.apache.spark.scheduler.Task.run(Task.scala:136)
  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Could not find a file system implementation for scheme 'oss'. The scheme is directly supported by Flink through the following plugin(s): flink-oss-fs-hadoop. Please ensure that each plugin resides within its own subfolder within the plugins directory. See https://nightlies.apache.org/flink/flink-docs-stable/docs/deployment/filesystems/plugins/ for more information. If you want to use a Hadoop file system for that scheme, please add the scheme to the configuration fs.allowed-fallback-filesystems. For a full list of supported file systems, please see https://nightlies.apache.org/flink/flink-docs-stable/ops/filesystems/.
  at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:515)
  at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:409)
  at org.apache.flink.core.fs.Path.getFileSystem(Path.java:274)
  at org.apache.flink.table.store.file.utils.SnapshotManager.findLatest(SnapshotManager.java:164)
  at org.apache.flink.table.store.file.utils.SnapshotManager.latestSnapshotId(SnapshotManager.java:79)
  ... 30 more
{code}",,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 27 02:14:19 UTC 2022,,,,,,,,,,"0|z1ecyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/22 02:14;TsReaper;master: 929c1110bdbf521ad0a9eb09a65d33f76a2b5990
release-0.3: fe8de4c32c148bb87f5a40649ca2373e88f321d8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-sql-connector-pulsar doesn't shade all dependencies,FLINK-30489,13515281,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,bayard,bayard,22/Dec/22 21:19,13/Feb/23 13:42,13/Jul/23 08:13,13/Feb/23 13:42,1.16.0,,,,,,,pulsar-4.0.0,,,,,Connectors / Pulsar,,,,,,,0,pull-request-available,,,,"Looking at [https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-pulsar/1.16.0/flink-sql-connector-pulsar-1.16.0.jar] I'm seeing that some dependencies are shaded (com.fasterxml, com.yahoo etc), but others are not (org.sfl4j, org.bouncycastel, com.scurrilous, ...) and will presumably clash with other jar files.

Additionally, this bundling is going on in the '.jar' file rather than in a more clearly indicated separate -bundle or -shaded jar file. 

As a jar file this seems confusing and potentially bug inducing; though I note I'm just a review of the jar and not Flink experienced.",,bayard,martijnvisser,syhily,tison,,,,,,,,,,,,,,,,,,,,,FLINK-30606,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 13:42:59 UTC 2023,,,,,,,,,,"0|z1ebq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Dec/22 12:14;martijnvisser;[~syhily] WDYT?;;;","30/Dec/22 01:03;syhily;We didn't shade these dependencies because these dependencies are not always required for Pulsar connector. Such as {{org.bouncycastel}}, you will need it only if you use the end-to-end encryption.

But we can truly shade all the dependencies.

{{org.sfl4j}} dependency will be removed in the next Pulsar 2.11 release. So no need to shade it.;;;","13/Feb/23 07:51;syhily;[~tison] Can you assign this issue to me?;;;","13/Feb/23 13:42;tison;master via 5f0bb2db9f1f357a11cce965eb5832bb908523a1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not properly blocking retries when timeout occurs in AsyncWaitOperator,FLINK-30477,13515094,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,21/Dec/22 13:24,02/Feb/23 07:54,13/Jul/23 08:13,31/Jan/23 03:00,1.16.0,,,,,,,1.16.2,1.17.0,,,,API / DataStream,,,,,,,0,pull-request-available,,,,"as user reported in ml https://lists.apache.org/thread/n1rqml8h9j8zkhxwc48rdvj7jrw2rjcy
there's issue in AsyncWaitOperator that it not properly blocking retries when timeout occurs

this happens when a retry timer is unfired and then the user function timeout was triggered first, the current RetryableResultHandlerDelegator doesn't take the timeout process properly and will cause more unexpected retries.",,lincoln.86xy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 07:54:32 UTC 2023,,,,,,,,,,"0|z1eam0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 03:00;lincoln.86xy;fixed in master: 0aab2cd369e700f3ea4c753f31c70802e9017ffc;;;","02/Feb/23 07:54;lincoln.86xy;fixed in 1.16: 8ee93dabf446b28795de431c376b03d41500b634;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The SortOrder of BusyRatio should be descend by default,FLINK-30468,13515024,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,21/Dec/22 05:15,30/Dec/22 11:20,13/Jul/23 08:13,30/Dec/22 07:53,,,,,,,,1.17.0,,,,,Runtime / Web Frontend,,,,,,,0,pull-request-available,,,,"Currently, the sort order is ascend by default, it should be descend.

The most busy subtask should be displayed on top.",,fanrui,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29998,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 30 07:53:30 UTC 2022,,,,,,,,,,"0|z1ea6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Dec/22 07:53;yunta;merged in master: 4b1d8f65f6b19ed05f5d850f2bbd1c1d0315b801;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metrics are collected in stabilization phase and do not always span a full metric window,FLINK-30464,13514892,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,20/Dec/22 12:05,03/Jan/23 17:37,13/Jul/23 08:13,03/Jan/23 17:37,kubernetes-operator-1.4.0,,,,,,,kubernetes-operator-1.4.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"The current code collects metrics as soon as the job goes into RUNNING mode, regardless of whether we are still in the stabilization period. This means the first metric collection window after rescaling will include metrics from the stabilization period which is not desired. The stabilization period is supposed to allow the job to stabilize and shouldn't be used to make new scaling decisions.

Further, the collected metrics for the scaling decider initially do not span a full metric window. Only after the first metric window is complete, we are guaranteed to have a full windows available. This leads to short-sighted scaling decisions.",,mxm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30463,,FLINK-30510,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-20 12:05:58.0,,,,,,,,,,"0|z1e9gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stabilization period starts before job goes into RUNNING ,FLINK-30463,13514891,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,20/Dec/22 11:57,23/Dec/22 17:08,13/Jul/23 08:13,23/Dec/22 17:08,kubernetes-operator-1.4.0,,,,,,,kubernetes-operator-1.4.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"We are tracking the stabilization period from the job start time which can greatly vary from the time the job goes into RUNNING state. This can result in no stabilization period at all leading to poor scaling decisions.

Thus, we should change the logic to use the update time in combination with a RUNNING job status.",,mxm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30464,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-20 11:57:22.0,,,,,,,,,,"0|z1e9go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some rocksdb sst files will remain forever,FLINK-30461,13514873,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,20/Dec/22 11:16,07/Feb/23 03:22,13/Jul/23 08:13,07/Feb/23 03:22,1.15.3,1.16.0,1.17.0,,,,,1.15.4,1.16.2,1.17.0,,,Runtime / Checkpointing,Runtime / State Backends,,,,,,0,pull-request-available,,,,"In rocksdb incremental checkpoint mode, during file upload, if some files have been uploaded and some files have not been uploaded, the checkpoint is canceled due to checkpoint timeout at this time, and the uploaded files will remain.

 
h2. Impact: 

The shared directory of a flink job has more than 1 million files. It exceeded the hdfs upper limit, causing new files not to be written.

However only 50k files are available, the other 950k files should be cleaned up.

!https://user-images.githubusercontent.com/38427477/207588272-dda7ba69-c84c-4372-aeb4-c54657b9b956.png|width=1962,height=364!
h2. Root cause:

If an exception is thrown during the checkpoint async phase, flink will clean up metaStateHandle, miscFiles and sstFiles.

However, when all sst files are uploaded, they are added together to sstFiles. If some sst files have been uploaded and some sst files are still being uploaded, and  the checkpoint is canceled due to checkpoint timeout at this time, all sst files will not be added to sstFiles. The uploaded sst will remain on hdfs.

[code link|https://github.com/apache/flink/blob/49146cdec41467445de5fc81f100585142728bdf/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/snapshot/RocksIncrementalSnapshotStrategy.java#L328]
h2. Solution:

Using the CloseableRegistry as the tmpResourcesRegistry. If the async phase is failed, the tmpResourcesRegistry will cleanup these temporary resources.

 

POC code:

[https://github.com/1996fanrui/flink/commit/86a456b2bbdad6c032bf8e0bff71c4824abb3ce1]

 

 

!image-2022-12-20-18-45-32-948.png|width=1114,height=442!

!image-2022-12-20-18-47-42-385.png|width=1332,height=552!

 ",,begginghard,fanrui,mapohl,pnowojski,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30916,,,,,,,,,,"20/Dec/22 10:45;fanrui;image-2022-12-20-18-45-32-948.png;https://issues.apache.org/jira/secure/attachment/13054013/image-2022-12-20-18-45-32-948.png","20/Dec/22 10:47;fanrui;image-2022-12-20-18-47-42-385.png;https://issues.apache.org/jira/secure/attachment/13054012/image-2022-12-20-18-47-42-385.png","06/Feb/23 02:09;fanrui;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13055166/screenshot-1.png",,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 03:17:46 UTC 2023,,,,,,,,,,"0|z1e9co:",9223372036854775807,clean-up left-over rocksdb ssts in the shared scope introduced in FLINK-11008 (Flink version 1.8),,,,,,,,,,,,,,,,,,,"20/Dec/22 11:28;fanrui;Hi [~pnowojski] , could you please help take a look in your free time? thanks :)

And I have a POC commit to fix this bug.

[https://github.com/1996fanrui/flink/commit/86a456b2bbdad6c032bf8e0bff71c4824abb3ce1]

 

BTW, FLINK-28984 is also a bug caused some checkpoint files remain forever. I have fixed these 2 bugs in my test job, no files remain with these fixes.

 

 ;;;","05/Feb/23 15:54;ym;merged commit [{{7326add}}|https://github.com/apache/flink/commit/7326addbb3f2b4867689755a04512412bcf69657] into apache:master;;;","05/Feb/23 16:17;ym;Is this problem introduced by FLINK-24611?

I think we should backport this fix to 1.16 and 1.15. Would you mind helping this? [~fanrui] ;;;","06/Feb/23 02:12;fanrui;Hi [~ym], thanks for your review and merge.

{quote}Is this problem introduced by FLINK-24611?
{quote}
After analysis, this problem was not introduced by FLINK-24611, and it was introduced by FLINK-11008.  FLINK-11008 added all sst files to the Map, once some sst uploads fail, all ssts will not be added to the Map.

{quote} I think we should backport this fix to 1.16 and 1.15. Would you mind helping this? 
{quote}

Sure, it's my pleasure, I will do it asap.


!screenshot-1.png|width=1120,height=743!;;;","06/Feb/23 08:28;mapohl;testUploadedSstCanBeCleanedUp was introduced by this Jira issue but causes a test instability (FLINK-30916). Shall I close FLINK-30916 in favor of this issue or keep it open? ;;;","06/Feb/23 08:33;mapohl;Please be careful on how Jira information is updated:
* Release notes are not meant to be used for sharing the commit hash. The release notes field should include a human-readable explanation of what this Jira issue fixes. This will be then added to the release notes of the corresponding release.
* fixVersion is used to specify what Flink version will include this fix. This Jira issue has 1.15.4 and 1.16.2 as fixVersion even though (accoridng to the comments) there's only a fix provided to {{master}} so far. The fixVersion is, as well, relevant for generating the release notes.;;;","06/Feb/23 08:35;ym;[~mapohl] , thanks for reminding, that's why I am re-opening this ticket.

 

It is fine to close FLINK-30916 and track in this one.

The fix also needs to be back-ported. ;;;","06/Feb/23 10:25;fanrui;Offline discussed with [~ym] , I will submit a PR to master (and 1.17, if needed) for FLINK-30916.

And cherry-pick FLINK-30461 and FLINK-30916 together to 1.15 and 1.16.

master PR: https://github.com/apache/flink/pull/21859
1.16 PR: https://github.com/apache/flink/pull/21853
1.15 PR: https://github.com/apache/flink/pull/21855;;;","07/Feb/23 03:17;ym;Thanks [~fanrui] for the fix and [~yanfei] and [~Zakelly]  for discussion and review!

 

merged commit [{{7326add}}|https://github.com/apache/flink/commit/7326addbb3f2b4867689755a04512412bcf69657] into apache:master (for cleaning-up left-over rocksdb shared ssts)

merged commit [{{d761f51}}|https://github.com/apache/flink/commit/d761f51ab5a76331eb9b8f423e0ffaf1d04f97f5] into apache:master (for the concurrent thread race-condition in unit tests)

 

merged commit [{{e2c3d61}}|https://github.com/apache/flink/commit/e2c3d6152d95074561d062ce0b61b6428f6dd6e1] into apache:release-1.16 (back port the above two to 1.16 branch)

merged commit [{{a63f298}}|https://github.com/apache/flink/commit/a63f2983703f438989d069ababcf4cc173441646] into apache:release-1.15 (back port the above two to 1.15 branch)

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OLM Bundle Description Version Problems,FLINK-30456,13514799,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jbusche,jbusche,jbusche,20/Dec/22 01:11,27/Dec/22 15:45,13/Jul/23 08:13,27/Dec/22 15:45,kubernetes-operator-1.3.0,,,,,,,kubernetes-operator-1.4.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"OLM is working great with OperatorHub, but noticed a few items that need fixing.

1.  The basic.yaml example version is release-1.1 instead of the latest release (release-1.3).  This needs fixing in two places:

tools/olm/generate-olm-bundle.sh

tools/olm/docker-entry.sh

2.  The label versions in the description are hardcoded to 1.2.0 instead of the latest release (1.3.0)

3. The Provider is blank space "" "" but soon needs to have some text in there to avoid CI errors with the latest operator-sdk versions.  The person who noticed it recommended ""Community"" for now, but maybe we can being making it ""The Apache Software Foundation"" now?  Not sure if we're ready for that yet or not...

 

I'm working on a PR to address these items.  Can you assign the issue to me?  Thanks!

fyi [~tedhtchang] [~gyfora] ",,gyfora,jbusche,tedhtchang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/22 16:21;tedhtchang;image-2022-12-20-08-21-02-597.png;https://issues.apache.org/jira/secure/attachment/13054029/image-2022-12-20-08-21-02-597.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 27 15:45:26 UTC 2022,,,,,,,,,,"0|z1e8w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/22 16:26;tedhtchang;Hi Jim,  could you also fix the LINKS showing n/a.
[https://operatorhub.io/operator/flink-kubernetes-operator]
!image-2022-12-20-08-21-02-597.png!

https://sdk.operatorframework.io/docs/olm-integration/generation/#csv-fields;;;","27/Dec/22 15:45;gyfora;merged to main 0009746cb3bf96bec0450e99e03c1af20f4038e9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix 'can't find CatalogFactory' error when using FLINK sql-client to add table store bundle jar,FLINK-30453,13514728,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,19/Dec/22 11:51,20/Dec/22 11:41,13/Jul/23 08:13,20/Dec/22 11:40,,,,,,,,table-store-0.3.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"The FLINK 1.16 has introduced new mechanism to allow passing a ClassLoader to EnvironmentSettings[FLINK-15635|https://issues.apache.org/jira/browse/FLINK-15635],  and the Flink SQL client will pass a `ClientWrapperClassLoader`, making that table store CatalogFactory can't be found if it is added by 'ADD JAR' statement through SQL Client.",,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 20 11:40:59 UTC 2022,,,,,,,,,,"0|z1e8go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/22 11:40;TsReaper;master: 7e0d55ff3dc9fd48455b17d9a439647b0554d020;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
State incompatibility issue might cause state loss,FLINK-30437,13514224,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,gyfora,gyfora,16/Dec/22 13:44,20/Dec/22 16:45,13/Jul/23 08:13,19/Dec/22 21:16,kubernetes-operator-1.2.0,kubernetes-operator-1.3.0,,,,,,kubernetes-operator-1.3.1,kubernetes-operator-1.4.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Even though we set:
execution.shutdown-on-application-finish: false
execution.submit-failed-job-on-application-error: true
If there is a state incompatibility the jobmanager marks the Job failed, cleans up HA metada and restarts itself. This is a very concerning behaviour, but we have to fix this on the operator side to at least guarantee no state loss.

The solution is to harden the HA metadata check properly ",,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 20 16:45:55 UTC 2022,,,,,,,,,,"0|z1e5cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/22 21:16;gyfora;merged to main c4e76402f02f05932c6446d97bdc3d60861b9b27;;;","20/Dec/22 16:45;gyfora;merged to release-1.3 4e583a0faa0991c61b37ecd9b937fa3e11c6493a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink command stops with IllegalArgumentException,FLINK-30429,13514058,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,15/Dec/22 14:51,29/Dec/22 14:53,13/Jul/23 08:13,29/Dec/22 14:53,1.17.0,,,,,,,1.17.0,,,,,Client / Job Submission,,,,,,,0,pull-request-available,,,,"The issue has been introduced [here|https://github.com/apache/flink/pull/20910/files#diff-0f8fa6f2e2f26da108646728a221ce0da9a0f090a040f5ee46f6269f6449fc59R1168].

The exception looks like the following:
{code:java}
java.lang.IllegalArgumentException: 1 > 0
	at java.base/java.util.Arrays.copyOfRange(Arrays.java:3990)
	at java.base/java.util.Arrays.copyOfRange(Arrays.java:3950)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1170)
{code}
",,gaborgsomogyi,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 29 14:53:34 UTC 2022,,,,,,,,,,"0|z1e4cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/22 14:53;gyfora;merged to master: 259c7a4776af4d9d7f174f871fb2d4145787f794;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkReadITCase.testSnapshotsTable is unstable,FLINK-30428,13514041,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,15/Dec/22 12:14,16/Dec/22 03:49,13/Jul/23 08:13,16/Dec/22 03:49,table-store-0.3.0,,,,,,,table-store-0.3.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,https://github.com/apache/flink-table-store/actions/runs/3702469144/jobs/6272779158,,lzljs3620320,zjureel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 16 03:49:53 UTC 2022,,,,,,,,,,"0|z1e48o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/22 03:49;lzljs3620320;master: 39f06f2ea5db8eee10a044316e54aeb9623c9f9f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar SQL connector lists not bundled dependencies,FLINK-30427,13514033,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,dwysakowicz,dwysakowicz,15/Dec/22 11:26,15/Dec/22 15:44,13/Jul/23 08:13,15/Dec/22 13:53,pulsar-3.0.0,,,,,,,pulsar-3.0.0,pulsar-4.0.0,,,,Connectors / Pulsar,,,,,,,0,pull-request-available,,,,"flink-connector-pulsar lists:
{code}
- org.bouncycastle:bcpkix-jdk15on:1.69
- org.bouncycastle:bcprov-ext-jdk15on:1.69
{code}

but does not bundle them. (It uses them in test scope)",,dwysakowicz,martijnvisser,syhily,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 15 15:44:23 UTC 2022,,,,,,,,,,"0|z1e46w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/22 13:53;martijnvisser;Fixed in:

main: 06ef03b620dbddb9dbcd55152ac315516e9bc7d4
v3.0: 78324dc30e934c033db25edd162491df35732d1f;;;","15/Dec/22 15:29;syhily;The bouncycastle dependency is introduced by {{pulsar-client-all}}, you can check the dependencies list here: https://mvnrepository.com/artifact/org.apache.pulsar/pulsar-client-all/2.10.2.

I don't know why we need to explicit set the bouncycastle in Pulsar connector? It seems like you mis set the bouncycastle as a test dependency. So just remove them in {{dependencyManagement}} would be better.;;;","15/Dec/22 15:44;martijnvisser;[~syhily] It was needed to resolve dependency convergence issues with the externalization. The scope was incorrect, but they need to be added else dependency convergence occurs. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Archunit can't find any tests in Pulsar repository,FLINK-30426,13514022,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Sergey Nuyanzin,martijnvisser,martijnvisser,15/Dec/22 09:43,15/Dec/22 12:24,13/Jul/23 08:13,15/Dec/22 12:24,pulsar-4.0.0,,,,,,,pulsar-4.0.0,,,,,Connectors / Pulsar,,,,,,,0,pull-request-available,,,,"CI is failing on {{main}} with:

{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (default-test) on project flink-connector-pulsar: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test failed: org.junit.platform.commons.JUnitException: TestEngine with ID 'archunit' failed to discover tests: 'java.lang.Object com.tngtech.archunit.lang.syntax.elements.MethodsThat.areAnnotatedWith(java.lang.Class)' -> [Help 1]
{code}

It does work for {{v3.0}} but there's no obvious difference between the two branches that might explain this issue. ",,martijnvisser,Sergey Nuyanzin,syhily,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 15 12:24:52 UTC 2022,,,,,,,,,,"0|z1e44g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/22 09:54;Sergey Nuyanzin;[~martijnvisser] since I did update of ArchUnit to 1.0.0 may be I can help here
Could you please give a bit more details like links to branches?;;;","15/Dec/22 09:59;martijnvisser;[~Sergey Nuyanzin] Would be great. See https://github.com/apache/flink-connector-pulsar for the repo

You can find a failed run in https://github.com/apache/flink-connector-pulsar/actions/runs/3695837014/jobs/6258724442 which I can reproduce locally by running the same Maven command as the CI does:

mvn clean deploy -Dscala-2.12 -Prun-end-to-end-tests -DdistDir=/path-to-flink-dist/flink-1.17-SNAPSHOT -Dflink.convergence.phase=install -Pcheck-convergence -Darchunit.freeze.store.default.allowStoreUpdate=false -Dlog4j.configurationFile=file:///path-to/flink-connector-pulsar/tools/ci/log4j.properties

For the {{v3.0}} branch it does pass. Command is the same, expect the distribution dir value of course. 

;;;","15/Dec/22 11:32;Sergey Nuyanzin;Seems that the reason is update ArchUnit to 1.0.0

There is a breaking change done in 0.23.0 (described at [https://github.com/TNG/ArchUnit/releases/tag/v0.23.0)]

After that any rule matching nothing will fail.
Flink-connector-pulsar depends on flink-snapshot and as a result pulls a newer version of flink-architecture-tests-test and flink-architecture-tests-production which are now built with 1.0.0 ArchUnit. And it leads to breaking change mentioned above.

A simple WA is use of {{archRule.failOnEmptyShould=false}} as mentioned in release notes.
At the same time it also questionable if there is a need for {{ProductionCodeArchitectureRules}} in this connector since there is no any {{api}} packages or {{Public}} annotation which is the subject of search for some of those rules;;;","15/Dec/22 12:24;martijnvisser;Fixed in main: d82256b3c7b7f028460bf0212766c5508371ac65;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sync JDBC connector to match with previously released 1.16 version,FLINK-30415,13513570,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,martijnvisser,martijnvisser,14/Dec/22 10:09,20/Dec/22 13:51,13/Jul/23 08:13,20/Dec/22 13:51,jdbc-3.0.0,,,,,,,jdbc-3.0.0,,,,,Connectors / JDBC,,,,,,,0,,,,,"The current {{3.0}} branch of the externalized JDBC connector contains commits that were synced from Flink's {{master}} branch. Those commits shouldn't be there, but should be in the {{main}} branch for the externalized JDBC connector (which could be released in a next {{3.1}} release)",,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 14 12:19:43 UTC 2022,,,,,,,,,,"0|z1e1c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/22 12:19;martijnvisser;Dropped 5 commits:
+ 00b5b71055ed88b1e101402b7cb215d4a78527c7 Update version to 1.17-SNAPSHOT
+ eceece3505381c46758ef6312e1d003af0fae723 [FLINK-14101][jdbc-connector] Support SQLServer dialect in the jdbc connector.
+ 9efb82b4d86d8326f73a147fa490916306855d30 [FLINK-29466][Connector/JDBC] Bump postgresql in /flink-connectors/flink-connector-jdbc
+ 3e3b40e8cfadfc16a8ab74d4ef6a3ab3ceafa57b [FLINK-16024][connector/jdbc] Support filter pushdown
+ 29c8af85a3495503192b95aede8a223762d67730 [FLINK-30177][Connectors/JDBC] Update pgjdbc to avoid CVE-2022-41946;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink deployment stuck in UPGRADING state when deploy flinkdeployment without resource,FLINK-30411,13513557,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nowke,tanjialiang,tanjialiang,14/Dec/22 09:26,04/Jan/23 13:07,13/Jul/23 08:13,04/Jan/23 13:07,kubernetes-operator-1.2.0,kubernetes-operator-1.3.0,,,,,,kubernetes-operator-1.4.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"In flink kubernetes operator 1.2.0. When i deploy a flinkdeployments without resource, the flink deployment stuck in UPGRADING state.
{code:java}
apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
name: socket-window-word-count
spec:
image: flink:1.16.0-scala_2.12-java8
flinkVersion: v1_16
flinkConfiguration:
taskmanager.numberOfTaskSlots: ""1""
serviceAccount: flink
job:
jarURI: local:///opt/flink/examples/streaming/WordCount.jar
parallelism: 2
upgradeMode: stateless{code}
 

when i kubectl describe flinkdeployments, i found this error message

!image-2022-12-14-17-22-12-656.png!

 

maybe we can validate it when apply flinkdeployment?  When it is invalid, throw an error rather than apply flinkdeployment succeed.

 

 ",,gyfora,nowke,tanjialiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/22 09:22;tanjialiang;image-2022-12-14-17-22-12-656.png;https://issues.apache.org/jira/secure/attachment/13053866/image-2022-12-14-17-22-12-656.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 04 13:07:59 UTC 2023,,,,,,,,,,"0|z1e194:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/22 09:30;gyfora;Could you please work on adding the required extra validation?;;;","18/Dec/22 07:02;nowke;[~gyfora] Could you assign this issue to me? I can send a PR right away and backport to {{release-1.2}} and {{release-1.3}} as well.;;;","18/Dec/22 08:45;gyfora;Thank you[~nowke] , we can backport this to 1.3 after it’s merged to main , I agree.

 

 ;;;","04/Jan/23 13:07;gyfora;merged to main 4feee47f0175d12ad192fbfc89d483793dcae5b8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jobmanager Deployment error without HA metadata should not lead to unrecoverable error,FLINK-30406,13513479,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,13/Dec/22 20:45,01/Jan/23 17:32,13/Jul/23 08:13,28/Dec/22 16:20,,,,,,,,kubernetes-operator-1.3.1,kubernetes-operator-1.4.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Currently we don't have a good way of asserting that the job never started after savepoint upgrade when the JM deployment fails (such as on an incorrect image).

This easily leads to scenarios which require manual recovery from the user.

We should try to avoid this with some mechanism to greately improve the robustness of savepoint ugrades.",,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 28 16:20:38 UTC 2022,,,,,,,,,,"0|z1e0s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Dec/22 16:20;gyfora;merged to 
main 6ff02908b254c0f8e7328d495870d4bd562d54b1
release-1.3 5ed117cb8e3e2b00f2abeb0f98d7555d78e0afe3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sql hint 'LOOKUP' which is defined in outer query block may take effect in inner query block,FLINK-30396,13513327,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lam167,lam167,lam167,13/Dec/22 08:40,27/Feb/23 06:48,13/Jul/23 08:13,27/Feb/23 06:47,1.16.0,,,,,,,1.17.0,,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"As [flink doc|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/queries/hints/#query-hints] said:

> {{Query hints}} can be used to suggest the optimizer to affect query execution plan within a specified query scope. Their effective scope is current {{{}Query block{}}}([What are query blocks ?|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/queries/hints/#what-are-query-blocks-]) which {{Query Hints}} are specified.

But the sql hint 'LOOKUP' behaves differently like the demo following:
{code:java}
-- DDL
CREATE TABLE left_table (
    lid INTEGER,
    lname VARCHAR,
    pts AS PROCTIME()
) WITH (
    'connector' = 'filesystem',
    'format' = 'csv',
    'path'='xxx'
) 

CREATE TABLE dim_table (
    id INTEGER,
    name VARCHAR,
    mentor VARCHAR,
    gender VARCHAR
) WITH (
    'connector' = 'jdbc',
    'url' = 'xxx',
    'table-name' = 'dim1',
    'username' = 'xxx',
    'password' = 'xxx',
    'driver'= 'com.mysql.cj.jdbc.Driver' 
)

-- DML
SELECT /*+ LOOKUP('table'='outer') */
    ll.id AS lid,
    ll.name,
    r.mentor,
    r.gender
FROM (
    SELECT /*+ LOOKUP('table'='inner') */
    l.lid AS id,
    l.lname AS name,
    r.mentor,
    r.gender,
    l.pts
    FROM left_table AS l
JOIN dim_table FOR SYSTEM_TIME AS OF l.pts AS r
ON l.lname = r.name
) ll JOIN dim_table FOR SYSTEM_TIME AS OF ll.pts AS r ON ll.name=r.name{code}
The inner correlate will have two hints:
{noformat}
{     
    [LOOKUP inheritPath:[0] options:{table=inner}],
    [LOOKUP inheritPath:[0, 0, 0] options:{table=outer}]
}{noformat}
and IMO which maybe is a bug.

The first hint comes from the inner query block and the second hint comes from the outer block, and ClearJoinHintWithInvalidPropagationShuttle will not clear the second hint cause the correlate has no 'ALIAS' hint.

The reason for the above case is that the hint 'ALIAS' now only works for join rel nodes and 'LOOKUP' works for correlate and join rel nodes.

I think maybe the better way would be to make 'ALIAS' support both correlate and join rel nodes like 'LOOKUP'.
 ",,lam167,lincoln.86xy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 26 06:04:11 UTC 2023,,,,,,,,,,"0|z1dzu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/23 06:04;lincoln.86xy;fixed in master: a72f88f36505e1ce45f2280584a11385c2c2bc14

fixed in release-1.17: 5ee98b544d936e8cde2d4429425da46185031bb3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Column constraint lacks primary key not enforced check,FLINK-30386,13513218,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,12/Dec/22 15:12,12/Jul/23 16:23,13/Jul/23 08:13,07/Feb/23 07:16,1.15.2,1.15.3,1.16.0,,,,,1.17.0,,,,,Table SQL / API,,,,,,,0,pull-request-available,,,,"Currently, only table constraint performs the enforced check. Not sure if it is by design or a bug.

The following case can be reproduced on Flink 1.16.0, 1.15.3, and 1.15.2. I think the earlier version might also reveal it.
{code:sql}
Flink SQL> create table T (f0 int not null primary key, f1 string) with ('connector' = 'datagen');
[INFO] Execute statement succeed.

Flink SQL> explain select * from T;
== Abstract Syntax Tree ==
LogicalProject(f0=[$0], f1=[$1])
+- LogicalTableScan(table=[[default_catalog, default_database, T]])

== Optimized Physical Plan ==
TableSourceScan(table=[[default_catalog, default_database, T]], fields=[f0, f1])

== Optimized Execution Plan ==
TableSourceScan(table=[[default_catalog, default_database, T]], fields=[f0, f1])


Flink SQL> create table S (f0 int not null, f1 string, primary key(f0)) with ('connector' = 'datagen');
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.ValidationException: Flink doesn't support ENFORCED mode for PRIMARY KEY constraint. ENFORCED/NOT ENFORCED  controls if the constraint checks are performed on the incoming/outgoing data. Flink does not own the data therefore the only supported mode is the NOT ENFORCED mode
{code}",,dwysakowicz,lincoln.86xy,martijnvisser,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 07:16:04 UTC 2023,,,,,,,,,,"0|z1dz60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 15:35;dwysakowicz;Do you mean that we should support the {{ENFORCED}}  mode? It was a conscious decision to support only {{NOT ENFORCED}} as the data is stored outside of Flink and thus there is no way to ensure the uniqueness of the key.;;;","13/Dec/22 02:16;qingyue;Re [~dwysakowicz] . The `NOT ENFORCED` check only performs on the table constraint but skips the column constraint. As a result, `PRIMARY KEY(a)` will throw an exception, but `a BIGINT NOT NULL PRIMARY KEY` is allowed.

I think we should also throw the exception when encountering `a BIGINT NOT NULL PRIMARY KEY` and inform users to use `a BIGINT NOT NULL PRIMARY KEY NOT ENFORCED` instead.;;;","13/Dec/22 03:22;qingyue;Would you help verify this issue, is it a bug or by design? cc [~lincoln.86xy] and [~fsk119];;;","13/Dec/22 08:12;dwysakowicz;Thanks for the explanation [~qingyue], I think that's fair assessment. ;;;","13/Dec/22 12:50;lincoln.86xy;[~qingyue] this is indeed a bug of current implementation,  `a BIGINT NOT NULL PRIMARY KEY` (without `NOT ENFORCED`) is expected to be unsupported and requires an error here;;;","14/Dec/22 06:14;qingyue;Thanks for the explanation, and let's fix this. Please assign the ticket to me. cc [~lincoln.86xy] ;;;","07/Feb/23 07:16;lincoln.86xy;fixed in master: e69e6514d0b901eb03f1a8bfc499680d076248c9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommonExecSink does not use ClientWrapperClassLoader while extracting Type from KeyedStream,FLINK-30377,13513170,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,samrat007,prabhujoseph,prabhujoseph,12/Dec/22 11:35,27/Jan/23 14:53,13/Jul/23 08:13,27/Jan/23 14:53,1.16.0,,,,,,,1.16.1,1.17.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"CommonExecSink does not use ClientWrapperClassLoader while extracting Type from KeyedStream. This will lead to ClassNotFoundException on user classes added through add jar command. This is working fine on Flink 1.15.

 
{code:java}
Caused by: java.lang.ClassNotFoundException: org.apache.hudi.common.model.HoodieRecord
 at java.net.URLClassLoader.findClass(URLClassLoader.java:387) ~[?:1.8.0_352]
 at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_352]
 at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352) ~[?:1.8.0_352]
 at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_352]
 at java.lang.Class.forName0(Native Method) ~[?:1.8.0_352]
 at java.lang.Class.forName(Class.java:348) ~[?:1.8.0_352]
 at org.apache.flink.api.java.typeutils.TypeExtractionUtils.checkAndExtractLambda(TypeExtractionUtils.java:143) ~[flink-dist-1.16.0.jar:1.16.0]
 at org.apache.flink.api.java.typeutils.TypeExtractor.getUnaryOperatorReturnType(TypeExtractor.java:539) ~[flink-dist-1.16.0.jar:1.16.0]
 at org.apache.flink.api.java.typeutils.TypeExtractor.getKeySelectorTypes(TypeExtractor.java:415) ~[flink-dist-1.16.0.jar:1.16.0]
 at org.apache.flink.api.java.typeutils.TypeExtractor.getKeySelectorTypes(TypeExtractor.java:406) ~[flink-dist-1.16.0.jar:1.16.0]
 at org.apache.flink.streaming.api.datastream.KeyedStream.<init>(KeyedStream.java:116) ~[flink-dist-1.16.0.jar:1.16.0]
 at org.apache.flink.streaming.api.datastream.DataStream.keyBy(DataStream.java:300) ~[flink-dist-1.16.0.jar:1.16.0]
 at org.apache.hudi.sink.utils.Pipelines.hoodieStreamWrite(Pipelines.java:339) ~[?:?]
 at org.apache.hudi.table.HoodieTableSink.lambda$getSinkRuntimeProvider$0(HoodieTableSink.java:104) ~[?:?]
 at org.apache.hudi.adapter.DataStreamSinkProviderAdapter.consumeDataStream(DataStreamSinkProviderAdapter.java:35) ~[?:?]
 at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.applySinkProvider(CommonExecSink.java:483) ~[?:?]
 at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.createSinkTransformation(CommonExecSink.java:203) ~[?:?]
 at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.java:176) ~[?:?]
 at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:158) ~[?:?]
 at org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:85) ~[?:?]
 at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) ~[flink-scala_2.12-1.16.0.jar:1.16.0]
 at scala.collection.Iterator.foreach(Iterator.scala:937) ~[flink-scala_2.12-1.16.0.jar:1.16.0]
 at scala.collection.Iterator.foreach$(Iterator.scala:937) ~[flink-scala_2.12-1.16.0.jar:1.16.0]
 at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) ~[flink-scala_2.12-1.16.0.jar:1.16.0]
 at scala.collection.IterableLike.foreach(IterableLike.scala:70) ~[flink-scala_2.12-1.16.0.jar:1.16.0]
 at scala.collection.IterableLike.foreach$(IterableLike.scala:69) ~[flink-scala_2.12-1.16.0.jar:1.16.0]
 at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-scala_2.12-1.16.0.jar:1.16.0]
 at scala.collection.TraversableLike.map(TraversableLike.scala:233) ~[flink-scala_2.12-1.16.0.jar:1.16.0]
 at scala.collection.TraversableLike.map$(TraversableLike.scala:226) ~[flink-scala_2.12-1.16.0.jar:1.16.0]
 at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-scala_2.12-1.16.0.jar:1.16.0]
 at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:84) ~[?:?]
 at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:197) ~[?:?]
 at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1733) ~[flink-table-api-java-uber-1.16.0.jar:1.16.0]
 at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:825) ~[flink-table-api-java-uber-1.16.0.jar:1.16.0]
 at org.apache.flink.table.client.gateway.local.LocalExecutor.executeModifyOperations(LocalExecutor.java:219) ~[flink-sql-client-1.16.0.jar:1.16.0]

 {code}
 

 ",,dannycranmer,JasonLee,prabhujoseph,samrat007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 27 14:52:47 UTC 2023,,,,,,,,,,"0|z1dyvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jan/23 14:52;dannycranmer;Merged commit [{{18a0ea9}}|https://github.com/apache/flink/commit/18a0ea99d6eb48cd4a0963c0e8a42b4d9d4833cf] into apache:master

Merged commit [{{d71cf8e}}|https://github.com/apache/flink/commit/d71cf8ee314d92f3425ae2dc80d0cc83e0d3c44e] into apache:release-1.16 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlClient leaks flink-table-planner jar under /tmp,FLINK-30375,13513168,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,samrat007,prabhujoseph,prabhujoseph,12/Dec/22 11:18,11/Jan/23 20:58,13/Jul/23 08:13,11/Jan/23 20:58,1.16.0,,,,,,,1.17.0,,,,,Table SQL / Client,,,,,,,0,pull-request-available,,,,"SqlClient leaks flink-table-planner jar under /tmp

 
{code:java}

[root@ip-172-1-1-3 lib]# ls -lrt /tmp/flink-table-planner_*
-rw-rw-r-- 1 hadoop hadoop 39138893 Dec 12 10:08 /tmp/flink-table-planner_acada33f-a10b-4a4a-ad16-6bca25a67e10.jar
-rw-rw-r-- 1 hadoop hadoop 39138893 Dec 12 10:17 /tmp/flink-table-planner_fb2f6e31-48a0-4c1e-ab7b-16129e776125.jar
-rw-rw-r-- 1 hadoop hadoop 39138893 Dec 12 10:22 /tmp/flink-table-planner_83499393-1621-43de-953c-2000bc6967ce.jar
-rw-rw-r-- 1 hadoop hadoop 39138893 Dec 12 10:24 /tmp/flink-table-planner_3aa798da-e794-4c6c-ad9f-49b4574da64b.jar
-rw-rw-r-- 1 hadoop hadoop 39138893 Dec 12 10:36 /tmp/flink-table-planner_ea52d9ea-3148-4fc3-8a58-f83063bb14d5.jar
-rw-rw-r-- 1 hadoop hadoop 39138893 Dec 12 10:44 /tmp/flink-table-planner_fdc64e21-6bc7-4e4a-b8b2-2a97629d0727.jar
-rw-rw-r-- 1 hadoop hadoop 39137545 Dec 12 11:05 /tmp/flink-table-planner_84c00b13-c6b5-4e8b-80cb-5631a2fa3150.jar
-rw-rw-r-- 1 hadoop hadoop 39137601 Dec 12 11:10 /tmp/flink-table-planner_d00b8a7b-e615-46f7-bd21-b5efa684c184.jar
-rw-rw-r-- 1 hadoop hadoop 39137601 Dec 12 11:11 /tmp/flink-table-planner_a413520d-ce15-41f9-aa5e-05a30f7eaff5.jar {code}",,dannycranmer,prabhujoseph,samrat007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 11 20:58:08 UTC 2023,,,,,,,,,,"0|z1dyuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/22 17:21;samrat007;I will like to work on this issue ;;;","11/Jan/23 09:58;samrat007;Please help review the pr. ;;;","11/Jan/23 20:58;dannycranmer;Merged commit [{{086f13a}}|https://github.com/apache/flink/commit/086f13a9cd625f856309d1d57d80a0b6a66c687b] into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python Group Agg failed in cleaning the idle state,FLINK-30366,13513096,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxb,hxb,hxb,12/Dec/22 03:39,12/Dec/22 11:49,13/Jul/23 08:13,12/Dec/22 11:49,1.15.3,1.16.0,,,,,,1.15.4,1.16.1,1.17.0,,,API / Python,,,,,,,0,pull-request-available,,,,"{code:java}
# aggregate_fast.pyx
cpdef void on_timer(self, InternalRow key):
    if self.state_cleaning_enabled:
        self.state_backend.set_current_key(key) # The key must be a list, but it is a InternalRow here.
        accumulator_state = self.state_backend.get_value_state(
            ""accumulators"", self.state_value_coder)
        accumulator_state.clear()
        self.aggs_handle.cleanup() {code}",,hxb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 12 11:49:00 UTC 2022,,,,,,,,,,"0|z1dyew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 11:49;hxb;Merged into master via 72a70313b59352736514b4927a1dfadc2e8e4232

Merged into release-1.16 via 041d863552396105d08af097a456ee291263d434

Merged into release-1.15 via d3413718bc5751a10dda6c5a7b41626666753c07;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster deleted and created back while updating replicas,FLINK-30361,13512207,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Swathi Chandrashekar,Swathi Chandrashekar,Swathi Chandrashekar,11/Dec/22 12:19,01/Jan/23 16:18,13/Jul/23 08:13,14/Dec/22 15:02,kubernetes-operator-1.2.0,kubernetes-operator-1.3.0,,,,,,kubernetes-operator-1.3.1,kubernetes-operator-1.4.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Whenever we try to update the replicas of the task manager for a flink standalone session cluster using the flink CR, any change in CR triggers a redeploy of the flink cluster ( delete + create of all the components - JM and TM ).

This might not be required for replica update and this should not affect the existing pods and only a new TM pod will added during a scale up and a TM pod should be deleted during a scale down.

Example tried --> Change the TM replicas from 2 to 3.

{quote}PS C:\Users\cswathi\Documents\flink-OSS-operator> kubectl get pods -w
NAME                                                              READY   STATUS              RESTARTS   AGE
basic-session-deployment-only-example-5dbbdf5dd8-cq8nb            0/1     ContainerCreating   0          1s
basic-session-deployment-only-example-taskmanager-77854fbb7vzvd   0/1     ContainerCreating   0          1s
basic-session-deployment-only-example-taskmanager-77854fbbg6vzs   0/1     ContainerCreating   0          1s
flink-kubernetes-operator-676897686f-5fc8r                        2/2     Running             0          18m
basic-session-deployment-only-example-5dbbdf5dd8-cq8nb            1/1     Running             0          1s
basic-session-deployment-only-example-taskmanager-77854fbb7vzvd   1/1     Running             0          1s
basic-session-deployment-only-example-taskmanager-77854fbbg6vzs   1/1     Running             0          13s
basic-session-deployment-only-example-taskmanager-77854fbb7vzvd   1/1     Terminating         0          65s
*basic-session-deployment-only-example-5dbbdf5dd8-cq8nb            1/1     Terminating         0          65s
basic-session-deployment-only-example-taskmanager-77854fbbg6vzs   1/1     Terminating         0          65s
basic-session-deployment-only-example-taskmanager-77854fbb7vzvd   1/1     Terminating         0          66s
basic-session-deployment-only-example-5dbbdf5dd8-cq8nb            1/1     Terminating         0          66s*
basic-session-deployment-only-example-taskmanager-77854fbbg6vzs   1/1     Terminating         0          66s
basic-session-deployment-only-example-taskmanager-77854fbb7vzvd   0/1     Terminating         0          66s
basic-session-deployment-only-example-taskmanager-77854fbb7vzvd   0/1     Terminating         0          66s
basic-session-deployment-only-example-taskmanager-77854fbb7vzvd   0/1     Terminating         0          66s
basic-session-deployment-only-example-taskmanager-77854fbbg6vzs   0/1     Terminating         0          67s
basic-session-deployment-only-example-taskmanager-77854fbbg6vzs   0/1     Terminating         0          67s
basic-session-deployment-only-example-taskmanager-77854fbbg6vzs   0/1     Terminating         0          67s
basic-session-deployment-only-example-5dbbdf5dd8-cq8nb            0/1     Terminating         0          67s
basic-session-deployment-only-example-5dbbdf5dd8-cq8nb            0/1     Terminating         0          67s
basic-session-deployment-only-example-5dbbdf5dd8-cq8nb            0/1     Terminating         0          67s
basic-session-deployment-only-example-588474bf97-nng85            0/1     Pending             0          0s
basic-session-deployment-only-example-588474bf97-nng85            0/1     Pending             0          0s
basic-session-deployment-only-example-588474bf97-nng85            0/1     ContainerCreating   0          0s
basic-session-deployment-only-example-taskmanager-77854fbb5ddxv   0/1     Pending             0          0s
basic-session-deployment-only-example-taskmanager-77854fbb5ddxv   0/1     Pending             0          0s
basic-session-deployment-only-example-taskmanager-77854fbbrfgvz   0/1     Pending             0          0s
basic-session-deployment-only-example-taskmanager-77854fbb57v4t   0/1     Pending             0          0s
basic-session-deployment-only-example-taskmanager-77854fbbrfgvz   0/1     Pending             0          1s
basic-session-deployment-only-example-taskmanager-77854fbb57v4t   0/1     Pending             0          1s
basic-session-deployment-only-example-taskmanager-77854fbb5ddxv   0/1     ContainerCreating   0          1s
basic-session-deployment-only-example-taskmanager-77854fbbrfgvz   0/1     ContainerCreating   0          1s
basic-session-deployment-only-example-taskmanager-77854fbb57v4t   0/1     ContainerCreating   0          1s
basic-session-deployment-only-example-588474bf97-nng85            0/1     ContainerCreating   0          1s
basic-session-deployment-only-example-taskmanager-77854fbbrfgvz   0/1     ContainerCreating   0          1s
basic-session-deployment-only-example-taskmanager-77854fbb57v4t   0/1     ContainerCreating   0          1s
basic-session-deployment-only-example-taskmanager-77854fbb5ddxv   0/1     ContainerCreating   0          1s
basic-session-deployment-only-example-taskmanager-77854fbbrfgvz   1/1     Running             0          1s
basic-session-deployment-only-example-taskmanager-77854fbb5ddxv   1/1     Running             0          1s
basic-session-deployment-only-example-588474bf97-nng85            1/1     Running             0          2s
basic-session-deployment-only-example-taskmanager-77854fbb57v4t   1/1     Running             0          17s{quote}

",,gyfora,pvary,ram_krish,Swathi Chandrashekar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 01 16:18:08 UTC 2023,,,,,,,,,,"0|z1dsxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/22 12:25;Swathi Chandrashekar;In SessionReconciler.java ( flink-kubernetes-operator), we are checking if there is any spec change, delete the session cluster, wait for it to be terminated and later deploy the new changes
{quote}@Override
protected void reconcileSpecChange(-----) throws Exception {
deleteSessionCluster(deployment, observeConfig);

// We record the target spec into an upgrading state before deploying
ReconciliationUtils.updateStatusBeforeDeploymentAttempt(deployment, deployConfig);
statusRecorder.patchAndCacheStatus(deployment);

deploy( deployment,deployment.getSpec(), deployment.getStatus(), ctx, deployConfig,Optional.empty( false
{quote}
The ask is, we can remove the deletion of session cluster whenever there is any update, as the kubernetes deployment will handle any kind of update [ i.e if there is a replica change, no change to the existing pods - only added/deletion of diff , there is a config change, depending on the rolling strategy, the update is passed to the corresponding pods by the kubernetes ].;;;","11/Dec/22 12:38;gyfora;We have similar logic in place for standalone applications with reactive scheduler enabled, so this should be a relatively easy fix.;;;","11/Dec/22 15:40;gyfora;[~Swathi Chandrashekar] do you want to give this issue a try? ;;;","11/Dec/22 17:38;Swathi Chandrashekar;Yes [~gyfora] , I would like to try this;;;","14/Dec/22 15:02;gyfora;merged to main efefd31b5155436b36e53101e92a51ada2f240b0;;;","01/Jan/23 16:18;gyfora;release-1.3 7da3cdd6ec4bfd20f24d27dc70df1c40abe6a341;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Encountered NoClassDefFoundError when using  flink-sql-connector-elasticsearch6 ,FLINK-30359,13511533,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,10/Dec/22 17:31,28/Feb/23 16:52,13/Jul/23 08:13,18/Jan/23 15:35,1.15.0,elasticsearch-3.0.0,,,,,,1.16.1,elasticsearch-3.0.1,elasticsearch-4.0.0,,,Connectors / ElasticSearch,,,,,,,0,pull-request-available,,,,"When I use sql-client to submit job that sink data to elasticsearch, it seems that the `NoClassDefFoundError` arises as shown in the figure below. Note that the connector jar I used is flink-sql-connector-elasticsearch6-4.0-SNAPSHOT.jar, it is a uber jar compiled from flink-connector-elasticsearch repo's main branch.

!image-2022-12-11-01-09-47-337.png|width=596,height=285!

Through some investigations, I found that in flink-sql-connector-elasticsearch6 module's pom.xml, we manually excluded the `com.carrotsearch: hppc` dependency, which is no problem for the 6.3.1 version of elasticsearch. However, FLINK-25189 bump this to 6.8.20, which does requires the dependence of hppc. Therefore, an `NoClassDefFoundError` occurred.

 

 ",,martijnvisser,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31263,,,,,,"10/Dec/22 17:09;Weijie Guo;image-2022-12-11-01-09-47-337.png;https://issues.apache.org/jira/secure/attachment/13053750/image-2022-12-11-01-09-47-337.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 18 15:35:48 UTC 2023,,,,,,,,,,"0|z1dork:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/22 17:36;Weijie Guo;To fix this problem, I think we no longer need to exclude `com.carrotsearch: hppc` from the sql connector jar of elasticsearch-6. In addition, I tested the 6.3.1 version of es excluding `hppc`, and the 6.8.20 version of es including `hppc`, all of which work well, hope this can help confirm which versions of flink(since the es connector was previously maintained in apache/flink repo) has this problem.

I am willing to fix this, [~chesnay] would you like to assign this ticket to me and I will open a pull request later.;;;","12/Dec/22 09:01;chesnay;[~Weijie Guo] Let's remove the exclusion from the shade--plugin. Ideally we also convert the shade-plugin exclusions into actual dependency exclusions so catch stuff like this earlier.;;;","17/Jan/23 12:03;martijnvisser;Fixed in main: 045a6a3acc6d3a6b76f4046f83f02d0b9c1bbb2d

[~Weijie Guo] Can you also open a PR towards the {{release-1.16}} branch of Flink and {{v3.0}} of the externalized Elasticsearch repo?
;;;","18/Jan/23 03:40;Weijie Guo;[~martijnvisser] Yes, I opened back-port PR towards these two repo respectively:

flink-1.16: https://github.com/apache/flink/pull/21708

v3.0: https://github.com/apache/flink-connector-elasticsearch/pull/51;;;","18/Jan/23 13:32;martijnvisser;release-1.16: f37048e78c94dcf2386e80c347fda02ff4b35d3d
v3.0: c6b15c4891147a0b39548db1897de4f3628c9eaa;;;","18/Jan/23 15:35;martijnvisser;Thanks [~Weijie Guo] - All fixed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
crictl causes long wait in e2e tests,FLINK-30355,13511142,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,mapohl,mapohl,09/Dec/22 16:04,19/Dec/22 13:47,13/Jul/23 08:13,19/Dec/22 13:47,1.17.0,,,,,,,1.17.0,,,,,Test Infrastructure,Tests,,,,,,0,pull-request-available,test-stability,,,"We observed strange behavior in the e2e test where the e2e test run times out: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43824&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&s=ae4f8708-9994-57d3-c2d7-b892156e7812&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=7446

The issue seems to be related to {{crictl}} again because we see the following error message in multiple tests. No logs are produced afterwards for ~30mins resulting in the overall test run taking too long:
{code}
Dec 09 08:55:39 crictl
fatal: destination path 'cri-dockerd' already exists and is not an empty directory.
fatal: a branch named 'v0.2.3' already exists
mkdir: cannot create directory ‘bin’: File exists
Dec 09 09:26:41 fs.protected_regular = 0
{code}",,mapohl,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 19 13:47:38 UTC 2022,,,,,,,,,,"0|z1dmco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/22 09:39;martijnvisser;[~mapohl] I don't think this is related to crictl but it is related to cri-dockerd. The test is cloning that repository. Let me see if I can harden it a bit. ;;;","19/Dec/22 13:47;martijnvisser;Fixed in master: f9557068cf203a2eabf31ffd3911c40373d7e9ce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connecting SqlGateway via beeline gets java.lang.UnsupportedOperationException:: Unrecognized TGetInfoType value: CLI_ODBC_KEYWORDS,FLINK-30347,13510861,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,paul8263,paul8263,09/Dec/22 03:31,09/Dec/22 03:35,13/Jul/23 08:13,09/Dec/22 03:35,1.16.0,,,,,,,,,,,,Table SQL / Gateway,,,,,,,0,,,,,"Connecting to Flink SqlGateway with hiveserver2 endpoint enabled via beeline(provided by Hive 3.1.0) resulted in the following error:
{code:java}
2022-12-09 10:24:28,600 ERROR org.apache.flink.table.endpoint.hive.HiveServer2Endpoint     [] - Failed to GetInfo.
java.lang.UnsupportedOperationException: Unrecognized TGetInfoType value: CLI_ODBC_KEYWORDS.
        at org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.GetInfo(HiveServer2Endpoint.java:371) [flink-connector-hive_2.12-1.16.0.jar:1.16.0]
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetInfo.getResult(TCLIService.java:1537) [hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetInfo.getResult(TCLIService.java:1522) [hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
2022-12-09 10:24:28,600 ERROR org.apache.thrift.server.TThreadPoolServer                   [] - Thrift error occurred during processing of message.
org.apache.thrift.protocol.TProtocolException: Required field 'infoValue' is unset! Struct:TGetInfoResp(status:TStatus(statusCode:ERROR_STATUS, infoMessages:[*java.lang.UnsupportedOperationException:Unrecognized TGetInfoType value: CLI_ODBC_KEYWORDS.:9:8, org.apache.flink.table.endpoint.hive.HiveServer2Endpoint:GetInfo:HiveServer2Endpoint.java:371, org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetInfo:getResult:TCLIService.java:1537, org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetInfo:getResult:TCLIService.java:1522, org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39, org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39, org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:286, java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1142, java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:617, java.lang.Thread:run:Thread.java:745], errorMessage:Unrecognized TGetInfoType value: CLI_ODBC_KEYWORDS.), infoValue:null)
        at org.apache.hive.service.rpc.thrift.TGetInfoResp.validate(TGetInfoResp.java:379) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.hive.service.rpc.thrift.TCLIService$GetInfo_result.validate(TCLIService.java:5228) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.hive.service.rpc.thrift.TCLIService$GetInfo_result$GetInfo_resultStandardScheme.write(TCLIService.java:5285) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.hive.service.rpc.thrift.TCLIService$GetInfo_result$GetInfo_resultStandardScheme.write(TCLIService.java:5254) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.hive.service.rpc.thrift.TCLIService$GetInfo_result.write(TCLIService.java:5205) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
2022-12-09 10:24:28,600 WARN  org.apache.thrift.transport.TIOStreamTransport               [] - Error closing output stream.
java.net.SocketException: Socket closed
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:118) ~[?:1.8.0_121]
        at java.net.SocketOutputStream.write(SocketOutputStream.java:155) ~[?:1.8.0_121]
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82) ~[?:1.8.0_121]
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140) ~[?:1.8.0_121]
        at java.io.FilterOutputStream.close(FilterOutputStream.java:158) ~[?:1.8.0_121]
        at org.apache.thrift.transport.TIOStreamTransport.close(TIOStreamTransport.java:110) [hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.thrift.transport.TSocket.close(TSocket.java:235) [hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:303) [hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121] {code}
The same problem occured when I tried the beeline tool provided by Kyuubi.

However, the SQL Gateway works well with DBeaver and JDBC Java code.","Flink 1.16.0

Hive 3.1.0

Hadoop 3.1.1",luoyuxia,paul8263,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29839,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 09 03:35:18 UTC 2022,,,,,,,,,,"0|z1dkm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/22 03:35;luoyuxia;Similar issue as FLINK-29839;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceCoordinator error splitRequest check cause HybridSource loss of data and hang,FLINK-30334,13510715,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,taoran,taoran,taoran,08/Dec/22 07:26,27/Dec/22 01:30,13/Jul/23 08:13,27/Dec/22 01:30,1.16.0,1.17.0,,,,,,1.16.1,1.17.0,,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"If we use hybrid source, for example, filesystem source A read a.csv, filesystem B read b.csv. It's a very simple case, but it will hang in second source with:

10802 [SourceCoordinator-Source: hybrid_source[1]] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: hybrid_source[1] received split request from parallel task 0 (#0)
10802 [SourceCoordinator-Source: hybrid_source[1]] INFO  org.apache.flink.connector.file.src.impl.StaticFileSplitEnumerator [] - Subtask 0 (on host '') is requesting a file source split
10803 [SourceCoordinator-Source: hybrid_source[1]] INFO  org.apache.flink.connector.file.src.assigners.LocalityAwareSplitAssigner [] - Assigning split to non-localized request: Optional[FileSourceSplit: [file:/Users/xxx/a.csv|file:///Users/xxx/a.csv] [0, 49) (no host info) ID=0000000001 position=null]
10808 [SourceCoordinator-Source: hybrid_source[1]] INFO  org.apache.flink.connector.file.src.impl.StaticFileSplitEnumerator [] - Assigned split to subtask 0 : FileSourceSplit: [file:/Users/xxx/a.csv|file:///Users/xxx/a.csv] [0, 49) (no host info) ID=0000000001 position=null
10816 [Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.hybrid.HybridSourceReader [] - Adding splits subtask=0 sourceIndex=0 currentReader=org.apache.flink.connector.file.src.impl.FileSourceReader@1e8e1971 [HybridSourceSplit

{sourceIndex=0, splitId=0000000001}

]
10817 [Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [FileSourceSplit: [file:/Users/xxx/a.csv|file:///Users/chucheng/TMP/a.csv] [0, 49) (no host info) ID=0000000001 position=null]
10822 [Source Data Fetcher for Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 0
10864 [Source Data Fetcher for Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Finished reading from splits [0000000001]
+I[hello_a, flink, 1]
+I[hello_a, hadoop, 2]
+I[hello_a, world, 3]
10866 [Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Finished reading split(s) [0000000001]
10868 [Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Closing splitFetcher 0 because it is idle.
10868 [Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Shutting down split fetcher 0
10868 [Source Data Fetcher for Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Split fetcher 0 exited.
10869 [SourceCoordinator-Source: hybrid_source[1]] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: hybrid_source[1] received split request from parallel task 0 (#0)
10870 [SourceCoordinator-Source: hybrid_source[1]] INFO  org.apache.flink.connector.file.src.impl.StaticFileSplitEnumerator [] - Subtask 0 (on host '') is requesting a file source split
10872 [SourceCoordinator-Source: hybrid_source[1]] INFO  org.apache.flink.connector.file.src.impl.StaticFileSplitEnumerator [] - No more splits available for subtask 0
10872 [Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Reader received NoMoreSplits event.
10872 [Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.hybrid.HybridSourceReader [] - End of input subtask=0 sourceIndex=0 org.apache.flink.connector.file.src.impl.FileSourceReader@1e8e1971
StaticFileSplitEnumerator:org.apache.flink.connector.file.src.impl.StaticFileSplitEnumerator@69906bb9
10874 [SourceCoordinator-Source: hybrid_source[1]] INFO  org.apache.flink.connector.base.source.hybrid.HybridSourceSplitEnumerator [] - Starting enumerator for sourceIndex=1
10879 [Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.hybrid.HybridSourceReader [] - Switch source event: subtask=0 sourceIndex=1 source=org.apache.flink.connector.file.src.FileSource@12ef574f
10879 [Source: hybrid_source[1] -> Sink: print_out[2] (1/1)#0] INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
10882 [SourceCoordinator-Source: hybrid_source[1]] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: hybrid_source[1] received split request from parallel task 0 (#0)

*do not read next source data and hang at 'received split request from parallel task'*

 

The reason is that in the 1.16 & master, latest code add context.hasNoMoreSplits check then call enumerator.handleSplitRequest.  We do understand the comments for reducing the call splits. But it not consider the the situation about HybridSource. When a subtask hasNoMoreSplits, it will switch to next source. But here just set a check without this situation. When first source read finish, the context just let this subtask with noMoreSplit Status. And the later check can't assign splits with next sources. However , the flink 1.15 is correct.

 

*SourceCoordinator*

 
{code:java}
private void handleRequestSplitEvent(int subtask, int attemptNumber, RequestSplitEvent event) {
    LOG.info(
            ""Source {} received split request from parallel task {} (#{})"",
            operatorName,
            subtask,
            attemptNumber);
    // request splits from the enumerator only if the enumerator has un-assigned splits
    // this helps to reduce unnecessary split requests to the enumerator
    if (!context.hasNoMoreSplits(subtask)) {
        enumerator.handleSplitRequest(subtask, event.hostName());
    }
} {code}
SourceCoordinator call `context.hasNoMoreSplits` check cause the subtask not read the other child sources in hybrid source.

 

SourceCoordinatorContext

 
{code:java}
boolean hasNoMoreSplits(int subtaskIndex) { return subtaskHasNoMoreSplits[subtaskIndex]; }


@Override
public void signalNoMoreSplits(int subtask) {
    checkSubtaskIndex(subtask);

    // Ensure the split assignment is done by the coordinator executor.
    callInCoordinatorThread(
            () -> {
                subtaskHasNoMoreSplits[subtask] = true;
                signalNoMoreSplitsToAttempts(subtask);
                return null; // void return value
            },
            ""Failed to send 'NoMoreSplits' to reader "" + subtask);
}

{code}
context set subtask noMoreSplit is true if source is done (without considering the hybrid situation).

 

 

1.15

 
{code:java}
public void handleEventFromOperator(int subtask, OperatorEvent event) {
    runInEventLoop(
            () -> {
                if (event instanceof RequestSplitEvent) {
                    LOG.info(
                            ""Source {} received split request from parallel task {}"",
                            operatorName,
                            subtask);
                    enumerator.handleSplitRequest(
                            subtask, ((RequestSplitEvent) event).hostName());
                }  {code}
 ",,martijnvisser,taoran,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 27 01:30:02 UTC 2022,,,,,,,,,,"0|z1djps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/22 07:42;taoran;Hi. [~zhuzh] [~thw]  please take a look?   I think this question is very important because it will lead to loss of data and unexpected runtime behavior.   thanks. ;;;","08/Dec/22 09:00;martijnvisser;[~lemonjing] Isn't your second source also a batch/bounded source, while it should be a streaming/unbounded source?;;;","08/Dec/22 09:11;taoran;[~martijnvisser] yes, my second source is bounded csv. But HybridSource second source no need to must be unbounded (hybridsource without this limitation).  if second source is bounded hybridsource will return bounded. if second source is unbounded (To be precise, it is the last source, because the hybridsource is not necessarily just two) it will return unbounded.  hybrid source can work in batch and streaming. In batch mode, it will read 2 sources and return.

And it doesn't matter, i change second source to be streaming data source, it cause same bug too. The key point is current second child source do not run now (block at split request).

1.15 works well.;;;","09/Dec/22 03:46;zhuzh;Thanks for reporting this issue! [~lemonjing]

It is unexpected that {{HybridSourceSplitEnumerator}} will assign splits after it has signaled a noMoreSplits event. I think this behavior is misleading. 
The solution you proposed may solve the issue of HybridSource. However, it may introduce problems to speculative execution. Therefore, we need a better solution.

At least, we should enable the {{SourceCoordinator}} to distinguish fake noMoreSplits event and true noMoreSplits event, to know whether the source is truly fully consumed, to make correct decisions. To achieve this, we can change {{HybridSourceSplitEnumerator}} to intercept noMoreSplits signaled by the internal enumerators, and notifies {{SourceCoordinatorContext}} in a different way(i.e. introduce a SourceCoordinatorContext#signalIntermediateNoMoreSplits(int)).

Besides that, we should also refine the HybridSource test to cover {{handleRequestSplitEvent}} code path, so that this problem can be identified in ahead.;;;","09/Dec/22 05:46;taoran;[~zhuzh] Yes, the key point is that we need to distinguish the noMoreSplits is intermediated event or final event. Can u assign this ticket to me？ i'm glad to modify the pr and fix this issue. ;;;","09/Dec/22 09:56;zhuzh;Thanks for volunteering to fix it. [~lemonjing]
I have assigned you the ticket. :);;;","20/Dec/22 10:10;taoran;[~zhuzh] Hi, i have updated the pr and tested it. currently it works well. 
Looking forward for your review, thanks. ;;;","27/Dec/22 01:30;zhuzh;master:
6e4e6c68a4a179d932dacafb2771cc84f730bbc0

1.16:
2b5e4ac8df41fe68ae8212eab4a254740e0c5ef4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-kubernetes-operator helm chart does not work with dynamic config because of use of volumeMount subPath,FLINK-30329,13510656,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ottomata,ottomata,ottomata,07/Dec/22 19:42,01/Jan/23 17:32,13/Jul/23 08:13,12/Dec/22 15:52,,,,,,,,kubernetes-operator-1.3.1,kubernetes-operator-1.4.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"flink-kubernetes-operator supports dynamic configuration when {{kubernetes.operator.dynamic.config.enabled}} is enabled.  The provided helm charts make use of a k8s ConfigMap to provide the flink-conf.yaml file.  These helm charts make use of the {{subPath}} VolumeMount feature.  When mounted volumes use {{{}subPath{}}}, the volume that uses them is not updated automatically.  So, even you do update the ConfigMap in k8s, the flink-conf.yaml file will never see those updates.
 * [https://kubernetes.io/docs/concepts/configuration/configmap/#mounted-configmaps-are-updated-automatically]
 * [https://kubernetes.io/docs/concepts/storage/volumes/#configmap]

Since all 'subPaths' of the volume are being mounted in both the operator and webhook containers at the same paths, I believe an easy fix would be to avoid declaring individual volumeMounts for each file, and just mount the whole ConfigMap as a volume at /opt/flink/conf.",,mbalassi,ottomata,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 12 15:52:46 UTC 2022,,,,,,,,,,"0|z1djco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 15:52;mbalassi;dc173cb in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskManagerWideRocksDbMemorySharingITCase.testBlockCache failed,FLINK-30328,13510463,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,mapohl,mapohl,07/Dec/22 16:28,04/Feb/23 19:07,13/Jul/23 08:13,04/Feb/23 19:07,1.17.0,,,,,,,1.17.0,,,,,Runtime / Coordination,Runtime / State Backends,,,,,,0,pull-request-available,test-stability,,,"{{TaskManagerWideRocksDbMemorySharingITCase.testBlockCache}} failed in this build: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43763&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9836]
{code:java}
Dec 06 16:33:59 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 12.926 s <<< FAILURE! - in org.apache.flink.test.state.TaskManagerWideRocksDbMemorySharingITCase
Dec 06 16:33:59 [ERROR] org.apache.flink.test.state.TaskManagerWideRocksDbMemorySharingITCase.testBlockCache  Time elapsed: 12.907 s  <<< FAILURE!
Dec 06 16:33:59 java.lang.AssertionError: 
Dec 06 16:33:59 Block cache usage reported by different tasks varies too much: DoubleSummaryStatistics{count=20, sum=3783523840.000000, min=189045056.000000, average=189176192.000000, max=189569600.000000}
Dec 06 16:33:59 That likely mean that they use different cache objects expected:<1.895696E8> but was:<1.89045056E8>
Dec 06 16:33:59 	at org.junit.Assert.fail(Assert.java:89)
Dec 06 16:33:59 	at org.junit.Assert.failNotEquals(Assert.java:835)
Dec 06 16:33:59 	at org.junit.Assert.assertEquals(Assert.java:555)
Dec 06 16:33:59 	at org.apache.flink.test.state.TaskManagerWideRocksDbMemorySharingITCase.testBlockCache(TaskManagerWideRocksDbMemorySharingITCase.java:133)
[...] {code}",,fsk119,JunRuiLi,leonard,mapohl,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30831,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Feb 04 19:07:03 UTC 2023,,,,,,,,,,"0|z1di5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/22 16:29;mapohl;[~roman] is there anyone else I could ping on these issues besides you to spread the work a bit?;;;","07/Dec/22 16:31;mapohl;The test failure appeared in a CI run for [hotfix PR #21459|https://github.com/apache/flink/pull/21459]. It's quite likely that the failure is unrelated to the change because the PR addresses deprecated Calcite APIs.;;;","08/Dec/22 23:44;roman;The failure is most likely unrelated and I can look into it. Thanks for reporting and pinging me.;;;","11/Dec/22 07:04;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43861&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a;;;","02/Jan/23 09:57;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44309&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9849;;;","02/Jan/23 11:07;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44373&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=11054;;;","04/Jan/23 11:39;fsk119;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44456&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=5c8e7682-d68f-54d1-16a2-a09310218a49;;;","09/Jan/23 15:38;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44613&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=7433;;;","19/Jan/23 13:35;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45036&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10185;;;","19/Jan/23 13:36;mapohl;I'm updating the priority to become a blocker since it appears to only happen on master. [~roman]  Are there any updates on that topic?;;;","19/Jan/23 14:09;roman;Sorry for the late reply, [~mapohl] 

I've checked it locally and made sure that it is a test issue, not a production code issue. In particular, I've found no issues with:
 - memory sharing (the feature that's tested)
 - concurrency - except metric reading, which is expected

The problem in test is caused by the timings of metrics collections.

The interval is hardcoded to 5s in Flink currently, but even if changed it still doesn't solve the problem completely. There seems to be no better alternative to testing via metrics either.

So I opened drafted a PR to check deviation instead of min/max which should eliminate false positives: https://github.com/apache/flink/pull/21733
I'll try to finalize it this week.;;;","24/Jan/23 22:11;roman;Merged into master as f8105bb584f8e7b6302a15c1c9270b988ff574cb.;;;","26/Jan/23 07:33;mapohl;I'm reopening this issue because we still see test failures on {{master}}. The following build failed with the fix [f8105bb5|https://github.com/apache/flink/commit/f8105bb584f8e7b6302a15c1c9270b988ff574cb] being included:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45196&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9881;;;","30/Jan/23 08:44;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45352&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8015;;;","30/Jan/23 12:44;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45374&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8043;;;","30/Jan/23 14:00;mapohl;{quote}
I've checked it locally and made sure that it is a test issue, not a production code issue. In particular, I've found no issues with:
 - memory sharing (the feature that's tested)
 - concurrency - except metric reading, which is expected

The problem in test is caused by the timings of metrics collections.
{quote}

[~roman] is there a chance that we can get it fixed before 1.17? Or would you vote for lowering the priority of this issue?;;;","30/Jan/23 15:39;roman;I think the priority can be lowered because it is a test code issue.

I've opened a [PR|https://github.com/apache/flink/pull/21791/] to ignore the test temporarily as a quick solution.

I've also opened a [PR|https://github.com/apache/flink/pull/21778] to fix the test, hope it can get merged soon.;;;","30/Jan/23 15:42;mapohl;thanks for getting back to me on that one. I'm lowering the priority to Major.;;;","30/Jan/23 15:51;mapohl;I created FLINK-30831 after going through the logs once more.;;;","31/Jan/23 07:40;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45408&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8037;;;","04/Feb/23 19:07;roman;Fixed by 7b65644161767e90f0834db76ba116d4d92449d5 in master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Client should not create table directory structure,FLINK-30326,13510425,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Gerrrr,Gerrrr,07/Dec/22 12:33,29/Mar/23 01:49,13/Jul/23 08:13,29/Mar/23 01:49,,,,,,,,,,,,,Table Store,,,,,,,0,,,,,"The SQL client creates the schema part of the directory structure for new tables. This behavior is incorrect because the SQL client could be attached to a SQL gateway that does not necessarily have access to DFSes for all catalogs/tables. 

I propose to change the behavior such that FTS sink creates the schema, either on the fly if `auto-create=true` or just as a separate job if `auto-create=false`.
",,Gerrrr,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 01:49:24 UTC 2023,,,,,,,,,,"0|z1dhxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/22 12:36;Gerrrr;I used the following example to conclude that the SQL client creates the table structure.
h2. Setup

I used a docker-compose with 1 JM and 1 TM that do not have shared volumes as well as an SQL client started on the JM.

 

_NOTE_ I used the local file system for the warehouse to find out what component was responsible for creation of what directories. I understand that it is not the production-ready setup.
h3. Step 1: Create FTS catalog and schema via a SQL client on JM and see where the schema files appear
{noformat}
Flink SQL> CREATE CATALOG my_catalog WITH (
>   'type'='table-store',
>   'warehouse'='file:/tmp/table_store'
> );
[INFO] Execute statement succeed.Flink SQL> USE CATALOG my_catalog;
>
Flink SQL> CREATE TABLE word_count (
>     word STRING PRIMARY KEY NOT ENFORCED,
>     cnt BIGINT
> );
[INFO] Execute statement succeed.{noformat}
h3. Step 2: Check where the directory was created

JM:
{noformat}
root@flink:/opt/flink# ls /tmp/table_store/default.db/word_count/schema/schema-0
/tmp/table_store/default.db/word_count/schema/schema-0
{noformat}
TM:
{noformat}
ls /tmp/table_store/default.db/
ls: cannot access '/tmp/table_store/default.db/': No such file or directory
{noformat}
h3. Step 3: Do a single insert and see where the new files appear
{noformat}
Flink SQL> INSERT INTO word_count (word, cnt) VALUES ('foo', 1);
[INFO] Submitting SQL update statement to the cluster...
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/opt/flink/lib/flink-dist-1.17-SNAPSHOT.jar) to field java.lang.Class.ANNOTATION
WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleaner
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: c9c175d0137280aeeba83cc7b2ce8454
{noformat}
On JM: nothing changed
On TM:
{noformat}
$ ls /tmp/table_store/default.db/word_count/
bucket-0  manifest  snapshot
{noformat};;;","07/Dec/22 12:59;Gerrrr;[~lzljs3620320] [~TsReaper] Could you please comment if you agree with the issue statement and would review a design proposal that addresses it?;;;","09/Dec/22 02:32;lzljs3620320;Thanks [~Gerrrr]for reporting.
I think it is the configuration of local files that causes some behavior ambiguity.
Maybe we just need to remind the user that there is a problem with this operation?;;;","12/Dec/22 18:13;Gerrrr;Hey [Jingsong Lee|https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lzljs3620320] ,

I agree that the local filesystem should not be used as storage, and we might warn users about it. I used the local filesystem only to show that the SQL client creates a directory structure on disk.

My main point was that the SQL session should not create files on DFS because it does not necessarily have access. AFAIU, neither SQL gateway nor JM require DFS credentials for anything but creating schema files on {{{}CREATE TABLE{}}}. The SQL gateway might run in a different DC and not have access to the DFS at all.

I suggest that TMs should always create the table files. If {{{}auto-create=true{}}}, FTS queries would do it on start. If {{{}auto-create=false{}}}, {{CREATE TABLE}} would submit a job that creates the directory structure and the schema files.;;;","29/Mar/23 01:49;lzljs3620320;I close this, I think it is difficult to implement in paimon, if necessary, please raise the issue again in paimon.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade ZLIB of FRocksDB to 1.2.13,FLINK-30321,13510404,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yanfei Lei,Yanfei Lei,Yanfei Lei,07/Dec/22 09:54,31/Mar/23 01:12,13/Jul/23 08:13,12/Dec/22 09:07,1.17.0,,,,,,,1.17.0,,,,,Runtime / State Backends,,,,,,,0,,,,,"In FRocksDB, the ZLIB version is 1.2.11, which may result in memory corruption, see [cve-2018-25032|https://nvd.nist.gov/vuln/detail/cve-2018-25032#vulnCurrentDescriptionTitle]

https://lists.apache.org/thread/rm40f45qfw6rls70k35o2dt0k4tz9bsr",,songwenbin,Yanfei Lei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 31 01:12:32 UTC 2023,,,,,,,,,,"0|z1dhso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 09:07;Yanfei Lei;Merged [3eac409|https://github.com/ververica/frocksdb/commit/3eac409606fcd9ce44a4bf7686db29c06c205039] into FRocksDB-6.20.3.;;;","30/Mar/23 03:12;songwenbin;How do you solve the problem with cve-2018-25032 in Flink rockdb? [~Yanfei Lei] ;;;","30/Mar/23 05:44;Yanfei Lei;By upgrading the version of ZLIB.;;;","31/Mar/23 01:12;songwenbin;Thank you very much for your answer. In flink1.16.1 , upgrade the frocksdbjni of the pom.xml of the flick-statebackend-rocksdb to 6.20.3-veverica-2.0 ,is is ok ? [~Yanfei Lei] 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validate illegal column name in table store,FLINK-30319,13510345,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,07/Dec/22 03:55,08/Dec/22 07:23,13/Jul/23 08:13,08/Dec/22 07:23,table-store-0.3.0,,,,,,,table-store-0.3.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"There're some specific column names such as `_KEY_` prefix, `_VALUE_COUNT`, `_SEQUENCE_NUMBER` and `_VALUE_KIND`, we should validate column names and throw exception in DDL",,lzljs3620320,zjureel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 08 07:23:59 UTC 2022,,,,,,,,,,"0|z1dhfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/22 07:23;lzljs3620320;master: 51d57803c3eee1943bc5a4ddf1228aff42aa88d3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create table in FTS catalog with s3 warehouse throws DatabaseNotExistException,FLINK-30317,13510091,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Gerrrr,Gerrrr,06/Dec/22 17:03,29/Mar/23 01:45,13/Jul/23 08:13,29/Mar/23 01:45,,,,,,,,,,,,,Table Store,,,,,,,0,,,,,"{noformat}
Flink SQL> CREATE CATALOG my_catalog WITH (
>   'type'='table-store',
>   'warehouse'='s3://bucket/my-tablestore'
> );
[INFO] Execute statement succeed.

Flink SQL> USE CATALOG my_catalog;
[INFO] Execute statement succeed.

Flink SQL> CREATE TABLE word_count (
>     word STRING PRIMARY KEY NOT ENFORCED,
>     cnt BIGINT
> );
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.catalog.exceptions.DatabaseNotExistException: Database default does not exist in Catalog my_catalog. {noformat}

Creating the table in the default catalog works though:

{noformat}
Flink SQL> use catalog default_catalog;
[INFO] Execute statement succeed.

Flink SQL> CREATE TABLE word_count (
>       word STRING PRIMARY KEY NOT ENFORCED,
>       cnt BIGINT
>  ) WITH (
>    'connector'='table-store',
>    'path'='s3://bucket/my-tablestore',
>    'auto-create'='true'
> );
[INFO] Execute statement succeed.
{noformat}",,Gerrrr,lzljs3620320,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 01:45:36 UTC 2023,,,,,,,,,,"0|z1dfvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/22 06:53;lzljs3620320;Thanks [~Gerrrr] for reporting.
We should check and create default database in FileSystemCatalog and HiveCatalog.;;;","29/Mar/23 01:45;lzljs3620320;https://github.com/apache/incubator-paimon/issues/733;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassCastException: class java.io.ObjectStreamClass$Caches$1 cannot be cast to class java.util.Map is showing in the logging when the job shutdown,FLINK-30308,13509979,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,06/Dec/22 02:11,09/Jan/23 09:50,13/Jul/23 08:13,09/Jan/23 09:50,,,,,,,,1.15.4,1.16.1,1.17.0,,,API / Python,,,,,,,0,pull-request-available,,,,"{code:java}
2022-12-05 18:26:40,229 WARN  org.apache.flink.streaming.api.operators.AbstractStreamOperator [] - Failed to clean up the leaking objects.
java.lang.ClassCastException: class java.io.ObjectStreamClass$Caches$1 cannot be cast to class java.util.Map (java.io.ObjectStreamClass$Caches$1 and java.util.Map are in module java.base of loader 'bootstrap')
        at org.apache.flink.streaming.api.utils.ClassLeakCleaner.clearCache(ClassLeakCleaner.java:58) ~[blob_p-a72e14b9030c3ca0d3d0a8fc6e70166c7419d431-f7f18b2164971cb6798db9ab762feabd:1.15.0]
        at org.apache.flink.streaming.api.utils.ClassLeakCleaner.cleanUpLeakingClasses(ClassLeakCleaner.java:39) ~[blob_p-a72e14b9030c3ca0d3d0a8fc6e70166c7419d431-f7f18b2164971cb6798db9ab762feabd:1.15.0]
        at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.close(AbstractPythonFunctionOperator.java:142) ~[blob_p-a72e14b9030c3ca0d3d0a8fc6e70166c7419d431-f7f18b2164971cb6798db9ab762feabd:1.15.0]
        at org.apache.flink.streaming.api.operators.python.AbstractExternalPythonFunctionOperator.close(AbstractExternalPythonFunctionOperator.java:73) ~[blob_p-a72e14b9030c3ca0d3d0a8fc6e70166c7419d431-f7f18b2164971cb6798db9ab762feabd:1.15.0]
        at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:163) ~[flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:125) ~[flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:997) ~[flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254) ~[flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72) ~[flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127) ~[flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:916) ~[flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:930) ~[flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948) [flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:930) [flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741) [flink-dist-1.15.2.jar:1.15.2]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) [flink-dist-1.15.2.jar:1.15.2]
        at java.lang.Thread.run(Unknown Source) [?:?]{code}

Reported in Slack: https://apache-flink.slack.com/archives/C03G7LJTS2G/p1670265131083639?thread_ts=1670265114.640369&cid=C03G7LJTS2G",,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 09 09:50:27 UTC 2023,,,,,,,,,,"0|z1df6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 09:50;dianfu;Fixed in:
- master via 8aa446a8aa71e1d03986970585dacd94e1a4182d
- release-1.16 via bbd7b2c0d2c9be0c42b7c4e8dcb8200a4bd4a285
- release-1.15 via 01127bc254ccceb4ed32afe3c130fe86cc9b3d96;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Turn off e2e test error check temporarily,FLINK-30307,13509910,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,05/Dec/22 16:31,06/Dec/22 09:09,13/Jul/23 08:13,06/Dec/22 09:09,,,,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,gaborgsomogyi,mbalassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 09:09:32 UTC 2022,,,,,,,,,,"0|z1der4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/22 09:09;mbalassi;b95e3e41fbac940fa281b69d05df32ef02591691 in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible Deadlock in Kinesis/Firehose/DynamoDB Connector,FLINK-30304,13509907,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dannycranmer,dannycranmer,dannycranmer,05/Dec/22 16:12,16/Jan/23 08:58,13/Jul/23 08:13,16/Jan/23 08:58,1.15.3,1.16.0,aws-connector-3.0.0,aws-connector-4.0.0,,,,1.15.4,1.16.1,1.17.0,aws-connector-3.1.0,aws-connector-4.1.0,Connectors / DynamoDB,Connectors / Firehose,Connectors / Kinesis,,,,,0,,,,,"AWS Sinks based on Async Sink can enter a deadlock situation if the AWS async client throws error outside of the future. We observed this with a local application:
{code:java}
java.lang.NullPointerException
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.utils.NettyUtils.closedChannelMessage(NettyUtils.java:135)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.utils.NettyUtils.decorateException(NettyUtils.java:71)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.NettyRequestExecutor.handleFailure(NettyRequestExecutor.java:310)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.NettyRequestExecutor.makeRequestListener(NettyRequestExecutor.java:189)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.CancellableAcquireChannelPool.lambda$acquire$1(CancellableAcquireChannelPool.java:58)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.HealthCheckedChannelPool.ensureAcquiredChannelIsHealthy(HealthCheckedChannelPool.java:114)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.HealthCheckedChannelPool.lambda$tryAcquire$1(HealthCheckedChannelPool.java:97)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:571)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:550)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise.access$200(DefaultPromise.java:35)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.DefaultPromise$1.run(DefaultPromise.java:502)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)
at org.apache.flink.kinesis.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
at org.apache.flink.kinesis.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
at java.base/java.lang.Thread.run(Thread.java:829){code}
Related AWS SDK issues that can cause this:
 * [https://github.com/aws/aws-sdk-java-v2/issues/3435]
 * [https://github.com/aws/aws-sdk-java-v2/issues/1812]

If an error is thrown and not handled by the future then the AsyncSink will never decrement {{{}inFlightRequestCount{}}}. the job will hang while trying flush for checkpoint

 

!sink-deadlock.png|width=736,height=374!",,chalixar,dannycranmer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30633,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/22 22:38;dannycranmer;sink-deadlock.png;https://issues.apache.org/jira/secure/attachment/13053546/sink-deadlock.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 08:58:51 UTC 2023,,,,,,,,,,"0|z1deqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/22 22:37;dannycranmer;I do not think we can do anything about this besides wait for the fix in the AWS SDK. Essentially there is an error in the Async client which results in the Future not being completed. This [issue](https://github.com/aws/aws-sdk-java-v2/pull/3574) should fix it once merged.

One way around this is to keep track of inflight request time, and fail the job (or retry) upon some timeout.;;;","06/Dec/22 17:06;chalixar;Thanks [~dannycranmer] for reporting the issue. The root cause seems correct to me.

{{- I do not think we can do anything about this besides wait for the fix in the AWS SDK.}}

I agree,I can't find a quick workaround from the concrete writer side.

- {{One way around this is to keep track of inflight request time, and fail the job (or retry) upon some timeout.}}

I think this might need more investigation as it might break some existing implementations, unless we enable sink implementer to override but that would not be backward compatible, wdyt?
;;;","06/Dec/22 17:36;dannycranmer;Thanks [~chalixar] ;;;","11/Jan/23 09:11;dannycranmer;This has been fixed in SDK v2.19.14, upgrade in FLINK-30633;;;","16/Jan/23 08:58;dannycranmer;Fixed with SDK upgrade FLINK-30633;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskExecutorTest.testSharedResourcesLifecycle failed with TaskException,FLINK-30301,13509855,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,mapohl,mapohl,05/Dec/22 11:04,25/Jan/23 01:59,13/Jul/23 08:13,25/Jan/23 01:59,1.17.0,,,,,,,1.17.0,,,,,Runtime / Coordination,,,,,,,0,pull-request-available,test-stability,,,"This seems to be a follow-up of FLINK-30275. Same test but different test failure (2x in the same build):
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43709&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=7479
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43709&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=7852
{code}
Dec 05 03:59:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
Dec 05 03:59:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
Dec 05 03:59:18 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
Dec 05 03:59:18 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
Dec 05 03:59:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
Dec 05 03:59:18 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Dec 05 03:59:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
Dec 05 03:59:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
Dec 05 03:59:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
Dec 05 03:59:18 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
Dec 05 03:59:18 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
Dec 05 03:59:18 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
Dec 05 03:59:18 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Dec 05 03:59:18 Caused by: org.apache.flink.runtime.taskexecutor.exceptions.TaskException: Cannot find task to stop for execution 096b33c46c225fd4af41a9484b64c7fe_010f83ce510d70707aaf04c441173b70_0_0.
Dec 05 03:59:18 	at org.apache.flink.runtime.taskexecutor.TaskExecutor.cancelTask(TaskExecutor.java:864)
Dec 05 03:59:18 	... 53 more
{code}",,mapohl,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30275,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 25 01:59:51 UTC 2023,,,,,,,,,,"0|z1deew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/22 11:05;mapohl;cc [~roman] aaand another round ;-) May you have a look at it?;;;","08/Dec/22 23:29;roman;Sorry for introducing the instability and thanks for reporting it.

 

I found locally that the existing heartbeat timeout of 1s can be too small when the machine is overloaded.

This timeout causes TM to release the connection with JM, cancelling all the tasks.

Then subsequent ""cancelTask"" calls will fail with the above exception.

 

This can be tested by increasing the number of tasks from 10 to e.g. 100 [here|https://github.com/apache/flink/blob/d86ae5d642fa578fb85118e81dd4140d504f818a/flink-runtime/src/test/java/org/apache/flink/runtime/taskexecutor/TaskExecutorTest.java#L3042]; or adding `Thread.sleep(1s)` right before `cancelTask` [here|https://github.com/apache/flink/blob/d86ae5d642fa578fb85118e81dd4140d504f818a/flink-runtime/src/test/java/org/apache/flink/runtime/taskexecutor/TaskExecutorTest.java#L3077].

 

I think simply increasing the timeouts to large values should be enough. Otherwise, a new `HeartbeatServices` has to be added.

I've created a [PR|https://github.com/apache/flink/pull/21467] for that - would you be able to take a look [~mapohl] ?;;;","25/Jan/23 01:59;roman;Merged into master as b4fe4a4288777c491019f8a59fb894bc9f60031b.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Alibaba001 not responding,FLINK-30297,13509840,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangyang0918,mapohl,mapohl,05/Dec/22 09:51,05/Dec/22 14:34,13/Jul/23 08:13,05/Dec/22 14:34,1.15.3,1.16.0,1.17.0,,,,,,,,,,Build System / Azure Pipelines,Test Infrastructure,,,,,,0,test-stability,,,,"{{Alibaba001}} seems to be corrupted causing build failures like [that one|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43692&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5]:
{code}
##[error]We stopped hearing from agent AlibabaCI001-agent01. Verify the agent machine is running and has a healthy network connection. Anything that terminates an agent process, starves it for CPU, or blocks its network access can cause this error. For more information, see: https://go.microsoft.com/fwlink/?linkid=846610
Agent: AlibabaCI001-agent01
Started: Sat at 1:48 AM
Duration: 1h 3m 27s
{code}",,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 05 14:34:12 UTC 2022,,,,,,,,,,"0|z1debk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/22 09:52;mapohl;[~wangyang0918] feel free to forward the assignee status to anyone who's able to handle the issue.;;;","05/Dec/22 10:50;mapohl;Looks like the issue was already resolved. Some of the new builds of the weekend execute jobs on {{Alibaba001}} without problems again (e.g. [this one|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43701&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90]).;;;","05/Dec/22 10:51;mapohl;I'm lowering the priority to Major but keep this issue open to observe. Feel free to confirm that the issue is resolved, Yang.;;;","05/Dec/22 14:34;mapohl;Closing the issue again. It appears that the issue was fixed already. No failures due to {{Alibaba001}} not being responsive occurred anymore.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configmaps get cleaned up when upgrading standalone Flink cluster,FLINK-30287,13509307,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,stevenpyzhang,stevenpyzhang,02/Dec/22 19:22,07/Dec/22 00:31,13/Jul/23 08:13,06/Dec/22 19:31,,,,,,,,,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"I started up a Standalone session Flink cluster and ran one job on it. I checked the configMaps and see the HA data.

 
{code:java}
kubectl get configmaps -n flink-operator
NAME                                                                            DATA   AGE
flink-config-sql-example-deployment-s3-testing                                  2      5m41s
flink-operator-config                                                           3      42h
kube-root-ca.crt                                                                1      42h
sql-example-deployment-s3-testing-000000003f57cd5f0000000000000002-config-map   0      11m
sql-example-deploymnt-s3-testing-cluster-config-map                             5      12m
{code}
 

I then update the FlinkDep image field and the Flink cluster gets restarted. The HA configmap for the job is now gone.
{code:java}
kubectl get configmaps -n flink-operator
NAME                                                   DATA   AGE
flink-config-sql-example-deployment-s3-testing         2      18m
flink-operator-config                                  3      43h
kube-root-ca.crt                                       1      43h
sql-example-deployment-s3-testing-cluster-config-map   3      31m {code}
 

I think this is due to a race condition where the TM first terminates which causes the JM to interpret the Job entering a failed state which causes it to clean up the configmaps.",,stevenpyzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-02 19:22:51.0,,,,,,,,,,"0|z1db1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parsing for log4j error entry instead of plain error string in e2e tests,FLINK-30281,13509247,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,02/Dec/22 13:44,05/Dec/22 09:05,13/Jul/23 08:13,05/Dec/22 09:05,,,,,,,,,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Error which came up:

[https://github.com/gaborgsomogyi/flink-kubernetes-operator/actions/runs/3601102650/jobs/6066522997#step:9:121]
{code:java}
Found error in log files.

flink-kubernetes-operator-7456697448-mgw42.k8soperator.flink.flink-kubernetes-operator.namespace.default.FlinkDeployment.JmDeploymentStatus.ERROR.Count: 0
121 {code}",,gaborgsomogyi,morhidi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 05 09:05:36 UTC 2022,,,,,,,,,,"0|z1dao0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/22 13:49;morhidi;The metrics must be enabled, Gabor. The ERROR matching patterns should handle this.;;;","02/Dec/22 14:09;gaborgsomogyi;OK, then adding a whitelist entry.;;;","05/Dec/22 09:05;gaborgsomogyi;[{{31e2e10}}|https://github.com/apache/flink-kubernetes-operator/commit/31e2e10f112385120ac44378eaaa2e39b338ea61] on main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default operator logging configuration is broken,FLINK-30280,13509244,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,02/Dec/22 13:33,06/Dec/22 17:01,13/Jul/23 08:13,06/Dec/22 17:01,kubernetes-operator-1.2.0,kubernetes-operator-1.3.0,,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"The default logging configuration is set here: [https://github.com/apache/flink-kubernetes-operator/blob/ea01e294cf1b68d597244d0a11b3c81822a163e7/helm/flink-kubernetes-operator/templates/flink-operator.yaml#L89]

However, this file is not available in the official Docker image. It's best to not set this value and rely on the included logging configuration in the operator JAR. Users can define overrides on the deployment in [https://github.com/apache/flink-kubernetes-operator/blob/ea01e294cf1b68d597244d0a11b3c81822a163e7/helm/flink-kubernetes-operator/values.yaml#L135 |https://github.com/apache/flink-kubernetes-operator/blob/ea01e294cf1b68d597244d0a11b3c81822a163e7/helm/flink-kubernetes-operator/values.yaml#L135]or re-add the environment variable pointing to a valid file.",,mbalassi,morhidi,mxm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 17:01:39 UTC 2022,,,,,,,,,,"0|z1danc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/22 13:43;morhidi;The Helm installs the default configs it refers to. Don't really see the issue here.;;;","05/Dec/22 16:18;morhidi;[~mxm] are you planning to change anything in the current release or can I remove this from 1.3?;;;","06/Dec/22 17:01;mbalassi;[{{41b9f4f}}|https://github.com/apache/flink-kubernetes-operator/commit/41b9f4fc199785168b9c31ef35d62cdef7f86561] in main, scope has been reduced to documentation for 1.3

[~mxm] feel free to open a follow up if you feel it is warranted.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unexpected config mutation in SinkTransformationTranslator ,FLINK-30278,13509227,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,akalashnikov,akalashnikov,02/Dec/22 11:30,24/Jan/23 16:15,13/Jul/23 08:13,24/Jan/23 16:15,1.17.0,,,,,,,1.17.0,,,,,Runtime / Configuration,,,,,,,0,pull-request-available,,,,"If we forbid changing configuration programmatically(`execution.program-config.enabled`) and try to use `FileSink`. The following exception will occur:
{noformat}
  org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Not allowed configuration change(s) were detected:
 - Configuration parallelism.default:1 not allowed in the configuration object ExecutionConfig.
   at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:364)
   at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:225)
   at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:98)
   at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:319)
   at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$runApplicationAsync$2(ApplicationDispatcherBootstrap.java:262)
   at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
   at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
   at org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter$ScheduledFutureTask.run(ActorSystemScheduledExecutorAdapter.java:171)
   at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
   at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$withContextClassLoader$0(ClassLoadingUtils.java:41)
   at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
   at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
   at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
   at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)
   at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)
   at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
   at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
Caused by: org.apache.flink.client.program.MutatedConfigurationException: Not allowed configuration change(s) were detected:
 - Configuration parallelism.default:1 not allowed in the configuration object ExecutionConfig.
   at org.apache.flink.client.program.StreamContextEnvironment.checkNotAllowedConfigurations(StreamContextEnvironment.java:235)
   at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:175)
   at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:115)
   at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2049)
   at org.apache.flink.streaming.examples.wordcount.WordCount.main(WordCount.java:81)
   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
   at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
   at java.base/java.lang.reflect.Method.invoke(Unknown Source)
   at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:347)
   ... 16 more
{noformat}

It happens since inside of `SinkTransformationTranslator` we have following logic:
* Remeber the current parallelism
* Set parallelism to default
* Do transformation
* Set parallelism to remembered one

But if the initial prallelism is default we actually should do nothing but according current logic we explicitly set default value to the configuration which actually is the programmatic config mutation(which we want to avoid)

See org.apache.flink.streaming.runtime.translators.SinkTransformationTranslator.SinkExpander#executionEnvironment:341 ",,akalashnikov,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 24 16:15:42 UTC 2023,,,,,,,,,,"0|z1dajk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/23 16:15;pnowojski;Merged to master as f38c2370e85;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkJoinToMultiJoinRule incorrectly combines Left/Right outer join to MultiJoin,FLINK-30270,13509165,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,02/Dec/22 06:50,27/Dec/22 06:39,13/Jul/23 08:13,27/Dec/22 06:39,1.16.0,,,,,,,1.17.0,,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"FlinkJoinToMultiJoinRule incorrectly combines Left/Right outer join to MultiJoin.  In some complex cases, it need to consider join conditions, ",,337361684@qq.com,godfrey,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 27 06:39:01 UTC 2022,,,,,,,,,,"0|z1da5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/22 08:28;martijnvisser;[~337361684@qq.com] With this ticket being a subticket on optimizing TPC-DS queries, it's not clear immediately if this is a bug or an improvement. Based on the description I think it's a bug, but then I don't think it should be listed as a ticket under an improvement story. WDYT? ;;;","22/Dec/22 09:07;337361684@qq.com;Thanks, [~martijnvisser]. I agree with you. I will remove this issue out and list it as a independent bug fix issue.;;;","27/Dec/22 06:39;godfrey;Fixed in master:7590cb7d84774b0e8afd7b0af31cc0af762d4c6a

1.16.1: ca42695dba5ab72c6b9b895bb6553321c30d5074;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HA metadata and other cluster submission related errors should not throw DeploymentFailedException,FLINK-30268,13508803,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pvary,gyfora,gyfora,01/Dec/22 22:11,06/Dec/22 13:42,13/Jul/23 08:13,06/Dec/22 13:42,,,,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Currently most critical cluster submission errors , and checks that validate HA metadata before deployment, end up throwing DeploymentFailedException.

This causes the operator to go into a weird state and actually hide the error in subsequent loops:


{noformat}
flink-kubernetes-operator 2022-12-01 21:55:03,978 o.a.f.k.o.l.AuditUtils         [INFO ][default/basic-checkpoint-ha-example] >>> Status | Info    | UPGRADING       | The resource is being upgraded 
flink-kubernetes-operator 2022-12-01 21:55:03,992 o.a.f.k.o.l.AuditUtils         [INFO ][default/basic-checkpoint-ha-example] >>> Event  | Info    | SUBMIT          | Starting deployment
flink-kubernetes-operator 2022-12-01 21:55:03,992 o.a.f.k.o.s.AbstractFlinkService [INFO ][default/basic-checkpoint-ha-example] Deploying application cluster requiring last-state from HA metadata
flink-kubernetes-operator 2022-12-01 21:55:03,997 o.a.f.k.o.c.FlinkDeploymentController [ERROR][default/basic-checkpoint-ha-example] Flink Deployment failed
flink-kubernetes-operator org.apache.flink.kubernetes.operator.exception.DeploymentFailedException: HA metadata not available to restore from last state. It is possible that the job has finished or terminally failed, or the configmaps have been deleted. Manual restore required.
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.service.AbstractFlinkService.validateHaMetadataExists(AbstractFlinkService.java:844)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.service.AbstractFlinkService.submitApplicationCluster(AbstractFlinkService.java:177)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.deploy(ApplicationReconciler.java:195)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.deploy(ApplicationReconciler.java:60)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractJobReconciler.restoreJob(AbstractJobReconciler.java:210)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractJobReconciler.reconcileSpecChange(AbstractJobReconciler.java:142)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:161)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:62)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:123)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:54)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:136)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:94)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:93)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:130)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:110)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:81)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:54)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:406)
flink-kubernetes-operator     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
flink-kubernetes-operator     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
flink-kubernetes-operator     at java.base/java.lang.Thread.run(Unknown Source)
flink-kubernetes-operator 2022-12-01 21:55:04,034 o.a.f.k.o.l.AuditUtils         [INFO ][default/basic-checkpoint-ha-example] >>> Event  | Warning | RESTOREFAILED   | HA metadata not available to restore from last state. It is possible that the job has finished or terminally failed, or the configmaps have been deleted. Manual restore required.
flink-kubernetes-operator 2022-12-01 21:55:04,034 o.a.f.k.o.c.FlinkDeploymentController [INFO ][default/basic-checkpoint-ha-example] End of reconciliation
flink-kubernetes-operator 2022-12-01 21:55:04,054 o.a.f.k.o.l.AuditUtils         [INFO ][default/basic-checkpoint-ha-example] >>> Status | Error   | UPGRADING       | {""type"":""org.apache.flink.kubernetes.operator.exception.DeploymentFailedException"",""message"":""HA metadata not available to restore from last state. It is possible that the job has finished or terminally failed, or the configmaps have been deleted. Manual restore required."",""additionalMetadata"":{""reason"":""RestoreFailed""},""throwableList"":[]} 
flink-kubernetes-operator 2022-12-01 21:55:19,056 o.a.f.k.o.c.FlinkDeploymentController [INFO ][default/basic-checkpoint-ha-example] Starting reconciliation
flink-kubernetes-operator 2022-12-01 21:55:19,058 o.a.f.k.o.r.d.AbstractFlinkResourceReconciler [INFO ][default/basic-checkpoint-ha-example] UPGRADE change(s) detected (FlinkDeploymentSpec[job.state=RUNNING] differs from FlinkDeploymentSpec[job.state=SUSPENDED]), starting reconciliation.
flink-kubernetes-operator 2022-12-01 21:55:19,092 o.a.f.k.o.l.AuditUtils         [INFO ][default/basic-checkpoint-ha-example] >>> Status | Info    | UPGRADING       | The resource is being upgraded 
flink-kubernetes-operator 2022-12-01 21:55:19,119 o.a.f.k.o.r.d.ApplicationReconciler [ERROR][default/basic-checkpoint-ha-example] Invalid status for deployment: FlinkDeploymentStatus(super=CommonStatus(jobStatus=JobStatus(jobName=CarTopSpeedWindowingExample, jobId=8d5c59b7e960984cd845b9977754d2ef, state=RECONCILING, startTime=1669931677233, updateTime=1669931696153, savepointInfo=SavepointInfo(lastSavepoint=null, triggerId=null, triggerTimestamp=null, triggerType=null, formatType=null, savepointHistory=[], lastPeriodicSavepointTimestamp=0)), error=null), clusterInfo={flink-version=1.15.2, flink-revision=69e8126 @ 2022-08-17T14:58:06+02:00}, jobManagerDeploymentStatus=ERROR, reconciliationStatus=FlinkDeploymentReconciliationStatus(super=ReconciliationStatus(reconciliationTimestamp=1669931719059, lastReconciledSpec={""spec"":{""job"":{""jarURI"":""local:///opt/flink/examples/streaming/TopSpeedWindowing.jar"",""parallelism"":2,""entryClass"":null,""args"":[],""state"":""suspended"",""savepointTriggerNonce"":0,""initialSavepointPath"":null,""upgradeMode"":""last-state"",""allowNonRestoredState"":null},""restartNonce"":2,""flinkConfiguration"":{""high-availability"":""org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory"",""high-availability.storageDir"":""file:///flink-data/ha"",""state.checkpoints.dir"":""file:///flink-data/checkpoints"",""state.savepoints.dir"":""file:///flink-data/savepoints"",""taskmanager.numberOfTaskSlots"":""2""},""image"":""flink:1.15"",""imagePullPolicy"":null,""serviceAccount"":""flink"",""flinkVersion"":""v1_15"",""ingress"":null,""podTemplate"":{""apiVersion"":""v1"",""kind"":""Pod"",""spec"":{""containers"":[{""name"":""flink-main-container"",""volumeMounts"":[{""mountPath"":""/flink-data"",""name"":""flink-volume""}]}],""volumes"":[{""hostPath"":{""path"":""/tmp/flink"",""type"":""Directory""},""name"":""flink-volume""}]}},""jobManager"":{""resource"":{""cpu"":1.0,""memory"":""2048m""},""replicas"":1,""podTemplate"":null},""taskManager"":{""resource"":{""cpu"":1.0,""memory"":""2048m""},""replicas"":null,""podTemplate"":null},""logConfiguration"":null,""mode"":null},""resource_metadata"":{""apiVersion"":""flink.apache.org/v1beta1"",""metadata"":{""generation"":5},""firstDeployment"":false}}, lastStableSpec=null, state=UPGRADING)), taskManager=TaskManagerInfo(labelSelector=, replicas=0))
flink-kubernetes-operator 2022-12-01 21:55:19,133 o.a.f.k.o.l.AuditUtils         [INFO ][default/basic-checkpoint-ha-example] >>> Event  | Warning | CLUSTERDEPLOYMENTEXCEPTION | This indicates a bug...
flink-kubernetes-operator 2022-12-01 21:55:19,136 o.a.f.k.o.r.ReconciliationUtils [WARN ][default/basic-checkpoint-ha-example] Attempt count: 0, last attempt: false
flink-kubernetes-operator 2022-12-01 21:55:19,163 o.a.f.k.o.l.AuditUtils         [INFO ][default/basic-checkpoint-ha-example] >>> Status | Error   | UPGRADING       | {""type"":""org.apache.flink.kubernetes.operator.exception.ReconciliationException"",""message"":""java.lang.RuntimeException: This indicates a bug..."",""throwableList"":[{""type"":""java.lang.RuntimeException"",""message"":""This indicates a bug...""}]} 
flink-kubernetes-operator 2022-12-01 21:55:19,164 i.j.o.p.e.ReconciliationDispatcher [ERROR][default/basic-checkpoint-ha-example] Error during event processing ExecutionScope{ resource id: ResourceID{name='basic-checkpoint-ha-example', namespace='default'}, version: 350553} failed.
flink-kubernetes-operator org.apache.flink.kubernetes.operator.exception.ReconciliationException: java.lang.RuntimeException: This indicates a bug...
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:133)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:54)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:136)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:94)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:93)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:130)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:110)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:81)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:54)
flink-kubernetes-operator     at io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:406)
flink-kubernetes-operator     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
flink-kubernetes-operator     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
flink-kubernetes-operator     at java.base/java.lang.Thread.run(Unknown Source)
flink-kubernetes-operator Caused by: java.lang.RuntimeException: This indicates a bug...
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.deploy(ApplicationReconciler.java:180)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.deploy(ApplicationReconciler.java:60)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractJobReconciler.restoreJob(AbstractJobReconciler.java:210)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractJobReconciler.reconcileSpecChange(AbstractJobReconciler.java:142)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:161)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:62)
flink-kubernetes-operator     at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:123)
flink-kubernetes-operator     ... 13 more
{noformat}

The main cause here is that DeploymentFailedExceptions were originally created so that the observer could signal a JobManager deployment failure (after it was submitted). Thus the error handler logic in the controller actually updates the jmDeploymentStatus and the job state which causes the problem.

To avoid this we should introduce a new Exception type or use something more suitable. We should not touch touch the jobmanagerDeploymentStatus or the jobstatus in most of these cases and simply retrigger the reconciliation. This will keep the CR in an error loop triggering warnings etc but that is expected in these critical failure scenarios.",,gyfora,mbalassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 13:42:00 UTC 2022,,,,,,,,,,"0|z1d7xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/22 13:42;mbalassi;[{{310ff30}}|https://github.com/apache/flink-kubernetes-operator/commit/310ff3072cd6196202ac37a171a896d3359cfc56] in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Recovery reconciliation loop fails if no checkpoint has been created yet,FLINK-30266,13507876,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,mxm,mxm,01/Dec/22 14:42,05/Dec/22 22:59,13/Jul/23 08:13,05/Dec/22 22:59,kubernetes-operator-1.3.0,,,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"When the upgradeMode is LAST-STATE, the operator fails to reconcile a failed application unless at least one checkpoint has already been created. The expected behavior would be that the job starts with empty state.
{noformat}
2022-12-01 10:58:35,596 o.a.f.k.o.l.AuditUtils         [INFO ] [app] >>> Status | Error   | UPGRADING       | {""type"":""org.apache.flink.kubernetes.operator.exception.DeploymentFailedException"",""message"":""HA metadata not available to restore from last state. It is possible that the job has finished or terminally failed, or the configmaps have been deleted. Manual restore required."",""additionalMetadata"":{""reason"":""RestoreFailed""},""throwableList"":[]} {noformat}
{noformat}
2022-12-01 10:44:49,480 i.j.o.p.e.ReconciliationDispatcher [ERROR] [app] Error during event processing ExecutionScope{ resource id: ResourceID{name='app', namespace='namespace'}, version: 216933301} failed.
org.apache.flink.kubernetes.operator.exception.ReconciliationException: java.lang.RuntimeException: This indicates a bug...
	at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:133)
	at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:54)
	at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:136)
	at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:94)
	at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80)
	at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:93)
	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:130)
	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:110)
	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:81)
	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:54)
	at io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:406)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
Caused by: java.lang.RuntimeException: This indicates a bug...
	at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.deploy(ApplicationReconciler.java:180)
	at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.deploy(ApplicationReconciler.java:61)
	at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractJobReconciler.restoreJob(AbstractJobReconciler.java:212)
	at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractJobReconciler.reconcileSpecChange(AbstractJobReconciler.java:144)
	at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:167)
	at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:64)
	at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:123)
	... 13 more {noformat}",,gyfora,morhidi,mxm,thw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 05 22:59:56 UTC 2022,,,,,,,,,,"0|z1d27k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/22 19:18;thw;I believe this was discussed before and the reason we decided to not allow this was that we cannot safely determine the reason why the HA metadata is missing. It could be because there was never any successful checkpoint or because it was removed by mistake? As long as we can ensure that we don't accidentally reset a job with prior state to empty state I would also prefer the solution that does not involve manual intervention.;;;","05/Dec/22 16:15;morhidi;[~gyfora], can this be closed?;;;","05/Dec/22 22:59;gyfora;Merged to main 72ad3639e60fbf27dd408dabbe69f46d69ff52f9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The flame graph type is wrong,FLINK-30250,13507437,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,30/Nov/22 11:09,12/Dec/22 07:34,13/Jul/23 08:13,12/Dec/22 03:40,1.15.0,1.16.0,1.17.0,,,,,1.17.0,,,,,Runtime / REST,Runtime / Web Frontend,,,,,,0,pull-request-available,,,,"When the flame graph type is switched from On-CPU to Mixed. It still show the graph of On-CPU.
h2. Root cause:

When click the other types, the web frontend will call the requestFlameGraph and update the graphType. However, the graphType is the old type during requestFlameGraph. So the graph type show the new type, but the flame graph is the result of old type.

 [code link |https://github.com/apache/flink/blob/8bbf52688758bbede45df060a4c11e5fa228b6f0/flink-runtime-web/web-dashboard/src/app/pages/job/overview/flamegraph/job-overview-drawer-flamegraph.component.ts#L82]

!image-2022-11-30-19-08-42-067.png|width=1026,height=389!",,fanrui,junhan,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13550,,,,,,,,,,,,,"30/Nov/22 11:08;fanrui;image-2022-11-30-19-08-42-067.png;https://issues.apache.org/jira/secure/attachment/13053327/image-2022-11-30-19-08-42-067.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 12 03:40:20 UTC 2022,,,,,,,,,,"0|z1czi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/22 08:15;xtsong;[~junhan], could you please take a look at this?;;;","09/Dec/22 02:21;junhan;[~fanrui] I left some comments in the PR :);;;","09/Dec/22 07:02;fanrui;[~xtsong] [~junhan] thanks for your help and review :);;;","12/Dec/22 03:40;junhan;master: 6af92a3eda4d243f765f6ea0c8c548ca70ca423a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableUtils.getRowTypeInfo() creating wrong TypeInformation,FLINK-30249,13507426,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,30/Nov/22 09:54,06/Jan/23 06:24,13/Jul/23 08:13,06/Jan/23 06:24,ml-2.0.0,ml-2.1.0,,,,,,ml-2.2.0,,,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,,,zhangzp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-30 09:54:03.0,,,,,,,,,,"0|z1czfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"NPE thrown when filtering decimal(18, 4) values after calling DecimalDataUtils.subtract method",FLINK-30245,13507394,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhongwei,zhongwei,zhongwei,30/Nov/22 07:16,06/Dec/22 07:54,13/Jul/23 08:13,06/Dec/22 07:53,1.13.6,1.17.0,,,,,,1.17.0,,,,,Table SQL / Runtime,,,,,,,0,pull-request-available,,,,"Reproduce code:
{code:java}
        TableEnvironment tableEnv = TableEnvironment.create(EnvironmentSettings.newInstance().build());

        tableEnv.executeSql(""create table datagen_source1 (disburse_amount int) with ('connector' = 'datagen')"");

        tableEnv.executeSql(""create table print_sink (disburse_amount Decimal(18,4)) with ('connector' = 'print')"");

        tableEnv.executeSql(""create view mid as select cast(disburse_amount as Decimal(18,4)) - cast(disburse_amount as Decimal(18,4)) as disburse_amount from datagen_source1"");

        tableEnv.executeSql(""insert into print_sink select * from mid where disburse_amount > 0 "").await();
{code}
Excpetion:
{code:java}
Exception in thread ""main"" java.util.concurrent.ExecutionException: org.apache.flink.table.api.TableException: Failed to wait job finish
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.table.api.internal.TableResultImpl.awaitInternal(TableResultImpl.java:118)
	at org.apache.flink.table.api.internal.TableResultImpl.await(TableResultImpl.java:81)
	at com.shopee.flink.BugExample2.main(BugExample2.java:21)
Caused by: org.apache.flink.table.api.TableException: Failed to wait job finish
	at org.apache.flink.table.api.internal.InsertResultProvider.hasNext(InsertResultProvider.java:85)
	at org.apache.flink.table.api.internal.InsertResultProvider.isFirstRowReady(InsertResultProvider.java:71)
	at org.apache.flink.table.api.internal.TableResultImpl.lambda$awaitInternal$1(TableResultImpl.java:105)
	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.table.api.internal.InsertResultProvider.hasNext(InsertResultProvider.java:83)
	... 6 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:267)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1277)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
	at akka.dispatch.OnComplete.internal(Future.scala:300)
	at akka.dispatch.OnComplete.internal(Future.scala:297)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:622)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1067)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1703)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:172)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:739)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:716)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479)
	at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
	at akka.actor.Actor.aroundReceive(Actor.scala:537)
	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579)
	at akka.actor.ActorCell.invoke(ActorCell.scala:547)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	... 4 more
Caused by: java.lang.NullPointerException
	at org.apache.flink.table.data.DecimalDataUtils.compare(DecimalDataUtils.java:217)
	at StreamExecCalc$17.processElement_split1(Unknown Source)
	at StreamExecCalc$17.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.streaming.api.functions.source.datagen.DataGeneratorSource.run(DataGeneratorSource.java:120)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
{code}
Root cause:

For above sql, the generated StreamExecCalc has following code:
{code:java}
          isNull$299 = externalResult$298 == null;
          result$299 = null;
          if (!isNull$299) {
            result$299 = externalResult$298;
          }
          
          isNull$300 = isNull$296 || isNull$299;
          result$301 = null;
          if (!isNull$300) {
            
          
          result$301 = org.apache.flink.table.data.DecimalDataUtils.subtract(result$296, result$299, 19, 4);  // note the preciesion is 19
          
            isNull$300 = (result$301 == null);
          }
          
          
          isNull$302 = isNull$300 || false;
          result$303 = false;
          if (!isNull$302) {
            
          
          result$303 = org.apache.flink.table.data.DecimalDataUtils.compare(result$301, ((int) 0)) < 0;
          
            
          }
{code}
It seems the precision param of the DecimalDataUtils.subtract method is 19 rather than 18, but the precision of DecimalData value (result$296, result$299) is still 18. So the isCompact() method still returns true. Finally, this method will generate a problematic DecimalData:

!image-2022-11-30-15-11-03-706.png|width=559,height=277!

The returned DecimalData is not compacted (precision > MAX_LONG_DIGITS == 18). When comparing it with other int value, the decimalVal will be used, but for this value, the decimalVal is null. So the NPE thrown.

We found it on flink 1.13 and the latest master branch. Other versions of flink have not been tested, but there should be this bug.",,jark,zhongwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Nov/22 07:11;zhongwei;image-2022-11-30-15-11-03-706.png;https://issues.apache.org/jira/secure/attachment/13053312/image-2022-11-30-15-11-03-706.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 07:54:13 UTC 2022,,,,,,,,,,"0|z1cz8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/22 10:36;zhongwei;The precision change (18 -> 19) is caused by org.apache.calcite.rel.type.RelDataTypeSystem.deriveDecimalPlusType.;;;","30/Nov/22 11:01;zhongwei;[~jark] Could you help to take a look at this issue?;;;","30/Nov/22 11:41;jark;cc [~lsy] do you have time to have a look at this?;;;","06/Dec/22 07:53;jark;Fixed in 
 - master: bca57b7a222615869ddc38acdeb46c436b1bc0b3
 - release-1.16: TODO
 - release-1.15: TODO;;;","06/Dec/22 07:54;jark;[~zhongwei] could you open PRs for release-1.16 and release-1.15 branches? 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The flame graph doesn't work due to groupExecutionsByLocation has bug,FLINK-30239,13507290,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,29/Nov/22 16:17,06/Dec/22 08:57,13/Jul/23 08:13,06/Dec/22 08:06,1.16.0,1.17.0,,,,,,1.16.1,1.17.0,,,,Runtime / REST,Runtime / Web Frontend,,,,,,0,pull-request-available,,,,"The flame graph cannot be generated forever when multiple tasks in the same TM. It's caused by FLINK-26074

 
h1. Root cause:

A Set cannot be converted to an ImmutableSet during the aggregation of ExecutionAttemptIDs. It will cause only the first ExecutionAttemptID of the TM to be added to the set, the second ExecutionAttemptID will fail.

 

!image-2022-11-30-00-14-11-355.png!

 

 

 

!image-2022-11-30-00-11-09-728.png!

 

Exception Info: 

!image-2022-11-30-00-10-48-940.png!",,fanrui,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26074,,,,,,,,,,,,,"29/Nov/22 16:10;fanrui;image-2022-11-30-00-10-48-940.png;https://issues.apache.org/jira/secure/attachment/13053286/image-2022-11-30-00-10-48-940.png","29/Nov/22 16:11;fanrui;image-2022-11-30-00-11-09-728.png;https://issues.apache.org/jira/secure/attachment/13053285/image-2022-11-30-00-11-09-728.png","29/Nov/22 16:14;fanrui;image-2022-11-30-00-14-11-355.png;https://issues.apache.org/jira/secure/attachment/13053284/image-2022-11-30-00-14-11-355.png",,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 08:06:04 UTC 2022,,,,,,,,,,"0|z1cylc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/22 08:06;xtsong;- master (1.17): 95be1e5abfe78d7ed8d171a779f280277f51569c
- release-1.16: 0f98c6aad1f5bc8ad25f7608c419a5e396b8e8ac;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
shading of netty epoll shared library does not account for ARM64 platform,FLINK-30232,13506892,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,cthomson,cthomson,28/Nov/22 22:05,19/Jan/23 09:13,13/Jul/23 08:13,19/Jan/23 09:13,1.15.2,,,,,,,shaded-17.0,,,,,BuildSystem / Shaded,,,,,,,0,pull-request-available,,,,"While evaluating migration of Flink application to Graviton 2 based EC2 instances in a AWS managed Kubernetes service (EKS) using Kubernetes 1.23, found that the shaded Netty library renames the AMD64 version of the shared library as part of relocation of the Netty library but does not rename the matching ARM64 shared library. This results in the following error when `taskmanager.network.netty.transport: epoll` is used:



 

 

{{Suppressed: java.lang.UnsatisfiedLinkError: no org_apache_flink_shaded_netty4_netty_transport_native_epoll in java.library.path}}
{{at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1860) ~[?:1.8.0_352]}}
{{at java.lang.Runtime.loadLibrary0(Runtime.java:843) ~[?:1.8.0_352]}}
{{at java.lang.System.loadLibrary(System.java:1136) ~[?:1.8.0_352]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:38) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_352]}}
{{at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_352]}}
{{at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_352]}}
{{at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_352]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader$1.run(NativeLibraryLoader.java:335) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_352]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.loadLibraryByHelper(NativeLibraryLoader.java:327) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:293) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:136) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.loadNativeLibrary(Native.java:309) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.<clinit>(Native.java:85) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Epoll.<clinit>(Epoll.java:40) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.<clinit>(EpollEventLoop.java:51) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoopGroup.newChild(EpollEventLoopGroup.java:185) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoopGroup.newChild(EpollEventLoopGroup.java:36) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:84) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:60) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:49) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:59) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoopGroup.<init>(EpollEventLoopGroup.java:113) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoopGroup.<init>(EpollEventLoopGroup.java:100) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoopGroup.<init>(EpollEventLoopGroup.java:77) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.io.network.netty.NettyClient.initEpollBootstrap(NettyClient.java:164) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.io.network.netty.NettyClient.init(NettyClient.java:79) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.start(NettyConnectionManager.java:87) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.io.network.NettyShuffleEnvironment.start(NettyShuffleEnvironment.java:329) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerServices.fromConfiguration(TaskManagerServices.java:293) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.startTaskManager(TaskManagerRunner.java:623) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.createTaskExecutorService(TaskManagerRunner.java:559) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.startTaskManagerRunnerServices(TaskManagerRunner.java:245) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.start(TaskManagerRunner.java:288) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManager(TaskManagerRunner.java:481) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.lambda$runTaskManagerProcessSecurely$5(TaskManagerRunner.java:525) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManagerProcessSecurely(TaskManagerRunner.java:525) [flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManagerProcessSecurely(TaskManagerRunner.java:505) [flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.kubernetes.taskmanager.KubernetesTaskExecutorRunner.main(KubernetesTaskExecutorRunner.java:39) [flink-dist-1.15.2.jar:1.15.2]}}
{{Caused by: java.io.FileNotFoundException: META-INF/native/liborg_apache_flink_shaded_netty4_netty_transport_native_epoll_aarch_64.so}}
{{at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:170) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.loadNativeLibrary(Native.java:306) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.<clinit>(Native.java:85) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Epoll.<clinit>(Epoll.java:40) ~[flink-dist-1.15.2.jar:1.15.2]}}
{{... 25 more}}

 

[https://github.com/apache/flink-shaded/blob/3082afc952e68366e9fefe4d1181c4666969ee67/flink-shaded-netty-4/pom.xml#L97] appears to be where the problem is, it only renames the x86_64 shared library, it doesn’t account for aarch_64 shared library.","Kubernetes 1.23 provided by AWS managed Kubernetes service (EKS) with Graviton 2 based EC2 instances (ARM64) using Flink 1.15.2, native epoll enabled (taskmanager.network.netty.transport: epoll)",cthomson,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 19 09:13:11 UTC 2023,,,,,,,,,,"0|z1cw4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/22 07:55;martijnvisser;[~cthomson] Thanks for the report! Do you want to open a PR to fix this for a future version?;;;","29/Nov/22 14:36;cthomson;Our team is trying to get approval to contribute the fix for this back to the Apache Flink project, I'll know in two weeks if that is an option.;;;","19/Dec/22 19:39;martijnvisser;[~cthomson] Have you heard anything? Else I can take a look myself;;;","20/Dec/22 14:07;cthomson;We haven't heard anything yet, more likely to be the latter part of January before we get feedback on one of our team members being able to contribute.;;;","21/Dec/22 11:33;martijnvisser;I've created https://github.com/apache/flink-shaded/pull/118 so that we can include it before the 1.17 feature freeze;;;","19/Jan/23 09:13;martijnvisser;Fixed in master: ba56570bbe395385c17240a2119c1c2cf8141675;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Suspended a job in last-state mode bug,FLINK-30222,13506106,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pvary,tanjialiang,tanjialiang,27/Nov/22 08:54,29/Nov/22 11:32,13/Jul/23 08:13,29/Nov/22 11:32,1.16.0,kubernetes-operator-1.2.0,,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"In flink 1.16.0, it support set kubernetes HA with options 'kubernetes', such as 'high-availability: kubernetes'. But in kubernetes operator 1.2.0, I try to suspended a job in last-state mode, it validate fail, because of 'Job could not be upgraded with last-state while Kubernetes HA disabled'.

 

I try to use kubectl patch to supsended a job with last-state
{code:sh}
kubectl -nbigdata-flink patch flinkdeployments.flink.apache.org/streaming-638223bf650ac869689faa62-flink --type=merge -p '{""spec"": {""job"":
{""state"": ""suspended"", ""upgradeMode"": ""last-state""}{code}
it found an error, because my kubernetes HA is disabled
{code:java}
Error from server: admission webhook ""flinkoperator.flink.apache.org"" denied the request: Job could not be upgraded with last-state while Kubernetes HA disabled {code}
but i enabled kubernetes HA with this follow options:
{code:yaml}
kubernetes.cluster-id: <cluster-id>
high-availability: kubernetes
high-availability.storageDir: hdfs:///flink/recovery {code}
and i found flink kubernetes operator 1.2.0 validate the kubernetes HA in the old options:
{code:yaml}
high-availability: org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory {code}
it may be in the org.apache.flink.kubernetes.operator.utils.FlinkUtils#isKubernetesHAActivated to judge.

!image-2022-11-27-16-48-08-445.png!",,gyfora,mbalassi,tanjialiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/22 08:48;tanjialiang;image-2022-11-27-16-48-08-445.png;https://issues.apache.org/jira/secure/attachment/13053166/image-2022-11-27-16-48-08-445.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 11:32:42 UTC 2022,,,,,,,,,,"0|z1craw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Nov/22 11:03;gyfora;Good catch! Would you like to work on this ticket?

The current workaround is to set:
```
    high-availability: org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory
```
instead of simply `kubernetes`;;;","29/Nov/22 11:32;mbalassi;[{{458022d}}|https://github.com/apache/flink-kubernetes-operator/commit/458022d2e67247c9941f102fb39d9dda96bd8837] in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The resource is not enough,FLINK-30214,13505903,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,25/Nov/22 09:23,29/Nov/22 16:50,13/Jul/23 08:13,29/Nov/22 15:38,,,,,,,,1.17.0,,,,,Runtime / Configuration,,,,,,,0,pull-request-available,,,,"When turn up the parallelism, the resources isn't enough for MiniCluster. I'm not sure whether the flink on yarn or k8s is right? I can test later.

 

I guess the parallelism should be changed in a right place. And we should add more unit test or ITCase to check these cases. And it's my honor to make these improvements and bug fixes.

!image-2022-11-25-17-18-36-935.png!

 

!image-2022-11-25-17-23-14-208.png!",,fanrui,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/22 09:18;fanrui;image-2022-11-25-17-18-36-935.png;https://issues.apache.org/jira/secure/attachment/13053116/image-2022-11-25-17-18-36-935.png","25/Nov/22 09:23;fanrui;image-2022-11-25-17-23-14-208.png;https://issues.apache.org/jira/secure/attachment/13053115/image-2022-11-25-17-23-14-208.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 15:38:04 UTC 2022,,,,,,,,,,"0|z1cq1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/22 09:24;fanrui;Hi [~gyfora] [~mxm] , please help take a look, thanks~:);;;","25/Nov/22 09:36;gyfora;Hi [~fanrui],
Thanks for jumping on this and testing it.
Probably the minicluster is started with fix resources and that is not enough after scaling up. You could try to allocate more tas slots in the minicluster than the job initially needs so you can scale up.;;;","25/Nov/22 10:22;fanrui;Hi [~gyfora] , thanks for your quick feedback.

I run a flink job in IDEA, I didn't set any conf for the minicluster. Flink helps user start the minicluster by maximumParallelism of all tasks.

This is my demo: [https://github.com/1996fanrui/flink/commit/52692249c86cbc366f3e5fb54d7744da75586972]

And I have updated the PR, it can fix this bug. In other words: minicluster should adapt {_}jobvertex-parallelism-overrides{_}.

 

And I will add some tests later.

 

 ;;;","29/Nov/22 15:38;gyfora;merged to master ee4d5b8d65768776f070ccc0c28e3fe3da8e10c6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The edge is wrong when the vertex parallelism is changed,FLINK-30213,13505895,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,fanrui,fanrui,25/Nov/22 09:08,12/Jan/23 15:24,13/Jul/23 08:13,02/Jan/23 13:07,1.17.0,,,,,,,1.17.0,,,,,Runtime / Configuration,,,,,,,0,pull-request-available,,,,"After FLINK-29501, flink allows overriding JobVertex parallelisms during job submission.

However, the edge should be changed as well. For example, the job has 4 vertex, and all shipStrategyName of all tasks are forward.

After the parallelism of the third task is changed to 1, the second and third edge should be changed from forward to rebalance. But they are still forward.

 

And from the second picture, the subtask_1 of sink cannot receive any data.

 

!image-2022-11-25-17-02-12-492.png|width=1318,height=356!

!image-2022-11-25-17-06-53-466.png!",,fanrui,gyfora,huwh,mxm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29501,,,,,,,,,,,,,"25/Nov/22 09:02;fanrui;image-2022-11-25-17-02-12-492.png;https://issues.apache.org/jira/secure/attachment/13053114/image-2022-11-25-17-02-12-492.png","25/Nov/22 09:06;fanrui;image-2022-11-25-17-06-53-466.png;https://issues.apache.org/jira/secure/attachment/13053113/image-2022-11-25-17-06-53-466.png","25/Nov/22 20:58;gyfora;image-2022-11-25-21-58-48-968.png;https://issues.apache.org/jira/secure/attachment/13053154/image-2022-11-25-21-58-48-968.png",,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 12 15:24:12 UTC 2023,,,,,,,,,,"0|z1cq00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/22 09:13;fanrui;Hi [~gyfora] [~mxm] , please help take a look, and I can fix this bug. 

BTW, I am very interested in AutoScaling and hope to participate due to it's a very useful feature for our company.;;;","25/Nov/22 14:24;mxm;Thanks for reporting this [~fanrui]! I think we'll have to patch the forward partitioner in this scenario. All other partitioners should continue to work with new parallelisms. I'll have a look.;;;","25/Nov/22 20:59;gyfora;The same issue already existed Flink with the reactive scaling logic. 
Take the following pipeline as an example:


{code:java}
public class RescaleTest {

    public static void main(String[] args) throws Exception {
        var env = StreamExecutionEnvironment.getExecutionEnvironment();
        var source = env.fromSequence(0, 100000000L).setParallelism(1);
        source.getTransformation().setMaxParallelism(1);

        source.disableChaining()
                .filter(
                        val -> {
                            Thread.sleep(1000);
                            return true;
                        })
                .print();

        env.execute();
    }
} {code}

If you start this on a cluster with 1 taskslot (parallelism = 1), that will create forward partitioning between the source and the filter.
Scaling it up by adding one more taskmanager will increase the parallelism of the filter but not the source (due to the forced max parallelism 1)
leading to the same problem:

!image-2022-11-25-21-58-48-968.png!
Furthermore the UI reports incorrect parallelism numbers but thats just cosmetics :) ;;;","02/Jan/23 13:07;gyfora;Merged to main:
ded2df542fd5d585842e77d021fb84a92a5bea76..1c2f4eb4ba1d1b5ad403d7991170ca16cb71df56

I am sorry I accidentally rebased the PR instead of squashing the commits. This was a mistake from my side.;;;","12/Jan/23 15:24;gyfora;Added a fix for this: fb482fe39844efda33a4c05858903f5b64e158a3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RateLimitedSourceReader may emit one more record than permitted,FLINK-30202,13505858,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,mapohl,mapohl,25/Nov/22 04:44,04/Dec/22 20:32,13/Jul/23 08:13,04/Dec/22 20:32,1.17.0,,,,,,,1.17.0,,,,,Connectors / Common,,,,,,,0,pull-request-available,test-stability,,," [This build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43483&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24747] failed due to a failed assertion in {{{}DataGeneratorSourceITCase.testGatedRateLimiter{}}}:
{code:java}
Nov 25 03:26:45 org.opentest4j.AssertionFailedError: 
Nov 25 03:26:45 
Nov 25 03:26:45 expected: 2
Nov 25 03:26:45  but was: 1 {code}",,mapohl,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Dec 04 20:32:44 UTC 2022,,,,,,,,,,"0|z1cprs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/22 10:33;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43519&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24747;;;","28/Nov/22 10:36;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43530&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24745;;;","30/Nov/22 06:55;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43604&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24751;;;","01/Dec/22 09:01;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43636&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24837;;;","02/Dec/22 10:36;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43662&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24837;;;","04/Dec/22 20:32;chesnay;master: 81ed6c649e1a219c457628c54a7165d75b803474;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HsSubpartitionFileReader may load data that has been consumed from memory,FLINK-30189,13505721,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,24/Nov/22 09:07,07/Dec/22 04:39,13/Jul/23 08:13,07/Dec/22 04:39,1.16.0,1.17.0,,,,,,1.16.1,1.17.0,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,"In order to solve the problem that data cannot be read from the disk correctly after failover, we changed the calculation logical of the buffer's readable state in FLINK-29238.  Buffers that are greater than consumingOffset and have been released can be pre-load from file. However, the update of consumingOffset is asynchronous, If it lags behind the actual consumption progress, the buffer will have a chance to be load from the disk again. 

IMO, we can record the consumed status of buffer by each consumer in the InternalRegion. Only the buffers that have not been consumed and have been released will be considered as readable. In the case of failover, a new consumerId will be generated, so all buffers will be considered as unconsumed and can be correctly read from the disk too.",,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29419,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 07 04:39:24 UTC 2022,,,,,,,,,,"0|z1coxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/22 04:39;xtsong;- master (1.17): 4c67f8fca529a72389d69990307bbf78fcd3d99d
- release-1.16: a9e65bc2377ee7a4b3599b58a58ff0301b79c5d8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multi session job test is silently not checking the operator log for errors,FLINK-30187,13505711,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,24/Nov/22 08:19,24/Nov/22 16:19,13/Jul/23 08:13,24/Nov/22 16:19,,,,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"{code:java}
Checking for operator log errors...
error: You must provide one or more resources by argument or filename.
Example resource specifications include:
   '-f rsrc.yaml'
   '--filename=rsrc.json'
   '<resource> <name>'
   '<resource>'
No errors in log files.
{code}
",,gaborgsomogyi,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 16:19:10 UTC 2022,,,,,,,,,,"0|z1cov4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 16:19;gyfora;merged to main b8483822516c526b8da19e031a2c695c2c59f207;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
assert_available_slots is passing even if curl is timing out,FLINK-30176,13505550,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,23/Nov/22 13:52,24/Nov/22 16:07,13/Jul/23 08:13,24/Nov/22 16:07,,,,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,gaborgsomogyi,mbalassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 16:07:30 UTC 2022,,,,,,,,,,"0|z1cnvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 16:07;mbalassi;[4027fce|https://github.com/apache/flink-kubernetes-operator/commit/4027fcea70644552eb876d2b2f2db95c452ae5b7] in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink Deserialization Error with Object Array,FLINK-30168,13505522,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,yunfengzhou,yunfengzhou,23/Nov/22 11:52,16/Jan/23 01:43,13/Jul/23 08:13,16/Jan/23 01:43,1.15.2,1.16.0,,,,,,1.15.4,1.16.1,1.17.0,,,API / Python,,,,,,,0,pull-request-available,,,,"When it is attempted to collect object array records from a DataStream in PyFlink, an exception like follows would be thrown
{code:java}
data = 0, field_type = DenseVectorTypeInfo
def pickled_bytes_to_python_converter(data, field_type):if isinstance(field_type, RowTypeInfo):
row_kind = RowKind(int.from_bytes(data[0], 'little'))
data = zip(list(data[1:]), field_type.get_field_types())
fields = []for d, d_type in data:
fields.append(pickled_bytes_to_python_converter(d, d_type))
row = Row.of_kind(row_kind, *fields)return rowelse:
> data = pickle.loads(data)
E TypeError: a bytes-like object is required, not 'int'{code}
I found that this error is invoked because PyFlink deals with object arrays differently on Java side and Python side. 

 

On Java side (org.apache.flink.api.common.python.PythonBridgeUtils.getPickledBytesFromJavaObject)
{code:java}
...
else if (dataType instanceof BasicArrayTypeInfo || dataType instanceof PrimitiveArrayTypeInfo) {
# recursively deal with array elements
} ...
else {
# ObjectArrayTypeInfo is here
TypeSerializer serializer = dataType.createSerializer(null); ByteArrayOutputStreamWithPos baos = new ByteArrayOutputStreamWithPos(); DataOutputViewStreamWrapper baosWrapper = new DataOutputViewStreamWrapper(baos); serializer.serialize(obj, baosWrapper); return pickler.dumps(baos.toByteArray());
}
{code}
 

On python side(pyflink.datastream.utils.pickled_bytes_to_python_converter)
{code:java}
...
elif isinstance(field_type,
(BasicArrayTypeInfo, PrimitiveArrayTypeInfo, ObjectArrayTypeInfo)):
  element_type = field_type._element_type
  elements = []
  for element_bytes in data:
    elements.append(pickled_bytes_to_python_converter(element_bytes, element_type))
  return elements{code}
 

Thus a possible fix for this bug is to align PyFlink's behavior on Java side and Python side.",,dianfu,hxb,yunfengzhou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 01:43:05 UTC 2023,,,,,,,,,,"0|z1cnp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/22 03:53;yunfengzhou;Apart from the proposed solution mentioned in the Description section, PyFlink needs to modify its behavior against None values as well. For example, the following code in PythonBridgeUtils.getPickledBytesFromJavaObject
{code:java}
if (obj == null) {
    return new byte[0];
} else {
{code}
might be modified into
{code:java}
if (obj == null) {
    return pickler.dumps(null);
} else {
{code}
 ;;;","11/Jan/23 05:26;dianfu;[~yunfengzhou] Could you share some code snippets which could reproduce the above issues?;;;","12/Jan/23 01:39;yunfengzhou;Thanks for the reminding. Here are the code snippets that could be used to reproduce the above issues. I reproduced the errors with these codes in Flink 1.15.

 
{code:java}
import unittest

from pyflink.common import Types
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment


class PyFlinkTest(unittest.TestCase):
    def setUp(self) -> None:
        self.env = StreamExecutionEnvironment.get_execution_environment()
        self.t_env = StreamTableEnvironment.create(self.env)

    def test_object_array(self):
        double_arrays = [
            ([0.0, 0.0],),
            ([0.0, 1.0],),
        ]

        input_table = self.t_env.from_data_stream(
            self.env.from_collection(
                double_arrays,
                type_info=Types.ROW_NAMED(
                    ['f0'],
                    [Types.OBJECT_ARRAY(Types.DOUBLE())]
                )
            )
        )

        input_table.print_schema()

        print([x for x in self.t_env.to_data_stream(input_table).execute_and_collect()])

    def test_none(self):
        string_array_with_none = [
            ([""test"", ""test""], ),
            ([None, ], )
        ]

        input_table = self.t_env.from_data_stream(
            self.env.from_collection(
                string_array_with_none,
                type_info=Types.ROW_NAMED(
                    ['f0', ],
                    [Types.OBJECT_ARRAY(Types.STRING()), ]
                )
            )
        )

        input_table.print_schema()

        print([x for x in self.t_env.to_data_stream(input_table).execute_and_collect()])

 {code};;;","12/Jan/23 02:08;dianfu;[~yunfengzhou] Thanks a lot (y);;;","16/Jan/23 01:43;dianfu;Fixed in:
- master via 46757739cf50c1e7b7305a4bc9cf779bb1945a1f
- release-1.16 via 46c91ed4bc22e2de3a662d52b11ade8ed64dba0b
- release-1.15 via cdecc21cad9f78b1555a0e2f5d7f1398949e7193;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flinkConfiguration in SessionJob is not accepted,FLINK-30154,13505353,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gsomogyi,eleroy,eleroy,22/Nov/22 13:42,06/Dec/22 13:45,13/Jul/23 08:13,06/Dec/22 13:45,kubernetes-operator-1.2.0,,,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"when declaring extra config in the FlinkSessionJob like in :

```yaml
apiVersion: flink.apache.org/v1beta1
kind: FlinkSessionJob
metadata:
  name: flink-state
  namespace: flink
spec:
  deploymentName: flink-session
  flinkConfiguration:
    kubernetes.operator.periodic.savepoint.interval: 30m
    kubernetes.operator.savepoint.history.max.age: 24h
    kubernetes.operator.savepoint.history.max.count: ""25""
  job:
    jarURI: 
[https://myendpoint/myjar.jar]

    parallelism: 2
    entryClass: com.example.MyClass
    upgradeMode: savepoint
Getting the error:
Invalid session job flinkConfiguration key: kubernetes.operator.periodic.savepoint.interval. Allowed keys are [kubernetes.operator.user.artifacts.http.header]
 ","kubernetes operator 1.2.0

kubernetes v1.24.1

 ",eleroy,mbalassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 13:45:54 UTC 2022,,,,,,,,,,"0|z1cmnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/22 13:45;mbalassi; [{{2c207ed}}|https://github.com/apache/flink-kubernetes-operator/commit/2c207edf5915f077b92c5b2bc2f0e6cac54daa9d] in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MinioTestContainerTest failed due to IllegalStateException in container startup,FLINK-30141,13505278,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rskraba,mapohl,mapohl,22/Nov/22 08:53,03/Jul/23 14:01,13/Jul/23 08:13,03/Jul/23 14:01,1.17.0,1.18.0,,,,,,1.16.3,1.17.2,1.18.0,,,Connectors / FileSystem,Tests,,,,,,0,pull-request-available,test-stability,,,"[This build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43182&view=logs&j=a1ac4ce4-9a4f-5fdb-3290-7e163fba19dc&t=3a8f44aa-4415-5b14-37d5-5fecc568b139&l=15531] failed due to an {{IllegalStateException}} during container startup:
{code:java}
Nov 15 02:34:04 [ERROR] org.apache.flink.fs.s3.common.MinioTestContainerTest.testBucketCreation  Time elapsed: 120.874 s  <<< ERROR!
Nov 15 02:34:04 org.testcontainers.containers.ContainerLaunchException: Container startup failed
Nov 15 02:34:04 	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:345)
Nov 15 02:34:04 	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:326)
Nov 15 02:34:04 	at org.apache.flink.core.testutils.TestContainerExtension.instantiateTestContainer(TestContainerExtension.java:59)
Nov 15 02:34:04 	at org.apache.flink.core.testutils.TestContainerExtension.before(TestContainerExtension.java:70)
Nov 15 02:34:04 	at org.apache.flink.core.testutils.EachCallbackWrapper.beforeEach(EachCallbackWrapper.java:45)
Nov 15 02:34:04 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeBeforeEachCallbacks$2(TestMethodTestDescriptor.java:166)
Nov 15 02:34:04 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeBeforeMethodsOrCallbacksUntilExceptionOccurs$6(TestMethodTestDescriptor.java:202)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Nov 15 02:34:04 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeBeforeMethodsOrCallbacksUntilExceptionOccurs(TestMethodTestDescriptor.java:202)
Nov 15 02:34:04 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeBeforeEachCallbacks(TestMethodTestDescriptor.java:165)
Nov 15 02:34:04 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:132)
Nov 15 02:34:04 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Nov 15 02:34:04 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
 {code}",,leonard,mapohl,rskraba,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26402,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 03 14:01:26 UTC 2023,,,,,,,,,,"0|z1cm6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/23 09:30;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44690&view=logs&j=dbe51908-4958-5c8c-9557-e10952d4259d&t=55d11a16-067d-538d-76a3-4c096a3a8e24&l=16775;;;","16/Mar/23 07:16;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47137&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9;;;","26/Apr/23 09:44;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48394&view=logs&j=7e3d33c3-a462-5ea8-98b8-27e1aafe4ceb&t=ef77f8d1-44c8-5ee2-f175-1c88f61de8c0&l=14049;;;","21/Jun/23 15:27;rskraba;I took a look at this and the related issue FLINK-26402 -- it looks like the *503 Service Unavailable* statuses are not rare: they occur about 1 in a couple hundred API calls to Minio on container startup.  On the other hand, the retry mechanism built into Amazon API clients _usually_ try again correctly until they succeed.  Sometimes, the Minio container doesn't move to the correct state to service API calls quickly enough, the default retry strategy fails eventually and we see the error here.

I can reproduce this pretty reliably by running a unit test somewhere between 1K-10K times.  At first I assumed it occurred when the system was loaded while running the test, but that doesn't appear to be the case.

Attempting to start up the container more than once might be the right thing to do here.  If the call to Minio fails while creating the default bucket, the container should be discarded and tried again.  This should have no overhead on the daily CI runs.;;;","03/Jul/23 14:01;mapohl;master: 972e15348c461d960b43919150480a0333e26ff8
1.17: 9344732e37469225ffc935e424fac0aac8434981
1.16: 8e6c586fc5f6596f4c8ae4ad338220b7202ea7e9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CodeGenLoader fails when temporary directory is a symlink,FLINK-30139,13505245,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,22/Nov/22 08:24,23/Nov/22 02:43,13/Jul/23 08:13,23/Nov/22 02:43,,,,,,,,table-store-0.3.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,Same to FLINK-28102 ,,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30143,,,,,,,,,,,FLINK-28102,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 23 02:43:03 UTC 2022,,,,,,,,,,"0|z1clzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/22 02:43;lzljs3620320;master: ae96b335e470a068a7e87f3d1a22c58bf2d82256;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BlobServerCleanupTest utilizes failsWithin with a quite low timeout which might result in test instabilities,FLINK-30138,13505237,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,22/Nov/22 08:07,01/Dec/22 07:41,13/Jul/23 08:13,01/Dec/22 07:41,1.15.2,1.16.0,1.17.0,,,,,1.15.4,1.16.1,1.17.0,,,Runtime / Coordination,Tests,,,,,,0,pull-request-available,starter,,,"{{BlobServerCleanupTest}} utilizes {{failsWithin}} using a low timeout threshold (100ms) in several locations. This might result in test instabilities (e.g. [here|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43221&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=8801]). We shouldn't rely on timeouts as stated in the Flink coding guidelines.",,mapohl,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 01 07:41:06 UTC 2022,,,,,,,,,,"0|z1clxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/22 07:07;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43606&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8389;;;","01/Dec/22 07:41;mapohl;master: c65591d4109f39dfa6a5b5f945c46f97dc5d967c
1.16: 8f7c35ec0e434d788ea0984b8918f1be3354596b
1.15: 738c861024a534cdd92c96f5a35c1eb13a8d992f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HadoopModuleFactory creates error if the security module cannot be loaded,FLINK-30133,13505229,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mapohl,mapohl,mapohl,22/Nov/22 07:35,25/Nov/22 02:57,13/Jul/23 08:13,25/Nov/22 02:57,1.15.2,1.16.0,1.17.0,,,,,1.15.4,1.16.1,1.17.0,,,Connectors / Hadoop Compatibility,,,,,,,0,pull-request-available,starter,,,"[HadoopModuleFactory|https://github.com/apache/flink/blob/26aa543b3bbe2b606bbc6d332a2ef7c5b46d25eb/flink-runtime/src/main/java/org/apache/flink/runtime/security/modules/HadoopModuleFactory.java#L51] tries to load the {{{}HadoopModule{}}}. If it fails to load the module, it will log an error an return {{null}} which is going to be handled properly. The resulting error log is, therefore, confusing. We might want to lower the log level to warning since the error doesn't affect the Flink cluster in a fatal way.

We might want to make the cluster fail fatally if we consider this a sever usability problem.",,gaborgsomogyi,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 25 02:57:50 UTC 2022,,,,,,,,,,"0|z1clw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/22 07:38;mapohl;[~gsomogyi] I'm curious what you think of that. Do you agree that warning the user through a log message is good enough or should we fail fatally if the hadoop security module failed to be loaded because the user might assume certain security measures being in place if he/she misses this log warning.;;;","22/Nov/22 09:40;gaborgsomogyi;I think this area is just conceptually not consistent so not sure what we can do about it w/o breaking change.

Here is my understanding:
 * All other factory classes make the workload finally fail if something bad happens
 * `security.module.factory.classes` contains `HadoopModuleFactory` by default which is fine
 * When no hadoop-common is on classpath then it silently prints an info and not loading the module. We can consider it [best effort behavior|https://github.com/apache/flink/blob/26aa543b3bbe2b606bbc6d332a2ef7c5b46d25eb/flink-runtime/src/main/java/org/apache/flink/runtime/security/modules/HadoopModuleFactory.java#L41].
 * Then it tries to load the hadoop configuration in the [mentioned place|https://github.com/apache/flink/blob/26aa543b3bbe2b606bbc6d332a2ef7c5b46d25eb/flink-runtime/src/main/java/org/apache/flink/runtime/security/modules/HadoopModuleFactory.java#L51], but this is just bad in general. It uses `flink-hadoop-fs` area code where [HdfsConfiguration|https://github.com/apache/flink/blob/26aa543b3bbe2b606bbc6d332a2ef7c5b46d25eb/flink-filesystems/flink-hadoop-fs/src/main/java/org/apache/flink/runtime/util/HadoopUtils.java#L59] is instantiated. This implicitly means one MUST have HDFS jars on classpath to run Flink securely. I'm constantly thinking about this to refactor but this is definitely a breaking change ( though I would support full rewrite of hadoop config loading since Flink has at least 6-7 different Hadoop config loading implementations which makes this area hell :) ).

Considering the actual situation we can decrease the error level to warning since the workload is going forward.
If you ask me then now/later on I would do the following for the clean solution:
 * Remove `HdfsConfiguration` from Flink Hadoop config loading since HDFS configs are not needed for Kerberos authentication.
 * Make the workload finally fail if module was not able to be loaded/installed (hadoop-common is on classpath so the user has intention to install the module)

I know that my clean solution would be a drastic change but that would be clear to the users.
;;;","24/Nov/22 06:20;mapohl;Thanks for your thorough analysis, Gabor. I went ahead and created PRs for that minor issue. It would be great if you could create a follow up ticket that covers the cleanup as this sounds like a reasonable refactoring.;;;","25/Nov/22 02:57;mapohl;master: 5086a33f41058e3a87bae4bf0107032fd0194b67
1.16: 622ed691c3e7f563d82f3e06273e7d2647dec1c1
1.15: 17b569f4c75064e4a5e76289bd598d9bc1140953;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Projection pushdown is not work for partial update,FLINK-30125,13505201,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,22/Nov/22 02:55,24/Nov/22 08:05,13/Jul/23 08:13,24/Nov/22 08:05,,,,,,,,table-store-0.3.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"We did not properly process the project in MergeFunction, which resulted in subsequent reading position errors.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 08:05:51 UTC 2022,,,,,,,,,,"0|z1clps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 08:05;lzljs3620320;master: 2b8fd32140ed0bf05aa5f8e42f561b19904be69b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GenericType<java.util.Map> is not supported in PyFlink currently,FLINK-30124,13505177,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,22/Nov/22 01:31,29/Nov/22 09:18,13/Jul/23 08:13,29/Nov/22 09:18,ml-2.1.0,,,,,,,ml-2.2.0,,,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,"When we add and execute the following test case to flink-ml-python/pyflink/ml/lib/classification/tests/test_naivebayes.py of the Flink ML repository,
{code:java}
def test_get_model_data(self):
model_data = self.estimator.fit(self.train_data).get_model_data()[0]
self.t_env.to_data_stream(model_data).execute_and_collect().next(){code}
The following exception would be thrown.

 
{code:java}
j_type_info = JavaObject id=o698
    def _from_java_type(j_type_info: JavaObject) -> TypeInformation:
        gateway = get_gateway()
        JBasicTypeInfo = gateway.jvm.org.apache.flink.api.common.typeinfo.BasicTypeInfo
    
        if _is_instance_of(j_type_info, JBasicTypeInfo.STRING_TYPE_INFO):
            return Types.STRING()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.BOOLEAN_TYPE_INFO):
            return Types.BOOLEAN()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.BYTE_TYPE_INFO):
            return Types.BYTE()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.SHORT_TYPE_INFO):
            return Types.SHORT()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.INT_TYPE_INFO):
            return Types.INT()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.LONG_TYPE_INFO):
            return Types.LONG()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.FLOAT_TYPE_INFO):
            return Types.FLOAT()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.DOUBLE_TYPE_INFO):
            return Types.DOUBLE()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.CHAR_TYPE_INFO):
            return Types.CHAR()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.BIG_INT_TYPE_INFO):
            return Types.BIG_INT()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.BIG_DEC_TYPE_INFO):
            return Types.BIG_DEC()
        elif _is_instance_of(j_type_info, JBasicTypeInfo.INSTANT_TYPE_INFO):
            return Types.INSTANT()
    
        JSqlTimeTypeInfo = gateway.jvm.org.apache.flink.api.common.typeinfo.SqlTimeTypeInfo
        if _is_instance_of(j_type_info, JSqlTimeTypeInfo.DATE):
            return Types.SQL_DATE()
        elif _is_instance_of(j_type_info, JSqlTimeTypeInfo.TIME):
            return Types.SQL_TIME()
        elif _is_instance_of(j_type_info, JSqlTimeTypeInfo.TIMESTAMP):
            return Types.SQL_TIMESTAMP()
    
        JPrimitiveArrayTypeInfo = gateway.jvm.org.apache.flink.api.common.typeinfo \
            .PrimitiveArrayTypeInfo
    
        if _is_instance_of(j_type_info, JPrimitiveArrayTypeInfo.BOOLEAN_PRIMITIVE_ARRAY_TYPE_INFO):
            return Types.PRIMITIVE_ARRAY(Types.BOOLEAN())
        elif _is_instance_of(j_type_info, JPrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO):
            return Types.PRIMITIVE_ARRAY(Types.BYTE())
        elif _is_instance_of(j_type_info, JPrimitiveArrayTypeInfo.SHORT_PRIMITIVE_ARRAY_TYPE_INFO):
            return Types.PRIMITIVE_ARRAY(Types.SHORT())
        elif _is_instance_of(j_type_info, JPrimitiveArrayTypeInfo.INT_PRIMITIVE_ARRAY_TYPE_INFO):
            return Types.PRIMITIVE_ARRAY(Types.INT())
        elif _is_instance_of(j_type_info, JPrimitiveArrayTypeInfo.LONG_PRIMITIVE_ARRAY_TYPE_INFO):
            return Types.PRIMITIVE_ARRAY(Types.LONG())
        elif _is_instance_of(j_type_info, JPrimitiveArrayTypeInfo.FLOAT_PRIMITIVE_ARRAY_TYPE_INFO):
            return Types.PRIMITIVE_ARRAY(Types.FLOAT())
        elif _is_instance_of(j_type_info, JPrimitiveArrayTypeInfo.DOUBLE_PRIMITIVE_ARRAY_TYPE_INFO):
            return Types.PRIMITIVE_ARRAY(Types.DOUBLE())
        elif _is_instance_of(j_type_info, JPrimitiveArrayTypeInfo.CHAR_PRIMITIVE_ARRAY_TYPE_INFO):
            return Types.PRIMITIVE_ARRAY(Types.CHAR())
    
        JBasicArrayTypeInfo = gateway.jvm.org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo
    
        if _is_instance_of(j_type_info, JBasicArrayTypeInfo.BOOLEAN_ARRAY_TYPE_INFO):
            return Types.BASIC_ARRAY(Types.BOOLEAN())
        elif _is_instance_of(j_type_info, JBasicArrayTypeInfo.BYTE_ARRAY_TYPE_INFO):
            return Types.BASIC_ARRAY(Types.BYTE())
        elif _is_instance_of(j_type_info, JBasicArrayTypeInfo.SHORT_ARRAY_TYPE_INFO):
            return Types.BASIC_ARRAY(Types.SHORT())
        elif _is_instance_of(j_type_info, JBasicArrayTypeInfo.INT_ARRAY_TYPE_INFO):
            return Types.BASIC_ARRAY(Types.INT())
        elif _is_instance_of(j_type_info, JBasicArrayTypeInfo.LONG_ARRAY_TYPE_INFO):
            return Types.BASIC_ARRAY(Types.LONG())
        elif _is_instance_of(j_type_info, JBasicArrayTypeInfo.FLOAT_ARRAY_TYPE_INFO):
            return Types.BASIC_ARRAY(Types.FLOAT())
        elif _is_instance_of(j_type_info, JBasicArrayTypeInfo.DOUBLE_ARRAY_TYPE_INFO):
            return Types.BASIC_ARRAY(Types.DOUBLE())
        elif _is_instance_of(j_type_info, JBasicArrayTypeInfo.CHAR_ARRAY_TYPE_INFO):
            return Types.BASIC_ARRAY(Types.CHAR())
        elif _is_instance_of(j_type_info, JBasicArrayTypeInfo.STRING_ARRAY_TYPE_INFO):
            return Types.BASIC_ARRAY(Types.STRING())
    
        JObjectArrayTypeInfo = gateway.jvm.org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo
        if _is_instance_of(j_type_info, JObjectArrayTypeInfo):
            return Types.OBJECT_ARRAY(_from_java_type(j_type_info.getComponentInfo()))
    
        JPickledBytesTypeInfo = gateway.jvm \
            .org.apache.flink.streaming.api.typeinfo.python.PickledByteArrayTypeInfo\
            .PICKLED_BYTE_ARRAY_TYPE_INFO
        if _is_instance_of(j_type_info, JPickledBytesTypeInfo):
            return Types.PICKLED_BYTE_ARRAY()
    
        JRowTypeInfo = gateway.jvm.org.apache.flink.api.java.typeutils.RowTypeInfo
        if _is_instance_of(j_type_info, JRowTypeInfo):
            j_row_field_names = j_type_info.getFieldNames()
            j_row_field_types = j_type_info.getFieldTypes()
            row_field_types = [_from_java_type(j_row_field_type) for j_row_field_type in
                               j_row_field_types]
            row_field_names = [field_name for field_name in j_row_field_names]
            return Types.ROW_NAMED(row_field_names, row_field_types)
    
        JTupleTypeInfo = gateway.jvm.org.apache.flink.api.java.typeutils.TupleTypeInfo
        if _is_instance_of(j_type_info, JTupleTypeInfo):
            j_field_types = []
            for i in range(j_type_info.getArity()):
                j_field_types.append(j_type_info.getTypeAt(i))
            field_types = [_from_java_type(j_field_type) for j_field_type in j_field_types]
            return TupleTypeInfo(field_types)
    
        JMapTypeInfo = get_gateway().jvm.org.apache.flink.api.java.typeutils.MapTypeInfo
        if _is_instance_of(j_type_info, JMapTypeInfo):
            j_key_type_info = j_type_info.getKeyTypeInfo()
            j_value_type_info = j_type_info.getValueTypeInfo()
            return MapTypeInfo(_from_java_type(j_key_type_info), _from_java_type(j_value_type_info))
    
        JListTypeInfo = get_gateway().jvm.org.apache.flink.api.java.typeutils.ListTypeInfo
        if _is_instance_of(j_type_info, JListTypeInfo):
            j_element_type_info = j_type_info.getElementTypeInfo()
            return ListTypeInfo(_from_java_type(j_element_type_info))
    
        JExternalTypeInfo = gateway.jvm.org.apache.flink.table.runtime.typeutils.ExternalTypeInfo
        if _is_instance_of(j_type_info, JExternalTypeInfo):
            TypeInfoDataTypeConverter = \
                gateway.jvm.org.apache.flink.table.types.utils.LegacyTypeInfoDataTypeConverter
            return ExternalTypeInfo(_from_java_type(
                TypeInfoDataTypeConverter.toLegacyTypeInfo(j_type_info.getDataType())))
    
>       raise TypeError(""The java type info: %s is not supported in PyFlink currently."" % j_type_info)
E       TypeError: The java type info: GenericType<java.util.Map> is not supported in PyFlink currently.
{code}
 ",,yunfengzhou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-22 01:31:47.0,,,,,,,,,,"0|z1clkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink ML KMeans getting model data throws TypeError,FLINK-30122,13505175,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,22/Nov/22 01:25,10/Jan/23 04:11,13/Jul/23 08:13,10/Jan/23 04:11,ml-2.1.0,,,,,,,ml-2.2.0,,,,,Library / Machine Learning,,,,,,,0,,,,,"When the following test case is added to flink-ml-python/pyflink/ml/lib/clustering/tests/test_kmeans.py，
{code:java}
def test_get_model_data(self):
kmeans = KMeans().set_max_iter(2).set_k(2)
model = kmeans.fit(self.data_table)
model_data = model.get_model_data()[0]
expected_field_names = ['centroids', 'weights']
self.assertEqual(expected_field_names, model_data.get_schema().get_field_names())self.t_env.to_data_stream(model_data).execute_and_collect().next(){code}
The following exception would be thrown.
{code:java}
data = 0, field_type = DenseVectorTypeInfo
def pickled_bytes_to_python_converter(data, field_type):
if isinstance(field_type, RowTypeInfo):
row_kind = RowKind(int.from_bytes(data[0], 'little'))
data = zip(list(data[1:]), field_type.get_field_types())
fields = []
for d, d_type in data:
fields.append(pickled_bytes_to_python_converter(d, d_type))
row = Row.of_kind(row_kind, *fields)
return row
else:
> data = pickle.loads(data)
E TypeError: a bytes-like object is required, not 'int'{code}",,yunfengzhou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 23 12:17:40 UTC 2022,,,,,,,,,,"0|z1clk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/22 12:17;yunfengzhou;It has been discovered that PyFlink has a bug dealing with object arrays.

https://issues.apache.org/jira/browse/FLINK-30168

Flink ML has worked around this problem with a temporary solution as shown in this PR.

[https://github.com/apache/flink-ml/pull/181];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Flink SQL][Protobuf] CompileException when querying Kafka topic using google.protobuf.Timestamp ,FLINK-30093,13503687,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,laughingman7743,jamesmcguirepro,jamesmcguirepro,18/Nov/22 18:36,06/Jul/23 05:25,13/Jul/23 08:13,19/Jan/23 12:56,1.16.0,,,,,,,1.17.0,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Ecosystem,,,,,,0,pull-request-available,,,,"I am encountering an issue when trying to use Flink SQL to query a Kafka topic that uses {{{}google.protobuf.Timestamp{}}}.

 

When attempting to use Flink SQL to query a protobuf serialized Kafka topic that uses  {{{}google.protobuf.Timestamp{}}}, a {{org.codehaus.commons.compiler.CompileException: Line 23, Column 5: Cannot determine simple type name ""com"" }}error occurs when trying to query the table.

 

*Replication steps:*

1. Use a protobuf definition that contains a {{{}google.protobuf.Timestamp{}}}:
{noformat}
syntax = ""proto3"";
package example.message;

import ""google/protobuf/timestamp.proto"";

option java_package = ""com.example.message"";
option java_multiple_files = true;

message Test {
  int64 id = 1;
  google.protobuf.Timestamp created_at = 5;
}{noformat}
2. Use protobuf definition to produce message to topic

3. Confirm message is deserializable by protoc:
{code:java}
kcat -C -t development.example.message -b localhost:9092 -o -1 -e -q -D """" | protoc --decode=example.message.Test --proto_path=/Users/jamesmcguire/repos/flink-proto-example/schemas/ example/message/test.proto 
id: 123
created_at {
  seconds: 456
  nanos: 789
}{code}
4. Create table in Flink SQL using kafka connector and protobuf format
{code:java}
CREATE TABLE tests (
  id BIGINT,
  created_at row<seconds BIGINT, nanos INT>
)
COMMENT ''
WITH (
  'connector' = 'kafka',
  'format' = 'protobuf',
  'protobuf.message-class-name' = 'com.example.message.Test',
  'properties.auto.offset.reset' = 'earliest',
  'properties.bootstrap.servers' = 'host.docker.internal:9092',
  'properties.group.id' = 'test-1',
  'topic' = 'development.example.message'
);{code}
5. Run query in Flink SQL and encounter error:
{code:java}
Flink SQL> select * from tests;
[ERROR] Could not execute SQL statement. Reason:
org.codehaus.commons.compiler.CompileException: Line 23, Column 5: Cannot determine simple type name ""com"" {code}
{*}NOTE{*}: If you repeat steps 4-5 without {{created_at row<seconds BIGINT, nanos INT>}} in the table, step 5 will complete successfully.

6. Observe in attached log file, Flink appears to be using the incorrect namespace (should be {{google.protobuf.Timestamp):}}
{code:java}
com.example.message.Timestamp message3 = message0.getCreatedAt(); {code}",Mac OS Ventura,aitozi,baugarten,hdulay,jamesmcguirepro,laughingman7743,libenchao,maosuhan,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32547,,,,,,"18/Nov/22 18:36;jamesmcguirepro;taskmanager_172.22.0 (1).4_46291-40eec2_log;https://issues.apache.org/jira/secure/attachment/13052385/taskmanager_172.22.0+%281%29.4_46291-40eec2_log",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 16 04:37:15 UTC 2023,,,,,,,,,,"0|z1cce0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/22 19:02;rmetzger;[~maosuhan] Sorry for pinging you directly, could you take a look at this?;;;","21/Nov/22 02:50;maosuhan;[~rmetzger] Thanks for your findings!

I did not consider that user will use *google.protobuf.Timestamp* in the proto file. So importing definition is not supported in current flink version, but it definitely should be supported in the future.

`;;;","28/Nov/22 17:12;hdulay;I may have found the solution to this. Basically the generated code doesn't compile and the error message originating from codehaus.janino is a terrible one. The line below is the culprit

[https://github.com/apache/flink/blob/master/flink-formats/flink-protobuf/src/main/java/org/apache/flink/formats/protobuf/util/PbFormatUtils.java#L39] 

 

Changing it from:
{code:java}
return outerProtoName + descriptor.getName();{code}
{{to}}
{code:java}
return descriptor.getFile().getOptions().getJavaPackage() + ""."" + descriptor.getName();{code}
 

Seemed to do the trick but I'm still working through it.;;;","24/Dec/22 16:20;laughingman7743;[~hdulay] 

I am using Flink 1.14.x by porting the early version of the branch where the protobuf format feature was developed, and google.protobuf.Timestamp can be handled as row<seconds BIGINT, nanos INT> without problems.

https://github.com/apache/flink/pull/14376

Perhaps you mey need to make sure that multiple_files is set and outer classes are defined as in the early version.

https://github.com/maosuhan/flink/blob/888696df9507a0129c894b913b7828c836a95985/flink-formats/flink-protobuf/src/main/java/org/apache/flink/formats/protobuf/PbFormatUtils.java#L55-L90;;;","29/Dec/22 04:14;maosuhan;[~rmetzger]  Since this issue can be solved by the way [~laughingman7743]  suggested. Can we close this issue?;;;","29/Dec/22 04:28;hdulay;I think that would be best. Thanks


;;;","29/Dec/22 04:45;libenchao;[~maosuhan] If this is the suggested way to handle {{google.protobuf.Timestamp}} type, how about adding it to the document explicitly?;;;","29/Dec/22 13:28;laughingman7743;I am not sure if 1.16 can handle the google.protobuf.Timestamp type. Since I am still using an early implementation of this Protobuf formatter.
If it is recommended to set multiple_files and define an outer class, it would help a lot of people if there is a description in the documentation.

It would also be better if a unit test of type google.protobuf.Timestamp could be added. It is possible that the google.protobuf.Timestamp type may no longer be handled since the implementation seems to have been changed from earlier versions.;;;","02/Jan/23 10:19;rmetzger;I'm not a protobuf expert, so I can not decide what's best from a user experience perspective.

Using {{row<seconds BIGINT, nanos INT>}} sounds like a workaround to me? Wouldn't it be nicer if the Timestamp-type is handled out of the box by Flink? If you think this is an ok user experience, then I agree with Benchao, that we should at least document how how to use the timestamp type with Flink.;;;","03/Jan/23 18:08;jamesmcguirepro;Using  {{row<seconds BIGINT, nanos INT>}} did not work with 1.16.0 for me.   If you review the ticket, you can see that is how I had the table schema defined.

 

+1 on updating the documentation no matter what.;;;","04/Jan/23 06:30;maosuhan;[~jamesmcguirepro] Thanks for your feedback, I'll update the documentation and try to find a better way to support this requirement.;;;","04/Jan/23 14:24;laughingman7743;Unfortunately, Flink 1.16 can no longer handle the google.protobuf.Timestamp type. I have added a test case for the google.protobuf.Timestamp type to confirm this.
https://github.com/laughingman7743/flink/pull/1/files

{code:java}
org.apache.flink.formats.protobuf.PbCodegenException: org.apache.flink.api.common.InvalidProgramException: Program cannot be compiled. This is a bug. Please file an issue.

	at org.apache.flink.formats.protobuf.deserialize.ProtoToRowConverter.<init>(ProtoToRowConverter.java:126)
	at org.apache.flink.formats.protobuf.deserialize.PbRowDataDeserializationSchema.open(PbRowDataDeserializationSchema.java:64)
	at org.apache.flink.formats.protobuf.ProtobufTestHelper.pbBytesToRow(ProtobufTestHelper.java:118)
	at org.apache.flink.formats.protobuf.ProtobufTestHelper.pbBytesToRow(ProtobufTestHelper.java:102)
	at org.apache.flink.formats.protobuf.ProtobufTestHelper.pbBytesToRow(ProtobufTestHelper.java:97)
	at org.apache.flink.formats.protobuf.TimestampToRowTest.testSimple(TimestampToRowTest.java:22)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: org.apache.flink.api.common.InvalidProgramException: Program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.formats.protobuf.util.PbCodegenUtils.compileClass(PbCodegenUtils.java:265)
	at org.apache.flink.formats.protobuf.deserialize.ProtoToRowConverter.<init>(ProtoToRowConverter.java:118)
	... 32 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 19, Column 5: Cannot determine simple type name ""org""
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6833)
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594)
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
	at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573)
	at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481)
	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476)
	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928)
	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476)
	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469)
	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927)
	at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
	at org.codehaus.janino.UnitCompiler.getLocalVariable(UnitCompiler.java:2616)
	at org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3662)
	at org.codehaus.janino.UnitCompiler.access$5800(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$12.visitLocalVariableDeclarationStatement(UnitCompiler.java:3545)
	at org.codehaus.janino.UnitCompiler$12.visitLocalVariableDeclarationStatement(UnitCompiler.java:3513)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:3522)
	at org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3512)
	at org.codehaus.janino.UnitCompiler.buildLocalVariableMap(UnitCompiler.java:3501)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3322)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
	at org.apache.flink.formats.protobuf.util.PbCodegenUtils.compileClass(PbCodegenUtils.java:262)
	... 33 more
{code}

I will try to check the difference of the implementation in Flink 1.16 from the earlier version.;;;","04/Jan/23 17:39;laughingman7743;I fixed it in the following branch.
https://github.com/laughingman7743/flink/pull/2
https://github.com/laughingman7743/flink/pull/2/commits/730b374b1056b5af9f39f21e2f83e8371e044948
Regardless of the user-defined protobuf option, the google.protobuf.Timestamp type always has multiple_files=true, in which case the best choice is to use the descriptor getJavaPackage or getPackage as the full name of the java class.
In other cases, it is best to use outerProtoName. A simple test has confirmed that this is not a problem.;;;","05/Jan/23 07:30;laughingman7743;I have investigated the Flink 1.16 implementation in detail.
The current implementation does not take into consideration the case where another package is imported and used for a message, so it seems that the way getOuterProtoPrefix is determined needs to be improved.

I made a fix in the following branch to try it out.
https://github.com/laughingman7743/flink/pull/3
https://github.com/laughingman7743/flink/pull/3/commits/f8ad68c401279b5911687473940918465d797692

It also takes into consideration cases where the multiple_files and java_outer_classname options are not specified and the suffix OuterClass is added when the file name and message name are the same.
This should probably cover most cases of protobuf definitions.

If this change is acceptable I will create a pull request in the Flink main repository.;;;","07/Jan/23 02:51;maosuhan;[~laughingman7743] Thanks for your code. It looks to me. You can go ahead and I will review the code when you're done.

I think the core change is to use a new getOuterProtoPrefix method which handle different cases. I appreciate that you also consider OuterClass suffix in a special case.  Maybe the reason it works in older version is that I wrote more codes to handle different options but deleted later for simplicity getting outer prefix name. But in fact this is an issue after probobuf is released, I think we should improve this logic.;;;","07/Jan/23 05:42;laughingman7743;[~maosuhan]
I have created a pull request in the Flink main repository.
https://github.com/apache/flink/pull/21613
Please take a look at it when you have time.;;;","19/Jan/23 12:56;libenchao;Fixed via https://github.com/apache/flink/commit/7ea4476c0544e17798cbb1e39609827954f6c266 (master)

[~laughingman7743] Thanks for your PR, and [~maosuhan] thanks for the review.;;;","09/Jun/23 16:17;baugarten;Hey, thanks for the contribution. I'm running into this same issue on Flink 1.16 and would like to backport the fix to the release-1.16 branch for a future patch release. I think this is a good candidate for backporting, and the commit against master applied cleanly to the release-1.16 branch.

I opened up [this PR |https://github.com/apache/flink/pull/22752] based on the original PR: https://github.com/apache/flink/pull/21613

;;;","10/Jun/23 02:28;libenchao;[~baugarten] Thanks for your work! 

However, I think this is more like a feature instead of bug, that's why we only merged it to ""master"" branch.;;;","15/Jun/23 19:49;baugarten;Thanks [~libenchao], at first I thought that this ticket might have been considered a bug.

I couldn't find guidance in the [contributing|https://flink.apache.org/how-to-contribute/contribute-code/] [guidelines|https://cwiki.apache.org/confluence/display/FLINK/Apache+Flink+Home] on what changes are eligible for backporting to patch releases. In the past, I know some features (ie HybridSource) ended up getting backported to a previous release;;;","16/Jun/23 02:30;libenchao;Usually the ""bugfix"" which fixes the functionality which is advertised to users by documentation/release note, is a good candidate to back port. For this one, in the original implementation, {{Timestamp}} is not considered as a supported type, and it's not tested/documented, hence supporting {{Timestamp}} is more like an improvement to me. What do you guys think?;;;","16/Jun/23 04:37;laughingman7743;I would like to see this change backported to 1.16, because without applying this change, we will not be able to use the imported portobuf definitions as well as google.protobuf.Timestamp.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"mvn package verify fails with ""Error creating shaded jar: duplicate entry""",FLINK-30091,13503616,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Gerrrr,Gerrrr,Gerrrr,18/Nov/22 16:00,21/Nov/22 07:33,13/Jul/23 08:13,21/Nov/22 07:33,,,,,,,,table-store-0.3.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"{{mvn package verify}} fails with

{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.1.1:shade (shade-flink) on project flink-table-store-dist: Error creating shaded jar: duplicate entry: META-INF/services/org.apache.flink.table.store.shaded.org.apache.kafka.common.config.provider.ConfigProvider -> [Help 1]
{noformat}

The fix is to update {{maven-shade-plugin}} version to 3.4.1.",,Gerrrr,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 21 07:33:06 UTC 2022,,,,,,,,,,"0|z1cby8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/22 16:19;Gerrrr;PR - [https://github.com/apache/flink-table-store/pull/391]

 ;;;","21/Nov/22 07:33;lzljs3620320;master: 0a31766290c36a0c9033da3850146c684a90468d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DelegatingConfiguration#set should return itself,FLINK-30067,13503047,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,17/Nov/22 08:58,17/Nov/22 14:57,13/Jul/23 08:13,17/Nov/22 14:57,,,,,,,,1.17.0,,,,,API / Core,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29993,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 17 14:57:00 UTC 2022,,,,,,,,,,"0|z1c8g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/22 14:57;chesnay;master: 38c6a4bb8af37f5a05e9d57d9787f0b25334abbf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot use digest in Helm chart to reference image,FLINK-30046,13502917,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mbalassi,tshofer,tshofer,16/Nov/22 15:30,07/Dec/22 15:07,13/Jul/23 08:13,07/Dec/22 15:07,kubernetes-operator-1.2.0,kubernetes-operator-1.2.1,kubernetes-operator-1.3.0,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Images can be referenced by tag only.

Referencing images by digest has a number of advantages:
 # Avoid unexpected or undesirable image changes.
 # Increase security and awareness by knowing the specific image running in your environment.

The following document describes a template to handle tags and digests:

[Adding Image Digest References to Your Helm Charts|https://blog.andyserver.com/2021/09/adding-image-digest-references-to-your-helm-charts/]",,mbalassi,morhidi,tshofer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 07 15:07:36 UTC 2022,,,,,,,,,,"0|z1c7nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/22 15:30;mbalassi;[~tshofer] the link you provided is no longer available. Could you give me access to the guidance in an alternative way please?;;;","30/Nov/22 11:07;tshofer;Hi [~mbalassi], I'm sorry for the situation. I try to contact the webmaster to get a copy.

The guide mainly proposed to use a single value named {{version}} to handle tags and digests transparently. The guide also contained a proposal of the changes in the Helm chart templates. Nothing worldshaking if you're used to create Helm charts (but I'm not).;;;","30/Nov/22 12:03;mbalassi;[~tshofer] that makes sense, thanks. For backwards compatibility reasons I will need to keep the tag configuration option, so I will simply add the digest as an additional option which takes precedence if defined (the tag is ignored in this case).

I would strongly prefer the option you suggest if we implemented this from scratch without prior releases.

I will work on this later this week.;;;","05/Dec/22 16:19;morhidi;[~mbalassi] are we keeping this in 1.3?;;;","06/Dec/22 14:53;mbalassi;Yes [~morhidi] ;;;","07/Dec/22 15:07;morhidi;added to main via 0365416773390ae7a4946ba7180abb9e66b0dd86;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Insert overwrite show excetion that query and insert schema do not match,FLINK-30039,13502866,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,StarBoy1005,StarBoy1005,16/Nov/22 09:51,19/Mar/23 05:52,13/Jul/23 08:13,19/Mar/23 05:52,1.16.0,,,,,,,,,,,,Table Store,,,,,,,0,,,,,"After change a table's bucket num in table store,I used a insert overwrite sql to reorganize records to new files (I guess form old orc file to the new).
Everytime if select column contains the partition column,then get the next exception: 
 !screenshot-2.png! 
When exclude the partition field,it do perform the overwrite.
I wonder the “EXPR$3: STRING NOT NULL” means what , ",Flink 1.16.0,lzljs3620320,StarBoy1005,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/22 09:54;StarBoy1005;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13052275/screenshot-2.png","16/Nov/22 10:01;StarBoy1005;screenshot-3.png;https://issues.apache.org/jira/secure/attachment/13052276/screenshot-3.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 18 06:18:46 UTC 2022,,,,,,,,,,"0|z1c7c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/22 10:01;StarBoy1005;Besides, when it was two partitions seems work:
 !screenshot-3.png! ;;;","18/Nov/22 06:18;lzljs3620320;Hi [~StarBoy1005], if you are using static partition inserting, you can not select dt field in your sql.
Example can be found in https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/dev/table/sql/insert/#examples ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveE2E test is not stable,FLINK-30038,13502832,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,16/Nov/22 06:35,31/Jan/23 04:25,13/Jul/23 08:13,31/Jan/23 04:25,table-store-0.3.0,,,,,,,table-store-0.4.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"https://github.com/apache/flink-table-store/actions/runs/3476726197/jobs/5812201704

Caused by: org.testcontainers.containers.ContainerLaunchException: Timed out waiting for log output matching '.*Starting HiveServer2.*'
	at org.testcontainers.containers.wait.strategy.LogMessageWaitStrategy.waitUntilReady(LogMessageWaitStrategy.java:49)",,lzljs3620320,zjureel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 04:25:12 UTC 2023,,,,,,,,,,"0|z1c74o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 04:25;lzljs3620320;master: d1f807a4183d357a4282ac831f29252e547a890b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink table store run abnormally when shade flink,FLINK-30031,13502718,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,iture123,iture123,15/Nov/22 15:43,19/Mar/23 05:52,13/Jul/23 08:13,19/Mar/23 05:52,1.15.0,table-store-0.2.1,,,,,,,,,,,Table Store,,,,,,,0,,,,,"{color:#172b4d}I try to sink flink-table-store in Apache SeaTunnel, SeaTunnel use the Flink version is 13.6.{color}

{color:#172b4d}To avoid flink conflict,I use maven-shade-plugin plugin to shade flink dependency.{color}

{color:#172b4d}However, runing build jar to write  flink-table-store occur error,throw exception:{color}

 
{code:java}
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/flink/table/store/codegen/CodeGenerator
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:756)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)
        at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
        at org.apache.flink.table.store.codegen.ComponentClassLoader.loadClassFromComponentOnly(ComponentClassLoader.java:127)
        at org.apache.flink.table.store.codegen.ComponentClassLoader.loadClass(ComponentClassLoader.java:106)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:348)
        at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:370)
        at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)
        at java.util.ServiceLoader$1.next(ServiceLoader.java:480)
        at java.util.Iterator.forEachRemaining(Iterator.java:116)
        at org.apache.flink.table.store.codegen.CodeGenLoader.discover(CodeGenLoader.java:123)
        at org.apache.flink.table.store.codegen.CodeGenUtils.generateRecordComparator(CodeGenUtils.java:65)
        at org.apache.flink.table.store.file.utils.KeyComparatorSupplier.<init>(KeyComparatorSupplier.java:40)
        at org.apache.flink.table.store.file.KeyValueFileStore.<init>(KeyValueFileStore.java:59)
        at org.apache.flink.table.store.table.ChangelogWithKeyFileStoreTable.<init>(ChangelogWithKeyFileStoreTable.java:103)
        at org.apache.flink.table.store.table.FileStoreTableFactory.create(FileStoreTableFactory.java:72)
        at org.apache.flink.table.store.table.FileStoreTableFactory.create(FileStoreTableFactory.java:50)
        at org.example.TestWrite.main(TestWrite.java:24)
Caused by: java.lang.ClassNotFoundException: org.apache.flink.table.store.codegen.CodeGenerator
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
        at org.apache.flink.table.store.codegen.ComponentClassLoader.loadClassFromComponentOnly(ComponentClassLoader.java:127)
        at org.apache.flink.table.store.codegen.ComponentClassLoader.loadClass(ComponentClassLoader.java:106)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
        ... 27 more
 {code}
 

 
{code:java}
// pom.xml
<dependencies>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-table-store-core</artifactId>
        <version>0.2.1</version>
    </dependency>

    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-table-store-format</artifactId>
        <version>0.2.1</version>
    </dependency>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-table-common</artifactId>
        <version>1.15.0</version>
    </dependency>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-table-runtime</artifactId>
        <version>1.15.0</version>
    </dependency>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-connector-base</artifactId>
        <version>1.15.0</version>
    </dependency>

    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-shaded-jackson</artifactId>
        <version>2.12.1-13.0</version>
    </dependency>
     <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-shaded-hadoop-2-uber</artifactId>
        <version>2.7.5-10.0</version>
    </dependency>
    <dependency>
        <groupId>org.apache.commons</groupId>
        <artifactId>commons-lang3</artifactId>
        <version>3.3.2</version>
    </dependency>
    <dependency>
        <groupId>log4j</groupId>
        <artifactId>log4j</artifactId>
        <version>1.2.17</version>
    </dependency>

</dependencies>

<build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-shade-plugin</artifactId>
            <version>3.1.1</version>
            <configuration>
                <!-- no create dependency-reduced-pom.xml-->
                <createDependencyReducedPom>false</createDependencyReducedPom>
            </configuration>
            <executions>
                <!-- Run shade goal on package phase -->
                <execution>
                    <phase>package</phase>
                    <goals>
                        <goal>shade</goal>
                    </goals>
                    <configuration>
                        <relocations>
                            <relocation>
                                <pattern>org.apache.flink</pattern>
                                <shadedPattern>shade.org.apache.flink</shadedPattern>
                                <excludes>
                                    <exclude>org.apache.flink.table.store.**</exclude>
                                </excludes>
                            </relocation>
                        </relocations>
                    </configuration>
                </execution>
            </executions>
        </plugin>

    </plugins>
</build>{code}
 

 ",,iture123,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30080,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 18 06:26:37 UTC 2022,,,,,,,,,,"0|z1c6fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/22 06:26;lzljs3620320;I created FLINK-30080 for this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE error in generated StreamExecCalc,FLINK-30018,13502111,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jackwangcs,jackwangcs,14/Nov/22 08:52,06/Jun/23 02:56,13/Jul/23 08:13,06/Jun/23 02:56,1.16.0,,,,,,,,,,,,Table SQL / Runtime,,,,,,,0,,,,,"Hi, I met a NPE exception running Flink SQL. The exception is
{code:java}
rg.apache.flink.runtime.taskmanager.Task                    [] - Join[292] -> Calc[293] -> ConstraintEnforcer[294] (10/48)#0 (e628391c0b38d4d22ae62a181a2d7f22_c9cd1581189658451a8850505c8a0007_9_0) switched from RUNNING to FAILED with failure cause: java.lang.NullPointerException
    at StreamExecCalc$20690.processElement_split881(Unknown Source)
    at StreamExecCalc$20690.processElement(Unknown Source)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
    at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51)
    at org.apache.flink.table.runtime.operators.join.stream.StreamingJoinOperator.outputNullPadding(StreamingJoinOperator.java:334)
    at org.apache.flink.table.runtime.operators.join.stream.StreamingJoinOperator.processElement(StreamingJoinOperator.java:219)
    at org.apache.flink.table.runtime.operators.join.stream.StreamingJoinOperator.processElement1(StreamingJoinOperator.java:124)
    at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory.processRecord1(StreamTwoInputProcessorFactory.java:217)
    at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory.lambda$create$0(StreamTwoInputProcessorFactory.java:183)
    at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessorFactory$StreamTaskNetworkOutput.emitRecord(StreamTwoInputProcessorFactory.java:266)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.processInput(StreamMultipleInputProcessor.java:85)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:542)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:831)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:780)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
    at java.lang.Thread.run(Thread.java:750) {code}
`StreamExecCalc$20690.processElement_split881()` function is below:
{code:java}
private final org.apache.flink.table.data.binary.BinaryStringData str$19838 = org.apache.flink.table.data.binary.BinaryStringData.fromString(""N"");

void processElement_split881(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {
if (isNull$19828) {
            out.setNullAt(4);
          } else {
            out.setNonPrimitiveValue(4, field$19829);
          }
writer$19903.reset();
writer$19903.writeBoolean(0, ((boolean) false));
if (isNull$19830) {
            writer$19903.setNullAt(1);
          } else {
            writer$19903.writeLong(1, field$19830);
          }
isNull$19831 = isNull$19821 || false;
result$19832 = false;
if (!isNull$19831) {
            
          
          result$19832 = field$19821 == ((long) 44571L);
          
            
          }
result$19834 = -1L;
if (result$19832) {
            
            if (!isNull$19833) {
              result$19834 = field$19833;
            }
            isNull$19834 = isNull$19833;
          } else {
            
            if (!false) {
              result$19834 = ((long) 0L);
            }
            isNull$19834 = false;
          }
if (isNull$19834) {
            writer$19903.setNullAt(2);
          } else {
            writer$19903.writeLong(2, result$19834);
          }
isNull$19851 = false;
if (!isNull$19851) {
          if (((org.apache.flink.table.data.binary.BinaryStringData) str$19838).numChars() > 1) {
          result$19852 = ((org.apache.flink.table.data.binary.BinaryStringData) str$19838).substring(0, 1);
          } else {
          if (((org.apache.flink.table.data.binary.BinaryStringData) str$19838).numChars() < 1) {
          
          padLength$19853 = 1 - ((org.apache.flink.table.data.binary.BinaryStringData) str$19838).numChars();
          
          padString$19854 = org.apache.flink.table.data.binary.BinaryStringData.blankString(padLength$19853);
          result$19852 = org.apache.flink.table.data.binary.BinaryStringDataUtil.concat(((org.apache.flink.table.data.binary.BinaryStringData) str$19838), padString$19854);
          } else {
          result$19852 = ((org.apache.flink.table.data.binary.BinaryStringData) str$19838);
          }
          }
          isNull$19851 = result$19852 == null;
          } else {
          result$19852 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
          }
isNull$19855 = isNull$19850;
if (!isNull$19855) {
          if (result$19850.numChars() > 1) {
          result$19856 = result$19850.substring(0, 1);
          } else {
          if (result$19850.numChars() < 1) {
          
          padLength$19857 = 1 - result$19850.numChars();
          
          padString$19858 = org.apache.flink.table.data.binary.BinaryStringData.blankString(padLength$19857);
          result$19856 = org.apache.flink.table.data.binary.BinaryStringDataUtil.concat(result$19850, padString$19858);
          } else {
          result$19856 = result$19850;
          }
          }
          isNull$19855 = result$19856 == null;
          } else {
          result$19856 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
          }
result$19859 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
} {code}
 

Could you take a look at this issue? I could not find why NPE is thrown.

 ",,jackwangcs,luoyuxia,samrat007,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30966,,,,,,FLINK-30559,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 19 08:06:12 UTC 2023,,,,,,,,,,"0|z1c2oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/22 06:51;luoyuxia;[~jackwangcs] Thanks for reporting. Could you please share us the sql so that we can try to reproduce it?;;;","19/May/23 08:06;yunta;I think this problem is the same as FLINK-30559.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Benchmarks are failing,FLINK-30015,13502093,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Yanfei Lei,martijnvisser,martijnvisser,14/Nov/22 07:57,14/Nov/22 14:00,13/Jul/23 08:13,14/Nov/22 14:00,,,,,,,,,,,,,Benchmarks,,,,,,,0,test-stability,,,,"{code:java}
Build interrupted 1411 of flink-master-benchmarks-regression-check (Open): org.jenkinsci.plugins.workflow.steps.FlowInterruptedException
{code}

Build 1405 until 1411 have all failed",,martijnvisser,Yanfei Lei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 14 14:00:13 UTC 2022,,,,,,,,,,"0|z1c2ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 08:03;Yanfei Lei;This was caused by waiting for the available executor.

http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/1411/console
{code:java}
Started by timer
Obtained jenkinsfiles/regression-check.jenkinsfile from git https://github.com/apache/flink-benchmarks.git
Running in Durability level: MAX_SURVIVABILITY
[Pipeline] Start of Pipeline
[Pipeline] timestamps
[Pipeline] {
[Pipeline] timeout
08:38:01  Timeout set to expire in 3 hr 0 min
[Pipeline] {
[Pipeline] node
08:38:16  Still waiting to schedule task
08:38:16  Waiting for next available executor on ‘hetzner EX42-NVMe-1245348’
11:38:01  Cancelling nested steps due to timeout
[Pipeline] // node
[Pipeline] }
[Pipeline] // timeout
[Pipeline] slackSend
11:38:01  Slack Send Pipeline step running, values are - baseUrl: <empty>, teamDomain: apache-flink, channel: #flink-dev-benchmarks, color: <empty>, botUser: false, tokenCredentialId: 2d52c4a5-ab95-42f5-b9b7-eb1a1a95b232, notifyCommitters: false, iconEmoji: <empty>, username: <empty>
[Pipeline] }
[Pipeline] // timestamps
[Pipeline] End of Pipeline
Finished: SUCCESS {code}
 
I changed the build trigger to ""H 11 * * *"" to reduce resource competition.

 ;;;","14/Nov/22 14:00;martijnvisser;Looks to have been resolved, thanks [~Yanfei Lei];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the NPE from aggregate util,FLINK-30014,13502091,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,14/Nov/22 07:46,10/Jan/23 04:05,13/Jul/23 08:13,10/Jan/23 04:05,,,,,,,,ml-2.2.0,,,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,"The following exception is thrown in Flink ML CI step.
{code:java}
[INFO] Running org.apache.flink.ml.feature.CountVectorizerTest
435Error:  Tests run: 12, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 6.419 s <<< FAILURE! - in org.apache.flink.ml.feature.CountVectorizerTest
436Error:  testFitAndPredict  Time elapsed: 0.66 s  <<< ERROR!
437java.lang.RuntimeException: Failed to fetch next result
438	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
439	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
440	at org.apache.commons.collections.IteratorUtils.toList(IteratorUtils.java:848)
441	at org.apache.commons.collections.IteratorUtils.toList(IteratorUtils.java:825)
442	at org.apache.flink.ml.feature.CountVectorizerTest.verifyPredictionResult(CountVectorizerTest.java:120)
443	at org.apache.flink.ml.feature.CountVectorizerTest.testFitAndPredict(CountVectorizerTest.java:208)
444	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
445	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
446	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
447	at java.lang.reflect.Method.invoke(Method.java:498)
448	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
449	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
450	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
451	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
452	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
453	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
454	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
455	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
456	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
457	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
458	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
459	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
460	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
461	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
462	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
463	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
464	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
465	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
466	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
467	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
468	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
469	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
470	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
471	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
472	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
473	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
474	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
475	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
476	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
477	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
478	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
479	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
480	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
481	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
482	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:142)
483	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:109)
484	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
485	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
486	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
487	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
488Caused by: java.io.IOException: Failed to fetch job execution result
489	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
490	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
491	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
492	... 49 more
493Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
494	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
495	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
496	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
497	... 51 more
498Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
499	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
500	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
501	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
502	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
503	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
504	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:138)
505	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:181)
506	... 51 more
507Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
508	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
509	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
510	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)
511	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)
512	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)
513	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)
514	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
515	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
516	at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
517	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
518	at java.lang.reflect.Method.invoke(Method.java:498)
519	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
520	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
521	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
522	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
523	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
524	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
525	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
526	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
527	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
528	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
529	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
530	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
531	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
532	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
533	at akka.actor.Actor.aroundReceive(Actor.scala:537)
534	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
535	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
536	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
537	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
538	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
539	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
540	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
541	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
542	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
543	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
544	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
545Caused by: java.io.IOException: Could not perform checkpoint 1 for operator aggregate -> *anonymous_datastream_source$658*[743] -> TableToDataSteam (1/1)#0.
546	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:1210)
547	at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:147)
548	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.triggerCheckpoint(SingleCheckpointBarrierHandler.java:287)
549	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.access$100(SingleCheckpointBarrierHandler.java:64)
550	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler$ControllerImpl.triggerGlobalCheckpoint(SingleCheckpointBarrierHandler.java:493)
551	at org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlignedBarrierHandlerState.triggerGlobalCheckpoint(AbstractAlignedBarrierHandlerState.java:74)
552	at org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlignedBarrierHandlerState.barrierReceived(AbstractAlignedBarrierHandlerState.java:66)
553	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.lambda$processBarrier$2(SingleCheckpointBarrierHandler.java:234)
554	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.markCheckpointAlignedAndTransformState(SingleCheckpointBarrierHandler.java:262)
555	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.processBarrier(SingleCheckpointBarrierHandler.java:231)
556	at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:181)
557	at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:159)
558	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
559	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
560	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
561	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
562	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
563	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
564	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
565	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
566	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
567	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
568	at java.lang.Thread.run(Thread.java:750)
569Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 1 for operator aggregate -> *anonymous_datastream_source$658*[743] -> TableToDataSteam (1/1)#0. Failure reason: Checkpoint was declined.
570	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:269)
571	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:173)
572	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:348)
573	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.checkpointStreamOperator(RegularOperatorChain.java:227)
574	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.buildOperatorSnapshotFutures(RegularOperatorChain.java:212)
575	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.snapshotState(RegularOperatorChain.java:192)
576	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:647)
577	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:320)
578	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$12(StreamTask.java:1253)
579	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
580	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:1241)
581	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:1198)
582	... 22 more
583Caused by: java.lang.NullPointerException: You cannot add null to a ListState.
584	at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:76)
585	at org.apache.flink.runtime.state.PartitionableListState.add(PartitionableListState.java:94)
586	at org.apache.flink.ml.common.datastream.DataStreamUtils$AggregateOperator.snapshotState(DataStreamUtils.java:463)
587	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:222)
588	... 33 more {code}",,Jiang Xin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-14 07:46:20.0,,,,,,,,,,"0|z1c2k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot remove columns that are incorrectly considered constants from an Aggregate In Streaming,FLINK-30006,13501570,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,13/Nov/22 07:41,06/Feb/23 12:25,13/Jul/23 08:13,06/Feb/23 12:25,1.16.0,,,,,,,1.17.0,,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"In Streaming, columns generated by dynamic functions are incorrectly considered constants and removed from an Aggregate via optimization rule `CoreRules.AGGREGATE_PROJECT_PULL_UP_CONSTANTS` (inside the RelMdPredicates, it only considers the non-deterministic functions, but this doesn't applicable for streaming)

an example query:
{code}
  @Test
  def testReduceGroupKey(): Unit = {
    util.tableEnv.executeSql(""""""
                               |CREATE TABLE t1(
                               | a int,
                               | b varchar,
                               | cat VARCHAR,
                               | gmt_date DATE,
                               | cnt BIGINT,
                               | PRIMARY KEY (cat) NOT ENFORCED
                               |) WITH (
                               | 'connector' = 'values'
                               |)
                               |"""""".stripMargin)
    util.verifyExecPlan(s""""""
                           |SELECT
                           |     cat, gmt_date, SUM(cnt), count(*)
                           |FROM t1
                           |WHERE gmt_date = current_date
                           |GROUP BY cat, gmt_date
                           |"""""".stripMargin)
  }
{code}

the wrong plan:
{code}
Calc(select=[cat, CAST(CURRENT_DATE() AS DATE) AS gmt_date, EXPR$2, EXPR$3])
+- GroupAggregate(groupBy=[cat], select=[cat, SUM(cnt) AS EXPR$2, COUNT(*) AS EXPR$3])
   +- Exchange(distribution=[hash[cat]])
      +- Calc(select=[cat, cnt], where=[=(gmt_date, CURRENT_DATE())])
         +- TableSourceScan(table=[[default_catalog, default_database, t1, filter=[], project=[cat, cnt, gmt_date], metadata=[]]], fields=[cat, cnt, gmt_date])
{code}

expect plan:
{code}
GroupAggregate(groupBy=[cat, gmt_date], select=[cat, gmt_date, SUM(cnt) AS EXPR$2, COUNT(*) AS EXPR$3])
+- Exchange(distribution=[hash[cat, gmt_date]])
   +- Calc(select=[cat, gmt_date, cnt], where=[(gmt_date = CURRENT_DATE())])
      +- TableSourceScan(table=[[default_catalog, default_database, t1, filter=[], project=[cat, gmt_date, cnt], metadata=[]]], fields=[cat, gmt_date, cnt])
{code}

In addition to this issue, we need to check all optimization rules in streaming completely to avoid similar problems.",,csq,leonard,lincoln.86xy,qingyue,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 12:25:13 UTC 2023,,,,,,,,,,"0|z1bzco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 12:25;lincoln.86xy;fixed in master: 01743000c10720598dfb6f27d849da2283772e50;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot resume deployment after suspend with savepoint due to leftover configmaps,FLINK-30004,13501204,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,thw,thw,thw,12/Nov/22 17:57,16/Nov/22 01:26,13/Jul/23 08:13,16/Nov/22 01:26,kubernetes-operator-1.2.0,,,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Due to the possibility of incomplete cleanup of HA data in Flink 1.14, the deployment can get into a limbo state that requires manual intervention after suspend with savepoint. If the config maps are not cleaned up the resumed job will be considered finished and the operator recognize the JM deployment as missing. Due to check for HA data which are now cleaned up, the job fails to start and manual redeployment with initial savepoint is necessary.

This can be avoided by removing any leftover HA config maps after the job has successfully stopped with savepoint (upgrade mode savepoint).",,thw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-12 17:57:22.0,,,,,,,,,,"0|z1bx3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive lookupJoin execution plan parsing error,FLINK-29992,13501073,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,HunterHunter,HunterHunter,11/Nov/22 09:39,16/Nov/22 08:00,13/Jul/23 08:13,16/Nov/22 07:59,1.14.7,1.15.4,1.16.0,1.17.0,,,,,,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"{code:java}
//
tableEnv.executeSql("" CREATE CATALOG hive WITH (\n""
        + ""  'type' = 'hive',\n""
        + "" 'default-database' = 'flinkdebug',\n""
        + "" 'hive-conf-dir' = '/programe/hadoop/hive-3.1.2/conf'\n""
        + "" )"");
tableEnv.executeSql(""create table datagen_tbl (\n""
        + ""id STRING\n""
        + "",name STRING\n""
        + "",age bigint\n""
        + "",ts bigint\n""
        + "",`par` STRING\n""
        + "",pro_time as PROCTIME()\n""
        + "") with (\n""
        + ""  'connector'='datagen'\n""
        + "",'rows-per-second'='10'\n""
        + "" \n""
        + "")"");
String dml1 = ""select * ""
        + "" from datagen_tbl as p ""
        + "" join hive.flinkdebug.default_hive_src_tbl ""
        + "" FOR SYSTEM_TIME AS OF p.pro_time AS c""
        + "" ON p.id = c.id"";
// Execution succeeded
  System.out.println(tableEnv.explainSql(dml1));
String dml2 = ""select p.id ""
        + "" from datagen_tbl as p ""
        + "" join hive.flinkdebug.default_hive_src_tbl ""
        + "" FOR SYSTEM_TIME AS OF p.pro_time AS c""
        + "" ON p.id = c.id"";
// Throw an exception
 System.out.println(tableEnv.explainSql(dml2)); {code}
{code:java}
org.apache.flink.table.api.TableException: Cannot generate a valid execution plan for the given query: FlinkLogicalCalc(select=[id]) +- FlinkLogicalJoin(condition=[=($0, $1)], joinType=[inner])    :- FlinkLogicalCalc(select=[id])    :  +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, datagen_tbl]], fields=[id, name, age, ts, par])    +- FlinkLogicalSnapshot(period=[$cor1.pro_time])       +- FlinkLogicalTableSourceScan(table=[[hive, flinkdebug, default_hive_src_tbl, project=[id]]], fields=[id])This exception indicates that the query uses an unsupported SQL feature. Please check the documentation for the set of currently supported SQL features.    at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:70)     at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)
 
{code}",,HunterHunter,leonard,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 14 02:05:42 UTC 2022,,,,,,,,,,"0|z1bwa8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/22 09:45;HunterHunter;This use case is normal in 1.15.;;;","11/Nov/22 13:00;luoyuxia;[~HunterHunter] Thanks for raising it. I'll fix it asap;;;","12/Nov/22 03:14;luoyuxia;FLINK-29138 introduce project push down for lookup source, then the `PushProjectIntoTableSourceScanRule` will work which will then 
{code:java}
sourceTable.tableSource().copy(); {code}
 But the HiveLookupTableSource doesn't implement copy method which will delegate to it's parent method, and then copy a `HiveTableSource` which is not a lookup source.;;;","14/Nov/22 02:05;leonard;master:a4f9bfd1483ef64b0ed167bd29c98596e3bd5f49
release-1.16: c946b0b95b7b8396eac7f03019eb279becddd301
release-1.15: c257f98bf38668f5bd9d48ccd307f9f76e2463b2
release-1.14: 9cb7b338c90b8183aa7ab4c9e094e47221d182cb
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unparsed SQL for SqlTableLike cannot be parsed correctly,FLINK-29990,13501044,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,icshuo,icshuo,icshuo,11/Nov/22 07:15,11/Nov/22 14:18,13/Jul/23 08:13,11/Nov/22 14:18,1.16.0,,,,,,,1.17.0,,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"Consider the following DDL sql (LIKE without any options):
{code:java}
create table source_table(
  a int,
  b bigint,
  c string
)
LIKE parent_table{code}
After unparsed by sql parser, we get the following result:
{code:java}
CREATE TABLE `SOURCE_TABLE` (
  `A` INTEGER,
  `B` BIGINT,
  `C` STRING
)
LIKE `PARENT_TABLE` (
) {code}
Exception will be thrown if you try to parse the above sql.",,icshuo,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 11 14:18:59 UTC 2022,,,,,,,,,,"0|z1bw3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/22 14:18;libenchao;Fixed via [https://github.com/apache/flink/commit/dd76844342489a252f8b76417090f137028af0bc] 

[~icshuo] thanks for reporting and fixing this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store with Hive3 profile lacks hive-standalone-metastore dependency,FLINK-29983,13500928,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,10/Nov/22 16:05,29/Nov/22 03:37,13/Jul/23 08:13,29/Nov/22 03:37,table-store-0.3.0,,,,,,,table-store-0.3.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"For Hive3,  {{org.apache.hadoop.hive.metastore.api}} is moved to {{{}org.apache.hive:hive-standalone-metastore{}}}. We should shade this package as well to avoid ClassNotFoundException",,lzljs3620320,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28157,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 03:37:34 UTC 2022,,,,,,,,,,"0|z1bve8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/22 03:37;lzljs3620320;master: 6152dbc07a4e802d77f0e2140c345f497306c948;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Session jobs in FINISHED/FAILED state cannot be suspended,FLINK-29974,13500852,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gaborgsomogyi,gyfora,gyfora,10/Nov/22 08:52,02/Dec/22 14:25,13/Jul/23 08:13,02/Dec/22 13:09,kubernetes-operator-1.2.0,,,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"The AbstractFlinkService#cancelSessionJob method currently does not take into consideration the current job state.

This means that if we call this on an already failed/canceld job we will get an exception from Flink:


ava.util.concurrent.ExecutionException: org.apache.flink.runtime.messages.FlinkJobNotFoundException",,gyfora,mbalassi,sriramgr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 02 13:09:04 UTC 2022,,,,,,,,,,"0|z1buxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 18:34;sriramgr;[~gyfora] - [https://github.com/apache/flink-kubernetes-operator/pull/438.] Please check.

 ;;;","28/Nov/22 09:34;gyfora;cc [~gsomogyi] 
Could you please help [~sriramgr] finish this?;;;","02/Dec/22 13:09;mbalassi;[{{ea01e29}}|https://github.com/apache/flink-kubernetes-operator/commit/ea01e294cf1b68d597244d0a11b3c81822a163e7] in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use optimistic locking when patching resource status,FLINK-29959,13500731,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,09/Nov/22 15:00,16/Nov/22 15:49,13/Jul/23 08:13,16/Nov/22 15:49,,,,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"The operator currently does not use optimistic locking on the CR when patching status. This worked because we always wanted to overwrite the status.

With leader election and potentially two operators running at the same time, we are now exposed to some race conditions that were not previously present with the status update logic.

To ensure that the operator always sees the latest status we should change our logic to optimistic locking with retries. If we get a lock error (resource updated) we check if only the spec changed and then retry locking on the new version.",,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 16 15:49:54 UTC 2022,,,,,,,,,,"0|z1bu6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 17:09;gyfora;There are 2 main problems that we target here:
 # Status updates by zombie operators who has last lost leadership but not realized/dead yet.
 # Stale status received when a new leader starts

*Why would these happen?*

Zombie operator:
It could in theory happen that an operator loses leadership in a middle of reconciliation due to a very long GC pause (or some network issue or whatever) and the current CR reconcile loop continues while the new leader already started to reconcile this resource. This is very unlikely but can happen with leader election and a standby operator. In these cases we don't want to allow the old operator who lost leadership to be able to make any status updates. The new logic guarantees that if the new leader made any status update the old would never be able to do so again.

Stale status:
When the new leader starts processing (if it was on standby) there is no guarantee that the status/spec reconciled at the first time is up to date. This can happen because due to some unlucky cache update timing or even a zombie operator submitting late status updates. The current operator logic very much relies on seeing the last status otherwise we can have some very weird cornercases that would definitely cause problems for the resources.

*How the new logic tackles this in a safe way*

What the new logic does is that it basically only allows status updates to go through when the operator has the latest status information. So it's sort of a locking on the current status. If anyone else changed the status in the meantime, we simply throw an error and retrigger the reconciliation. This is actually safe to do as the operator reconcile logic already runs with the assumption that the operator can fail at any time before status update, and we always use the status as a ""write-ahead-log"" of the actions we are taking. In these cases zombie operators who have already lost leadership would never reconcile again (the leader election guarantees that), and in other cases this would give us the latest version of the resource.;;;","16/Nov/22 15:49;gyfora;merged to main 99dc38f1270ae9b13a8b461df8a0bf66e9d8b5f7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ParquetFileStatsExtractorTest is unstable,FLINK-29930,13499664,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,08/Nov/22 11:37,08/Nov/22 11:53,13/Jul/23 08:13,08/Nov/22 11:53,table-store-0.3.0,,,,,,,table-store-0.3.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"https://github.com/apache/flink-table-store/actions/runs/3418971069/jobs/5691913303


[INFO] Running org.apache.flink.table.store.format.parquet.ParquetFileStatsExtractorTest
Error:  Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.919 s <<< FAILURE! - in org.apache.flink.table.store.format.parquet.ParquetFileStatsExtractorTest
Error:  testExtract  Time elapsed: 0.91 s  <<< ERROR!
java.lang.UnsupportedOperationException: type CHAR not supported for extracting statistics in parquet format
	at org.apache.flink.table.store.format.parquet.ParquetFileStatsExtractor.toFieldStats(ParquetFileStatsExtractor.java:85)
	at org.apache.flink.table.store.format.parquet.ParquetFileStatsExtractor.lambda$extract$0(ParquetFileStatsExtractor.java:73)
	at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250)
	at java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:110)
	at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:546)
	at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260)
	at java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:505)
	at org.apache.flink.table.store.format.parquet.ParquetFileStatsExtractor.extract(ParquetFileStatsExtractor.java:75)
	at org.apache.flink.table.store.format.FileStatsExtractorTestBase.testExtract(FileStatsExtractorTestBase.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.util.ArrayList.forEach(ArrayList.java:1259)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.util.ArrayList.forEach(ArrayList.java:1259)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:142)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:109)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

[INFO] Running org.apache.flink.table.store.format.parquet.ParquetFileFormatTest
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.037 s - in org.apache.flink.table.store.format.parquet.ParquetFileFormatTest
[INFO] ",,lzljs3620320,zjureel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 11:53:06 UTC 2022,,,,,,,,,,"0|z1bnlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 11:53;lzljs3620320;master: 6104b46f608ae67c6704590045a83db58dd4444c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AkkaUtils#getAddress may cause memory leak,FLINK-29927,13499637,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,pltbkd,pltbkd,08/Nov/22 09:20,09/Nov/22 09:40,13/Jul/23 08:13,09/Nov/22 09:40,1.15.2,1.16.0,,,,,,1.15.3,1.16.1,1.17.0,,,Runtime / RPC,,,,,,,0,pull-request-available,,,,"We found a slow memory leak in JM. When MetricFetcherImpl tries to retrieve metrics, it always call MetricQueryServiceRetriever#retrieveService first. And the method will acquire the address of a task manager, which will use AkkaUtil#getAddress internally. While the getAddress method is implemented like this:

{code:java}
    public static Address getAddress(ActorSystem system) {
        return new RemoteAddressExtension().apply(system).getAddress();
    }
{code}

and the RemoteAddressExtension#apply is like this:

{code:scala}
  def apply(system: ActorSystem): T = {
    java.util.Objects.requireNonNull(system, ""system must not be null!"").registerExtension(this)
  }
{code}

This means every call of AkkaUtils#getAddress will register a new extension to the ActorSystem, and can never be released until the ActorSystem exits.

Most of the usage of the method are called only once while initializing, but as described above, MetricFetcherImpl will also use the method. It can happens periodically while users open the WebUI, or happens when the users call the RESTful API directly to get metrics. This means the memory may keep leaking. 

The leak may be introduced in FLINK-23662 when porting the scala version of AkkaUtils to the java one, while I'm not sure if the scala version has the same issue.

The leak seems very slow. We observed it on a job running for more than one month with only 1G memory for job manager. So I suppose it's not an emergency one but still needs to fix.
",,pltbkd,qingyue,tanyuxin,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/22 09:26;pltbkd;RemoteAddressExtensionLeaking.png;https://issues.apache.org/jira/secure/attachment/13051936/RemoteAddressExtensionLeaking.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 09 09:40:23 UTC 2022,,,,,,,,,,"0|z1bnfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 10:25;chesnay;Unless I'm several underestimating Scala magic this leak should've been there for a long time.

Should be easy to fix though; I've opened a PR.;;;","09/Nov/22 09:40;chesnay;master: 304122eadc52dd6ee8c04d9777d97eb66aec5e0e
1.16: 313e30483e9770d18461b5dc655da423d465b7e3
1.15: 6feaa440ffce2afc6b0222c49de598b94e60c825;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hybrid Shuffle may face deadlock when running a task need to execute big size data,FLINK-29923,13499310,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,AlexXXX,AlexXXX,08/Nov/22 04:03,13/Dec/22 04:32,13/Jul/23 08:13,13/Dec/22 04:32,1.16.0,,,,,,,1.16.1,1.17.0,,,,Runtime / Network,,,,,,,0,,,,,"The flink 1.16 offers hybrid shuffle to combine the superiority of blocking shuffle and pipeline shuffle. But when I want to test this new feature I face a problem that it may cause deadlock when it running. 

Actually, it will run well at beginning. However, when it runs to a certain number it may failure for the buffer size and if I set a bigger size it may running without data execution like the picture. So I want to ask the cause of this problem and a solution.",,AlexXXX,huwh,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29298,,,,,,,,,,,,,"08/Nov/22 04:03;AlexXXX;性能差距.png;https://issues.apache.org/jira/secure/attachment/13051923/%E6%80%A7%E8%83%BD%E5%B7%AE%E8%B7%9D.png","08/Nov/22 04:03;AlexXXX;死锁2-select.png;https://issues.apache.org/jira/secure/attachment/13051925/%E6%AD%BB%E9%94%812-select.png","08/Nov/22 04:03;AlexXXX;死锁检测.png;https://issues.apache.org/jira/secure/attachment/13051924/%E6%AD%BB%E9%94%81%E6%A3%80%E6%B5%8B.png",,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 13 04:32:51 UTC 2022,,,,,,,,,,"0|z1blew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 04:09;xtsong;Thanks for reporting this, [~AlexXXX].

[~Weijie Guo], could you take a look at this?;;;","08/Nov/22 04:22;Weijie Guo;[~AlexXXX] Thanks for the feedback. If I'm not wrong, the reason for the failure should be insufficient network memory or batch read memory, and this is an expected behavior. After all, pipelined execution requires more resources than all blocking. So now we have to solve the problem that the task thread is stuck. Can you provide more detailed information, such as the thread dump of the stuck subtask. In addition, if it is difficult to describe the problem clearly, you can communicate with me offline via wechat(a644813550) or any other contact ways you want.;;;","08/Nov/22 07:35;Weijie Guo;Through offline discussion with [~AlexXXX] , it is true that the task are stuck forever. Further, the cause of the problem should be the same as FLINK-29298 previously reported. It is a bug in the `LocalBufferPool`, and hybrid shuffle does increase the competition of network buffers, which makes it difficult to reproduce this bug under blocking shuffle, but it almost repeats under the specific query of hybrid shuffle, so I think it should be considered as a very serious bug.;;;","08/Nov/22 08:05;Weijie Guo;Casued by FLINK-29298;;;","13/Dec/22 04:32;xtsong;Should have been fixed by FLINK-29298.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minor reformat Kafka connector documentation,FLINK-29920,13499119,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,liuml07,liuml07,liuml07,07/Nov/22 22:59,09/Nov/22 02:01,13/Jul/23 08:13,09/Nov/22 02:01,1.16.0,,,,,,,1.17.0,,,,,Documentation,,,,,,,0,pull-request-available,,,,"We used some HTML tag in the documentation which does not interpret Markdown format nicely. This fixes this by replacing with Markdown tags.

",,jark,liuml07,yanxinyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17831,,,,,,"07/Nov/22 22:58;liuml07;Screenshot 2022-11-07 at 2.55.00 PM.png;https://issues.apache.org/jira/secure/attachment/13051909/Screenshot+2022-11-07+at+2.55.00+PM.png","07/Nov/22 22:59;liuml07;Screenshot 2022-11-07 at 2.55.08 PM.png;https://issues.apache.org/jira/secure/attachment/13051908/Screenshot+2022-11-07+at+2.55.08+PM.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 09 02:01:03 UTC 2022,,,,,,,,,,"0|z1bk8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/22 02:01;jark;Fixed in master: 9c1a1a18c4cf968e7e627cc9b542bb60faf9dfb3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Levels in Table Store may mistakenly ignore level 0 files when two files have the same sequence number,FLINK-29916,13498269,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,07/Nov/22 09:30,08/Nov/22 12:20,13/Jul/23 08:13,08/Nov/22 12:20,table-store-0.2.2,table-store-0.3.0,,,,,,table-store-0.2.2,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,"Current constructor of {{Levels}} class contains the following code:

{code:java}
this.level0 = new TreeSet<>(Comparator.comparing(DataFileMeta::maxSequenceNumber).reversed());
{code}

However when two or more jobs writing the same bucket, they may produce files containing the same sequence number. If two files have the same {{maxSequenceNumber}}, one of them will be mistakenly ignored by {{TreeSet}}.",,dianer17,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 12:20:35 UTC 2022,,,,,,,,,,"0|z1bezs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 12:20;TsReaper;master: 95dd0c32da31b6528e96988b21e01126f3c24b7d
release-0.2: 1c66dbf7aa05e18acc8724eed3a48f005326311d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
netty-tcnative-static not built on CI,FLINK-29915,13498179,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,07/Nov/22 08:37,23/Nov/22 14:58,13/Jul/23 08:13,23/Nov/22 14:58,shaded-16.0,,,,,,,shaded-17.0,,,,,BuildSystem / Shaded,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28246,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 23 14:58:08 UTC 2022,,,,,,,,,,"0|z1befs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/22 08:48;chesnay;LicenseChecker fails categorically because the static netty dependencies use qualifiers that aren't currently supported.;;;","23/Nov/22 14:58;chesnay;shaded-master: 0cf919601e310c4eb6f982ada2aa11625b3d26b5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaTableITCase.testKafkaSourceSink fails,FLINK-29914,13498101,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,mapohl,mapohl,07/Nov/22 08:07,30/Jan/23 08:02,13/Jul/23 08:13,24/Nov/22 11:07,1.15.3,,,,,,,1.17.0,,,,,Connectors / Kafka,,,,,,,0,pull-request-available,test-stability,,,"[This build failed|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42857&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=918e890f-5ed9-5212-a25e-962628fb4bc5&l=36412] due to an error in {{{}KafkaTableITCase.testKafkaSourceSink{}}}:
{code:java}
Caused by: org.apache.kafka.common.errors.TimeoutException: Topic tstopic_avro not present in metadata after 60000 ms. {code}",,gaborgsomogyi,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24119,,FLINK-25438,FLINK-30822,,,,FLINK-29956,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 25 07:57:49 UTC 2022,,,,,,,,,,"0|z1bdyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/22 08:55;gaborgsomogyi;All other Kafka tests are also unstable. While I was hanging around the mentioned test I've found a relatively big test code mistake which introduces race. Let me check whether it helps in this case...;;;","07/Nov/22 08:57;mapohl;Awesome, thanks for picking up on that one [~gaborgsomogyi];;;","07/Nov/22 09:19;gaborgsomogyi;Well, not saying I'm able to fix this issue but I've found a fundamental issue which is definitely a blocker in this area :);;;","07/Nov/22 09:28;gaborgsomogyi;I've added a PR, let's see the result. The test failures are super annoying and would be good to reach a better stability.;;;","23/Nov/22 09:01;gaborgsomogyi;Can we close this jira or the issue popped up again?;;;","24/Nov/22 11:09;gaborgsomogyi;master: 9b6bae4eb87e1f472fd0f6cf9403911a88ed89ce
release-1.16: 9e7ebdc671386a8127ddf5affb66e997d877cb7b
release-1.15: 0f2c6bc80cd8c8cb3dbc442f6c981ff4c095694a
;;;","25/Nov/22 03:40;mapohl;[~gaborgsomogyi] what's the intention behind the release notes in this ticket? This would show up in the release notes of 1.17 if we keep it like that. It feels like it should rather be a comment instead.;;;","25/Nov/22 07:56;gaborgsomogyi;Oh gosh, I've put it into the wrong place :/;;;","25/Nov/22 07:57;gaborgsomogyi;Please re-open it if the issue comes again.;;;","25/Nov/22 07:57;gaborgsomogyi;Moved to comment section, where it belongs. Thanks for notifying...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stacktrace printing in DefaultExecutionGraphCacheTest is confusing maven test log output,FLINK-29899,13495972,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mapohl,renqs,renqs,05/Nov/22 06:09,25/Nov/22 08:12,13/Jul/23 08:13,25/Nov/22 08:12,1.15.3,1.16.0,1.17.0,,,,,1.15.4,1.16.1,1.17.0,,,Runtime / Task,,,,,,,0,pull-request-available,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42849&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8700]

 

 
{code:java}
java.util.concurrent.ExecutionException: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (408c1ab89f41c2d4f99c870e8abde94d)
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCacheTest.testImmediateCacheInvalidationAfterFailure(DefaultExecutionGraphCacheTest.java:147)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
{code}
 

 ",,mapohl,martijnvisser,renqs,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 25 08:12:42 UTC 2022,,,,,,,,,,"0|z1b0tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/22 09:20;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43045&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=41438;;;","11/Nov/22 09:20;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43045&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc&l=38408;;;","17/Nov/22 09:55;zhuzh;The exception seems to be unrelated.

I saw in the weekly updates that ""Matthias and Qingsheng investigated the memory issue due to multiple azure agents on one machine use too much resources. We’ve reduced the agents number from 7 to 5, let’s keep an eyes on this issue.""
[~renqs] does it refer to this JIRA?;;;","25/Nov/22 04:19;mapohl;You are right, [~zhuzh] . The error presented in the issue description is misleading. We shouldn't print the stacktrace. I guess, that was accidentally added in [DefaultExecutionGraphCacheTest:151|https://github.com/apache/flink/blob/ad47fe246f7416c5685d1a38b8b8fb44ec503e86/flink-runtime/src/test/java/org/apache/flink/runtime/rest/handler/legacy/DefaultExecutionGraphCacheTest.java#L151].

Anyway, about the 137 exit code that's mentioned in the issue title: It's quite likely caused by some memory leak in one of the {{flink-table}} modules. It's just that other modules are affected by it if they run on the same machine at the same time.

I'm gonna go ahead and rephrase the title of this Jira issue and remove the stacktrace printing.;;;","25/Nov/22 08:12;mapohl;master: cc66d4855e6f8ee9986809a18f68a458bcfe3c12
1.16: 5f9d2519a3cf3adb9e76de416f5100db167d8afc
1.15: 60eedc15671a6bd8c98241b3180e25f9efb30c25;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DynamoDB CI Failing to run checks,FLINK-29896,13495886,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,darenwkt,dannycranmer,dannycranmer,04/Nov/22 20:21,04/Nov/22 22:03,13/Jul/23 08:13,04/Nov/22 22:03,,,,,,,,aws-connector-3.0.0,,,,,Connectors / DynamoDB,,,,,,,0,pull-request-available,,,,"The checks are failing to actually run the build and test step:
- https://github.com/apache/flink-connector-aws/actions/runs/3396739520/jobs/5648279513

{code}
[INFO] Downloading from central: https://repo.maven.apache.org/maven2/%20-DaltDeploymentRepository=validation_repository/default/file/default-file.pom
Warning:  The POM for  -DaltDeploymentRepository=validation_repository:default:jar:file is missing, no dependency information available

Error:  Plugin  -DaltDeploymentRepository=validation_repository:default:file or one of its dependencies could not be resolved: Could not find artifact  -DaltDeploymentRepository=validation_repository:default:jar:file in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots) -> [Help 1]
Error:  
Error:  To see the full stack trace of the errors, re-run Maven with the -e switch.
Error:  Re-run Maven using the -X switch to enable full debug logging.
Error:  
Error:  For more information about the errors and possible solutions, please read the following articles:
Error:  [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginResolutionException
{code}",,dannycranmer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 22:03:48 UTC 2022,,,,,,,,,,"0|z1b0ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 22:03;dannycranmer;merged commit [399b3af|https://github.com/apache/flink-connector-aws/commit/399b3af7f0400e22b200c949f07970e13c0a901b] into main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
netty-tcnative-static does not bundle tcnative-classes,FLINK-29889,13495571,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,04/Nov/22 14:36,08/Nov/22 10:44,13/Jul/23 08:13,08/Nov/22 10:43,shaded-16.0,,,,,,,shaded-16.1,shaded-17.0,,,,BuildSystem / Shaded,,,,,,,0,pull-request-available,,,,"The shade plugin configuration was not adjusted to also bundle the tcnative classes, which were previously pulled in via other bundled artifacts.",,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29886,FLINK-29874,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 10:43:57 UTC 2022,,,,,,,,,,"0|z1aycg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 10:43;chesnay;master: 3082afc952e68366e9fefe4d1181c4666969ee67
16: 8bd55d79dc721a6bb7749243e33e7e446081c8ca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test failure in finegrained_resource_management/SortMergeResultPartitionTest.testRelease,FLINK-29884,13495417,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,nkruber,nkruber,04/Nov/22 09:05,02/Mar/23 15:31,13/Jul/23 08:13,02/Mar/23 15:31,1.16.1,1.17.0,,,,,,1.17.0,,,,,Runtime / Coordination,Runtime / Network,Tests,,,,,0,pull-request-available,test-stability,,,"{{SortMergeResultPartitionTest.testRelease}} failed with a timeout in the finegrained_resource_management tests:
{code:java}
Nov 03 17:28:07 [ERROR] Tests run: 20, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 64.649 s <<< FAILURE! - in org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionTest
Nov 03 17:28:07 [ERROR] SortMergeResultPartitionTest.testRelease  Time elapsed: 60.009 s  <<< ERROR!
Nov 03 17:28:07 org.junit.runners.model.TestTimedOutException: test timed out after 60 seconds
Nov 03 17:28:07 	at org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionTest.testRelease(SortMergeResultPartitionTest.java:374)
Nov 03 17:28:07 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Nov 03 17:28:07 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Nov 03 17:28:07 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Nov 03 17:28:07 	at java.lang.reflect.Method.invoke(Method.java:498)
Nov 03 17:28:07 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Nov 03 17:28:07 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Nov 03 17:28:07 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Nov 03 17:28:07 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Nov 03 17:28:07 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Nov 03 17:28:07 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Nov 03 17:28:07 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
Nov 03 17:28:07 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
Nov 03 17:28:07 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Nov 03 17:28:07 	at java.lang.Thread.run(Thread.java:748) {code}
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42806&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7]",,kevin.cyj,mapohl,martijnvisser,nkruber,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29008,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 15:31:50 UTC 2023,,,,,,,,,,"0|z1axe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/22 09:17;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43045&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=8360;;;","17/Nov/22 10:06;kevin.cyj;I will try to reproduce it and will update if any progress.;;;","28/Nov/22 09:38;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43512&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=7604;;;","28/Nov/22 13:17;mapohl;I assigned the ticket to you for now, [~yingjie];;;","09/Jan/23 08:19;mapohl;This time, we have a thread dump provided for the timeout of this test:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44569&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=8735;;;","09/Jan/23 08:20;mapohl;[~yingjie] did you manage to look into it?;;;","15/Jan/23 06:52;kevin.cyj;Sorry for the delay, I already found the root cause. It is only a test issue and I will submit a PR soon. ;;;","18/Jan/23 03:23;Weijie Guo;Thanks [~kevin.cyj] for analyzing and telling me the cause of this problem, I have opened a PR, PTAL~;;;","19/Jan/23 01:16;kevin.cyj;Merged into master via 0b8a83ce54d39d0d5a5b82573c5037f306e9f7f7.;;;","02/Mar/23 09:13;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46686&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=7837

We ran into this issue in 1.16 as well. I'm reopening the issue. [~Weijie Guo] Can you provide a backport?;;;","02/Mar/23 09:47;Weijie Guo;[~mapohl] Thanks for reopening this, I have created the [backport pr|https://github.com/apache/flink/pull/22070], we can merge it after CI passed.;;;","02/Mar/23 15:31;Weijie Guo;release-1.16 via c05c7722e19f3525cf9444cc74ac8071480102d5.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LargeDataITCase is not stable,FLINK-29882,13495311,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,qingyue,qingyue,04/Nov/22 07:58,19/Mar/23 05:43,13/Jul/23 08:13,19/Mar/23 05:43,table-store-0.3.0,,,,,,,table-store-0.4.0,,,,,Table Store,,,,,,,0,,,,,https://github.com/apache/flink-table-store/actions/runs/3391781964/jobs/5637271002,,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-04 07:58:28.0,,,,,,,,,,"0|z1awqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to use flink-sql-connector-hive-3.1.3,FLINK-29878,13495179,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,samrat007,luoyuxia,luoyuxia,04/Nov/22 04:19,31/Jan/23 04:21,13/Jul/23 08:13,31/Jan/23 04:21,1.17.0,,,,,,,1.17.0,,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,"When I try to use flink-sql-connector-hive-3.1.3, it will throw the following exception:
{code:java}
ava.lang.NoClassDefFoundError: org/apache/hadoop/hive/conf/HiveConfUtil
    at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:5170) ~[flink-sql-connector-hive-3.1.3_2.12-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
    at org.apache.hadoop.hive.conf.HiveConf.<init>(HiveConf.java:5114) ~[flink-sql-connector-hive-3.1.3_2.12-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
    at org.apache.flink.table.catalog.hive.HiveCatalog.createHiveConf(HiveCatalog.java:261) ~[flink-sql-connector-hive-3.1.3_2.12-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
    at org.apache.flink.table.endpoint.hive.HiveServer2EndpointFactory.createSqlGatewayEndpoint(HiveServer2EndpointFactory.java:71) ~[flink-sql-connector-hive-3.1.3_2.12-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]{code}
After I don't exclude it in the pom,
{code:java}
<exclude>org/apache/hadoop/hive/conf/HiveConf.class</exclude>
<exclude>org/apache/hadoop/hive/metastore/HiveMetaStoreClient.class</exclude> {code}
it'll throw the exception:
{code:java}
aused by: java.lang.NoClassDefFoundError: com/facebook/fb303/FacebookService$Iface
    at java.lang.ClassLoader.defineClass1(Native Method) ~[?:1.8.0_252]
    at java.lang.ClassLoader.defineClass(ClassLoader.java:756) ~[?:1.8.0_252]
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) ~[?:1.8.0_252]
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:468) ~[?:1.8.0_252]
    at java.net.URLClassLoader.access$100(URLClassLoader.java:74) ~[?:1.8.0_252]
    at java.net.URLClassLoader$1.run(URLClassLoader.java:369) ~[?:1.8.0_252]
    at java.net.URLClassLoader$1.run(URLClassLoader.java:363) ~[?:1.8.0_252]
    at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_252]
    at java.net.URLClassLoader.findClass(URLClassLoader.java:362) ~[?:1.8.0_252]
    at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_252]
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352) ~[?:1.8.0_252]
    at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_252]
    at java.lang.Class.forName0(Native Method) ~[?:1.8.0_252]
    at java.lang.Class.forName(Class.java:348) ~[?:1.8.0_252]
    at org.apache.hadoop.hive.metastore.utils.JavaUtils.getClass(JavaUtils.java:52) ~[flink-sql-connector-hive-3.1.3_2.12-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:146) ~[flink-sql-connector-hive-3.1.3_2.12-1.17-SNAPSHOT.jar:1.17-SNAPSHOT] {code}
Then i add the dependency. Seems it works now.
{code:java}
<dependency>
   <groupId>org.apache.thrift</groupId>
   <artifactId>libfb303</artifactId>
   <version>0.9.3</version>
</dependency> {code}
 ",,dannycranmer,luoyuxia,renqs,samrat007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30034,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 04:20:59 UTC 2023,,,,,,,,,,"0|z1avxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 04:22;luoyuxia;Introduce by FLINK-29478.

[~samrat007] Could you please have a look? Is it possible to add these back?;;;","04/Nov/22 07:05;samrat007;Yes ! looking into it ;;;","08/Nov/22 08:18;dannycranmer;merged commit [a777478|https://github.com/apache/flink/commit/a77747892b1724fa5ec388c2b0fe519db32664e9] into master;;;","18/Nov/22 10:20;chesnay;The added dependency is not bundled in the sql jar, since the shade-plugin artifactSet was not adjusted accordingly.;;;","22/Nov/22 08:15;samrat007;I did the testing manually for the flinksql jar. 
I will recheck and update with changes . ;;;","31/Jan/23 04:20;renqs;Fixed on master: b1a9fe12f78999e555204a69d7e360705dc88450;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dependency convergence error for org.osgi:org.osgi.core:jar,FLINK-29868,13494580,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,nkruber,nkruber,03/Nov/22 12:22,04/Nov/22 09:09,13/Jul/23 08:13,04/Nov/22 09:09,1.17.0,,,,,,,1.17.0,,,,,Build System,Table SQL / Runtime,,,,,,0,,,,,"While working on FLINK-29867, the following new error is popping up while running

{code}
./mvnw clean install -pl flink-dist -am -DskipTests -Dflink.convergence.phase=install -Pcheck-convergence
{code}

(this is also done by CI which therefore fails)

{code}
[WARNING] 
Dependency convergence error for org.osgi:org.osgi.core:jar:4.3.0:runtime paths to dependency are:
+-org.apache.flink:flink-table-planner-loader-bundle:jar:1.17-SNAPSHOT
  +-org.apache.flink:flink-table-planner_2.12:jar:1.17-SNAPSHOT:runtime
    +-org.apache.flink:flink-table-api-java-bridge:jar:1.17-SNAPSHOT:runtime
      +-org.apache.flink:flink-streaming-java:jar:1.17-SNAPSHOT:runtime
        +-org.apache.flink:flink-runtime:jar:1.17-SNAPSHOT:runtime
          +-org.xerial.snappy:snappy-java:jar:1.1.8.3:runtime
            +-org.osgi:org.osgi.core:jar:4.3.0:runtime
and
+-org.apache.flink:flink-table-planner-loader-bundle:jar:1.17-SNAPSHOT
  +-org.apache.flink:flink-table-planner_2.12:jar:1.17-SNAPSHOT:runtime
    +-org.apache.flink:flink-scala_2.12:jar:1.17-SNAPSHOT:runtime
      +-org.apache.flink:flink-core:jar:1.17-SNAPSHOT:runtime
        +-org.apache.commons:commons-compress:jar:1.21:runtime
          +-org.osgi:org.osgi.core:jar:6.0.0:runtime
{code}",,nkruber,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29867,,,,,,,,,,,,,,,,,,,MENFORCER-437,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 09:09:43 UTC 2022,,,,,,,,,,"0|z1as88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/22 12:45;chesnay;The strange thing is that osgi.core is listed as a runtime dependency, when it is actually provided.;;;","04/Nov/22 09:09;nkruber;fixed in {{master}} via [614fc2a5fd301789f7daa1282a5b500bf8d67d4b|https://github.com/apache/flink/commit/614fc2a5fd301789f7daa1282a5b500bf8d67d4b];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar connector bug when using Hybrid.Builder,FLINK-29860,13494512,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,noelo,noelo,noelo,03/Nov/22 07:15,10/Nov/22 11:28,13/Jul/23 08:13,10/Nov/22 11:28,,,,,,,,1.17.0,,,,,Connectors / Pulsar,,,,,,,0,pull-request-available,,,,"When using a HybridSource with a set of pulsar sources submitting a job to a flink cluster results in the following error

------------------------------------------------------------
 The program finished with the following exception:

The implementation of the BlockElement is not serializable. The object probably contains or references non serializable fields.
    org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:164)
    org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:132)
    org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:132)
    org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:132)
    org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:132)
    org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:69)
    org.apache.flink.connector.base.source.hybrid.HybridSource$HybridSourceBuilder.addSource(HybridSource.java:246)
    org.apache.flink.connector.base.source.hybrid.HybridSource$HybridSourceBuilder.addSource(HybridSource.java:233)
    org.apache.flink.connector.base.source.hybrid.HybridSource.builder(HybridSource.java:104)

 

I think this is related to https://issues.apache.org/jira/browse/FLINK-25444

From a pulsar connector perspective it's simple fixed, just mark the ""private final InlineElement desc"" attribute in flink-connectors/flink-connector-pulsar/src/main/java/org/apache/flink/connector/pulsar/source/config/CursorVerification.java as transient to avoid the serialisation process.

 

I've tested this and it seems to solve the issue. I can submit a PR with this fix.

 ",,noelo,syhily,tison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 10 11:28:20 UTC 2022,,,,,,,,,,"0|z1art4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 01:34;syhily;I think we can just remove the extra document field for {{CursorVerification}}.;;;","08/Nov/22 07:01;tison;[~syhily] [~noelo] I'd prefer the solution by setting {{InlineElement}} as {{transient}}.

[~noelo] if you're still willing to submit a patch, please submit one and I'll assign to you.;;;","08/Nov/22 07:46;noelo;[~syhily] yep I do have a patch so assign me. I assume this is just for 1.17 snapshot and not other versions ?;;;","08/Nov/22 21:19;syhily;OK. I'll close my PR and in favor of the Noel's one. [~noelo] Can you help me add the {{transient}} field for {{TopicRoutingMode}} and {{MessageKeyHash}} in your PR?;;;","09/Nov/22 07:09;noelo;[~syhily] yep I'll do that today;;;","09/Nov/22 08:45;noelo;[~syhily] PR updated;;;","10/Nov/22 11:28;tison;master via fa6d18818348b052876ed5db16ecc6d5f5bad30c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TPC-DS end-to-end test with adaptive batch scheduler failed due to oo non-empty .out files.,FLINK-29859,13494506,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,JunRuiLi,leonard,leonard,03/Nov/22 06:30,28/Feb/23 12:05,13/Jul/23 08:13,28/Feb/23 12:05,1.16.0,1.17.0,,,,,,1.16.2,1.17.0,,,,Tests,,,,,,,0,pull-request-available,test-stability,,,"
Nov 03 02:02:12 [FAIL] 'TPC-DS end-to-end test with adaptive batch scheduler' failed after 21 minutes and 44 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out files 


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42766&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a

",,leonard,mapohl,renqs,wanglijie,Weijie Guo,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 28 12:05:30 UTC 2023,,,,,,,,,,"0|z1arrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 03:33;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43077&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;","05/Dec/22 10:58;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43707&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=9816;;;","13/Dec/22 07:20;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43904&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=39a035c3-c65e-573c-fb66-104c66c28912;;;","13/Dec/22 08:26;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43910&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=9557;;;","19/Dec/22 09:48;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44025&view=logs&j=f8e16326-dc75-5ba0-3e95-6178dd55bf6c&t=15c1d318-5ca8-529f-77a2-d113a700ec34&l=11938;;;","20/Dec/22 07:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44084&view=logs&j=f8e16326-dc75-5ba0-3e95-6178dd55bf6c&t=15c1d318-5ca8-529f-77a2-d113a700ec34&l=9253;;;","23/Dec/22 07:50;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44184&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=7938;;;","23/Dec/22 07:52;mapohl;[~wanglijie] may you have a look at this issue?;;;","23/Dec/22 08:01;wanglijie;[~mapohl] I will take a look soon.;;;","28/Dec/22 07:14;wanglijie;Fixed via

master 14a61f368332320d7e38cc93a04f95bb63c66788

release-1.16 9e51cb8c117fd9a5f887e0a0e8faee4ff11462ea;;;","05/Jan/23 07:38;mapohl;I'm reopening this issue because the error reappeared even though it contained the fix of 9e51cb8c117fd9a5f887e0a0e8faee4ff11462ea in the {{release-1.16}} branch:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44443&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=39a035c3-c65e-573c-fb66-104c66c28912&l=8611

[~wanglijie] may you have a look?;;;","05/Jan/23 15:25;wanglijie;[~mapohl] I 'll take a look.;;;","19/Jan/23 13:37;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45036&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678&l=6279;;;","19/Jan/23 13:38;mapohl;[~wanglijie] any updates on that one?;;;","25/Jan/23 08:30;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45184&view=logs&j=f8e16326-dc75-5ba0-3e95-6178dd55bf6c&t=15c1d318-5ca8-529f-77a2-d113a700ec34&l=8267;;;","30/Jan/23 14:06;mapohl;[~wanglijie] Have you had the chance to take a look at it?;;;","30/Jan/23 14:16;wanglijie;[~mapohl]  Sorry for the late reply, I'll take a look and prepare a fix this week.;;;","02/Feb/23 03:03;zhuzh;Fixed via
master:
19f2230a4758f471305727ad82d36984ebbc7e3a

release-1.16:
e6a91aa39fc425412a1a3ab32c2c479a970ae439;;;","22/Feb/23 09:55;mapohl;I'm reopening this issue again because of the following test failure in release-1.16:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46384&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=9287

There are exceptions caught in the TaskExecutor like:
{code}
2023-02-22 03:42:04,372 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Cannot find task to fail for execution d77a85f5ec62862896b264410c77e177_406c896546ef6157dd2c29ccb928b2d0_6_1 with exception:
org.apache.flink.runtime.jobmaster.ExecutionGraphException: The execution attempt d77a85f5ec62862896b264410c77e177_406c896546ef6157dd2c29ccb928b2d0_6_1 was not found.
        at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:483) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
        at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source) ~[?:?]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_362]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_362]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_3ecc1910-ccd9-46c3-b55e-5f6996125a24.jar:1.16-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_362]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_362]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_362]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_362]
{code}

Other previously reported test instabilities contained this error as well. Therefore, it looks like it's not fixed, yet. [~wanglijie] may you have another look at it?;;;","22/Feb/23 15:00;wanglijie;[~mapohl] I 'll have a look.;;;","27/Feb/23 10:15;mapohl;1.16: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46543&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678&l=9218];;;","28/Feb/23 12:05;zhuzh;master:
28e75620db7b0794ff1c19f8928ce7d33516ba64

release-1.17:
3de6b337cceda5b4b81ac8d34e099e3f1c802df9

release-1.16:
5b9398ae067a7073cadca25e883b278f6e7cd5bd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix failure to connect to 'HiveServer2Endpoint' when using hive3 beeline,FLINK-29857,13494489,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yzl,yzl,yzl,03/Nov/22 03:12,10/Nov/22 03:46,13/Jul/23 08:13,10/Nov/22 03:44,1.16.0,,,,,,,1.16.1,,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,"Hive3 add a new 'TGetInfoType' value 'CLI_ODBC_KEYWORDS', but 'HiveServer2Endpoint' doesn't handle this value, causing crush when connecting to Hive metastore through 'HiveServer2Endpoint' by Hive3 Beeline.",,fsk119,yzl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29839,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 10 03:44:07 UTC 2022,,,,,,,,,,"0|z1aro0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/22 03:44;fsk119;release-1.16: a74023501e3f48c6a0c61cab8b1d212b8ea83ee9
master: 194df8de947baf30995371d87decec2b7c470610;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Older jackson-databind found in flink-kubernetes-operator-1.2.0-shaded.jar,FLINK-29853,13494372,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jbusche,jbusche,jbusche,02/Nov/22 17:44,13/Dec/22 01:41,13/Jul/23 08:13,10/Nov/22 10:34,kubernetes-operator-1.2.1,,,,,,,kubernetes-operator-1.3.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"A Twistlock security scan of the existing 1.2.0 operator as well as the current main release shows a high vulnerability with the current jackson-databind version.

======
severity: High

cvss: 7.5

riskFactors:  Attack complexity: low,Attack vector: network,Has fix,High severity,Recent vulnerability

CVE link: [https://nvd.nist.gov/vuln/detail/CVE-2022-42003]

packageName: com.fasterxml.jackson.core_jackson-databind

packagePath: /flink-kubernetes-operator/flink-kubernetes-operator-1.2.0-shaded.jar and/or /flink-kubernetes-operator/flink-kubernetes-operator-1.3-SNAPSHOT-shaded.jar

description: In FasterXML jackson-databind before 2.14.0-rc1, resource exhaustion can occur because of a lack of a check in primitive value deserializers to avoid deep wrapper array nesting, when the UNWRAP_SINGLE_VALUE_ARRAYS feature is enabled. Additional fix version in 2.13.4.1 and 2.12.17.1

====

This is exactly like the older issue https://issues.apache.org/jira/browse/FLINK-27654 

I'm going to see if I can fix it myself and create a PR if I'm successful.",,gyfora,jbusche,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 10 10:34:13 UTC 2022,,,,,,,,,,"0|z1aqy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/22 20:46;jbusche;OK - I tried changing the pom.xml from:

<version>2.13.4</version>

to

<version>2.13.4.1</version>

But that version wasn't found.

 

I then tried  <version>2.14.0-rc1</version>

and this solves the vulnerability, but not sure we'd want to accept a rc candidate as a permanent fix..  can maybe wait until the version is GA'd.

Testing the image with rc1 seems to work well:
{quote}oc get pods

NAME                                        READY   STATUS    RESTARTS   AGE

basic-example-56876dc586-9vlv7              1/1     Running   0          88s

basic-example-taskmanager-1-1               1/1     Running   0          48s

flink-kubernetes-operator-f7c8bc9b6-gct4d   2/2     Running   0          2m27s
{quote};;;","09/Nov/22 22:09;jbusche;OK, good news - 2.14.0 has GA'd and looks good.  Image scans clean (no fixable vulnerabilities) and the basic example works:
{quote}kubectl get pods

NAME                                         READY   STATUS    RESTARTS   AGE

basic-example-56876dc586-df769               1/1     Running   0          13m

basic-example-taskmanager-1-1                1/1     Running   0          13m

flink-kubernetes-operator-7d5c7b77f7-5wgb8   2/2     Running   0          14m
{quote}
I'll put a PR through, see how it looks...;;;","10/Nov/22 10:34;gyfora;merged to main 82fca328488be8ddf7d67ec6ce8ad1b20a6da3a6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adaptive Scheduler duplicates operators for each parallel instance in the Web UI,FLINK-29852,13494357,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huwh,JasonLee,JasonLee,02/Nov/22 15:58,05/May/23 15:37,13/Jul/23 08:13,02/Mar/23 17:18,1.16.0,1.16.1,,,,,,1.16.2,1.17.0,,,,Runtime / Coordination,Runtime / Web Frontend,,,,,,0,pull-request-available,,,,"All the operators in the DAG are shown repeatedly

!image-2022-11-02-23-57-39-387.png!",Flink 1.16.0,dmvk,gaoyunhaii,huwh,JasonLee,Leone,qingyue,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/22 15:57;JasonLee;image-2022-11-02-23-57-39-387.png;https://issues.apache.org/jira/secure/attachment/13051718/image-2022-11-02-23-57-39-387.png","09/Nov/22 08:09;huwh;image-2022-11-09-16-09-44-233.png;https://issues.apache.org/jira/secure/attachment/13051987/image-2022-11-09-16-09-44-233.png","09/Nov/22 09:32;JasonLee;image-2022-11-09-17-32-27-377.png;https://issues.apache.org/jira/secure/attachment/13051988/image-2022-11-09-17-32-27-377.png",,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 05 15:37:29 UTC 2023,,,,,,,,,,"0|z1aquo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/22 07:16;gaoyunhaii;Hi [~JasonLee] thanks for reporting the issue! Could this phenomenon be reproduced? And if possible could you provide the content of the response from the web server via http://<ip>:<port>/jobs/<job_id> ?;;;","03/Nov/22 08:18;JasonLee;Hi [~gaoyunhaii]  yes,Stably repeated, I tested two different SQL jobs and one datastream api job,There will be repetition.
{code:java}
// the content of the response from the web server

{
    ""jid"":""a294907b86943c3892070f07b98ec6e3"",
    ""name"":""FlinkStreamingBroadcastMysqlDemo"",
    ""isStoppable"":false,
    ""state"":""RUNNING"",
    ""start-time"":1667462380130,
    ""end-time"":-1,
    ""duration"":858837,
    ""maxParallelism"":-1,
    ""now"":1667463238967,
    ""timestamps"":Object{...},
    ""vertices"":Array[3],
    ""status-counts"":Object{...},
    ""plan"":{
        ""jid"":""a294907b86943c3892070f07b98ec6e3"",
        ""name"":""FlinkStreamingBroadcastMysqlDemo"",
        ""type"":""STREAMING"",
        ""nodes"":[
            {
                ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""Source: Kafka Source<br/>+- Map<br/>"",
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""Source: Kafka Source<br/>+- Map<br/>"",
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""Source: Kafka Source<br/>+- Map<br/>"",
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""Source: Kafka Source<br/>+- Map<br/>"",
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""Source: Kafka Source<br/>+- Map<br/>"",
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""Source: Kafka Source<br/>+- Map<br/>"",
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""9dd63673dd41ea021b896d5203f3ba7c"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""test<br/>"",
                ""inputs"":[
                    {
                        ""num"":0,
                        ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                        ""ship_strategy"":""HASH"",
                        ""exchange"":""pipelined_bounded""
                    }
                ],
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""9dd63673dd41ea021b896d5203f3ba7c"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""test<br/>"",
                ""inputs"":[
                    {
                        ""num"":0,
                        ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                        ""ship_strategy"":""HASH"",
                        ""exchange"":""pipelined_bounded""
                    }
                ],
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""9dd63673dd41ea021b896d5203f3ba7c"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""test<br/>"",
                ""inputs"":[
                    {
                        ""num"":0,
                        ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                        ""ship_strategy"":""HASH"",
                        ""exchange"":""pipelined_bounded""
                    }
                ],
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""9dd63673dd41ea021b896d5203f3ba7c"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""test<br/>"",
                ""inputs"":[
                    {
                        ""num"":0,
                        ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                        ""ship_strategy"":""HASH"",
                        ""exchange"":""pipelined_bounded""
                    }
                ],
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""9dd63673dd41ea021b896d5203f3ba7c"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""test<br/>"",
                ""inputs"":[
                    {
                        ""num"":0,
                        ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                        ""ship_strategy"":""HASH"",
                        ""exchange"":""pipelined_bounded""
                    }
                ],
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""9dd63673dd41ea021b896d5203f3ba7c"",
                ""parallelism"":6,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""test<br/>"",
                ""inputs"":[
                    {
                        ""num"":0,
                        ""id"":""cbc357ccb763df2852fee8c4fc7d55f2"",
                        ""ship_strategy"":""HASH"",
                        ""exchange"":""pipelined_bounded""
                    }
                ],
                ""optimizer_properties"":{

                }
            },
            {
                ""id"":""1a936cb48657826a536f331e9fb33b5e"",
                ""parallelism"":1,
                ""operator"":"""",
                ""operator_strategy"":"""",
                ""description"":""Sink: Print to Std. Out<br/>"",
                ""inputs"":[
                    {
                        ""num"":0,
                        ""id"":""9dd63673dd41ea021b896d5203f3ba7c"",
                        ""ship_strategy"":""REBALANCE"",
                        ""exchange"":""pipelined_bounded""
                    }
                ],
                ""optimizer_properties"":{

                }
            }
        ]
    }
}{code}
 

 ;;;","08/Nov/22 13:14;huwh;Hi [~JasonLee] , I can't reproduce this problem with flink-1.16 WordCount, can you show how to reproduce this?;;;","08/Nov/22 13:34;JasonLee;Hi [~huwh] , I just used the command on the website like the following:
{code:java}
./flink run-application -t yarn-application ../examples/streaming/TopSpeedWindowing.jar
./flink run -t yarn-per-job --detached ../examples/streaming/TopSpeedWindowing.jar {code}
They all repeat as in the picture above.;;;","09/Nov/22 08:11;huwh;I can't reproduce this with this jar.{*}{*}

 

*My Flink version is* 

*Version:* 1.16.0 *Commit:* af6eff8 @ 2022-10-20T04:21:45+02:00 

 ;;;","09/Nov/22 10:10;JasonLee;Hi [~huwh] That's a little weird,I submit the task with the following command:
{code:java}
./flink run-application -t yarn-application ../examples/streaming/TopSpeedWindowing.jar {code}
The DAG is shown below,The window and sink operators have 4 parallelism, shown four times below

!image-2022-11-09-17-32-27-377.png!;;;","13/Dec/22 14:46;JasonLee;hi all, I re-downloaded and deployed the Flink cluster, the flink job is normal, and there is no duplication of operator. I do not know what the problem is, in a word, it is OK now, so I will close this issue, thanks.;;;","02/Mar/23 06:03;huwh;Hi, [~JasonLee] I found this is a bug while adaptiveScheduler update the ExecutionGraph#JsonPlan. It put all the execution vertices into the jsonPlan. Could you reopen this tickets, I would like to fix it. cc [~dwysakowicz] ;;;","02/Mar/23 06:11;JasonLee;[~huwh] yeah,Thank you for your reply,I've already open it.;;;","02/Mar/23 13:02;dmvk;master: 758ce72158922e00ed7078e00706f8502145e461

release-1.16: 57cabdccf88f03523edfab413c252c8f16e9890a

release-1.17: 30344f1c79f3760c88de73a0169f35dbe8951c3d;;;","05/May/23 14:58;Leone;We are facing this issue in 1.16.1 and need to make a go/no-go decision and wondering whether this problem is just affecting the endpoint /jobs/\{jobId} (and so the UI) or does it have deeper implications?;;;","05/May/23 15:37;dmvk;This is just a problem with the REST API representation, there are no broader consequences;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Event time temporal join on an upsert source may produce incorrect execution plan,FLINK-29849,13494261,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,02/Nov/22 08:27,21/Feb/23 06:21,13/Jul/23 08:13,21/Dec/22 07:23,1.15.3,1.16.0,,,,,,1.16.1,1.17.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"For current implementation, the execution plan is incorrect when do event time temporal join on an upsert source. There's two problems:
1.  for an upsert source, we should not add a ChangelogNormalize node under a temporal join input, or it will damage the versions of the version table. For versioned tables, we use a single-temporal mechanism which relies sequencial records of a same key to ensure the valid period of each version, so if the ChangelogNormalize was added then an UB message will be produced based on the previous  UA or Insert message, and all the columns are totally same include event time, e.g., 
original upsert input
{code}
+I (key1, '2022-11-02 10:00:00', a1)
+U (key1, '2022-11-02 10:01:03', a2)
{code}

the versioned data should be:
{code}
v1  [~, '2022-11-02 10:00:00')
v2  ['2022-11-02 10:00:00', '2022-11-02 10:01:03')
{code}

after ChangelogNormalize's processing, will output:
{code}
+I (key1, '2022-11-02 10:00:00', a1)
-U (key1, '2022-11-02 10:00:00', a1)
+U (key1, '2022-11-02 10:01:03', a2)
{code}

versions are incorrect:
{code}
v1  ['2022-11-02 10:00:00', '2022-11-02 10:00:00')  // invalid period
v2  ['2022-11-02 10:00:00', '2022-11-02 10:01:03')
{code}

2. semantically, a filter cannot be pushed into an event time temporal join, otherwise, the filter may also corrupt the versioned table
",,FredTing,godfrey,leonard,lincoln.86xy,qinjunjerry,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 21 06:17:19 UTC 2023,,,,,,,,,,"0|z1aq9c:",9223372036854775807,"This resolves the correctness issue when do event time temporal join with a versioned table backed by an upsert source. When the right input of the join is an upsert source, it no longer generates a ChangelogNormalize node for it.
Note this is an incompatible plan change compare to 1.16.0",,,,,,,,,,,,,,,,,,,"21/Dec/22 07:23;godfrey;Fixed in master: eb44ac01c9969cb22ab832b6b2155b109f015b06

1.16.1: dbb6654c9d211e0944a0a1f58921c12bda6916cf;;;","21/Feb/23 06:17;lincoln.86xy;Just for record: the conclusions of reviewing the semantic impact of filter-related optimizations/rewrites on streaming scenarios(filter pushdown disturb time attributes). The following releated operations were checked:

1. temporal join, including eventtime and proctime(proctime temporal join is unsupported yet, one similar solution is lookup join), the essence is that filter should not go through snapshot to pre-filter data which may destroy the data version (corresponding to the original data may be changed over time), otherwise it affects the result. FLINK-28988 prevent the filter push down through an eventime temporal join. 
2. window related, does not affect the result, but only affects the density of watermark generation and the timing of the output result

3. interval join, does not affect the result(the on condition in left join can be pushdown, the post where condition can change join to inner join)
4. deduplication, does not affect the result, the current filter push down already consider the 'containsOver' protection judgment;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Euclidean Distance Measure generates NAN distance values,FLINK-29843,13494217,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,02/Nov/22 04:17,10/Jan/23 05:17,13/Jul/23 08:13,02/Nov/22 07:07,ml-2.1.0,,,,,,,ml-2.2.0,,,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,Currently Flink ML's `EuclideanDistanceMeasure.distance(...)` method might return a negative value as the distance between two vectors given the calculation accuracy of java doubles. This bug should be fixed to guarantee that the distance is a non-negative value.,,yunfengzhou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-02 04:17:21.0,,,,,,,,,,"0|z1apzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
docs/content/docs/deployment/config.md has error,FLINK-29841,13494212,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Wencong Liu,hcj,hcj,02/Nov/22 03:21,11/May/23 08:07,13/Jul/23 08:13,11/May/23 08:07,1.18.0,,,,,,,1.18.0,,,,,Documentation,,,,,,,0,pull-request-available,,,,"line 64 in docs/content/docs/deployment/config.md has error.

line 64 is :

These value are configured as memory sizes, for example *1536m* or *2g*.",,hcj,leonard,Weijie Guo,Wencong Liu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 11 08:07:15 UTC 2023,,,,,,,,,,"0|z1apyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/22 03:33;leonard;[~hcj] I checked our docs website, the display is ok. What errors do you mean?;;;","02/Nov/22 03:46;hcj;[~leonard] ""these values are"" is  more appropriate?;;;","18/Apr/23 08:46;Wencong Liu;I'd like to fix this. WDYT? [~Weijie Guo] ;;;","11/May/23 08:07;Weijie Guo;master(1.18) via 69aca3e064bd30efa45802fbc9f14c2885e4fd1e.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Old record may overwrite new record in Table Store when snapshot committing is slow,FLINK-29840,13494210,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,02/Nov/22 03:12,09/Nov/22 08:08,13/Jul/23 08:13,09/Nov/22 08:08,table-store-0.2.2,table-store-0.3.0,,,,,,table-store-0.2.2,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,"Consider the following scenario when snapshot committing is slow:
* A writer produces some records at checkpoint T.
* It produces no record at checkpoint T+1 and is closed.
* It produces some records at checkpoint T+2. It will be reopened and read the latest sequence number from disk. However snapshot at checkpoint T may not be committed so the sequence number it reads might be too small.

In this scenario, records from checkpoint T may overwrite records from checkpoint T+2 because they have larger sequence numbers.",,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29842,FLINK-29943,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 09 08:08:29 UTC 2022,,,,,,,,,,"0|z1apy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/22 08:08;TsReaper;master: f54008c98f51e419fa4a80d6fe0e62ab97358c54
release-0.2: fe0f333f5289081ca210540576e81c469ba18dd6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer2 endpoint doesn't support TGetInfoType value 'CLI_ODBC_KEYWORDS',FLINK-29839,13494207,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yzl,libra_816,libra_816,02/Nov/22 02:53,09/Dec/22 03:35,13/Jul/23 08:13,10/Nov/22 03:46,1.16.0,,,,,,,1.16.1,1.17.0,,,,Connectors / Hive,Table SQL / Gateway,,,,,,0,,,,," I had starting the SQL Gateway with the HiveServer2 Endpoint, and then I submit SQL with Hive Beeline, but I get the following exception:
{code:java}
java.lang.UnsupportedOperationException: Unrecognized TGetInfoType value: CLI_ODBC_KEYWORDS.
at org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.GetInfo(HiveServer2Endpoint.java:371) [flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetInfo.getResult(TCLIService.java:1537) [flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetInfo.getResult(TCLIService.java:1522) [flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
at java.lang.Thread.run(Thread.java:834) [?:?]
2022-11-01 13:55:33,885 ERROR org.apache.thrift.server.TThreadPoolServer                   [] - Thrift error occurred during processing of message.
org.apache.thrift.protocol.TProtocolException: Required field 'infoValue' is unset! Struct:TGetInfoResp(status:TStatus(statusCode:ERROR_STATUS, infoMessages:[*java.lang.UnsupportedOperationException:Unrecognized TGetInfoType value: CLI_ODBC_KEYWORDS.:9:8, org.apache.flink.table.endpoint.hive.HiveServer2Endpoint:GetInfo:HiveServer2Endpoint.java:371, org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetInfo:getResult:TCLIService.java:1537, org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetInfo:getResult:TCLIService.java:1522, org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39, org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39, org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:286, java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1128, java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:628, java.lang.Thread:run:Thread.java:834], errorMessage:Unrecognized TGetInfoType value: CLI_ODBC_KEYWORDS.), infoValue:null)
at org.apache.hive.service.rpc.thrift.TGetInfoResp.validate(TGetInfoResp.java:379) ~[flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.hive.service.rpc.thrift.TCLIService$GetInfo_result.validate(TCLIService.java:5228) ~[flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.hive.service.rpc.thrift.TCLIService$GetInfo_result$GetInfo_resultStandardScheme.write(TCLIService.java:5285) ~[flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.hive.service.rpc.thrift.TCLIService$GetInfo_result$GetInfo_resultStandardScheme.write(TCLIService.java:5254) ~[flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.hive.service.rpc.thrift.TCLIService$GetInfo_result.write(TCLIService.java:5205) ~[flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53) ~[flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
at java.lang.Thread.run(Thread.java:834) [?:?]
2022-11-01 13:55:33,886 WARN  org.apache.thrift.transport.TIOStreamTransport               [] - Error closing output stream.
java.net.SocketException: Socket closed
at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113) ~[?:?]
at java.net.SocketOutputStream.write(SocketOutputStream.java:150) ~[?:?]
at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81) ~[?:?]
at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142) ~[?:?]
at java.io.FilterOutputStream.close(FilterOutputStream.java:182) ~[?:?]
at org.apache.thrift.transport.TIOStreamTransport.close(TIOStreamTransport.java:110) [flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.thrift.transport.TSocket.close(TSocket.java:235) [flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:303) [flink-sql-connector-hive-3.1.2_2.12-1.16.0.jar:1.16.0]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
at java.lang.Thread.run(Thread.java:834) [?:?]
{code}

 I found the following code is where the exception is thrown, but I don’t know how to solve it.

{code:java}
public TGetInfoResp GetInfo(TGetInfoReq tGetInfoReq) throws TException {
    TGetInfoResp resp = new TGetInfoResp();
try {
        GatewayInfo info = service.getGatewayInfo();
TGetInfoValue tInfoValue;
switch (tGetInfoReq.getInfoType()) {
            case CLI_SERVER_NAME:
            case CLI_DBMS_NAME:
                tInfoValue = TGetInfoValue.stringValue(info.getProductName());
             break;
            case CLI_DBMS_VER:
                tInfoValue = TGetInfoValue.stringValue(info.getVersion().toString());
             break;
            default:
                throw new UnsupportedOperationException(
                        String.format(
                                ""Unrecognized TGetInfoType value: %s."",
tGetInfoReq.getInfoType()));
}
        resp.setStatus(OK_STATUS);
resp.setInfoValue(tInfoValue);
} catch (Throwable t) {
        LOG.error(""Failed to GetInfo."", t);
resp.setStatus(toTStatus(t));
}
    return resp;
}
{code}

CLI_ODBC_KEYWORDS----- It's a new definition in the TGetInfoType enumeration class in the Hive dependency package. It seems to be in a high version, but my Hive environment and Flink reference connector should be 3.1.2, which is available to the enumeration Definition, but from the source code of Flink-1.16.0, only three enumerations of TGetInfoType are supported: CLI_SERVER_NAME, CLI_DBMS_NAME, CLI_DBMS_VER.
Is it a problem that Flink-1.16.0 does not support, or is there something wrong with my environment or configuration? How can i fix it?


","Flink version: 1.16.0
Hive version: 3.1.2",fsk119,leonard,libra_816,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29857,,,,,,,,,FLINK-30347,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 02 09:15:08 UTC 2022,,,,,,,,,,"0|z1apxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/22 03:29;leonard;CC [~fsk119] Could you take a look this issue？;;;","02/Nov/22 08:26;yzl;Hi, I have replied in mailing list. Please check out.;;;","02/Nov/22 09:15;libra_816;[~yzl] Thank you. I got it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve switch to default database in docs,FLINK-29832,13493969,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zjureel,zjureel,01/Nov/22 08:41,19/Mar/23 05:43,13/Jul/23 08:13,19/Mar/23 05:43,table-store-0.2.2,table-store-0.3.0,,,,,,,,,,,Table Store,,,,,,,0,pull-request-available,,,,"`FlinkCatalogFactory` creates a default database named `default` in table store. The `default` is a keyword in SQL, and when we create a new database, we cant execute `use default` to switch to `default` directly. We can switch to default database ""use `default`;"" in flink table store",,TsReaper,zjureel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/22 08:40;zjureel;image-2022-11-01-16-40-47-539.png;https://issues.apache.org/jira/secure/attachment/13051671/image-2022-11-01-16-40-47-539.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 02:43:22 UTC 2022,,,,,,,,,,"0|z1aogw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 01:35;TsReaper;Hi [~zjureel].

Before diving into your implementation, I'd like to discuss the necessity of this ticket. There are many other catalog (for example, {{HiveCatalog}}) whose default database name is just ""default"". To switch to this database, we should run {{USE `default`}}, using backtick(`) to escape the keyword.

Why do you think this change is necessary? I'd like to hear your options.;;;","04/Nov/22 02:43;zjureel;Thanks [~TsReaper] When i tried to fixed this issue, i found the names of default database in hive and spark are also `default`. So i think the `default` is better than `default_database`. I'm new to sql, and when using hive, i can switch to default database `use default;` directly, and it cause in flink table store. I will change this issue to improve the document in flink table store. Can you help to review it? THX again :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple Hive tests are failing,FLINK-29831,13493967,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,luoyuxia,martijnvisser,martijnvisser,01/Nov/22 08:30,04/Nov/22 02:15,13/Jul/23 08:13,04/Nov/22 02:15,1.16.0,1.17.0,,,,,,1.17.0,,,,,Connectors / Hive,,,,,,,0,pull-request-available,test-stability,,,"{code:java}
Nov 01 01:56:03 [ERROR] org.apache.flink.table.module.hive.HiveModuleTest.testNumberOfBuiltinFunctions  Time elapsed: 0.042 s  <<< FAILURE!

Nov 01 01:57:17 [ERROR] org.apache.flink.connectors.hive.HiveTableSourceStatisticsReportTest.testFlinkOrcFormatHiveTableSourceStatisticsReport  Time elapsed: 12.846 s  <<< FAILURE!

Nov 01 01:57:17 [ERROR] org.apache.flink.connectors.hive.HiveTableSourceStatisticsReportTest.testMapRedOrcFormatHiveTableSourceStatisticsReport  Time elapsed: 10.355 s  <<< FAILURE!

Nov 01 01:57:17 [ERROR]   HiveTableSourceStatisticsReportTest.testFlinkOrcFormatHiveTableSourceStatisticsReport:124->assertHiveTableOrcFormatTableStatsEquals:368 

Nov 01 01:57:17 [ERROR]   HiveTableSourceStatisticsReportTest.testMapRedOrcFormatHiveTableSourceStatisticsReport:164->assertHiveTableOrcFormatTableStatsEquals:368 

Nov 01 01:57:17 [ERROR]   HiveModuleTest.testNumberOfBuiltinFunctions:53->verifyNumBuiltInFunctions:75 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42680&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=24818",,hxbks2ks,luoyuxia,martijnvisser,renqs,samrat007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 02:14:50 UTC 2022,,,,,,,,,,"0|z1aogg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 09:20;martijnvisser;1.16:
{code:java}
Nov 01 02:28:13 [ERROR] Errors: 
Nov 01 02:28:13 [ERROR]   HiveCatalogDataTypeTest.testComplexDataTypes:186->verifyDataTypes:212 » Catalog
Nov 01 02:28:13 [ERROR]   HiveCatalogDataTypeTest.testDataTypes:118->verifyDataTypes:212 » Catalog Faile...
Nov 01 02:28:13 [ERROR]   HiveCatalogDataTypeTest.testNonSupportedBinaryDataTypes:127 » Catalog Failed t...
Nov 01 02:28:13 [ERROR]   HiveCatalogDataTypeTest.testNonSupportedVarBinaryDataTypes:139 » Catalog Faile...
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42682&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=26311

[~luoyuxia] Any ideas on this?;;;","02/Nov/22 04:13;luoyuxia;The HiveTableSourceStatisticsReportTest / testNumberOfBuiltinFunctions failure  is caused by upgrading Hive to 3.1.3 in [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42680&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=24818].

To fix it, we need to do adjustment in test code.

But for the  HiveCatalogDataTypeTest failure in [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42682&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=26311], it really confused me.I couldn't reproduce it in my local env and our test ci with a [pr|[https://github.com/apache/flink/pull/21213]] to trigger it . But I will still try to debug to see why it happens.;;;","02/Nov/22 08:11;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42724&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=25263;;;","04/Nov/22 02:14;renqs;master: 8e66be89dfcb54b7256d51e9d89222ae6701061f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PulsarSinkITCase$DeliveryGuaranteeTest.writeRecordsToPulsar failed,FLINK-29830,13493964,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,syhily,martijnvisser,martijnvisser,01/Nov/22 08:25,19/Jan/23 10:27,13/Jul/23 08:13,02/Dec/22 14:23,1.15.3,1.16.0,1.17.0,,,,,1.15.4,1.16.1,1.17.0,pulsar-3.0.1,,Connectors / Pulsar,,,,,,,0,pull-request-available,test-stability,,,"{code:java}
Nov 01 01:28:03 [ERROR] Failures: 
Nov 01 01:28:03 [ERROR]   PulsarSinkITCase$DeliveryGuaranteeTest.writeRecordsToPulsar:140 
Nov 01 01:28:03 Actual and expected should have same size but actual size is:
Nov 01 01:28:03   0
Nov 01 01:28:03 while expected size is:
Nov 01 01:28:03   115
Nov 01 01:28:03 Actual was:
Nov 01 01:28:03   []
Nov 01 01:28:03 Expected was:
Nov 01 01:28:03   [""AT_LEAST_ONCE-isxrFGAL-0-kO65unDUKX"",
Nov 01 01:28:03     ""AT_LEAST_ONCE-isxrFGAL-1-4tBNu1UmeR"",
Nov 01 01:28:03     ""AT_LEAST_ONCE-isxrFGAL-2-9PTnEahlNU"",
Nov 01 01:28:03     ""AT_LEAST_ONCE-isxrFGAL-3-GjWqEp21yz"",
Nov 01 01:28:03     ""AT_LEAST_ONCE-isxrFGAL-4-jnbJr9C0w8"",
Nov 01 01:28:03     ""AT_LEAST_ONCE-isxrFGAL-5-e8Wacz5yDO"",
Nov 01 01:28:03     ""AT_LEAST_ONCE-isxrFGAL-6-9cW53j3Zcf"",
Nov 01 01:28:03     ""AT_LEAST_ONCE-isxrFGAL-7-jk8z3m2Aa5"",
Nov 01 01:28:03     ""AT_LEAST_ONCE-isxrFGAL-8-VU56KmMeiz"",
Nov 01 01:28:03     ""AT_LEAST_ONCE-isxrFGAL-9-uvMdFxxDAj"",
Nov 01 01:28:03     ""AT_LEAST_ONCE-isxrFGAL-10-FQyWfwJFbH"",
...
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42680&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37544",,fpaul,leonard,mapohl,martijnvisser,syhily,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30109,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 02 14:23:10 UTC 2022,,,,,,,,,,"0|z1aofs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 08:57;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42681&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=27843;;;","02/Nov/22 13:09;fpaul;[~martijnvisser] do you think this is a block for the 1.15.3 release? The pulsar tests seem to be flaky for a while.;;;","02/Nov/22 13:45;martijnvisser;This is not a blocker for 1.15.3;;;","07/Nov/22 08:01;mapohl;Same build:
* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42857&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=28624]
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42857&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=28463;;;","07/Nov/22 13:30;syhily;The log couldn't show the real cause. We shall add some log to print the hidden error stack.

{{Timeout for waiting the records from Pulsar. We have consumed 0 messages, expect 137 messages.}}

Aha, I find the cause, it's my mistake in consuming messages.;;;","07/Nov/22 13:56;mapohl;There is some warning in the logs about some timeout on the Pulsar side:
{code}
160037 01:57:02,826 [Legacy Source Thread - Source: Custom Source (1/1)#0] WARN  org.apache.flink.connector.pulsar.testutils.function.ControlSource$StopSignal [] - Timeout for waiting the records from Pulsar. We have consumed 0 messages, expect 137 messages.
 160038 01:57:02,987 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 600 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1667699822986 for job 6fa5e9f506b63276a4933daa137fdf43.
 160039 01:57:02,994 [jobmanager-io-thread-13] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 600 for job 6fa5e9f506b63276a4933daa137fdf43 (314 bytes, checkpointDuration=8 ms, finalizationTime=0 ms).
 160040 01:57:02,996 [Source: Custom Source (1/1)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Custom Source (1/1)#0 (3cfeb328eb182887d817886aa848606d) switched from RUNNING to FINISHED.
{code}

Subsequent checkpoints are declined as a consequence (AFAIU).;;;","07/Nov/22 14:03;syhily;[~mapohl] It's my mistake in {{ControlSource.StopSignal}}. I consume messages with the latest stop cursor. Which would cause the race condition that all message has been sent before I start the consumer.

Can you add 1.16.0 in affected version?;;;","07/Nov/22 14:05;mapohl;Done. Are you going to provide a fix for that? I would assign this Jira issue to you in that case.;;;","07/Nov/22 14:10;syhily;Yep, I'll submit a fix right now. Plz assign this ticket to me and thanks for helping me find this undetectable bug.;;;","07/Nov/22 15:00;syhily;[~mapohl] Can you help review the PR? https://github.com/apache/flink/pull/21252;;;","14/Nov/22 03:38;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43089&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc;;;","14/Nov/22 03:43;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43089&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc;;;","21/Nov/22 09:32;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43253&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199;;;","24/Nov/22 03:35;mapohl;[~syhily] may you create a backport PR for 1.16 and 1.15 as well, please?;;;","30/Nov/22 06:54;mapohl;I moved this under the Umbrella Pulsar stability ticket.;;;","01/Dec/22 13:46;martijnvisser;Fixed in Pulsar external connector repo: 5162b766bb042705ecafefc773bb2600b7e56263;;;","02/Dec/22 14:23;mapohl;master: f8b3b33ce1c2b36aa8e0011131a1ba74f540035f
1.16: 0ff47bedba48714c79b82cfb0379768d79e2d6ef
1.15: 3eb002eb30ebffba6306081f79ead3a487b2cb5c
flink-connector-pulsar:v3.0: 5162b766bb042705ecafefc773bb2600b7e56263;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Connector][AsyncSinkWriter] Checkpointed states block writer from sending records,FLINK-29827,13493947,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chalixar,mc8max,mc8max,01/Nov/22 07:45,10/Nov/22 08:37,13/Jul/23 08:13,10/Nov/22 08:37,1.15.2,1.16.0,,,,,,1.15.3,1.16.1,1.17.0,,,Connectors / Common,,,,,,,0,pull-request-available,,,,"Hi every one,

Recently we discovered an issue which blocks Sink operators from sending records to client's endpoint.

To *reproduce* the issue, we started our Flink app from an existing savepoint, in which some Sink operators hold some buffered records. For instance, app employs KinesisStreamSink with a parallelism of 4. 2 of them has no buffered records, the other 2 start with existing states of some records, which are leftover from the previous run. 

{*}Behavior{*}: during runtime, we sent records (let's say 200) to this sink in rebalance mode. But only 100 of them (50%) were dispatched from the sink operators.

After {*}investigation{*}, we found that the implementation AsyncSinkWriter invokes submitRequestEntries() to send the records to their destination. This invocation is performed when a callback is performed, a flush(true) or forced-flush is called, or when the buffered is full (either in size or in quantity).

The case falls in the first scenario: the _callback is not registered_ {_}when the writer starts with some existing buffered records{_}, initialized from savepoint. Hence in our case, those operators were holding records till their buffers become full, while other operators still perform the usual sending.

Impacted {*}scope{*}: flink-1.15.2 or later version, for any Sink that implements AsyncSinkWriter.

We currently treat this as an abnormal behavior of Flink, but please let me know if this behavior is intended by design.

Thanks in advance.",,dannycranmer,mc8max,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 07 08:49:43 UTC 2022,,,,,,,,,,"0|z1aoc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 11:21;dannycranmer;[~mc8max] Thanks for reporting this issue. I agree it sounds like a bug, [~chalixar] is going to take a look.;;;","07/Nov/22 08:49;dannycranmer;Merged commit [{{d76053a}}|https://github.com/apache/flink/commit/d76053a0e670f45b478841f92ef27f9b641643f5] into master 
Merged commit [{{f5f8060}}|https://github.com/apache/flink/commit/f5f806044db373971b3cd611180d1e0227ceaedd] into release-1.16
Merged commit [{{7b50b3a}}|https://github.com/apache/flink/commit/7b50b3a8d13be8931de07897e326cb96022ebdbe] into release-1.15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AgglomerativeClustering fails when the distanceThreshold is very large,FLINK-29824,13493898,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,01/Nov/22 04:17,10/Jan/23 05:19,13/Jul/23 08:13,02/Nov/22 07:08,,,,,,,,ml-2.2.0,,,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,"The current implementation did not consider following case:

When distanceThreshold not null and set as a large value, all data points are supposed to be assigned into a single cluster.

 

In this case, we should stop training when the number of the active clusters is one.",,zhangzp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-11-01 04:17:20.0,,,,,,,,,,"0|z1ao14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix wrong description in comments of StreamExecutionEnvironment#setMaxParallelism(),FLINK-29822,13493869,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,01/Nov/22 01:50,03/Apr/23 14:25,13/Jul/23 08:13,03/Apr/23 14:25,1.18.0,,,,,,,1.18.0,,,,,API / DataStream,,,,,,,0,pull-request-available,,,,"The upper limit (inclusive) of max parallelism is Short.MAX_VALUE + 1, not Short.MAX_VALUE.",,Feifan Wang,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-5473,,,,,FLINK-4380,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 03 14:25:44 UTC 2023,,,,,,,,,,"0|z1anuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 14:25;pnowojski;merged commit ea9c9d9 into apache:master

Thanks [~Feifan Wang]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HsResultPartitionTest.testAvailability fails  ,FLINK-29818,13493785,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Weijie Guo,Sergey Nuyanzin,Sergey Nuyanzin,31/Oct/22 14:51,16/Nov/22 09:53,13/Jul/23 08:13,16/Nov/22 09:51,1.17.0,,,,,,,1.17.0,,,,,Runtime / Network,,,,,,,0,pull-request-available,test-stability,,,"{noformat}
13:13:31,079 [ForkJoinPool-27-worker-25] ERROR org.apache.flink.util.TestLoggerExtension                    [] - 
--------------------------------------------------------------------------------
Test org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartitionTest.testAvailability[testAvailability()] failed with:
org.opentest4j.AssertionFailedError: 
Expecting value to be false but was true
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartitionTest.testAvailability(HsResultPartitionTest.java:414)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
        at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
        at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
        at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)

        at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
        at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
        at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
        at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
        at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
        at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
        at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
        at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
        at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
        at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
        at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
        at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
        at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
        at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
        at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
        at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
        at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
        at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
        at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
        at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
        at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
        at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
        at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
        at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

{noformat}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42653&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=8390",,leonard,mapohl,martijnvisser,pnowojski,Sergey Nuyanzin,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/22 14:50;Sergey Nuyanzin;logs-ci-test_ci_finegrained_resource_management-1667221819.zip;https://issues.apache.org/jira/secure/attachment/13051643/logs-ci-test_ci_finegrained_resource_management-1667221819.zip",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 16 08:06:28 UTC 2022,,,,,,,,,,"0|z1ancw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 01:10;Weijie Guo;[~Sergey Nuyanzin] Thanks for reporting this, I will fix this unstable test today.;;;","02/Nov/22 08:09;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42724&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8199;;;","02/Nov/22 15:42;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42733&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8195;;;","02/Nov/22 15:46;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42749&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=8427;;;","02/Nov/22 16:17;Weijie Guo;I have found the reason and proposed pull request, it will merge as soon as possible.;;;","03/Nov/22 06:12;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42766&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","03/Nov/22 06:13;leonard;[~Weijie Guo] Could you find someone who are familiar with runtime module to review this PR？;;;","03/Nov/22 06:21;Weijie Guo;[~leonard] Sure, [~xtsong] will take a look later.;;;","04/Nov/22 10:34;pnowojski;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42795&view=results;;;","16/Nov/22 08:06;xtsong;- master (1.17): 39f0e9bfadf0317328ede1d8c7dd37894beaae38;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Published metadata for apache-flink in pypi are inconsistent and causes poetry to fail,FLINK-29817,13493780,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,ah.casimiro,ah.casimiro,31/Oct/22 14:26,18/Nov/22 07:58,13/Jul/23 08:13,18/Nov/22 07:58,1.16.0,,,,,,,1.16.1,1.17.0,,,,API / Python,,,,,,,0,pull-request-available,,,,"Hi, 

Following the debug steps described in [this github thread|https://github.com/python-poetry/poetry/issues/3011] I got to the conclusion that the metadata of the apache-flink 1.16.0 package is wrong. And because of that I cannot properly manage my dependencies using poetry.

I can successfully install it with pip (runs with no errors), as stated in the docs:
{code:java}
python -m pip install apache-flink {code}
But when I try to include the dependency in my poetry project I got the following error:

 
{code:java}
❯ poetry add apache-flink@1.16.0Updating dependencies
Resolving dependencies... (2.0s)Because pemja (0.2.6) depends on numpy (1.21.4)
 and apache-flink (1.16.0) depends on numpy (>=1.14.3,<1.20), pemja (0.2.6) is incompatible with apache-flink (1.16.0).
So, because cv-features depends on apache-flink (1.16.0) which depends on pemja (0.2.6), version solving failed. {code}
 

I've followed the same debug steps as in [this github thread|https://github.com/python-poetry/poetry/issues/3011] and can confirm that apache-flink has exactly the same problem as described in the thread: the wheel package in pypi has correct dependency metadata but the pypi published don't.

 
{code:java}
❯ pkginfo -f requires_dist /Users/andre/Downloads/apache_flink-1.16.0-cp39-cp39-macosx_11_0_arm64.whl
requires_dist: ['py4j (==0.10.9.3)', 'python-dateutil (==2.8.0)', 'apache-beam (==2.38.0)', 'cloudpickle (==2.1.0)', 'avro-python3 (!=1.9.2,<1.10.0,>=1.8.1)', 'pytz (>=2018.3)', 'fastavro (<1.4.8,>=1.1.0)', 'requests (>=2.26.0)', 'protobuf (<3.18)', 'httplib2 (<=0.20.4,>=0.19.0)', 'apache-flink-libraries (<1.16.1,>=1.16.0)', 'numpy (<1.22.0,>=1.21.4)', 'pandas (<1.4.0,>=1.3.0)', 'pyarrow (<9.0.0,>=5.0.0)', 'pemja (==0.2.6) ; python_full_version >= ""3.7"" and platform_system != ""Windows""'] {code}
but the pipy json metadata is wrong:

 

 
{code:java}
❯ curl -sL https://pypi.org/pypi/apache-flink/json | jq '.info.requires_dist'[
  ""py4j (==0.10.9.3)"",
  ""python-dateutil (==2.8.0)"",
  ""apache-beam (==2.38.0)"",
  ""cloudpickle (==2.1.0)"",
  ""avro-python3 (!=1.9.2,<1.10.0,>=1.8.1)"",
  ""pytz (>=2018.3)"",
  ""fastavro (<1.4.8,>=1.1.0)"",
  ""requests (>=2.26.0)"",
  ""protobuf (<3.18)"",
  ""httplib2 (<=0.20.4,>=0.19.0)"",
  ""apache-flink-libraries (<1.16.1,>=1.16.0)"",
  ""numpy (<1.20,>=1.14.3)"",
  ""pandas (<1.2.0,>=1.0)"",
  ""pyarrow (<7.0.0,>=0.15.1)"",
  ""pemja (==0.2.6) ; python_full_version >= \""3.7\"" and platform_system != \""Windows\""""
]{code}
 

As per [this comment|https://github.com/python-poetry/poetry/issues/3011#issuecomment-702826616], could you please republish the package correcting this metadata information, please? This [other comment|https://github.com/apple/turicreate/issues/3342#issuecomment-702957550] can help gain more context.

 

Thanks

 

 ","macos 12.6 (M1)

Poetry Version: 1.2.2
Python Version:  3.9.12",ah.casimiro,dianfu,hxbks2ks,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 18 07:58:05 UTC 2022,,,,,,,,,,"0|z1anbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/22 15:13;martijnvisser;CC [~hxbks2ks] [~dianfu];;;","01/Nov/22 02:07;hxbks2ks;The `install_requirements` of PyFlink depends on your `python_version`.   
{code:python}
if sys.version_info < (3, 7):
        # python 3.6 upper and lower limit
        install_requires.append('numpy>=1.14.3,<1.20')
        install_requires.append('pandas>=1.0,<1.2.0')
        install_requires.append('pyarrow>=0.15.1,<7.0.0')
    else:
        # python 3.7, 3.8 and 3.9 upper limit and M1 chip lower limit,
        install_requires.append('numpy>=1.21.4,<1.22.0')
        install_requires.append('pandas>=1.3.0,<1.4.0')
        install_requires.append('pyarrow>=5.0.0,<9.0.0')
{code}
 So I guess Pypi collects the `metadata` used in Python3.6.
 
;;;","02/Nov/22 02:01;hxbks2ks;I think we can optimize the `metadata` to solve this problem in the 1.16.1.;;;","18/Nov/22 07:58;hxbks2ks;Merged into master via e5762a558f3697294cd73da4247a741fc6f73456
Merged into release-1.16 via d5b10d8ec9ca9fa03201ce57421bb0e714e224a7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Userfunction exception in ProcessWindowFunction was called before invoke during restore state(subtask was in INITIALIZING state), but SteamTask skip handle Exception",FLINK-29816,13493754,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,RocMarshal,xieyi,xieyi,31/Oct/22 12:52,27/Feb/23 15:31,13/Jul/23 08:13,27/Feb/23 15:31,1.15.3,1.16.1,1.17.0,,,,,1.17.0,,,,,Runtime / Task,,,,,,,0,pull-request-available,,,,"h4. 1. How to repeat 

ProcessWindowFunction, and make some exception in process()
test code
{code:java}
public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        env.enableCheckpointing(60 * 1000);
        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
        env.getCheckpointConfig().setCheckpointTimeout(60000);

        KafkaSource<String> kafkaConsumer = KafkaSource.<String>builder()
                .setBootstrapServers(""****"")
                .setTopics(""****"")
                .setGroupId(""****"")
                .setValueOnlyDeserializer(new SimpleStringSchema())
                .setStartingOffsets(OffsetsInitializer.earliest())
                .build();

        DataStreamSource<String> kafkaSource = env.fromSource(kafkaConsumer, WatermarkStrategy.noWatermarks(), ""Kafka Source"");

        SingleOutputStreamOperator<String> mapSourse = kafkaSource.keyBy(s -> s).window(TumblingProcessingTimeWindows.of(Time.seconds(15)))
                .process(new ProcessWindowFunction<String, String, String, TimeWindow>() {
                    @Override
                    public void process(String s, ProcessWindowFunction<String, String, String, TimeWindow>.Context context, Iterable<String> iterable, Collector<String> collector) throws Exception {
                        //when process event:""abc"" .It causes java.lang.NumberFormatException
                        Integer intS = Integer.valueOf(s);
                        collector.collect(s);
                    }
                })
                .name(""name-process"").uid(""uid-process"");

        mapSourse.print();
        env.execute();
    }
{code}
kafka input event
{code:java}
>1
>1
>2
>2
>3
>3
>abc
>abc
>
{code}
h4. 2. fault phenomena

when job process the event:""abc"",It will cause java.lang.NumberFormatException and failover ,Then attempt and failover continuously.
However, it only failover 2 times(attempt 0, attempt 1) and when attempt for third time, It work normally, and no exception
!image-2022-10-31-19-54-12-546.png!

checkpoint 1  complete in attempt 1,before failover exception 1
{code:java}
2022-10-31 16:59:53,644 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Triggering checkpoint 1 (type=CHECKPOINT) @ 1667206793605 for job 7bca78a75b089d447bb4c99efcfd6527.2022-10-31 16:59:54,010 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Completed checkpoint 1 for job 7bca78a75b089d447bb4c99efcfd6527 (21630 bytes, checkpointDuration=333 ms, finalizationTime=72 ms).  {code}
 

attempt 2 was restore from checkpoint
{code:java}
2022-10-31 17:00:30,033 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Restoring job 7bca78a75b089d447bb4c99efcfd6527 from Checkpoint 1 @ 1667206793605 for 7bca78a75b089d447bb4c99efcfd6527 located at hdfs://eadhadoop/user/sloth/sloth-fs-checkpoints/meta/1_7/7bca78a75b089d447bb4c99efcfd6527/chk-1.
{code}
 

 
h4. 3. possible reasons

during attempt 2 , task restore from checkpoint, userfunction in ProcessWindowFunction was called in SteamTask.restore and produce ""java.lang.NumberFormatException"", However, SteamTask catch exception and didn't handle exception because subtask is not in RUNNING state.

*the stack trace in attempt 2*
user function was called in SteamTask.restore(subtask state is INITIALIZING)
{code:java}
java.lang.Thread.getStackTrace(Thread.java:1552)
com.youdao.analysis.KafkaCheckpointWindowProcessTest$1.process(KafkaCheckpointWindowProcessTest.java:45)
com.youdao.analysis.KafkaCheckpointWindowProcessTest$1.process(KafkaCheckpointWindowProcessTest.java:40)
org.apache.flink.streaming.runtime.operators.windowing.functions.InternalIterableProcessWindowFunction.process(InternalIterableProcessWindowFunction.java:57)
org.apache.flink.streaming.runtime.operators.windowing.functions.InternalIterableProcessWindowFunction.process(InternalIterableProcessWindowFunction.java:32)
org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.emitWindowContents(WindowOperator.java:568)
org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.onProcessingTime(WindowOperator.java:524)
org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.onProcessingTime(InternalTimerServiceImpl.java:284)
org.apache.flink.streaming.runtime.tasks.StreamTask.invokeProcessingTimeCallback(StreamTask.java:1693)
org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$null$22(StreamTask.java:1684)
org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:353)
org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:317)
org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:690)
org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654)
org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
java.lang.Thread.run(Thread.java:745)
{code}
stack trace(which cause failover) in attempt 0 and attempt 1
user function was called in SteamTask.invoke
{code:java}
com.youdao.analysis.KafkaCheckpointWindowProcessTest$1.process(KafkaCheckpointWindowProcessTest.java:45)
com.youdao.analysis.KafkaCheckpointWindowProcessTest$1.process(KafkaCheckpointWindowProcessTest.java:40)
org.apache.flink.streaming.runtime.operators.windowing.functions.InternalIterableProcessWindowFunction.process(InternalIterableProcessWindowFunction.java:57)
org.apache.flink.streaming.runtime.operators.windowing.functions.InternalIterableProcessWindowFunction.process(InternalIterableProcessWindowFunction.java:32)
org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.emitWindowContents(WindowOperator.java:568)
org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.onProcessingTime(WindowOperator.java:524)
org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.onProcessingTime(InternalTimerServiceImpl.java:284)
org.apache.flink.streaming.runtime.tasks.StreamTask.invokeProcessingTimeCallback(StreamTask.java:1693)
org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$null$22(StreamTask.java:1684)
org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:338)
org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324)
org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
java.lang.Thread.run(Thread.java:745)
{code}
in org.apache.flink.streaming.runtime.tasks.StreamTask handleAsyncException
SteamTask only handleAsyncException when is Running==true
[https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java#L1540]
{code:java}
    @Override
    public void handleAsyncException(String message, Throwable exception) {
        if (isRunning) {
            // only fail if the task is still running
            asyncExceptionHandler.handleAsyncException(message, exception);
        }
    }
{code}
but during restore,isRunning==false
[https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java#L673]

 

So during Steam.restore, SteamTask skip exception in userfunction of ProcessWindowFunction.

 

 
h4.  

 ",,fanrui,kevin.cyj,pgaref,RocMarshal,Weijie Guo,xieyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30511,,,,,,,,,,,,,,"31/Oct/22 11:49;xieyi;image-2022-10-31-19-49-52-432.png;https://issues.apache.org/jira/secure/attachment/13051642/image-2022-10-31-19-49-52-432.png","31/Oct/22 11:54;xieyi;image-2022-10-31-19-54-12-546.png;https://issues.apache.org/jira/secure/attachment/13051641/image-2022-10-31-19-54-12-546.png","02/Nov/22 02:42;xieyi;image-2022-11-02-10-42-21-099.png;https://issues.apache.org/jira/secure/attachment/13051687/image-2022-11-02-10-42-21-099.png","02/Nov/22 02:57;xieyi;image-2022-11-02-10-57-08-064.png;https://issues.apache.org/jira/secure/attachment/13051688/image-2022-11-02-10-57-08-064.png","02/Nov/22 03:06;xieyi;image-2022-11-02-11-06-37-925.png;https://issues.apache.org/jira/secure/attachment/13051692/image-2022-11-02-11-06-37-925.png","02/Nov/22 03:10;xieyi;image-2022-11-02-11-10-25-508.png;https://issues.apache.org/jira/secure/attachment/13051691/image-2022-11-02-11-10-25-508.png","22/Feb/23 09:26;RocMarshal;image-2023-02-22-17-26-06-200.png;https://issues.apache.org/jira/secure/attachment/13055722/image-2023-02-22-17-26-06-200.png",,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 14:49:51 UTC 2023,,,,,,,,,,"0|z1an60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 10:18;Weijie Guo;[~xieyi] Thanks for reporting this, which version of flink do you use when you encounter this problems? And did you use unalignCheckpoint?;;;","02/Nov/22 02:23;xieyi;[~Weijie Guo]  I didn't use unalignCheckpoint, I had test flink 1.14.0, 1.15.2, 1.16.0,  All of the version can repeat the problem.;;;","02/Nov/22 02:48;xieyi;I also found some phenomenon
h4. 1. userfunction in   ProcessWindowFunction was called during SteamTask.restore, because MailboxProcessore is doing Mail(""Timer callback for org.apache.flink.streaming.api.operators.InternalTimerServiceImpl"")

 

here is the Timer callback for '1667358089999'

!image-2022-11-02-11-06-37-925.png!

 
h4.  2. The  Mail (""Timer callback for org.apache.flink.streaming.api.operators.InternalTimerServiceImpl"") was put in Mailbiox during restoreGates and initializeStateAndOpenOperators

 

Timer callback for '1667358089999' was put in Mailbox, because during restoreGates and initializeStateAndOpenOperators, register timer '1667358089999'(it seems restored from state)

!image-2022-11-02-11-10-25-508.png!

 

 
 
 
 

 ;;;","02/Nov/22 03:14;Weijie Guo;[~xieyi] Yes,This phenomenon is normal. When restoring, the mailbox is working, which means that the recovered channel can process data normally . The key to the problem is that only after all channels are recovered, the StreamTask will become the running state, but all exceptions will be caught when the processFunction callback executes, and will only be thrown when the running state is reached.;;;","02/Nov/22 03:57;Weijie Guo;# Because the restored channel can work normally in this stage, and the registerTimer is part of the working logic of the windowOperator. If I remember correctly, the defaultAction of mailbox (the logic that the operator actually processes data) will not be run in the state recovery phase before the introduction of unaligned checkpoint. But the current implementation is that no matter whether unaligned checkpoint is enabled or not, it will undergo a transformation from recoveredInputChannel ->normal InputChannel, which brings some confusions.
 # This is exactly the workflow of window operator. We will register timers to trigger window calculation(including user function) when they are out of date. And we expect execute these callback in mailbox thread.
 # I still need to think about the solution to the problem that the error was swallowed in the recovery phase.;;;","17/Nov/22 09:43;kevin.cyj;Any update on this issue?;;;","20/Feb/23 10:57;RocMarshal;hi, [~Weijie Guo] Any process on this issue ? ;;;","20/Feb/23 11:14;Weijie Guo;[~RocMarshal] Thanks for the reminder. TBH, I don't have time to do this asap, feel free to take over this if you want.;;;","21/Feb/23 02:27;RocMarshal;[~Weijie Guo] thanks for the reply.
I'm interested in it. May I get the ticket ?
IMO, before starting the process, we need sort out the `handleAsyncException` mechanism & state-switch of `SteamTask` ;;;","21/Feb/23 02:37;Weijie Guo;[~RocMarshal] You are assigned.;;;","22/Feb/23 09:31;RocMarshal;As described in the historical comments and description text,

The exceptions happen to `StreamTask` during the `restore()` was ignored by `asyncExceptionHandler`.
At the `Execution` side, it is possible to enter the \{@code FAILED} state from any other state described at `ExecutionState` class. However, here's no `isInitializing` flag or `Initializing` state in StreamTask.
We can deal the issue with the state rule of `ExecutionState`.

- Introduce is `isInitializing` flag for `StreamTask` in order to help `asyncExceptionHandler` judge handle branch.   It is worth noting that such an approach would result in two adjacent states where it is unsafe to change the value of the flags, and we can only rely on overlapping boundary conditions to ensure that exceptions can be handled

!image-2023-02-22-17-26-06-200.png!


 * Or we can introduce a State Enum for `StreamTask` like `ExecutionState`, If so, we should ensure that the state introduced is simple and overrides the current StreamTask state transition as a basic standard,  and the security of state transitions(thread-safe).


Please let me know what's your opinon. Thanks so much~

CC [~xieyi] [~Weijie Guo] [~kevin.cyj] ;;;","22/Feb/23 10:32;Weijie Guo;Thanks [~RocMarshal] for your analysis. It is a good proposal to change the state of `StreamTask` to enumeration. I have a look at the recently active tickets and just found FLINK-13871 seems to be doing the same thing.;;;","22/Feb/23 13:56;fanrui;Hi [~Weijie Guo] [~RocMarshal] , thanks for your analysis and PR.

StreamTask should `handleAsyncException` during initializing, so introducing isInitializing can solve this bug. But it makes the state of StreamTask difficult to maintain.

 

Hi [~pnowojski] [~akalashnikov] , nice to see TaskState introduced in FLINK-13871 to improve the state of StreamTask.

I recommend fixing the bug before refactoring the state machine for two reasons:
1. After the introduction of initializing, the state of TaskState will become more complicated, and FLINK-13871 will do more thought when designing TaskState. If we design TaskState first, not sure if it will be easily compatible with initializing in the future.
2. Fix bugs before refactoring the state machine, it will be easier to backport the bugfix to 1.16 and 1.17.

WDYT?;;;","22/Feb/23 14:56;Weijie Guo;[~fanrui] If we prefer to fix it in the first way (that is, without introducing state enumeration), I agree to fix it before refactoring. But before starting this work, we'd better make sure that all participants in `FLINK-13871` knows this in advance or even directly participates in the code review, which will be more conducive to their overall control of the state machine design.;;;","22/Feb/23 15:39;fanrui;Hi [~Weijie Guo] , thanks for your reminder. I have requested them, and I can go ahead after they agree. 

Anyway, I don't have strong opinion about which PR is merged first.;;;","27/Feb/23 02:24;fanrui;Thanks [~RocMarshal] for your contribution, and [~Weijie Guo] [~akalashnikov] for the discussion and review:)

Merged commit [a98bb9a6c978d5458cf4cc11dc7b68034f4def4b|https://github.com/apache/flink/commit/a98bb9a6c978d5458cf4cc11dc7b68034f4def4b] into apache:master;;;","27/Feb/23 14:49;Weijie Guo;Thanks [~fanrui] for merging this to master.

All the commits are as follows:
master 1.18 via a98bb9a6c978d5458cf4cc11dc7b68034f4def4b.
release 1.17 via 65e455189d2a88fd62be2e8a464feab4687ee088.
release 1.16 via 2f0df8076cff97b579f739b9605d32704c428213.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
image apache/flink:1.16.0-scala_2.12 does not exist,FLINK-29815,13493752,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mbalassi,voidking,voidking,31/Oct/22 12:23,29/Nov/22 13:33,13/Jul/23 08:13,22/Nov/22 08:20,1.16.0,,,,,,,,,,,,,,,,,,,0,,,,,"[https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/resource-providers/standalone/kubernetes/]

In the document, image `apache/flink:1.16.0-scala_2.12` was used.

!image-2022-10-31-20-17-38-977.png!

However, there is no image named `apache/flink:1.16.0-scala_2.12` in docker hub. !image-2022-10-31-20-18-35-718.png!

[https://hub.docker.com/r/apache/flink/tags?page=1&name=1.16.0]

So, we should either modify the document or upload the image.

If the document should be modified, I'd like to make a pull request to github repo.

 ",,mbalassi,voidking,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/22 12:17;voidking;image-2022-10-31-20-17-38-977.png;https://issues.apache.org/jira/secure/attachment/13051640/image-2022-10-31-20-17-38-977.png","31/Oct/22 12:18;voidking;image-2022-10-31-20-18-35-718.png;https://issues.apache.org/jira/secure/attachment/13051639/image-2022-10-31-20-18-35-718.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 13:33:03 UTC 2022,,,,,,,,,,"0|z1an5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/22 13:41;mbalassi;Thanks for reporting [~voidking] , taking a look.;;;","21/Nov/22 15:42;mbalassi;[~voidking] I reuploaded the 1.16.0 images, please check again:

[https://hub.docker.com/r/apache/flink/tags?page=1&name=1.16.0]

cc [~hxbks2ks] ;;;","29/Nov/22 13:33;voidking;Good job! The image now exists!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Table Store sink continuously fails with ""Trying to add file which is already added"" when snapshot committing is slow",FLINK-29805,13493693,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,31/Oct/22 09:17,08/Nov/22 07:00,13/Jul/23 08:13,08/Nov/22 07:00,table-store-0.2.2,table-store-0.3.0,,,,,,table-store-0.2.2,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,"Table Store sink continuously fails with ""Trying to add file which is already added"" when snapshot committing is slow.

This is due to a bug in {{FileStoreCommitImpl#filterCommitted}}. When this method finds an identifier, it removes the identifier from a map. However different snapshots may have the same identifier (for example an APPEND commit and the following COMPACT commit will have the same identifier), so we need to use another set to check for identifiers.

When snapshot committing is fast there is at most 1 identifier to check after the job restarts, so nothing happens. However when snapshot committing is slow, there will be multiple identifiers to check and some identifiers will be mistakenly kept.",,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29842,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 07:00:21 UTC 2022,,,,,,,,,,"0|z1amsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 07:00;TsReaper;master: 0a193a8110aa4716250fed3c4223018ab519c9b1
release-0.2: d41d14a5921aa27ebd903eed45365841093e97ed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table API Scala APIs lack proper source jars,FLINK-29803,13493683,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,31/Oct/22 08:11,01/Nov/22 09:23,13/Jul/23 08:13,01/Nov/22 09:23,1.15.0,,,,,,,1.15.3,1.16.1,1.17.0,,,Table SQL / API,,,,,,,0,pull-request-available,,,,,,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 01 09:23:48 UTC 2022,,,,,,,,,,"0|z1amq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 09:23;chesnay;master: 0bc9c538ac71249d39a27a0a6cca0ad9a0a87d8f
1.16: e9a3f7ca1002be5d119efce1aad79c7023bc1bf0
1.15: e45e5c3fa7d6c5d7d6b88e8a99cf0028987014a8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyflink protobuf requirement out of date,FLINK-29796,13493599,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jorgev,jorgev,29/Oct/22 20:08,05/Jan/23 02:12,13/Jul/23 08:13,05/Jan/23 02:12,1.16.0,,,,,,,,,,,,API / Python,,,,,,,0,,,,,"The setup.py file for pyflink currently requires protobuf<3.18 but the dev-requirements.txt file lists protubuf<=3.21 which seems to indicate that the library works with newer version of protobuf. The latest version of protobuf which satisfies the requirement was 3.17.3 which was released over a year ago, and notably the various gcloud api packages all require much newer versions (3.19+ I think). Obviously there are ways around this but the right answer is likely to ease/change the requirement.",,dianfu,engnatha,jorgev,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28786,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 15 18:42:55 UTC 2022,,,,,,,,,,"0|z1am7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/22 08:24;martijnvisser;I think the minimum version needs to be bumped; do you want to open a PR for it?;;;","04/Nov/22 05:20;dianfu;cc [~hxb];;;","15/Dec/22 17:51;engnatha;Just wanted to bump this. Looking at [https://github.com/apache/flink/blob/release-1.16/flink-python/setup.py#L314,] it seems this has already made its way back to 1.16 if I'm understanding this correctly? This is blocking me from pulling apache-flink in through our requirements.txt since we require protobuf > 3.19 due to the security vulnerabilities detailed [here|https://github.com/protocolbuffers/protobuf/security/advisories/GHSA-8gq9-2x98-w8hf]. We use [pantsbuild|https://www.pantsbuild.org/] for python repo management so there's no easy way to separate out our requirements for a temporary solution.;;;","15/Dec/22 18:42;martijnvisser;It looks this was changed as part of FLINK-28786 which is scheduled for 1.16.1 indeed. So if this change is sufficient, we can close this ticket (while linking them to each other);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileStoreCommitTest is unstable and may stuck,FLINK-29792,13493443,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,28/Oct/22 09:45,01/Nov/22 04:27,13/Jul/23 08:13,01/Nov/22 04:26,table-store-0.3.0,,,,,,,table-store-0.2.2,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,"{{FileStoreCommitTest}} may stuck because the {{FileStoreCommit}} in {{TestCommitThread}} does not commit APPEND snapshot when no new files are produced. In this case, if the following COMPACT snapshot conflicts with the current merge tree, the test will stuck.",,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 01 04:26:50 UTC 2022,,,,,,,,,,"0|z1al94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 04:26;TsReaper;master: 153804749deeb335083aeefc619b44a9276310e2
release-0.2: 2f97ff0c51d189685a1b6ca81414d12487c5725a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Print sink result mess up with GC log in E2eTestBase ,FLINK-29791,13493264,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,qingyue,qingyue,28/Oct/22 07:16,04/Nov/22 09:09,13/Jul/23 08:13,04/Nov/22 09:09,table-store-0.3.0,,,,,,,table-store-0.2.2,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,https://github.com/apache/flink-table-store/actions/runs/3343373246/jobs/5536523910,,qingyue,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 09:09:57 UTC 2022,,,,,,,,,,"0|z1ak5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 09:09;TsReaper;master: 534f8b5588dc6a96c3f92f9d9b4ea9230f1e550b
release-0.2: a50f29c1f06534faadedf5f531d6f2de64af2cee;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StatefulJobWBroadcastStateMigrationITCase failed in native savepoints,FLINK-29788,13492693,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,masteryhx,hxbks2ks,hxbks2ks,28/Oct/22 03:19,31/Oct/22 08:07,13/Jul/23 08:13,31/Oct/22 08:07,1.16.0,,,,,,,1.16.1,1.17.0,,,,Release System,Runtime / State Backends,,,,,,0,pull-request-available,,,, !image-2022-10-28-11-18-45-471.png! ,,hxbks2ks,masteryhx,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/22 03:18;hxbks2ks;image-2022-10-28-11-18-45-471.png;https://issues.apache.org/jira/secure/attachment/13051534/image-2022-10-28-11-18-45-471.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 31 08:07:15 UTC 2022,,,,,,,,,,"0|z1agmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/22 03:20;hxbks2ks;[~masteryhx] Could you help take a look? Thx.;;;","28/Oct/22 04:27;masteryhx;Sure. It's known limitation that changelog hasn't supported native savepoint in 1.16.
So I think we could just disable changelog when triggering native savepoint in this case.
It will not block the release of 1.16.
BTW, IIUC, Should the migration it case be executed before we decide to release 1.16 ? [~hxbks2ks] 

   ;;;","28/Oct/22 07:46;hxbks2ks;It is not the blocker of release 1.16.0，but it belongs to the finalized step of a release.;;;","31/Oct/22 06:41;masteryhx;I have commited a pr to fix it in the test.


I think the release processes could be improved here.

We could update the FlinkVersion of these classes finally, but the migration test should be executed when we prepare for the release.

It could help us to find the migration exception earily.

(Some exceptions maybe not so simple)
WDYT? [~hxbks2ks] ;;;","31/Oct/22 06:59;hxbks2ks;[~masteryhx] Thanks a lot for the fix. Before the release, the code may change, so I think preparing the migrated test data in advance may result in some problems.
;;;","31/Oct/22 08:07;hxbks2ks;Merged into master via c89e400ae379c8b7490d9af20f82f49319895dce
Merged into release-1.16 via 87d86c05c7a91d7546935ec1ceee3c0c55e2e191;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix ci METHOD_NEW_DEFAULT issue,FLINK-29787,13492677,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,liyubin117,liyubin117,liyubin117,28/Oct/22 03:12,28/Oct/22 09:19,13/Jul/23 08:13,28/Oct/22 09:19,1.17.0,,,,,,,1.17.0,,,,,Build System / CI,,,,,,,0,pull-request-available,,,,"`org.apache.flink.api.connector.source.SourceReader` declared a new default function `pauseOrResumeSplits()`, japicmp plugin failed during ci running, we should configure the plugin to make it compatible.",,aitozi,liyubin117,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29784,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 28 09:19:02 UTC 2022,,,,,,,,,,"0|z1agiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/22 06:00;aitozi;I also encountered this issue. Does it related to this commit ? [https://github.com/apache/flink/commit/82567cc9e9a23a2b6ca41f433c4b9310c0075767]

cc [~Yellow] ;;;","28/Oct/22 09:19;mapohl;master: cb442936156583d6cf42550f3d9187da427b9c68;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogNormalize uses wrong keys after transformation by WatermarkAssignerChangelogNormalizeTransposeRule ,FLINK-29781,13492530,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,27/Oct/22 13:22,21/Dec/22 14:25,13/Jul/23 08:13,15/Nov/22 09:06,1.15.3,1.16.0,,,,,,1.16.1,1.17.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"currently WatermarkAssignerChangelogNormalizeTransposeRule didn't remap the uniquekey indexes for its new input after plan rewrite, this may produce wrong result.

A simple case:
{code}

  @Test
  def testPushdownCalcNotAffectChangelogNormalizeKey(): Unit = {
    util.addTable(""""""
                    |CREATE TABLE t1 (
                    |  ingestion_time TIMESTAMP(3) METADATA FROM 'ts',
                    |  a VARCHAR NOT NULL,
                    |  b VARCHAR NOT NULL,
                    |  WATERMARK FOR ingestion_time AS ingestion_time
                    |) WITH (
                    | 'connector' = 'values',
                    | 'readable-metadata' = 'ts:TIMESTAMP(3)'
                    |)
      """""".stripMargin)
    util.addTable(""""""
                    |CREATE TABLE t2 (
                    |  k VARBINARY,
                    |  ingestion_time TIMESTAMP(3) METADATA FROM 'ts',
                    |  a VARCHAR NOT NULL,
                    |  f BOOLEAN NOT NULL,
                    |  WATERMARK FOR `ingestion_time` AS `ingestion_time`,
                    |  PRIMARY KEY (`a`) NOT ENFORCED
                    |) WITH (
                    | 'connector' = 'values',
                    | 'readable-metadata' = 'ts:TIMESTAMP(3)',
                    | 'changelog-mode' = 'I,UA,D'
                    |)
      """""".stripMargin)
    val sql =
      """"""
        |SELECT t1.a, t1.b, t2.f
        |FROM t1 INNER JOIN t2 FOR SYSTEM_TIME AS OF t1.ingestion_time
        | ON t1.a = t2.a WHERE t2.f = true
        |"""""".stripMargin
    util.verifyRelPlan(sql, ExplainDetail.CHANGELOG_MODE)
  }
{code}

the generated plan is incorrect for now:  {color:red}ChangelogNormalize(key=[ingestion_time]){color} uses wrong key 'ingestion_time' (should be 'a')

optimize result: 
{code}
Calc(select=[a, b, f])
+- TemporalJoin(joinType=[InnerJoin], where=[AND(=(a, a0), __TEMPORAL_JOIN_CONDITION(ingestion_time, ingestion_time0, __TEMPORAL_JOIN_CONDITION_PRIMARY_KEY(a0), __TEMPORAL_JOIN_LEFT_KEY(a), __TEMPORAL_JOIN_RIGHT_KEY(a0)))], select=[ingestion_time, a, b, ingestion_time0, a0, f])
   :- Exchange(distribution=[hash[a]])
   :  +- WatermarkAssigner(rowtime=[ingestion_time], watermark=[ingestion_time])
   :     +- Calc(select=[CAST(ingestion_time AS TIMESTAMP(3) *ROWTIME*) AS ingestion_time, a, b])
   :        +- TableSourceScan(table=[[default_catalog, default_database, t1]], fields=[a, b, ingestion_time])
   +- Exchange(distribution=[hash[a]])
      +- Calc(select=[ingestion_time, a, f], where=[f])
         +- ChangelogNormalize(key=[ingestion_time])
            +- Exchange(distribution=[hash[a]])
               +- WatermarkAssigner(rowtime=[ingestion_time], watermark=[ingestion_time])
                  +- Calc(select=[CAST(ingestion_time AS TIMESTAMP(3) *ROWTIME*) AS ingestion_time, a, f])
                     +- TableSourceScan(table=[[default_catalog, default_database, t2, project=[a, f], metadata=[ts]]], fields=[a, f, ingestion_time])
{code}",,godfreyhe,leonard,lincoln.86xy,qinjunjerry,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 14 09:25:25 UTC 2022,,,,,,,,,,"0|z1afm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 09:25;godfreyhe;Fixed in master: 5463f244ec69f623d75c15374b55bb8695e92b3e

in 1.16.1: 5466716b20d5c720bf29dea560909e7055870555;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PreAggregationITCase.LastValueAggregation and PreAggregationITCase.LastNonNullValueAggregation are unstable,FLINK-29773,13492154,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,27/Oct/22 03:20,27/Oct/22 07:46,13/Jul/23 08:13,27/Oct/22 07:46,table-store-0.3.0,,,,,,,,,,,,Table Store,,,,,,,0,pull-request-available,,,,"{{PreAggregationITCase.LastValueAggregation}} and {{PreAggregationITCase.LastNonNullValueAggregation}} need to make sure that the order of input data is determined. However the default parallelism of {{FileStoreTableITCase}} is 2, so the order of input data might change across tests.",,qingyue,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 07:46:47 UTC 2022,,,,,,,,,,"0|z1adao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/22 07:46;TsReaper;master: 147ea7528edfade6025e76fc66252765dc6f8fe4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flaky RollbackTest,FLINK-29771,13491892,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,darenwkt,gyfora,gyfora,26/Oct/22 15:15,11/Nov/22 08:47,13/Jul/23 08:13,11/Nov/22 08:47,kubernetes-operator-1.3.0,,,,,,,,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 13:58:35 UTC 2022,,,,,,,,,,"0|z1aboo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/22 13:58;gyfora;merged to main 3b59de9129c0677dd41a454fe5178103ce29f815;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve PostgresCatalog#listTables() by reusing resources,FLINK-29750,13491213,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liuml07,liuml07,liuml07,25/Oct/22 06:09,26/May/23 09:38,13/Jul/23 08:13,26/May/23 09:38,,,,,,,,jdbc-3.2.0,,,,,Connectors / JDBC,Table SQL / Ecosystem,,,,,,0,pull-request-available,,,,Currently the {{PostgresCatalog#listTables()}} creates a new connection and prepared statement for every schema and table when listing tables. This can be optimized by reusing those resources.,,liuml07,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 26 09:38:42 UTC 2023,,,,,,,,,,"0|z1a7i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/23 09:38;martijnvisser;Fixed in 
apache/flink-connector-jdbc:main 7fc202be3dfcdb6510f9855a6943dd97fa2bd3af;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink info command support dynamic properties,FLINK-29749,13491210,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,25/Oct/22 05:47,28/Oct/22 01:53,13/Jul/23 08:13,28/Oct/22 01:51,1.17.0,,,,,,,1.15.3,1.16.1,1.17.0,,,,,,,,,,0,pull-request-available,,,,,,jackylau,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27579,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 28 01:53:28 UTC 2022,,,,,,,,,,"0|z1a7hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/22 01:51;wangyang0918;Fixed via:

master: f8c6a668cd2b887f33a0cf4608de2d6b95c71f03

release-1.16: 38e90428bf7e603fdd353243f1edeba3553af2a3

release-1.15: 1d29f540a0692540a01b951033a8dc04fdb74d4f;;;","28/Oct/22 01:53;wangyang0918;Thanks [~jackylau] for your contribution.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split reader/writer factory for compaction in MergeTreeTest,FLINK-29745,13490975,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,25/Oct/22 01:24,25/Oct/22 03:41,13/Jul/23 08:13,25/Oct/22 03:41,table-store-0.2.2,,,,,,,table-store-0.3.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,,,lzljs3620320,zjureel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 25 03:41:01 UTC 2022,,,,,,,,,,"0|z1a614:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/22 03:41;lzljs3620320;master: 537e8cf4b3733d93720381c65b30024870ece533;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error Flink connector hive Test failing,FLINK-29733,13490500,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,samrat007,samrat007,samrat007,24/Oct/22 06:30,31/Oct/22 08:22,13/Jul/23 08:13,31/Oct/22 08:18,1.17.0,,,,,,,1.17.0,,,,,Connectors / Hive,,,,,,,0,pull-request-available,test-stability,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42328&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f]

This is caused by FLINK-29478

reported by [~hxbks2ks] ",,hxbks2ks,samrat007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 31 08:18:47 UTC 2022,,,,,,,,,,"0|z1a33s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/22 11:50;samrat007;Hi [~hxbks2ks] 

please review the changes in free time ;;;","31/Oct/22 08:18;hxbks2ks;Merged into master via 759d954960cbec28437955e5f482788902342035;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TablePlanner prevents Flink from starting is working directory is a symbolic link,FLINK-29728,13489026,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,angelok,angelok,22/Oct/22 03:04,14/Nov/22 14:41,13/Jul/23 08:13,31/Oct/22 01:37,1.15.2,,,,,,,1.16.1,1.17.0,,,,Table SQL / Planner,,,,,,,0,,,,,"The Flink runtime throws an exception when using the table API if the working directory is a symbolic link. This is the case when run on AWS EMR with Yarn. There is a similar issue [here|https://issues.apache.org/jira/browse/FLINK-20267] and I believe the same fix applied there would work.

 

 
{code:java}
Caused by: org.apache.flink.table.api.TableException: Could not initialize the table planner components loader.
    at org.apache.flink.table.planner.loader.PlannerModule.<init>(PlannerModule.java:123) ~[flink-table-planner-loader-1.15.1.jar:1.15.1]
    at org.apache.flink.table.planner.loader.PlannerModule.<init>(PlannerModule.java:52) ~[flink-table-planner-loader-1.15.1.jar:1.15.1]
    at org.apache.flink.table.planner.loader.PlannerModule$PlannerComponentsHolder.<clinit>(PlannerModule.java:131) ~[flink-table-planner-loader-1.15.1.jar:1.15.1]
    at org.apache.flink.table.planner.loader.PlannerModule.getInstance(PlannerModule.java:135) ~[flink-table-planner-loader-1.15.1.jar:1.15.1]
    at org.apache.flink.table.planner.loader.DelegateExecutorFactory.<init>(DelegateExecutorFactory.java:34) ~[flink-table-planner-loader-1.15.1.jar:1.15.1]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_342]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_342]
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_342]
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_342]
    at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_342]
    at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_342]
    at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) ~[?:1.8.0_342]
    at java.util.ServiceLoader$1.next(ServiceLoader.java:480) ~[?:1.8.0_342]
    at org.apache.flink.table.factories.ServiceLoaderUtil.load(ServiceLoaderUtil.java:42) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:798) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:517) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.create(TableEnvironmentImpl.java:276) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.TableEnvironment.create(TableEnvironment.java:93) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at com.ballista.Hermes.BCSE$.useLocalCatalog(BCSE.scala:210) ~[?:?]
    at com.ballista.Hermes.BCSE$.main(BCSE.scala:114) ~[?:?]
    at com.ballista.Hermes.BCSE.main(BCSE.scala) ~[?:?]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_342]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_342]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_342]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_342]
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355) ~[flink-dist-1.15.1.jar:1.15.1]
    ... 7 more
Caused by: java.nio.file.FileAlreadyExistsException: /tmp
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:88) ~[?:1.8.0_342]
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) ~[?:1.8.0_342]
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) ~[?:1.8.0_342]
    at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384) ~[?:1.8.0_342]
    at java.nio.file.Files.createDirectory(Files.java:674) ~[?:1.8.0_342]
    at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781) ~[?:1.8.0_342]
    at java.nio.file.Files.createDirectories(Files.java:727) ~[?:1.8.0_342]
    at org.apache.flink.table.planner.loader.PlannerModule.<init>(PlannerModule.java:96) ~[flink-table-planner-loader-1.15.1.jar:1.15.1]
    at org.apache.flink.table.planner.loader.PlannerModule.<init>(PlannerModule.java:52) ~[flink-table-planner-loader-1.15.1.jar:1.15.1]
    at org.apache.flink.table.planner.loader.PlannerModule$PlannerComponentsHolder.<clinit>(PlannerModule.java:131) ~[flink-table-planner-loader-1.15.1.jar:1.15.1]
    at org.apache.flink.table.planner.loader.PlannerModule.getInstance(PlannerModule.java:135) ~[flink-table-planner-loader-1.15.1.jar:1.15.1]
    at org.apache.flink.table.planner.loader.DelegateExecutorFactory.<init>(DelegateExecutorFactory.java:34) ~[flink-table-planner-loader-1.15.1.jar:1.15.1]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_342]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_342]
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_342]
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_342]
    at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_342]
    at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_342]
    at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) ~[?:1.8.0_342]
    at java.util.ServiceLoader$1.next(ServiceLoader.java:480) ~[?:1.8.0_342]
    at org.apache.flink.table.factories.ServiceLoaderUtil.load(ServiceLoaderUtil.java:42) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:798) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:517) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.create(TableEnvironmentImpl.java:276) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.TableEnvironment.create(TableEnvironment.java:93) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at com.ballista.Hermes.BCSE$.useLocalCatalog(BCSE.scala:210) ~[?:?]
    at com.ballista.Hermes.BCSE$.main(BCSE.scala:114) ~[?:?]
    at com.ballista.Hermes.BCSE.main(BCSE.scala) ~[?:?]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_342]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_342]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_342]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_342]
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355) ~[flink-dist-1.15.1.jar:1.15.1] {code}
 

 ",,angelok,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30001,,,,,,,,FLINK-28102,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 31 01:37:18 UTC 2022,,,,,,,,,,"0|z19u1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Oct/22 04:51;Weijie Guo;[~angelok] Thanks for reporting this, the code that will cause problems is the same as FLINK-28102, I will fix it together in that ticket. To facilitate tracking issue, I suggest closing this ticket.;;;","31/Oct/22 01:37;xtsong;- master (1.17): e8e9db37e17110ff04175d2720484b34f5c4d5ba
- release-1.16: 8fd9aa63a30a6037fcad752ab74fbdd6649ca3f0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The same batch task works fine in 1.15.2 and 1.16.0-rc1, but fails in 1.16.0-rc2",FLINK-29712,13488218,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,macdoor615,macdoor615,21/Oct/22 05:08,02/Nov/22 09:04,13/Jul/23 08:13,21/Oct/22 08:00,1.16.0,,,,,,,,,,,,Table SQL / Client,,,,,,,0,,,,,"All my batch jobs have failed with same error. All streaming jobs work fine.
{code:java}
org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=2, backoffTimeMS=60000)
    at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
    at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getGlobalFailureHandlingResult(ExecutionFailureHandler.java:102)
    at org.apache.flink.runtime.scheduler.DefaultScheduler.handleGlobalFailure(DefaultScheduler.java:299)
    at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder$LazyInitializedCoordinatorContext.lambda$failJob$0(OperatorCoordinatorHolder.java:635)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:453)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:453)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:218)
    at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
    at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
    at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
    at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at akka.actor.Actor.aroundReceive(Actor.scala:537)
    at akka.actor.Actor.aroundReceive$(Actor.scala:535)
    at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
    at akka.actor.ActorCell.invoke(ActorCell.scala:548)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
    at akka.dispatch.Mailbox.run(Mailbox.scala:231)
    at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
    at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: org.apache.flink.util.FlinkException: Global failure triggered by OperatorCoordinator for 'Source: p_hswtv[4] -> Calc[5]' (operator 6cdc5bb954874d922eaee11a8e7b5dd5).
    at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder$LazyInitializedCoordinatorContext.failJob(OperatorCoordinatorHolder.java:617)
    at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$QuiesceableContext.failJob(RecreateOnResetOperatorCoordinator.java:237)
    at org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.failJob(SourceCoordinatorContext.java:360)
    at org.apache.flink.runtime.source.coordinator.SourceCoordinator.start(SourceCoordinator.java:217)
    at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$DeferrableCoordinator.applyCall(RecreateOnResetOperatorCoordinator.java:315)
    at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.start(RecreateOnResetOperatorCoordinator.java:70)
    at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.start(OperatorCoordinatorHolder.java:198)
    at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.startOperatorCoordinators(DefaultOperatorCoordinatorHandler.java:165)
    at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.startAllOperatorCoordinators(DefaultOperatorCoordinatorHandler.java:82)
    at org.apache.flink.runtime.scheduler.SchedulerBase.startScheduling(SchedulerBase.java:605)
    at org.apache.flink.runtime.jobmaster.JobMaster.startScheduling(JobMaster.java:1046)
    at org.apache.flink.runtime.jobmaster.JobMaster.startJobExecution(JobMaster.java:963)
    at org.apache.flink.runtime.jobmaster.JobMaster.onStart(JobMaster.java:422)
    at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStart(RpcEndpoint.java:198)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.lambda$start$0(AkkaRpcActor.java:622)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.start(AkkaRpcActor.java:621)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:190)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
    at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
    at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
    at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
    ... 13 more
Caused by: java.lang.NumberFormatException: null
    at java.lang.Integer.parseInt(Integer.java:542)
    at java.lang.Integer.parseInt(Integer.java:615)
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.calculateFilesSizeWithOpenCost(HiveSourceFileEnumerator.java:157)
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.setSplitMaxSize(HiveSourceFileEnumerator.java:135)
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:89)
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.enumerateSplits(HiveSourceFileEnumerator.java:67)
    at org.apache.flink.connector.file.src.AbstractFileSource.createEnumerator(AbstractFileSource.java:141)
    at org.apache.flink.connectors.hive.HiveSource.createEnumerator(HiveSource.java:129)
    at org.apache.flink.runtime.source.coordinator.SourceCoordinator.start(SourceCoordinator.java:213)
    ... 33 more
 
 
{code}
 ","Flink 1.16.0-rc2

Hive 3.1.3

Hadoop 3.3.4",godfreyhe,luoyuxia,macdoor615,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/22 05:13;macdoor615;flink-conf.yaml;https://issues.apache.org/jira/secure/attachment/13051265/flink-conf.yaml",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 21 08:00:27 UTC 2022,,,,,,,,,,"0|z19p20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/22 05:14;macdoor615;job parameters 

 
{code:java}
SET table.dml-sync = true;
SET execution.runtime-mode = batch;
SET parallelism.default = -1;
SET table.exec.hive.infer-source-parallelism = true;
SET table.exec.hive.infer-source-parallelism.max = 16;
SET taskmanager.network.memory.buffers-per-channel = 0;
SET jobmanager.adaptive-batch-scheduler.avg-data-volume-per-task = 4kb;
SET table.dynamic-table-options.enabled = true;
SET taskmanager.network.memory.buffer-debloat.enabled = true;
SET table.exec.legacy-cast-behaviour=enabled;
{code}
 ;;;","21/Oct/22 06:18;Weijie Guo;cc [~luoyuxia];;;","21/Oct/22 06:56;luoyuxia;[~macdoor615] Which mode do you run Flink? Application, Session mode or others?;;;","21/Oct/22 07:05;macdoor615;[~luoyuxia] session mode;;;","21/Oct/22 07:35;luoyuxia;[~macdoor615] Hi, thanks for raising it. But I can't reproduce it in my local env. I test with hive 2.9 and hadoop2 since I have no hive3, but i think the version shouldn't make difference.

I try with the following steps:

1: build flink

2: start flink cluster

 
{code:java}
./bin/start-cluster {code}
3: start sql-client

 

 
{code:java}
./bin/sql-client
{code}
4: create a hive catalog  which has contains a table `t2` with orc format.

 

 
{code:java}
create catalog hive_catalog with ('type' ='hive', 'hive-conf-dir' = 'xxx'); 

use catalog hive_catalog;{code}
5:  run a query

 

 
{code:java}
select * from t2;
{code}
 

[~macdoor615] Since you use session mode, have you also updated the client's version to the rc2?;;;","21/Oct/22 07:59;macdoor615;@luoyuxia I upgraded client to Flink 1.16.0-rc2 from 1.16.0-rc1. Problem solved. Thx;;;","21/Oct/22 08:00;macdoor615;upgraded client to Flink 1.16.0-rc2 from 1.16.0-rc1. Problem solved;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Topic notification not present in metadata after 60000 ms.,FLINK-29711,13487859,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,durgeshkumarmishragfx,durgeshkumarmishragfx,21/Oct/22 02:26,24/Nov/22 08:04,13/Jul/23 08:13,24/Nov/22 06:51,1.14.4,1.14.6,,,,,,,,,,,Connectors / Kafka,,,,,,,0,,,,,"Failed to send data to Kafka null with FlinkKafkaInternalProducer\{transactionalId='null', inTransaction=false, closed=false}
at org.apache.flink.connector.kafka.sink.KafkaWriter$WriterCallback.throwException(KafkaWriter.java:405)
at org.apache.flink.connector.kafka.sink.KafkaWriter$WriterCallback.lambda$onCompletion$0(KafkaWriter.java:391)
at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:353)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:317)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
at java.base/java.lang.Thread.run(Unknown Source)",,durgeshkumarmishragfx,martijnvisser,mason6345,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/22 03:11;durgeshkumarmishragfx;image-2022-11-01-08-41-21-836.png;https://issues.apache.org/jira/secure/attachment/13051658/image-2022-11-01-08-41-21-836.png","02/Nov/22 04:49;durgeshkumarmishragfx;image-2022-11-02-10-19-01-824.png;https://issues.apache.org/jira/secure/attachment/13051695/image-2022-11-02-10-19-01-824.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 06:49:59 UTC 2022,,,,,,,,,,"0|z19mu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/22 08:16;martijnvisser;Please add more information to the ticket next to the stacktrace. How did you end up with this error, what steps can be taken to reproduce the issue?;;;","21/Oct/22 08:35;durgeshkumarmishragfx;Hello [~martijnvisser]  We created one flink job and one Azure eventhub. Flink is processing real time data which published to Azure event hub and after continually running of flink job for about 10 hours  above  exception occurs.

 

 Used  following configuration.
 # Checkpoints configurations

checkpoints.interval= 240000

checkpoints.minPauseBetweenCheckpoints= 120000

checkpoints.timeout= 110000

 
 # Common Flink-Kafka-Connector(Source and Sink) configurations

allow.auto.create.topics=false

auto.offset.reset=latest

request.timeout.ms=60000

transaction.timeout.ms=60000

kafka.semantic=1

kafka.internalProducerPoolSize=5
 # For reducing the kafka timeout

max.block.ms=5000
 # For increasing the metadata fetch time

metadata.max.idle.ms= 180000

 ;;;","21/Oct/22 08:53;martijnvisser;Which Kafka sink are you using, FlinkKafkaProducer or KafkaSink? 
Are you using Exactly Once or a different guarantee? 
I know that Azure Eventhub can emulate Kafka, but we've also seen that these emulators can't exactly mirror everything that Kafka is doing (we've seen the same with RedPanda). My suspicion would be that there's something at the Azure Eventhub side of things that doesn't work exactly the same as in Kafka. ;;;","21/Oct/22 10:42;durgeshkumarmishragfx;[~martijnvisser] I am using the KafkaDink and using the AtleastOnce gurantee. 

We are using Azure eventhub in other projects as well, but there we are not facing this kind of issue.;;;","25/Oct/22 05:23;durgeshkumarmishragfx;[~martijnvisser]  and [~mason6345]  Can you guys help us here to fix this problem ?;;;","25/Oct/22 06:50;mason6345;Hi, I did a quick pass over the source code. Can you double check that you are not producing a null record? For example, you can add a log to your KafkaRecordSerializationSchema to verify.;;;","26/Oct/22 13:25;durgeshkumarmishragfx;[~mason6345]  I added the logs but didn't got any null values.;;;","26/Oct/22 14:21;martijnvisser;Is this something that you can make reproducible for us? ;;;","26/Oct/22 16:35;mason6345;Can you provide exact details where you added logs? I can reproduce this error message in unit test.;;;","27/Oct/22 06:34;durgeshkumarmishragfx;[~mason6345]  I added logs in class which implements KafkaRecordSerializationSchema.;;;","28/Oct/22 14:33;durgeshkumarmishragfx;[~martijnvisser]  I checked with microsoft  Azure Eventhub side of things, but sadly no luck they showed me everything working well on their side.

 

Do you guys can suggest something on this ? If possible we can have call to discuss this problem sometime. What's your thought ?;;;","29/Oct/22 19:30;mason6345;[~durgeshkumarmishragfx] can you post a minimal example of your KafkaSink setup?;;;","01/Nov/22 03:13;durgeshkumarmishragfx;[~mason6345]  Please find below example for kafka sink setup. Kindly let me know if you need anything else.

!image-2022-11-01-08-41-21-836.png|width=521,height=207!;;;","01/Nov/22 03:33;mason6345;Yes, I'm wondering about the TripSerializer implementation. Is it possible for it to return null?

I'm pretty sure this improvement will resolve your issue that you are experiencing:
https://issues.apache.org/jira/browse/FLINK-29480;;;","01/Nov/22 03:43;durgeshkumarmishragfx;[~mason6345]  Yes TripSerializer can return some entity which contains null values but object will never be null.

As i see the pull request we just need to @Nullable annotation which should resolved issue. Correct ?;;;","01/Nov/22 03:50;mason6345;No, the fix is included in 1.17.

That's your issue then–the API breaks when null is returned by TripSerializer.

You need to filter out inputs that result in `null` before the KafkaSink and before these values are serialized into the `null` value.;;;","01/Nov/22 03:58;durgeshkumarmishragfx;[~mason6345]  Not sure that will be issue, Because as a whole json object which we collecting is never null.

Object contains some entity which might be null.

Code which we are using inside the TripSerializer. Null records will only send in case of exception.

[~mason6345]  Now can you help me out here ? I understand we need to add null check before sending out the data. But apart from that, Is anything else we can try ?

!image-2022-11-02-10-19-01-824.png|width=354,height=133!;;;","23/Nov/22 08:26;martijnvisser;[~durgeshkumarmishragfx] I've verified with other services that this issue no longer occurs with Flink 1.16, most likely due to the upgrade to a newer version of the Kafka Client. Can you also check on your end with this version?;;;","24/Nov/22 06:49;durgeshkumarmishragfx;[~martijnvisser]  Yes you are correct we upgrade to 1.16 and the issue gone off after changing some request.timeout.ms to 30 sec.

 

So we are good to close this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix possible comparator violation for ""flink list""",FLINK-29707,13487279,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ferenc-csaky,ferenc-csaky,ferenc-csaky,20/Oct/22 14:34,26/Oct/22 14:28,13/Jul/23 08:13,26/Oct/22 14:28,1.16.0,,,,,,,,,,,,Command Line Client,,,,,,,0,pull-request-available,,,,"For the {{list}} CLI option, the code that prints the jobs, there is a {{startTimeComparator}} definition, which orders the jobs and it is done this way:
{code:java}
Comparator<JobStatusMessage> startTimeComparator =
                (o1, o2) -> (int) (o1.getStartTime() - o2.getStartTime());
{code}
In some rare situation this can lead to this:
{code:java}
2022-10-19 09:58:11,690 ERROR org.apache.flink.client.cli.CliFrontend                      [] - Error while running the command.
java.lang.IllegalArgumentException: Comparison method violates its general contract!
	at java.util.TimSort.mergeLo(TimSort.java:777) ~[?:1.8.0_312]
	at java.util.TimSort.mergeAt(TimSort.java:514) ~[?:1.8.0_312]
	at java.util.TimSort.mergeForceCollapse(TimSort.java:457) ~[?:1.8.0_312]
	at java.util.TimSort.sort(TimSort.java:254) ~[?:1.8.0_312]
	at java.util.Arrays.sort(Arrays.java:1512) ~[?:1.8.0_312]
	at java.util.ArrayList.sort(ArrayList.java:1464) ~[?:1.8.0_312]
	at java.util.stream.SortedOps$RefSortingSink.end(SortedOps.java:392) ~[?:1.8.0_312]
	at java.util.stream.Sink$ChainedReference.end(Sink.java:258) ~[?:1.8.0_312]
	at java.util.stream.Sink$ChainedReference.end(Sink.java:258) ~[?:1.8.0_312]
	at java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:363) ~[?:1.8.0_312]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:483) ~[?:1.8.0_312]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_312]
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ~[?:1.8.0_312]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) ~[?:1.8.0_312]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_312]
	at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:490) ~[?:1.8.0_312]
	at org.apache.flink.client.cli.CliFrontend.printJobStatusMessages(CliFrontend.java:574)
{code}",,ferenc-csaky,mbalassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 26 14:28:28 UTC 2022,,,,,,,,,,"0|z19j9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 15:13;ferenc-csaky;Just opened a PR with the proposed fix.;;;","26/Oct/22 14:28;mbalassi;[5176fb2|https://github.com/apache/flink/commit/5176fb2c334edceb828f92ed90606401b8381d3f] in master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to call unix_timestamp  in runtime in Hive dialect ,FLINK-29703,13487242,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,luoyuxia,luoyuxia,20/Oct/22 10:21,21/Oct/22 03:57,13/Jul/23 08:13,21/Oct/22 03:57,,,,,,,,,,,,,Connectors / Hive,,,,,,,0,,,,,"Can be reproduced by the following sql with Hive dialect:
{code:java}
select unix_timestamp();{code}",,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26474,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 21 03:12:08 UTC 2022,,,,,,,,,,"0|z19j14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/22 03:12;luoyuxia;The reason is the hive function unix_timestamp will call `SessionState.get().getQueryCurrentTimestamp()` in runtime.

But the SessionState.get() will return null since the SessionState will be closed in Flink.

Considering `SessionState.get().getQueryCurrentTimestamp()` is actually a fixed value set in query parse phase.

To fix it, we need to convert the function call `unix_timestamp` to literal instead of evaluting in runtime siince the value has no difference between convert it to a literal and evaluating it in runtime. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Serializer to BinaryInMemorySortBuffer is wrong,FLINK-29700,13487218,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,20/Oct/22 08:04,21/Oct/22 06:52,13/Jul/23 08:13,21/Oct/22 06:52,,,,,,,,table-store-0.2.2,,,,,Table Store,,,,,,,0,pull-request-available,,,,"In SortBufferMemTable, it will use `BinaryInMemorySortBuffer.createBuffer(BinaryRowDataSerializer serializer)`, the serializer is for full row, not just sort key fields.

Problems may occur when there are many fields.
",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 21 06:52:29 UTC 2022,,,,,,,,,,"0|z19ivs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/22 06:52;lzljs3620320;release-0.2: aa6846089afc61e6715a7d50ae40e6bb9d8efc0f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
listTables in HiveCatalog should only return table store tables,FLINK-29690,13487081,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,lzljs3620320,lzljs3620320,19/Oct/22 13:00,20/Oct/22 03:52,13/Jul/23 08:13,20/Oct/22 03:52,,,,,,,,table-store-0.2.2,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,lzljs3620320,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 20 03:52:22 UTC 2022,,,,,,,,,,"0|z19i1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 03:52;lzljs3620320;master: 997682a9507ccf9165e7d2ff4e95aca8c56df113
release-0.2: 159b1446d458f84a85c452dd5ef297458e1a673b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not throw exception when Lookup table is empty,FLINK-29687,13487039,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,19/Oct/22 09:42,21/Oct/22 03:26,13/Jul/23 08:13,21/Oct/22 03:26,table-store-0.2.1,table-store-0.3.0,,,,,,table-store-0.2.2,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,"!image-2022-10-19-17-44-10-062.png|width=724,height=431!

When scanning the Lookup table, it is likely that the snapshot does not be committed at that moment. So it's better to wait for the commit other than throwing an exception.",,lzljs3620320,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/22 09:44;qingyue;image-2022-10-19-17-44-10-062.png;https://issues.apache.org/jira/secure/attachment/13051172/image-2022-10-19-17-44-10-062.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 21 03:26:34 UTC 2022,,,,,,,,,,"0|z19hs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/22 03:26;lzljs3620320;master: b54c474e2596271fedfffaec5eddc895e0bb5455
release-0.2: 56c952e059686a5096163356f563548bb1dc3f2e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python side-output operator not generated in some cases,FLINK-29681,13486876,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,18/Oct/22 16:36,17/Feb/23 07:45,13/Jul/23 08:13,19/Oct/22 06:15,1.16.0,,,,,,,1.16.0,,,,,API / Python,,,,,,,0,pull-request-available,,,,"If a SideOutputDataStream is used in `execute_and_collect`, `from_data_stream`, `create_temporary_view`, the side-outputing operator will not be properly configured, since we rely on the bottom-up scan of transformations while there's no solid transformation after the SideOutputTransformation in these cases. Thus, an error, ""tuple object has no attribute get_fields_by_names"", will be raised.",,hxb,Juntao Hu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 19 06:15:40 UTC 2022,,,,,,,,,,"0|z19gug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/22 06:15;hxb;Merged into master via 93a9c504b882356fdf65ca962c4169ecb1bf66e5

Merged into release-1.16 via 06309528603333153dcabe1bd0e74bcde19cf6f5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent dropping the current catalog,FLINK-29677,13486797,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,18/Oct/22 09:06,24/Nov/22 19:28,13/Jul/23 08:13,24/Oct/22 07:00,1.16.0,1.17.0,,,,,,1.16.1,1.17.0,,,,Table SQL / API,,,,,,,0,pull-request-available,,,,"h3. Issue Description

Currently, the drop catalog statement

 
{code:java}
DROP CATALOG my_cat{code}
 

does not reset the current catalog. As a result, if dropping a catalog in use, then the following statements will yield different results.

 
{code:java}
SHOW CURRENT CATALOG
SHOW CATALOGS
{code}
 
h3. How to Reproduce

!image-2022-10-18-16-55-38-525.png|width=444,height=421!

 
h3. Proposed Fix Plan

The root cause is that `CatalogManager#unregisterCatalog` does not reset `currentCatalogName`. 

Regarding this issue, I checked MySQL and PG's behavior.

For MySQL, it is allowed to drop a database current-in-use and set the current database to NULL.

!image-2022-10-18-17-02-30-318.png|width=288,height=435!

 

For PG, it is not allowed to drop the database currently in use.

 

I think both behaviors are reasonable, while for simplicity I suggest adhering to PG, that throw an Exception when dropping the current catalog.

cc [~jark]  [~fsk119]  ",,fsk119,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17357,,,,,,,,"18/Oct/22 08:55;qingyue;image-2022-10-18-16-55-38-525.png;https://issues.apache.org/jira/secure/attachment/13051072/image-2022-10-18-16-55-38-525.png","18/Oct/22 09:02;qingyue;image-2022-10-18-17-02-30-318.png;https://issues.apache.org/jira/secure/attachment/13051071/image-2022-10-18-17-02-30-318.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 22 14:41:25 UTC 2022,,,,,,,,,,"0|z19gcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/22 09:09;qingyue;I'd like to resolve this issue, and please assign this ticket to me. Thanks.;;;","18/Oct/22 09:27;fsk119;+1 to align with the PG behaviour.;;;","22/Oct/22 14:41;fsk119;Merged into the master: 271ef7b7dfac195b43abced8b81c38998855dabf
Merged into the release-1.16: cc17d700dd6ec1708ed6b32e6051f0b00371669f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""LocalDateTime not supported"" error when retrieving Java TypeInformation from PyFlink",FLINK-29648,13486429,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,15/Oct/22 09:28,18/Oct/22 06:50,13/Jul/23 08:13,18/Oct/22 06:50,1.16.0,,,,,,,1.16.0,,,,,API / Python,,,,,,,0,pull-request-available,,,,"The following code raises ""TypeError: The java type info: LocalDateTime is not supported in PyFlink currently."":
{code:java}
t_env.to_data_stream(t).key_by(...){code}
However, this works:
{code:java}
t_env.to_data_stream(t).map(lambda r: r).key_by(...){code}
Although we add Python coders for LocalTimeTypeInfo in 1.16, there's no corresponding typeinfo at Python side. So it works when a user immediately does processing after to_data_stream since date/time data has already been converted to Python object, but when key_by tries to retrieve typeinfo from Java TypeInformation, it fails.",,dianfu,hxb,Juntao Hu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29658,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 18 06:50:16 UTC 2022,,,,,,,,,,"0|z19e34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/22 06:50;hxb;Merged into master via ef0489b5131d03112d8a4ef11962a2d1ba6a9f54

Merged into release-1.16 via 59a276b38813a66f9ad80ed81b3dfcfe26decb7a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BatchExecutionKeyedStateBackend is using incorrect ExecutionConfig when creating serializer,FLINK-29645,13486345,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,dwysakowicz,pnowojski,pnowojski,14/Oct/22 13:22,17/Oct/22 15:27,13/Jul/23 08:13,17/Oct/22 15:27,1.12.7,1.13.6,1.14.6,1.15.2,1.16.0,1.17.0,,1.15.3,1.16.0,1.17.0,,,Runtime / State Backends,,,,,,,0,pull-request-available,,,,"{{org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionKeyedStateBackend#getOrCreateKeyedState}} is using freshly constructed {{ExecutionConfig}}, instead of the one configured by the user from the environment.


{code:java}
    public <N, S extends State, T> S getOrCreateKeyedState(
            TypeSerializer<N> namespaceSerializer, StateDescriptor<S, T> stateDescriptor)
            throws Exception {
        checkNotNull(namespaceSerializer, ""Namespace serializer"");
        checkNotNull(
                keySerializer,
                ""State key serializer has not been configured in the config. ""
                        + ""This operation cannot use partitioned state."");

        if (!stateDescriptor.isSerializerInitialized()) {
            stateDescriptor.initializeSerializerUnlessSet(new ExecutionConfig());
        }
{code}

The correct one could be obtained from {{env.getExecutionConfig()}} in {{org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend#createKeyedStateBackend}} ",,dwysakowicz,maguowei,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 17 15:27:22 UTC 2022,,,,,,,,,,"0|z19dkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/22 07:24;dwysakowicz;This is not a significant issue as we actually do not use any of the serializers during operation. The code is there to fulfill contracts (there is a check in the stack if a serializer has been initialized). I'll fix that anyhow.;;;","17/Oct/22 15:27;dwysakowicz;Fixed in:
* master
** 0ffcfb757965ca62f7b4f1b93fd1387a45a50b2c
* 1.16
** c07d5aa98c016201eab38f883d20f2e807213113
* 1.15
** f19f032daee2fabefb4ccc6257740dd491b3a925;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SortMergeResultPartitionReadSchedulerTest.testCreateSubpartitionReader,FLINK-29641,13486301,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,mapohl,mapohl,14/Oct/22 10:03,19/Oct/22 06:10,13/Jul/23 08:13,19/Oct/22 06:10,1.16.0,1.17.0,,,,,,1.16.0,,,,,Runtime / Network,Tests,,,,,,0,pull-request-available,test-stability,,,"[This build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42011&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=8433] failed (not exclusively) due to {{SortMergeResultPartitionReadSchedulerTest.testCreateSubpartitionReader}}. The assert checking that the {{SortedMergeSubpartitionReader}} is in running state fails.

My suspicion is that the condition in [SortMergeResultPartitionReadScheduler.mayTriggerReading|https://github.com/apache/flink/blob/87d4f70e49255b551d0106117978b1aa0747358c/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/SortMergeResultPartitionReadScheduler.java#L425-L428] (or something related to that condition) needs to be reconsidered since that's the only time {{isRunning}} is actually set to true.",,hxb,mapohl,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24898,FLINK-28374,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 19 06:10:57 UTC 2022,,,,,,,,,,"0|z19dao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/22 10:09;mapohl;I'm linking some tickets that might be related to the issue. [~tanyuxin] [~kevin.cyj] [~Weijie Guo] (based on the commit history) do you have some opinion on that one?;;;","14/Oct/22 11:26;Weijie Guo;[~mapohl] Thank you for your report and investigation,I will take a look;;;","18/Oct/22 10:10;Weijie Guo;This JIRA can be closed as the pull request has been merged.;;;","19/Oct/22 06:10;hxb;Merged into master via 959fe97beeac15bbac70f4980cc6a3a110b432a2

Merged into release-1.16 via d233b39be94f330dabba593ac3e709a73eb714d2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Operator doesn't pass initialSavepointPath as fromSavepoint argument,FLINK-29633,13486188,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sap1ens,sap1ens,sap1ens,13/Oct/22 17:43,16/Oct/22 08:35,13/Jul/23 08:13,15/Oct/22 20:14,kubernetes-operator-1.2.0,,,,,,,kubernetes-operator-1.2.1,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"The Kubernetes Operator doesn't pass *initialSavepointPath* from the JobSpec as a *--fromSavepoint* argument to the JobManager. The operator does update the configuration, but in the standalone mode, Flink actually [overrides that|https://github.com/apache/flink/blob/012dc6a9b800bae0cfa5250d38de992ccbabc015/flink-container/src/main/java/org/apache/flink/container/entrypoint/StandaloneApplicationClusterEntryPoint.java#L57-L63] based on the command-line arguments. 

*CmdStandaloneJobManagerDecorator* should be updated to include *fromSavepoint.*",,gyfora,sap1ens,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 15 20:14:43 UTC 2022,,,,,,,,,,"0|z19clk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Oct/22 20:14;gyfora;aa88e9a6738160b539a0974eae176db633a408f7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[CVE-2022-42003] flink-shaded-jackson,FLINK-29631,13486180,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,snuyanzin,sergiosp,sergiosp,13/Oct/22 16:52,20/Oct/22 10:03,13/Jul/23 08:13,20/Oct/22 10:03,shaded-16.0,,,,,,,shaded-17.0,,,,,BuildSystem / Shaded,,,,,,,0,pull-request-available,,,,"flink-shaded-jackson vulnerable to 7.5 (high) [https://nvd.nist.gov/vuln/detail/CVE-2022-42003]

 

Ref:

[https://nvd.nist.gov/vuln/detail/CVE-2022-42003]

[https://repo1.maven.org/maven2/org/apache/flink/flink-shaded/16.0/flink-shaded-16.0.pom]

[https://mvnrepository.com/artifact/org.apache.flink/flink-shaded-jackson-parent/2.13.4-16.0]

[https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-jackson/2.13.4-16.0/flink-shaded-jackson-2.13.4-16.0.pom]",,martijnvisser,sergiosp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 20 10:03:57 UTC 2022,,,,,,,,,,"0|z19cjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 10:03;martijnvisser;Fixed in master: 7f6ff435bcfdf593e16a82452aae53faec8ed6c4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sink - Duplicate key exception during recover more than 1 committable.,FLINK-29627,13486099,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,KristoffSC,KristoffSC,KristoffSC,13/Oct/22 10:12,20/Oct/22 09:05,13/Jul/23 08:13,20/Oct/22 09:05,1.15.2,1.16.0,1.17.0,,,,,1.15.3,1.16.1,1.17.0,,,Connectors / Common,,,,,,,0,,,,,"Recovery more than one Committable  causes `IllegalStateException` and prevents cluster to start.

When we recover the `CheckpointCommittableManager` we deserialize SubtaskCommittableManager instances from recovery state and we put them into `Map<Integer, SubtaskCommittableManager<CommT>>`. The key of this map is subtaskId of the recovered manager. However this will fail if we have to recover more than one committable. 

What w should do is to call `SubtaskCommittableManager::merge` if we already deserialize manager for this subtaskId.


Stack Trace:
{code:java}
28603 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Sink: Global Committer (1/1) (485dc57aca56235b9d1ab803c8c966ad_47d89856a1cf553f16e7063d953b7d42_0_1) switched from INITIALIZING to FAILED on 2ed5c848-d360-48ae-9a92-730b022c8a39 @ kubernetes.docker.internal (dataPort=-1).
java.lang.IllegalStateException: Duplicate key 0 (attempted merging values org.apache.flink.streaming.runtime.operators.sink.committables.SubtaskCommittableManager@631940ac and org.apache.flink.streaming.runtime.operators.sink.committables.SubtaskCommittableManager@7ff3bd7)
	at java.util.stream.Collectors.duplicateKeyException(Collectors.java:133) ~[?:?]
	at java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:180) ~[?:?]
	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ~[?:?]
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655) ~[?:?]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ~[?:?]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ~[?:?]
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ~[?:?]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:?]
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ~[?:?]
	at org.apache.flink.streaming.runtime.operators.sink.committables.CommittableCollectorSerializer$CheckpointSimpleVersionedSerializer.deserialize(CommittableCollectorSerializer.java:153) ~[classes/:?]
	at org.apache.flink.streaming.runtime.operators.sink.committables.CommittableCollectorSerializer$CheckpointSimpleVersionedSerializer.deserialize(CommittableCollectorSerializer.java:124) ~[classes/:?]
	at org.apache.flink.core.io.SimpleVersionedSerialization.readVersionAndDeserializeList(SimpleVersionedSerialization.java:148) ~[classes/:?]
	at org.apache.flink.streaming.runtime.operators.sink.committables.CommittableCollectorSerializer.deserializeV2(CommittableCollectorSerializer.java:105) ~[classes/:?]
	at org.apache.flink.streaming.runtime.operators.sink.committables.CommittableCollectorSerializer.deserialize(CommittableCollectorSerializer.java:82) ~[classes/:?]
	at org.apache.flink.streaming.runtime.operators.sink.committables.CommittableCollectorSerializer.deserialize(CommittableCollectorSerializer.java:41) ~[classes/:?]
	at org.apache.flink.core.io.SimpleVersionedSerialization.readVersionAndDeSerialize(SimpleVersionedSerialization.java:121) ~[classes/:?]
	at org.apache.flink.streaming.api.connector.sink2.GlobalCommitterSerializer.deserializeV2(GlobalCommitterSerializer.java:128) ~[classes/:?]
	at org.apache.flink.streaming.api.connector.sink2.GlobalCommitterSerializer.deserialize(GlobalCommitterSerializer.java:99) ~[classes/:?]
	at org.apache.flink.streaming.api.connector.sink2.GlobalCommitterSerializer.deserialize(GlobalCommitterSerializer.java:42) ~[classes/:?]
	at org.apache.flink.core.io.SimpleVersionedSerialization.readVersionAndDeSerialize(SimpleVersionedSerialization.java:227) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.util.SimpleVersionedListState$DeserializingIterator.next(SimpleVersionedListState.java:138) ~[classes/:?]
	at java.lang.Iterable.forEach(Iterable.java:74) ~[?:?]
	at org.apache.flink.streaming.api.connector.sink2.GlobalCommitterOperator.initializeState(GlobalCommitterOperator.java:133) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:122) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:286) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:727) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:703) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:670) ~[classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935) ~[classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904) ~[classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728) ~[classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550) ~[classes/:?]
	at java.lang.Thread.run(Thread.java:834) ~[?:?]
{code}


",,fpaul,KristoffSC,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29459,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 20 09:04:50 UTC 2022,,,,,,,,,,"0|z19c1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 10:18;KristoffSC;I have a fix and tests for this issue.
Will provide PR shortly;;;","14/Oct/22 09:53;KristoffSC;PR ready:
https://github.com/apache/flink/pull/21052


Pending on https://github.com/apache/flink/pull/21022.;;;","18/Oct/22 13:58;KristoffSC;New PR without SinkItTest
https://github.com/apache/flink/pull/21101;;;","19/Oct/22 13:17;KristoffSC;Backports:
1.15 - https://github.com/apache/flink/pull/21113
1.16 - https://github.com/apache/flink/pull/21115;;;","20/Oct/22 09:04;fpaul;Merged into:

master: ac044f894dc930dcad0def447823055d9386a2eb

release-1.16: 55c1095f81d6c95df753c68bc39bc249cf36a81c

release-1.15: 47bcb5a0cfa384fd8de61c6eff934d68323d566e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Append-only with eventual log.consistency can not work,FLINK-29621,13486065,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,13/Oct/22 07:55,13/Oct/22 12:02,13/Jul/23 08:13,13/Oct/22 12:02,,,,,,,,table-store-0.2.2,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,"{code:java}
                        ""CREATE TABLE T (i INT, j INT) WITH (""
                                + ""'log.system'='kafka', ""
                                + ""'write-mode'='append-only', ""
                                + ""'log.consistency'='eventual', ""
                                + ""'kafka.bootstrap.servers'='%s', ""
                                + ""'kafka.topic'='T')"",
{code}

Above DDL table should work.
",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 13 12:02:08 UTC 2022,,,,,,,,,,"0|z19bu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 12:02;lzljs3620320;master: 5d7af498785b5a6be2a2bad43b6195da79e6d819
release-0.2: 482eb917d0496e50ca92797bdde04316c09d9aab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink deployment stuck in UPGRADING state when changing configuration,FLINK-29620,13486057,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,liadsh,liadsh,13/Oct/22 07:36,18/Oct/22 10:22,13/Jul/23 08:13,18/Oct/22 10:22,1.14.2,,,,,,,1.15.0,,,,,Kubernetes Operator,,,,,,,0,,,,,"When I update the configuration of a flink deployment I observe one of two scenarios:

Success:

This happens when the job has not started - if I change the configuration quick enough:
{code:java}
2022-10-13 06:50:54,336 o.a.f.k.o.r.d.AbstractJobReconciler [INFO ][load-streaming/validator-process-124] Upgrading/Restarting running job, suspending first...
2022-10-13 06:50:54,343 o.a.f.k.o.r.d.ApplicationReconciler [INFO ][load-streaming/validator-process-124] Job is not running but HA metadata is available for last state restore, ready for upgrade
2022-10-13 06:50:54,353 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Deleting JobManager deployment while preserving HA metadata.
2022-10-13 06:50:58,415 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (5s)
2022-10-13 06:51:03,451 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (10s)
2022-10-13 06:51:06,469 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Cluster shutdown completed.
2022-10-13 06:51:06,470 o.a.f.k.o.c.FlinkDeploymentController [INFO ][load-streaming/validator-process-124] End of reconciliation
2022-10-13 06:51:06,493 o.a.f.k.o.c.FlinkDeploymentController [INFO ][load-streaming/validator-process-124] Starting reconciliation
2022-10-13 06:51:06,494 o.a.f.k.o.c.FlinkConfigManager [INFO ][load-streaming/validator-process-124] Generating new config
 {code}
In this scenario I see that the job manager and task manager pods are terminated and then recreated.

 

 

Failure:

This happens when I let the job start (wait more than 30-60 seconds) and change the configuration:
{code:java}
2022-10-13 06:53:06,637 o.a.f.k.o.r.d.AbstractJobReconciler [INFO ][load-streaming/validator-process-124] Upgrading/Restarting running job, suspending first...
2022-10-13 06:53:06,637 o.a.f.k.o.r.d.AbstractJobReconciler [INFO ][load-streaming/validator-process-124] Job is in running state, ready for upgrade with SAVEPOINT
2022-10-13 06:53:06,659 o.a.f.k.o.s.FlinkService       [INFO ][load-streaming/validator-process-124] Suspending job with savepoint.
2022-10-13 06:53:07,042 o.a.f.k.o.s.FlinkService       [INFO ][load-streaming/validator-process-124] Job successfully suspended with savepoint s3://cu-flink-load-checkpoints-us-east-1/validator-process-124/savepoints/savepoint-000000-947975b509b2.
2022-10-13 06:53:11,111 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (5s)
2022-10-13 06:53:16,176 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (10s)
2022-10-13 06:53:21,238 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (15s)
2022-10-13 06:53:26,293 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (20s)
2022-10-13 06:53:31,355 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (25s)
2022-10-13 06:53:36,412 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (30s)
2022-10-13 06:53:41,512 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (35s)
2022-10-13 06:53:46,568 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (40s)
2022-10-13 06:53:51,625 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (45s)
2022-10-13 06:53:56,740 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (50s)
2022-10-13 06:54:01,811 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (55s)
2022-10-13 06:54:06,866 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Waiting for cluster shutdown... (60s)
2022-10-13 06:54:07,866 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Cluster shutdown completed.
2022-10-13 06:54:07,866 o.a.f.k.o.c.FlinkDeploymentController [INFO ][load-streaming/validator-process-124] End of reconciliation
2022-10-13 06:54:07,894 o.a.f.k.o.c.FlinkDeploymentController [INFO ][load-streaming/validator-process-124] Starting reconciliation
2022-10-13 06:54:07,894 o.a.f.k.o.o.d.ApplicationObserver [WARN ][load-streaming/validator-process-124] Running deployment generation 3 doesn't match upgrade target generation 4.
2022-10-13 06:54:07,895 o.a.f.k.o.r.d.AbstractFlinkResourceReconciler [INFO ][load-streaming/validator-process-124] Detected spec change, starting reconciliation.
2022-10-13 06:54:07,941 o.a.f.k.o.s.FlinkService       [INFO ][load-streaming/validator-process-124] Deploying application cluster
2022-10-13 06:54:07,947 o.a.f.k.o.u.FlinkUtils         [INFO ][load-streaming/validator-process-124] Job graph in ConfigMap validator-process-124-dispatcher-leader is deleted
2022-10-13 06:54:08,029 o.a.f.c.d.a.c.ApplicationClusterDeployer [INFO ][load-streaming/validator-process-124] Submitting application in 'Application Mode'.
2022-10-13 06:54:08,031 o.a.f.r.u.c.m.ProcessMemoryUtils [INFO ][load-streaming/validator-process-124] The derived from fraction jvm overhead memory (102.400mb (107374184 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
2022-10-13 06:54:08,087 o.a.f.k.o.r.ReconciliationUtils [WARN ][load-streaming/validator-process-124] Attempt count: 0, last attempt: false
2022-10-13 06:54:08,111 i.j.o.p.e.ReconciliationDispatcher [ERROR][load-streaming/validator-process-124] Error during event processing ExecutionScope{ resource id: ResourceID{name='validator-process-124', namespace='load-streaming'}, version: 1116792084} failed.
org.apache.flink.kubernetes.operator.exception.ReconciliationException: org.apache.flink.client.deployment.ClusterDeploymentException: The Flink cluster validator-process-124 already exists.
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:119)
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:54)
    at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:201)
    at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:153)
    at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:83)
    at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:152)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:135)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:115)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:86)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:59)
    at io.javaoperatorsdk.operator.processing.event.EventProcessor$ControllerExecution.run(EventProcessor.java:390)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.apache.flink.client.deployment.ClusterDeploymentException: The Flink cluster validator-process-124 already exists.
    at org.apache.flink.kubernetes.KubernetesClusterDescriptor.deployApplicationCluster(KubernetesClusterDescriptor.java:181)
    at org.apache.flink.client.deployment.application.cli.ApplicationClusterDeployer.run(ApplicationClusterDeployer.java:67)
    at org.apache.flink.kubernetes.operator.service.FlinkService.submitApplicationCluster(FlinkService.java:200)
    at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.deploy(ApplicationReconciler.java:155)
    at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.deploy(ApplicationReconciler.java:52)
    at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractJobReconciler.restoreJob(AbstractJobReconciler.java:188)
    at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractJobReconciler.reconcileSpecChange(AbstractJobReconciler.java:122)
    at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:145)
    at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:55)
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:115)
    ... 13 more {code}
In this scenario I see that the job manager pod is restarted (not recreated), task manager pods are not updated, flink config maps are not updated.

The flink deployment state changes to UPGRADING and the above exception is repeated.

error in flink deployment: org.apache.flink.client.deployment.ClusterDeploymentException: The Flink cluster validator-process-124 already exists.
Job Manager Deployment Status:  MISSING

 

Flink deployment spec:

 
{code:java}
flinkVersion: v1_14 
job:
    allowNonRestoredState: true
    args: ...
    entryClass: ...
    jarURI: ...
    parallelism: x
    savepointTriggerNonce: 0
    state: running
    upgradeMode: savepoint
jobManager:
    podTemplate:
      apiVersion: v1
      kind: Pod
      metadata:
        annotations:
          configmap.reloader.stakater.com/reload: flink-config-validator-process-124,pod-template-validator-process-124
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: nodeType
                  operator: In
                  values:
                  - someValue
        containers:
        - name: flink-main-container
          resources:
            limits:
              cpu: ""1""
              memory: 1.6Gi
            requests:
              cpu: ""0.2""
              memory: 1Gi
        tolerations:
        - effect: NoSchedule
          key: someValue
          value: ""true""
    replicas: 1
podTemplate:
    apiVersion: v1
    kind: Pod
    metadata:
      annotations:
        configmap.reloader.stakater.com/reload: flink-config-validator-process-124,pod-template-validator-process-124
        prometheus.io/path: /metrics
        prometheus.io/port: ""9260""
        prometheus.io/scrape: ""true""
      labels:
        app.kubernetes.io/instance: flink-validator-process-124
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: apache-flink
        app.kubernetes.io/version: test
        helm.sh/chart: apache-flink-1.0.0
    spec:
      containers: []
      imagePullSecrets: []
  serviceAccount: validator-process-124
  taskManager:
    podTemplate:
      apiVersion: v1
      kind: Pod
      metadata:
        annotations:
          configmap.reloader.stakater.com/reload: flink-config-validator-process-124,pod-template-validator-process-124
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: nodeType
                  operator: In
                  values:
                  - someValue
        containers:
        - name: flink-main-container
          resources:
            limits:
              cpu: ""1""
              memory: 3.6Gi
            requests:
              cpu: ""0.2""
              memory: 3Gi
        tolerations:
        - effect: NoSchedule
          key: someValue
          value: ""true""{code}
 

Please let me know if more details are required.

 ","AWS EKS v1.21

Operator version: 1.1.0",gyfora,liadsh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 18 10:22:11 UTC 2022,,,,,,,,,,"0|z19bsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 08:43;liadsh;might be related to FLINK-26345;;;","15/Oct/22 15:29;gyfora;I think that this might be fixed on 1.2.0 already, could you please verify?;;;","17/Oct/22 06:53;liadsh;i'll try it tomorrow, thanks :);;;","18/Oct/22 04:51;liadsh;not solved by 1.2.0, I will try to upgrade to flink 1.15 and see if this helps - https://github.com/apache/flink-kubernetes-operator/blob/c6ad96980278aa34864c33447e8c8aec3a6a5909/docs/content/docs/custom-resource/job-management.md#recovery-of-missing-job-deployments;;;","18/Oct/22 10:22;liadsh;solved in flink 1.15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove redundant MeterView updater thread from KubernetesClientMetrics,FLINK-29619,13486056,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,morhidi,morhidi,,13/Oct/22 07:25,24/Nov/22 01:02,13/Jul/23 08:13,14/Oct/22 11:29,kubernetes-operator-1.2.0,,,,,,,kubernetes-operator-1.3.0,,,,,,,,,,,,0,pull-request-available,,,,"The `MetricRegistryImpl` already has a solution to update `MeterView` objects periodically.

https://github.com/apache/flink/blob/7a509c46e45b9a91f2b7d01f13afcdef266b1faf/flink-runtime/src/main/java/org/apache/flink/runtime/metrics/MetricRegistryImpl.java#L404",,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 14 11:29:19 UTC 2022,,,,,,,,,,"0|z19bs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/22 11:29;gyfora;merged to main f1387f723da3f274bcb3268d401d91042064255d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARNSessionFIFOSecuredITCase.testDetachedMode timed out in Azure CI,FLINK-29618,13486054,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,mapohl,mapohl,13/Oct/22 07:14,22/May/23 07:35,13/Jul/23 08:13,22/May/23 07:35,1.17.0,,,,,,,,,,,,Deployment / YARN,Tests,,,,,,0,pull-request-available,starter,test-stability,,"We experienced a [build failure|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41931&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=30284] that was caused (exclusively) by {{YARNSessionFIFOSecuredITCase.testDetachedMode}} running into a timeout.

The test specific logs which were extracted from the build's are attached to this Jira issue.

JUnit tries to stop the thread running the test but fails to due so because it's interrupting a sleep. The {{InterruptedException}} is not properly handled in [YarnTestBase:744|https://github.com/apache/flink/blob/573ed922346c791760d27653543c2b8df56f51f7/flink-yarn-tests/src/test/java/org/apache/flink/yarn/YarnTestBase.java#L744] (it doesn't forward the exception). Therefore, we only see the warning being logged after 60s:
{code}
11:33:51,124 [ForkJoinPool-1-worker-25] WARN  org.apache.flink.yarn.YarnTestBase                           [] - Interruped
java.lang.InterruptedException: sleep interrupted
        at java.lang.Thread.sleep(Native Method) ~[?:1.8.0_292]
        at org.apache.flink.yarn.YarnTestBase.sleep(YarnTestBase.java:716) ~[test-classes/:?]
        at org.apache.flink.yarn.YarnTestBase.startWithArgs(YarnTestBase.java:906) ~[test-classes/:?]
        at org.apache.flink.yarn.YARNSessionFIFOITCase.runDetachedModeTest(YARNSessionFIFOITCase.java:141) ~[test-classes/:?]
        at org.apache.flink.yarn.YARNSessionFIFOSecuredITCase.lambda$testDetachedMode$2(YARNSessionFIFOSecuredITCase.java:173) ~[test-classes/:?]
        at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:288) ~[test-classes/:?]
        at org.apache.flink.yarn.YARNSessionFIFOSecuredITCase.testDetachedMode(YARNSessionFIFOSecuredITCase.java:160) ~[test-classes/:?]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
[...]
{code}

The test code itself eventually continues and succeeds (despite the interruption). The job submission takes suspiciously long, though.

Removing the timeout from the test (as this is the desired approach for tests in general now) should solve this test instability.
",,mapohl,Wencong Liu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/22 07:15;mapohl;build-20221012.7.YARNSessionFIFOSecuredITCase.testDetachedMode.log;https://issues.apache.org/jira/secure/attachment/13050873/build-20221012.7.YARNSessionFIFOSecuredITCase.testDetachedMode.log",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 22 07:35:45 UTC 2023,,,,,,,,,,"0|z19brs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/23 09:32;Wencong Liu;Hi [~mapohl] , do you think it's suitable to let CI judge whether it's timed out for this test? Currently it use 
{code:java}
@Timeout(value = 60) {code}
to judge the timed out status.;;;","11/May/23 07:18;mapohl;I guess, you're right. The test timed out after 60 seconds (which is where the log warning about the {{InterruptedException}} occurred). But the test itself continues because we're not forwarding the interrupt exception within the sleep call (see [YarnTestBase:744|https://github.com/apache/flink/blob/573ed922346c791760d27653543c2b8df56f51f7/flink-yarn-tests/src/test/java/org/apache/flink/yarn/YarnTestBase.java#L744]) and succeeds eventually.

Removing the timeout would have helped here to avoid test instabilities. Even though, it would have been interesting to investigate why the job submission takes that long. Unfortunately, the build artifacts are gone already.

I updated the ticket's description.;;;","11/May/23 10:09;Wencong Liu;Thanks for the reply [~mapohl] . I'd like to remove the annotation. 😊;;;","11/May/23 11:38;mapohl;Thanks for picking it up, [~Wencong Liu]. I assigned the ticket to you.;;;","22/May/23 07:35;mapohl;master: 5b637243f27063f3a2f5fbc46a9e0d87207ff37a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MetricStore does not remove metrics of nonexistent subtasks when adaptive scheduler lowers job parallelism,FLINK-29615,13486041,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,13/Oct/22 06:45,21/Oct/22 02:20,13/Jul/23 08:13,21/Oct/22 02:20,1.15.0,1.16.0,,,,,,1.16.1,1.17.0,,,,Runtime / Metrics,Runtime / REST,,,,,,0,pull-request-available,,,,"We are exploring autoscaling Flink with Reactive mode using metrics from Flink REST for guidance, and found that the metrics are not correctly updated.

 

*Problem*

MetricStore does not remove metrics of nonexistent subtasks when adaptive scheduler lowers job parallelism (aka, num of subtasks decreases) and users will see metrics of nonexistent subtasks on Web UI (e.g. the task backpressure page) or REST API response. It causes confusion and occupies extra memory.

 

*Proposed Solution*

Thanks to FLINK-29132 & FLINK-28588,  Flink will now update current execution attempts when updating metrics. Since the active subtask info is included in the current execution attempt info, we are able to retain active subtasks using the current execution attempt info.

 ",,mason6345,xtsong,Zhanghao Chen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 21 02:20:34 UTC 2022,,,,,,,,,,"0|z19bow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 10:18;Zhanghao Chen;[~chesnay] could you help take a look? I've prepared a quick fix for it.;;;","20/Oct/22 05:55;Zhanghao Chen;Hi, [~xtsong]. Could you help take a look at this? We encountered this issue when exploring autoscaling with Reactive mode using metrics from Flink REST for guidance, and would like to fix it.;;;","20/Oct/22 07:00;xtsong;Thanks for reporting and fixing this, [~Zhanghao Chen]. I'll take a look at the PR.;;;","21/Oct/22 02:20;xtsong;- master (1.17): f11c322467aa7e8a6d58703a72149152e8b58883
- release-1.16: 9f5ad7ea6ae04722d3934424f7f37cbfcbf2a7f5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong message size assertion in Pulsar's batch message,FLINK-29613,13486037,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,qiaomengnan,qiaomengnan,13/Oct/22 06:29,20/Oct/22 02:37,13/Jul/23 08:13,19/Oct/22 03:45,1.15.2,1.16.0,1.17.0,,,,,1.15.3,1.16.0,1.17.0,,,Connectors / Pulsar,,,,,,,0,pull-request-available,,,,"java.lang.RuntimeException: One or more fetchers have encountered exception
at nextMessageIdorg.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)
at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)
at org.apache.flink.connector.pulsar.source.reader.source.PulsarOrderedSourceReader.pollNext(PulsarOrderedSourceReader.java:109)
at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:385)
at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.RuntimeException: SplitFetcher thread 1 received unexpected exception while polling the records
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150)
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
... 1 more
Suppressed: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
... 7 more
Caused by: java.lang.IllegalArgumentException: We only support normal message id currently.
at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)
at org.apache.flink.connector.pulsar.source.enumerator.cursor.MessageIdUtils.unwrapMessageId(MessageIdUtils.java:61)
at org.apache.flink.connector.pulsar.source.enumerator.cursor.MessageIdUtils.nextMessageId(MessageIdUtils.java:43)
at org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.beforeCreatingConsumer(PulsarOrderedPartitionSplitReader.java:94)
at org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.handleSplitsChanges(PulsarPartitionSplitReaderBase.java:160)
at org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.handleSplitsChanges(PulsarOrderedPartitionSplitReader.java:52)
at org.apache.flink.connector.base.source.reader.fetcher.AddSplitsTask.run(AddSplitsTask.java:51)
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)
... 6 more
Caused by: java.lang.IllegalArgumentException: We only support normal message id currently.
at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)
at org.apache.flink.connector.pulsar.source.enumerator.cursor.MessageIdUtils.unwrapMessageId(MessageIdUtils.java:61)
at org.apache.flink.connector.pulsar.source.enumerator.cursor.MessageIdUtils.nextMessageId(MessageIdUtils.java:43)
at org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.beforeCreatingConsumer(PulsarOrderedPartitionSplitReader.java:94)
at org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.handleSplitsChanges(PulsarPartitionSplitReaderBase.java:160)
at org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.handleSplitsChanges(PulsarOrderedPartitionSplitReader.java:52)
at org.apache.flink.connector.base.source.reader.fetcher.AddSplitsTask.run(AddSplitsTask.java:51)
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)
... 6 more",,martijnvisser,qiaomengnan,syhily,tison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 19 03:45:21 UTC 2022,,,,,,,,,,"0|z19bo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 08:03;martijnvisser;I don't think this is a bug, like the error message says this is currently not supported so it would be a new feature request

Please explain how this can be reproduced. ;;;","13/Oct/22 11:56;syhily;[~martijnvisser] This should be a bug. Because we just accidentally use a wrong assertion for judging if this is a batch message. ;;;","13/Oct/22 12:18;martijnvisser;Thanks for clarifying [~syhily] - What would be a descriptive title for this ticket? ;;;","13/Oct/22 12:42;syhily;I think it should be ""Wrong message size assertion in Pulsar's batch message"";;;","19/Oct/22 01:20;tison;master via 8cb0297c2f092f42f989b4811aa438b6810aef8d
1.15 via f44ffe5a62ebb8992cfcbce0841a0062b6e3cb4d;;;","19/Oct/22 03:45;tison;1.16 via f625a3244ca2c704eda04bdf3b009ffac1735978;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix literal issue in HiveDialect,FLINK-29590,13485775,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,12/Oct/22 02:16,17/Oct/22 09:16,13/Jul/23 08:13,17/Oct/22 09:16,1.16.0,,,,,,,1.16.0,1.17.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,"in FLINK-26474, we try to fold constant, but it brings a issue that the folded constant like `Double.NAN` and no-primitive type  can't be convert into calcite literal in method  `HiveParserRexNodeConverter#convertConstant`.

For example, the following code will throw an exception ""org.apache.hadoop.hive.ql.parse.SemanticException: NaN"" in method `HiveParserRexNodeConverter#convertConstant`
{code:java}
// hive dialect
SELECT asin(2); {code}
To fix it, we need to figure out such case and then not to fold constant .

 

in FLINK-27017, we use Hive's `GenericUDFOPDivide` to do divide for better compatibility, but it bring a issue that when use a int/long literal as divisor, the result type passed and inferred type may not match.

The fix it, we need to make the result type match the inferred type.

 ",,jark,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 14 06:32:59 UTC 2022,,,,,,,,,,"0|z19a28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 11:27;jark;Fixed in 
 - master: dbf5dce61493cd0b7e73ae3af39146caf52c0767
 - release-1.16: 990eac8c69fc14d7dc18cc1032268619ac46edbc;;;","13/Oct/22 11:27;jark;[~luoyuxia] could you open a pull request for release-1.16 branch?;;;","14/Oct/22 01:28;luoyuxia;[~jark]  Sure, but we have to merge the pr [https://github.com/apache/flink/pull/21034]  for FLINK-29337  first since there's some conflicts between them.;;;","14/Oct/22 06:32;luoyuxia;[~jark] The pr. for release-1.16 is available in [https://github.com/apache/flink/pull/21061];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Serialized OperatorCoordinators are collected in a not thread-safe ArrayList even though serialization was parallelized in FLINK-26675,FLINK-29576,13485591,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mapohl,mapohl,mapohl,11/Oct/22 07:34,14/Oct/22 09:36,13/Jul/23 08:13,14/Oct/22 09:36,1.16.0,1.17.0,,,,,,1.16.0,1.17.0,,,,API / DataStream,Runtime / Coordination,,,,,,0,pull-request-available,test-stability,,,"There's a [build failure|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41843&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8523] being caused by {{SourceNAryInputChainingITCase.testDirectSourcesOnlyExecution}} on {{master}}:
{code}
Oct 11 01:45:36 [ERROR] Errors: 
Oct 11 01:45:36 [ERROR]   SourceNAryInputChainingITCase.testDirectSourcesOnlyExecution:89 » Runtime Fail...
Oct 11 01:45:36 [INFO] 
Oct 11 01:45:36 [ERROR] Tests run: 1931, Failures: 0, Errors: 1, Skipped: 4
{code}

The actual cause might be a missing {{OperatorCoordinatorHolder}} in {{DefaultOperatorCoordinatorHandler}} (see attached Maven logs that were extracted from the linked build):
{code}
01:44:28,248 [flink-akka.actor.default-dispatcher-5] WARN  org.apache.flink.runtime.taskmanager.Task                    [] - MultipleInputOperator [Source: source-1, Source: source-2, Source: source-3] (1/4)#0 (9babb402557eb959216c28116aabddbe_1dd2eb40b0971d6d849b9e4a69494c88_0_0) switched from RUNNING to FAILED with failure cause: org.apache.flink.util.FlinkException: No coordinator registered for operator bc764cd8ddf7a0cff126f51c16239658
        at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.deliverOperatorEventToCoordinator(DefaultOperatorCoordinatorHandler.java:117)
        at org.apache.flink.runtime.scheduler.SchedulerBase.deliverOperatorEventToCoordinator(SchedulerBase.java:1031)
        at org.apache.flink.runtime.jobmaster.JobMaster.sendOperatorEventToCoordinator(JobMaster.java:588)
        at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at akka.actor.Actor.aroundReceive(Actor.scala:537)
        at akka.actor.Actor.aroundReceive$(Actor.scala:535)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
        at akka.actor.ActorCell.invoke(ActorCell.scala:548)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
        at akka.dispatch.Mailbox.run(Mailbox.scala:231)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
{code}",,hxb,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26675,,,,,,,,,,,,,"11/Oct/22 07:45;mapohl;SourceNAryInputChainingITCase.testDirectSourcesOnlyExecution.log;https://issues.apache.org/jira/secure/attachment/13050282/SourceNAryInputChainingITCase.testDirectSourcesOnlyExecution.log",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 14 09:36:45 UTC 2022,,,,,,,,,,"0|z198y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 08:15;mapohl;The issue is reproducible locally by running the test repeatedly until failure.;;;","11/Oct/22 08:20;mapohl;Maybe unrelated, but one test run failed with a different stacktrace while initializing the {{JobMaster}}:
{code}
Caused by: java.lang.NullPointerException
	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.create(OperatorCoordinatorHolder.java:488) ~[classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.createOperatorCoordinatorHolder(ExecutionJobVertex.java:286) ~[classes/:?]
	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.initialize(ExecutionJobVertex.java:223) ~[classes/:?]
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.initializeJobVertex(DefaultExecutionGraph.java:901) ~[classes/:?]
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.initializeJobVertices(DefaultExecutionGraph.java:891) ~[classes/:?]
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.attachJobGraph(DefaultExecutionGraph.java:848) ~[classes/:?]
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.attachJobGraph(DefaultExecutionGraph.java:830) ~[classes/:?]
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraphBuilder.buildGraph(DefaultExecutionGraphBuilder.java:203) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:156) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:361) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:206) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:134) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:152) ~[classes/:?]
	at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:119) ~[classes/:?]
	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:369) ~[classes/:?]
	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:346) ~[classes/:?]
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:123) ~[classes/:?]
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:95) ~[classes/:?]
	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112) ~[classes/:?]
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604) ~[?:1.8.0_345]
	... 3 more
{code};;;","11/Oct/22 08:24;mapohl;I'm increasing the issue's priority to CRITICAL because it seems to be coordination-related and might affect other components. I added {{1.16.0}} because I haven't seen any evidence, yet, that this is a recently added problem.;;;","11/Oct/22 08:55;mapohl;Adding additional log output reveals that the Operator ID in question, indeed, is not registered with the {{DefaultOperatorCoordinatorHandler}}:
{code}
38013 [jobmanager-io-thread-1] DEBUG org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler [] - OperatorIds for ExecutionJobVertex[initialized=true] MultipleInputOperator [Source: source-1, Source: source-2, Source: source-3]: [bc764cd8ddf7a0cff126f51c16239658, 605b35e407e90cda15ad084365733fdd]
38013 [jobmanager-io-thread-1] DEBUG org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler [] - OperatorIds for ExecutionJobVertex[initialized=true] Sink: Data stream collect sink [Source: source-1, Source: source-2, Source: source-3]: [ad41044d493a326176171462a740339f]
38013 [jobmanager-io-thread-1] DEBUG org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler [] - Registered Operators: [bc764cd8ddf7a0cff126f51c16239658, 605b35e407e90cda15ad084365733fdd, ad41044d493a326176171462a740339f]
{code}
...but the run fails with not finding the {{OperatorHandler}} for a completely different ID {{feca28aff5a3958840bee985ee7de4d3}}:
{code}
38096 [flink-akka.actor.default-dispatcher-10] WARN  org.apache.flink.runtime.taskmanager.Task [] - MultipleInputOperator [Source: source-1, Source: source-2, Source: source-3] (2/4)#0 (dfa76d436eae78327927f1d9bf6db514_1dd2eb40b0971d6d849b9e4a69494c88_1_0) switched from RUNNING to FAILED with failure cause: org.apache.flink.util.FlinkException: No coordinator registered for operator feca28aff5a3958840bee985ee7de4d3
	at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.deliverOperatorEventToCoordinator(DefaultOperatorCoordinatorHandler.java:134)
	at org.apache.flink.runtime.scheduler.SchedulerBase.deliverOperatorEventToCoordinator(SchedulerBase.java:1031)
	at org.apache.flink.runtime.jobmaster.JobMaster.sendOperatorEventToCoordinator(JobMaster.java:588)
[...]
{code};;;","11/Oct/22 10:28;mapohl;I finally found the cause of this issue: FLINK-26675 introduces parallel serialization for the operators. These serialized operators are added to the JobVertex's operatorCoordinators which is a basic {{ArrayList}} and, therefore, not thread-safe. Concurrently adding the serialized OperatorCoordinators might result in dropping some of them occassionally.

As a consequence, I'm marking this one as a blocker for 1.16.;;;","11/Oct/22 10:32;mapohl;FYI: [~guoyangze];;;","14/Oct/22 09:36;mapohl;master: dc83e69dbabdc53e89bdc787a93f1fe904211cee
1.16: b05fac1560add86dcb41b0b5a8297326dbc99a19;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revert sink output metric names from numRecordsSend back to numRecordsOut,FLINK-29567,13485429,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,renqs,renqs,renqs,10/Oct/22 12:26,03/Nov/22 02:24,13/Jul/23 08:13,20/Oct/22 01:12,1.15.3,1.16.0,,,,,,1.15.3,1.16.0,,,,Connectors / Common,,,,,,,0,pull-request-available,,,,"As discussed in [the mailing list|https://lists.apache.org/thread/vxhty3q97s7pw2zn0jhkyd6sxwwodzbv], all{color:#333333} sink metrics with name “numXXXOut” defined in FLIP-33 are replace by “numXXXSend” in FLINK-26126 and FLINK-26492. {color}

{color:#333333}Considering metric names are public APIs, this is a breaking change to end users and not backward compatible. We need to revert these metric names back. {color}",,godfreyhe,hxb,hxbks2ks,liyu,luoyuxia,mason6345,renqs,syhily,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 02 09:13:06 UTC 2022,,,,,,,,,,"0|z197yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 01:12;renqs;master: d0e855090683920d57922acbddb64c9a99dceccd

release-1.16: 6e743963cc689931acdab68f3559884d5d48fcc4

release-1.15: 221e9f7c5e59265b7448eba70aea393b60a1fe42;;;","01/Nov/22 02:57;hxbks2ks; Hi [~renqs], the fixed PR hasn't been backported to release-1.15?
;;;","02/Nov/22 09:13;renqs;Ah thanks for the reminder [~hxb] ! I created the backport PR just now: https://github.com/apache/flink/pull/21220;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JM/SQl gateway OpenAPI specs should have different titles,FLINK-29562,13485407,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,10/Oct/22 10:24,20/Oct/22 02:33,13/Jul/23 08:13,10/Oct/22 13:36,,,,,,,,1.16.0,1.17.0,,,,Documentation,Runtime / REST,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 10 13:36:05 UTC 2022,,,,,,,,,,"0|z197ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/22 13:36;chesnay;master: 1c2fd3584b899412f6a94303ecdd29513664e9dc
1.16: ae5c7046f11504d5399f11e5be1762cf378e3c30;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Use select count(*) from xxx; and get SQL syntax",FLINK-29558,13485384,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,StarBoy1005,StarBoy1005,10/Oct/22 07:37,30/Jan/23 08:12,13/Jul/23 08:13,30/Jan/23 08:12,1.15.3,,,,,,,1.17.0,,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"Hi, I use flink sql to make kafka records to mysql.
so I create these 2 tables in flink sql,here is the mysql ,and I created the table in mysql before I did the insert action in flink sql.

  CREATE TABLE mysql_MyUserTable (
  id STRING,
  name STRING,
  age STRING,
  status STRING,
  PRIMARY KEY (id) NOT ENFORCED
) WITH (
   'connector' = 'jdbc',
   'url' = 'jdbc:mysql://10.19.29.170:3306/fromflink152',
   'table-name' = 'users',
   'username' = 'root',
   'password' = '******'
);

In mysql, I created database ""fromflink152"" then created the table like this way

 CREATE TABLE `users` (
  `id` varchar(64) NOT NULL DEFAULT '',
  `name` varchar(255) DEFAULT NULL,
  `age` varchar(255) DEFAULT NULL,
  `status` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`)
)  

After executed insert sql,I found 'select * from mysql_MyUserTable' can get correct result,but ’select count(\*) from mysql_MyUserTable‘  or ’select count(id) from mysql_MyUserTable‘ ,the collect job in flink app keep restarting again and again.The exception is:
 !image-2022-10-10-15-31-34-341.png! 

So I wonder which config that I missed about the table in flink or mysql side :(

","flink 1.15.2
CentOS Linux release 7.9.2009 (Core)
5.7.32-log MySQL Community Server (GPL)",jiabao.sun,lincoln.86xy,martijnvisser,StarBoy1005,tanjialiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/22 07:32;StarBoy1005;image-2022-10-10-15-31-34-341.png;https://issues.apache.org/jira/secure/attachment/13050230/image-2022-10-10-15-31-34-341.png","19/Oct/22 09:55;jiabao.sun;image-2022-10-19-17-55-14-700.png;https://issues.apache.org/jira/secure/attachment/13051173/image-2022-10-19-17-55-14-700.png","03/Nov/22 10:17;StarBoy1005;image-2022-11-03-18-16-12-127.png;https://issues.apache.org/jira/secure/attachment/13051744/image-2022-11-03-18-16-12-127.png","14/Oct/22 03:25;StarBoy1005;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13050925/screenshot-1.png","20/Oct/22 02:16;StarBoy1005;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13051215/screenshot-2.png",,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 08:12:48 UTC 2023,,,,,,,,,,"0|z197oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 13:06;martijnvisser;[~StarBoy1005] You should change your password as this was now shared publicly. 

The error that's being reported is coming from MySQL: have you verified that this syntax is compatible with your MySQL server? ;;;","14/Oct/22 02:57;StarBoy1005;[~martijnvisser] Thanks for the security notice.What's more, if I used flink sql to connect hudi or hive table(there do exist the table and some record in hudi or hive sides),execute sql like 'select  count(\*) from table1;' could get the correct result, but when do the save query  to mysql or Tidb(both connect by jdbc), the exception always show up,and I connected mysql by mysql client to check the table with same sql execution, 'select  count(\*) from table1;'  result is fine and correct.
If you never meet this situation,I guess the problem have some relation with mysql-connector-java**.jar in flink/lib 
;;;","14/Oct/22 03:25;StarBoy1005;It seems not work when I exchange mysql-connector-java-**.jar    , neither mysql-connector-java-5.1.49.jar    nor   mysql-connector-java-8.0.29.jar .

 !screenshot-1.png! ;;;","18/Oct/22 07:34;martijnvisser;Hive (and I assume Hudi, that I don't know for sure) use the Hive dialect for SQL. This can be different from the other JDBC dialects. ;;;","19/Oct/22 10:01;jiabao.sun;Hi [~martijnvisser].

I did some tests to reproduce the problem. Maybe there is some problem when extracting the columns of the COUNT function.

I have tried SUM and MAX function and the column can be extracted correctly.

!image-2022-10-19-17-55-14-700.png!;;;","19/Oct/22 14:08;martijnvisser;[~RocMarshal] Any thoughts on this?;;;","20/Oct/22 07:44;StarBoy1005;[~jiabao.sun] Hi,if column like 'id',and 'id' is the primary key ,then use select count(id) from xxx; can cause the problem.I guess some logic is not suit with JDBC situation.Maybe in StreamPhysicalRel  or something else;;;","03/Nov/22 09:40;StarBoy1005;I tried flink 1.16.0 with the same env,now in sql-client can swiftly get the exception：""[ERROR] Could not execute SQL statement. Reason:
java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'FROM `users1013a`' at line 1""  ,then the collect job just failed and never retry 

 !image-2022-11-03-18-16-12-127.png! ;;;","14/Jan/23 05:48;tanjialiang;[~StarBoy1005] It had been fixed in FLINK-27268, but i found a problem, when using flink sql like this ""SELECT COUNT( * ) FROM table"" or ""SELECT COUNT(1) FROM table"", projection always return 'ROW<> NOT NULL', so it cause mysql connector transform to ""SELECT FROM table"". After FLINK-27268, it transform to ""SELECT '' FROM table"". But is it projection's bug? When projection can not match something, it will be return 'ROW<> NOT NULL'. I think maybe not only mysql connector has this bug. cc [~martijnvisser] [~jark] ;;;","17/Jan/23 09:37;martijnvisser;[~lincoln.86xy] Is this a ticket that fits for you? If so, could you have a look?;;;","17/Jan/23 13:19;lincoln.86xy;[~martijnvisser] I looked at the problem and it is indeed a planner bug, I'll fix it.;;;","18/Jan/23 01:17;lincoln.86xy;[~godfreyhe] Could you also take a look at this when you have time? There're several existing cases already tested the case  which select nothing from source, e.g., ""SELECT COUNT(1) FROM T"" in 
 `org.apache.flink.table.planner.plan.stream.sql.TableSourceTest#testProjectWithoutInputRef`
and the expected plan is:
{code}
  <TestCase name=""testProjectWithoutInputRef"">
    <Resource name=""sql"">
      <![CDATA[SELECT COUNT(1) FROM T]]>
    </Resource>
    <Resource name=""ast"">
      <![CDATA[
LogicalAggregate(group=[{}], EXPR$0=[COUNT()])
+- LogicalProject($f0=[0])
   +- LogicalTableScan(table=[[default_catalog, default_database, T]])
]]>
    </Resource>
    <Resource name=""optimized exec plan"">
      <![CDATA[
GroupAggregate(select=[COUNT(*) AS EXPR$0])
+- Exchange(distribution=[single])
   +- TableSourceScan(table=[[default_catalog, default_database, T, project=[], metadata=[]]], fields=[])
]]>
    </Resource>
  </TestCase>
{code}

so is there any contract that connectors should follow in such cases？;;;","29/Jan/23 05:52;lincoln.86xy;After discussed with [~godfreyhe], the problem is confirmed. I've updated the pr to move on.;;;","30/Jan/23 08:12;lincoln.86xy;fixed in master: 6d17ba8855683fb6c1f8291ff4bad1c9abc01cb9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The SinkOperator's OutputFormat function is not recognized,FLINK-29557,13485363,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aitozi,aitozi,aitozi,10/Oct/22 04:21,20/Nov/22 10:02,13/Jul/23 08:13,20/Nov/22 10:02,,,,,,,,1.17.0,,,,,API / Core,Table SQL / API,,,,,,0,pull-request-available,,,,"In the {{SimpleOperatorFactory#of}}, only {{StreamSink}} is handled to register as {{SimpleOutputFormatOperatorFactory}}. So it will lost the output format information in  {{SinkOperator}}. Then some hook functions like {{FinalizeOnMaster}} will have no chance to be executed.
Due to the {{SinkOperator}} is in the table module, it can't be reached directly in the {{flink-streaming-java}}. So maybe we need introduce an extra common class eg: {{SinkFunctionOperator}} to describe the {{Sink}} operator and handle it individually.",,aitozi,gaoyunhaii,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Nov 20 10:02:19 UTC 2022,,,,,,,,,,"0|z197k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/22 02:20;aitozi;Anyone can help confirm this issue ? ;;;","14/Oct/22 09:31;gaoyunhaii;Hi [~aitozi] may I have a double confirmation on the scenario of this issue? Namely why we could not directly use OutputFormatSinkFunction so that it could be registered as SimpleOutputFormatOperatorFactory? ;;;","15/Oct/22 11:16;aitozi;Thanks [~gaoyunhaii] for your reply. The {{OutputFormatProvider}} in the table module will generate the {{SinkOperator}}. And the {{SinkOperator}} with {{OutputFormatSinkFunction}} will not be treated as an operator with {{OutputFormat}} in the {{SimpleOperatorFactory}}. In the end, it will miss the chance to execute the hook method eg: {{InitializeOnMaster}};;;","21/Oct/22 04:17;aitozi;ping [~gaoyunhaii] ;;;","25/Oct/22 07:10;gaoyunhaii;Thanks [~aitozi] for reporting the issue! Now I have understood why there is issue here, I'll have a double confirmation of the proper way to fix it here, and do you already have some thoughts on it?;;;","25/Oct/22 08:39;aitozi;I make a solution to add a common interface {{SinkFunctionOperator}} in the {{flink-streaming-java}} module and let the {{SinkOperator}} and {{StreamSink}} extends from it. So that we can check for the {{SinkFunctionOperator#sinkFunction}} in the {{SimpleOperatorFactory}} directly. what do you think [~gaoyunhaii] ?;;;","27/Oct/22 05:47;gaoyunhaii;Hi [~aitozi] LGTM, very thanks for tracking the issue. Would you like to open a PR to fix the issue? Both side is ok to me. ;;;","27/Oct/22 09:38;aitozi;Yes, I will open a PR to fix this. Please help assign the ticket;;;","20/Nov/22 10:02;gaoyunhaii;Merged on master via a56af3b5702f805b477f8cf11a27167e626ad9fe.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dnsPolicy in FlinkPod is not overridable ,FLINK-29539,13485210,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,carloscastro,carloscastro,carloscastro,07/Oct/22 22:44,28/Oct/22 09:59,13/Jul/23 08:13,28/Oct/22 09:58,,,,,,,,1.15.3,1.16.1,1.17.0,,,Deployment / Kubernetes,,,,,,,0,pull-request-available,,,,"With this PR [https://github.com/apache/flink/pull/18119 |https://github.com/apache/flink/pull/18119]it stopped being possible to override the dnsPolicy in the FlinkPod spec.

To fix it, it should check first if the dnsPolicy is not null before applying the default.",,carloscastro,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 28 09:59:12 UTC 2022,,,,,,,,,,"0|z196mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/22 09:58;wangyang0918;Fixed via:

master: bb9f2525e6e16d00ef2f0739d9cb96c2e47e35e7

release-1.16: 6325adf40a1baa8c3ac82aa06c57425c3c6005c4

release-1.15: d9413d6bc6548ddd4c2e4d6e05db0903da064476;;;","28/Oct/22 09:59;wangyang0918;Thanks [~carloscastro] for your contribution.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Java doc mistake in SequenceNumberRange#contains(),FLINK-29526,13484685,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,06/Oct/22 02:50,19/Oct/22 11:48,13/Jul/23 08:13,19/Oct/22 11:48,,,,,,,,1.17.0,,,,,Runtime / State Backends,,,,,,,0,pull-request-available,,,,"!image-2022-10-06-10-50-16-927.png|width=554,height=106!

Hi [~masteryhx] , It seems a typo, I have submit a pr for it.",,Feifan Wang,Yanfei Lei,ym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/22 02:50;Feifan Wang;image-2022-10-06-10-50-16-927.png;https://issues.apache.org/jira/secure/attachment/13050116/image-2022-10-06-10-50-16-927.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 19 11:48:06 UTC 2022,,,,,,,,,,"0|z193e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Oct/22 03:06;Yanfei Lei;Thanks for reporting this, you're right. From the implementation in {{{}GenericSequenceNumberRange#contains{}}}, the range should be left-bounded.;;;","19/Oct/22 11:48;ym;merged commit c84c9d7 into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Align SubtaskCommittableManager checkpointId with CheckpointCommittableManagerImpl checkpointId during recovery,FLINK-29512,13484635,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fpaul,fpaul,fpaul,05/Oct/22 15:49,20/Oct/22 02:36,13/Jul/23 08:13,14/Oct/22 13:40,1.15.1,1.16.0,1.17.0,,,,,1.15.3,1.16.0,1.17.0,,,Connectors / Common,,,,,,,0,pull-request-available,,,,"Similar to the issue described in https://issues.apache.org/jira/browse/FLINK-29509 during the recovery of committables, the subtaskCommittables checkpointId is set to always 1 [https://github.com/apache/flink/blob/9152af41c2d401e5eacddee1bb10d1b6bea6c61a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/sink/committables/CommittableCollectorSerializer.java#L193] while the holding CheckpointCommittableManager is initialized with the checkpointId that is written into state [https://github.com/apache/flink/blob/9152af41c2d401e5eacddee1bb10d1b6bea6c61a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/sink/committables/CommittableCollectorSerializer.java#L155 .|https://github.com/apache/flink/blob/9152af41c2d401e5eacddee1bb10d1b6bea6c61a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/sink/committables/CommittableCollectorSerializer.java#L155.]

 

This leads to that during a recovery, the post-commit topology will receive a committable summary with the recovered checkpoint id and multiple `CommittableWithLinage`s with the reset checkpointId causing orphaned `CommittableWithLinages` without a `CommittableSummary` failing the job.",,fpaul,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29459,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 14 13:39:41 UTC 2022,,,,,,,,,,"0|z19334:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/22 13:39;fpaul;Merged into: 

 

master: d03c334b7ac954e70877a5bdb7b50cf54b50624c

release-1.16: 1bef1aeb6abd4edda0e178a6a5da7f6dc6c3b074

release-1.15: 126ae4df9ef8bab98a53433ef39c698cf8f04c60;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set correct subtaskId during recovery of committables,FLINK-29509,13484603,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,KristoffSC,fpaul,fpaul,05/Oct/22 12:29,20/Oct/22 02:36,13/Jul/23 08:13,13/Oct/22 06:58,1.15.2,1.16.0,1.17.0,,,,,1.15.3,1.16.0,1.17.0,,,Connectors / Common,,,,,,,0,pull-request-available,,,,"When we recover the `CheckpointCommittableManager` we ignore the subtaskId it is recovered on. [https://github.com/apache/flink/blob/d191bda7e63a2c12416cba56090e5cd75426079b/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/sink/committables/CheckpointCommittableManagerImpl.java#L58]

This becomes a problem when a sink uses a post-commit topology because multiple committer operators might forward committable summaries coming from the same subtaskId.

 

It should be possible to use the subtaskId already present in the `CommittableCollector` when creating the `CheckpointCommittableManager`s.",,fpaul,KristoffSC,martijnvisser,Tagar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29459,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 13 06:58:29 UTC 2022,,,,,,,,,,"0|z192w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Oct/22 13:21;KristoffSC;Hi,
I would like to work on this thicket.

Can someone assign it to me? It seems I can't do that.;;;","07/Oct/22 06:20;KristoffSC;PR ready for review :)
[https://github.com/apache/flink/pull/20979];;;","13/Oct/22 06:58;fpaul;Merge into

master: 7a509c46e45b9a91f2b7d01f13afcdef266b1faf

release-1.16: d51389dc33af21038c982b733b86af8bbb736d19

release-1.15: 6b4882791dd0fd1b0df952ed7712ae7bd68adf36;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ParquetInputFormatFactory fails to create format on Flink 1.14,FLINK-29506,13484545,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,05/Oct/22 07:19,08/Oct/22 02:38,13/Jul/23 08:13,08/Oct/22 02:38,table-store-0.3.0,,,,,,,table-store-0.3.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"The current way to instantiate format has issues. See
[https://github.com/apache/flink-table-store/blob/master/flink-table-store-format/src/main/java/org/apache/flink/table/store/format/parquet/ParquetInputFormatFactory.java#L36]

ParquetColumnarRowInputFormat#createPartitionedFormat only differs in arguments for Flink 1.14 and Flink 1.15. It'll direct throw IllegalArgumentException when using Flink1.14.

!image-2022-10-05-15-19-25-641.png|width=617,height=375!

 

!image-2022-10-05-15-20-19-422.png|width=617,height=390!",,lzljs3620320,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27207,FLINK-29149,,"05/Oct/22 07:19;qingyue;image-2022-10-05-15-19-25-641.png;https://issues.apache.org/jira/secure/attachment/13050089/image-2022-10-05-15-19-25-641.png","05/Oct/22 07:20;qingyue;image-2022-10-05-15-20-19-422.png;https://issues.apache.org/jira/secure/attachment/13050090/image-2022-10-05-15-20-19-422.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 08 02:38:55 UTC 2022,,,,,,,,,,"0|z192j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Oct/22 02:38;lzljs3620320;master: 09827774e2f435de3133ced33c61dcf1e6ceae0a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InitializeOnMaster uses wrong parallelism with AdaptiveScheduler,FLINK-29500,13484417,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,04/Oct/22 11:42,20/Oct/22 02:32,13/Jul/23 08:13,06/Oct/22 15:10,1.14.6,1.15.2,1.16.0,,,,,1.15.3,1.16.0,1.17.0,,,API / Core,Runtime / Coordination,,,,,,0,pull-request-available,,,,"{{InputOutputFormatVertex}} uses {{JobVertex#getParallelism}} to invoke {{InitializeOnMaster#initializeGlobal}}. However, this parallelism might not be the actual one which will be used to execute the node in combination with Adaptive Scheduler. In case of Adaptive Scheduler the execution parallelism is provided via {{VertexParallelismStore}}.",,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 06 15:10:10 UTC 2022,,,,,,,,,,"0|z191qo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Oct/22 15:10;dwysakowicz;Fixed in:
* master
** cadfab59a9a5f6eaaaae7f95e2a9fb3ad7b1b1d9
* 1.16
** 6c6f152e9f3ac10e8d6993e1d9ab8076053e2a44
* 1.15
** 74bc6d2f776baad35b2f7f5be41cdfe87759192f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Async I/O Retry Strategies Do Not Work for Scala AsyncDataStream API,FLINK-29498,13484317,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,eric.xiao,eric.xiao,eric.xiao,03/Oct/22 19:37,17/Nov/22 16:08,13/Jul/23 08:13,17/Nov/22 09:54,1.15.3,,,,,,,1.17.0,,,,,API / Scala,,,,,,,0,pull-request-available,,,,"We are using the async I/O to make HTTP calls and one of the features we wanted to leverage was the retries, so we pulled the newest commit: [http://github.com/apache/flink/pull/19983] into our internal Flink fork.

When I try calling the function {{AsyncDataStream.unorderedWaitWithRetry}} from the scala API I with a retry strategy from the java API I get an error as {{unorderedWaitWithRetry}} expects a scala retry strategy. The problem is that retry strategies were only implemented in java and not Scala in this PR: [http://github.com/apache/flink/pull/19983].

 

Here is some of the code to reproduce the error:
{code:java}
import org.apache.flink.streaming.api.scala.AsyncDataStream
import org.apache.flink.streaming.util.retryable.{AsyncRetryStrategies => JAsyncRetryStrategies}

val javaAsyncRetryStrategy = new JAsyncRetryStrategies.FixedDelayRetryStrategyBuilder[Int](3, 100L)
    .build()

val data = AsyncDataStream.unorderedWaitWithRetry(
  source,
  asyncOperator,
  pipelineTimeoutInMs,
  TimeUnit.MILLISECONDS,
  javaAsyncRetryStrategy
){code}",,eric.xiao,gaoyunhaii,lincoln.86xy,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 17 09:54:24 UTC 2022,,,,,,,,,,"0|z1914g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Oct/22 06:08;lincoln.86xy;[~eric.xiao] There's a separate `org.apache.flink.streaming.api.scala.async.AsyncRetryStrategy` for scala api usage, the java utilities can not be used directly in scala (by design). For your case, you can try to implement your own `AsyncRetryStrategy` to enable retries.;;;","04/Oct/22 14:01;eric.xiao;> For your case, you can try to implement your own `AsyncRetryStrategy` to enable retries.

Thanks [~lincoln.86xy] for replying :), this is what we have done so far, thankfully the code is not a lot but we were wondering if there was a reason why the `AsyncRetryStrategies` are only available in Java API and not the Scala API? Similar for the `RetryPredicates`?

 

I am fairly new to Flink, but I believe I have seem some `.toJava` and `.toScala` helper methods on other Flink components and was wondering if there is room to add the same such functionality to the retry strategies builder and retry predicates?;;;","09/Oct/22 07:26;lincoln.86xy;[~eric.xiao] the retry feature is implemented under the interface-level equivalence between the java and scala api but not included utilities.
Recently a discussion ""Deprecate and remove Scala API support""(https://lists.apache.org/thread/d3borhdzj496nnggohq42fyb6zkwob3h) was raised on the ML,
before a conclusion is reached on it, I think we can move forward with improvements that are valuable to users, [~gaoyunhaii] WDYT?;;;","17/Oct/22 02:25;eric.xiao;As discussed in the [slack thread|https://apache-flink.slack.com/archives/C03G7LJTS2G/p1663957561957419] I am eager to contribute the Scala util classes :). Should we assign the Jira ticket to me (Not sure how to do that)?;;;","19/Oct/22 08:37;gaoyunhaii;Thanks [~eric.xiao] for the efforts! I also think we could first add the improvement since the Scala API would still be exists for several versions after deprecated. I have assigned the issue to you. ;;;","20/Oct/22 19:30;eric.xiao;Hi [~gaoyunhaii], I believe I have a [PR|https://github.com/apache/flink/pull/21077] that is in a reviewable state, what is the process of getting folks from the community to review the PR?;;;","21/Oct/22 03:00;gaoyunhaii;[~eric.xiao] Thanks for the PR! I'll have a look;;;","17/Nov/22 09:54;gaoyunhaii;Merged on master via d426489c9e5c634e2eec8fde6c71356700b7d4b2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PulsarSinkE2ECase hang,FLINK-29495,13484256,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,hxb,hxb,03/Oct/22 13:55,08/Nov/22 06:56,13/Jul/23 08:13,08/Nov/22 06:56,1.15.2,1.16.0,1.17.0,,,,,1.15.3,1.16.0,1.17.0,,,Connectors / Pulsar,,,,,,,0,pull-request-available,test-stability,,,"{code:java}
2022-10-02T05:53:56.0611489Z ""main"" #1 prio=5 os_prio=0 cpu=5171.60ms elapsed=9072.82s tid=0x00007f9508028000 nid=0x54ef1 waiting on condition  [0x00007f950f994000]
2022-10-02T05:53:56.0612041Z    java.lang.Thread.State: TIMED_WAITING (parking)
2022-10-02T05:53:56.0612475Z 	at jdk.internal.misc.Unsafe.park(java.base@11.0.16.1/Native Method)
2022-10-02T05:53:56.0613302Z 	- parking to wait for  <0x0000000087d261f8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2022-10-02T05:53:56.0613959Z 	at java.util.concurrent.locks.LockSupport.parkNanos(java.base@11.0.16.1/LockSupport.java:234)
2022-10-02T05:53:56.0614661Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(java.base@11.0.16.1/AbstractQueuedSynchronizer.java:2123)
2022-10-02T05:53:56.0615428Z 	at org.apache.pulsar.common.util.collections.GrowableArrayBlockingQueue.poll(GrowableArrayBlockingQueue.java:203)
2022-10-02T05:53:56.0616165Z 	at org.apache.pulsar.client.impl.MultiTopicsConsumerImpl.internalReceive(MultiTopicsConsumerImpl.java:370)
2022-10-02T05:53:56.0616807Z 	at org.apache.pulsar.client.impl.ConsumerBase.receive(ConsumerBase.java:198)
2022-10-02T05:53:56.0617486Z 	at org.apache.flink.connector.pulsar.testutils.sink.PulsarPartitionDataReader.poll(PulsarPartitionDataReader.java:72) {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41526&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=5846934b-7a4f-545b-e5b0-eb4d8bda32e1",,hxb,martijnvisser,syhily,tison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24302,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 06:56:20 UTC 2022,,,,,,,,,,"0|z190r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Oct/22 13:56;hxb;cc [~syhily]  Could you help take a look? Thx.;;;","03/Oct/22 13:57;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41525&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=5846934b-7a4f-545b-e5b0-eb4d8bda32e1&l=21963;;;","03/Oct/22 14:01;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41527&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=5846934b-7a4f-545b-e5b0-eb4d8bda32e1;;;","04/Oct/22 08:44;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41551&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=5846934b-7a4f-545b-e5b0-eb4d8bda32e1&l=21573;;;","04/Oct/22 08:46;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41552&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=5846934b-7a4f-545b-e5b0-eb4d8bda32e1;;;","07/Oct/22 18:02;syhily;I'll check it latter today.;;;","08/Oct/22 06:13;syhily;[~hxb] PulsarSinkE2ECase should be failed on Java 11. Since this class has been marked with 
{code:java}
@Category(value = {FailsOnJava11.class})
{code}
 I don't know why it would be executed on {{e2e_2_cron_jdk11}}?

The real cause is
{code}
Could not complete the operation. Number of retries has been exhausted. Failed reason: java.lang.OutOfMemoryError: Direct buffer memory
{code}

We are still waiting for Pulsar 2.11.0 for fixing this memory issue.;;;","10/Oct/22 13:40;martijnvisser;The Pulsar test annotation still relies on JUnit4, which is why they still get run on Java 11. I've created https://github.com/apache/flink/pull/21005 to disable them with the correct JUnit5 annotation, pending a successful review;;;","10/Oct/22 19:32;martijnvisser;Test disabled in master via be32eddd3a775cc336412b1cca14ecfa522320ec;;;","10/Oct/22 22:25;syhily;[~martijnvisser] Tks, but how could we know your modification works without running this on nightly build?;;;","11/Oct/22 07:20;martijnvisser;Downgraded from Critical to Major due to disabling the test

[~syhily] You can run {{mvn verify -Prun-end-to-end-tests}} on the {{flink-end-to-end-tests-pulsar}} with JDK11. When doing that, you can see that after the modification the tests are skipped. ;;;","11/Oct/22 08:50;tison;I notice this ticket is tagged to affect 1.15.2 and 1.16.0, shall we backport the fix?;;;","11/Oct/22 09:32;syhily;Yep. We should backport to both 1.15 and 1.16.;;;","11/Oct/22 18:01;martijnvisser;Tests alsof disabled for/via:

release-1.16: 203f36bf2165f3ebfcb1ec3ad36c35af44996015
release-1.15: aa78e3fc7af80d1f465a1747a50372b28c6cb302;;;","02/Nov/22 08:15;martijnvisser;[~syhily] This test is now also hanging for a JDK8 run: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42725&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=17472;;;","02/Nov/22 08:53;syhily;[~martijnvisser] I have checked the CI logs. It's wired and shows a NP exception. I think we can create a new one and close this one?

{code}
org.apache.pulsar.client.api.PulsarClientException: java.util.concurrent.CompletionException: java.lang.NullPointerException
   at org.apache.pulsar.client.impl.ConnectionHandler.handleConnectionError(ConnectionHandler.java:89) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.uniExceptionallyStage(CompletableFuture.java:898) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.exceptionally(CompletableFuture.java:2209) ~[?:1.8.0_342]
   at org.apache.pulsar.client.impl.ConnectionHandler.grabCnx(ConnectionHandler.java:74) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.TransactionMetaStoreHandler.<init>(TransactionMetaStoreHandler.java:105) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.lambda$startAsync$0(TransactionCoordinatorClientImpl.java:89) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:995) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2137) ~[?:1.8.0_342]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.startAsync(TransactionCoordinatorClientImpl.java:78) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.start(TransactionCoordinatorClientImpl.java:68) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.PulsarClientImpl.<init>(PulsarClientImpl.java:204) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.PulsarClientImpl.<init>(PulsarClientImpl.java:140) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.ClientBuilderImpl.build(ClientBuilderImpl.java:67) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.flink.connector.pulsar.common.utils.PulsarExceptionUtils.sneaky(PulsarExceptionUtils.java:69) ~[blob_p-8b5ac19c79f1f5e8e495fec1e487d5c051157af1-c417788a2d249f7022633051d21c30dd:1.15-SNAPSHOT]
   at org.apache.flink.connector.pulsar.common.utils.PulsarExceptionUtils.sneakyClient(PulsarExceptionUtils.java:46) ~[blob_p-8b5ac19c79f1f5e8e495fec1e487d5c051157af1-c417788a2d249f7022633051d21c30dd:1.15-SNAPSHOT]
   at org.apache.flink.connector.pulsar.common.config.PulsarClientFactory.createClient(PulsarClientFactory.java:152) ~[blob_p-8b5ac19c79f1f5e8e495fec1e487d5c051157af1-c417788a2d249f7022633051d21c30dd:1.15-SNAPSHOT]
   at org.apache.flink.connector.pulsar.sink.committer.PulsarCommitter.transactionCoordinatorClient(PulsarCommitter.java:158) ~[blob_p-8b5ac19c79f1f5e8e495fec1e487d5c051157af1-c417788a2d249f7022633051d21c30dd:1.15-SNAPSHOT]
   at org.apache.flink.connector.pulsar.sink.committer.PulsarCommitter.commit(PulsarCommitter.java:70) ~[blob_p-8b5ac19c79f1f5e8e495fec1e487d5c051157af1-c417788a2d249f7022633051d21c30dd:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.operators.sink.committables.CheckpointCommittableManagerImpl.commit(CheckpointCommittableManagerImpl.java:126) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.operators.sink.CommitterOperator.commitAndEmit(CommitterOperator.java:176) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.operators.sink.CommitterOperator.commitAndEmitCheckpoints(CommitterOperator.java:160) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.operators.sink.CommitterOperator.notifyCheckpointComplete(CommitterOperator.java:150) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.notifyCheckpointComplete(StreamOperatorWrapper.java:104) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.notifyCheckpointComplete(RegularOperatorChain.java:145) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpoint(SubtaskCheckpointCoordinatorImpl.java:409) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointComplete(SubtaskCheckpointCoordinatorImpl.java:343) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:1387) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointCompleteAsync$14(StreamTask.java:1328) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$17(StreamTask.java:1367) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:353) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:317) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:807) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:756) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at java.lang.Thread.run(Thread.java:750) [?:1.8.0_342]
Caused by: java.util.concurrent.CompletionException: java.lang.NullPointerException
   at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:673) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.uniAcceptStage(CompletableFuture.java:683) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.thenAccept(CompletableFuture.java:2010) ~[?:1.8.0_342]
   at org.apache.pulsar.client.impl.ConnectionHandler.grabCnx(ConnectionHandler.java:73) ~[blob_p-13182ae1972a0afba0da920da789e044f64d9
3a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   ... 37 more
Caused by: java.lang.NullPointerException
   at org.apache.pulsar.client.impl.TransactionMetaStoreHandler.connectionOpened(TransactionMetaStoreHandler.java:121) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.ConnectionHandler.lambda$grabCnx$0(ConnectionHandler.java:73) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.uniAcceptStage(CompletableFuture.java:683) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.thenAccept(CompletableFuture.java:2010) ~[?:1.8.0_342]
   at org.apache.pulsar.client.impl.ConnectionHandler.grabCnx(ConnectionHandler.java:73) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   ... 37 more
2022-11-02 03:49:56,321 WARN  org.apache.pulsar.client.impl.ConnectionHandler              [] - [persistent://pulsar/system/transaction_coordinator_assign-partition-0] [Transaction meta store handler [0]] Could not get connection to broker: java.lang.NullPointerException -- Will try again in 0.1 s
2022-11-02 03:49:56,322 ERROR org.apache.pulsar.client.impl.PulsarClientImpl               [] - Start transactionCoordinatorClient error.
org.apache.pulsar.client.api.transaction.TransactionCoordinatorClientException: org.apache.pulsar.client.api.PulsarClientException: java.util.concurrent.CompletionException: java.lang.NullPointerException
   at org.apache.pulsar.client.api.transaction.TransactionCoordinatorClientException.unwrap(TransactionCoordinatorClientException.java:131) ~[blob_p-4b3d6ed05b6715a4188854f7546af20bd5082468-a8d166b6a98b33c8b856fdb9ed3d9ee1:2.10.0]
   at org.apache.pulsar.client.api.transaction.TransactionCoordinatorClientException.unwrap(TransactionCoordinatorClientException.java:129) ~[blob_p-4b3d6ed05b6715a4188854f7546af20bd5082468-a8d166b6a98b33c8b856fdb9ed3d9ee1:2.10.0]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.start(TransactionCoordinatorClientImpl.java:70) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.PulsarClientImpl.<init>(PulsarClientImpl.java:204) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.PulsarClientImpl.<init>(PulsarClientImpl.java:140) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.ClientBuilderImpl.build(ClientBuilderImpl.java:67) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.flink.connector.pulsar.common.utils.PulsarExceptionUtils.sneaky(PulsarExceptionUtils.java:69) ~[blob_p-8b5ac19c79f1f5e8e495fec1e487d5c051157af1-c417788a2d249f7022633051d21c30dd:1.15-SNAPSHOT]
   at org.apache.flink.connector.pulsar.common.utils.PulsarExceptionUtils.sneakyClient(PulsarExceptionUtils.java:46) ~[blob_p-8b5ac19c79f1f5e8e495fec1e487d5c051157af1-c417788a2d249f7022633051d21c30dd:1.15-SNAPSHOT]
   at org.apache.flink.connector.pulsar.common.config.PulsarClientFactory.createClient(PulsarClientFactory.java:152) ~[blob_p-8b5ac19c79f1f5e8e495fec1e487d5c051157af1-c417788a2d249f7022633051d21c30dd:1.15-SNAPSHOT]
   at org.apache.flink.connector.pulsar.sink.committer.PulsarCommitter.transactionCoordinatorClient(PulsarCommitter.java:158) ~[blob_p-8b5ac19c79f1f5e8e495fec1e487d5c051157af1-c417788a2d249f7022633051d21c30dd:1.15-SNAPSHOT]
   at org.apache.flink.connector.pulsar.sink.committer.PulsarCommitter.commit(PulsarCommitter.java:70) ~[blob_p-8b5ac19c79f1f5e8e495fec1e487d5c051157af1-c417788a2d249f7022633051d21c30dd:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.operators.sink.committables.CheckpointCommittableManagerImpl.commit(CheckpointCommittableManagerImpl.java:126) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.operators.sink.CommitterOperator.commitAndEmit(CommitterOperator.java:176) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.operators.sink.CommitterOperator.commitAndEmitCheckpoints(CommitterOperator.java:160) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.operators.sink.CommitterOperator.notifyCheckpointComplete(CommitterOperator.java:150) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.notifyCheckpointComplete(StreamOperatorWrapper.java:104) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.notifyCheckpointComplete(RegularOperatorChain.java:145) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpoint(SubtaskCheckpointCoordinatorImpl.java:409) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointComplete(SubtaskCheckpointCoordinatorImpl.java:343) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:1387) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointCompleteAsync$14(StreamTask.java:1328) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$17(StreamTask.java:1367) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:353) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:317) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:807) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:756) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at java.lang.Thread.run(Thread.java:750) [?:1.8.0_342]
Caused by: org.apache.pulsar.client.api.PulsarClientException: java.util.concurrent.CompletionException: java.lang.NullPointerException
   at org.apache.pulsar.client.impl.ConnectionHandler.handleConnectionError(ConnectionHandler.java:89) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.uniExceptionallyStage(CompletableFuture.java:898) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.exceptionally(CompletableFuture.java:2209) ~[?:1.8.0_342]
   at org.apache.pulsar.client.impl.ConnectionHandler.grabCnx(ConnectionHandler.java:74) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.TransactionMetaStoreHandler.<init>(TransactionMetaStoreHandler.java:105) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.lambda$startAsync$0(TransactionCoordinatorClientImpl.java:89) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:995) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2137) ~[?:1.8.0_342]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.startAsync(TransactionCoordinatorClientImpl.java:78) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.start(TransactionCoordinatorClientImpl.java:68) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   ... 31 more
Caused by: java.util.concurrent.CompletionException: java.lang.NullPointerException
   at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:673) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.uniAcceptStage(CompletableFuture.java:683) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.thenAccept(CompletableFuture.java:2010) ~[?:1.8.0_342]
   at org.apache.pulsar.client.impl.ConnectionHandler.grabCnx(ConnectionHandler.java:73) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.TransactionMetaStoreHandler.<init>(TransactionMetaStoreHandler.java:105) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.lambda$startAsync$0(TransactionCoordinatorClientImpl.java:89) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:995) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2137) ~[?:1.8.0_342]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.startAsync(TransactionCoordinatorClientImpl.java:78) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.start(TransactionCoordinatorClientImpl.java:68) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   ... 31 more
Caused by: java.lang.NullPointerException
   at org.apache.pulsar.client.impl.TransactionMetaStoreHandler.connectionOpened(TransactionMetaStoreHandler.java:121) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.ConnectionHandler.lambda$grabCnx$0(ConnectionHandler.java:73) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.uniAcceptStage(CompletableFuture.java:683) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.thenAccept(CompletableFuture.java:2010) ~[?:1.8.0_342]
   at org.apache.pulsar.client.impl.ConnectionHandler.grabCnx(ConnectionHandler.java:73) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.TransactionMetaStoreHandler.<init>(TransactionMetaStoreHandler.java:105) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.lambda$startAsync$0(TransactionCoordinatorClientImpl.java:89) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:995) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2137) ~[?:1.8.0_342]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.startAsync(TransactionCoordinatorClientImpl.java:78) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.start(TransactionCoordinatorClientImpl.java:68) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   ... 31 more
{code}
;;;","04/Nov/22 01:42;syhily;[~martijnvisser] The CI link you have provided is tests on flink release-1.15 branch. And this is an error which has been fixed in https://github.com/apache/pulsar/pull/15840 and released in Pulsar 2.10.1. Do we need to bump the pulsar-client to 2.10.1 in release-1.15 branch to fix this hang issue?;;;","07/Nov/22 15:13;tison;[~syhily] If this is a release-1.15 specific issue, I suggest you open a new issue so that we don't mangle different issues into one entry.;;;","08/Nov/22 06:56;tison;follow-up to fix 1.15 via b37999514cbbd019b31fb2d9c4ae751a956f6c87;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Timestamp LTZ is unsupported in table store ,FLINK-29490,13484079,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,01/Oct/22 03:32,29/Mar/23 01:50,13/Jul/23 08:13,29/Mar/23 01:50,,,,,,,,table-store-0.4.0,,,,,Table Store,,,,,,,0,,,,,"Due to orc format limitation, timestamp ltz is unsupported now. We should fix this, and validate this type cross multiple engines (hive spark trino).
We need to careful about time zone.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-01 03:32:46.0,,,,,,,,,,"0|z18zo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink python udf arrow in thread model bug,FLINK-29483,13483974,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,30/Sep/22 09:03,20/Oct/22 02:33,13/Jul/23 08:13,10/Oct/22 12:20,1.15.2,1.16.0,,,,,,1.15.3,1.16.0,1.17.0,,,API / Python,,,,,,,0,pull-request-available,,,,!image-2022-09-30-17-03-05-005.png!,,dianfu,hxb,hxbks2ks,jackylau,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/22 09:03;jackylau;image-2022-09-30-17-03-05-005.png;https://issues.apache.org/jira/secure/attachment/13049977/image-2022-09-30-17-03-05-005.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 10 12:20:54 UTC 2022,,,,,,,,,,"0|z18z0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/22 14:25;hxbks2ks;[~jackylau] You means your job runs in `thread mode` will raise this exception? ;;;","09/Oct/22 02:10;jackylau;yes [~hxbks2ks] ;;;","10/Oct/22 12:20;hxb;Merged into master via 7a6fc2433c746b8d97286d1c758f5a933f40e3e7

Merged into release-1.16 via 6de8d291ef2761c469fb5ba5b3394a838846bea5

Merged into release-1.15 via 5d748f37661cca11f168fae0b54a8c0d0f1ef2a6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support whether using system PythonPath for PyFlink jobs,FLINK-29479,13483937,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,30/Sep/22 05:33,26/Oct/22 08:26,13/Jul/23 08:13,25/Oct/22 11:33,,,,,,,,1.15.3,1.16.1,1.17.0,,,API / Python,,,,,,,0,pull-request-available,,,,"It exists PYTHONPATH env in system,like yarn/k8s images, it will cause conflict with users python depdendency sometimes. so i suggest add a config to do whether using system env of PYTHONPATH",,hxb,hxbks2ks,jackylau,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 25 11:33:00 UTC 2022,,,,,,,,,,"0|z18yso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/22 05:34;jackylau;hi [~dianfu] [~hxbks2ks] , what do you think?;;;","30/Sep/22 05:35;jackylau;{code:java}
// conf like
/** Whether preserves system env of PYTHONPATH. */
@Experimental
public static final ConfigOption<Boolean> PYTHON_PRESERVE_PYTHONPATH_ENABLED =
        ConfigOptions.key(""python.preserve.pythonpath.enabled"")
                .booleanType()
                .defaultValue(true)
                .withDescription(""Whether preserves system env of PYTHONPATH."");
 {code};;;","30/Sep/22 09:38;hxbks2ks;[~jackylau] From my side, the users dependency will have a higher priority, so it shouldn't be affected by the PYTHONPATH of the system env  under normal circumstances. Could you give a more specific example of how the system's PYTHONPATH is having a bad effect in some circumstance.;;;","11/Oct/22 09:25;hxb;I have discussed this problem with [~jackylau]  offline. I think we may really need to introduce a config to decide whether we need to use the PYTHONPATH that comes with the system, but we can reconsider the name of the config. For example, a more general meaning of config means whether to use a completely isolated and independent of all environment variables. cc [~dianfu] ;;;","19/Oct/22 07:29;jackylau;After researching spark, it does not rely on system environment variables. Through careful offline communication with [~hxb] , we reached an agreement to use this parameter to decide whether to retain system environment variables
{code:java}
// code placeholder
public static final ConfigOption<Boolean> PYTHON_SYSTEMENV_ENABLED =
ConfigOptions.key(""python.systemenv.enabled "")
.booleanType()
.defaultValue(true)
.withDescription(""Whether uses system env in python.""); {code};;;","25/Oct/22 11:33;hxb;Merged into master via 8e16cc8e424e352c5b45b46f1520ecf0edec70be

Merged into release-1.16 via 9213effb32a4e80d8113ba7bf36782f33a5e197c

Merged into release-1.15 via 91ccde95c7eae7f020d68592a7fa76201674724a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassCastException when collect primitive array to Python,FLINK-29477,13483924,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,30/Sep/22 02:21,16/Oct/22 04:34,13/Jul/23 08:13,16/Oct/22 04:34,1.15.2,1.16.0,,,,,,1.15.3,1.16.0,,,,API / Python,,,,,,,0,pull-request-available,,,,"How to reproduce this bug:
{code:java}
ds = env.from_collection([1, 2], type_info=Types.PRIMITIVE_ARRAY(Types.INT()))
ds.execute_and_collect(){code}
got:
{code:java}
java.lang.ClassCastException: class [I cannot be cast to class [Ljava.lang.Object {code}",,hxb,Juntao Hu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Oct 16 04:34:04 UTC 2022,,,,,,,,,,"0|z18yq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/22 04:34;hxb;Merged into master via c85e6ec45bebb2eb376a911e11294cd118893fb3

Merged into release-1.16 via b0d4dd3a1eaf648f67e0ca7cb075591ecb69e2c4

Merged into release-1.15 via 507b93eef2af79ef2ad5752e1271e5c8915bb15f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Name collision: Group already contains a Metric with the name,FLINK-29474,13483853,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,morhidi,,29/Sep/22 13:38,24/Nov/22 01:01,13/Jul/23 08:13,29/Sep/22 19:16,kubernetes-operator-1.2.0,,,,,,,kubernetes-operator-1.2.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"k create -f examples/basic-session-deployment-and-job.yaml

results in warnings:
{quote} flink-kubernetes-operator 2022-09-29 13:30:00,001 o.a.f.m.MetricGroup            [WARN ][default/basic-session-job-example] Name collision: Group already contains a Metric with the name  │
│ 'TimeSeconds'. Metric will not be reported.[flink-kubernetes-operator-6f9bbfd557-ljp6w, k8soperator, default, flink-kubernetes-operator, system, Lifecycle, Transition, Resume]            │
│ flink-kubernetes-operator 2022-09-29 13:30:00,001 o.a.f.m.MetricGroup            [WARN ][default/basic-session-job-example] Name collision: Group already contains a Metric with the name  │
│ 'TimeSeconds'. Metric will not be reported.[flink-kubernetes-operator-6f9bbfd557-ljp6w, k8soperator, default, flink-kubernetes-operator, system, Lifecycle, Transition, Upgrade]
{quote}",,gyfora,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 29 19:16:14 UTC 2022,,,,,,,,,,"0|z18yag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Sep/22 19:16;gyfora;merged to main 3ae66dbddd4126a19c1e8f0105027bc370f5e753;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProcessDataStreamStreamingTests.test_process_function unstable,FLINK-29461,13483752,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,29/Sep/22 03:35,13/Dec/22 03:12,13/Jul/23 08:13,13/Dec/22 03:12,1.16.0,1.17.0,,,,,,1.16.1,1.17.0,,,,API / Python,,,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-09-29T02:10:45.3571648Z Sep 29 02:10:45 self = <pyflink.datastream.tests.test_data_stream.ProcessDataStreamStreamingTests testMethod=test_process_function>
2022-09-29T02:10:45.3572279Z Sep 29 02:10:45 
2022-09-29T02:10:45.3572810Z Sep 29 02:10:45     def test_process_function(self):
2022-09-29T02:10:45.3573495Z Sep 29 02:10:45         self.env.set_parallelism(1)
2022-09-29T02:10:45.3574148Z Sep 29 02:10:45         self.env.get_config().set_auto_watermark_interval(2000)
2022-09-29T02:10:45.3580634Z Sep 29 02:10:45         self.env.set_stream_time_characteristic(TimeCharacteristic.EventTime)
2022-09-29T02:10:45.3583194Z Sep 29 02:10:45         data_stream = self.env.from_collection([(1, '1603708211000'),
2022-09-29T02:10:45.3584515Z Sep 29 02:10:45                                                 (2, '1603708224000'),
2022-09-29T02:10:45.3585957Z Sep 29 02:10:45                                                 (3, '1603708226000'),
2022-09-29T02:10:45.3587132Z Sep 29 02:10:45                                                 (4, '1603708289000')],
2022-09-29T02:10:45.3588094Z Sep 29 02:10:45                                                type_info=Types.ROW([Types.INT(), Types.STRING()]))
2022-09-29T02:10:45.3589090Z Sep 29 02:10:45     
2022-09-29T02:10:45.3589949Z Sep 29 02:10:45         class MyProcessFunction(ProcessFunction):
2022-09-29T02:10:45.3590710Z Sep 29 02:10:45     
2022-09-29T02:10:45.3591856Z Sep 29 02:10:45             def process_element(self, value, ctx):
2022-09-29T02:10:45.3592873Z Sep 29 02:10:45                 current_timestamp = ctx.timestamp()
2022-09-29T02:10:45.3593862Z Sep 29 02:10:45                 current_watermark = ctx.timer_service().current_watermark()
2022-09-29T02:10:45.3594915Z Sep 29 02:10:45                 yield ""current timestamp: {}, current watermark: {}, current_value: {}""\
2022-09-29T02:10:45.3596201Z Sep 29 02:10:45                     .format(str(current_timestamp), str(current_watermark), str(value))
2022-09-29T02:10:45.3597089Z Sep 29 02:10:45     
2022-09-29T02:10:45.3597942Z Sep 29 02:10:45         watermark_strategy = WatermarkStrategy.for_monotonous_timestamps()\
2022-09-29T02:10:45.3599260Z Sep 29 02:10:45             .with_timestamp_assigner(SecondColumnTimestampAssigner())
2022-09-29T02:10:45.3600611Z Sep 29 02:10:45         data_stream.assign_timestamps_and_watermarks(watermark_strategy)\
2022-09-29T02:10:45.3601877Z Sep 29 02:10:45             .process(MyProcessFunction(), output_type=Types.STRING()).add_sink(self.test_sink)
2022-09-29T02:10:45.3603527Z Sep 29 02:10:45         self.env.execute('test process function')
2022-09-29T02:10:45.3604445Z Sep 29 02:10:45         results = self.test_sink.get_results()
2022-09-29T02:10:45.3605684Z Sep 29 02:10:45         expected = [""current timestamp: 1603708211000, current watermark: ""
2022-09-29T02:10:45.3607157Z Sep 29 02:10:45                     ""-9223372036854775808, current_value: Row(f0=1, f1='1603708211000')"",
2022-09-29T02:10:45.3608256Z Sep 29 02:10:45                     ""current timestamp: 1603708224000, current watermark: ""
2022-09-29T02:10:45.3609650Z Sep 29 02:10:45                     ""-9223372036854775808, current_value: Row(f0=2, f1='1603708224000')"",
2022-09-29T02:10:45.3610854Z Sep 29 02:10:45                     ""current timestamp: 1603708226000, current watermark: ""
2022-09-29T02:10:45.3612279Z Sep 29 02:10:45                     ""-9223372036854775808, current_value: Row(f0=3, f1='1603708226000')"",
2022-09-29T02:10:45.3613382Z Sep 29 02:10:45                     ""current timestamp: 1603708289000, current watermark: ""
2022-09-29T02:10:45.3615683Z Sep 29 02:10:45                     ""-9223372036854775808, current_value: Row(f0=4, f1='1603708289000')""]
2022-09-29T02:10:45.3617687Z Sep 29 02:10:45 >       self.assert_equals_sorted(expected, results)
2022-09-29T02:10:45.3618620Z Sep 29 02:10:45 
2022-09-29T02:10:45.3619425Z Sep 29 02:10:45 pyflink/datastream/tests/test_data_stream.py:986: 
2022-09-29T02:10:45.3620424Z Sep 29 02:10:45 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-09-29T02:10:45.3621886Z Sep 29 02:10:45 pyflink/datastream/tests/test_data_stream.py:66: in assert_equals_sorted
2022-09-29T02:10:45.3622847Z Sep 29 02:10:45     self.assertEqual(expected, actual)
2022-09-29T02:10:45.3624658Z Sep 29 02:10:45 E   AssertionError: Lists differ: [""cur[414 chars]ark: -9223372036854775808, current_value: Row([22 chars]0')""] != [""cur[414 chars]ark: 1603708225999, current_value: Row(f0=4, f[15 chars]0')""]
2022-09-29T02:10:45.3625881Z Sep 29 02:10:45 E   
2022-09-29T02:10:45.3626591Z Sep 29 02:10:45 E   First differing element 3:
2022-09-29T02:10:45.3627726Z Sep 29 02:10:45 E   ""curr[44 chars]ark: -9223372036854775808, current_value: Row([21 chars]00')""
2022-09-29T02:10:45.3628758Z Sep 29 02:10:45 E   ""curr[44 chars]ark: 1603708225999, current_value: Row(f0=4, f[14 chars]00')""
2022-09-29T02:10:45.3629276Z Sep 29 02:10:45 E   
2022-09-29T02:10:45.3629842Z Sep 29 02:10:45 E   Diff is 753 characters long. Set self.maxDiff to None to see it.
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41436&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901",,hxb,hxbks2ks,leonard,mapohl,martijnvisser,renqs,清月,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 13 03:12:35 UTC 2022,,,,,,,,,,"0|z18xo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 08:32;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42680&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=25187;;;","02/Nov/22 08:12;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42724&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=25197;;;","04/Nov/22 01:58;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42803&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=24482;;;","04/Nov/22 14:51;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42827&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=25085;;;","07/Nov/22 08:15;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42858&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=28095;;;","07/Nov/22 08:28;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42867&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=27223;;;","08/Nov/22 07:23;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42908&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=27019;;;","11/Nov/22 09:14;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43043&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=25767;;;","21/Nov/22 04:53;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43332&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2;;;","22/Nov/22 07:36;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43367&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901;;;","22/Nov/22 07:49;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43369&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf;;;","22/Nov/22 08:41;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43167&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=24422;;;","22/Nov/22 08:41;mapohl;[~hxbks2ks] any updates on this issue?;;;","24/Nov/22 07:21;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43436&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=27416;;;","25/Nov/22 04:40;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43483&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=28464;;;","28/Nov/22 09:40;mapohl;Same build, two failures:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43512&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=29899
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43512&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=27869;;;","28/Nov/22 13:02;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43541&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=26458;;;","29/Nov/22 06:03;mapohl;Same build, multiple failures:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43572&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=27194
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43572&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=29488;;;","29/Nov/22 08:41;hxb;[~mapohl] Sorry for the late reply, I will take a look into this issue in these two days.;;;","30/Nov/22 06:59;mapohl;No worries. Thanks for picking it up. :)

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43604&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=29496;;;","01/Dec/22 09:03;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43636&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=27587;;;","02/Dec/22 10:36;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43662&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=28078;;;","02/Dec/22 10:56;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43664&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=28979;;;","05/Dec/22 09:54;mapohl;Same build:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43692&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=27154
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43692&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=26932;;;","05/Dec/22 11:00;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43709&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=28522;;;","05/Dec/22 11:16;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43711&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=29177;;;","06/Dec/22 08:32;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43742&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=28926;;;","07/Dec/22 04:23;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43771&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=28618;;;","09/Dec/22 08:04;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43817&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=28358;;;","09/Dec/22 08:07;mapohl;[~dianfu] [~hxbks2ks] any updates on that one? Or is there anyone else we can ping to have a look at it?;;;","12/Dec/22 10:14;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43871&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901;;;","12/Dec/22 11:40;hxb;[~mapohl] I have prepared a PR to make the test more stable.;;;","13/Dec/22 03:12;hxb;Merged into master via 4df6a398bbe2a9de7c23977176789e54cc0848fa

Merged into release-1.16 via 36c86f1c6cd34482c2eb3cc939d348e08fd08a2b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HsResultPartitionTest.testRelease failed with AssertionFailedError,FLINK-29460,13483740,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,hxbks2ks,hxbks2ks,29/Sep/22 02:18,30/Sep/22 01:25,13/Jul/23 08:13,30/Sep/22 01:25,1.16.0,,,,,,,1.16.0,,,,,Runtime / Network,,,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-09-29T01:52:44.4460454Z Sep 29 01:52:44 [ERROR] org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartitionTest.testRelease  Time elapsed: 0.271 s  <<< FAILURE!
2022-09-29T01:52:44.4461655Z Sep 29 01:52:44 org.opentest4j.AssertionFailedError: 
2022-09-29T01:52:44.4462489Z Sep 29 01:52:44 
2022-09-29T01:52:44.4463018Z Sep 29 01:52:44 expected: 10
2022-09-29T01:52:44.4463549Z Sep 29 01:52:44  but was: 6
2022-09-29T01:52:44.4464382Z Sep 29 01:52:44 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2022-09-29T01:52:44.4465591Z Sep 29 01:52:44 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2022-09-29T01:52:44.4466937Z Sep 29 01:52:44 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2022-09-29T01:52:44.4468568Z Sep 29 01:52:44 	at org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartitionTest.testRelease(HsResultPartitionTest.java:245)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41436&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=8811",,hxbks2ks,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 30 01:25:51 UTC 2022,,,,,,,,,,"0|z18xlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Sep/22 02:19;hxbks2ks; [~Weijie Guo] Could you help take a look? Thx.;;;","29/Sep/22 03:01;Weijie Guo;[~hxbks2ks] Thanks for reporting this unstable test, I think this is caused by FLINK-29425 and I will fix this problem as soon as possible.;;;","30/Sep/22 01:25;xtsong;- master (1.17): df1681c7ef3f5a946f626c86748a9fe46ee687d7
- release-1.16: 729168f0edde39b5f2313788427a7f8d1a487bd6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Calcite classes which were fixed in 1.24,FLINK-29446,13483617,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,28/Sep/22 08:47,11/Oct/22 06:47,13/Jul/23 08:13,11/Oct/22 06:47,1.15.2,1.16.0,,,,,,1.17.0,,,,,Table SQL / API,Table SQL / Planner,,,,,,0,pull-request-available,,,,"{{SqlDotOperator}}, {{SqlItemOperator}}, {{AliasNamespace}} were introduced as copies from Calcite and with a fix inside at https://github.com/apache/flink/pull/12649
At the same side the fixed was applied in Calcite itself in 1.24. https://issues.apache.org/jira/browse/CALCITE-4085

Since now Flink depends on 1.26 the fix is included and there is no need to have these classes in Flink repo",,Sergey Nuyanzin,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 11 06:47:28 UTC 2022,,,,,,,,,,"0|z18wu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 06:47;twalthr;Fixed in master: 90f4239f4af58611ae4922a6b8d032a604cbc650;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The partition of data before and after the Kafka Shuffle are not aligned,FLINK-29437,13483589,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zakelly,Zakelly,Zakelly,28/Sep/22 06:52,21/Nov/22 12:55,13/Jul/23 08:13,21/Nov/22 12:55,1.15.3,,,,,,,1.17.0,,,,,API / DataStream,Connectors / Kafka,,,,,,0,pull-request-available,,,,"I notice that the key group range in consumer side of Kafka Shuffle is not aligned with the producer side, there are two problems:
 # The data partitioning of the sink(producer) is exactly the same way as a keyed stream that as the same maximum parallelism as the number of kafka partitions does, but in consumer side the number of partitions and key groups are not the same.
 # There is a distribution of assigning kafka partitions to consumer subtasks (See KafkaTopicPartitionAssigner#assign), but the producer of Kafka Shuffle simply assume the partition index equals the subtask index. e.g.

       !image-2022-09-28-14-32-28-116.png|width=1133,height=274!

My proposed change:
 # Set the max parallelism of the key stream in consumer side as the number of kafka partitions. 
 # Use the same method when assigning kafka partitions to consumer subtasks to maintain a map from subtasks to kafka partitions, which is used by the producer to insert into the right partition for data for a subtask. i.e.

       !image-2022-09-28-14-35-47-954.png|width=1030,height=283!",,kezhuw,liam8,martijnvisser,pnowojski,ram_krish,renqs,ym,Zakelly,,,,,,,,,,,,,,,,,,,FLINK-29430,,,,,,,,,,,,,,,,,,,,,FLINK-21317,,,,,,"28/Sep/22 06:32;Zakelly;image-2022-09-28-14-32-28-116.png;https://issues.apache.org/jira/secure/attachment/13049852/image-2022-09-28-14-32-28-116.png","28/Sep/22 06:35;Zakelly;image-2022-09-28-14-35-47-954.png;https://issues.apache.org/jira/secure/attachment/13049851/image-2022-09-28-14-35-47-954.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 21 12:55:49 UTC 2022,,,,,,,,,,"0|z18wo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/22 07:57;martijnvisser;[~renqs] WDYT?;;;","28/Sep/22 09:42;renqs;[~martijnvisser] This feature was initially implemented by [~yuanmei], so maybe she could make some comments on this. ;;;","30/Sep/22 10:26;Zakelly;After offline sync with [~ym] , we agree to fix this as I proposed.;;;","21/Nov/22 12:55;ym;merged commit [{{0b81668}}|https://github.com/apache/flink/commit/0b816680e51283d261eb7b7bce560dd1641691ce] into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exceptions during job graph serialization lock up client,FLINK-29431,13483417,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Weijie Guo,chesnay,chesnay,27/Sep/22 09:44,28/Sep/22 09:24,13/Jul/23 08:13,28/Sep/22 09:24,1.16.0,,,,,,,1.16.0,,,,,Client / Job Submission,,,,,,,0,pull-request-available,,,,"The parallel serialization introduced in FLINK-26675 does not properly handle errors.

{{StreamConfig#triggerSerializationAndReturnFuture}} returns a future that is never completed if the serialization fails, because all we do is throw an exception but that isn't propagated anywhere.

As a result {{StreamingJobGraphGenerator#createJobGraph}} blocks indefinitely.",,hxbks2ks,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26675,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 28 09:24:03 UTC 2022,,,,,,,,,,"0|z18vm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/22 12:32;Weijie Guo;[~chesnay] Thank you for reporting this bug, I'll take a look.;;;","28/Sep/22 09:24;xtsong;- master (1.17): 853ea452bff191fb0416017aeb5acd2046665996
- release-1.16: f66cb67687e7df355006630d50b81307d281c150;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sanity check in InternalKeyContextImpl#setCurrentKeyGroupIndex,FLINK-29430,13483373,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zakelly,Zakelly,Zakelly,27/Sep/22 08:12,02/Dec/22 09:13,13/Jul/23 08:13,02/Dec/22 09:13,1.15.3,,,,,,,1.17.0,,,,,Runtime / State Backends,,,,,,,0,pull-request-available,,,,"Currently the HeapStateBackend check whether the current key group index is a valid one while the RocksDBStateBackend will not. When using HeapStateBackend, if the user uses a non-deterministic shuffle key, an exception is thrown as follows:

 
{code:java}
java.lang.IllegalArgumentException: Key group 84 is not in KeyGroupRange{startKeyGroup=32, endKeyGroup=63}. Unless you're directly using low level state access APIs, this is most likely caused by non-deterministic shuffle key (hashCode and equals implementation).
    at org.apache.flink.runtime.state.KeyGroupRangeOffsets.newIllegalKeyGroupException(KeyGroupRangeOffsets.java:37)
    at org.apache.flink.runtime.state.heap.StateTable.getMapForKeyGroup(StateTable.java:305)
    at org.apache.flink.runtime.state.heap.StateTable.get(StateTable.java:261)
    at org.apache.flink.runtime.state.heap.StateTable.get(StateTable.java:143)
    at org.apache.flink.runtime.state.heap.HeapValueState.value(HeapValueState.java:72)
    at com.alibaba.ververica.flink.state.benchmark.wordcount.WordCount$MixedFlatMapper.flatMap(WordCount.java:169)
    at com.alibaba.ververica.flink.state.benchmark.wordcount.WordCount$MixedFlatMapper.flatMap(WordCount.java:160)
    at org.apache.flink.streaming.api.operators.StreamFlatMap.processElement(StreamFlatMap.java:47)
    at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:135)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:106)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:526)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:811)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:760)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:954)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:933)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:568)
    at java.lang.Thread.run(Thread.java:750)
 {code}
However, the RocksDBStateBackend will run without an exception. The wrong key group index will cause a state correctness problem, so it is better to do a check in {_}InternalKeyContextImpl#{_}{_}setCurrentKeyGroupIndex{_}, and throw an exception immediately.

 ",,yunta,Zakelly,,,,,,,,,,,,,,,,,,,,,,,FLINK-29437,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 02 09:13:14 UTC 2022,,,,,,,,,,"0|z18vcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/22 08:25;Zakelly;Correct me if I'm wrong.

If it is valid, I would like to fix it.;;;","02/Dec/22 09:13;yunta;merged in master: 1af9446248677b9540ed5d53bd2b42f3b724f7b5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LookupJoinITCase failed with classloader problem,FLINK-29427,13483327,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,renqs,hxbks2ks,hxbks2ks,27/Sep/22 06:22,09/Feb/23 13:11,13/Jul/23 08:13,06/Feb/23 08:23,1.16.0,1.17.0,,,,,,1.16.2,1.17.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-09-27T02:49:20.9501313Z Sep 27 02:49:20 Caused by: org.codehaus.janino.InternalCompilerException: Compiling ""KeyProjection$108341"": Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2022-09-27T02:49:20.9502654Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382)
2022-09-27T02:49:20.9503366Z Sep 27 02:49:20 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
2022-09-27T02:49:20.9504044Z Sep 27 02:49:20 	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
2022-09-27T02:49:20.9504704Z Sep 27 02:49:20 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
2022-09-27T02:49:20.9505341Z Sep 27 02:49:20 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
2022-09-27T02:49:20.9505965Z Sep 27 02:49:20 	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
2022-09-27T02:49:20.9506584Z Sep 27 02:49:20 	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
2022-09-27T02:49:20.9507261Z Sep 27 02:49:20 	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104)
2022-09-27T02:49:20.9507883Z Sep 27 02:49:20 	... 30 more
2022-09-27T02:49:20.9509266Z Sep 27 02:49:20 Caused by: java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2022-09-27T02:49:20.9510835Z Sep 27 02:49:20 	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:184)
2022-09-27T02:49:20.9511760Z Sep 27 02:49:20 	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:192)
2022-09-27T02:49:20.9512456Z Sep 27 02:49:20 	at java.lang.Class.forName0(Native Method)
2022-09-27T02:49:20.9513014Z Sep 27 02:49:20 	at java.lang.Class.forName(Class.java:348)
2022-09-27T02:49:20.9513649Z Sep 27 02:49:20 	at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:89)
2022-09-27T02:49:20.9514339Z Sep 27 02:49:20 	at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:312)
2022-09-27T02:49:20.9514990Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:8556)
2022-09-27T02:49:20.9515659Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6749)
2022-09-27T02:49:20.9516337Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594)
2022-09-27T02:49:20.9516989Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573)
2022-09-27T02:49:20.9517632Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215)
2022-09-27T02:49:20.9518319Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481)
2022-09-27T02:49:20.9519018Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476)
2022-09-27T02:49:20.9519680Z Sep 27 02:49:20 	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928)
2022-09-27T02:49:20.9520386Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476)
2022-09-27T02:49:20.9521042Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469)
2022-09-27T02:49:20.9521677Z Sep 27 02:49:20 	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927)
2022-09-27T02:49:20.9522299Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
2022-09-27T02:49:20.9522929Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.hasAnnotation(UnitCompiler.java:1365)
2022-09-27T02:49:20.9523658Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1349)
2022-09-27T02:49:20.9524365Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
2022-09-27T02:49:20.9525030Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
2022-09-27T02:49:20.9525750Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
2022-09-27T02:49:20.9526383Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
2022-09-27T02:49:20.9527069Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
2022-09-27T02:49:20.9527832Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
2022-09-27T02:49:20.9528560Z Sep 27 02:49:20 	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
2022-09-27T02:49:20.9529217Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
2022-09-27T02:49:20.9529862Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
2022-09-27T02:49:20.9530427Z Sep 27 02:49:20 	... 37 more
2022-09-27T02:49:20.9530852Z Sep 27 02:49:20 
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41369&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4",,hxb,hxbks2ks,JunRuiLi,leonard,lincoln.86xy,mapohl,martijnvisser,renqs,rmetzger,roman,smiralex,stayrascal,straw,Weijie Guo,xzw0223,yunta,,,,,,,,,,,,,,,,,,,,,FLINK-29462,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 13:11:42 UTC 2023,,,,,,,,,,"0|z18v28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/22 06:22;hxbks2ks;cc [~renqs] [~smiralex];;;","29/Sep/22 02:14;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41434&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21554;;;","30/Sep/22 14:37;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41493&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21545;;;","30/Sep/22 14:38;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41488&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21930;;;","03/Oct/22 13:59;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41527&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","10/Oct/22 03:21;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41753&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","14/Oct/22 06:54;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41996&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=17943;;;","14/Oct/22 09:47;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41984&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21610;;;","17/Oct/22 03:45;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42055&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=17575;;;","18/Oct/22 08:31;hxb;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42117&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9]

Hi [~smiralex]  is any update on this issue?;;;","27/Oct/22 04:43;yunta;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42466&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","31/Oct/22 14:15;rmetzger;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42642&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4

I'm including the entire stack trace here again for better discoverability of the ticket
{code}
2022-10-31T10:36:52.5727692Z Oct 31 10:36:52 [ERROR] LookupJoinITCase.testJoinTemporalTableWithComputedColumnAndPushDown  Time elapsed: 0.259 s  <<< ERROR!
2022-10-31T10:36:52.5728256Z Oct 31 10:36:52 java.lang.RuntimeException: Failed to fetch next result
2022-10-31T10:36:52.5729008Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
2022-10-31T10:36:52.5729945Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2022-10-31T10:36:52.5730765Z Oct 31 10:36:52 	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222)
2022-10-31T10:36:52.5731457Z Oct 31 10:36:52 	at java.util.Iterator.forEachRemaining(Iterator.java:115)
2022-10-31T10:36:52.5732046Z Oct 31 10:36:52 	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:115)
2022-10-31T10:36:52.5732709Z Oct 31 10:36:52 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:308)
2022-10-31T10:36:52.5733412Z Oct 31 10:36:52 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:144)
2022-10-31T10:36:52.5734111Z Oct 31 10:36:52 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:108)
2022-10-31T10:36:52.5734927Z Oct 31 10:36:52 	at org.apache.flink.table.planner.runtime.batch.sql.join.LookupJoinITCase.testJoinTemporalTableWithComputedColumnAndPushDown(LookupJoinITCase.scala:350)
2022-10-31T10:36:52.5735660Z Oct 31 10:36:52 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-10-31T10:36:52.5736232Z Oct 31 10:36:52 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-10-31T10:36:52.5736871Z Oct 31 10:36:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-10-31T10:36:52.5737463Z Oct 31 10:36:52 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-10-31T10:36:52.5738053Z Oct 31 10:36:52 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-10-31T10:36:52.5738698Z Oct 31 10:36:52 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-10-31T10:36:52.5739454Z Oct 31 10:36:52 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-10-31T10:36:52.5740160Z Oct 31 10:36:52 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-10-31T10:36:52.5740803Z Oct 31 10:36:52 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-10-31T10:36:52.5741416Z Oct 31 10:36:52 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-10-31T10:36:52.5742038Z Oct 31 10:36:52 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-10-31T10:36:52.5742704Z Oct 31 10:36:52 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-10-31T10:36:52.5743255Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-10-31T10:36:52.5743882Z Oct 31 10:36:52 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-10-31T10:36:52.5744496Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-10-31T10:36:52.5745093Z Oct 31 10:36:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-10-31T10:36:52.5745755Z Oct 31 10:36:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-10-31T10:36:52.5746353Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-10-31T10:36:52.5746901Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-10-31T10:36:52.5747477Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-10-31T10:36:52.5748061Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-10-31T10:36:52.5748617Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-10-31T10:36:52.5749257Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-10-31T10:36:52.5749783Z Oct 31 10:36:52 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-10-31T10:36:52.5750313Z Oct 31 10:36:52 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-10-31T10:36:52.5750938Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-10-31T10:36:52.5751498Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-10-31T10:36:52.5752060Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-10-31T10:36:52.5752638Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-10-31T10:36:52.5753215Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-10-31T10:36:52.5753783Z Oct 31 10:36:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-10-31T10:36:52.5754384Z Oct 31 10:36:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-10-31T10:36:52.5754943Z Oct 31 10:36:52 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-10-31T10:36:52.5755475Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-10-31T10:36:52.5756039Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-10-31T10:36:52.5756566Z Oct 31 10:36:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-10-31T10:36:52.5757065Z Oct 31 10:36:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-10-31T10:36:52.5757642Z Oct 31 10:36:52 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-10-31T10:36:52.5758322Z Oct 31 10:36:52 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-10-31T10:36:52.5759047Z Oct 31 10:36:52 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-10-31T10:36:52.5759749Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
2022-10-31T10:36:52.5760564Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
2022-10-31T10:36:52.5761314Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
2022-10-31T10:36:52.5762093Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
2022-10-31T10:36:52.5762908Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
2022-10-31T10:36:52.5763770Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
2022-10-31T10:36:52.5764459Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-10-31T10:36:52.5765113Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-10-31T10:36:52.5765833Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-10-31T10:36:52.5766596Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-10-31T10:36:52.5767359Z Oct 31 10:36:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
2022-10-31T10:36:52.5768022Z Oct 31 10:36:52 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2022-10-31T10:36:52.5768649Z Oct 31 10:36:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
2022-10-31T10:36:52.5769488Z Oct 31 10:36:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-10-31T10:36:52.5770294Z Oct 31 10:36:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2022-10-31T10:36:52.5771012Z Oct 31 10:36:52 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-10-31T10:36:52.5771721Z Oct 31 10:36:52 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-10-31T10:36:52.5772347Z Oct 31 10:36:52 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-10-31T10:36:52.5772962Z Oct 31 10:36:52 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-10-31T10:36:52.5773513Z Oct 31 10:36:52 Caused by: java.io.IOException: Failed to fetch job execution result
2022-10-31T10:36:52.5774190Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
2022-10-31T10:36:52.5774987Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
2022-10-31T10:36:52.5775774Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2022-10-31T10:36:52.5776354Z Oct 31 10:36:52 	... 67 more
2022-10-31T10:36:52.5776877Z Oct 31 10:36:52 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-10-31T10:36:52.5777540Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-10-31T10:36:52.5778162Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2022-10-31T10:36:52.5778960Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
2022-10-31T10:36:52.5779537Z Oct 31 10:36:52 	... 69 more
2022-10-31T10:36:52.5780017Z Oct 31 10:36:52 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-10-31T10:36:52.5780646Z Oct 31 10:36:52 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-10-31T10:36:52.5781393Z Oct 31 10:36:52 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
2022-10-31T10:36:52.5782101Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-10-31T10:36:52.5782734Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2022-10-31T10:36:52.5783372Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2022-10-31T10:36:52.5784133Z Oct 31 10:36:52 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:138)
2022-10-31T10:36:52.5784939Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:181)
2022-10-31T10:36:52.5785508Z Oct 31 10:36:52 	... 69 more
2022-10-31T10:36:52.5785968Z Oct 31 10:36:52 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-10-31T10:36:52.5786717Z Oct 31 10:36:52 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
2022-10-31T10:36:52.5787595Z Oct 31 10:36:52 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
2022-10-31T10:36:52.5788391Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256)
2022-10-31T10:36:52.5789199Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247)
2022-10-31T10:36:52.5789962Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
2022-10-31T10:36:52.5790681Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:739)
2022-10-31T10:36:52.5791463Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:716)
2022-10-31T10:36:52.5792172Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)
2022-10-31T10:36:52.5792867Z Oct 31 10:36:52 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479)
2022-10-31T10:36:52.5793446Z Oct 31 10:36:52 	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
2022-10-31T10:36:52.5794040Z Oct 31 10:36:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-10-31T10:36:52.5794630Z Oct 31 10:36:52 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-10-31T10:36:52.5795234Z Oct 31 10:36:52 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
2022-10-31T10:36:52.5795989Z Oct 31 10:36:52 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-10-31T10:36:52.5796729Z Oct 31 10:36:52 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
2022-10-31T10:36:52.5797394Z Oct 31 10:36:52 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
2022-10-31T10:36:52.5798089Z Oct 31 10:36:52 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2022-10-31T10:36:52.5798780Z Oct 31 10:36:52 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2022-10-31T10:36:52.5799461Z Oct 31 10:36:52 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-10-31T10:36:52.5800100Z Oct 31 10:36:52 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-10-31T10:36:52.5800659Z Oct 31 10:36:52 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-10-31T10:36:52.5801204Z Oct 31 10:36:52 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-10-31T10:36:52.5801782Z Oct 31 10:36:52 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-10-31T10:36:52.5802373Z Oct 31 10:36:52 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-10-31T10:36:52.5802960Z Oct 31 10:36:52 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-10-31T10:36:52.5803531Z Oct 31 10:36:52 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-10-31T10:36:52.5804146Z Oct 31 10:36:52 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-10-31T10:36:52.5804654Z Oct 31 10:36:52 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-10-31T10:36:52.5805178Z Oct 31 10:36:52 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-10-31T10:36:52.5805734Z Oct 31 10:36:52 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-10-31T10:36:52.5806261Z Oct 31 10:36:52 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-10-31T10:36:52.5806768Z Oct 31 10:36:52 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-10-31T10:36:52.5807283Z Oct 31 10:36:52 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-10-31T10:36:52.5807757Z Oct 31 10:36:52 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-10-31T10:36:52.5808293Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-10-31T10:36:52.5808974Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-10-31T10:36:52.5809591Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-10-31T10:36:52.5810258Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-10-31T10:36:52.5811368Z Oct 31 10:36:52 Caused by: java.lang.RuntimeException: java.util.concurrent.CompletionException: java.lang.RuntimeException: Failed to reload lookup 'FULL' cache.
2022-10-31T10:36:52.5812174Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.LookupFullCache.getIfPresent(LookupFullCache.java:85)
2022-10-31T10:36:52.5813050Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.CachingLookupFunction.lookup(CachingLookupFunction.java:123)
2022-10-31T10:36:52.5813766Z Oct 31 10:36:52 	at org.apache.flink.table.functions.LookupFunction.eval(LookupFunction.java:52)
2022-10-31T10:36:52.5814306Z Oct 31 10:36:52 	at LookupFunction$173622.flatMap(Unknown Source)
2022-10-31T10:36:52.5814897Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.doFetch(LookupJoinRunner.java:92)
2022-10-31T10:36:52.5815974Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:79)
2022-10-31T10:36:52.5817125Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:34)
2022-10-31T10:36:52.5818114Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.ProcessOperator.processElement(ProcessOperator.java:66)
2022-10-31T10:36:52.5818851Z Oct 31 10:36:52 	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:99)
2022-10-31T10:36:52.5819688Z Oct 31 10:36:52 	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:80)
2022-10-31T10:36:52.5820440Z Oct 31 10:36:52 	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
2022-10-31T10:36:52.5821117Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
2022-10-31T10:36:52.5821798Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
2022-10-31T10:36:52.5822579Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
2022-10-31T10:36:52.5823416Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
2022-10-31T10:36:52.5824227Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
2022-10-31T10:36:52.5825006Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.run(FromElementsFunction.java:231)
2022-10-31T10:36:52.5825688Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
2022-10-31T10:36:52.5826474Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
2022-10-31T10:36:52.5827203Z Oct 31 10:36:52 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
2022-10-31T10:36:52.5828314Z Oct 31 10:36:52 Caused by: java.util.concurrent.CompletionException: java.lang.RuntimeException: Failed to reload lookup 'FULL' cache.
2022-10-31T10:36:52.5829081Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
2022-10-31T10:36:52.5829755Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
2022-10-31T10:36:52.5830482Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1643)
2022-10-31T10:36:52.5831117Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1632)
2022-10-31T10:36:52.5831732Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-10-31T10:36:52.5832335Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-10-31T10:36:52.5832927Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-10-31T10:36:52.5833542Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-10-31T10:36:52.5834331Z Oct 31 10:36:52 Caused by: java.lang.RuntimeException: Failed to reload lookup 'FULL' cache.
2022-10-31T10:36:52.5835306Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.CacheLoader.run(CacheLoader.java:121)
2022-10-31T10:36:52.5836012Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
2022-10-31T10:36:52.5836485Z Oct 31 10:36:52 	... 5 more
2022-10-31T10:36:52.5836904Z Oct 31 10:36:52 Caused by: java.lang.RuntimeException: Failed to create InputFormatCacheLoadTask
2022-10-31T10:36:52.5837668Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.createCacheLoadTask(InputFormatCacheLoader.java:137)
2022-10-31T10:36:52.5838837Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.lambda$reloadCache$0(InputFormatCacheLoader.java:84)
2022-10-31T10:36:52.5839754Z Oct 31 10:36:52 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-10-31T10:36:52.5840466Z Oct 31 10:36:52 	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
2022-10-31T10:36:52.5841085Z Oct 31 10:36:52 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-10-31T10:36:52.5841706Z Oct 31 10:36:52 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-10-31T10:36:52.5842320Z Oct 31 10:36:52 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
2022-10-31T10:36:52.5842941Z Oct 31 10:36:52 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-10-31T10:36:52.5843534Z Oct 31 10:36:52 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
2022-10-31T10:36:52.5844279Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.reloadCache(InputFormatCacheLoader.java:85)
2022-10-31T10:36:52.5845108Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.CacheLoader.run(CacheLoader.java:105)
2022-10-31T10:36:52.5845634Z Oct 31 10:36:52 	... 6 more
2022-10-31T10:36:52.5846343Z Oct 31 10:36:52 Caused by: java.lang.RuntimeException: Could not instantiate generated class 'KeyProjection$173616'
2022-10-31T10:36:52.5847018Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:74)
2022-10-31T10:36:52.5847875Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.keyselector.GenericRowDataKeySelector.open(GenericRowDataKeySelector.java:50)
2022-10-31T10:36:52.5848704Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputSplitCacheLoadTask.<init>(InputSplitCacheLoadTask.java:60)
2022-10-31T10:36:52.5849736Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.createCacheLoadTask(InputFormatCacheLoader.java:135)
2022-10-31T10:36:52.5850430Z Oct 31 10:36:52 	... 16 more
2022-10-31T10:36:52.5851049Z Oct 31 10:36:52 Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
2022-10-31T10:36:52.5851820Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:94)
2022-10-31T10:36:52.5852497Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:101)
2022-10-31T10:36:52.5853207Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:68)
2022-10-31T10:36:52.5853713Z Oct 31 10:36:52 	... 19 more
2022-10-31T10:36:52.5854630Z Oct 31 10:36:52 Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
2022-10-31T10:36:52.5855559Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)
2022-10-31T10:36:52.5856380Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
2022-10-31T10:36:52.5857126Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
2022-10-31T10:36:52.5857833Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92)
2022-10-31T10:36:52.5858325Z Oct 31 10:36:52 	... 21 more
2022-10-31T10:36:52.5858848Z Oct 31 10:36:52 Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
2022-10-31T10:36:52.5859653Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:107)
2022-10-31T10:36:52.5860451Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92)
2022-10-31T10:36:52.5861199Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
2022-10-31T10:36:52.5862000Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
2022-10-31T10:36:52.5862793Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
2022-10-31T10:36:52.5863574Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
2022-10-31T10:36:52.5864587Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
2022-10-31T10:36:52.5865127Z Oct 31 10:36:52 	... 24 more
2022-10-31T10:36:52.5866599Z Oct 31 10:36:52 Caused by: org.codehaus.janino.InternalCompilerException: Compiling ""KeyProjection$173616"": Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2022-10-31T10:36:52.5867747Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382)
2022-10-31T10:36:52.5868442Z Oct 31 10:36:52 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
2022-10-31T10:36:52.5869169Z Oct 31 10:36:52 	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
2022-10-31T10:36:52.5869792Z Oct 31 10:36:52 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
2022-10-31T10:36:52.5870422Z Oct 31 10:36:52 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
2022-10-31T10:36:52.5870977Z Oct 31 10:36:52 	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
2022-10-31T10:36:52.5871546Z Oct 31 10:36:52 	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
2022-10-31T10:36:52.5872163Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104)
2022-10-31T10:36:52.5872641Z Oct 31 10:36:52 	... 30 more
2022-10-31T10:36:52.5873934Z Oct 31 10:36:52 Caused by: java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2022-10-31T10:36:52.5875109Z Oct 31 10:36:52 	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:184)
2022-10-31T10:36:52.5875948Z Oct 31 10:36:52 	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:192)
2022-10-31T10:36:52.5876653Z Oct 31 10:36:52 	at java.lang.Class.forName0(Native Method)
2022-10-31T10:36:52.5877103Z Oct 31 10:36:52 	at java.lang.Class.forName(Class.java:348)
2022-10-31T10:36:52.5877678Z Oct 31 10:36:52 	at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:89)
2022-10-31T10:36:52.5878302Z Oct 31 10:36:52 	at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:312)
2022-10-31T10:36:52.5879007Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:8556)
2022-10-31T10:36:52.5879625Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6749)
2022-10-31T10:36:52.5880282Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594)
2022-10-31T10:36:52.5880881Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573)
2022-10-31T10:36:52.5881471Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215)
2022-10-31T10:36:52.5882097Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481)
2022-10-31T10:36:52.5882731Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476)
2022-10-31T10:36:52.5883330Z Oct 31 10:36:52 	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928)
2022-10-31T10:36:52.5883907Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476)
2022-10-31T10:36:52.5884494Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469)
2022-10-31T10:36:52.5885078Z Oct 31 10:36:52 	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927)
2022-10-31T10:36:52.5885645Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
2022-10-31T10:36:52.5886223Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.hasAnnotation(UnitCompiler.java:1365)
2022-10-31T10:36:52.5886846Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1349)
2022-10-31T10:36:52.5887487Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
2022-10-31T10:36:52.5888077Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
2022-10-31T10:36:52.5888650Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
2022-10-31T10:36:52.5889393Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
2022-10-31T10:36:52.5890079Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
2022-10-31T10:36:52.5890785Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
2022-10-31T10:36:52.5891462Z Oct 31 10:36:52 	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
2022-10-31T10:36:52.5892050Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
2022-10-31T10:36:52.5892642Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
2022-10-31T10:36:52.5893088Z Oct 31 10:36:52 	... 37 more
2022-10-31T10:36:52.5893363Z Oct 31 10:36:52 
2022-10-31T10:36:52.5893778Z Oct 31 10:36:52 [ERROR] LookupJoinITCase.testJoinTemporalTable  Time elapsed: 0.149 s  <<< ERROR!
2022-10-31T10:36:52.5894285Z Oct 31 10:36:52 java.lang.RuntimeException: Failed to fetch next result
2022-10-31T10:36:52.5894935Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
2022-10-31T10:36:52.5895747Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2022-10-31T10:36:52.5896561Z Oct 31 10:36:52 	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222)
2022-10-31T10:36:52.5897343Z Oct 31 10:36:52 	at java.util.Iterator.forEachRemaining(Iterator.java:115)
2022-10-31T10:36:52.5897914Z Oct 31 10:36:52 	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:115)
2022-10-31T10:36:52.5898589Z Oct 31 10:36:52 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:308)
2022-10-31T10:36:52.5899401Z Oct 31 10:36:52 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:144)
2022-10-31T10:36:52.5900168Z Oct 31 10:36:52 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:108)
2022-10-31T10:36:52.5900938Z Oct 31 10:36:52 	at org.apache.flink.table.planner.runtime.batch.sql.join.LookupJoinITCase.testJoinTemporalTable(LookupJoinITCase.scala:219)
2022-10-31T10:36:52.5901602Z Oct 31 10:36:52 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-10-31T10:36:52.5902154Z Oct 31 10:36:52 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-10-31T10:36:52.5902809Z Oct 31 10:36:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-10-31T10:36:52.5903400Z Oct 31 10:36:52 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-10-31T10:36:52.5903972Z Oct 31 10:36:52 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-10-31T10:36:52.5904634Z Oct 31 10:36:52 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-10-31T10:36:52.5905295Z Oct 31 10:36:52 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-10-31T10:36:52.5905933Z Oct 31 10:36:52 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-10-31T10:36:52.5906584Z Oct 31 10:36:52 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-10-31T10:36:52.5907216Z Oct 31 10:36:52 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-10-31T10:36:52.5907831Z Oct 31 10:36:52 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-10-31T10:36:52.5908421Z Oct 31 10:36:52 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-10-31T10:36:52.5909086Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-10-31T10:36:52.5909700Z Oct 31 10:36:52 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-10-31T10:36:52.5910451Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-10-31T10:36:52.5911065Z Oct 31 10:36:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-10-31T10:36:52.5911725Z Oct 31 10:36:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-10-31T10:36:52.5912316Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-10-31T10:36:52.5912881Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-10-31T10:36:52.5913461Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-10-31T10:36:52.5914021Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-10-31T10:36:52.5914598Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-10-31T10:36:52.5915162Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-10-31T10:36:52.5915672Z Oct 31 10:36:52 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-10-31T10:36:52.5916182Z Oct 31 10:36:52 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-10-31T10:36:52.5916704Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-10-31T10:36:52.5917252Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-10-31T10:36:52.5917826Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-10-31T10:36:52.5918469Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-10-31T10:36:52.5919118Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-10-31T10:36:52.5919711Z Oct 31 10:36:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-10-31T10:36:52.5920369Z Oct 31 10:36:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-10-31T10:36:52.5920921Z Oct 31 10:36:52 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-10-31T10:36:52.5921471Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-10-31T10:36:52.5922027Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-10-31T10:36:52.5922541Z Oct 31 10:36:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-10-31T10:36:52.5923057Z Oct 31 10:36:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-10-31T10:36:52.5923646Z Oct 31 10:36:52 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-10-31T10:36:52.5924299Z Oct 31 10:36:52 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-10-31T10:36:52.5924960Z Oct 31 10:36:52 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-10-31T10:36:52.5925655Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
2022-10-31T10:36:52.5926403Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
2022-10-31T10:36:52.5927158Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
2022-10-31T10:36:52.5927933Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
2022-10-31T10:36:52.5928737Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
2022-10-31T10:36:52.5929619Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
2022-10-31T10:36:52.5930373Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-10-31T10:36:52.5931100Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-10-31T10:36:52.5931809Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-10-31T10:36:52.5932576Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-10-31T10:36:52.5933339Z Oct 31 10:36:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
2022-10-31T10:36:52.5933991Z Oct 31 10:36:52 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2022-10-31T10:36:52.5934631Z Oct 31 10:36:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
2022-10-31T10:36:52.5935386Z Oct 31 10:36:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-10-31T10:36:52.5936116Z Oct 31 10:36:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2022-10-31T10:36:52.5936818Z Oct 31 10:36:52 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-10-31T10:36:52.5937478Z Oct 31 10:36:52 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-10-31T10:36:52.5938086Z Oct 31 10:36:52 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-10-31T10:36:52.5938703Z Oct 31 10:36:52 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-10-31T10:36:52.5939435Z Oct 31 10:36:52 Caused by: java.io.IOException: Failed to fetch job execution result
2022-10-31T10:36:52.5940153Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
2022-10-31T10:36:52.5940940Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
2022-10-31T10:36:52.5941750Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2022-10-31T10:36:52.5942334Z Oct 31 10:36:52 	... 67 more
2022-10-31T10:36:52.5942840Z Oct 31 10:36:52 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-10-31T10:36:52.5943518Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-10-31T10:36:52.5944140Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2022-10-31T10:36:52.5944848Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
2022-10-31T10:36:52.5945415Z Oct 31 10:36:52 	... 69 more
2022-10-31T10:36:52.5945864Z Oct 31 10:36:52 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-10-31T10:36:52.5946478Z Oct 31 10:36:52 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-10-31T10:36:52.5947233Z Oct 31 10:36:52 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
2022-10-31T10:36:52.5947955Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-10-31T10:36:52.5948577Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2022-10-31T10:36:52.5949307Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2022-10-31T10:36:52.5950057Z Oct 31 10:36:52 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:138)
2022-10-31T10:36:52.5950866Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:181)
2022-10-31T10:36:52.5951490Z Oct 31 10:36:52 	... 69 more
2022-10-31T10:36:52.5951967Z Oct 31 10:36:52 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-10-31T10:36:52.5952717Z Oct 31 10:36:52 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
2022-10-31T10:36:52.5953584Z Oct 31 10:36:52 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
2022-10-31T10:36:52.5954396Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256)
2022-10-31T10:36:52.5955108Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247)
2022-10-31T10:36:52.5955793Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
2022-10-31T10:36:52.5956514Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:739)
2022-10-31T10:36:52.5957242Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:716)
2022-10-31T10:36:52.5957934Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)
2022-10-31T10:36:52.5958631Z Oct 31 10:36:52 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479)
2022-10-31T10:36:52.5959378Z Oct 31 10:36:52 	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
2022-10-31T10:36:52.5960033Z Oct 31 10:36:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-10-31T10:36:52.5960609Z Oct 31 10:36:52 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-10-31T10:36:52.5961233Z Oct 31 10:36:52 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
2022-10-31T10:36:52.5961996Z Oct 31 10:36:52 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-10-31T10:36:52.5962722Z Oct 31 10:36:52 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
2022-10-31T10:36:52.5963405Z Oct 31 10:36:52 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
2022-10-31T10:36:52.5964104Z Oct 31 10:36:52 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2022-10-31T10:36:52.5964785Z Oct 31 10:36:52 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2022-10-31T10:36:52.5965392Z Oct 31 10:36:52 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-10-31T10:36:52.5965956Z Oct 31 10:36:52 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-10-31T10:36:52.5966507Z Oct 31 10:36:52 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-10-31T10:36:52.5967067Z Oct 31 10:36:52 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-10-31T10:36:52.5967643Z Oct 31 10:36:52 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-10-31T10:36:52.5968220Z Oct 31 10:36:52 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-10-31T10:36:52.5968810Z Oct 31 10:36:52 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-10-31T10:36:52.5969483Z Oct 31 10:36:52 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-10-31T10:36:52.5970098Z Oct 31 10:36:52 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-10-31T10:36:52.5970603Z Oct 31 10:36:52 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-10-31T10:36:52.5971141Z Oct 31 10:36:52 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-10-31T10:36:52.5971753Z Oct 31 10:36:52 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-10-31T10:36:52.5972286Z Oct 31 10:36:52 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-10-31T10:36:52.5972806Z Oct 31 10:36:52 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-10-31T10:36:52.5973303Z Oct 31 10:36:52 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-10-31T10:36:52.5973789Z Oct 31 10:36:52 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-10-31T10:36:52.5974320Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-10-31T10:36:52.5974907Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-10-31T10:36:52.5975515Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-10-31T10:36:52.5976117Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-10-31T10:36:52.5977185Z Oct 31 10:36:52 Caused by: java.lang.RuntimeException: java.util.concurrent.CompletionException: java.lang.RuntimeException: Failed to reload lookup 'FULL' cache.
2022-10-31T10:36:52.5978005Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.LookupFullCache.getIfPresent(LookupFullCache.java:85)
2022-10-31T10:36:52.5978816Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.CachingLookupFunction.lookup(CachingLookupFunction.java:123)
2022-10-31T10:36:52.5979625Z Oct 31 10:36:52 	at org.apache.flink.table.functions.LookupFunction.eval(LookupFunction.java:52)
2022-10-31T10:36:52.5980299Z Oct 31 10:36:52 	at LookupFunction$173667.flatMap(Unknown Source)
2022-10-31T10:36:52.5980902Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.doFetch(LookupJoinRunner.java:92)
2022-10-31T10:36:52.5981665Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:79)
2022-10-31T10:36:52.5982425Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:34)
2022-10-31T10:36:52.5983174Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.ProcessOperator.processElement(ProcessOperator.java:66)
2022-10-31T10:36:52.5983887Z Oct 31 10:36:52 	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:99)
2022-10-31T10:36:52.5984564Z Oct 31 10:36:52 	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:80)
2022-10-31T10:36:52.5985247Z Oct 31 10:36:52 	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
2022-10-31T10:36:52.5985922Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
2022-10-31T10:36:52.5986585Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
2022-10-31T10:36:52.5987366Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
2022-10-31T10:36:52.5988216Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
2022-10-31T10:36:52.5989111Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
2022-10-31T10:36:52.5989941Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.run(FromElementsFunction.java:231)
2022-10-31T10:36:52.5990647Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
2022-10-31T10:36:52.5991298Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
2022-10-31T10:36:52.5992002Z Oct 31 10:36:52 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
2022-10-31T10:36:52.5993055Z Oct 31 10:36:52 Caused by: java.util.concurrent.CompletionException: java.lang.RuntimeException: Failed to reload lookup 'FULL' cache.
2022-10-31T10:36:52.5993733Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
2022-10-31T10:36:52.5994378Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
2022-10-31T10:36:52.5995034Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1643)
2022-10-31T10:36:52.5995687Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1632)
2022-10-31T10:36:52.5996297Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-10-31T10:36:52.5996884Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-10-31T10:36:52.5997489Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-10-31T10:36:52.5998102Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-10-31T10:36:52.5998852Z Oct 31 10:36:52 Caused by: java.lang.RuntimeException: Failed to reload lookup 'FULL' cache.
2022-10-31T10:36:52.5999594Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.CacheLoader.run(CacheLoader.java:121)
2022-10-31T10:36:52.6000332Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
2022-10-31T10:36:52.6000882Z Oct 31 10:36:52 	... 5 more
2022-10-31T10:36:52.6001319Z Oct 31 10:36:52 Caused by: java.lang.RuntimeException: Failed to create InputFormatCacheLoadTask
2022-10-31T10:36:52.6002081Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.createCacheLoadTask(InputFormatCacheLoader.java:137)
2022-10-31T10:36:52.6003036Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.lambda$reloadCache$0(InputFormatCacheLoader.java:84)
2022-10-31T10:36:52.6003830Z Oct 31 10:36:52 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-10-31T10:36:52.6004461Z Oct 31 10:36:52 	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
2022-10-31T10:36:52.6005063Z Oct 31 10:36:52 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-10-31T10:36:52.6005676Z Oct 31 10:36:52 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-10-31T10:36:52.6006299Z Oct 31 10:36:52 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
2022-10-31T10:36:52.6006891Z Oct 31 10:36:52 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-10-31T10:36:52.6007488Z Oct 31 10:36:52 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
2022-10-31T10:36:52.6008244Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.reloadCache(InputFormatCacheLoader.java:85)
2022-10-31T10:36:52.6009168Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.CacheLoader.run(CacheLoader.java:105)
2022-10-31T10:36:52.6009680Z Oct 31 10:36:52 	... 6 more
2022-10-31T10:36:52.6010443Z Oct 31 10:36:52 Caused by: java.lang.RuntimeException: Could not instantiate generated class 'KeyProjection$173661'
2022-10-31T10:36:52.6011113Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:74)
2022-10-31T10:36:52.6011840Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.keyselector.GenericRowDataKeySelector.open(GenericRowDataKeySelector.java:50)
2022-10-31T10:36:52.6012894Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputSplitCacheLoadTask.<init>(InputSplitCacheLoadTask.java:60)
2022-10-31T10:36:52.6013868Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.createCacheLoadTask(InputFormatCacheLoader.java:135)
2022-10-31T10:36:52.6014599Z Oct 31 10:36:52 	... 16 more
2022-10-31T10:36:52.6015253Z Oct 31 10:36:52 Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
2022-10-31T10:36:52.6016074Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:94)
2022-10-31T10:36:52.6016782Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:101)
2022-10-31T10:36:52.6017490Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:68)
2022-10-31T10:36:52.6018020Z Oct 31 10:36:52 	... 19 more
2022-10-31T10:36:52.6018749Z Oct 31 10:36:52 Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
2022-10-31T10:36:52.6019804Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)
2022-10-31T10:36:52.6020718Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
2022-10-31T10:36:52.6021576Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
2022-10-31T10:36:52.6022650Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92)
2022-10-31T10:36:52.6023350Z Oct 31 10:36:52 	... 21 more
2022-10-31T10:36:52.6023920Z Oct 31 10:36:52 Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
2022-10-31T10:36:52.6024653Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:107)
2022-10-31T10:36:52.6025358Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92)
2022-10-31T10:36:52.6026139Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
2022-10-31T10:36:52.6026984Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
2022-10-31T10:36:52.6027822Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
2022-10-31T10:36:52.6028608Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
2022-10-31T10:36:52.6029500Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
2022-10-31T10:36:52.6030147Z Oct 31 10:36:52 	... 24 more
2022-10-31T10:36:52.6031805Z Oct 31 10:36:52 Caused by: org.codehaus.janino.InternalCompilerException: Compiling ""KeyProjection$173661"": Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2022-10-31T10:36:52.6032942Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382)
2022-10-31T10:36:52.6033536Z Oct 31 10:36:52 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
2022-10-31T10:36:52.6034154Z Oct 31 10:36:52 	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
2022-10-31T10:36:52.6034754Z Oct 31 10:36:52 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
2022-10-31T10:36:52.6035453Z Oct 31 10:36:52 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
2022-10-31T10:36:52.6036031Z Oct 31 10:36:52 	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
2022-10-31T10:36:52.6036599Z Oct 31 10:36:52 	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
2022-10-31T10:36:52.6037202Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104)
2022-10-31T10:36:52.6037701Z Oct 31 10:36:52 	... 30 more
2022-10-31T10:36:52.6039060Z Oct 31 10:36:52 Caused by: java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2022-10-31T10:36:52.6040273Z Oct 31 10:36:52 	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:184)
2022-10-31T10:36:52.6041119Z Oct 31 10:36:52 	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:192)
2022-10-31T10:36:52.6041755Z Oct 31 10:36:52 	at java.lang.Class.forName0(Native Method)
2022-10-31T10:36:52.6042202Z Oct 31 10:36:52 	at java.lang.Class.forName(Class.java:348)
2022-10-31T10:36:52.6042760Z Oct 31 10:36:52 	at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:89)
2022-10-31T10:36:52.6043496Z Oct 31 10:36:52 	at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:312)
2022-10-31T10:36:52.6044095Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:8556)
2022-10-31T10:36:52.6044695Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6749)
2022-10-31T10:36:52.6045315Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594)
2022-10-31T10:36:52.6045912Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573)
2022-10-31T10:36:52.6046485Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215)
2022-10-31T10:36:52.6047113Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481)
2022-10-31T10:36:52.6047762Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476)
2022-10-31T10:36:52.6048349Z Oct 31 10:36:52 	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928)
2022-10-31T10:36:52.6049024Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476)
2022-10-31T10:36:52.6049633Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469)
2022-10-31T10:36:52.6050298Z Oct 31 10:36:52 	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927)
2022-10-31T10:36:52.6050871Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
2022-10-31T10:36:52.6051465Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.hasAnnotation(UnitCompiler.java:1365)
2022-10-31T10:36:52.6052068Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1349)
2022-10-31T10:36:52.6052706Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
2022-10-31T10:36:52.6053313Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
2022-10-31T10:36:52.6053892Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
2022-10-31T10:36:52.6054463Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
2022-10-31T10:36:52.6055106Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
2022-10-31T10:36:52.6055811Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
2022-10-31T10:36:52.6056530Z Oct 31 10:36:52 	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
2022-10-31T10:36:52.6057130Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
2022-10-31T10:36:52.6057712Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
2022-10-31T10:36:52.6058146Z Oct 31 10:36:52 	... 37 more
{code};;;","01/Nov/22 09:00;Weijie Guo;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42693&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21195;;;","02/Nov/22 15:39;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42734&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21329;;;","04/Nov/22 07:57;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42802&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=17638;;;","07/Nov/22 08:23;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42863&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21328;;;","07/Nov/22 09:01;chesnay;I'm wondering if this is due to bad locking in the {{CacheLoader}}; since the setting and checking of {{isStopped}} in {{CacheLoader#run/close}} don't run under the lock it's possible for a cache reload to run while the loader was shut down already.;;;","08/Nov/22 07:28;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42910&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=17971;;;","11/Nov/22 09:10;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43035&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21693;;;","14/Nov/22 03:40;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43089&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94;;;","14/Nov/22 03:41;leonard;[~fsk119] Could you take a look this issue?;;;","21/Nov/22 03:22;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43309&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9;;;","21/Nov/22 09:20;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43270&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","21/Nov/22 09:24;leonard;I changed this ticket priority to blocker as too many failures happens.  
Could you also help take a look [~lsy] ？;;;","21/Nov/22 09:30;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43254&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9;;;","22/Nov/22 07:57;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43221&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=22265;;;","22/Nov/22 12:41;smiralex;Hi everyone! First of all, I'm very sorry for causing so much troubles with this unstable test to you. Unfortunately, I didn't succeed to reproduce this error locally, so I can only assume what can be the cause of the problem. I tried to fix this problem {color:#000000}[once|https://github.com/apache/flink/pull/20734]{color}, but the core of it still was not solved. Now I suppose, that the core can lie in that {_}{color:#000000}Thread{color}.currentThread{color:#871094}(){color}.getContextClassLoader{_}{color:#871094}_()_ {color}was returning wrong ClassLoader, because it was executed in separate thread, so I changed it in this {color:#000000}[PR|https://github.com/apache/flink/pull/21365] {color} _-_  now Projection class is being compiled just one time in main thread, so ClassLoader leak can't occur anymore. {color:#000000}Please read the description of this PR where I tried to explain my thoughts about this problem. I hope this fix will eventually close this ticket, and I'm waiting for someone's review! {color};;;","23/Nov/22 02:16;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43380&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=26173;;;","23/Nov/22 08:44;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43400&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=17698;;;","30/Nov/22 15:32;mapohl;aaand we're back:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43615&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21327;;;","01/Dec/22 11:24;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43642&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21331;;;","02/Dec/22 10:13;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43649&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=17978;;;","05/Dec/22 10:31;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43694&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=17846;;;","06/Dec/22 08:34;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43744&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=17982;;;","08/Dec/22 11:54;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43794&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=17703;;;","08/Dec/22 11:57;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43794&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=17219;;;","09/Dec/22 15:55;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43824&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&s=ae4f8708-9994-57d3-c2d7-b892156e7812&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=17528;;;","10/Dec/22 12:24;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43850&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","10/Dec/22 12:32;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43850&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9;;;","13/Dec/22 08:25;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43910&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21913;;;","13/Dec/22 11:19;yunta;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43919&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","16/Dec/22 08:27;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43983&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=17301;;;","19/Dec/22 09:52;mapohl;2x in same build:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44056&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=21055
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44056&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21256;;;","20/Dec/22 07:46;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44084&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=17551;;;","22/Dec/22 08:43;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44162&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=17978;;;","02/Jan/23 09:49;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44285&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=17978;;;","02/Jan/23 11:00;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44374&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=21863;;;","05/Jan/23 07:40;mapohl;Same build, 2 jobs failed:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44443&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=21863
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44443&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=17978;;;","06/Jan/23 07:49;mapohl;Same build, 2 jobs failed:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44525&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=20486
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44525&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=20741;;;","06/Jan/23 07:55;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44526&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=17619;;;","09/Jan/23 08:10;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44569&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=20249;;;","10/Jan/23 13:00;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44657&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=25822;;;","11/Jan/23 08:40;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44683&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=18524;;;","11/Jan/23 09:29;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44690&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=18068;;;","11/Jan/23 12:54;rmetzger;[~smiralex] what's the status of this issue?;;;","11/Jan/23 14:35;mapohl;[~renqs] is planning to pick up on that one.;;;","12/Jan/23 12:35;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44752&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21788;;;","13/Jan/23 11:03;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44777&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=21750;;;","13/Jan/23 13:16;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44782&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21788;;;","16/Jan/23 09:52;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44863&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=18389;;;","17/Jan/23 15:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44941&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=22314;;;","17/Jan/23 15:51;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44965&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=26411;;;","19/Jan/23 13:34;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45024&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=18557;;;","19/Jan/23 13:44;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45039&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=16184;;;","20/Jan/23 08:15;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45099&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=18009;;;","27/Jan/23 07:32;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45220&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=16759;;;","30/Jan/23 07:30;lincoln.86xy;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45368&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","30/Jan/23 14:02;mapohl;[~renqs] this is still an open blocker. Have you had the chance to work on it? Or shall we assign someone else to it? (for now, I changed the assignee from [~smiralex] to Qingsheng because that's the latest state we agreed on);;;","31/Jan/23 00:52;JunRuiLi;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45438&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","02/Feb/23 09:01;mapohl;same run, multiple times:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45588&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=17624]
 * https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45588&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=17624;;;","03/Feb/23 04:04;renqs;Fixed on master: 0516427d198420fa28d586ed052ef26daf2845ba;;;","03/Feb/23 10:14;mapohl;Reopening this issue: Shouldn't we provide a 1.16 backport PR as well?;;;","03/Feb/23 10:18;leonard;[~mapohl] Of course yes, I add 1.16.2 to fix versions;;;","06/Feb/23 08:23;renqs;Thanks for the reminder [~mapohl] [~leonard] !

Backported to 1.16: 0211d5b3ad3d26d8a70fd2d0234ffbd690483e2f;;;","09/Feb/23 12:56;stayrascal;May I check what use case will meet this problem, does it means LookupJoin will not working well in 1.16.1?;;;","09/Feb/23 13:11;martijnvisser;[~stayrascal] This was a test stability, not a user reported issue. ;;;"
Hybrid full spilling strategy triggering spilling frequently,FLINK-29425,13483315,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,27/Sep/22 04:21,28/Sep/22 08:50,13/Jul/23 08:13,28/Sep/22 08:50,1.16.0,,,,,,,1.16.0,,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,"In hybrid shuffle mode, we have an internal config option 'DEFAULT_FULL_STRATEGY_NUM_BUFFERS_TRIGGER_SPILLED' to control spilling frequency in the full spilling strategy. Unfortunately, the default value(10) is too small. As a result, frequent disk spilling calls are observed in the TPC-DS test, which seriously affects performance. When we increase the value, the query performance is improved significantly. We should set a more reasonable default value, or adopt an adaptive strategy to determine the spilling frequency.",,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 28 08:50:03 UTC 2022,,,,,,,,,,"0|z18uzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/22 08:50;xtsong;- master (1.17): fe3bdcf0182ded35f1a8ac6eee4d7bf7acec832d
- release-1.16: 790434b9fb7296420c1ea15af0d640273776d0b0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobDetails is incorrect OpenAPI spec ,FLINK-29423,13483258,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,26/Sep/22 18:16,22/Nov/22 08:22,13/Jul/23 08:13,22/Nov/22 08:22,1.16.0,,,,,,,1.17.0,,,,,Documentation,Runtime / REST,,,,,,0,pull-request-available,,,,"The JobDetails use custom serialization, but the introspection ignores that and analyzes the class as-is, resulting in various fields being documented that shouldn't be.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 22 08:22:58 UTC 2022,,,,,,,,,,"0|z18un4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/22 08:22;chesnay;master: ec89c17da5d555ca44f89d3f8739fa9ae00734b7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HybridShuffleITCase.testHybridFullExchangesRestart hangs,FLINK-29419,13483201,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,hxbks2ks,hxbks2ks,26/Sep/22 12:55,13/Dec/22 04:39,13/Jul/23 08:13,13/Dec/22 04:39,1.16.0,1.17.0,,,,,,1.16.1,1.17.0,,,,Runtime / Network,,,,,,,0,pull-request-available,test-stability,,,"{code:java}
2022-09-26T10:56:44.0766792Z Sep 26 10:56:44 ""ForkJoinPool-1-worker-25"" #27 daemon prio=5 os_prio=0 tid=0x00007f41a4efa000 nid=0x6d76 waiting on condition [0x00007f40ac135000]
2022-09-26T10:56:44.0767432Z Sep 26 10:56:44    java.lang.Thread.State: WAITING (parking)
2022-09-26T10:56:44.0767892Z Sep 26 10:56:44 	at sun.misc.Unsafe.park(Native Method)
2022-09-26T10:56:44.0768644Z Sep 26 10:56:44 	- parking to wait for  <0x00000000a0704e18> (a java.util.concurrent.CompletableFuture$Signaller)
2022-09-26T10:56:44.0769287Z Sep 26 10:56:44 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2022-09-26T10:56:44.0769949Z Sep 26 10:56:44 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2022-09-26T10:56:44.0770623Z Sep 26 10:56:44 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)
2022-09-26T10:56:44.0771349Z Sep 26 10:56:44 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2022-09-26T10:56:44.0772092Z Sep 26 10:56:44 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2022-09-26T10:56:44.0772777Z Sep 26 10:56:44 	at org.apache.flink.test.runtime.JobGraphRunningUtil.execute(JobGraphRunningUtil.java:57)
2022-09-26T10:56:44.0773534Z Sep 26 10:56:44 	at org.apache.flink.test.runtime.BatchShuffleITCaseBase.executeJob(BatchShuffleITCaseBase.java:115)
2022-09-26T10:56:44.0774333Z Sep 26 10:56:44 	at org.apache.flink.test.runtime.HybridShuffleITCase.testHybridFullExchangesRestart(HybridShuffleITCase.java:59)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41343&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7
",,dwysakowicz,hxb,hxbks2ks,mapohl,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29682,FLINK-30189,FLINK-29298,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 13 04:39:01 UTC 2022,,,,,,,,,,"0|z18ub4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 12:56;hxbks2ks;cc [~Weijie Guo];;;","26/Sep/22 12:58;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41345&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","26/Sep/22 12:59;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41345&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","26/Sep/22 13:02;Weijie Guo;[~hxbks2ks] Thanks for the feedback. I will check the cause of this problem.;;;","27/Sep/22 06:05;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41367&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9;;;","30/Sep/22 14:35;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41494&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=11782;;;","03/Oct/22 13:49;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41535&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae;;;","05/Oct/22 12:14;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41564&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=9625;;;","06/Oct/22 06:36;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41620&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12048;;;","11/Oct/22 02:07;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41837&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","17/Oct/22 03:52;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42061&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca;;;","17/Oct/22 03:53;hxb;[~Weijie Guo]  Is any update on this issue?;;;","17/Oct/22 08:18;Weijie Guo;The specific reason needs to be further investigated. In order not to block flink code merge pipeline, temporarily disable them. I will reactivate these two tests when we find the reason.;;;","17/Oct/22 08:35;hxb;Temporarily disabled on master via fca71d237b2440c0129ef7e6f8266f4091df4884

Temporarily disabled on master via 5d03327d016c6e3250a82566ff5530c65cf5d344;;;","18/Oct/22 08:27;hxb;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42115&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae]

HybridShuffleITCase.testHybridSelectiveExchanges hangs too
{code:java}
3:03:31.9738556Z Oct 18 03:03:31 ""ForkJoinPool-1-worker-51"" #28 daemon prio=5 os_prio=0 cpu=5134.15ms elapsed=3370.05s tid=0x00007f03f4dad000 nid=0x4f71 waiting on condition  [0x00007f03c8c12000]
2022-10-18T03:03:31.9739287Z Oct 18 03:03:31    java.lang.Thread.State: WAITING (parking)
2022-10-18T03:03:31.9739815Z Oct 18 03:03:31 	at jdk.internal.misc.Unsafe.park(java.base@11.0.11/Native Method)
2022-10-18T03:03:31.9740662Z Oct 18 03:03:31 	- parking to wait for  <0x00000000a2748288> (a java.util.concurrent.CompletableFuture$Signaller)
2022-10-18T03:03:31.9741425Z Oct 18 03:03:31 	at java.util.concurrent.locks.LockSupport.park(java.base@11.0.11/LockSupport.java:194)
2022-10-18T03:03:31.9742584Z Oct 18 03:03:31 	at java.util.concurrent.CompletableFuture$Signaller.block(java.base@11.0.11/CompletableFuture.java:1796)
2022-10-18T03:03:31.9743340Z Oct 18 03:03:31 	at java.util.concurrent.ForkJoinPool.managedBlock(java.base@11.0.11/ForkJoinPool.java:3118)
2022-10-18T03:03:31.9744059Z Oct 18 03:03:31 	at java.util.concurrent.CompletableFuture.waitingGet(java.base@11.0.11/CompletableFuture.java:1823)
2022-10-18T03:03:31.9744783Z Oct 18 03:03:31 	at java.util.concurrent.CompletableFuture.get(java.base@11.0.11/CompletableFuture.java:1998)
2022-10-18T03:03:31.9745501Z Oct 18 03:03:31 	at org.apache.flink.test.runtime.JobGraphRunningUtil.execute(JobGraphRunningUtil.java:57)
2022-10-18T03:03:31.9746297Z Oct 18 03:03:31 	at org.apache.flink.test.runtime.BatchShuffleITCaseBase.executeJob(BatchShuffleITCaseBase.java:115)
2022-10-18T03:03:31.9747132Z Oct 18 03:03:31 	at org.apache.flink.test.runtime.HybridShuffleITCase.testHybridSelectiveExchanges(HybridShuffleITCase.java:51) {code};;;","19/Oct/22 03:23;hxb;Temporarily disabled `HybridShuffleITCase` on master via 426c39107f434da5805bef0ff84f95912098465b

Temporarily disabled `HybridShuffleITCase` on release-1.16 via 20eb0168aa602d1b7a6b8dd116ddd1abeac8dc5e;;;","24/Nov/22 09:16;Weijie Guo;Through further investigation, we found two possible causes. One is the problem of hybrid result partition may loading data from the file that already consumed from memory, and the other is the bug in the LocalBufferPool (hybrid shuffle scene greatly increases the probability of recurrence).

After these two tickets are resolved, we should be able to enable the relevant tests.;;;","13/Dec/22 04:39;xtsong;Both FLINK-29298 and FLINK-30189 have been fixed.

Re-enabled HybridShuffleITCase in:
- master (1.17): 1a2f638503d5364edbe947b61e29b62b981bb2e8
- release-1.16: ad7ac1decbe97e12e203e294cfb58deeea866aca

Closing the ticket. Feel free to re-open if that happens again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connection leak in orc reader,FLINK-29412,13483164,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,26/Sep/22 09:17,26/Sep/22 13:04,13/Jul/23 08:13,26/Sep/22 13:04,,,,,,,,table-store-0.2.1,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,"1. OrcFileStatsExtractor forget closing reader.
2. HadoopReadOnlyFileSystem forget closing fsDataInputStream.

We need a pocket test to assert all connections are closed. ",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 26 13:04:53 UTC 2022,,,,,,,,,,"0|z18u2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 13:04;lzljs3620320;master: 29fc9adf023a04b02126f50d915feb55fdbc2327
release-0.2: 311ab67dd2d7dcb4a7a18bff1de6a2da5861964e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveCatalogITCase failed with NPE,FLINK-29408,13483098,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,luoyuxia,hxbks2ks,hxbks2ks,26/Sep/22 03:02,20/Oct/22 02:34,13/Jul/23 08:13,11/Oct/22 11:27,1.16.0,1.17.0,,,,,,1.16.0,,,,,Connectors / Hive,,,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-09-25T03:41:07.4212129Z Sep 25 03:41:07 [ERROR] org.apache.flink.table.catalog.hive.HiveCatalogUdfITCase.testFlinkUdf  Time elapsed: 0.098 s  <<< ERROR!
2022-09-25T03:41:07.4212662Z Sep 25 03:41:07 java.lang.NullPointerException
2022-09-25T03:41:07.4213189Z Sep 25 03:41:07 	at org.apache.flink.table.catalog.hive.HiveCatalogUdfITCase.testFlinkUdf(HiveCatalogUdfITCase.java:109)
2022-09-25T03:41:07.4213753Z Sep 25 03:41:07 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-09-25T03:41:07.4224643Z Sep 25 03:41:07 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-09-25T03:41:07.4225311Z Sep 25 03:41:07 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-09-25T03:41:07.4225879Z Sep 25 03:41:07 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-09-25T03:41:07.4226405Z Sep 25 03:41:07 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-09-25T03:41:07.4227201Z Sep 25 03:41:07 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-09-25T03:41:07.4227807Z Sep 25 03:41:07 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-09-25T03:41:07.4228394Z Sep 25 03:41:07 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-09-25T03:41:07.4228966Z Sep 25 03:41:07 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-09-25T03:41:07.4229514Z Sep 25 03:41:07 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-09-25T03:41:07.4230066Z Sep 25 03:41:07 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-09-25T03:41:07.4230587Z Sep 25 03:41:07 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-09-25T03:41:07.4231258Z Sep 25 03:41:07 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-09-25T03:41:07.4231823Z Sep 25 03:41:07 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-09-25T03:41:07.4232384Z Sep 25 03:41:07 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-09-25T03:41:07.4232930Z Sep 25 03:41:07 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-09-25T03:41:07.4233511Z Sep 25 03:41:07 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-09-25T03:41:07.4234039Z Sep 25 03:41:07 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-09-25T03:41:07.4234546Z Sep 25 03:41:07 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-09-25T03:41:07.4235057Z Sep 25 03:41:07 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-09-25T03:41:07.4235573Z Sep 25 03:41:07 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-09-25T03:41:07.4236087Z Sep 25 03:41:07 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-09-25T03:41:07.4236635Z Sep 25 03:41:07 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-09-25T03:41:07.4237314Z Sep 25 03:41:07 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-09-25T03:41:07.4238211Z Sep 25 03:41:07 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-09-25T03:41:07.4238775Z Sep 25 03:41:07 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-09-25T03:41:07.4239277Z Sep 25 03:41:07 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-09-25T03:41:07.4239769Z Sep 25 03:41:07 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-09-25T03:41:07.4240265Z Sep 25 03:41:07 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-09-25T03:41:07.4240731Z Sep 25 03:41:07 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-09-25T03:41:07.4241196Z Sep 25 03:41:07 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-09-25T03:41:07.4241715Z Sep 25 03:41:07 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-09-25T03:41:07.4242316Z Sep 25 03:41:07 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-09-25T03:41:07.4242904Z Sep 25 03:41:07 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-09-25T03:41:07.4243528Z Sep 25 03:41:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-09-25T03:41:07.4244201Z Sep 25 03:41:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-09-25T03:41:07.4244883Z Sep 25 03:41:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-09-25T03:41:07.4245801Z Sep 25 03:41:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-09-25T03:41:07.4246600Z Sep 25 03:41:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-09-25T03:41:07.4247226Z Sep 25 03:41:07 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-09-25T03:41:07.4247808Z Sep 25 03:41:07 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-09-25T03:41:07.4248449Z Sep 25 03:41:07 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-09-25T03:41:07.4249567Z Sep 25 03:41:07 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-09-25T03:41:07.4250222Z Sep 25 03:41:07 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-09-25T03:41:07.4250889Z Sep 25 03:41:07 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-09-25T03:41:07.4251559Z Sep 25 03:41:07 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-09-25T03:41:07.4252193Z Sep 25 03:41:07 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-09-25T03:41:07.4252776Z Sep 25 03:41:07 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-09-25T03:41:07.4253335Z Sep 25 03:41:07 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-09-25T03:41:07.4253884Z Sep 25 03:41:07 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41316&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199",,dwysakowicz,hxbks2ks,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29407,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 11 11:27:13 UTC 2022,,,,,,,,,,"0|z18too:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/22 06:02;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41366&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199;;;","27/Sep/22 06:06;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41367&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199;;;","27/Sep/22 06:20;hxbks2ks;cc [~luoyuxia];;;","28/Sep/22 07:09;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41396&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24932;;;","28/Sep/22 07:10;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41397&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=26033;;;","28/Sep/22 09:35;luoyuxia;I try to debug the failure in this pr [https://github.com/apache/flink/pull/20905].

I found when I just change the parameters for CI:

 
{code:java}
test_pool_definition: name: Default{code}
 

to 

 
{code:java}
test_pool_definition: vmImage: 'ubuntu-20.04' 
{code}
 

 

The ci will fail [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41403&view=results].

But when I revert such changes, it pass again [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41415&view=results]

I'm confused about it. [~hxbks2ks] Do you know what's the difference between `name: Default` and `vmImage: 'ubuntu-20.04' `?;;;","28/Sep/22 11:56;chesnay;If you change the {{test_pool_definition}} you run the tests on azure machines, and not our CI machines. Naturally you shouldn't do that.

Note that it may just be a timing thing. I saw plenty of those NPEs in my personal azure which also uses azure machines.;;;","06/Oct/22 08:56;dwysakowicz;It fails reliable most of the time on my private Azure. Any progress on that?

https://dev.azure.com/wysakowiczdawid/Flink/_build/results?buildId=1531&view=logs&j=f3dc9b18-b77a-55c1-591e-264c46fe44d1&t=2d3cd81e-1c37-5c31-0ee4-f5d5cdb9324d&l=25733;;;","11/Oct/22 11:27;chesnay;master: 23e027230838ac5728b2974f02235f512c97010c
1.16: 4403ea5811d9d1edc5896818081005c4d6237efa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InputFormatCacheLoaderTest is unstable,FLINK-29405,13483080,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,renqs,chesnay,chesnay,25/Sep/22 17:23,06/Feb/23 05:53,13/Jul/23 08:13,06/Feb/23 02:14,1.16.0,1.17.0,,,,,,1.16.2,1.17.0,,,,Table SQL / Runtime,,,,,,,0,pull-request-available,test-stability,,,"#testExceptionDuringReload/#testCloseAndInterruptDuringReload fail reliably when run in a loop.

{code}
java.lang.AssertionError: 
Expecting AtomicInteger(0) to have value:
  0
but did not.

	at org.apache.flink.table.runtime.functions.table.fullcache.inputformat.InputFormatCacheLoaderTest.testCloseAndInterruptDuringReload(InputFormatCacheLoaderTest.java:161)
{code}",,fsk119,hxb,hxbks2ks,JunRuiLi,leonard,mapohl,martijnvisser,qingyue,renqs,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30103,,,FLINK-29463,,,,,,FLINK-28948,,FLINK-29463,,,,,FLINK-30354,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 02:26:51 UTC 2023,,,,,,,,,,"0|z18tko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 02:29;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41328&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4 1.16 instance;;;","26/Sep/22 02:31;hxbks2ks;[~smiralex][~renqs] Could you help take a look? Thx.;;;","12/Oct/22 03:01;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41881&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","18/Oct/22 08:23;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42115&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94;;;","24/Oct/22 06:17;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42328&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9;;;","24/Oct/22 06:20;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42340&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=11649;;;","04/Nov/22 06:16;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42808&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f;;;","05/Nov/22 05:38;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42837&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=10878;;;","09/Nov/22 07:59;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42955&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=11245;;;","09/Nov/22 14:03;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42968&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11609;;;","22/Nov/22 11:12;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43216&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=11334;;;","23/Nov/22 08:39;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43398&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=10945;;;","25/Nov/22 04:41;mapohl;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43483&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10750]

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43483&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=11317;;;","28/Nov/22 10:35;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43530&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=10582;;;","29/Nov/22 06:02;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43572&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10872;;;","01/Dec/22 08:29;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43636&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10879;;;","05/Dec/22 09:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43692&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10755;;;","05/Dec/22 10:39;mapohl;2x in the same build:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43700&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=10877
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43700&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10757;;;","05/Dec/22 11:15;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43711&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11259;;;","06/Dec/22 08:32;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43744&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11616;;;","10/Dec/22 12:35;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43851&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9;;;","11/Dec/22 07:03;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43861&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9;;;","12/Dec/22 10:23;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43871&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be;;;","12/Dec/22 10:25;leonard;I upgrade this priority to blocker as too many failures happens, Could you take a look when you have time [~renqs] [~smiralex] ?;;;","16/Dec/22 08:23;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43960&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=10804;;;","16/Dec/22 08:29;martijnvisser;release-1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44002&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11259;;;","19/Dec/22 09:47;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44025&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11257;;;","19/Dec/22 09:49;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44034&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11084;;;","02/Jan/23 09:37;mapohl;Copied from FLINK-29463: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44249&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11616;;;","02/Jan/23 09:58;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44309&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=10718;;;","02/Jan/23 10:52;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44366&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=10289;;;","09/Jan/23 07:49;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44553&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=11690;;;","18/Jan/23 07:44;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44987&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11633;;;","23/Jan/23 08:15;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45142&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10032;;;","23/Jan/23 08:29;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45148&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10032;;;","24/Jan/23 07:34;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45157&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=9723;;;","24/Jan/23 07:51;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45135&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10032;;;","26/Jan/23 07:53;mapohl;1 build, 2 failures:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45202&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=9719
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45202&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=10001;;;","27/Jan/23 07:34;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45230&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10958;;;","30/Jan/23 08:42;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45352&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=9718;;;","30/Jan/23 08:43;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45353&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10956;;;","31/Jan/23 04:13;JunRuiLi;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45443&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","31/Jan/23 07:53;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45440&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=9717;;;","01/Feb/23 09:21;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45521&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10032;;;","01/Feb/23 09:25;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45522&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10959;;;","01/Feb/23 09:32;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45490&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=9740;;;","02/Feb/23 08:42;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45586&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=9716;;;","02/Feb/23 09:02;mapohl;same run, multiple times:
* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45588&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10956]
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45588&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=11779;;;","02/Feb/23 12:18;renqs;Fixed on master: 64e8c349c4ee04623f2c874314a763f9fc1a3054;;;","03/Feb/23 10:12;mapohl;I'm reopening this one. Shouldn't we also provide a 1.16 backport, [~renqs]?;;;","03/Feb/23 12:24;mapohl;1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45678&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11337;;;","06/Feb/23 02:13;renqs;Thanks for the reminder [~mapohl] 

Backported to 1.16: 793b0f791c4db721d2c81d2252ff785e4731be07;;;","06/Feb/23 02:20;qingyue;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45662&view=logs&j=086353db-23b2-5446-2315-18e660618ef2&t=6cd785f3-2a2e-58a8-8e69-b4a03be28843&l=10314;;;","06/Feb/23 02:26;leonard; [~qingyue] Could you rebase you PR to latest master and re-trigger the CI? I checked you CI branch which does not  contain the fixed commit.;;;",,,,,,,,,,,,,,,,,,,,,,
Race condition in StreamTask can lead to NPE if changelog is disabled,FLINK-29397,13482790,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,22/Sep/22 14:35,22/Sep/22 23:17,13/Jul/23 08:13,22/Sep/22 23:17,1.15.0,,,,,,,1.15.3,1.16.0,,,,Runtime / Task,,,,,,,0,pull-request-available,,,,"{{StreamTask#processInput}} contains a branch where the changelogWriterAvailabilityProvider is accessed without a null check; this field however is nullable in case the changelog is disabled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 22 19:47:58 UTC 2022,,,,,,,,,,"0|z18ru0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 14:38;chesnay;This is trivially reproducible by disabling the changelog int he StreamTaskTest.;;;","22/Sep/22 19:47;chesnay;master: 162db046e1c63e4610393d14cd9843962321915e
1.16: a8979b29e084641a5160768f48400682a5d79bbb
1.15: a5ae2fa25522084c1616a8d11e3c1d1152f08b29;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Kinesis][EFO] Issue using EFO consumer at timestamp with empty shard,FLINK-29395,13482777,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liangtl,liangtl,liangtl,22/Sep/22 13:21,20/Oct/22 02:33,13/Jul/23 08:13,10/Oct/22 13:42,1.12.7,1.13.6,1.14.5,1.15.2,,,,1.15.3,1.16.0,1.17.0,,,Connectors / Kinesis,,,,,,,0,pull-request-available,,,,"*Background*

The consumer fails when an EFO record publisher uses a timestamp sentinel starting position, the first record batch is not empty, but the first deaggregated record batch is empty. This can happen if the user explicitly specifies the hashkey in the KPL, and does not ensure that the explicitHashKey of every record in the aggregated batch is the same.

When resharding occurs, the aggregated record batch can have records that are out of the shard's hash key range. This causes the records to be dropped when deaggregating, and can result in this situation, where record batch is not empty, but the deaggregated record batch is empty.

The symptom seen is similar to the issue seen in https://issues.apache.org/jira/browse/FLINK-20088.

See [here|https://github.com/awslabs/kinesis-aggregation/blob/master/potential_data_loss.md] and [here|https://github.com/awslabs/kinesis-aggregation/issues/11] for a more detailed explanation

*Replicate*

Get shard information
{code:java}
aws kinesis describe-stream --stream-name <stream_name>
{
    ""StreamDescription"": {
        ""Shards"": [
            ...
            {
                ""ShardId"": ""shardId-000000000037"",
                ""ParentShardId"": ""shardId-000000000027"",
                ""HashKeyRange"": {
                    ""StartingHashKey"": ""272225893536750770770699685945414569164"",
                    ""EndingHashKey"": ""340282366920938463463374607431768211455""
                }
            ...
            },
            {
                ""ShardId"": ""shardId-000000000038"",
                ""ParentShardId"": ""shardId-000000000034"",
                ""AdjacentParentShardId"": ""shardId-000000000036"",
                ""HashKeyRange"": {
                    ""StartingHashKey"": ""204169420152563078078024764459060926873"",
                    ""EndingHashKey"": ""272225893536750770770699685945414569163""
                }
            ...
            }
        ]
...
    }
}{code}
Create an aggregate record with two records, each with explicit hash keys belonging to different shards
{code:java}
RecordAggregator aggregator = new RecordAggregator();
String record1 = ""RECORD_1"";
String record2 = ""RECORD_2"";
aggregator.addUserRecord(""pk"", ""272225893536750770770699685945414569162"", record1.getBytes());
aggregator.addUserRecord(""pk"", ""272225893536750770770699685945414569165"", record2.getBytes());

AmazonKinesis kinesisClient = AmazonKinesisClient.builder()
   .build();
kinesisClient.putRecord(aggregator.clearAndGet().toPutRecordRequest(""EFOStreamTest"")); {code}
Consume from given stream whilst specifying a Timestamp where the only record retrieved is the record above.

*Error*
{code:java}
java.lang.IllegalArgumentException: Unexpected sentinel type: AT_TIMESTAMP_SEQUENCE_NUM
	at org.apache.flink.streaming.connectors.kinesis.model.StartingPosition.fromSentinelSequenceNumber(StartingPosition.java:115)
	at org.apache.flink.streaming.connectors.kinesis.model.StartingPosition.fromSequenceNumber(StartingPosition.java:91)
	at org.apache.flink.streaming.connectors.kinesis.model.StartingPosition.continueFromSequenceNumber(StartingPosition.java:72)

	at 
org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.lambda$run$0(FanOutRecordPublisher.java:120)

	at 
org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber.consumeAllRecordsFromKinesisShard(FanOutShardSubscriber.java:356)

	at 
org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber.subscribeToShardAndConsumeRecords(FanOutShardSubscriber.java:188)

	at 
org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.runWithBackoff(FanOutRecordPublisher.java:154)

	at 
org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.run(FanOutRecordPublisher.java:123)
	at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.run(ShardConsumer.java:114)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829) {code}
 

*Solution*

This is fixed by reusing the existing timestamp starting position in this condition.",,dannycranmer,liangtl,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20088,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 10 13:01:32 UTC 2022,,,,,,,,,,"0|z18rr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/22 13:01;dannycranmer;Merged commit [{{ef93ae4}}|https://github.com/apache/flink/commit/ef93ae4525ea42a87baf77747dd3ffbc007112fd] into master
Merged commit [{{7a1dccd}}|https://github.com/apache/flink/commit/7a1dccd3020ffefe83f1fa80ab70bc3150144640] into release-1.16 
Merged commit [{{ae20e52}}|https://github.com/apache/flink/commit/ae20e524671c26d87f4ffedd171c1ed3418be6d8] into release-1.15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SessionJobs are lost when Session FlinkDeployment is upgraded without HA,FLINK-29392,13482761,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,22/Sep/22 12:32,27/Sep/22 11:57,13/Jul/23 08:13,27/Sep/22 11:57,kubernetes-operator-1.1.0,kubernetes-operator-1.2.0,,,,,,kubernetes-operator-1.2.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Currently SessionJobs are completely lost if the session FlinkDeployment was upgraded and HA wasn't enabled. This is related to FLINK-27979 but its a quite critical manifestation of it.

After that the session job is never restarted and the observer thinks it's ""fine"" and keeps it in the RECONCILING state for some reason.",,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 27 11:57:16 UTC 2022,,,,,,,,,,"0|z18rnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 12:32;gyfora;cc [~aitozi] ;;;","27/Sep/22 11:57;gyfora;merged to main c4445bcc7d00f774489007a3e961dfa0dae3e38d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix args in JobSpec not being passed through to Flink in Standalone mode,FLINK-29388,13482733,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,usamj,usamj,22/Sep/22 09:34,13/Apr/23 14:04,13/Jul/23 08:13,23/Sep/22 13:59,kubernetes-operator-1.2.0,,,,,,,kubernetes-operator-1.2.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,gyfora,usamj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 23 13:59:35 UTC 2022,,,,,,,,,,"0|z18rhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/22 13:59;gyfora;merged to main e2b829c7df7501760dec8f9aa47685c680b227cf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IntervalJoinITCase.testIntervalJoinSideOutputRightLateData failed with AssertionError,FLINK-29387,13482731,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,stupid_pig,hxbks2ks,hxbks2ks,22/Sep/22 09:29,15/Nov/22 08:30,13/Jul/23 08:13,15/Nov/22 08:30,1.17.0,,,,,,,1.17.0,,,,,API / DataStream,,,,,,,0,pull-request-available,test-stability,,,"{code:java}
2022-09-22T04:40:21.9296331Z Sep 22 04:40:21 [ERROR] org.apache.flink.test.streaming.runtime.IntervalJoinITCase.testIntervalJoinSideOutputRightLateData  Time elapsed: 2.46 s  <<< FAILURE!
2022-09-22T04:40:21.9297487Z Sep 22 04:40:21 java.lang.AssertionError: expected:<[(key,2)]> but was:<[]>
2022-09-22T04:40:21.9298208Z Sep 22 04:40:21 	at org.junit.Assert.fail(Assert.java:89)
2022-09-22T04:40:21.9298927Z Sep 22 04:40:21 	at org.junit.Assert.failNotEquals(Assert.java:835)
2022-09-22T04:40:21.9299655Z Sep 22 04:40:21 	at org.junit.Assert.assertEquals(Assert.java:120)
2022-09-22T04:40:21.9300403Z Sep 22 04:40:21 	at org.junit.Assert.assertEquals(Assert.java:146)
2022-09-22T04:40:21.9301538Z Sep 22 04:40:21 	at org.apache.flink.test.streaming.runtime.IntervalJoinITCase.expectInAnyOrder(IntervalJoinITCase.java:521)
2022-09-22T04:40:21.9302578Z Sep 22 04:40:21 	at org.apache.flink.test.streaming.runtime.IntervalJoinITCase.testIntervalJoinSideOutputRightLateData(IntervalJoinITCase.java:280)
2022-09-22T04:40:21.9303641Z Sep 22 04:40:21 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-09-22T04:40:21.9304472Z Sep 22 04:40:21 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-09-22T04:40:21.9305371Z Sep 22 04:40:21 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-09-22T04:40:21.9306195Z Sep 22 04:40:21 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-09-22T04:40:21.9307011Z Sep 22 04:40:21 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-09-22T04:40:21.9308077Z Sep 22 04:40:21 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-09-22T04:40:21.9308968Z Sep 22 04:40:21 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-09-22T04:40:21.9309849Z Sep 22 04:40:21 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-09-22T04:40:21.9310704Z Sep 22 04:40:21 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-09-22T04:40:21.9311533Z Sep 22 04:40:21 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-09-22T04:40:21.9312386Z Sep 22 04:40:21 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-09-22T04:40:21.9313231Z Sep 22 04:40:21 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-09-22T04:40:21.9314985Z Sep 22 04:40:21 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-09-22T04:40:21.9315857Z Sep 22 04:40:21 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-09-22T04:40:21.9316633Z Sep 22 04:40:21 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-09-22T04:40:21.9317450Z Sep 22 04:40:21 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-09-22T04:40:21.9318209Z Sep 22 04:40:21 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-09-22T04:40:21.9318949Z Sep 22 04:40:21 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-09-22T04:40:21.9319680Z Sep 22 04:40:21 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-09-22T04:40:21.9320401Z Sep 22 04:40:21 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-09-22T04:40:21.9321130Z Sep 22 04:40:21 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-09-22T04:40:21.9321822Z Sep 22 04:40:21 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-09-22T04:40:21.9322498Z Sep 22 04:40:21 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-09-22T04:40:21.9323248Z Sep 22 04:40:21 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-09-22T04:40:21.9324080Z Sep 22 04:40:21 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-09-22T04:40:21.9324899Z Sep 22 04:40:21 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-09-22T04:40:21.9325763Z Sep 22 04:40:21 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-09-22T04:40:21.9326690Z Sep 22 04:40:21 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-09-22T04:40:21.9336750Z Sep 22 04:40:21 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-09-22T04:40:21.9337843Z Sep 22 04:40:21 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-09-22T04:40:21.9339018Z Sep 22 04:40:21 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-09-22T04:40:21.9339907Z Sep 22 04:40:21 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-09-22T04:40:21.9340728Z Sep 22 04:40:21 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-09-22T04:40:21.9341624Z Sep 22 04:40:21 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-09-22T04:40:21.9342569Z Sep 22 04:40:21 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-09-22T04:40:21.9344426Z Sep 22 04:40:21 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-09-22T04:40:21.9345700Z Sep 22 04:40:21 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-09-22T04:40:21.9346694Z Sep 22 04:40:21 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-09-22T04:40:21.9347619Z Sep 22 04:40:21 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-09-22T04:40:21.9348551Z Sep 22 04:40:21 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-09-22T04:40:21.9349389Z Sep 22 04:40:21 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-09-22T04:40:21.9350229Z Sep 22 04:40:21 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-09-22T04:40:21.9351777Z Sep 22 04:40:21 
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41236&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b",,hxb,hxbks2ks,mapohl,martijnvisser,pnowojski,stupid_pig,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24907,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 15 08:30:47 UTC 2022,,,,,,,,,,"0|z18rgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 09:30;hxbks2ks;[~stupid_pig] Could you help take a look? Thx.;;;","22/Sep/22 09:30;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41236&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798;;;","23/Sep/22 07:17;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41276&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8100;;;","23/Sep/22 07:18;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41276&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae;;;","26/Sep/22 02:54;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41316&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8267;;;","27/Sep/22 06:03;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41366&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b;;;","28/Sep/22 11:20;stupid_pig;I have re-run testIntervalJoinSideOutputLeftLateData() and testIntervalJoinSideOutputRightLateData() localy , and they passed.

I'd like to help fix it  , but I no idea  what wrong with the unit tests.;;;","29/Sep/22 02:00;hxbks2ks;This is an unstable case, not sure if you have run it thousands of times locally to see if you can reproduce it. Another possibility is that the performance of the CI machine is not so good. If this is the case, it will be a little more troublesome to investigate the root cause. 
;;;","30/Sep/22 11:51;chesnay;Running the test multiple times in parallel locally makes this pretty easy to reproduce.;;;","03/Oct/22 13:50;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41535&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca;;;","12/Oct/22 08:38;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41913&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10203;;;","14/Oct/22 10:10;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42011&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10207;;;","17/Oct/22 03:47;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42059&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","17/Oct/22 03:48;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42059&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b;;;","17/Oct/22 03:48;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42059&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798;;;","17/Oct/22 08:26;stupid_pig;[~chesnay]  Well, how could I  run the unit test in parallel locally .;;;","17/Oct/22 11:26;mapohl;[~stupid_pig] you should create a Run configuration for each of the tests in Intellij. Edit these Run configurations to enable repeated execution (e.g. ""Until failure"") and just start both tests.

Initially, I thought that the issue is due to the test sharing the static member {{IntervalJoinITCase#testResult}} if running the tests in the same JVM. The static member gets reset in {{IntervalJoinITCase#setup}}. That would explain the assert. But that would mean that any other concurrently executed test method of this class would have the same issue which is not the case.;;;","18/Oct/22 08:21;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42115&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798;;;","24/Oct/22 06:08;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42328&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8069;;;","01/Nov/22 08:27;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42680&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8208;;;","01/Nov/22 08:28;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42680&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10198;;;","03/Nov/22 14:12;pnowojski;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42784&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","07/Nov/22 07:57;mapohl;2 times in the same build:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42856&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=7330]
 * https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42856&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=9486;;;","08/Nov/22 07:07;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42908&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10229;;;","12/Nov/22 08:21;stupid_pig;[~mapohl]

[~hxbks2ks] 

I think the problem may be the watermark in the stream instead of parallel execution. For example , let's see the two sources in method _testIntervalJoinSideOutputRightLateData_ : 
{code:java}
DataStream<Tuple2<String, Integer>> streamOne =
        buildSourceStream(
                env,
                (ctx) -> {
                    ctx.collectWithTimestamp(Tuple2.of(""key"", 2), 2L);
                    ctx.collectWithTimestamp(Tuple2.of(""key"", 3), 3L);
                    ctx.emitWatermark(new Watermark(3));
                    ctx.collectWithTimestamp(Tuple2.of(""key"", 4), 4L);
                });

DataStream<Tuple2<String, Integer>> streamTwo =
        buildSourceStream(
                env,
                (ctx) -> {
                    ctx.collectWithTimestamp(Tuple2.of(""key"", 1), 1L);
                    ctx.collectWithTimestamp(Tuple2.of(""key"", 3), 3L);
                    ctx.emitWatermark(new Watermark(3));
                    ctx.collectWithTimestamp(Tuple2.of(""key"", 2), 2L); // late data
                }); {code}
 

If _streamTwo_ emit late data with timestamp=2L *before*  _streamOne emit_ _Watermark(3),  the_ _Watemark_ in IntervalJoin Operator is still the Long.MIN_VALUE. Thus when IntervalJoin Operator handle the late data, it won't sideout. 

 

I try to fix it, but I found it diffcult to control the data order between two streams.  Could you do me a favor?

Finally, I'd like to take this ticket .;;;","14/Nov/22 09:00;pnowojski;I've assigned the ticket to you [~stupid_pig]. 

It sounds to me like this test should not be implemented as an ITCase, but rather a unit test using [the test harnesses|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/testing/#unit-testing-stateful-or-timely-udfs--custom-operators]. This way we would have full control over order of processed elements/watermarks.;;;","15/Nov/22 08:30;pnowojski;merged commit 7e51db9 into apache:master;;;","15/Nov/22 08:30;pnowojski;Thanks for fixing the issue [~stupid_pig];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix fail to compile flink-connector-hive when profile is hive3,FLINK-29386,13482712,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,luoyuxia,luoyuxia,luoyuxia,22/Sep/22 07:31,26/Sep/22 11:10,13/Jul/23 08:13,26/Sep/22 11:10,1.16.0,1.17.0,,,,,,1.16.0,1.17.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,"The compile will fail in hive3. [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41238&view=logs&j=b1fcf054-9138-5463-c73c-a49979b9ac2a&t=9291ac46-dd95-5135-b799-3839e65a8691]

Introduced by FLINK-29152 which introduces org.apache.hadoop.hive.metastore.MetaStoreUtils.

DEFAULT_SERIALIZATION_FORMAT,  TableType.INDEX_TABLE, ErrorMsg.SHOW_CREATETABLE_INDEX.    But they don't exist in Hive3.",,fsk119,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 26 07:09:16 UTC 2022,,,,,,,,,,"0|z18rco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 07:09;fsk119;Merged into master: b7b366c0fda1ae8335a458d449b72fec31c409dd
Merged into release-1.16: 21444e5eec7ccf25a54ba0675105ab47a4096ac7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AddColumn in flink table store should check the duplicate field names,FLINK-29385,13482680,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,22/Sep/22 03:03,22/Sep/22 06:52,13/Jul/23 08:13,22/Sep/22 06:52,table-store-0.3.0,,,,,,,table-store-0.2.1,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,"AddColumn in table store should check the duplicate field names, otherwise the ddl will be successful and create flink store table failed for flink job",,lzljs3620320,zjureel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 22 06:52:10 UTC 2022,,,,,,,,,,"0|z18r5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 06:52;lzljs3620320;master: 98774161860055aef2113a5442ad63dcfe3ea9eb
release-0.2: 9f0acad62c7a21a547cab1312b775baa0b6ab4e4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
snakeyaml version 1.30 in flink-kubernetes-operator-1.2-SNAPSHOT-shaded.jar has vulnerabilities,FLINK-29384,13482634,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mbalassi,jbusche,jbusche,21/Sep/22 17:57,27/Sep/22 17:21,13/Jul/23 08:13,27/Sep/22 07:04,kubernetes-operator-1.2.0,,,,,,,,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"I did a twistlock scan of the current operator image from main, and it looks good except for in the flink-kubernetes-operator-1.2-SNAPSHOT-shaded.jar, I'm seeing 5 CVEs on snakeyaml.  Looks like updating from 1.30 to 1.32 should fix it, but I'm not sure how to bump that up, other than the [NOTICES|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/resources/META-INF/NOTICE#L65] entry.

The 5 CVEs are:
[https://nvd.nist.gov/vuln/detail/CVE-2022-25857]

[https://nvd.nist.gov/vuln/detail/CVE-2022-25857]

[https://nvd.nist.gov/vuln/detail/CVE-2022-38751]

[https://nvd.nist.gov/vuln/detail/CVE-2022-38750]

[https://nvd.nist.gov/vuln/detail/CVE-2022-38752]

Resulting in 1 High (CVSS 7.5) and 4 Mediums (CVSS 6.5, 6.5, 5.5, 4)",,gyfora,jbusche,mbalassi,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 27 17:21:31 UTC 2022,,,,,,,,,,"0|z18qvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 03:10;wangyang0918;The {{org.yaml:snakeyaml:jar:1.30}} is introduces by {{{}io.fabric8:kubernetes-client:jar:5.12.3{}}}. I think we could use the dependencyManagement to pin the version to 1.32, just like what we have done for {{{}com.fasterxml.jackson{}}}.;;;","27/Sep/22 07:04;gyfora;Merged to main 0952b0dd05f04390f549e3ca20f4ba345b067a18;;;","27/Sep/22 16:17;jbusche;Looks clean - thank you [~gyfora]  [~mbalassi] and [~wangyang0918]!;;;","27/Sep/22 17:21;mbalassi;Thanks for confirming [~jbusche] .;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Key_Shared subscription isn't works in the latest Pulsar connector,FLINK-29381,13482583,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,syhily,syhily,syhily,21/Sep/22 12:42,30/Sep/22 04:31,13/Jul/23 08:13,30/Sep/22 04:31,1.14.6,1.15.2,1.16.0,,,,,1.15.3,1.16.0,,,,Connectors / Pulsar,,,,,,,0,,,,,"Pulsar add [message retry policy|https://github.com/apache/pulsar/pull/14014] for Key_Shared subscription which makes the flink messages consuming not works now in Key_Shared subscription. We can't consume messages by using a subset of sticky key hash range https://github.com/apache/pulsar/issues/17679. So we have to change the Key_Shared subscription to Exclusive subscription which supports consuming messages with partial key hash ranges.",,syhily,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-21 12:42:59.0,,,,,,,,,,"0|z18qk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Misleading logging in Execution for failed state trannsitions,FLINK-29378,13482532,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,chesnay,chesnay,21/Sep/22 08:49,22/Sep/22 23:14,13/Jul/23 08:13,22/Sep/22 23:14,,,,,,,,1.16.0,,,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"{code}
String.format(
    ""Concurrent unexpected state transition of task %s to %s while deployment was in progress."",
     getVertexWithAttempt(), currentState);
{code}

{{to}} is not the target state.

This whole line needs improvements; log the current, expected and target state. Additionally I'd suggest to log the attempt ID which is much easier to correlate with other messages (like what the TM actually submits).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 22 12:44:36 UTC 2022,,,,,,,,,,"0|z18q8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 12:44;chesnay;master: 5766d50dc1401b1269ec83e670c2d21257e20fc5
1.16: 5ba6525db731aa62a770a9a7971b8a1cbf12d9fb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deployment already exists error if Flink version is not set correctly,FLINK-29376,13482524,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gyfora,gyfora,21/Sep/22 08:07,27/Sep/22 08:35,13/Jul/23 08:13,27/Sep/22 08:35,kubernetes-operator-1.1.0,kubernetes-operator-1.2.0,,,,,,kubernetes-operator-1.2.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"If the user incorrectly sets a Flink version lower than 1.15 when using 1.15 or above there are some strange behaviour currently around cluster shutdown.

Since we always set SHUTDOWN_ON_APPLICATION_FINISH (in the FlinkConfigBuilder) regardless of version but set the JmDeployStatus to MISSING/READY based on version. It can happen that we set the JmStatus to MISSING when the jm deployment is still running.

As the observer skips observing UPGRADING CRs it never updates the MISSING status and the deployment logic fails on duplicate deployment.

An easy fix could be to only set SHUTDOWN_ON_APPLICATION_FINISH for version >= 1.15",,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 27 08:35:34 UTC 2022,,,,,,,,,,"0|z18q74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/22 08:35;gyfora;merged to main 395956910af19f0ba6f1de1ae4d730aee4b6504f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commit delete file failure due to Checkpoint aborted,FLINK-29369,13482494,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,21/Sep/22 04:28,26/Sep/22 07:39,13/Jul/23 08:13,26/Sep/22 07:39,,,,,,,,table-store-0.2.1,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,"After checkpoint abort, the files in cp5 may fall into cp6, because the compaction commit is deleted first and then added, which may lead to:
-Delete a file
-Add the same file again

This causes the deleted file not to be found.

We need to properly process the merge of the compression files.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 26 07:39:04 UTC 2022,,,,,,,,,,"0|z18q0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 07:39;lzljs3620320;master: 2219fbad07e413a2961a1c806b0f9647ccf84bc8
release-0.2: 6066b1fd7ab06c853ae87d57804ef10683a976dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid manifest corruption for incorrect checkpoint recovery,FLINK-29367,13482491,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,lzljs3620320,lzljs3620320,21/Sep/22 04:10,22/Sep/22 10:15,13/Jul/23 08:13,22/Sep/22 10:15,table-store-0.2.0,,,,,,,table-store-0.2.1,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,"When the job runs to checkpoint N, if the user recovers from an old checkpoint (such as checkpoint N-5), the sink of the current FTS will cause a manifest corruption because duplicate files may be committed.

We should avoid such corruption, and the storage should be robust enough.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 22 10:15:59 UTC 2022,,,,,,,,,,"0|z18pzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 10:15;lzljs3620320;master: 33896da3aeeef1ad3aa523a3cc8e78c7e5347dbe
release-0.2: 9c16283e04cffccfaa42ed854381990448c787d1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The DPP(dynamic partition pruning) can not work with adaptive batch scheduler,FLINK-29348,13482312,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pltbkd,,,20/Sep/22 07:36,29/Sep/22 05:29,13/Jul/23 08:13,29/Sep/22 05:29,1.16.0,,,,,,,1.16.0,,,,,Runtime / Coordination,Table SQL / Runtime,,,,,,0,pull-request-available,,,,"When running tpcds with both DPP(dynamic partition pruning) and adaptive batch scheduler enabled, q14a.sql fails due to the following exception:
{code:java}
2022-09-20 10:34:18,244 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job q14a.sql (6d4355bdde514be083b9762e286626d2) switched from state FAILING to FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getGlobalFailureHandlingResult(ExecutionFailureHandler.java:102) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleGlobalFailure(DefaultScheduler.java:299) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.deliverOperatorEventToCoordinator(DefaultOperatorCoordinatorHandler.java:125) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.SchedulerBase.deliverOperatorEventToCoordinator(SchedulerBase.java:1031) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.sendOperatorEventToCoordinator(JobMaster.java:588) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at sun.reflect.GeneratedMethodAccessor69.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_332]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_332]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_332]
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_332]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_332]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_332]
Caused by: java.lang.IllegalStateException: Dynamic filtering data listener is missing: b9e97be4-bde0-4718-bfb1-e13d490517f1
	at org.apache.flink.table.runtime.operators.dynamicfiltering.DynamicFilteringDataCollectorOperatorCoordinator.handleEventFromOperator(DynamicFilteringDataCollectorOperatorCoordinator.java:98) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.lambda$handleEventFromOperator$0(RecreateOnResetOperatorCoordinator.java:84) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$DeferrableCoordinator.applyCall(RecreateOnResetOperatorCoordinator.java:315) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.handleEventFromOperator(RecreateOnResetOperatorCoordinator.java:82) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.handleEventFromOperator(OperatorCoordinatorHolder.java:218) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.deliverOperatorEventToCoordinator(DefaultOperatorCoordinatorHandler.java:121) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	... 31 more{code}",,gaoyunhaii,hxbks2ks,jackwangcs,wanglijie#1,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 29 05:29:38 UTC 2022,,,,,,,,,,"0|z18owo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/22 07:37;wanglijie#1;cc [~gaoyunhaii] [~pltbkd] [~zhuzh] ;;;","29/Sep/22 05:29;gaoyunhaii;Merged on master via 7bae0ebb6379c175522bd903838bb3737fc6c65d

Merged on release-1.16 via b30a502c53eaca95630eddd03022871f17fdf299;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to restore from list state with empty protobuf object,FLINK-29347,13482293,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,shenjiaqi,shenjiaqi,shenjiaqi,20/Sep/22 06:12,26/Oct/22 09:32,13/Jul/23 08:13,26/Oct/22 09:32,1.14.2,1.15.0,,,,,,1.17.0,,,,,Runtime / Checkpointing,Runtime / State Backends,,,,,,0,bugfix,checkpoint,pull-request-available,states,"I use protobuf generated class in an union list state.
When my flink job restores from checkpoint, I get exception:
{code:java}
Caused by: java.lang.RuntimeException: Could not create class com.MY_PROTOBUF_GENERATED_CLASS
	at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:76) ~[my-lib-0.1.1-SNAPSHOT.jar:?] 
	at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:40) ~[my-lib-0.1.1-SNAPSHOT.jar:?] 
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:679) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:354) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.state.OperatorStateRestoreOperation.deserializeOperatorStateValues(OperatorStateRestoreOperation.java:217) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.state.OperatorStateRestoreOperation.restore(OperatorStateRestoreOperation.java:188) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.state.DefaultOperatorStateBackendBuilder.build(DefaultOperatorStateBackendBuilder.java:80) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createOperatorStateBackend(EmbeddedRocksDBStateBackend.java:482) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$operatorStateBackend$0(StreamTaskStateInitializerImpl.java:277) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.operatorStateBackend(StreamTaskStateInitializerImpl.java:286) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:174) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:109) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:711) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:687) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292] 
Caused by: com.esotericsoftware.kryo.KryoException: java.io.EOFException: No more bytes left. 
	at org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.readBytes(NoFetchingInput.java:128) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.io.Input.readBytes(Input.java:314) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:73) ~[my-lib-0.1.1-SNAPSHOT.jar:?] 
	at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:40) ~[my-lib-0.1.1-SNAPSHOT.jar:?] 
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:679) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:354) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.state.OperatorStateRestoreOperation.deserializeOperatorStateValues(OperatorStateRestoreOperation.java:217) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.state.OperatorStateRestoreOperation.restore(OperatorStateRestoreOperation.java:188) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.state.DefaultOperatorStateBackendBuilder.build(DefaultOperatorStateBackendBuilder.java:80) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createOperatorStateBackend(EmbeddedRocksDBStateBackend.java:482) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$operatorStateBackend$0(StreamTaskStateInitializerImpl.java:277) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.operatorStateBackend(StreamTaskStateInitializerImpl.java:286) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:174) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:109) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:711) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:687) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292] 
Caused by: java.io.EOFException: No more bytes left. 
	at org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.readBytes(NoFetchingInput.java:128) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.io.Input.readBytes(Input.java:314) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:73) ~[my-lib-0.1.1-SNAPSHOT.jar:?] 
	at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:40) ~[my-lib-0.1.1-SNAPSHOT.jar:?] 
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:679) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:354) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.state.OperatorStateRestoreOperation.deserializeOperatorStateValues(OperatorStateRestoreOperation.java:217) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.state.OperatorStateRestoreOperation.restore(OperatorStateRestoreOperation.java:188) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.state.DefaultOperatorStateBackendBuilder.build(DefaultOperatorStateBackendBuilder.java:80) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createOperatorStateBackend(EmbeddedRocksDBStateBackend.java:482) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$operatorStateBackend$0(StreamTaskStateInitializerImpl.java:277) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.operatorStateBackend(StreamTaskStateInitializerImpl.java:286) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:174) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:109) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:711) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:687) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292] 
{code}
 

I find it is because when protobuf serializer serializes an object, which is built directly with builder without assign any value to field, the serializer will generate a zero length byte[] and then write it into state with content '\0'（indicates zero length data).

When recovered from checkpoint, protobuf seralizer deserialize the data. It get length 0, and call InputStream#read(byte[] bytes, int offset, int count) with count = 0.

The underlying Input implementation is [NoFetchingInput|https://github.com/apache/flink/blob/9d2ae5572897f3e2d9089414261a250cfc2a2ab8/flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/NoFetchingInput.java]. It will call Inputsteam#read(byte[] bytes, int offset, int count) with count = 0.

The InputStream implementation is [ByteStateHandleInputStream|https://github.com/apache/flink/blob/53d5e1cf9666517bc2fded60b510f2fd13d93f10/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/ByteStreamStateHandle.java#L140-L153], It will {*}return -1 as long as no data left in memory，even if count is 0{*}.

A simple fix is add check before return -1. If caller reads 0 bytes, it should always return 0 instead of -1.",,klion26,martijnvisser,shenjiaqi,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 26 09:32:35 UTC 2022,,,,,,,,,,"0|z18osg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Oct/22 09:32;yunta;merged in master: db46d434820e8cd964ea97ab2396d8dc9b43ad9f ... d3c7fc9be6aa13872fa2877236e7b14acd076679;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Too many open files in table store orc writer,FLINK-29345,13482279,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,lzljs3620320,lzljs3620320,20/Sep/22 03:58,08/Oct/22 07:45,13/Jul/23 08:13,08/Oct/22 07:45,,,,,,,,table-store-0.3.0,,,,,Table Store,,,,,,,0,pull-request-available,,,," !image-2022-09-20-11-57-11-373.png! 

We can avoid reading the local file to obtain the config every time we create a new writer by reusing the prepared configuration.",,lzljs3620320,zjureel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/22 03:57;lzljs3620320;image-2022-09-20-11-57-11-373.png;https://issues.apache.org/jira/secure/attachment/13049484/image-2022-09-20-11-57-11-373.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 08 07:45:38 UTC 2022,,,,,,,,,,"0|z18opc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/22 05:38;zjureel;Hi [~lzljs3620320] I'd like to pick this issue to start my contribution to flink-table-store, can you assign it to me? THX;;;","20/Sep/22 06:07;lzljs3620320;[~zjureel] Thanks!;;;","21/Sep/22 03:39;lzljs3620320;master: 835632c6e4758ad7d11ccbdb3a8ebb8dfa6aa709
release-0.2: 1973b737fc97fe1ecc83ee2c80f3e9253557ed7a;;;","28/Sep/22 09:56;lzljs3620320;The orc writer has memory management and needs to be initialized in the writing thread to avoid thread safety conflicts caused by memory management.

We should revert this PR.;;;","28/Sep/22 10:11;lzljs3620320;revert in:
master: 696d65c387b708228c99bef6914a8859c301d6e8
release-0.2:  2cddc8f7df36a2ecce77184a4a6060bfe1f0a916;;;","08/Oct/22 07:45;lzljs3620320;Re-merged in:
master: bee050f0301b00c620d4129c7a74ffbbc3d628ec;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix fail to query non-hive table in Hive dialect,FLINK-29337,13482171,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,19/Sep/22 12:17,11/Dec/22 10:51,13/Jul/23 08:13,14/Oct/22 06:28,,,,,,,,1.16.0,1.17.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,Flink will fail for the query with non-hive table in HiveDialect.,,jark,jingzhang,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29447,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 02:45:20 UTC 2022,,,,,,,,,,"0|z18o1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/22 12:18;jark;Fixed in 
 - master: 3387fffb05e3973a508726771e6ae48bc5f7c303
 - release-1.16: c8161fde08fc15e2bb102e8678665f4e1d26b7a8;;;","12/Oct/22 12:19;jark;Shall we open a pull request for 1.16 as well? [~luoyuxia];;;","12/Oct/22 12:34;luoyuxia;[~jark] Yes. I have opened the pr [https://github.com/apache/flink/pull/21034];;;","24/Nov/22 02:45;jingzhang;Shall we open a pr for 1.15 as well? [~jark] [~luoyuxia] . ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix documentation bug on how to enable batch mode for streaming examples,FLINK-29325,13481871,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jun He,Jun He,Jun He,16/Sep/22 11:44,21/Sep/22 02:50,13/Jul/23 08:13,21/Sep/22 02:50,1.15.2,,,,,,,1.15.3,1.16.0,1.17.0,,,Client / Job Submission,Documentation,,,,,,0,pull-request-available,,,,"In latest flink doc, it says that we should use '-Dexecution.runtime-mode=BATCH' to enable batch mode，but it does not work actually. The wrong way is as below:

bin/flink run -Dexecution.runtime-mode=BATCH examples/streaming/WordCount.jar

we should use '--execution-mode batch' instead, the correct way is as below

bin/flink run examples/streaming/WordCount.jar --execution-mode batch",,Jun He,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24830,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 21 02:50:57 UTC 2022,,,,,,,,,,"0|z18m74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/22 02:50;yunta;merged in master(1.17): 05600f844a904f34ab45f512715a76193974b497

release-1.16: de4aa4b7fee0f112fa3cfe66d0ad620841e18d74

release-1.15: 9ee1589c42565f47fdae8b82d488e6610bdb7fc6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Calling Kinesis connector close method before subtask starts running results in NPE,FLINK-29324,13481870,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiaohei,foxus,foxus,16/Sep/22 11:30,28/Nov/22 11:04,13/Jul/23 08:13,20/Sep/22 15:52,1.14.5,1.15.2,,,,,,1.15.3,1.16.0,1.17.0,,,Connectors / Kinesis,,,,,,,0,pull-request-available,,,,"When a Flink application is stopped before a Kinesis connector subtask has been started, the following exception is thrown:
{noformat}
java.lang.NullPointerException
at org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer.close(FlinkKinesisConsumer.java:421)
...{noformat}
This appears to be related to the fact that [fetcher creation|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/FlinkKinesisConsumer.java#L307] may not occur by [the time it is referenced when the consumer is closed|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/FlinkKinesisConsumer.java#L421].

A suggested fix is to make the {{close()}} method null safe [as it has been in the {{cancel()}} method|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/FlinkKinesisConsumer.java#L407].",,dannycranmer,foxus,xiaohei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30224,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 20 15:52:01 UTC 2022,,,,,,,,,,"0|z18m6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/22 08:12;xiaohei;Hello, could please assign this ticket to me?;;;","19/Sep/22 18:35;dannycranmer;Thanks [~xiaohei];;;","20/Sep/22 15:32;dannycranmer;Merged commit [{{71fea9a}}|https://github.com/apache/flink/commit/71fea9a4522505a6c0f23f1de599b7f87a633ccf] into master.

Merged commit [{{22086c}}|https://github.com/apache/flink/commit/22086c67a6a97148eb74ed32b281eec393721738] into release-1.16

Merged commit [{{eb6565}}|https://github.com/apache/flink/commit/eb65655f8ce39627a6bd28c8bdddd0c92db44d2e] into release-1.15;;;","20/Sep/22 15:52;dannycranmer;Thanks for the contribution [~xiaohei];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some config overrides are ignored when set under spec.flinkConfiguration,FLINK-29313,13481685,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,morhidi,,15/Sep/22 11:13,24/Nov/22 01:03,13/Jul/23 08:13,21/Sep/22 10:12,kubernetes-operator-1.2.0,,,,,,,kubernetes-operator-1.2.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Some [configs|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/operations/configuration/#resourceuser-configuration] that can be specified under spec.flinkConfiguration won't take affect without an upgrade, e.g.:
 * {{kubernetes.operator.periodic.savepoint.interval}}
 * {{kubernetes.operator.savepoint.format.type}}

These properties are used mainly from the so called 'observeConfig', and won't be available in the operator until the job is restarted. Ideally these should be changed without an upgrade, but at the moment they won't take affect at all.

 

 ",,bgeng777,gyfora,xiaohei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 21 10:12:10 UTC 2022,,,,,,,,,,"0|z18l2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/22 11:15;morhidi;cc [~gyfora] ;;;","16/Sep/22 09:12;xiaohei;[~matyas]  - Just want to understand more about this issue, is there any configs under spec.flinkConfiguration can work as expected? Thanks;;;","16/Sep/22 11:20;morhidi;{{This behaviour affects only the properties prefixed with kubernetes.operator and only o the main branch.}};;;","16/Sep/22 11:27;morhidi;The short term fix could be to do an upgrade when these properties are changed in the spec.flinkConfiguration. (This was the original behaviour in 1.1.0.) I'll try to come up with a good solution before going down this path.;;;","21/Sep/22 10:12;gyfora;merged to main 5e3e9962906c605ea55d4caf2fef2ee5ba400019;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fail to check multiple flink-dist*.jar for config.sh,FLINK-29306,13481542,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,LiuZeshan,LiuZeshan,LiuZeshan,14/Sep/22 16:48,27/Oct/22 07:26,13/Jul/23 08:13,27/Oct/22 07:26,1.16.0,,,,,,,1.17.0,,,,,Client / Job Submission,,,,,,,0,pull-request-available,,,,"The following shell command always make FLINK_DIST_COUNT=1 ([config.sh|https://github.com/apache/flink/blob/db98322472cb65ca0358ec1cce7f9ef737198189/flink-dist/src/main/flink-bin/bin/config.sh#L35])
{code:java}
FLINK_DIST_COUNT=""$(echo ""$FLINK_DIST"" | wc -l)"" {code}
and the following condition will always be false, so fail to check multiple flink-dist*.jar。
{code:java}
[[ ""$FLINK_DIST_COUNT"" -gt 1 ]] {code}
examples:
{code:java}
# FLINK_DIST="":/Users/lzs/.data/github/flink/flink-dist/target/flink-1.17-SNAPSHOT-bin/flink-1.17-SNAPSHOT/lib/flink-dist-1.17-SNAPSHOT.jar:/Users/lzs/.data/github/flink/flink-dist/target/flink-1.17-SNAPSHOT-bin/flink-1.17-SNAPSHOT/lib/flink-dist-1.17-xxx.jar""
# echo ""$FLINK_DIST"" | wc -l
1{code}",,LiuZeshan,rudi.kershaw,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 07:26:11 UTC 2022,,,,,,,,,,"0|z18k6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/22 17:25;LiuZeshan;[~rudi.kershaw] would you please to have a look ?;;;","15/Sep/22 14:22;rudi.kershaw;[~LiuZeshan], this is a good spot. I've obviously not tested this adequately when I made the change. 

I've checked over the change and added my approval to the pull request.;;;","27/Oct/22 07:26;xtsong;master (1.17): c0e3500722a6b8491748f57daf35ef9d79a4178d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UpsertTestSinkWriter fails if parent of output file does not exist,FLINK-29305,13481539,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,14/Sep/22 16:41,15/Sep/22 07:54,13/Jul/23 08:13,15/Sep/22 07:54,1.16.0,,,,,,,1.17.0,,,,,Tests,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 15 07:54:18 UTC 2022,,,,,,,,,,"0|z18k60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/22 07:54;chesnay;master. 33afc3c8924861025094ae94291805edad7afcd6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the network memory size calculation issue in fine-grained resource mode,FLINK-29299,13481423,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,kevin.cyj,kevin.cyj,kevin.cyj,14/Sep/22 03:21,20/Sep/22 06:31,13/Jul/23 08:13,20/Sep/22 06:31,1.16.0,,,,,,,1.16.0,,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,"After FLINK-28663, one intermediate dataset can be consumed by multiple consumers, there is a case where one vertex can consume one intermediate dataset multiple times. However, currently in fine-grained resource mode, when computing the required network buffer size, the intermediate dataset is used as key to record the size of network buffer per input gate, which means it may allocate less network buffers than needed if two input gate of the same vertex consumes the same intermediate dataset.",,aitozi,kevin.cyj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 20 06:31:02 UTC 2022,,,,,,,,,,"0|z18jgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/22 06:31;kevin.cyj;Fix via 

d3513d98953b0922e3dc753ef90806ed4e264926 on master

7367e358ccf34fb1e9ea2cea9d5b1f630b00e10c on 1.16;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LocalBufferPool request buffer from NetworkBufferPool hanging,FLINK-29298,13481422,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,14/Sep/22 03:15,18/Mar/23 03:27,13/Jul/23 08:13,13/Dec/22 04:30,1.16.0,,,,,,,1.16.1,1.17.0,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,"In the scenario where the buffer contention is fierce, sometimes the task hang can be observed. Through the thread dump information, we can found that the task thread is blocked by requestMemorySegmentBlocking forever. After investigating the dumped heap information, I found that the NetworkBufferPool actually has many buffers, but the LocalBufferPool is still unavailable and no buffer has been obtained.

By looking at the code, I am sure that this is a bug in thread race: when the task thread polled out the last buffer in LocalBufferPool and triggered the onGlobalPoolAvailable callback itself, it will skip this notification  (as currently the LocalBufferPool is available), which will cause the BufferPool to eventually become unavailable and will never register a callback to the NetworkBufferPool.

The conditions for triggering the problem are relatively strict, but I have found a stable way to reproduce it, I will try to fix and verify this problem.

!image-2022-09-14-10-52-15-259.png|width=1021,height=219!

!image-2022-09-14-10-58-45-987.png|width=997,height=315!

!image-2022-09-14-11-00-47-309.png|width=453,height=121!",,aitozi,AlexXXX,clouding,fanrui,hackergin,kevin.cyj,lichen1109,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29923,FLINK-29419,,,,,,,,,"14/Sep/22 02:52;Weijie Guo;image-2022-09-14-10-52-15-259.png;https://issues.apache.org/jira/secure/attachment/13049258/image-2022-09-14-10-52-15-259.png","14/Sep/22 02:58;Weijie Guo;image-2022-09-14-10-58-45-987.png;https://issues.apache.org/jira/secure/attachment/13049257/image-2022-09-14-10-58-45-987.png","14/Sep/22 03:00;Weijie Guo;image-2022-09-14-11-00-47-309.png;https://issues.apache.org/jira/secure/attachment/13049256/image-2022-09-14-11-00-47-309.png",,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 18 03:27:32 UTC 2023,,,,,,,,,,"0|z18jg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 07:34;kevin.cyj;[~Weijie Guo] Thanks for reporting this.;;;","15/Nov/22 03:03;AlexXXX;I tested the new github commit, but it still exists the same problem. If I change the first for() to 1024 times, this bug will be trigged everytimes.;;;","15/Nov/22 03:32;Weijie Guo;[~AlexXXX] This pr has not been reviewed, so it may not be the final solution. In addition, I'm a little suspicious that this problem may not be the only one in the LocalBufferPool, I need to confirm after the problem is fixed. As for the change of the first for() statement to 1024, this is expected, because there are only 1024 buffers in the networkBufferPool in the test class, and one buffer will be taken from the @Before method, so the maximum number of buffers can be requested in line 259 is 1023.;;;","13/Dec/22 04:30;xtsong;- master (1.17): 875f27ef38af20d2548d225c0583ebffe0f700fa
- release-1.16: 40fbea33c35bf9d04bff35f01554bb91c874b975;;;","18/Mar/23 01:00;lichen1109;We  alse faced to this problem, how to reproduce this problem with a stable way, Thank you sir;;;","18/Mar/23 03:27;Weijie Guo;Hi [~lichen1109]. Unfortunately, this bug is only possible to reproduce in the case of strong buffer competition. However, I wrote a unit test for this PR, which can reproduce the problem with a high probability. 
In addition, there is another bug (FLINK-31293) that can also cause a similar phenomenon. Whether your job is a batch job or a stream job, there should be no similar problems with batch jobs in the latest release-1.17 and master branches.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OperatorCoordinatorHolder.create throws NPE,FLINK-29296,13481416,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,hxb,hxb,14/Sep/22 02:39,20/Oct/22 07:50,13/Jul/23 08:13,20/Oct/22 07:50,1.16.0,,,,,,,,,,,,Runtime / Coordination,,,,,,,0,,,,,"{code:java}
2022-09-13T15:22:42.3864318Z Sep 13 15:22:42 [ERROR] Tests run: 8, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.633 s <<< FAILURE! - in org.apache.flink.test.streaming.runtime.SourceNAryInputChainingITCase
2022-09-13T15:22:42.3865377Z Sep 13 15:22:42 [ERROR] org.apache.flink.test.streaming.runtime.SourceNAryInputChainingITCase.testDirectSourcesOnlyExecution  Time elapsed: 0.165 s  <<< ERROR!
2022-09-13T15:22:42.3867571Z Sep 13 15:22:42 java.lang.RuntimeException: Failed to fetch next result
2022-09-13T15:22:42.3919112Z Sep 13 15:22:42 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
2022-09-13T15:22:42.3920935Z Sep 13 15:22:42 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2022-09-13T15:22:42.3922442Z Sep 13 15:22:42 	at org.apache.flink.streaming.api.datastream.DataStreamUtils.collectBoundedStream(DataStreamUtils.java:106)
2022-09-13T15:22:42.3924085Z Sep 13 15:22:42 	at org.apache.flink.test.streaming.runtime.SourceNAryInputChainingITCase.testDirectSourcesOnlyExecution(SourceNAryInputChainingITCase.java:89)
2022-09-13T15:22:42.3925493Z Sep 13 15:22:42 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-09-13T15:22:42.3926635Z Sep 13 15:22:42 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-09-13T15:22:42.3928378Z Sep 13 15:22:42 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-09-13T15:22:42.3964273Z Sep 13 15:22:42 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-09-13T15:22:42.3965054Z Sep 13 15:22:42 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-09-13T15:22:42.3965788Z Sep 13 15:22:42 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-09-13T15:22:42.3966508Z Sep 13 15:22:42 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-09-13T15:22:42.3967476Z Sep 13 15:22:42 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-09-13T15:22:42.3968432Z Sep 13 15:22:42 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-09-13T15:22:42.3969233Z Sep 13 15:22:42 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-09-13T15:22:42.3969871Z Sep 13 15:22:42 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-09-13T15:22:42.3970534Z Sep 13 15:22:42 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-09-13T15:22:42.3971453Z Sep 13 15:22:42 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-09-13T15:22:42.3972453Z Sep 13 15:22:42 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-09-13T15:22:42.3973193Z Sep 13 15:22:42 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-09-13T15:22:42.3973857Z Sep 13 15:22:42 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-09-13T15:22:42.3974634Z Sep 13 15:22:42 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-09-13T15:22:42.3975420Z Sep 13 15:22:42 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-09-13T15:22:42.3976060Z Sep 13 15:22:42 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-09-13T15:22:42.3976689Z Sep 13 15:22:42 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-09-13T15:22:42.3977555Z Sep 13 15:22:42 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-09-13T15:22:42.3978248Z Sep 13 15:22:42 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-09-13T15:22:42.3978856Z Sep 13 15:22:42 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-09-13T15:22:42.3979696Z Sep 13 15:22:42 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-09-13T15:22:42.3980716Z Sep 13 15:22:42 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-09-13T15:22:42.3981785Z Sep 13 15:22:42 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-09-13T15:22:42.3982352Z Sep 13 15:22:42 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-09-13T15:22:42.3982989Z Sep 13 15:22:42 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-09-13T15:22:42.3983913Z Sep 13 15:22:42 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-09-13T15:22:42.3985205Z Sep 13 15:22:42 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-09-13T15:22:42.3986116Z Sep 13 15:22:42 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-09-13T15:22:42.3987027Z Sep 13 15:22:42 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-09-13T15:22:42.3988003Z Sep 13 15:22:42 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-09-13T15:22:42.3988886Z Sep 13 15:22:42 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-09-13T15:22:42.3989753Z Sep 13 15:22:42 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-09-13T15:22:42.3990534Z Sep 13 15:22:42 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-09-13T15:22:42.3991262Z Sep 13 15:22:42 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-09-13T15:22:42.3992048Z Sep 13 15:22:42 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-09-13T15:22:42.3992887Z Sep 13 15:22:42 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-09-13T15:22:42.3993670Z Sep 13 15:22:42 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-09-13T15:22:42.3994498Z Sep 13 15:22:42 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-09-13T15:22:42.3995471Z Sep 13 15:22:42 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-09-13T15:22:42.3996252Z Sep 13 15:22:42 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-09-13T15:22:42.3997203Z Sep 13 15:22:42 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-09-13T15:22:42.3998042Z Sep 13 15:22:42 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-09-13T15:22:42.3998717Z Sep 13 15:22:42 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-09-13T15:22:42.3999316Z Sep 13 15:22:42 Caused by: java.io.IOException: Failed to fetch job execution result
2022-09-13T15:22:42.4000057Z Sep 13 15:22:42 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
2022-09-13T15:22:42.4000925Z Sep 13 15:22:42 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
2022-09-13T15:22:42.4001806Z Sep 13 15:22:42 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2022-09-13T15:22:42.4002441Z Sep 13 15:22:42 	... 49 more
2022-09-13T15:22:42.4003019Z Sep 13 15:22:42 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-09-13T15:22:42.4003764Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-09-13T15:22:42.4004418Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2022-09-13T15:22:42.4005239Z Sep 13 15:22:42 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
2022-09-13T15:22:42.4005865Z Sep 13 15:22:42 	... 51 more
2022-09-13T15:22:42.4006359Z Sep 13 15:22:42 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-09-13T15:22:42.4007514Z Sep 13 15:22:42 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-09-13T15:22:42.4008348Z Sep 13 15:22:42 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
2022-09-13T15:22:42.4009146Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-09-13T15:22:42.4009823Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2022-09-13T15:22:42.4010525Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2022-09-13T15:22:42.4011295Z Sep 13 15:22:42 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:138)
2022-09-13T15:22:42.4012173Z Sep 13 15:22:42 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:181)
2022-09-13T15:22:42.4012810Z Sep 13 15:22:42 	... 51 more
2022-09-13T15:22:42.4013448Z Sep 13 15:22:42 Caused by: org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
2022-09-13T15:22:42.4014278Z Sep 13 15:22:42 	at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.lambda$new$0(DefaultJobMasterServiceProcess.java:97)
2022-09-13T15:22:42.4015229Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-09-13T15:22:42.4015959Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-09-13T15:22:42.4016686Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-09-13T15:22:42.4017624Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1609)
2022-09-13T15:22:42.4018352Z Sep 13 15:22:42 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-09-13T15:22:42.4019064Z Sep 13 15:22:42 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-09-13T15:22:42.4019644Z Sep 13 15:22:42 	at java.lang.Thread.run(Thread.java:748)
2022-09-13T15:22:42.4021477Z Sep 13 15:22:42 Caused by: java.util.concurrent.CompletionException: java.lang.RuntimeException: org.apache.flink.runtime.JobException: Cannot instantiate the coordinator for operator MultipleInputOperator [Source: source-1, Source: source-2, Source: source-3]
2022-09-13T15:22:42.4022481Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
2022-09-13T15:22:42.4023211Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
2022-09-13T15:22:42.4023950Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1606)
2022-09-13T15:22:42.4024490Z Sep 13 15:22:42 	... 3 more
2022-09-13T15:22:42.4025502Z Sep 13 15:22:42 Caused by: java.lang.RuntimeException: org.apache.flink.runtime.JobException: Cannot instantiate the coordinator for operator MultipleInputOperator [Source: source-1, Source: source-2, Source: source-3]
2022-09-13T15:22:42.4026394Z Sep 13 15:22:42 	at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321)
2022-09-13T15:22:42.4027181Z Sep 13 15:22:42 	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:114)
2022-09-13T15:22:42.4028117Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
2022-09-13T15:22:42.4028635Z Sep 13 15:22:42 	... 3 more
2022-09-13T15:22:42.4029580Z Sep 13 15:22:42 Caused by: org.apache.flink.runtime.JobException: Cannot instantiate the coordinator for operator MultipleInputOperator [Source: source-1, Source: source-2, Source: source-3]
2022-09-13T15:22:42.4030458Z Sep 13 15:22:42 	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.initialize(ExecutionJobVertex.java:229)
2022-09-13T15:22:42.4031405Z Sep 13 15:22:42 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.initializeJobVertex(DefaultExecutionGraph.java:901)
2022-09-13T15:22:42.4032277Z Sep 13 15:22:42 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.initializeJobVertices(DefaultExecutionGraph.java:891)
2022-09-13T15:22:42.4033150Z Sep 13 15:22:42 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.attachJobGraph(DefaultExecutionGraph.java:848)
2022-09-13T15:22:42.4034250Z Sep 13 15:22:42 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.attachJobGraph(DefaultExecutionGraph.java:830)
2022-09-13T15:22:42.4035132Z Sep 13 15:22:42 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraphBuilder.buildGraph(DefaultExecutionGraphBuilder.java:198)
2022-09-13T15:22:42.4036060Z Sep 13 15:22:42 	at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:156)
2022-09-13T15:22:42.4037173Z Sep 13 15:22:42 	at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:361)
2022-09-13T15:22:42.4038096Z Sep 13 15:22:42 	at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:206)
2022-09-13T15:22:42.4038824Z Sep 13 15:22:42 	at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:134)
2022-09-13T15:22:42.4039610Z Sep 13 15:22:42 	at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:152)
2022-09-13T15:22:42.4040587Z Sep 13 15:22:42 	at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:119)
2022-09-13T15:22:42.4041442Z Sep 13 15:22:42 	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:369)
2022-09-13T15:22:42.4042111Z Sep 13 15:22:42 	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:346)
2022-09-13T15:22:42.4042959Z Sep 13 15:22:42 	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:123)
2022-09-13T15:22:42.4043989Z Sep 13 15:22:42 	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:95)
2022-09-13T15:22:42.4045007Z Sep 13 15:22:42 	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112)
2022-09-13T15:22:42.4045573Z Sep 13 15:22:42 	... 4 more
2022-09-13T15:22:42.4045981Z Sep 13 15:22:42 Caused by: java.lang.NullPointerException
2022-09-13T15:22:42.4046657Z Sep 13 15:22:42 	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.create(OperatorCoordinatorHolder.java:488)
2022-09-13T15:22:42.4047747Z Sep 13 15:22:42 	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.createOperatorCoordinatorHolder(ExecutionJobVertex.java:286)
2022-09-13T15:22:42.4048604Z Sep 13 15:22:42 	at org.apache.f {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40968&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7",,hxb,kevin.cyj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 20 07:50:35 UTC 2022,,,,,,,,,,"0|z18jew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/22 02:40;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40968&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","20/Oct/22 07:50;kevin.cyj;I think it is already fixed by https://issues.apache.org/jira/browse/FLINK-29576. I am closing it. Feel free to reopen it if it still reproduces.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clear RecordWriter slower to avoid causing frequent compaction conflicts,FLINK-29295,13481414,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,14/Sep/22 02:20,14/Sep/22 06:27,13/Jul/23 08:13,14/Sep/22 06:27,,,,,,,,table-store-0.2.1,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,"In AbstractTableWrite, the Writer is cleaned up as soon as no new files are generated, which may lead to the changes generated after the compaction have not been committed, but the new data from the next checkpoint comes to create a new writer, which conflicts with the changes generated in the next round of checkpoint and the previous round, resulting in an exception:

{code:java}
Caused by: java.lang.IllegalStateException: Trying to delete file {org.apache.flink.table.data.binary.BinaryRowData@5759f99e, 0, 0, data-7bf2498e-d0a1-42a5-97b7-b3860f10b076-0.orc} which is not previously added. Manifest might be corrupted.
{code}
",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 14 06:27:15 UTC 2022,,,,,,,,,,"0|z18jeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/22 06:27;lzljs3620320;master: 6a1b1bca4da131e47ee17cbf5d10472762a66da6
release-0.2: 75a732b40c868bab026f45478ce33530861407d8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't start a job with a jar in the system classpath,FLINK-29288,13481389,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,sap1ens,sap1ens,13/Sep/22 21:53,23/Sep/22 06:31,13/Jul/23 08:13,23/Sep/22 06:31,kubernetes-operator-1.1.0,,,,,,,kubernetes-operator-1.2.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"I'm using the latest (unreleased) version of the Kubernetes operator.

It looks like currently, it's impossible to use it with a job jar file in the system classpath (/opt/flink/lib). *jarURI* is required and it's always passed as a *pipeline.jars* parameter to the Flink process. In practice, it means that the same class is loaded twice: once by the system classloader and another time by the user classloader. This leads to exceptions like this:
{quote}java.lang.LinkageError: loader constraint violation: when resolving method 'XXX' the class loader org.apache.flink.util.ChildFirstClassLoader @47a5b70d of the current class, YYY, and the class loader 'app' for the method's defining class, ZZZ, have different Class objects for the type AAA used in the signature
{quote}
In my opinion, jarURI must be made optional even for the application mode. In this case, it's assumed that it's already available in the system classpath.",,gyfora,sap1ens,zhongqishang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 23 06:31:35 UTC 2022,,,,,,,,,,"0|z18j8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/22 00:05;sap1ens;Apparently removing *pipeline.jars* is not enough, because the operator will always create */opt/flink/usrlib* folder on the jobmanager as a volume mount. This forces Flink's DefaultPackagedProgramRetriever to try using *usrlib* folder for loading classes (which is empty in this case). 

So, in my mind, operator's *UserLibMountDecorator* should only be used when *pipeline.jars / pipeline.classpaths* is specified.;;;","23/Sep/22 06:31;gyfora;merged to main 8f53441a4978eeb38dc5ef229c179cc60598ce87;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Join hint are not propagated in subquery,FLINK-29280,13481287,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuyangzhong,xuyangzhong,xuyangzhong,13/Sep/22 10:58,19/Sep/22 03:40,13/Jul/23 08:13,19/Sep/22 03:31,1.16.0,,,,,,,1.16.0,1.17.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"Add the following code in JoinHintTestBase to re-produce this bug.
{code:java}

@Test
public void testJoinHintWithJoinHintInSubQuery() {
    String sql =
            ""select * from T1 WHERE a1 IN (select /*+ %s(T2) */ a2 from T2 join T3 on T2.a2 = T3.a3)"";

    verifyRelPlanByCustom(String.format(sql, buildCaseSensitiveStr(getTestSingleJoinHint())));
} {code}
This is because that calcite will not propagate the hint in subquery and flink also doesn't resolve it in FlinkSubQueryRemoveRule",,godfreyhe,xuyangzhong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 19 03:31:26 UTC 2022,,,,,,,,,,"0|z18imo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/22 03:31;godfreyhe;Fixed in master: 22cb554008320e6684280b5205f93d7a6f685c6c
in 1.16.0: b37a8153f22b62982ca144604a34056246f6f36c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BINARY type is not supported in table store,FLINK-29278,13481247,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,lzljs3620320,lzljs3620320,13/Sep/22 07:21,21/Sep/22 14:15,13/Jul/23 08:13,21/Sep/22 14:15,,,,,,,,table-store-0.2.1,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,, !image-2022-09-13-15-21-55-116.png! ,,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/22 07:21;lzljs3620320;image-2022-09-13-15-21-55-116.png;https://issues.apache.org/jira/secure/attachment/13049223/image-2022-09-13-15-21-55-116.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 21 14:15:47 UTC 2022,,,,,,,,,,"0|z18ids:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/22 14:15;lzljs3620320;master: 422ea6072b251b04041b6ca8a738316a30069aad
release-0.2: 3eee4bf4eddd6d22a0225e1958601a44f7dd9ba7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer2EndpointITCase.testGetFunctionWithPattern failed with Persistence Manager has been closed,FLINK-29274,13481207,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fsk119,hxb,hxb,13/Sep/22 03:07,26/Sep/22 08:58,13/Jul/23 08:13,26/Sep/22 08:58,1.16.0,,,,,,,1.16.0,1.17.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,test-stability,,,"{code:java}
4.6807800Z Sep 13 02:07:54 [ERROR] org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.testGetFunctionWithPattern  Time elapsed: 22.127 s  <<< ERROR!
2022-09-13T02:07:54.6813586Z Sep 13 02:07:54 java.sql.SQLException: javax.jdo.JDOFatalUserException: Persistence Manager has been closed
2022-09-13T02:07:54.6815315Z Sep 13 02:07:54 	at org.apache.hive.jdbc.HiveStatement.waitForOperationToComplete(HiveStatement.java:401)
2022-09-13T02:07:54.6816917Z Sep 13 02:07:54 	at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:266)
2022-09-13T02:07:54.6818338Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.lambda$testGetFunctionWithPattern$29(HiveServer2EndpointITCase.java:542)
2022-09-13T02:07:54.6819988Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.runGetObjectTest(HiveServer2EndpointITCase.java:633)
2022-09-13T02:07:54.6821484Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.runGetObjectTest(HiveServer2EndpointITCase.java:621)
2022-09-13T02:07:54.6823318Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.testGetFunctionWithPattern(HiveServer2EndpointITCase.java:539)
2022-09-13T02:07:54.6824711Z Sep 13 02:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-09-13T02:07:54.6825817Z Sep 13 02:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-09-13T02:07:54.6827003Z Sep 13 02:07:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-09-13T02:07:54.6828259Z Sep 13 02:07:54 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-09-13T02:07:54.6829478Z Sep 13 02:07:54 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-09-13T02:07:54.6830717Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-09-13T02:07:54.6832444Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-09-13T02:07:54.6834028Z Sep 13 02:07:54 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-09-13T02:07:54.6835304Z Sep 13 02:07:54 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-09-13T02:07:54.6836734Z Sep 13 02:07:54 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2022-09-13T02:07:54.6838257Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-09-13T02:07:54.6839775Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-09-13T02:07:54.6841400Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-09-13T02:07:54.6843309Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-09-13T02:07:54.6845300Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-09-13T02:07:54.6846879Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-09-13T02:07:54.6848406Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-09-13T02:07:54.6849760Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-09-13T02:07:54.6851297Z Sep 13 02:07:54 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-09-13T02:07:54.6853032Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.6854384Z Sep 13 02:07:54 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-09-13T02:07:54.6856052Z Sep 13 02:07:54 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-09-13T02:07:54.6857406Z Sep 13 02:07:54 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-09-13T02:07:54.6858824Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-09-13T02:07:54.6860227Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.6861752Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-09-13T02:07:54.6863318Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-09-13T02:07:54.6864623Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-09-13T02:07:54.6866007Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.6867339Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-09-13T02:07:54.6868565Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-09-13T02:07:54.6870141Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-09-13T02:07:54.6872335Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-09-13T02:07:54.6874262Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-09-13T02:07:54.6875954Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-09-13T02:07:54.6877332Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.6878774Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-09-13T02:07:54.6880095Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-09-13T02:07:54.6881341Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-09-13T02:07:54.6882928Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.6884550Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-09-13T02:07:54.6885847Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-09-13T02:07:54.6887389Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-09-13T02:07:54.6889201Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-09-13T02:07:54.6890940Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-09-13T02:07:54.6892639Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.6894303Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-09-13T02:07:54.6895709Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-09-13T02:07:54.6897057Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-09-13T02:07:54.6898411Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.6899771Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-09-13T02:07:54.6901135Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-09-13T02:07:54.6902936Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-09-13T02:07:54.6904607Z Sep 13 02:07:54 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-09-13T02:07:54.6905852Z Sep 13 02:07:54 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-09-13T02:07:54.6907027Z Sep 13 02:07:54 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-09-13T02:07:54.6908157Z Sep 13 02:07:54 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-09-13T02:07:54.6933730Z Sep 13 02:07:54 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-09-13T02:07:54.6934713Z Sep 13 02:07:54 
2022-09-13T02:07:54.6935600Z Sep 13 02:07:54 [ERROR] org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.testGetPrimaryKeyWithPattern  Time elapsed: 2.187 s  <<< ERROR!
2022-09-13T02:07:54.6936957Z Sep 13 02:07:54 org.apache.hive.service.cli.HiveSQLException: Failed to getOperationResultSchema.
2022-09-13T02:07:54.6938148Z Sep 13 02:07:54 	at org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:300)
2022-09-13T02:07:54.6939157Z Sep 13 02:07:54 	at org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:291)
2022-09-13T02:07:54.6940213Z Sep 13 02:07:54 	at org.apache.hive.jdbc.HiveQueryResultSet.retrieveSchema(HiveQueryResultSet.java:254)
2022-09-13T02:07:54.6941413Z Sep 13 02:07:54 	at org.apache.hive.jdbc.HiveQueryResultSet.<init>(HiveQueryResultSet.java:198)
2022-09-13T02:07:54.6942920Z Sep 13 02:07:54 	at org.apache.hive.jdbc.HiveQueryResultSet$Builder.build(HiveQueryResultSet.java:179)
2022-09-13T02:07:54.6944264Z Sep 13 02:07:54 	at org.apache.hive.jdbc.HiveDatabaseMetaData.getPrimaryKeys(HiveDatabaseMetaData.java:583)
2022-09-13T02:07:54.6945876Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.lambda$testGetPrimaryKeyWithPattern$23(HiveServer2EndpointITCase.java:465)
2022-09-13T02:07:54.6947624Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.runGetObjectTest(HiveServer2EndpointITCase.java:633)
2022-09-13T02:07:54.6949577Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.runGetObjectTest(HiveServer2EndpointITCase.java:621)
2022-09-13T02:07:54.6951289Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.testGetPrimaryKeyWithPattern(HiveServer2EndpointITCase.java:464)
2022-09-13T02:07:54.6952899Z Sep 13 02:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-09-13T02:07:54.6954059Z Sep 13 02:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-09-13T02:07:54.6955317Z Sep 13 02:07:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-09-13T02:07:54.6956333Z Sep 13 02:07:54 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-09-13T02:07:54.7118261Z Sep 13 02:07:54 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-09-13T02:07:54.7120151Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-09-13T02:07:54.7121551Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-09-13T02:07:54.7123271Z Sep 13 02:07:54 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-09-13T02:07:54.7124596Z Sep 13 02:07:54 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-09-13T02:07:54.7125915Z Sep 13 02:07:54 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2022-09-13T02:07:54.7127292Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-09-13T02:07:54.7128686Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-09-13T02:07:54.7130110Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-09-13T02:07:54.7131626Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-09-13T02:07:54.7133278Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-09-13T02:07:54.7134718Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-09-13T02:07:54.7136060Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-09-13T02:07:54.7137325Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-09-13T02:07:54.7138729Z Sep 13 02:07:54 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-09-13T02:07:54.7140187Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.7141566Z Sep 13 02:07:54 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-09-13T02:07:54.7143229Z Sep 13 02:07:54 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-09-13T02:07:54.7144564Z Sep 13 02:07:54 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-09-13T02:07:54.7145876Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-09-13T02:07:54.7147208Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.7148884Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-09-13T02:07:54.7150187Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-09-13T02:07:54.7151460Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-09-13T02:07:54.7153057Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.7154414Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-09-13T02:07:54.7155751Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-09-13T02:07:54.7157291Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-09-13T02:07:54.7159439Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-09-13T02:07:54.7161313Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-09-13T02:07:54.7163138Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-09-13T02:07:54.7164533Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.7165931Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-09-13T02:07:54.7167229Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-09-13T02:07:54.7168552Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-09-13T02:07:54.7170224Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.7171631Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-09-13T02:07:54.7173130Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-09-13T02:07:54.7174728Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-09-13T02:07:54.7176520Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-09-13T02:07:54.7178170Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-09-13T02:07:54.7179572Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.7180993Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-09-13T02:07:54.7182514Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-09-13T02:07:54.7183754Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-09-13T02:07:54.7185162Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.7186729Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-09-13T02:07:54.7188068Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-09-13T02:07:54.7189602Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-09-13T02:07:54.7191074Z Sep 13 02:07:54 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-09-13T02:07:54.7192376Z Sep 13 02:07:54 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-09-13T02:07:54.7193420Z Sep 13 02:07:54 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-09-13T02:07:54.7194454Z Sep 13 02:07:54 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-09-13T02:07:54.7195678Z Sep 13 02:07:54 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-09-13T02:07:54.7196862Z Sep 13 02:07:54 Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to getOperationResultSchema.
2022-09-13T02:07:54.7198223Z Sep 13 02:07:54 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.getOperationResultSchema(SqlGatewayServiceImpl.java:158)
2022-09-13T02:07:54.7199739Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.GetResultSetMetadata(HiveServer2Endpoint.java:681)
2022-09-13T02:07:54.7201158Z Sep 13 02:07:54 	at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetResultSetMetadata.getResult(TCLIService.java:1817)
2022-09-13T02:07:54.7202730Z Sep 13 02:07:54 	at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetResultSetMetadata.getResult(TCLIService.java:1802)
2022-09-13T02:07:54.7204000Z Sep 13 02:07:54 	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
2022-09-13T02:07:54.7205124Z Sep 13 02:07:54 	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
2022-09-13T02:07:54.7206294Z Sep 13 02:07:54 	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
2022-09-13T02:07:54.7207488Z Sep 13 02:07:54 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-09-13T02:07:54.7208672Z Sep 13 02:07:54 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-09-13T02:07:54.7209644Z Sep 13 02:07:54 	at java.lang.Thread.run(Thread.java:748)
2022-09-13T02:07:54.7212142Z Sep 13 02:07:54 Caused by: org.apache.flink.table.gateway.service.utils.SqlExecutionException: Failed to execute the operation d8443ce0-7ee1-4997-a2b6-46970328febd.
2022-09-13T02:07:54.7213657Z Sep 13 02:07:54 	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.processThrowable(OperationManager.java:389)
2022-09-13T02:07:54.7215131Z Sep 13 02:07:54 	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:248)
2022-09-13T02:07:54.7216437Z Sep 13 02:07:54 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2022-09-13T02:07:54.7217505Z Sep 13 02:07:54 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-09-13T02:07:54.7218567Z Sep 13 02:07:54 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2022-09-13T02:07:54.7219589Z Sep 13 02:07:54 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-09-13T02:07:54.7220364Z Sep 13 02:07:54 	... 3 more
2022-09-13T02:07:54.7221223Z Sep 13 02:07:54 Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to listDatabases.
2022-09-13T02:07:54.7222702Z Sep 13 02:07:54 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.listDatabases(SqlGatewayServiceImpl.java:240)
2022-09-13T02:07:54.7224028Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.util.OperationExecutorFactory.executeGetPrimaryKeys(OperationExecutorFactory.java:362)
2022-09-13T02:07:54.7225719Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.util.OperationExecutorFactory.lambda$createGetPrimaryKeys$6(OperationExecutorFactory.java:154)
2022-09-13T02:07:54.7227198Z Sep 13 02:07:54 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$0(OperationManager.java:91)
2022-09-13T02:07:54.7228661Z Sep 13 02:07:54 	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:239)
2022-09-13T02:07:54.7229693Z Sep 13 02:07:54 	... 7 more
2022-09-13T02:07:54.7230567Z Sep 13 02:07:54 Caused by: org.apache.flink.table.catalog.exceptions.CatalogException: Failed to list all databases in hive
2022-09-13T02:07:54.7231721Z Sep 13 02:07:54 	at org.apache.flink.table.catalog.hive.HiveCatalog.listDatabases(HiveCatalog.java:414)
2022-09-13T02:07:54.7233235Z Sep 13 02:07:54 	at org.apache.flink.table.gateway.service.operation.OperationExecutor.listDatabases(OperationExecutor.java:143)
2022-09-13T02:07:54.7234814Z Sep 13 02:07:54 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.listDatabases(SqlGatewayServiceImpl.java:237)
2022-09-13T02:07:54.7235737Z Sep 13 02:07:54 	... 11 more
2022-09-13T02:07:54.7236730Z Sep 13 02:07:54 Caused by: MetaException(message:java.lang.RuntimeException: openTransaction called in an interior transaction scope, but currentTransaction is not active.)
2022-09-13T02:07:54.7238137Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:6935)
2022-09-13T02:07:54.7239416Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_databases(HiveMetaStore.java:1670)
2022-09-13T02:07:54.7240514Z Sep 13 02:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-09-13T02:07:54.7241525Z Sep 13 02:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-09-13T02:07:54.7242953Z Sep 13 02:07:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-09-13T02:07:54.7243980Z Sep 13 02:07:54 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-09-13T02:07:54.7245076Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
2022-09-13T02:07:54.7246373Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
2022-09-13T02:07:54.7247443Z Sep 13 02:07:54 	at com.sun.proxy.$Proxy46.get_databases(Unknown Source)
2022-09-13T02:07:54.7248576Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAllDatabases(HiveMetaStoreClient.java:1356)
2022-09-13T02:07:54.7249973Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAllDatabases(HiveMetaStoreClient.java:1351)
2022-09-13T02:07:54.7251113Z Sep 13 02:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-09-13T02:07:54.7252388Z Sep 13 02:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-09-13T02:07:54.7253613Z Sep 13 02:07:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-09-13T02:07:54.7254665Z Sep 13 02:07:54 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-09-13T02:07:54.7255792Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:208)
2022-09-13T02:07:54.7256921Z Sep 13 02:07:54 	at com.sun.proxy.$Proxy47.getAllDatabases(Unknown Source)
2022-09-13T02:07:54.7258124Z Sep 13 02:07:54 	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.getAllDatabases(HiveMetastoreClientWrapper.java:102)
2022-09-13T02:07:54.7259340Z Sep 13 02:07:54 	at org.apache.flink.table.catalog.hive.HiveCatalog.listDatabases(HiveCatalog.java:411)
2022-09-13T02:07:54.7260095Z Sep 13 02:07:54 	... 13 more
2022-09-13T02:07:54.7260935Z Sep 13 02:07:54 Caused by: java.lang.RuntimeException: openTransaction called in an interior transaction scope, but currentTransaction is not active.
2022-09-13T02:07:54.7262611Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.ObjectStore.openTransaction(ObjectStore.java:723)
2022-09-13T02:07:54.7263682Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.ObjectStore.getAllDatabases(ObjectStore.java:1150)
2022-09-13T02:07:54.7264592Z Sep 13 02:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-09-13T02:07:54.7265459Z Sep 13 02:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-09-13T02:07:54.7266370Z Sep 13 02:07:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-09-13T02:07:54.7267222Z Sep 13 02:07:54 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-09-13T02:07:54.7268066Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
2022-09-13T02:07:54.7269122Z Sep 13 02:07:54 	at com.sun.proxy.$Proxy44.getAllDatabases(Unknown Source)
2022-09-13T02:07:54.7270453Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_databases(HiveMetaStore.java:1661)
2022-09-13T02:07:54.7271347Z Sep 13 02:07:54 	... 30 more {code}",,fsk119,hxb,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29118,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 23 07:37:43 UTC 2022,,,,,,,,,,"0|z18i54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 03:07;hxb;[~yzl] Could you help take a look? Thx.;;;","13/Sep/22 11:09;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40928&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=26736;;;","14/Sep/22 07:38;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40987&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f;;;","14/Sep/22 07:39;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40985&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f;;;","14/Sep/22 08:28;yzl;[~hxb] I'm trying to find the bug now. Please assign to me.;;;","23/Sep/22 07:37;fsk119;Merged into release-1.16: c653d162f224b8b938e32323a636ae07119f0c11

Merged into master: 6c5f8a8e3cc4e2bc420495ce5b72e43817027978;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Page not enough Exception in SortBufferMemTable,FLINK-29273,13481206,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,13/Sep/22 03:03,13/Sep/22 09:44,13/Jul/23 08:13,13/Sep/22 09:44,,,,,,,,table-store-0.2.1,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,"When there are many partitions, the partition writer will seize memory and may have the following exceptions:
 !screenshot-1.png! 

We need to make sure that the writer has enough memory before it can start.

Actually, there is enough memory, because it can preempt from other writers. The problem is in OwnerMemoryPool.freePages, it should contain preemptable memory.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29276,,,,,,"13/Sep/22 03:03;lzljs3620320;image-2022-09-13-11-03-07-855.png;https://issues.apache.org/jira/secure/attachment/13049201/image-2022-09-13-11-03-07-855.png","13/Sep/22 03:04;lzljs3620320;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13049202/screenshot-1.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 13 09:44:30 UTC 2022,,,,,,,,,,"0|z18i4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 09:44;lzljs3620320;master: a353d1535694fa181a7e2032f6314d74925934bd
release-0.2: f33a12be878006a915daab2ac6e0e0d5e484486d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefaultJobmanagerRunnerRegistry#localCleanupAsync calls close instead of closeAsync,FLINK-29253,13481052,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,12/Sep/22 06:26,13/Sep/22 07:12,13/Jul/23 08:13,13/Sep/22 07:12,1.15.2,1.16.0,1.17.0,,,,,1.15.3,1.16.0,1.17.0,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"{{DefaultJobmanagerRunnerRegistry#localCleanupAsync}} is meant to be called from the main thread. The current implementation calls {{close}} on the {{JobManagerRunner}} instead of {{closeAsync}}. This results in a blocking call on the {{Dispatcher}}'s main thread which we want to avoid.

Thanks for identifying this issue, [~chesnay]",,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 13 07:12:31 UTC 2022,,,,,,,,,,"0|z18h6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 07:12;mapohl;master: 4e99d64b4914ca5fbb1fe338455a8f653c164172

1.16: ef5941e36c06070b49b4557e77ddb6750f2af2af

1.15: 776ccfa27d2d29b1ccd674878eeccee1649ee938;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not overwrite from empty input,FLINK-29241,13480780,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,09/Sep/22 06:52,09/Sep/22 07:22,13/Jul/23 08:13,09/Sep/22 07:22,,,,,,,,table-store-0.2.1,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,"There is currently no data, which will not trigger an overwrite, which causes the semantics of the sql to not be correct",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 09 07:22:02 UTC 2022,,,,,,,,,,"0|z18fio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/22 07:22;lzljs3620320;master: 62d60d53dd52d08bdae2017e2d2a6dd8effc5a8a
release-0.2: 59a1992726ccd437e6cb59a73ec8866b54d7908b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong index information will be obtained after the downstream failover in hybrid full mode,FLINK-29238,13480759,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,09/Sep/22 02:42,13/Sep/22 09:03,13/Jul/23 08:13,13/Sep/22 09:03,1.16.0,,,,,,,1.16.0,,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,"Hybrid shuffle relies on the index to read the disk data. Since the spilled data may be consumed from memory, the readable status is introduced. For the readable buffer, FileDataManager does not pre-load it. However, when the downstream fails, the previous readable status will be used incorrectly, resulting in that some buffer cannot be read correctly.",,hxb,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 13 09:03:43 UTC 2022,,,,,,,,,,"0|z18fe0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 09:03;xtsong;- master (1.17): 12dfe6abd66646c7473bbe353d7b5eb0cfd41130
- release-1.16: 438eddefbf13efa604a6bb8c3b0d3e0e10d741ce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update SnakeYAML to 1.31,FLINK-29235,13480700,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,sergiosp,sergiosp,08/Sep/22 16:35,11/Nov/22 10:06,13/Jul/23 08:13,11/Nov/22 10:06,1.15.3,,,,,,,1.17.0,,,,,Build System,BuildSystem / Shaded,,,,,,0,,,,,"Flink uses snakeyaml v1.27. 
flink-shaded uses Jackson 2.12.4, which used snakeyaml v1.29

Those version are vulnerable to CVE-2022-25857. Flink itself is not directly impacted by this CVE, but we should bump this to avoid false flags. 

Ref:

https://nvd.nist.gov/vuln/detail/CVE-2022-25857

https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-jackson/2.12.4-15.0/flink-shaded-jackson-2.12.4-15.0.pom

https://github.com/apache/flink-shaded/blob/master/flink-shaded-jackson-parent/flink-shaded-jackson-2/pom.xml#L73",,martijnvisser,sergiosp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 11 10:06:21 UTC 2022,,,,,,,,,,"0|z18f14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/22 14:18;martijnvisser;[~chesnay] Should this be fixed before Flink 1.16? ;;;","19/Sep/22 14:44;chesnay;I guess so.;;;","19/Sep/22 14:50;chesnay;Mind you that the CVE likely doesn't really apply because we only use it to parse the configuration.;;;","24/Oct/22 19:45;sergiosp;Hello [~chesnay] 

Noticed the flink-shaded-jackson v2.13.4-16.0 already has the fix (it uses jackson's own snakeyaml version which is 1.31). Could we upgrade flink-shaded version in flink version 1.16.0 to use 2.13.4-16.0?

[https://github.com/apache/flink/blob/release-1.16.0-rc2/pom.xml#L125]

{{<flink.shaded.version>16.0</flink.shaded.version>    <flink.shaded.jackson.version>2.13.4</flink.shaded.jackson.version>}}

 

Ref:

[https://repo1.maven.org/maven2/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.13.4/jackson-dataformat-yaml-2.13.4.pom]

[https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-jackson/2.13.4-16.0/flink-shaded-jackson-2.13.4-16.0.pom]

 ;;;","01/Nov/22 10:38;martijnvisser;Fixed in master: cffb8d9dfc0aed7038574cb16826cf9e9573248e;;;","07/Nov/22 20:00;sergiosp;Hi , could we evaluate for addition in 1.16.1?;;;","11/Nov/22 10:06;martijnvisser;[~sergiosp] I've changed the description of the ticket to better match the current situation:

1. Flink used snakeyaml v1.27 and has been updated to snakeyaml v1.31. That is planned to be released for Flink 1.17.0
2. There is already flink-shaded release 1.16.0, which uses Jackson 2.13.4 and snakeyaml v1.31. 

Given that Flink only uses snakeyaml to parse the config, the impact for Flink is that low that I don't see an immediate need to backport this to Flink 1.16.0. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dead lock in DefaultLeaderElectionService,FLINK-29234,13480686,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,LucentWong,LucentWong,08/Sep/22 14:52,18/Nov/22 14:23,13/Jul/23 08:13,18/Nov/22 14:22,1.13.5,1.14.5,1.15.3,,,,,1.15.4,1.16.1,1.17.0,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"Jobmanager stop working because the deadlock in DefaultLeaderElectionService.

The log stopped at
{code:java}
org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Stopping DefaultLeaderElectionService. {code}
Which may similar to this ticket https://issues.apache.org/jira/browse/FLINK-20008

Here is the jstack info
{code:java}
Found one Java-level deadlock: 
============================= 
""flink-akka.actor.default-dispatcher-18"": waiting to lock monitor 0x00007f15c7eae3a8 (object 0x0000000678d395e8, a java.lang.Object), which is held by ""main-EventThread"" ""main-EventThread"": waiting to lock monitor 0x00007f15a3811258 (object 0x0000000678cf1be0, a java.lang.Object), which is held by ""flink-akka.actor.default-dispatcher-18"" Java stack information for the threads listed above: 
=================================================== 

""flink-akka.actor.default-dispatcher-18"": 
 at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.stop(DefaultLeaderElectionService.java:104) - waiting to lock <0x0000000678d395e8> (a java.lang.Object)
 at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.lambda$closeAsync$0(JobMasterServiceLeadershipRunner.java:147)
 at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner$$Lambda$735/1742012752.run(Unknown Source)
 at org.apache.flink.runtime.concurrent.FutureUtils.lambda$runAfterwardsAsync$18(FutureUtils.java:687)
 at org.apache.flink.runtime.concurrent.FutureUtils$$Lambda$736/6716561.accept(Unknown Source)
 at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
 at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
 at org.apache.flink.runtime.concurrent.DirectExecutorService.execute(DirectExecutorService.java:217)
 at java.util.concurrent.CompletableFuture$UniCompletion.claim(CompletableFuture.java:543)
 at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:765)
 at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
 at java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:795)
 at java.util.concurrent.CompletableFuture.whenCompleteAsync(CompletableFuture.java:2163)
 at org.apache.flink.runtime.concurrent.FutureUtils.runAfterwardsAsync(FutureUtils.java:684)
 at org.apache.flink.runtime.concurrent.FutureUtils.runAfterwards(FutureUtils.java:651)
 at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.closeAsync(JobMasterServiceLeadershipRunner.java:143) - locked <0x0000000678cf1be0> (a java.lang.Object)
 at org.apache.flink.runtime.dispatcher.Dispatcher.terminateJob(Dispatcher.java:807)
 at org.apache.flink.runtime.dispatcher.Dispatcher.terminateRunningJobs(Dispatcher.java:799)
 at org.apache.flink.runtime.dispatcher.Dispatcher.terminateRunningJobsAndGetTerminationFuture(Dispatcher.java:812)
 at org.apache.flink.runtime.dispatcher.Dispatcher.onStop(Dispatcher.java:268)
 at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$$Lambda$444/1289054037.apply(Unknown Source)
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
 at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
 at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
 at akka.actor.Actor.aroundReceive(Actor.scala:517)
 at akka.actor.Actor.aroundReceive$(Actor.scala:515)
 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
 at akka.actor.ActorCell.invoke(ActorCell.scala:561)
 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
 at akka.dispatch.Mailbox.run(Mailbox.scala:225)
 at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


""main-EventThread"":
 at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.runIfStateRunning(JobMasterServiceLeadershipRunner.java:468) - waiting to lock <0x0000000678cf1be0> (a java.lang.Object)
 at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.grantLeadership(JobMasterServiceLeadershipRunner.java:248)
 at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.onGrantLeadership(DefaultLeaderElectionService.java:211) - locked <0x0000000678d395e8> (a java.lang.Object)
 at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.isLeader(ZooKeeperLeaderElectionDriver.java:166)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:693)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:689)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:100)
 at org.apache.flink.shaded.curator4.org.apache.curator.shaded.com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:30)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:92)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch.setLeadership(LeaderLatch.java:688) - locked <0x0000000678d39788> (a org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch.checkLeadership(LeaderLatch.java:567)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch.access$700(LeaderLatch.java:65)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch$7.processResult(LeaderLatch.java:618)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:883)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:653)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:187)
 at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:601)
 at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:508)
{code}",,ConradJam,fpaul,klion26,LucentWong,martijnvisser,tanyuxin,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 18 14:22:06 UTC 2022,,,,,,,,,,"0|z18ey0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/22 09:31;martijnvisser;[~LucentWong] Given that Flink 1.13 is not supported by the community anymore, could you verify that this still occurs in Flink 1.15? ;;;","10/Sep/22 09:46;LucentWong;[~martijnvisser] I think this issue still exists in the master branch. The issue is caused by the ordering of Flink to get the lock.

 

In this line, the class *DefaultLeaderElectionService* will try to get the lock of itself, then invoke the method of 

*leaderContender(which is JobMasterServiceLeaderShipRunner in this case).*  

[https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/DefaultLeaderElectionService.java#L204]

So the order of getting locks is
 # DefaultLeaderElectionService
 # JobMasterServiceLeaderShipRunner

 

And in this line *JobMasterServiceLeaderShipRunner* will try to get the lock of itself, then invoke the method of *leaderElectionService(which is* {*}DefaultLeaderElectionService in this case{*}{*}).{*}

[https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMasterServiceLeadershipRunner.java#L148]

So the order of getting the locks is
 # JobMasterServiceLeaderShipRunner
 # DefaultLeaderElectionService

 

So if these two functions are invoked nearly at the same time, it will cause the dead lock issue.

 ;;;","20/Oct/22 07:58;xtsong;Looks like a valid bug to me. [~Weijie Guo], could you please take a look?;;;","20/Oct/22 09:31;Weijie Guo;Yes, I will take a look and fix this~;;;","02/Nov/22 13:14;fpaul;[~Weijie Guo] can you give an estimate of when this will be fixed? I want to start the release process of 1.15.3 and include this change.;;;","02/Nov/22 13:29;Weijie Guo;[~fpaul] , The PR changes the production code very little, and it may take 1～2 days to complete the review.;;;","18/Nov/22 14:22;xtsong;- master (1.17): d745f5b3f7a64445854c735668afa9b72edb3fee
- release-1.16: a5b9efec917c696a5836fd1779ed787d6e7a7108
- release-1.15: 74097409e7a874584d28b84d10bf6bfe98a11af3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink UDAF produces different results in the same sliding window,FLINK-29231,13480612,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,xuannan,xuannan,08/Sep/22 09:21,16/Jan/23 01:36,13/Jul/23 08:13,16/Jan/23 01:36,1.15.3,,,,,,,1.15.4,1.16.1,1.17.0,,,API / Python,,,,,,,0,pull-request-available,,,,"It seems that PyFlink udtaf produces different results in the same sliding window. It can be reproduced with the given code and input. It is not always happening but the possibility is relatively high.

The incorrect output is the following:

!image-2022-09-08-17-20-06-296.png!

 

We can see that the output contains different `val_sum` at `window_time` 2022-01-01 00:01:59.999.",,dianfu,hxb,xuannan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/22 09:20;xuannan;image-2022-09-08-17-20-06-296.png;https://issues.apache.org/jira/secure/attachment/13049079/image-2022-09-08-17-20-06-296.png","08/Sep/22 09:20;xuannan;input;https://issues.apache.org/jira/secure/attachment/13049078/input","08/Sep/22 09:20;xuannan;test_agg.py;https://issues.apache.org/jira/secure/attachment/13049077/test_agg.py",,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 01:36:56 UTC 2023,,,,,,,,,,"0|z18ehk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/23 01:36;dianfu;Fixed in:
- master via addca4e18aa1806984070b8c450d3d0df5e473d3
- release-1.16 via 6d3d66ca71a87c50f9d160b5c05aa6cbe49ae1b0
- release-1.15 via d9115d8c2aae75c0b17471c1ab8a0769e606def1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix HiveServer2 Endpoint doesn't support execute statements in sync mode,FLINK-29229,13480589,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,08/Sep/22 07:03,23/Sep/22 07:15,13/Jul/23 08:13,22/Sep/22 10:32,1.16.0,,,,,,,,,,,,Connectors / Hive,Table SQL / Gateway,,,,,,0,pull-request-available,,,,,,fsk119,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 22 10:32:35 UTC 2022,,,,,,,,,,"0|z18ecg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 10:32;fsk119;Merged into master: f2c72772345e25f51faa70185e6ebd764f63b05a

Merged into release-1.16: 6493e7c1c6a2eb050e5415c1748ae5c267625ea2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Align the schema of the  HiveServer2 getMetadata with JDBC,FLINK-29228,13480588,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,fsk119,fsk119,08/Sep/22 07:02,22/Sep/22 10:35,13/Jul/23 08:13,22/Sep/22 10:35,1.16.0,,,,,,,1.16.0,,,,,Connectors / Hive,Table SQL / Gateway,,,,,,0,pull-request-available,,,,,,fsk119,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 22 10:35:33 UTC 2022,,,,,,,,,,"0|z18ec8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 10:35;fsk119;Merged into release-1.16: fd76fb5bba579ed85da84a361a406db7af54ae9e

Merged into master: 1650734eeb3e4400e97ee0dd2089539f35afe467;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-mirror does not pick up the new release branch 1.16,FLINK-29224,13480542,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,07/Sep/22 22:20,12/Sep/22 18:47,13/Jul/23 08:13,12/Sep/22 18:47,,,,,,,,1.17.0,,,,,Build System / CI,,,,,,,0,,,,,"The {{release-1.16}} branch has been cut by the community, but it's not getting picked up by flink-ci/flink-mirror. ",,jingge,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 12 18:46:56 UTC 2022,,,,,,,,,,"0|z18e28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/22 22:24;jingge;Fixed. It seems there was some issues with git fetch.;;;","12/Sep/22 12:22;jingge;permission is missing while pushing the workflow change;;;","12/Sep/22 18:46;jingge;fixed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong behavior for Hive's load data inpath,FLINK-29222,13480472,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,luoyuxia,luoyuxia,luoyuxia,07/Sep/22 12:06,21/Sep/22 01:48,13/Jul/23 08:13,21/Sep/22 01:48,1.16.0,,,,,,,1.16.0,1.17.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,"In hive, `load data inpath` will remove src file, and `load data local inpath` won't remove the src file.

But When using the following sql with Hive dialect:
{code:java}
load data local inpath 'test.txt' INTO TABLE tab2 {code}
The file `test.txt` will be removed, although the expected is not to remove the `test.txt`.

The reason is the parameter order is not right when try to call `HiveCatalog#loadTable(...,  isOverWrite, isSourceLocal)`,

It'll call it with 
{code:java}
hiveCatalog.loadTable(
       ..., 
        hiveLoadDataOperation.isSrcLocal(), // should be isOverwrite
        hiveLoadDataOperation.isOverwrite()); // should be isSrcLocal{code}
 

 

 ",,jark,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28952,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 20 14:30:37 UTC 2022,,,,,,,,,,"0|z18dnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/22 14:30;jark;Fixed in  
 - master: 4448d9fd5e344bd0c2e197c2676c403bc2b665b9
 - release-1.16: bff0985aef4ed43681e6ad3bd81fc460bef3c6a5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CREATE TABLE AS statement blocks SQL client's execution,FLINK-29219,13480410,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lsy,renqs,renqs,07/Sep/22 07:56,01/Nov/22 08:47,13/Jul/23 08:13,27/Sep/22 01:53,1.16.0,,,,,,,1.16.0,1.17.0,,,,Table SQL / API,Table SQL / Client,,,,,,0,pull-request-available,,,,"When executing CREATE TABLE AS statement to create a sink table in SQL client, the client could create the table in catalog and submit the job to cluster successfully, but stops emitting new prompts and accepts new inputs, and user has to use SIGTERM (Control + C) to forcefully stop the SQL client. 

As contrast the behavior of INSERT INTO statement in SQL client is printing ""Job is submitted with JobID xxxx"" and being ready to accept user's input. 

From the log it looks like the client was waiting for the job to finish.",,hxbks2ks,jark,lsy,renqs,tartarus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 27 01:53:39 UTC 2022,,,,,,,,,,"0|z18d9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/22 04:22;lsy;cc [~tartarus] ;;;","08/Sep/22 06:21;tartarus;[~renqs]  Thank you very much for your feedback on this issue!

I will try to reproduce this soon and can you give more informations such as logs or sql, thanks;;;","15/Sep/22 07:06;tartarus;[~renqs]  This problem has been located because sql-client does not handle `CreateTableASOperation`, I have reproduced offline.

I will fix it soon;;;","27/Sep/22 01:53;jark;Fixed in 
 - master: 44009efc5400c6706563fa53335eccf445cf5d0c
 - release-1.16: 4c126223cbc2dec3f08b50c1b398bdf522747ba9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CoordinatorEventsToStreamOperatorRecipientExactlyOnceITCase.testConcurrentCheckpoint failed with AssertionFailedError,FLINK-29217,13480390,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yunfengzhou,hxb,hxb,07/Sep/22 05:59,03/Jan/23 09:15,13/Jul/23 08:13,03/Jan/23 09:15,1.16.0,,,,,,,1.16.1,1.17.0,,,,Runtime / Checkpointing,,,,,,,0,pull-request-available,test-stability,,,"{code:java}
2022-09-07T02:00:50.2507464Z Sep 07 02:00:50 [ERROR] org.apache.flink.streaming.runtime.tasks.CoordinatorEventsToStreamOperatorRecipientExactlyOnceITCase.testConcurrentCheckpoint  Time elapsed: 2.137 s  <<< FAILURE!
2022-09-07T02:00:50.2508673Z Sep 07 02:00:50 org.opentest4j.AssertionFailedError: 
2022-09-07T02:00:50.2509309Z Sep 07 02:00:50 
2022-09-07T02:00:50.2509945Z Sep 07 02:00:50 Expecting value to be false but was true
2022-09-07T02:00:50.2511950Z Sep 07 02:00:50 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2022-09-07T02:00:50.2513254Z Sep 07 02:00:50 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2022-09-07T02:00:50.2514621Z Sep 07 02:00:50 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2022-09-07T02:00:50.2516342Z Sep 07 02:00:50 	at org.apache.flink.streaming.runtime.tasks.CoordinatorEventsToStreamOperatorRecipientExactlyOnceITCase.testConcurrentCheckpoint(CoordinatorEventsToStreamOperatorRecipientExactlyOnceITCase.java:173)
2022-09-07T02:00:50.2517852Z Sep 07 02:00:50 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-09-07T02:00:50.2518888Z Sep 07 02:00:50 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-09-07T02:00:50.2520065Z Sep 07 02:00:50 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-09-07T02:00:50.2521153Z Sep 07 02:00:50 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-09-07T02:00:50.2522747Z Sep 07 02:00:50 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-09-07T02:00:50.2523973Z Sep 07 02:00:50 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-09-07T02:00:50.2525158Z Sep 07 02:00:50 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-09-07T02:00:50.2526347Z Sep 07 02:00:50 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-09-07T02:00:50.2527525Z Sep 07 02:00:50 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-09-07T02:00:50.2528646Z Sep 07 02:00:50 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-09-07T02:00:50.2529708Z Sep 07 02:00:50 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-09-07T02:00:50.2530744Z Sep 07 02:00:50 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-09-07T02:00:50.2532008Z Sep 07 02:00:50 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-09-07T02:00:50.2533137Z Sep 07 02:00:50 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-09-07T02:00:50.2544265Z Sep 07 02:00:50 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-09-07T02:00:50.2545595Z Sep 07 02:00:50 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-09-07T02:00:50.2546782Z Sep 07 02:00:50 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-09-07T02:00:50.2547810Z Sep 07 02:00:50 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-09-07T02:00:50.2548890Z Sep 07 02:00:50 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-09-07T02:00:50.2549932Z Sep 07 02:00:50 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-09-07T02:00:50.2550933Z Sep 07 02:00:50 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-09-07T02:00:50.2552325Z Sep 07 02:00:50 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-09-07T02:00:50.2553660Z Sep 07 02:00:50 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-09-07T02:00:50.2554661Z Sep 07 02:00:50 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-09-07T02:00:50.2555590Z Sep 07 02:00:50 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-09-07T02:00:50.2556454Z Sep 07 02:00:50 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-09-07T02:00:50.2557291Z Sep 07 02:00:50 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-09-07T02:00:50.2558317Z Sep 07 02:00:50 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-09-07T02:00:50.2559462Z Sep 07 02:00:50 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-09-07T02:00:50.2560581Z Sep 07 02:00:50 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-09-07T02:00:50.2562110Z Sep 07 02:00:50 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-09-07T02:00:50.2563590Z Sep 07 02:00:50 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-09-07T02:00:50.2564992Z Sep 07 02:00:50 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-09-07T02:00:50.2566400Z Sep 07 02:00:50 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-09-07T02:00:50.2567801Z Sep 07 02:00:50 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-09-07T02:00:50.2569115Z Sep 07 02:00:50 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-09-07T02:00:50.2570303Z Sep 07 02:00:50 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-09-07T02:00:50.2572140Z Sep 07 02:00:50 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-09-07T02:00:50.2573462Z Sep 07 02:00:50 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-09-07T02:00:50.2574744Z Sep 07 02:00:50 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-09-07T02:00:50.2576081Z Sep 07 02:00:50 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-09-07T02:00:50.2577397Z Sep 07 02:00:50 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-09-07T02:00:50.2578627Z Sep 07 02:00:50 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-09-07T02:00:50.2579773Z Sep 07 02:00:50 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-09-07T02:00:50.2580911Z Sep 07 02:00:50 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-09-07T02:00:50.2582658Z Sep 07 02:00:50 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548) {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40763&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8",,hxb,yunfengzhou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28941,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 26 08:11:52 UTC 2022,,,,,,,,,,"0|z18d5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/22 05:59;hxb;[~yunfengzhou] Could you help take a look? Thx.;;;","08/Sep/22 06:11;hxb;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40789&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef]

 ;;;","20/Oct/22 11:42;chesnay;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42281&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","26/Dec/22 08:11;yunfengzhou;According to offline discussions with Dong Lin, a solution to this problem could be to make OperatorCoordinators generate checkpoint barriers and send them to their subtasks. The subtasks would need to align these barriers with the ones they receive from upstream operators or sources, and actually trigger the checkpoint when checkpoint barrier alignment is reached.

The solution mentioned above requires further discussion about the behavior and performance of Flink runtime during checkpoints. Given that currently no subclass of OperatorCoordinator would be affected by this function, it is thus of lower priority and we would temporarily remove the guarantee that this function works correctly under concurrent checkpoints.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documented SQL Client setup when using Docker Compose doesn't work,FLINK-29210,13480259,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,06/Sep/22 10:10,07/Sep/22 14:47,13/Jul/23 08:13,07/Sep/22 14:47,,,,,,,,1.17.0,,,,,Deployment / Scripts,Table SQL / Client,,,,,,0,pull-request-available,,,,The SQL Client Docker Compose setup at https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/ is missing required parameter {{rest.address}},,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 07 14:47:29 UTC 2022,,,,,,,,,,"0|z18cc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/22 14:47;martijnvisser;Fixed in master: 442ab0ce6cbba4e276659db765846f93ab07abdf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar message eventTime may be incorrectly set to a negative number,FLINK-29207,13480247,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,wenbing.shen,wenbing.shen,wenbing.shen,06/Sep/22 09:26,09/Sep/22 10:53,13/Jul/23 08:13,09/Sep/22 10:52,1.15.2,,,,,,,1.15.3,1.16.0,,,,Connectors / Pulsar,,,,,,,0,pull-request-available,,,,"We'd better judge that the timestamp is greater than 0, we should skip setting eventTime when timestamp less than or equal to 0, otherwise the pulsar client will throw an exception.

!image-2022-09-06-17-21-46-923.png!

!image-2022-09-06-17-20-27-220.png!",,tison,wenbing.shen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28506,,,FLINK-27736,,,,,,,,,,,,,,"06/Sep/22 09:20;wenbing.shen;image-2022-09-06-17-20-27-220.png;https://issues.apache.org/jira/secure/attachment/13049001/image-2022-09-06-17-20-27-220.png","06/Sep/22 09:21;wenbing.shen;image-2022-09-06-17-21-46-923.png;https://issues.apache.org/jira/secure/attachment/13049000/image-2022-09-06-17-21-46-923.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 09 10:53:05 UTC 2022,,,,,,,,,,"0|z18c9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/22 10:52;tison;master via 1e6897c24e3b05e5eecebfc6c5b75a9492011529
release-1.16 via 77500580899c94ccf1adba8f0aeb0d28aeec7a56
release-1.15 via a3adb1343681f3d3b01e7cc373027be63755f0b2;;;","09/Sep/22 10:53;tison;Thanks for your report and fixing [~wenbing.shen]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkKinesisConsumer not respecting Credential Provider configuration for EFO,FLINK-29205,13480240,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,dannycranmer,dannycranmer,06/Sep/22 08:37,06/Sep/22 16:00,13/Jul/23 08:13,06/Sep/22 16:00,1.15.2,,,,,,,1.15.3,1.16.0,,,,Connectors / Kinesis,,,,,,,0,pull-request-available,,,,"Reported in [https://lists.apache.org/thread/xgpk0n59z5wq7kg6j8m8pgy5mcjzvvw5]

 

FlinkKinesisConsumer is not respecting the credential provider configuration. It appears as though the legacy property transform is discarding valid config, this results in the AUTO credential provider being used

- https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/proxy/KinesisProxyV2Factory.java#L65",,dannycranmer,nuafonso,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 06 16:00:13 UTC 2022,,,,,,,,,,"0|z18c88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/22 16:00;dannycranmer;1.15: Merged commit [{{e1cbf9e}}|https://github.com/apache/flink/commit/e1cbf9ed54c04118f3d6f6a89fc23599be0a0bb7] into apache:release-1.15
1.16: Merged commit [{{b0830ce}}|https://github.com/apache/flink/commit/b0830cedb3f0d30a16fc2a312bc7beb6900b2cd9] into apache:release-1.16 
1.17: Merged commit [{{cb32dad}}|https://github.com/apache/flink/commit/cb32dad300652cceb3c909c2e3d495538463a55e] into apache:master ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive dialect can't get value for the variables set by  set command,FLINK-29191,13480092,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,05/Sep/22 08:11,21/Sep/22 01:47,13/Jul/23 08:13,21/Sep/22 01:47,1.16.0,,,,,,,1.16.0,1.17.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,"When using Hive dialect, we can use 
{code:java}
set k1=v1;
{code}
to set variable to Flink's table config.

But if we want the get the value for `k1` by using 
{code:java}
set k1;
{code}
we will get nothing.

The reason is Hive dialect won't lookup the vairable in Flink's table config.

To fix it, we also need to lookup in Flink's table config.",,jark,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 21 01:47:39 UTC 2022,,,,,,,,,,"0|z18bc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/22 01:47;jark;Fixed in 
 - master: 64c550c67c2d580f369dfaa6ff48e2e6816c6fcd
 - release-1.16: d4d855a3c08733afac935d87df6544f0811aef84;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink hive left join mysql error,FLINK-29190,13480084,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,kcz,kcz,05/Sep/22 07:22,05/Sep/22 08:53,13/Jul/23 08:13,05/Sep/22 08:53,1.14.0,,,,,,,1.15.2,,,,,Table SQL / Runtime,,,,,,,0,,,,,"If I remove the custom function, it is working fine.

select * from hive_table left join mysql_table on hive_table.id=mysql_table .id where

mysql_table .name='kcz' and hive_table.dt=date_sub(CURRENT_DATE, 1);

 

date_sub is a user function.

public class DateSubUDF extends ScalarFunction {


public String eval(LocalDate date, int day) {
return DateUtils.getStringByLocalDate(date.minusDays(day));
}
}

 

CREATE TABLE if not exists car_info
(
vin STRING,
battery_factory STRING
) WITH (
'connector' = 'jdbc',
'url' = 'url',
'table-name' = 'car_info',
'username' = 'name',
'password' = 'password'
);

hive table does not use flink's table building syntax, using the hive native table.

 

this is a error log.

 

 

org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Could not instantiate generated class 'PartitionPruner$9'
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372)
    at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
    at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)
    at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:812)
    at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:246)
    at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1054)
    at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1132)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1761)
    at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
    at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1132)
Caused by: java.lang.RuntimeException: Could not instantiate generated class 'PartitionPruner$9'
    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:75)
    at org.apache.flink.table.planner.plan.utils.PartitionPruner$.prunePartitions(PartitionPruner.scala:112)
    at org.apache.flink.table.planner.plan.utils.PartitionPruner.prunePartitions(PartitionPruner.scala)
    at org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.lambda$onMatch$3(PushPartitionIntoTableSourceScanRule.java:163)
    at org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.readPartitionFromCatalogWithoutFilterAndPrune(PushPartitionIntoTableSourceScanRule.java:373)
    at org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.readPartitionFromCatalogAndPrune(PushPartitionIntoTableSourceScanRule.java:351)
    at org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.readPartitionsAndPrune(PushPartitionIntoTableSourceScanRule.java:303)
    at org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.onMatch(PushPartitionIntoTableSourceScanRule.java:171)
    at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
    at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
    at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
    at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)
    at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
    at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
    at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:63)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:60)
    at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
    at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
    at scala.collection.Iterator$class.foreach(Iterator.scala:891)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
    at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:60)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:55)
    at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
    at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
    at scala.collection.immutable.Range.foreach(Range.scala:160)
    at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:55)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
    at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
    at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
    at scala.collection.Iterator$class.foreach(Iterator.scala:891)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
    at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:163)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:77)
    at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
    at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:300)
    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:183)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1665)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:805)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1274)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:742)
    at com.hycan.bigdata.utils.SqlUtils.callCommand(SqlUtils.java:49)
    at com.hycan.bigdata.job.SchemaJob.main(SchemaJob.java:94)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
    ... 11 more
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:76)
    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102)
    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69)
    ... 72 more
Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74)
    ... 74 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:89)
    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
    ... 77 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 11, Column 30: Cannot determine simple type name ""com""
    at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6833)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573)
    at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481)
    at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476)
    at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928)
    at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476)
    at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469)
    at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927)
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
    at org.codehaus.janino.UnitCompiler.access$1300(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$25.getType(UnitCompiler.java:8271)
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6873)
    at org.codehaus.janino.UnitCompiler.access$14400(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$22$2$1.visitFieldAccess(UnitCompiler.java:6499)
    at org.codehaus.janino.UnitCompiler$22$2$1.visitFieldAccess(UnitCompiler.java:6494)
    at org.codehaus.janino.Java$FieldAccess.accept(Java.java:4310)
    at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6494)
    at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6490)
    at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148)
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6490)
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6469)
    at org.codehaus.janino.Java$Rvalue.accept(Java.java:4116)
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6855)
    at org.codehaus.janino.UnitCompiler.access$14200(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$22$2$1.visitAmbiguousName(UnitCompiler.java:6497)
    at org.codehaus.janino.UnitCompiler$22$2$1.visitAmbiguousName(UnitCompiler.java:6494)
    at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4224)
    at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6494)
    at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6490)
    at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148)
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6490)
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6469)
    at org.codehaus.janino.Java$Rvalue.accept(Java.java:4116)
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
    at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9026)
    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5062)
    at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423)
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396)
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)
    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396)
    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3783)
    at org.codehaus.janino.UnitCompiler.access$5900(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3762)
    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3734)
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)
    at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)
    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)
    at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
    at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
    at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
    at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86)
    ... 83 more",,jark,kcz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 05 08:52:11 UTC 2022,,,,,,,,,,"0|z18bag:",9223372036854775807,https://issues.apache.org/jira/browse/FLINK-24761,,,,,,,,,,,,,,,,,,,"05/Sep/22 08:03;jark;Is it caused by FLINK-24761? Could you try a newer version? ;;;","05/Sep/22 08:52;kcz;tks jark,use 1.15.2 ok.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The option `sql-gateway.endpint.hiveserver2.catalog.hive-conf-dir` should be requried,FLINK-29188,13480045,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,fsk119,fsk119,05/Sep/22 02:22,24/Oct/22 02:05,13/Jul/23 08:13,24/Oct/22 02:05,1.16.0,,,,,,,1.16.0,,,,,Connectors / Hive,Table SQL / Gateway,,,,,,0,,,,,,,fsk119,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28954,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-05 02:22:52.0,,,,,,,,,,"0|z18b1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to execute USING JAR in Hive Dialect,FLINK-29185,13479826,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,fsk119,fsk119,02/Sep/22 09:11,21/Sep/22 01:48,13/Jul/23 08:13,21/Sep/22 01:48,1.16.0,,,,,,,1.16.0,1.17.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,,,fsk119,jark,luoyuxia,Paul Lin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28952,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 20 09:12:30 UTC 2022,,,,,,,,,,"0|z189pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/22 06:06;luoyuxia;Only happens for `create temporary function xxx using jar`. The reason is haven't registered such resource when creating  temporary function.

To fix it, we need to register such resource.;;;","20/Sep/22 09:12;jark;Fixed in
 - master: 3994788892fc761cf0c2fd09f362d4dab8f14c61
 - release-1.16: 82ab2918e992f747043dbe49d900b36fe28df282;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Gateway should remove the downloaded resources,FLINK-29184,13479820,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,02/Sep/22 09:05,08/Sep/22 11:42,13/Jul/23 08:13,08/Sep/22 06:28,1.16.0,,,,,,,1.16.0,,,,,Table SQL / Gateway,,,,,,,0,,,,,SessionContext doesn't close the `ResourceManager`.,,fsk119,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28953,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 08 06:28:54 UTC 2022,,,,,,,,,,"0|z189o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/22 06:28;fsk119;Merged into master: bd1d97391be408885dc8df35c8a83770fe7e0c38

Merged into release-1.16: 219b78d840cedbf71cc1345cf1a182a931456a17;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SumAggFunction redundant computations,FLINK-29182,13479784,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,02/Sep/22 06:42,05/Sep/22 04:16,13/Jul/23 08:13,05/Sep/22 04:16,1.16.0,,,,,,,1.17.0,,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"{code:java}
// code placeholder
public Expression[] accumulateExpressions() {
    return new Expression[] {
        /* sum = */ ifThenElse(
                isNull(operand(0)),
                sum,
                ifThenElse(
                        isNull(operand(0)),
                        sum,
                        ifThenElse(isNull(sum), operand(0), adjustedPlus(sum, operand(0)))))
    };
} {code}
it exists redundant computations",,jackylau,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 05 04:16:43 UTC 2022,,,,,,,,,,"0|z189g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/22 04:16;jark;Fixed in master: b98534cafd03cde47ecd6d54248a397540dffa71;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaConsumerThread should catch WakeupException when committing offsets,FLINK-29153,13479430,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,renqs,renqs,renqs,31/Aug/22 07:01,11/Oct/22 08:47,13/Jul/23 08:13,13/Sep/22 02:20,1.16.0,,,,,,,1.16.0,,,,,Connectors / Kafka,,,,,,,0,pull-request-available,,,,"{{KafkaConsumerThread}} in the legacy {{FlinkKafkaConsumer}} makes a wakeup on the {{KafkaConsumer}} on offset commit to wakeup the potential blocking {{KafkaConsumer.poll()}}. However the wakeup might happen when the consumer is not polling. The wakeup will be remembered by the consumer and re-examined while committing the offset asynchronously, which leads to an unnecessary {{WakeupException}}. 

As the JavaDoc and method signature of KafkaConsumer does not show that the WakeupException could be thrown in KafkaConsumer#commitAsync, this could be considered as a bug on Kafka side (breaking the contract of public API), but we can still fix it by catching the WakeupException and retrying. ",,hxb,renqs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29018,FLINK-24119,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 09 03:18:12 UTC 2022,,,,,,,,,,"0|z187aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/22 03:01;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40563&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","02/Sep/22 02:27;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40612&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","02/Sep/22 08:48;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40616&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc;;;","02/Sep/22 08:48;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40616&view=logs&j=9c5a5fe6-2f39-545e-1630-feb3d8d0a1ba&t=99b23320-1d05-5741-d63f-9e78473da39e;;;","05/Sep/22 11:06;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40674&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","05/Sep/22 11:07;hxb;[~renqs] Any updates on the progress?;;;","06/Sep/22 06:33;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40714&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=ffa8837a-b445-534e-cdf4-db364cf8235d;;;","07/Sep/22 01:46;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40742&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=36925;;;","07/Sep/22 01:52;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40759&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","07/Sep/22 06:07;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40763&view=logs&j=9c5a5fe6-2f39-545e-1630-feb3d8d0a1ba&t=99b23320-1d05-5741-d63f-9e78473da39e;;;","08/Sep/22 06:09;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40789&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=36934;;;","08/Sep/22 06:11;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40789&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc;;;","08/Sep/22 06:11;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40789&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a;;;","09/Sep/22 02:21;renqs;Fixed on master: 3e883e687d2bdb80ad2f160b08184fca11f4f554

release-1.16: dbcf949f89b5bead6c2bec4de77ca68bc8614fe6;;;","09/Sep/22 03:16;hxb;[~renqs] Should we need to cp this fix to release-1.16;;;","09/Sep/22 03:18;renqs;[~hxb] Yes I'm waiting for the CI on release-1.16. Will close the ticket then.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Describe statement resutls is different from the Hive ,FLINK-29152,13479428,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,fsk119,fsk119,31/Aug/22 06:50,21/Sep/22 15:10,13/Jul/23 08:13,21/Sep/22 15:10,1.16.0,,,,,,,1.16.0,1.17.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,"In hive, the results schema is 


{code:java}
+-----------+------------+----------+
| col_name  | data_type  | comment  |
+-----------+------------+----------+
| a         | int        |          |
| b         | string     |          |
+-----------+------------+----------+
{code}

but our implementation is 

{code:java}
0: jdbc:hive2://localhost:10000/default> describe sink;
+-------+-------+-------+-------+---------+------------+
| name  | type  | null  |  key  | extras  | watermark  |
+-------+-------+-------+-------+---------+------------+
| a     | INT   | true  | NULL  | NULL    | NULL       |
+-------+-------+-------+-------+---------+------------+
{code}

BTW, it's better we can support {{DESCRIBE FORMATTED}} like hive does.

{code:java}
+-------------------------------+----------------------------------------------------+-----------------------+
|           col_name            |                     data_type                      |        comment        |
+-------------------------------+----------------------------------------------------+-----------------------+
| # col_name                    | data_type                                          | comment               |
|                               | NULL                                               | NULL                  |
| a                             | int                                                |                       |
| b                             | string                                             |                       |
|                               | NULL                                               | NULL                  |
| # Detailed Table Information  | NULL                                               | NULL                  |
| Database:                     | default                                            | NULL                  |
| Owner:                        | null                                               | NULL                  |
| CreateTime:                   | Tue Aug 30 06:54:00 UTC 2022                       | NULL                  |
| LastAccessTime:               | UNKNOWN                                            | NULL                  |
| Retention:                    | 0                                                  | NULL                  |
| Location:                     | hdfs://namenode:8020/user/hive/warehouse/sink      | NULL                  |
| Table Type:                   | MANAGED_TABLE                                      | NULL                  |
| Table Parameters:             | NULL                                               | NULL                  |
|                               | comment                                            |                       |
|                               | numFiles                                           | 0                     |
|                               | totalSize                                          | 0                     |
|                               | transient_lastDdlTime                              | 1661842440            |
|                               | NULL                                               | NULL                  |
| # Storage Information         | NULL                                               | NULL                  |
| SerDe Library:                | org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe | NULL                  |
| InputFormat:                  | org.apache.hadoop.mapred.TextInputFormat           | NULL                  |
| OutputFormat:                 | org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat | NULL                  |
| Compressed:                   | No                                                 | NULL                  |
| Num Buckets:                  | -1                                                 | NULL                  |
| Bucket Columns:               | []                                                 | NULL                  |
| Sort Columns:                 | []                                                 | NULL                  |
| Storage Desc Params:          | NULL                                               | NULL                  |
|                               | serialization.format                               | 1                     |
+-------------------------------+----------------------------------------------------+-----------------------+
{code}

",,fsk119,jark,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28952,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 21 15:10:16 UTC 2022,,,,,,,,,,"0|z187ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/22 08:10;luoyuxia;I will first to fix insistent behavior for `describe table` statement.;;;","21/Sep/22 15:10;jark;Fixed in
 - master: b5cd9f34ab73fa69a3db5a09908c1aa954ed0597
 - release-1.16: a6e954ca3bff9c62713d475627b49dd18a4f02fd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SHOW CREATE TABLE doesn't work for Hive dialect,FLINK-29151,13479426,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,fsk119,fsk119,31/Aug/22 06:45,21/Sep/22 15:09,13/Jul/23 08:13,21/Sep/22 15:09,1.16.0,,,,,,,1.16.0,1.17.0,,,,Connectors / Hive,,,,,,,0,,,,,"{code:java}
0: jdbc:hive2://localhost:10000/default> show create table sink;
Error: org.apache.flink.table.gateway.service.utils.SqlExecutionException: Failed to execute the operation 9b060771-34b8-453d-abf5-674c86b62921.
    at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.processThrowable(OperationManager.java:389)
    at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:248)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.table.api.ValidationException
    at org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.handleUnsupportedOperation(HiveParserDDLSemanticAnalyzer.java:2188)
    at org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.convertToOperation(HiveParserDDLSemanticAnalyzer.java:414)
    at org.apache.flink.table.planner.delegation.hive.HiveParser.processCmd(HiveParser.java:334)
    at org.apache.flink.table.planner.delegation.hive.HiveParser.parse(HiveParser.java:213)
    at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:90)
    at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$0(SqlGatewayServiceImpl.java:182)
    at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:111)
    at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:239)
    ... 7 more
Caused by: java.lang.UnsupportedOperationException: Unsupported operation: TOK_SHOW_CREATETABLE
    ... 15 more (state=,code=0) {code}",,fsk119,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28952,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 21 07:31:27 UTC 2022,,,,,,,,,,"0|z187a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/22 07:31;jark;Fixed in 
 - master: 7ddf059d3b7b6888f550bfca9eb09c0cdeb7d682
 - release-1.16: 8a5eec9945a3cdbd35b934508586803f470a3f2a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Project pushdown not work for lookup source,FLINK-29138,13479270,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,30/Aug/22 09:02,04/Jul/23 07:41,13/Jul/23 08:13,05/Sep/22 02:37,,,,,,,,1.14.6,1.15.3,1.16.0,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"Current tests: LookupJoinTest#testJoinTemporalTableWithProjectionPushDown
{code:java}
@Test
def testJoinTemporalTableWithProjectionPushDown(): Unit = {
val sql =
""""""
|SELECT T.*, D.id
|FROM MyTable AS T
|JOIN LookupTable FOR SYSTEM_TIME AS OF T.proctime AS D
|ON T.a = D.id
"""""".stripMargin

util.verifyExecPlan(sql)
}

{code}
the optimized plan doesn't print the selected columns from lookup source, but actually it didn't push the project into lookup source (still select all columns from source), this is not as expected
{code:java}
<Resource name=""optimized exec plan"">
<![CDATA[
Calc(select=[a, b, c, PROCTIME_MATERIALIZE(proctime) AS proctime, rowtime, id])
+- LookupJoin(table=[default_catalog.default_database.LookupTable], joinType=[InnerJoin], lookup=[id=a], select=[a, b, c, proctime, rowtime, id])
+- DataStreamScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, proctime, rowtime])
]]>
</Resource>

{code}
 

incorrect intermediate optimization result
{code:java}
=========  logical_rewrite ========
 optimize result: 
FlinkLogicalJoin(condition=[=($0, $5)], joinType=[inner])
:- FlinkLogicalDataStreamTableScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, proctime, rowtime])
+- FlinkLogicalSnapshot(period=[$cor0.proctime])
   +- FlinkLogicalCalc(select=[id])
      +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LookupTable]], fields=[id, name, age])


=========  time_indicator ========
 optimize result: 
FlinkLogicalCalc(select=[a, b, c, PROCTIME_MATERIALIZE(proctime) AS proctime, rowtime, id])
+- FlinkLogicalJoin(condition=[=($0, $5)], joinType=[inner])
   :- FlinkLogicalDataStreamTableScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, proctime, rowtime])
   +- FlinkLogicalSnapshot(period=[$cor0.proctime])
      +- FlinkLogicalCalc(select=[id])
         +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LookupTable]], fields=[id, name, age])

{code}
 

plan comparison after fix

!image-2022-08-30-20-33-24-105.png!",,aitozi,godfreyhe,leonard,lincoln.86xy,lzljs3620320,qingyue,waywtdcc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20840,,,,,,"30/Aug/22 12:33;lincoln.86xy;image-2022-08-30-20-33-24-105.png;https://issues.apache.org/jira/secure/attachment/13048768/image-2022-08-30-20-33-24-105.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 04 07:41:54 UTC 2023,,,,,,,,,,"0|z186c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 09:11;lzljs3620320;[~lincoln.86xy] Thanks for the reporting. Assigned to u~;;;","31/Aug/22 02:24;lzljs3620320;I think we should fix this in 1.14 and 1.15 too, what do you think? [~lincoln.86xy] [~godfreyhe];;;","31/Aug/22 03:32;godfreyhe;+1 for 1.14 and 1.15, because this change does effect plan and state. The project will be pushed into lookup operator but not into the lookup source, so this is a inner change in lookup operator. WDYT ? [~lincoln.86xy][~lzljs3620320];;;","31/Aug/22 12:33;lincoln.86xy;[~godfreyhe] [~lzljs3620320]  Since it will not affect compatibility, I agree to fix it also in 1.14 and 1.15, this is a baisc and important optimization.;;;","05/Sep/22 02:37;lzljs3620320;master: 2e2fb24bdbc69c9d85883081b0c29f9db254088e
release-1.15: e7c7df4c9b07667344e33c23bb92cb8a07e3ac0b
release-1.14: 54b6b69941533b756dbd4f68b43d8d4118a8c4e5;;;","04/Jul/23 07:41;waywtdcc;hello, Can this pr be merged into 1.14.5? Which PR do you still rely on?   [~lzljs3620320] 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SubtaskMetricStore causes memory leak.,FLINK-29132,13479208,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pltbkd,nagist,nagist,30/Aug/22 03:08,08/Sep/22 12:59,13/Jul/23 08:13,08/Sep/22 12:59,1.16.0,,,,,,,1.16.0,,,,,Runtime / Metrics,,,,,,,0,pull-request-available,,,,"In [FLINK-28588], MetricStore supports multiple attempts of a subtask. However, `SubtaskMetricStore` doesn't have a clean mechanism.  In failover scenario, `attempts` pile up and cause OOM.",,gaoyunhaii,mayuehappy,nagist,pltbkd,Zhanghao Chen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28588,,,,,,,,,,,,,"30/Aug/22 06:09;nagist;dump-attempts.png;https://issues.apache.org/jira/secure/attachment/13048745/dump-attempts.png","30/Aug/22 06:09;nagist;dump-metricStore.png;https://issues.apache.org/jira/secure/attachment/13048744/dump-metricStore.png","30/Aug/22 03:56;mayuehappy;image-2022-08-30-11-56-34-608.png;https://issues.apache.org/jira/secure/attachment/13048739/image-2022-08-30-11-56-34-608.png","30/Aug/22 03:58;mayuehappy;image-2022-08-30-11-57-59-325.png;https://issues.apache.org/jira/secure/attachment/13048740/image-2022-08-30-11-57-59-325.png",,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 08 12:59:51 UTC 2022,,,,,,,,,,"0|z185y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 03:58;mayuehappy; i think u may add some log or heap dump analysis to better describe the problem;;;","30/Aug/22 08:02;nagist;updated. thanks for your comment;;;","31/Aug/22 06:55;nagist;Anyone has any thoughts on this?;;;","31/Aug/22 08:25;chesnay;True, the attempts will continue to pile up until the job/tm terminates.

[~pltbkd] please fix by leveraging the execution history size parameter;;;","31/Aug/22 10:33;pltbkd;[~nagist] 
Thanks a lot for reporting the issue! 
And thanks [~chesnay] and [~mayuehappy] for taking care of it! 
I'll fix the issue as soon as possible. ;;;","08/Sep/22 12:59;gaoyunhaii;Merged on master via 26eeabfdd1f5f976ed1b5d761a3469bbcb7d3223

Merged on 1.16 via 4dad2e29a090c731a0474a80e320264040c348ce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the doc description of state.backend.local-recovery,FLINK-29130,13479104,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,29/Aug/22 12:41,31/Aug/22 09:47,13/Jul/23 08:13,31/Aug/22 09:47,,,,,,,,1.14.6,1.15.3,1.16.0,,,Documentation,Runtime / State Backends,,,,,,0,pull-request-available,,,,"Currently, the docs of configuration {{state.backend.local-recovery}} said wrongly on the active scope of this configuration. MemoryStateBackend and HashMapStateBackend actually support this feature.",,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 31 09:47:55 UTC 2022,,,,,,,,,,"0|z185bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/22 09:47;yunta;merged 

master(1.16.0): 581e1fe3682e95e0935e0cd1cf4565529af49f84

release-1.15: ad33fe8ef104801a6470b11c792d6663120c172e

release-1.14: 2fa574910d52804f7bbdeb9597d736644aca7301;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamic paramters are not pushed to working with kubernetes,FLINK-29123,13478856,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pvary,pvary,pvary,26/Aug/22 16:58,30/Aug/22 12:15,13/Jul/23 08:13,30/Aug/22 12:15,1.15.2,,,,,,,1.16.0,,,,,Deployment / Kubernetes,,,,,,,0,pull-request-available,,,,It is not possible to push dynamic parameters for the kubernetes deployments,,gyfora,pvary,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 30 12:15:32 UTC 2022,,,,,,,,,,"0|z183sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/22 09:38;gyfora;I agree this would be an important fix that would allow us to finally leverage this feature in the Flink Kubernetes Operator;;;","30/Aug/22 12:15;gyfora;merged to main c37643031dca2e6d4c299c0d704081a8bffece1d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlGatewayRestAPIStabilityTest.testSqlGatewayRestAPIStability is failed,FLINK-29121,13478811,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,jark,jark,26/Aug/22 11:46,29/Aug/22 06:11,13/Jul/23 08:13,29/Aug/22 06:11,,,,,,,,1.16.0,,,,,Table SQL / Gateway,,,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40416&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4


{code}
2022-08-26T11:03:07.6108823Z Aug 26 11:03:07 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.379 s <<< FAILURE! - in org.apache.flink.table.gateway.rest.compatibility.SqlGatewayRestAPIStabilityTest
2022-08-26T11:03:07.6110033Z Aug 26 11:03:07 [ERROR] org.apache.flink.table.gateway.rest.compatibility.SqlGatewayRestAPIStabilityTest.testSqlGatewayRestAPIStability(SqlGatewayRestAPIVersion)[1]  Time elapsed: 0.347 s  <<< FAILURE!
2022-08-26T11:03:07.6110730Z Aug 26 11:03:07 org.opentest4j.AssertionFailedError: 
2022-08-26T11:03:07.6112493Z Aug 26 11:03:07 No compatible call could be found for {""url"":""/sessions/:session_handle/:operation_handle/cancel"",""method"":""PUT"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""},{""key"":""operation_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody"",""properties"":{""status"":{""type"":""string""}}}}.
2022-08-26T11:03:07.6115158Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/sessions/:session_handle/operations/:operation_handle/cancel"",""method"":""PUT"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""},{""key"":""operation_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody"",""properties"":{""status"":{""type"":""string""}}}}.
2022-08-26T11:03:07.6116664Z Aug 26 11:03:07 	Compatibility grade: 7/8
2022-08-26T11:03:07.6122943Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6123711Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6124489Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6125445Z Aug 26 11:03:07  but was: ""/sessions/:session_handle/operations/:operation_handle/cancel""
2022-08-26T11:03:07.6128775Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/sessions/:session_handle/operations/:operation_handle/close"",""method"":""DELETE"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""},{""key"":""operation_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody"",""properties"":{""status"":{""type"":""string""}}}}.
2022-08-26T11:03:07.6130953Z Aug 26 11:03:07 	Compatibility grade: 6/8
2022-08-26T11:03:07.6131525Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6132055Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6132735Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6133551Z Aug 26 11:03:07  but was: ""/sessions/:session_handle/operations/:operation_handle/close""
2022-08-26T11:03:07.6134338Z Aug 26 11:03:07 		method: 
2022-08-26T11:03:07.6134777Z Aug 26 11:03:07 expected: ""PUT""
2022-08-26T11:03:07.6135124Z Aug 26 11:03:07  but was: ""DELETE""
2022-08-26T11:03:07.6137281Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/sessions/:session_handle/operations/:operation_handle/status"",""method"":""GET"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""},{""key"":""operation_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody"",""properties"":{""status"":{""type"":""string""}}}}.
2022-08-26T11:03:07.6138587Z Aug 26 11:03:07 	Compatibility grade: 6/8
2022-08-26T11:03:07.6138940Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6139281Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6139687Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6140200Z Aug 26 11:03:07  but was: ""/sessions/:session_handle/operations/:operation_handle/status""
2022-08-26T11:03:07.6140643Z Aug 26 11:03:07 		method: 
2022-08-26T11:03:07.6141136Z Aug 26 11:03:07 expected: ""PUT""
2022-08-26T11:03:07.6141622Z Aug 26 11:03:07  but was: ""GET""
2022-08-26T11:03:07.6144287Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/sessions/:session_handle"",""method"":""DELETE"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:CloseSessionResponseBody"",""properties"":{""status"":{""type"":""string""}}}}.
2022-08-26T11:03:07.6146275Z Aug 26 11:03:07 	Compatibility grade: 5/8
2022-08-26T11:03:07.6147068Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6147651Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6148335Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6149084Z Aug 26 11:03:07  but was: ""/sessions/:session_handle""
2022-08-26T11:03:07.6149721Z Aug 26 11:03:07 		method: 
2022-08-26T11:03:07.6150272Z Aug 26 11:03:07 expected: ""PUT""
2022-08-26T11:03:07.6150775Z Aug 26 11:03:07  but was: ""DELETE""
2022-08-26T11:03:07.6151913Z Aug 26 11:03:07 		path-parameters: Existing Path parameter operation_handle was removed.
2022-08-26T11:03:07.6154692Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/api_versions"",""method"":""GET"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:util:GetApiVersionResponseBody"",""properties"":{""versions"":{""type"":""array"",""items"":{""type"":""string""}}}}}.
2022-08-26T11:03:07.6157059Z Aug 26 11:03:07 	Compatibility grade: 4/8
2022-08-26T11:03:07.6157436Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6157777Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6158191Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6158618Z Aug 26 11:03:07  but was: ""/api_versions""
2022-08-26T11:03:07.6159120Z Aug 26 11:03:07 		method: 
2022-08-26T11:03:07.6159445Z Aug 26 11:03:07 expected: ""PUT""
2022-08-26T11:03:07.6159759Z Aug 26 11:03:07  but was: ""GET""
2022-08-26T11:03:07.6160504Z Aug 26 11:03:07 		path-parameters: Existing Path parameter session_handle was removed.
2022-08-26T11:03:07.6161015Z Aug 26 11:03:07 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:07.6161424Z Aug 26 11:03:07 Expecting actual not to be null
2022-08-26T11:03:07.6162919Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/info"",""method"":""GET"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:util:GetInfoResponseBody"",""properties"":{""productName"":{""type"":""string""},""version"":{""type"":""string""}}}}.
2022-08-26T11:03:07.6164087Z Aug 26 11:03:07 	Compatibility grade: 4/8
2022-08-26T11:03:07.6164470Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6164791Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6165205Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6165633Z Aug 26 11:03:07  but was: ""/info""
2022-08-26T11:03:07.6165944Z Aug 26 11:03:07 		method: 
2022-08-26T11:03:07.6166278Z Aug 26 11:03:07 expected: ""PUT""
2022-08-26T11:03:07.6166607Z Aug 26 11:03:07  but was: ""GET""
2022-08-26T11:03:07.6167384Z Aug 26 11:03:07 		path-parameters: Existing Path parameter session_handle was removed.
2022-08-26T11:03:07.6167902Z Aug 26 11:03:07 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:07.6168321Z Aug 26 11:03:07 Expecting actual not to be null
2022-08-26T11:03:07.6170825Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/sessions"",""method"":""POST"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:OpenSessionRequestBody"",""properties"":{""sessionName"":{""type"":""string""},""properties"":{""type"":""object"",""additionalProperties"":{""type"":""string""}}}},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:OpenSessionResponseBody"",""properties"":{""sessionHandle"":{""type"":""string""}}}}.
2022-08-26T11:03:07.6173021Z Aug 26 11:03:07 	Compatibility grade: 4/8
2022-08-26T11:03:07.6173390Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6173735Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6174215Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6174652Z Aug 26 11:03:07  but was: ""/sessions""
2022-08-26T11:03:07.6174989Z Aug 26 11:03:07 		method: 
2022-08-26T11:03:07.6175300Z Aug 26 11:03:07 expected: ""PUT""
2022-08-26T11:03:07.6175639Z Aug 26 11:03:07  but was: ""POST""
2022-08-26T11:03:07.6176375Z Aug 26 11:03:07 		path-parameters: Existing Path parameter session_handle was removed.
2022-08-26T11:03:07.6177054Z Aug 26 11:03:07 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:07.6177499Z Aug 26 11:03:07 Expecting actual not to be null
2022-08-26T11:03:07.6179177Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/sessions/:session_handle"",""method"":""GET"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:GetSessionConfigResponseBody"",""properties"":{""properties"":{""type"":""object"",""additionalProperties"":{""type"":""string""}}}}}.
2022-08-26T11:03:07.6180529Z Aug 26 11:03:07 	Compatibility grade: 4/8
2022-08-26T11:03:07.6180898Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6181219Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6181630Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6182075Z Aug 26 11:03:07  but was: ""/sessions/:session_handle""
2022-08-26T11:03:07.6182536Z Aug 26 11:03:07 		method: 
2022-08-26T11:03:07.6182860Z Aug 26 11:03:07 expected: ""PUT""
2022-08-26T11:03:07.6183174Z Aug 26 11:03:07  but was: ""GET""
2022-08-26T11:03:07.6184091Z Aug 26 11:03:07 		path-parameters: Existing Path parameter operation_handle was removed.
2022-08-26T11:03:07.6184595Z Aug 26 11:03:07 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:07.6185022Z Aug 26 11:03:07 Expecting actual not to be null
2022-08-26T11:03:07.6186196Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/sessions/:session_handle/heartbeat"",""method"":""POST"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""any""}}.
2022-08-26T11:03:07.6187190Z Aug 26 11:03:07 	Compatibility grade: 4/8
2022-08-26T11:03:07.6187563Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6187891Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6188286Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6188773Z Aug 26 11:03:07  but was: ""/sessions/:session_handle/heartbeat""
2022-08-26T11:03:07.6189152Z Aug 26 11:03:07 		method: 
2022-08-26T11:03:07.6189459Z Aug 26 11:03:07 expected: ""PUT""
2022-08-26T11:03:07.6189791Z Aug 26 11:03:07  but was: ""POST""
2022-08-26T11:03:07.6190437Z Aug 26 11:03:07 		path-parameters: Existing Path parameter operation_handle was removed.
2022-08-26T11:03:07.6191111Z Aug 26 11:03:07 		response: [Type of field was changed from 'object' to 'any'.] 
2022-08-26T11:03:07.6191536Z Aug 26 11:03:07 expected: ""object""
2022-08-26T11:03:07.6191867Z Aug 26 11:03:07  but was: ""any""
2022-08-26T11:03:07.6193723Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/sessions/:session_handle/operations/:operation_handle/result/:token"",""method"":""GET"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""},{""key"":""operation_handle""},{""key"":""token""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:statement:FetchResultsResponseBody"",""properties"":{""results"":{""type"":""any""},""resultType"":{""type"":""string""},""nextResultUri"":{""type"":""string""}}}}.
2022-08-26T11:03:07.6195169Z Aug 26 11:03:07 	Compatibility grade: 4/8
2022-08-26T11:03:07.6195538Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6195877Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6196268Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6196954Z Aug 26 11:03:07  but was: ""/sessions/:session_handle/operations/:operation_handle/result/:token""
2022-08-26T11:03:07.6197421Z Aug 26 11:03:07 		method: 
2022-08-26T11:03:07.6197730Z Aug 26 11:03:07 expected: ""PUT""
2022-08-26T11:03:07.6198114Z Aug 26 11:03:07  but was: ""GET""
2022-08-26T11:03:07.6198975Z Aug 26 11:03:07 		path-parameters: New path parameter token was added.
2022-08-26T11:03:07.6199588Z Aug 26 11:03:07 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:07.6200250Z Aug 26 11:03:07 Expecting actual not to be null
2022-08-26T11:03:07.6202961Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/sessions/:session_handle/statements"",""method"":""POST"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:statement:ExecuteStatementRequestBody"",""properties"":{""statement"":{""type"":""string""},""executionTimeout"":{""type"":""integer""},""executionConfig"":{""type"":""object"",""additionalProperties"":{""type"":""string""}}}},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:statement:ExecuteStatementResponseBody"",""properties"":{""operationHandle"":{""type"":""string""}}}}.
2022-08-26T11:03:07.6205404Z Aug 26 11:03:07 	Compatibility grade: 4/8
2022-08-26T11:03:07.6206095Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6206523Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6207210Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6207873Z Aug 26 11:03:07  but was: ""/sessions/:session_handle/statements""
2022-08-26T11:03:07.6208387Z Aug 26 11:03:07 		method: 
2022-08-26T11:03:07.6208943Z Aug 26 11:03:07 expected: ""PUT""
2022-08-26T11:03:07.6209508Z Aug 26 11:03:07  but was: ""POST""
2022-08-26T11:03:07.6210608Z Aug 26 11:03:07 		path-parameters: Existing Path parameter operation_handle was removed.
2022-08-26T11:03:07.6211479Z Aug 26 11:03:07 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:07.6212064Z Aug 26 11:03:07 Expecting actual not to be null
2022-08-26T11:03:07.6212552Z Aug 26 11:03:07 	at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:39)
2022-08-26T11:03:07.6213409Z Aug 26 11:03:07 	at org.junit.jupiter.api.Assertions.fail(Assertions.java:134)
2022-08-26T11:03:07.6214712Z Aug 26 11:03:07 	at org.apache.flink.runtime.rest.compatibility.RestAPIStabilityTestUtils.fail(RestAPIStabilityTestUtils.java:209)
2022-08-26T11:03:07.6216196Z Aug 26 11:03:07 	at org.apache.flink.runtime.rest.compatibility.RestAPIStabilityTestUtils.assertCompatible(RestAPIStabilityTestUtils.java:138)
2022-08-26T11:03:07.6217787Z Aug 26 11:03:07 	at org.apache.flink.runtime.rest.compatibility.RestAPIStabilityTestUtils.testStability(RestAPIStabilityTestUtils.java:78)
2022-08-26T11:03:07.6218756Z Aug 26 11:03:07 	at org.apache.flink.table.gateway.rest.compatibility.SqlGatewayRestAPIStabilityTest.testSqlGatewayRestAPIStability(SqlGatewayRestAPIStabilityTest.java:57)
2022-08-26T11:03:07.6219518Z Aug 26 11:03:07 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-26T11:03:07.6220092Z Aug 26 11:03:07 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-26T11:03:07.6220833Z Aug 26 11:03:07 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-26T11:03:07.6221472Z Aug 26 11:03:07 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-26T11:03:07.6222067Z Aug 26 11:03:07 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-08-26T11:03:07.6222767Z Aug 26 11:03:07 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-08-26T11:03:07.6223567Z Aug 26 11:03:07 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-08-26T11:03:07.6224456Z Aug 26 11:03:07 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-08-26T11:03:07.6225187Z Aug 26 11:03:07 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-08-26T11:03:07.6225958Z Aug 26 11:03:07 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)
2022-08-26T11:03:07.6226971Z Aug 26 11:03:07 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-08-26T11:03:07.6227801Z Aug 26 11:03:07 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-08-26T11:03:07.6228811Z Aug 26 11:03:07 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-08-26T11:03:07.6229837Z Aug 26 11:03:07 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-08-26T11:03:07.6230701Z Aug 26 11:03:07 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-08-26T11:03:07.6231478Z Aug 26 11:03:07 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-08-26T11:03:07.6232220Z Aug 26 11:03:07 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-08-26T11:03:07.6233022Z Aug 26 11:03:07 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-08-26T11:03:07.6233781Z Aug 26 11:03:07 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-08-26T11:03:07.6234656Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-26T11:03:07.6235446Z Aug 26 11:03:07 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-08-26T11:03:07.6236487Z Aug 26 11:03:07 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-08-26T11:03:07.6237597Z Aug 26 11:03:07 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-08-26T11:03:07.6238794Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-08-26T11:03:07.6239601Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-26T11:03:07.6240373Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-26T11:03:07.6241101Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-26T11:03:07.6241881Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-26T11:03:07.6242923Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-26T11:03:07.6243996Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-26T11:03:07.6244758Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-26T11:03:07.6245627Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-26T11:03:07.6246673Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.submit(ForkJoinPoolHierarchicalTestExecutorService.java:118)
2022-08-26T11:03:07.6248243Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
2022-08-26T11:03:07.6249734Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:204)
2022-08-26T11:03:07.6251055Z Aug 26 11:03:07 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)
2022-08-26T11:03:07.6251888Z Aug 26 11:03:07 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)
2022-08-26T11:03:07.6252628Z Aug 26 11:03:07 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-08-26T11:03:07.6253463Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-26T11:03:07.6254491Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
2022-08-26T11:03:07.6255342Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-26T11:03:07.6256361Z Aug 26 11:03:07 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-08-26T11:03:07.6257198Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-26T11:03:07.6257831Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-26T11:03:07.6258620Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-26T11:03:07.6259230Z Aug 26 11:03:07 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-08-26T11:03:07.6259857Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-26T11:03:07.6260794Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
2022-08-26T11:03:07.6261877Z Aug 26 11:03:07 	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
2022-08-26T11:03:07.6262842Z Aug 26 11:03:07 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-08-26T11:03:07.6264074Z Aug 26 11:03:07 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-08-26T11:03:07.6265240Z Aug 26 11:03:07 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-08-26T11:03:07.6266437Z Aug 26 11:03:07 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-08-26T11:03:07.6267746Z Aug 26 11:03:07 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-08-26T11:03:07.6268842Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-08-26T11:03:07.6270170Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2022-08-26T11:03:07.6271320Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-26T11:03:07.6272436Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-26T11:03:07.6273549Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-26T11:03:07.6274812Z Aug 26 11:03:07 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2022-08-26T11:03:07.6275964Z Aug 26 11:03:07 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-08-26T11:03:07.6277234Z Aug 26 11:03:07 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-08-26T11:03:07.6278413Z Aug 26 11:03:07 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-08-26T11:03:07.6279630Z Aug 26 11:03:07 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-08-26T11:03:07.6280773Z Aug 26 11:03:07 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-08-26T11:03:07.6281903Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-08-26T11:03:07.6283011Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2022-08-26T11:03:07.6284251Z Aug 26 11:03:07 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2022-08-26T11:03:07.6285420Z Aug 26 11:03:07 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-08-26T11:03:07.6286356Z Aug 26 11:03:07 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-08-26T11:03:07.6287606Z Aug 26 11:03:07 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-08-26T11:03:07.6288756Z Aug 26 11:03:07 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-08-26T11:03:07.6289721Z Aug 26 11:03:07 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-08-26T11:03:07.6290598Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-08-26T11:03:07.6291318Z Aug 26 11:03:07 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)
2022-08-26T11:03:07.6292107Z Aug 26 11:03:07 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)
2022-08-26T11:03:07.6293092Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-08-26T11:03:07.6293968Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-26T11:03:07.6294757Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-26T11:03:07.6295473Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-26T11:03:07.6296174Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-26T11:03:07.6297316Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-26T11:03:07.6298067Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-26T11:03:07.6298789Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-26T11:03:07.6299667Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-26T11:03:07.6300711Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-08-26T11:03:07.6301615Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-08-26T11:03:07.6302559Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-26T11:03:07.6303356Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-26T11:03:07.6304135Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-26T11:03:07.6304850Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-26T11:03:07.6305612Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-26T11:03:07.6306371Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-26T11:03:07.6307249Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-26T11:03:07.6308365Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-26T11:03:07.6309412Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-08-26T11:03:07.6310319Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-08-26T11:03:07.6311225Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-26T11:03:07.6311985Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-26T11:03:07.6312884Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-26T11:03:07.6314150Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-26T11:03:07.6314950Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-26T11:03:07.6315874Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-26T11:03:07.6316609Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-26T11:03:07.6317690Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-26T11:03:07.6318978Z Aug 26 11:03:07 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-08-26T11:03:07.6319845Z Aug 26 11:03:07 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-26T11:03:07.6320479Z Aug 26 11:03:07 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-26T11:03:07.6321103Z Aug 26 11:03:07 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-26T11:03:07.6321734Z Aug 26 11:03:07 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-08-26T11:03:07.6322201Z Aug 26 11:03:07 
2022-08-26T11:03:07.6498722Z Aug 26 11:03:07 [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.022 s - in org.apache.flink.table.gateway.rest.SessionCaseITTest
2022-08-26T11:03:08.4973396Z Aug 26 11:03:08 [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.396 s - in org.apache.flink.table.gateway.service.session.SessionManagerTest
2022-08-26T11:03:08.8545807Z Aug 26 11:03:08 [INFO] 
2022-08-26T11:03:08.8546577Z Aug 26 11:03:08 [INFO] Results:
2022-08-26T11:03:08.8547189Z Aug 26 11:03:08 [INFO] 
2022-08-26T11:03:08.8547603Z Aug 26 11:03:08 [ERROR] Failures: 
2022-08-26T11:03:08.8550509Z Aug 26 11:03:08 [ERROR]   SqlGatewayRestAPIStabilityTest.testSqlGatewayRestAPIStability:57 No compatible call could be found for {""url"":""/sessions/:session_handle/:operation_handle/cancel"",""method"":""PUT"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""},{""key"":""operation_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody"",""properties"":{""status"":{""type"":""string""}}}}.
2022-08-26T11:03:08.8554432Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/sessions/:session_handle/operations/:operation_handle/cancel"",""method"":""PUT"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""},{""key"":""operation_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody"",""properties"":{""status"":{""type"":""string""}}}}.
2022-08-26T11:03:08.8555944Z Aug 26 11:03:08 	Compatibility grade: 7/8
2022-08-26T11:03:08.8556404Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8556878Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8557341Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8558016Z Aug 26 11:03:08  but was: ""/sessions/:session_handle/operations/:operation_handle/cancel""
2022-08-26T11:03:08.8560084Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/sessions/:session_handle/operations/:operation_handle/close"",""method"":""DELETE"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""},{""key"":""operation_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody"",""properties"":{""status"":{""type"":""string""}}}}.
2022-08-26T11:03:08.8561585Z Aug 26 11:03:08 	Compatibility grade: 6/8
2022-08-26T11:03:08.8561952Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8562272Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8562682Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8563323Z Aug 26 11:03:08  but was: ""/sessions/:session_handle/operations/:operation_handle/close""
2022-08-26T11:03:08.8563736Z Aug 26 11:03:08 		method: 
2022-08-26T11:03:08.8564157Z Aug 26 11:03:08 expected: ""PUT""
2022-08-26T11:03:08.8564499Z Aug 26 11:03:08  but was: ""DELETE""
2022-08-26T11:03:08.8566446Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/sessions/:session_handle/operations/:operation_handle/status"",""method"":""GET"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""},{""key"":""operation_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody"",""properties"":{""status"":{""type"":""string""}}}}.
2022-08-26T11:03:08.8569300Z Aug 26 11:03:08 	Compatibility grade: 6/8
2022-08-26T11:03:08.8569937Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8570496Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8571215Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8572246Z Aug 26 11:03:08  but was: ""/sessions/:session_handle/operations/:operation_handle/status""
2022-08-26T11:03:08.8572825Z Aug 26 11:03:08 		method: 
2022-08-26T11:03:08.8573133Z Aug 26 11:03:08 expected: ""PUT""
2022-08-26T11:03:08.8573460Z Aug 26 11:03:08  but was: ""GET""
2022-08-26T11:03:08.8575227Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/sessions/:session_handle"",""method"":""DELETE"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:CloseSessionResponseBody"",""properties"":{""status"":{""type"":""string""}}}}.
2022-08-26T11:03:08.8576635Z Aug 26 11:03:08 	Compatibility grade: 5/8
2022-08-26T11:03:08.8577360Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8578025Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8578743Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8579537Z Aug 26 11:03:08  but was: ""/sessions/:session_handle""
2022-08-26T11:03:08.8579915Z Aug 26 11:03:08 		method: 
2022-08-26T11:03:08.8580234Z Aug 26 11:03:08 expected: ""PUT""
2022-08-26T11:03:08.8580549Z Aug 26 11:03:08  but was: ""DELETE""
2022-08-26T11:03:08.8581230Z Aug 26 11:03:08 		path-parameters: Existing Path parameter operation_handle was removed.
2022-08-26T11:03:08.8582810Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/api_versions"",""method"":""GET"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:util:GetApiVersionResponseBody"",""properties"":{""versions"":{""type"":""array"",""items"":{""type"":""string""}}}}}.
2022-08-26T11:03:08.8583981Z Aug 26 11:03:08 	Compatibility grade: 4/8
2022-08-26T11:03:08.8584332Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8584663Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8585070Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8585736Z Aug 26 11:03:08  but was: ""/api_versions""
2022-08-26T11:03:08.8586081Z Aug 26 11:03:08 		method: 
2022-08-26T11:03:08.8586407Z Aug 26 11:03:08 expected: ""PUT""
2022-08-26T11:03:08.8586722Z Aug 26 11:03:08  but was: ""GET""
2022-08-26T11:03:08.8587606Z Aug 26 11:03:08 		path-parameters: Existing Path parameter session_handle was removed.
2022-08-26T11:03:08.8588091Z Aug 26 11:03:08 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:08.8588516Z Aug 26 11:03:08 Expecting actual not to be null
2022-08-26T11:03:08.8589990Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/info"",""method"":""GET"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:util:GetInfoResponseBody"",""properties"":{""productName"":{""type"":""string""},""version"":{""type"":""string""}}}}.
2022-08-26T11:03:08.8591280Z Aug 26 11:03:08 	Compatibility grade: 4/8
2022-08-26T11:03:08.8591646Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8591959Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8592369Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8592795Z Aug 26 11:03:08  but was: ""/info""
2022-08-26T11:03:08.8593101Z Aug 26 11:03:08 		method: 
2022-08-26T11:03:08.8593427Z Aug 26 11:03:08 expected: ""PUT""
2022-08-26T11:03:08.8593756Z Aug 26 11:03:08  but was: ""GET""
2022-08-26T11:03:08.8594450Z Aug 26 11:03:08 		path-parameters: Existing Path parameter session_handle was removed.
2022-08-26T11:03:08.8594951Z Aug 26 11:03:08 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:08.8595371Z Aug 26 11:03:08 Expecting actual not to be null
2022-08-26T11:03:08.8619910Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/sessions"",""method"":""POST"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:OpenSessionRequestBody"",""properties"":{""sessionName"":{""type"":""string""},""properties"":{""type"":""object"",""additionalProperties"":{""type"":""string""}}}},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:OpenSessionResponseBody"",""properties"":{""sessionHandle"":{""type"":""string""}}}}.
2022-08-26T11:03:08.8622586Z Aug 26 11:03:08 	Compatibility grade: 4/8
2022-08-26T11:03:08.8623158Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8623683Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8624400Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8625066Z Aug 26 11:03:08  but was: ""/sessions""
2022-08-26T11:03:08.8625574Z Aug 26 11:03:08 		method: 
2022-08-26T11:03:08.8626077Z Aug 26 11:03:08 expected: ""PUT""
2022-08-26T11:03:08.8626554Z Aug 26 11:03:08  but was: ""POST""
2022-08-26T11:03:08.8627935Z Aug 26 11:03:08 		path-parameters: Existing Path parameter session_handle was removed.
2022-08-26T11:03:08.8628783Z Aug 26 11:03:08 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:08.8629445Z Aug 26 11:03:08 Expecting actual not to be null
2022-08-26T11:03:08.8632160Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/sessions/:session_handle"",""method"":""GET"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:GetSessionConfigResponseBody"",""properties"":{""properties"":{""type"":""object"",""additionalProperties"":{""type"":""string""}}}}}.
2022-08-26T11:03:08.8634154Z Aug 26 11:03:08 	Compatibility grade: 4/8
2022-08-26T11:03:08.8634728Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8635242Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8635900Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8637125Z Aug 26 11:03:08  but was: ""/sessions/:session_handle""
2022-08-26T11:03:08.8637679Z Aug 26 11:03:08 		method: 
2022-08-26T11:03:08.8638215Z Aug 26 11:03:08 expected: ""PUT""
2022-08-26T11:03:08.8638757Z Aug 26 11:03:08  but was: ""GET""
2022-08-26T11:03:08.8639897Z Aug 26 11:03:08 		path-parameters: Existing Path parameter operation_handle was removed.
2022-08-26T11:03:08.8640744Z Aug 26 11:03:08 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:08.8641438Z Aug 26 11:03:08 Expecting actual not to be null
2022-08-26T11:03:08.8643308Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/sessions/:session_handle/heartbeat"",""method"":""POST"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""any""}}.
2022-08-26T11:03:08.8645016Z Aug 26 11:03:08 	Compatibility grade: 4/8
2022-08-26T11:03:08.8645627Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8646163Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8647000Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8647784Z Aug 26 11:03:08  but was: ""/sessions/:session_handle/heartbeat""
2022-08-26T11:03:08.8648331Z Aug 26 11:03:08 		method: 
2022-08-26T11:03:08.8648821Z Aug 26 11:03:08 expected: ""PUT""
2022-08-26T11:03:08.8649321Z Aug 26 11:03:08  but was: ""POST""
2022-08-26T11:03:08.8650391Z Aug 26 11:03:08 		path-parameters: Existing Path parameter operation_handle was removed.
2022-08-26T11:03:08.8651470Z Aug 26 11:03:08 		response: [Type of field was changed from 'object' to 'any'.] 
2022-08-26T11:03:08.8652215Z Aug 26 11:03:08 expected: ""object""
2022-08-26T11:03:08.8652757Z Aug 26 11:03:08  but was: ""any""
2022-08-26T11:03:08.8655846Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/sessions/:session_handle/operations/:operation_handle/result/:token"",""method"":""GET"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""},{""key"":""operation_handle""},{""key"":""token""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:statement:FetchResultsResponseBody"",""properties"":{""results"":{""type"":""any""},""resultType"":{""type"":""string""},""nextResultUri"":{""type"":""string""}}}}.
2022-08-26T11:03:08.8658253Z Aug 26 11:03:08 	Compatibility grade: 4/8
2022-08-26T11:03:08.8658849Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8659376Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8660027Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8660918Z Aug 26 11:03:08  but was: ""/sessions/:session_handle/operations/:operation_handle/result/:token""
2022-08-26T11:03:08.8661621Z Aug 26 11:03:08 		method: 
2022-08-26T11:03:08.8662145Z Aug 26 11:03:08 expected: ""PUT""
2022-08-26T11:03:08.8662684Z Aug 26 11:03:08  but was: ""GET""
2022-08-26T11:03:08.8663723Z Aug 26 11:03:08 		path-parameters: New path parameter token was added.
2022-08-26T11:03:08.8664619Z Aug 26 11:03:08 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:08.8665305Z Aug 26 11:03:08 Expecting actual not to be null
2022-08-26T11:03:08.8669057Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/sessions/:session_handle/statements"",""method"":""POST"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:statement:ExecuteStatementRequestBody"",""properties"":{""statement"":{""type"":""string""},""executionTimeout"":{""type"":""integer""},""executionConfig"":{""type"":""object"",""additionalProperties"":{""type"":""string""}}}},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:statement:ExecuteStatementResponseBody"",""properties"":{""operationHandle"":{""type"":""string""}}}}.
2022-08-26T11:03:08.8671949Z Aug 26 11:03:08 	Compatibility grade: 4/8
2022-08-26T11:03:08.8672450Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8672936Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8673517Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8674326Z Aug 26 11:03:08  but was: ""/sessions/:session_handle/statements""
2022-08-26T11:03:08.8674933Z Aug 26 11:03:08 		method: 
2022-08-26T11:03:08.8675409Z Aug 26 11:03:08 expected: ""PUT""
2022-08-26T11:03:08.8675870Z Aug 26 11:03:08  but was: ""POST""
2022-08-26T11:03:08.8676947Z Aug 26 11:03:08 		path-parameters: Existing Path parameter operation_handle was removed.
2022-08-26T11:03:08.8677838Z Aug 26 11:03:08 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:08.8678425Z Aug 26 11:03:08 Expecting actual not to be null
2022-08-26T11:03:08.8678939Z Aug 26 11:03:08 [INFO] 
2022-08-26T11:03:08.8679504Z Aug 26 11:03:08 [ERROR] Tests run: 32, Failures: 1, Errors: 0, Skipped: 0
{code}",,aitozi,fsk119,hxb,jark,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 29 06:10:48 UTC 2022,,,,,,,,,,"0|z183ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/22 11:46;jark;cc [~fsk119];;;","27/Aug/22 08:30;wangyang0918;Another instance, https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40413&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","28/Aug/22 00:54;jark;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40428&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","29/Aug/22 02:31;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40398&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","29/Aug/22 02:37;fsk119;Sorry for everyone. It seems I merge a commit with probelms. The root cause is we updte the rest api but not updating the api document. The test here compares the difference between the actual api and the api doc and find they are different. So I update the api document in the PR.;;;","29/Aug/22 06:10;fsk119;Merged into master: e246a05cd898e16758a52335250571e165a5148a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unexpected join hint propagation into view,FLINK-29120,13478786,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xuyangzhong,lincoln.86xy,lincoln.86xy,26/Aug/22 09:48,10/Sep/22 02:47,13/Jul/23 08:13,10/Sep/22 02:47,1.16.0,,,,,,,1.16.0,,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"As expected, Join Hint should only affects the current query block, and does not affect the Join strategy in subquery and view.

But current implementation behaviors inconsistently:

use source tables of flink-tpch-test, the following join hint takes effect unexpectedly
{code:java}
Flink SQL> create temporary view v1 as SELECT
>    p_name,
>    p_mfgr,
>    p_brand,
>    p_type,
>    s_name,
>    s_address
>  FROM
>    part,
>    supplier
>  WHERE p_partkey = s_suppkey;
[INFO] Execute statement succeed.

 


Flink SQL> explain SELECT /*+ SHUFFLE_MERGE(part)  */ * from v1;
== Abstract Syntax Tree ==
LogicalProject(p_name=[$1], p_mfgr=[$2], p_brand=[$3], p_type=[$4], s_name=[$10], s_address=[$11])
+- LogicalFilter(condition=[=($0, $9)])
   +- LogicalJoin(condition=[true], joinType=[inner], joinHints=[[[SHUFFLE_MERGE inheritPath:[0, 0] options:[part]]]])
      :- LogicalTableScan(table=[[default_catalog, default_database, part]])
      +- LogicalTableScan(table=[[default_catalog, default_database, supplier]])

== Optimized Physical Plan ==
Calc(select=[p_name, p_mfgr, p_brand, p_type, s_name, s_address])
+- SortMergeJoin(joinType=[InnerJoin], where=[=(p_partkey, s_suppkey)], select=[p_partkey, p_name, p_mfgr, p_brand, p_type, s_suppkey, s_name, s_address])
   :- Exchange(distribution=[hash[p_partkey]])
   :  +- TableSourceScan(table=[[default_catalog, default_database, part, project=[p_partkey, p_name, p_mfgr, p_brand, p_type], metadata=[]]], fields=[p_partkey, p_name, p_mfgr, p_brand, p_type])
   +- Exchange(distribution=[hash[s_suppkey]])
      +- TableSourceScan(table=[[default_catalog, default_database, supplier, project=[s_suppkey, s_name, s_address], metadata=[]]], fields=[s_suppkey, s_name, s_address])

== Optimized Execution Plan ==
Calc(select=[p_name, p_mfgr, p_brand, p_type, s_name, s_address])
+- SortMergeJoin(joinType=[InnerJoin], where=[(p_partkey = s_suppkey)], select=[p_partkey, p_name, p_mfgr, p_brand, p_type, s_suppkey, s_name, s_address])
   :- Exchange(distribution=[hash[p_partkey]])
   :  +- TableSourceScan(table=[[default_catalog, default_database, part, project=[p_partkey, p_name, p_mfgr, p_brand, p_type], metadata=[]]], fields=[p_partkey, p_name, p_mfgr, p_brand, p_type])
   +- Exchange(distribution=[hash[s_suppkey]])
      +- TableSourceScan(table=[[default_catalog, default_database, supplier, project=[s_suppkey, s_name, s_address], metadata=[]]], fields=[s_suppkey, s_name, s_address])

{code}
 

without hint

{code}

Flink SQL> explain SELECT * from v1;
== Abstract Syntax Tree ==
LogicalProject(p_name=[$1], p_mfgr=[$2], p_brand=[$3], p_type=[$4], s_name=[$10], s_address=[$11])
+- LogicalFilter(condition=[=($0, $9)])
   +- LogicalJoin(condition=[true], joinType=[inner])
      :- LogicalTableScan(table=[[default_catalog, default_database, part]])
      +- LogicalTableScan(table=[[default_catalog, default_database, supplier]])

== Optimized Physical Plan ==
Calc(select=[p_name, p_mfgr, p_brand, p_type, s_name, s_address])
+- HashJoin(joinType=[InnerJoin], where=[=(p_partkey, s_suppkey)], select=[p_partkey, p_name, p_mfgr, p_brand, p_type, s_suppkey, s_name, s_address], isBroadcast=[true], build=[right])
   :- TableSourceScan(table=[[default_catalog, default_database, part, project=[p_partkey, p_name, p_mfgr, p_brand, p_type], metadata=[]]], fields=[p_partkey, p_name, p_mfgr, p_brand, p_type])
   +- Exchange(distribution=[broadcast])
      +- TableSourceScan(table=[[default_catalog, default_database, supplier, project=[s_suppkey, s_name, s_address], metadata=[]]], fields=[s_suppkey, s_name, s_address])

== Optimized Execution Plan ==
Calc(select=[p_name, p_mfgr, p_brand, p_type, s_name, s_address])
+- MultipleInput(readOrder=[1,0], members=[\nHashJoin(joinType=[InnerJoin], where=[(p_partkey = s_suppkey)], select=[p_partkey, p_name, p_mfgr, p_brand, p_type, s_suppkey, s_name, s_address], isBroadcast=[true], build=[right])\n:- [#1] TableSourceScan(table=[[default_catalog, default_database, part, project=[p_partkey, p_name, p_mfgr, p_brand, p_type], metadata=[]]], fields=[p_partkey, p_name, p_mfgr, p_brand, p_type])\n+- [#2] Exchange(distribution=[broadcast])\n])
   :- TableSourceScan(table=[[default_catalog, default_database, part, project=[p_partkey, p_name, p_mfgr, p_brand, p_type], metadata=[]]], fields=[p_partkey, p_name, p_mfgr, p_brand, p_type])
   +- Exchange(distribution=[broadcast])
      +- TableSourceScan(table=[[default_catalog, default_database, supplier, project=[s_suppkey, s_name, s_address], metadata=[]]], fields=[s_suppkey, s_name, s_address])

{code}

 

 ",,godfreyhe,lincoln.86xy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28969,FLINK-29221,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Sep 10 02:47:42 UTC 2022,,,,,,,,,,"0|z183cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Sep/22 02:47;godfreyhe;Fixed in master: 6722c89d0df35643dde38c1b8f096aa785579884
in 1.16: 357221b9b3543447b2b439a413639c0ed201ab35;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove default_catalog in the HiveServer2 Endpoint,FLINK-29118,13478783,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,fsk119,fsk119,26/Aug/22 09:38,13/Sep/22 03:07,13/Jul/23 08:13,08/Sep/22 06:29,1.16.0,,,,,,,1.16.0,,,,,Connectors / Hive,Table SQL / Gateway,,,,,,0,pull-request-available,,,,"Hive only has one Catalog. We don't require the default_catalog. Hive JDBC Driver also doesn't support multiple catalogs.

 

 

!image-2022-08-26-17-40-49-989.png!",,fsk119,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28954,,,,,,,,,,,,,,,,,,,FLINK-29274,,,,,,"26/Aug/22 09:40;fsk119;image-2022-08-26-17-40-49-989.png;https://issues.apache.org/jira/secure/attachment/13048632/image-2022-08-26-17-40-49-989.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 08 06:29:56 UTC 2022,,,,,,,,,,"0|z183c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/22 06:29;fsk119;Merged into master: 833e7ffbb5f075d2014dfaec547b6987d59bc89f

Merged into release-1.16: c8e75d201df6225e21facb7433667ac186e07f00;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Join hint with invalid table name mixed with valid names will not raise an error,FLINK-29113,13478757,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuyangzhong,lincoln.86xy,lincoln.86xy,26/Aug/22 07:27,14/Sep/22 02:20,13/Jul/23 08:13,14/Sep/22 02:20,1.16.0,,,,,,,1.16.0,1.17.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,," 

add a  BROADCAST hint to tpch q2 with a non exists table 'supp' works fine, while a single invalid table name 'supp' will raise an error

 

```sql

explain  SELECT /*+ BROADCAST(partsupp,supp) */ 

s_acctbal,
  s_name,
  n_name,
  p_partkey,
  p_mfgr,
  s_address,
  s_phone,
  s_comment
FROM
  part,
  supplier,
  partsupp,
  nation,
  region
WHERE
  p_partkey = ps_partkey
  AND s_suppkey = ps_suppkey
  AND p_size = 15
  AND p_type LIKE '%BRASS'
  AND s_nationkey = n_nationkey
  AND n_regionkey = r_regionkey
  AND r_name = 'EUROPE'
  AND ps_supplycost = (
    SELECT min(ps_supplycost)
    FROM
      partsupp, supplier,
      nation, region
    WHERE
      p_partkey = ps_partkey
      AND s_suppkey = ps_suppkey
      AND s_nationkey = n_nationkey
      AND n_regionkey = r_regionkey
      AND r_name = 'EUROPE'
  )
ORDER BY
  s_acctbal DESC,
  n_name,
  s_name,
  p_partkey
LIMIT 100

```

!image-2022-08-26-15-26-33-305.png!",,godfreyhe,lincoln.86xy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28969,,,,,,"26/Aug/22 07:26;lincoln.86xy;image-2022-08-26-15-26-33-305.png;https://issues.apache.org/jira/secure/attachment/13048619/image-2022-08-26-15-26-33-305.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 14 02:20:14 UTC 2022,,,,,,,,,,"0|z1836g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/22 02:20;godfreyhe;Fixed in master: d518086f475ec92a18592ec3c423bf6398e776cf
in 1.16.0: 0e02c082037979766b61da90e37f5fc555d71770;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoint path conflict with stateless upgrade mode,FLINK-29109,13478724,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,thw,thw,thw,26/Aug/22 00:59,04/Dec/22 20:49,13/Jul/23 08:13,04/Dec/22 20:49,kubernetes-operator-1.1.0,,,,,,,kubernetes-operator-1.2.0,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"A stateful job with stateless upgrade mode (yes, there are such use cases) fails with checkpoint path conflict due to constant jobId and FLINK-19358 (applies to Flink < 1.16x). Since with stateless upgrade mode the checkpoint id resets on restart the job is going to write to previously used locations and fail. The workaround is to rotate the jobId on every redeploy when the upgrade mode is stateless. While this can be worked around externally it is best done in the operator itself because reconciliation resolves when a restart is actually required while rotating jobId externally may trigger unnecessary restarts.",,gyfora,mason6345,rmetzger,thw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Dec 03 15:19:26 UTC 2022,,,,,,,,,,"0|z182z4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 15:18;gyfora;merged to main 6f0914bbd296c9daf2664afe0a77d1df4f2e157e;;;","23/Nov/22 08:11;gyfora;[~thw] , looking at this again, it seems that we might need this logic for all Flink versions (not just before 1.16) in 1.16 you get a generated jobId based on the clusterid but in our case thats also fixed, so it will lead to the same issues.

What do you think?;;;","03/Dec/22 15:19;thw;[~gyfora] thanks for catching this. Because the jobId assigned by Flink is deterministic (HighAvailabilityOptions.HA_CLUSTER_ID), we will also need to apply the random jobId for stateless upgrade mode for Flink version >= 1.16 to avoid the checkpoint path collisions. 

https://github.com/apache/flink/blob/e70fe68dea764606180ca3728184c00fc63ea0ff/flink-clients/src/main/java/org/apache/flink/client/deployment/application/ApplicationDispatcherBootstrap.java#L227;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KubernetesStateHandleStoreTest.testAddAndLockShouldNotThrowAlreadyExistExceptionWithSameContents failed with AssertionFailedError,FLINK-29105,13478588,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wangyang0918,hxb,hxb,25/Aug/22 07:38,27/Aug/22 10:20,13/Jul/23 08:13,27/Aug/22 10:20,1.16.0,,,,,,,1.16.0,,,,,Deployment / Kubernetes,,,,,,,0,pull-request-available,test-stability,,,"{code:java}
2022-08-25T04:19:22.1429302Z Aug 25 04:19:22 [ERROR] Tests run: 25, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.827 s <<< FAILURE! - in org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStoreTest
2022-08-25T04:19:22.1447098Z Aug 25 04:19:22 [ERROR] org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStoreTest.testAddAndLockShouldNotThrowAlreadyExistExceptionWithSameContents  Time elapsed: 0.031 s  <<< FAILURE!
2022-08-25T04:19:22.1447862Z Aug 25 04:19:22 org.opentest4j.AssertionFailedError: 
2022-08-25T04:19:22.1448236Z Aug 25 04:19:22 
2022-08-25T04:19:22.1448561Z Aug 25 04:19:22 expected: 2
2022-08-25T04:19:22.1448893Z Aug 25 04:19:22  but was: 0
2022-08-25T04:19:22.1449330Z Aug 25 04:19:22 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2022-08-25T04:19:22.1450330Z Aug 25 04:19:22 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2022-08-25T04:19:22.1451114Z Aug 25 04:19:22 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2022-08-25T04:19:22.1452006Z Aug 25 04:19:22 	at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStoreTest.retryWithFirstFailedK8sOperation(KubernetesStateHandleStoreTest.java:1218)
2022-08-25T04:19:22.1452956Z Aug 25 04:19:22 	at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStoreTest.access$600(KubernetesStateHandleStoreTest.java:59)
2022-08-25T04:19:22.1453863Z Aug 25 04:19:22 	at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStoreTest$5.lambda$null$0(KubernetesStateHandleStoreTest.java:245)
2022-08-25T04:19:22.1454744Z Aug 25 04:19:22 	at org.apache.flink.kubernetes.kubeclient.TestingFlinkKubeClient.checkAndUpdateConfigMap(TestingFlinkKubeClient.java:182)
2022-08-25T04:19:22.1455619Z Aug 25 04:19:22 	at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStore.updateConfigMap(KubernetesStateHandleStore.java:634)
2022-08-25T04:19:22.1456553Z Aug 25 04:19:22 	at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStore.addAndLock(KubernetesStateHandleStore.java:219)
2022-08-25T04:19:22.1457435Z Aug 25 04:19:22 	at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStoreTest$5.lambda$new$1(KubernetesStateHandleStoreTest.java:258)
2022-08-25T04:19:22.1458482Z Aug 25 04:19:22 	at org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityTestBase$Context.runTest(KubernetesHighAvailabilityTestBase.java:107)
2022-08-25T04:19:22.1459383Z Aug 25 04:19:22 	at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStoreTest$5.<init>(KubernetesStateHandleStoreTest.java:237)
2022-08-25T04:19:22.1460391Z Aug 25 04:19:22 	at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStoreTest.testAddAndLockShouldNotThrowAlreadyExistExceptionWithSameContents(KubernetesStateHandleStoreTest.java:235)
2022-08-25T04:19:22.1461357Z Aug 25 04:19:22 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-25T04:19:22.1461965Z Aug 25 04:19:22 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-25T04:19:22.1462653Z Aug 25 04:19:22 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-25T04:19:22.1463280Z Aug 25 04:19:22 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-25T04:19:22.1463903Z Aug 25 04:19:22 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-08-25T04:19:22.1464604Z Aug 25 04:19:22 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-08-25T04:19:22.1465397Z Aug 25 04:19:22 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-08-25T04:19:22.1466211Z Aug 25 04:19:22 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-08-25T04:19:22.1466943Z Aug 25 04:19:22 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-08-25T04:19:22.1467701Z Aug 25 04:19:22 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2022-08-25T04:19:22.1468531Z Aug 25 04:19:22 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-08-25T04:19:22.1469350Z Aug 25 04:19:22 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-08-25T04:19:22.1470180Z Aug 25 04:19:22 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-08-25T04:19:22.1471027Z Aug 25 04:19:22 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-08-25T04:19:22.1471919Z Aug 25 04:19:22 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-08-25T04:19:22.1472727Z Aug 25 04:19:22 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-08-25T04:19:22.1473455Z Aug 25 04:19:22 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-08-25T04:19:22.1474161Z Aug 25 04:19:22 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-08-25T04:19:22.1474946Z Aug 25 04:19:22 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-08-25T04:19:22.1475761Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-25T04:19:22.1476561Z Aug 25 04:19:22 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-08-25T04:19:22.1477360Z Aug 25 04:19:22 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-08-25T04:19:22.1478133Z Aug 25 04:19:22 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-08-25T04:19:22.1478911Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-08-25T04:19:22.1479701Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-25T04:19:22.1480486Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-25T04:19:22.1481214Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-25T04:19:22.1481944Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-25T04:19:22.1482797Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-25T04:19:22.1483563Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-25T04:19:22.1484287Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-25T04:19:22.1485176Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-25T04:19:22.1486258Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-08-25T04:19:22.1487314Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-08-25T04:19:22.1488228Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-08-25T04:19:22.1489017Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-25T04:19:22.1489801Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-25T04:19:22.1490525Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-25T04:19:22.1491248Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-25T04:19:22.1492090Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-25T04:19:22.1492846Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-25T04:19:22.1493581Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-25T04:19:22.1494474Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-25T04:19:22.1495547Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-08-25T04:19:22.1496450Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-08-25T04:19:22.1497242Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-25T04:19:22.1498115Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-25T04:19:22.1498842Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-25T04:19:22.1499553Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-25T04:19:22.1500336Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-25T04:19:22.1501101Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-25T04:19:22.1501835Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-25T04:19:22.1502727Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-25T04:19:22.1503654Z Aug 25 04:19:22 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-08-25T04:19:22.1504262Z Aug 25 04:19:22 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-25T04:19:22.1504872Z Aug 25 04:19:22 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-25T04:19:22.1505502Z Aug 25 04:19:22 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-25T04:19:22.1506135Z Aug 25 04:19:22 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-08-25T04:19:22.1506610Z Aug 25 04:19:22  {code}
https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/40355/logs/956",,hxb,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28265,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 27 10:20:43 UTC 2022,,,,,,,,,,"0|z18254:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/22 07:51;wangyang0918;I will have a look.;;;","26/Aug/22 06:13;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40394&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199;;;","27/Aug/22 10:20;wangyang0918;Fixed via:

master(1.16): 7b394a3ddd57e1bf88426fad92090f93fcdf6e4d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogLocalRecoveryITCase.testRestartTM failed with AssertionFailedError,FLINK-29102,13478532,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Yanfei Lei,hxb,hxb,25/Aug/22 03:13,14/Sep/22 07:48,13/Jul/23 08:13,14/Sep/22 07:48,1.16.0,,,,,,,1.16.0,,,,,Runtime / State Backends,,,,,,,0,pull-request-available,test-stability,,,"{code:java}
2022-08-25T02:10:19.1407131Z Aug 25 02:10:19 [ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 20.126 s <<< FAILURE! - in org.apache.flink.test.checkpointing.ChangelogLocalRecoveryITCase
2022-08-25T02:10:19.1410088Z Aug 25 02:10:19 [ERROR] ChangelogLocalRecoveryITCase.testRestartTM  Time elapsed: 12.501 s  <<< FAILURE!
2022-08-25T02:10:19.1411734Z Aug 25 02:10:19 org.opentest4j.AssertionFailedError: Graph is in globally terminal state (FAILED)
2022-08-25T02:10:19.1413028Z Aug 25 02:10:19 	at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:43)
2022-08-25T02:10:19.1414753Z Aug 25 02:10:19 	at org.junit.jupiter.api.Assertions.fail(Assertions.java:146)
2022-08-25T02:10:19.1416077Z Aug 25 02:10:19 	at org.apache.flink.runtime.testutils.CommonTestUtils.lambda$waitForAllTaskRunning$3(CommonTestUtils.java:213)
2022-08-25T02:10:19.1417807Z Aug 25 02:10:19 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:150)
2022-08-25T02:10:19.1419455Z Aug 25 02:10:19 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:144)
2022-08-25T02:10:19.1421184Z Aug 25 02:10:19 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForAllTaskRunning(CommonTestUtils.java:208)
2022-08-25T02:10:19.1422893Z Aug 25 02:10:19 	at org.apache.flink.test.checkpointing.ChangelogLocalRecoveryITCase.testRestartTM(ChangelogLocalRecoveryITCase.java:149)
2022-08-25T02:10:19.1424381Z Aug 25 02:10:19 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-25T02:10:19.1425730Z Aug 25 02:10:19 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-25T02:10:19.1427428Z Aug 25 02:10:19 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-25T02:10:19.1428842Z Aug 25 02:10:19 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-25T02:10:19.1430173Z Aug 25 02:10:19 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-08-25T02:10:19.1431683Z Aug 25 02:10:19 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-08-25T02:10:19.1433150Z Aug 25 02:10:19 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-08-25T02:10:19.1434760Z Aug 25 02:10:19 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-08-25T02:10:19.1436296Z Aug 25 02:10:19 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-08-25T02:10:19.1437704Z Aug 25 02:10:19 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-08-25T02:10:19.1439268Z Aug 25 02:10:19 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-08-25T02:10:19.1441412Z Aug 25 02:10:19 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-08-25T02:10:19.1442792Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-25T02:10:19.1444190Z Aug 25 02:10:19 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-08-25T02:10:19.1445593Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-08-25T02:10:19.1446946Z Aug 25 02:10:19 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-08-25T02:10:19.1448568Z Aug 25 02:10:19 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-08-25T02:10:19.1449909Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-08-25T02:10:19.1451376Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-08-25T02:10:19.1452655Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-08-25T02:10:19.1454003Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-08-25T02:10:19.1455239Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-08-25T02:10:19.1456511Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-08-25T02:10:19.1458149Z Aug 25 02:10:19 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-08-25T02:10:19.1460189Z Aug 25 02:10:19 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-08-25T02:10:19.1461574Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-08-25T02:10:19.1462877Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-08-25T02:10:19.1464471Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-08-25T02:10:19.1465869Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-08-25T02:10:19.1467177Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-08-25T02:10:19.1468801Z Aug 25 02:10:19 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-08-25T02:10:19.1470145Z Aug 25 02:10:19 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-08-25T02:10:19.1471536Z Aug 25 02:10:19 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-08-25T02:10:19.1472951Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-25T02:10:19.1474218Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-08-25T02:10:19.1475390Z Aug 25 02:10:19 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-08-25T02:10:19.1476566Z Aug 25 02:10:19 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-08-25T02:10:19.1478077Z Aug 25 02:10:19 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-08-25T02:10:19.1480014Z Aug 25 02:10:19 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-08-25T02:10:19.1481463Z Aug 25 02:10:19 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-08-25T02:10:19.1482776Z Aug 25 02:10:19 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-08-25T02:10:19.1486583Z Aug 25 02:10:19 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-08-25T02:10:19.1488264Z Aug 25 02:10:19 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-08-25T02:10:19.1489877Z Aug 25 02:10:19 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-08-25T02:10:19.1491691Z Aug 25 02:10:19 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-08-25T02:10:19.1492986Z Aug 25 02:10:19 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-08-25T02:10:19.1494236Z Aug 25 02:10:19 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-08-25T02:10:19.1495567Z Aug 25 02:10:19 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-08-25T02:10:19.1496961Z Aug 25 02:10:19 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-08-25T02:10:19.1498390Z Aug 25 02:10:19 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-08-25T02:10:19.1499796Z Aug 25 02:10:19 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-08-25T02:10:19.1501267Z Aug 25 02:10:19 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-08-25T02:10:19.1502560Z Aug 25 02:10:19 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-08-25T02:10:19.1503777Z Aug 25 02:10:19 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-08-25T02:10:19.1504924Z Aug 25 02:10:19 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-08-25T02:10:19.1506127Z Aug 25 02:10:19 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-08-25T02:10:19.1507702Z Aug 25 02:10:19 Caused by: org.apache.flink.runtime.JobException: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=10)
2022-08-25T02:10:19.1509606Z Aug 25 02:10:19 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
2022-08-25T02:10:19.1511280Z Aug 25 02:10:19 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
2022-08-25T02:10:19.1512791Z Aug 25 02:10:19 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256)
2022-08-25T02:10:19.1514116Z Aug 25 02:10:19 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247)
2022-08-25T02:10:19.1515412Z Aug 25 02:10:19 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
2022-08-25T02:10:19.1516738Z Aug 25 02:10:19 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:738)
2022-08-25T02:10:19.1518192Z Aug 25 02:10:19 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:715)
2022-08-25T02:10:19.1519748Z Aug 25 02:10:19 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51)
2022-08-25T02:10:19.1521544Z Aug 25 02:10:19 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1619)
2022-08-25T02:10:19.1522964Z Aug 25 02:10:19 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1137)
2022-08-25T02:10:19.1524151Z Aug 25 02:10:19 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1077)
2022-08-25T02:10:19.1525332Z Aug 25 02:10:19 	at org.apache.flink.runtime.executiongraph.Execution.fail(Execution.java:778)
2022-08-25T02:10:19.1526624Z Aug 25 02:10:19 	at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.signalPayloadRelease(SingleLogicalSlot.java:195)
2022-08-25T02:10:19.1528105Z Aug 25 02:10:19 	at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.release(SingleLogicalSlot.java:182)
2022-08-25T02:10:19.1529562Z Aug 25 02:10:19 	at org.apache.flink.runtime.scheduler.SharedSlot.lambda$release$4(SharedSlot.java:270)
2022-08-25T02:10:19.1530816Z Aug 25 02:10:19 	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670)
2022-08-25T02:10:19.1531900Z Aug 25 02:10:19 	at java.util.concurrent.CompletableFuture.uniAcceptStage(CompletableFuture.java:683)
2022-08-25T02:10:19.1532921Z Aug 25 02:10:19 	at java.util.concurrent.CompletableFuture.thenAccept(CompletableFuture.java:2010)
2022-08-25T02:10:19.1533628Z Aug 25 02:10:19 	at org.apache.flink.runtime.scheduler.SharedSlot.release(SharedSlot.java:270)
2022-08-25T02:10:19.1534360Z Aug 25 02:10:19 	at org.apache.flink.runtime.jobmaster.slotpool.AllocatedSlot.releasePayload(AllocatedSlot.java:152)
2022-08-25T02:10:19.1535210Z Aug 25 02:10:19 	at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.releasePayload(DefaultDeclarativeSlotPool.java:483)
2022-08-25T02:10:19.1536322Z Aug 25 02:10:19 	at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.freeAndReleaseSlots(DefaultDeclarativeSlotPool.java:475)
2022-08-25T02:10:19.1537411Z Aug 25 02:10:19 	at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.releaseSlots(DefaultDeclarativeSlotPool.java:446)
2022-08-25T02:10:19.1538422Z Aug 25 02:10:19 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolService.internalReleaseTaskManager(DeclarativeSlotPoolService.java:275)
2022-08-25T02:10:19.1539375Z Aug 25 02:10:19 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolService.releaseTaskManager(DeclarativeSlotPoolService.java:231)
2022-08-25T02:10:19.1540193Z Aug 25 02:10:19 	at org.apache.flink.runtime.jobmaster.JobMaster.disconnectTaskManager(JobMaster.java:533)
2022-08-25T02:10:19.1540913Z Aug 25 02:10:19 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-25T02:10:19.1541778Z Aug 25 02:10:19 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-25T02:10:19.1542700Z Aug 25 02:10:19 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-25T02:10:19.1543354Z Aug 25 02:10:19 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-25T02:10:19.1544039Z Aug 25 02:10:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
2022-08-25T02:10:19.1544853Z Aug 25 02:10:19 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-08-25T02:10:19.1545661Z Aug 25 02:10:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
2022-08-25T02:10:19.1546413Z Aug 25 02:10:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
2022-08-25T02:10:19.1547179Z Aug 25 02:10:19 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2022-08-25T02:10:19.1548110Z Aug 25 02:10:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2022-08-25T02:10:19.1548869Z Aug 25 02:10:19 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-08-25T02:10:19.1549682Z Aug 25 02:10:19 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-08-25T02:10:19.1550579Z Aug 25 02:10:19 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-08-25T02:10:19.1551529Z Aug 25 02:10:19 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-08-25T02:10:19.1552170Z Aug 25 02:10:19 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-08-25T02:10:19.1552821Z Aug 25 02:10:19 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-08-25T02:10:19.1553467Z Aug 25 02:10:19 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-25T02:10:19.1554125Z Aug 25 02:10:19 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-25T02:10:19.1554732Z Aug 25 02:10:19 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-08-25T02:10:19.1555398Z Aug 25 02:10:19 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-08-25T02:10:19.1555993Z Aug 25 02:10:19 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-08-25T02:10:19.1556612Z Aug 25 02:10:19 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-08-25T02:10:19.1557203Z Aug 25 02:10:19 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-08-25T02:10:19.1557942Z Aug 25 02:10:19 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-08-25T02:10:19.1558512Z Aug 25 02:10:19 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-08-25T02:10:19.1559032Z Aug 25 02:10:19 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-08-25T02:10:19.1559620Z Aug 25 02:10:19 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-25T02:10:19.1560282Z Aug 25 02:10:19 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-25T02:10:19.1561023Z Aug 25 02:10:19 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-25T02:10:19.1561690Z Aug 25 02:10:19 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-08-25T02:10:19.1562440Z Aug 25 02:10:19 Caused by: org.apache.flink.util.FlinkExpectedException: org.apache.flink.util.FlinkExpectedException: The TaskExecutor is shutting down.
2022-08-25T02:10:19.1563197Z Aug 25 02:10:19 	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:456)
2022-08-25T02:10:19.1563908Z Aug 25 02:10:19 	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:238)
2022-08-25T02:10:19.1564671Z Aug 25 02:10:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.lambda$terminate$0(AkkaRpcActor.java:578)
2022-08-25T02:10:19.1565497Z Aug 25 02:10:19 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-08-25T02:10:19.1566406Z Aug 25 02:10:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:577)
2022-08-25T02:10:19.1567177Z Aug 25 02:10:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:196)
2022-08-25T02:10:19.1568114Z Aug 25 02:10:19 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-08-25T02:10:19.1568721Z Aug 25 02:10:19 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-08-25T02:10:19.1569337Z Aug 25 02:10:19 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-08-25T02:10:19.1569944Z Aug 25 02:10:19 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-08-25T02:10:19.1570568Z Aug 25 02:10:19 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-08-25T02:10:19.1571302Z Aug 25 02:10:19 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-08-25T02:10:19.1571808Z Aug 25 02:10:19 	... 13 more {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40355&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9",,hxb,roman,Yanfei Lei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29147,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 14 07:48:50 UTC 2022,,,,,,,,,,"0|z181so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/22 03:13;hxb;[~Yanfei Lei] Could you help take a look? Thx.;;;","25/Aug/22 03:17;Yanfei Lei;[~hxb]  Thanks for reporting, I will fix it later.;;;","13/Sep/22 03:29;hxb;Hi [~Yanfei Lei] any updates on this issue?;;;","14/Sep/22 07:48;hxb;Merged into master via 545ce28da098eaa7be2b0f1549336db5d939d023

Merged into release-1.16 via 8fb13a803904485e0d44ffee8e1510b14543129f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PipelinedRegionSchedulingStrategy benchmark shows performance degradation,FLINK-29101,13478530,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,25/Aug/22 03:00,14/Sep/22 14:57,13/Jul/23 08:13,14/Sep/22 14:57,1.16.0,,,,,,,1.16.0,,,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"Throw TPC-DS and flink-benchmark testing, we found that PipelinedRegionSchedulingStrategy has performance degradation. By investigation, I can confirm that this was introduced by FLINK-28799 which introduce HYBRID type edge support for scheduling strategy.

The key to the problem is for blocking ALL_TO_ALL type edges should only enter the scheduling method when the last execution becomes finished, but the current implementation ignores this fact, resulting in the complexity of O(n ^ 2) in this case.",,freeke,Weijie Guo,xtsong,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 14 14:57:36 UTC 2022,,,,,,,,,,"0|z181s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/22 14:57;xtsong;- master (1.17): c0165a8a7e3ccc6e82df7a30c67497a10e281153
- release-1.16: eefeb6a539186e2bfe49f72d346f00474229547c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deployment with last-state upgrade mode stuck after initial error,FLINK-29100,13478515,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,thw,thw,thw,25/Aug/22 01:48,31/Aug/22 17:06,13/Jul/23 08:13,31/Aug/22 17:06,kubernetes-operator-1.1.0,,,,,,,kubernetes-operator-1.2.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"A deployment with last_state upgrade mode that never succeeds will be stuck in deploying state because no HA data exists. This can be reproduced by creating a deployment with invalid image or exception in entry point. Update to the CR that corrects the issue won't be reconciled due to ""o.a.f.k.o.r.d.ApplicationReconciler [INFO ] [default.basic-checkpoint-ha-example] Job is not running yet and HA metadata is not available, waiting for upgradeable state"". This forces manual intervention to delete the CR.

Instead,  operator should check if this is the initial deployment and if so skip the HA metadata check.",,thw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29159,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-25 01:48:41.0,,,,,,,,,,"0|z181ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock for Single Subtask in Kinesis Consumer,FLINK-29099,13478475,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sethsaperstein,sethsaperstein,sethsaperstein,24/Aug/22 19:05,28/Nov/22 23:37,13/Jul/23 08:13,28/Nov/22 23:36,1.10.3,1.11.6,1.12.7,1.13.6,1.14.5,1.15.3,1.9.3,1.17.0,,,,,Connectors / Kinesis,,,,,,,0,connector,consumer,kinesis,pull-request-available,"Deadlock is reached as the result of:
 * max lookahead reached for local watermark
 * idle state for subtask

The lookahead prevents the RecordEmitter from emitting a new record. The idle state prevents the global watermark from being updated.

To exit this deadlock state, we need to complete the [TODO here|https://github.com/apache/flink/blob/221d70d9930f72147422ea24b399f006ebbfb8d7/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/KinesisDataFetcher.java#L1268] which updates the global watermark while the subtask is marked idle, which will then allow us to emit a record again as the lookahead is no longer reached.

 

*Context:*

We reached this scenario at Lyft as a result of prolonged CPU throttling on all FlinkKinesisConsumer threads for multiple minutes.

Walking through the series of events for a single subtask:
 * prolonged CPU throttling occurs and no logs are seen from any FlinkKinesisConsumer thread for up to 15 minutes
 * after CPU throttling the subtask is marked idle
 * the subtask has reached the lookahead for its local watermark relative to the global watermark
 * WatermarkSyncCallback indicates the subtask as idle and does not update the global watermark
 * emitQueue fills to max
 * RecordEmitter cannot emit records due to the max lookahead
 * Deadlock on subtask

At this point, we had not realized what had happened and processing of all other shards/subtasks had continued for multiple days. When we finally restarted the application, we saw the following behavior:
 * global watermark recalculated after all subtasks consumed data based on the last kinesis record sequence number
 * global watermark moved back in time multiple days, to when the subtask was first marked idle
 * the single subtask processed data while all others remained idle due to the lookahead

This would have continued until the subtask had caught up to the others and thus the global watermark is within reach of the lookahead for other subtasks.

 

*Repro:*

Too difficult to repro the exact scenario.",,jeffreytolar,premsantosh,sethsaperstein,thw,,,,,,,,,,,,,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,java,Mon Nov 28 23:37:01 UTC 2022,,,,,,,,,,"0|z181g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 20:55;premsantosh;cc [~thomasWeise];;;","28/Nov/22 23:37;thw;[~sethsaperstein] thanks for the thorough investigation and fix!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StoreWriteOperator#prepareCommit should let logSinkFunction flush first before fetching offset,FLINK-29098,13478424,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,24/Aug/22 13:58,24/Sep/22 02:38,13/Jul/23 08:13,29/Aug/22 05:21,table-store-0.3.0,,,,,,,table-store-0.2.1,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,The cause for unstable CompositePkAndMultiPartitionedTableITCase#testEnableLogAndStreamingReadWriteMultiPartitionedRecordsWithMultiPk was found that the StoreWriteOperator#prepareCommit may get an empty log offset due to some condition where KafkaProducer#flush are not called.,,lzljs3620320,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 29 05:21:55 UTC 2022,,,,,,,,,,"0|z1814o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/22 05:21;lzljs3620320;master: 50b1b2951fcdfbfc139e21950086dddb3e01c690
release-0.2: db55ca641654a75e69f56ae3c34ccc08ec90f205;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"json_value when the path has blank, the result is not right",FLINK-29096,13478401,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,24/Aug/22 12:33,26/Aug/22 07:35,13/Jul/23 08:13,26/Aug/22 07:35,1.16.0,,,,,,,1.16.0,,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"!https://aone.alipay.com/v2/api/workitem/adapter/file/url?fileIdentifier=workitem%2Falipay%2Fdefault%2F1661334308971image.png!

 

 

!image-2022-08-24-20-33-59-052.png!

 

!https://aone.alipay.com/v2/api/workitem/adapter/file/url?fileIdentifier=workitem%2Falipay%2Fdefault%2F1661334330457image.png!

 

 

 ",,jackylau,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/22 12:34;jackylau;image-2022-08-24-20-33-59-052.png;https://issues.apache.org/jira/secure/attachment/13048513/image-2022-08-24-20-33-59-052.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 26 07:35:48 UTC 2022,,,,,,,,,,"0|z180zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Aug/22 12:35;jackylau;the sql escape is ' , so i think we should add test for it. ;;;","26/Aug/22 07:35;jark;Fixed in master: 60e594414a7a0ee0930b63d8e859e8d6000b1f76 to 3149c621671605ccd676cbbe393074ae25715773;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LookupJoinITCase failed with InternalCompilerException,FLINK-29093,13478353,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,smiralex,hxb,hxb,24/Aug/22 07:43,29/Sep/22 03:39,13/Jul/23 08:13,23/Sep/22 14:32,1.16.0,,,,,,,1.16.0,,,,,Table SQL / Runtime,,,,,,,0,pull-request-available,test-stability,,,"{code:java}
2022-08-24T03:45:02.5915521Z Aug 24 03:45:02 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-08-24T03:45:02.5916823Z Aug 24 03:45:02 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-08-24T03:45:02.5919320Z Aug 24 03:45:02 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
2022-08-24T03:45:02.5920833Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-08-24T03:45:02.5922361Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-08-24T03:45:02.5923733Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-08-24T03:45:02.5924922Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-08-24T03:45:02.5926191Z Aug 24 03:45:02 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:268)
2022-08-24T03:45:02.5927677Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-08-24T03:45:02.5929091Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-08-24T03:45:02.5930430Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-08-24T03:45:02.5931966Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-08-24T03:45:02.5933293Z Aug 24 03:45:02 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1277)
2022-08-24T03:45:02.5934708Z Aug 24 03:45:02 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-08-24T03:45:02.5936228Z Aug 24 03:45:02 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-08-24T03:45:02.5937998Z Aug 24 03:45:02 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-08-24T03:45:02.5939627Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-08-24T03:45:02.5941051Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-08-24T03:45:02.5942650Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-08-24T03:45:02.5944203Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-08-24T03:45:02.5945740Z Aug 24 03:45:02 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-08-24T03:45:02.5947020Z Aug 24 03:45:02 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-08-24T03:45:02.5948130Z Aug 24 03:45:02 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-08-24T03:45:02.5949255Z Aug 24 03:45:02 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-08-24T03:45:02.5950405Z Aug 24 03:45:02 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-08-24T03:45:02.5951638Z Aug 24 03:45:02 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-08-24T03:45:02.5953564Z Aug 24 03:45:02 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-08-24T03:45:02.5955214Z Aug 24 03:45:02 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-08-24T03:45:02.5956587Z Aug 24 03:45:02 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-08-24T03:45:02.5958037Z Aug 24 03:45:02 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-08-24T03:45:02.5959448Z Aug 24 03:45:02 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-08-24T03:45:02.5960679Z Aug 24 03:45:02 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-08-24T03:45:02.5962183Z Aug 24 03:45:02 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-08-24T03:45:02.5963694Z Aug 24 03:45:02 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-08-24T03:45:02.5965024Z Aug 24 03:45:02 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-08-24T03:45:02.5966227Z Aug 24 03:45:02 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-08-24T03:45:02.5967444Z Aug 24 03:45:02 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-08-24T03:45:02.5968642Z Aug 24 03:45:02 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-08-24T03:45:02.5969913Z Aug 24 03:45:02 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-08-24T03:45:02.5971340Z Aug 24 03:45:02 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-08-24T03:45:02.5972824Z Aug 24 03:45:02 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-08-24T03:45:02.5974100Z Aug 24 03:45:02 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-08-24T03:45:02.5975389Z Aug 24 03:45:02 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-08-24T03:45:02.5976670Z Aug 24 03:45:02 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-08-24T03:45:02.5978063Z Aug 24 03:45:02 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-08-24T03:45:02.5979479Z Aug 24 03:45:02 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-24T03:45:02.5980728Z Aug 24 03:45:02 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-24T03:45:02.5982143Z Aug 24 03:45:02 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-24T03:45:02.5983410Z Aug 24 03:45:02 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-08-24T03:45:02.5984613Z Aug 24 03:45:02 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-08-24T03:45:02.5986005Z Aug 24 03:45:02 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
2022-08-24T03:45:02.5988034Z Aug 24 03:45:02 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
2022-08-24T03:45:02.5989611Z Aug 24 03:45:02 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256)
2022-08-24T03:45:02.5990833Z Aug 24 03:45:02 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247)
2022-08-24T03:45:02.5992309Z Aug 24 03:45:02 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
2022-08-24T03:45:02.5993616Z Aug 24 03:45:02 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:738)
2022-08-24T03:45:02.5994883Z Aug 24 03:45:02 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:715)
2022-08-24T03:45:02.5996320Z Aug 24 03:45:02 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2022-08-24T03:45:02.5997572Z Aug 24 03:45:02 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:477)
2022-08-24T03:45:02.5998837Z Aug 24 03:45:02 	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
2022-08-24T03:45:02.6000095Z Aug 24 03:45:02 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-24T03:45:02.6001334Z Aug 24 03:45:02 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-24T03:45:02.6002879Z Aug 24 03:45:02 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
2022-08-24T03:45:02.6004484Z Aug 24 03:45:02 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-08-24T03:45:02.6006045Z Aug 24 03:45:02 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
2022-08-24T03:45:02.6007510Z Aug 24 03:45:02 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
2022-08-24T03:45:02.6009009Z Aug 24 03:45:02 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2022-08-24T03:45:02.6010458Z Aug 24 03:45:02 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2022-08-24T03:45:02.6011933Z Aug 24 03:45:02 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-08-24T03:45:02.6013136Z Aug 24 03:45:02 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-08-24T03:45:02.6014334Z Aug 24 03:45:02 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-08-24T03:45:02.6015516Z Aug 24 03:45:02 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-08-24T03:45:02.6016746Z Aug 24 03:45:02 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-08-24T03:45:02.6017988Z Aug 24 03:45:02 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-08-24T03:45:02.6019207Z Aug 24 03:45:02 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-24T03:45:02.6020453Z Aug 24 03:45:02 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-24T03:45:02.6021664Z Aug 24 03:45:02 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-08-24T03:45:02.6022813Z Aug 24 03:45:02 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-08-24T03:45:02.6023947Z Aug 24 03:45:02 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-08-24T03:45:02.6025133Z Aug 24 03:45:02 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-08-24T03:45:02.6026232Z Aug 24 03:45:02 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-08-24T03:45:02.6027190Z Aug 24 03:45:02 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-08-24T03:45:02.6028082Z Aug 24 03:45:02 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-08-24T03:45:02.6029101Z Aug 24 03:45:02 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-08-24T03:45:02.6030155Z Aug 24 03:45:02 	... 4 more
2022-08-24T03:45:02.6032269Z Aug 24 03:45:02 Caused by: java.lang.RuntimeException: java.util.concurrent.CompletionException: java.lang.RuntimeException: Failed to reload lookup 'FULL' cache.
2022-08-24T03:45:02.6033990Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.LookupFullCache.getIfPresent(LookupFullCache.java:74)
2022-08-24T03:45:02.6035675Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.functions.table.lookup.CachingLookupFunction.lookup(CachingLookupFunction.java:122)
2022-08-24T03:45:02.6037117Z Aug 24 03:45:02 	at org.apache.flink.table.functions.LookupFunction.eval(LookupFunction.java:52)
2022-08-24T03:45:02.6038277Z Aug 24 03:45:02 	at LookupFunction$109548.flatMap(Unknown Source)
2022-08-24T03:45:02.6039528Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.doFetch(LookupJoinRunner.java:92)
2022-08-24T03:45:02.6041280Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:79)
2022-08-24T03:45:02.6043077Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:34)
2022-08-24T03:45:02.6044507Z Aug 24 03:45:02 	at org.apache.flink.streaming.api.operators.ProcessOperator.processElement(ProcessOperator.java:66)
2022-08-24T03:45:02.6045903Z Aug 24 03:45:02 	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:99)
2022-08-24T03:45:02.6047336Z Aug 24 03:45:02 	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:80)
2022-08-24T03:45:02.6048783Z Aug 24 03:45:02 	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
2022-08-24T03:45:02.6050162Z Aug 24 03:45:02 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
2022-08-24T03:45:02.6051690Z Aug 24 03:45:02 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
2022-08-24T03:45:02.6053386Z Aug 24 03:45:02 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
2022-08-24T03:45:02.6055157Z Aug 24 03:45:02 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
2022-08-24T03:45:02.6056876Z Aug 24 03:45:02 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
2022-08-24T03:45:02.6058505Z Aug 24 03:45:02 	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.run(FromElementsFunction.java:231)
2022-08-24T03:45:02.6059974Z Aug 24 03:45:02 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
2022-08-24T03:45:02.6061347Z Aug 24 03:45:02 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
2022-08-24T03:45:02.6063003Z Aug 24 03:45:02 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
2022-08-24T03:45:02.6065141Z Aug 24 03:45:02 Caused by: java.util.concurrent.CompletionException: java.lang.RuntimeException: Failed to reload lookup 'FULL' cache.
2022-08-24T03:45:02.6066557Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
2022-08-24T03:45:02.6067945Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
2022-08-24T03:45:02.6069310Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1643)
2022-08-24T03:45:02.6070702Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1632)
2022-08-24T03:45:02.6072177Z Aug 24 03:45:02 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-24T03:45:02.6073457Z Aug 24 03:45:02 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-24T03:45:02.6074907Z Aug 24 03:45:02 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-24T03:45:02.6076156Z Aug 24 03:45:02 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-08-24T03:45:02.6077752Z Aug 24 03:45:02 Caused by: java.lang.RuntimeException: Failed to reload lookup 'FULL' cache.
2022-08-24T03:45:02.6079119Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.CacheLoader.run(CacheLoader.java:105)
2022-08-24T03:45:02.6080604Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
2022-08-24T03:45:02.6081691Z Aug 24 03:45:02 	... 5 more
2022-08-24T03:45:02.6083708Z Aug 24 03:45:02 Caused by: java.lang.RuntimeException: Failed to load data into the lookup 'FULL' cache from InputSplit org.apache.flink.table.runtime.functions.table.fullcache.FullCacheTestInputFormat$QueueInputSplit@67854815
2022-08-24T03:45:02.6085871Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputSplitCacheLoadTask.run(InputSplitCacheLoadTask.java:94)
2022-08-24T03:45:02.6087785Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.reloadCache(InputFormatCacheLoader.java:102)
2022-08-24T03:45:02.6089522Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.CacheLoader.run(CacheLoader.java:90)
2022-08-24T03:45:02.6090630Z Aug 24 03:45:02 	... 6 more
2022-08-24T03:45:02.6092180Z Aug 24 03:45:02 Caused by: java.lang.RuntimeException: Could not instantiate generated class 'KeyProjection$109542'
2022-08-24T03:45:02.6093614Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:74)
2022-08-24T03:45:02.6095161Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.keyselector.GenericRowDataKeySelector.getKey(GenericRowDataKeySelector.java:52)
2022-08-24T03:45:02.6096823Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.keyselector.GenericRowDataKeySelector.getKey(GenericRowDataKeySelector.java:29)
2022-08-24T03:45:02.6098583Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputSplitCacheLoadTask.run(InputSplitCacheLoadTask.java:79)
2022-08-24T03:45:02.6099844Z Aug 24 03:45:02 	... 8 more
2022-08-24T03:45:02.6101114Z Aug 24 03:45:02 Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
2022-08-24T03:45:02.6102880Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:94)
2022-08-24T03:45:02.6104217Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:101)
2022-08-24T03:45:02.6105525Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:68)
2022-08-24T03:45:02.6106608Z Aug 24 03:45:02 	... 11 more
2022-08-24T03:45:02.6108117Z Aug 24 03:45:02 Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
2022-08-24T03:45:02.6110087Z Aug 24 03:45:02 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)
2022-08-24T03:45:02.6111720Z Aug 24 03:45:02 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
2022-08-24T03:45:02.6113370Z Aug 24 03:45:02 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
2022-08-24T03:45:02.6114888Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92)
2022-08-24T03:45:02.6115931Z Aug 24 03:45:02 	... 13 more
2022-08-24T03:45:02.6117177Z Aug 24 03:45:02 Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
2022-08-24T03:45:02.6118673Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:107)
2022-08-24T03:45:02.6120123Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92)
2022-08-24T03:45:02.6121778Z Aug 24 03:45:02 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
2022-08-24T03:45:02.6123576Z Aug 24 03:45:02 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
2022-08-24T03:45:02.6125251Z Aug 24 03:45:02 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
2022-08-24T03:45:02.6126991Z Aug 24 03:45:02 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
2022-08-24T03:45:02.6128591Z Aug 24 03:45:02 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
2022-08-24T03:45:02.6130111Z Aug 24 03:45:02 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
2022-08-24T03:45:02.6131763Z Aug 24 03:45:02 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
2022-08-24T03:45:02.6133369Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92)
2022-08-24T03:45:02.6134777Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:101)
2022-08-24T03:45:02.6136236Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:68)
2022-08-24T03:45:02.6137497Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.keyselector.GenericRowDataKeySelector.getKey(GenericRowDataKeySelector.java:52)
2022-08-24T03:45:02.6138796Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.keyselector.GenericRowDataKeySelector.getKey(GenericRowDataKeySelector.java:29)
2022-08-24T03:45:02.6140234Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputSplitCacheLoadTask.run(InputSplitCacheLoadTask.java:79)
2022-08-24T03:45:02.6141457Z Aug 24 03:45:02 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2022-08-24T03:45:02.6146427Z Aug 24 03:45:02 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-08-24T03:45:02.6147685Z Aug 24 03:45:02 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-08-24T03:45:02.6149049Z Aug 24 03:45:02 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-08-24T03:45:02.6150192Z Aug 24 03:45:02 	at java.lang.Thread.run(Thread.java:748)
2022-08-24T03:45:02.6153609Z Aug 24 03:45:02 Caused by: org.codehaus.janino.InternalCompilerException: Compiling ""KeyProjection$109542"": Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2022-08-24T03:45:02.6156044Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382)
2022-08-24T03:45:02.6157265Z Aug 24 03:45:02 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
2022-08-24T03:45:02.6158569Z Aug 24 03:45:02 	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
2022-08-24T03:45:02.6159872Z Aug 24 03:45:02 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
2022-08-24T03:45:02.6161097Z Aug 24 03:45:02 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
2022-08-24T03:45:02.6162659Z Aug 24 03:45:02 	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
2022-08-24T03:45:02.6163829Z Aug 24 03:45:02 	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
2022-08-24T03:45:02.6165126Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104)
2022-08-24T03:45:02.6166156Z Aug 24 03:45:02 	... 19 more
2022-08-24T03:45:02.6168806Z Aug 24 03:45:02 Caused by: java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2022-08-24T03:45:02.6171268Z Aug 24 03:45:02 	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:184)
2022-08-24T03:45:02.6173345Z Aug 24 03:45:02 	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:192)
2022-08-24T03:45:02.6174695Z Aug 24 03:45:02 	at java.lang.Class.forName0(Native Method)
2022-08-24T03:45:02.6175663Z Aug 24 03:45:02 	at java.lang.Class.forName(Class.java:348)
2022-08-24T03:45:02.6176871Z Aug 24 03:45:02 	at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:89)
2022-08-24T03:45:02.6178209Z Aug 24 03:45:02 	at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:312)
2022-08-24T03:45:02.6179482Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:8556)
2022-08-24T03:45:02.6180766Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6601)
2022-08-24T03:45:02.6182178Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573)
2022-08-24T03:45:02.6183437Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215)
2022-08-24T03:45:02.6184758Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481)
2022-08-24T03:45:02.6186120Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476)
2022-08-24T03:45:02.6187399Z Aug 24 03:45:02 	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928)
2022-08-24T03:45:02.6188610Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476)
2022-08-24T03:45:02.6189872Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469)
2022-08-24T03:45:02.6191091Z Aug 24 03:45:02 	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927)
2022-08-24T03:45:02.6192407Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
2022-08-24T03:45:02.6193644Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.access$1300(UnitCompiler.java:215)
2022-08-24T03:45:02.6194938Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler$34.getInterfaces2(UnitCompiler.java:10002)
2022-08-24T03:45:02.6196169Z Aug 24 03:45:02 	at org.codehaus.janino.IClass.getInterfaces(IClass.java:497)
2022-08-24T03:45:02.6197312Z Aug 24 03:45:02 	at org.codehaus.janino.IClass.getIMethods(IClass.java:263)
2022-08-24T03:45:02.6198461Z Aug 24 03:45:02 	at org.codehaus.janino.IClass.getIMethods(IClass.java:237)
2022-08-24T03:45:02.6199646Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:492)
2022-08-24T03:45:02.6200857Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
2022-08-24T03:45:02.6202211Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
2022-08-24T03:45:02.6203556Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
2022-08-24T03:45:02.6205035Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
2022-08-24T03:45:02.6206461Z Aug 24 03:45:02 	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
2022-08-24T03:45:02.6207873Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
2022-08-24T03:45:02.6209088Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
2022-08-24T03:45:02.6210038Z Aug 24 03:45:02 	... 26 more {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40327&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21331",,hxb,jark,renqs,smiralex,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29463,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 23 14:32:12 UTC 2022,,,,,,,,,,"0|z180ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/22 03:03;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40564&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","01/Sep/22 03:03;hxb;[~renqs] Any progress on this issue？;;;","06/Sep/22 09:04;smiralex;Hi [~hxb]. I made a PR that should fix this issue. I think it's connected either with concurrent creating of Project instances from generated code (maybe #newInstance method is not thread-safe) or with race condition during closing of LookupFullCache (currently #close method doesn't wait until the end of active reload, if it's happening). Both of these problems are fixed in the MR.;;;","07/Sep/22 01:52;hxb;Thanks [~smiralex] for the fix.;;;","19/Sep/22 14:02;jark;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41125&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","21/Sep/22 09:40;jark;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41202&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","23/Sep/22 14:32;renqs;Fixed on master: 340b100f2de5e0d90ba475aa8a00e359a61442ce 

release-1.16: c10a727990668b1a0d706f16e4f5220780422ed9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HadoopPathBasedPartFileWriterTest.testWriteFile failed with AssertionError,FLINK-29092,13478347,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kurt.ding,hxb,hxb,24/Aug/22 07:24,29/Nov/22 12:15,13/Jul/23 08:13,29/Nov/22 12:15,1.15.2,1.16.0,1.17.0,,,,,1.15.4,1.16.1,1.17.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,pull-request-available,test-stability,,,"{code:java}
2022-08-24T02:00:01.1670618Z Aug 24 02:00:01 [ERROR] org.apache.flink.formats.hadoop.bulk.HadoopPathBasedPartFileWriterTest.testWriteFile  Time elapsed: 2.311 s  <<< FAILURE!
2022-08-24T02:00:01.1671250Z Aug 24 02:00:01 java.lang.AssertionError: 
2022-08-24T02:00:01.1671626Z Aug 24 02:00:01 
2022-08-24T02:00:01.1672001Z Aug 24 02:00:01 Expected size: 1 but was: 2 in:
2022-08-24T02:00:01.1673656Z Aug 24 02:00:01 [DeprecatedRawLocalFileStatus{path=file:/tmp/junit3893779198554813459/junit1595046776902782406/2022-08-24--02; isDirectory=true; modification_time=1661306400000; access_time=1661306400396; owner=; group=; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false},
2022-08-24T02:00:01.1676131Z Aug 24 02:00:01     DeprecatedRawLocalFileStatus{path=file:/tmp/junit3893779198554813459/junit1595046776902782406/2022-08-24--01; isDirectory=true; modification_time=1661306400000; access_time=1661306400326; owner=; group=; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}]
2022-08-24T02:00:01.1677339Z Aug 24 02:00:01 	at org.apache.flink.formats.hadoop.bulk.HadoopPathBasedPartFileWriterTest.validateResult(HadoopPathBasedPartFileWriterTest.java:107)
2022-08-24T02:00:01.1678274Z Aug 24 02:00:01 	at org.apache.flink.formats.hadoop.bulk.HadoopPathBasedPartFileWriterTest.testWriteFile(HadoopPathBasedPartFileWriterTest.java:97)
2022-08-24T02:00:01.1679017Z Aug 24 02:00:01 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-24T02:00:01.1679666Z Aug 24 02:00:01 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-24T02:00:01.1680369Z Aug 24 02:00:01 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-24T02:00:01.1681019Z Aug 24 02:00:01 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-24T02:00:01.1681666Z Aug 24 02:00:01 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-08-24T02:00:01.1682385Z Aug 24 02:00:01 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-08-24T02:00:01.1683094Z Aug 24 02:00:01 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-08-24T02:00:01.1683965Z Aug 24 02:00:01 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-08-24T02:00:01.1684713Z Aug 24 02:00:01 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-08-24T02:00:01.1685956Z Aug 24 02:00:01 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
2022-08-24T02:00:01.1687185Z Aug 24 02:00:01 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
2022-08-24T02:00:01.1688347Z Aug 24 02:00:01 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-08-24T02:00:01.1689324Z Aug 24 02:00:01 	at java.lang.Thread.run(Thread.java:748)
2022-08-24T02:00:01.1690062Z Aug 24 02:00:01 
2022-08-24T02:00:03.4727706Z Aug 24 02:00:03 Formatting using clusterid: testClusterID
2022-08-24T02:00:07.9860626Z Aug 24 02:00:07 [INFO] Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.712 s - in org.apache.flink.formats.hadoop.bulk.committer.HadoopRenameCommitterHDFSTest
2022-08-24T02:00:08.4139747Z Aug 24 02:00:08 [INFO] 
2022-08-24T02:00:08.4140678Z Aug 24 02:00:08 [INFO] Results:
2022-08-24T02:00:08.4141326Z Aug 24 02:00:08 [INFO] 
2022-08-24T02:00:08.4142008Z Aug 24 02:00:08 [ERROR] Failures: 
2022-08-24T02:00:08.4144242Z Aug 24 02:00:08 [ERROR]   HadoopPathBasedPartFileWriterTest.testWriteFile:97->validateResult:107 
2022-08-24T02:00:08.4145317Z Aug 24 02:00:08 Expected size: 1 but was: 2 in:
2022-08-24T02:00:08.4147711Z Aug 24 02:00:08 [DeprecatedRawLocalFileStatus{path=file:/tmp/junit3893779198554813459/junit1595046776902782406/2022-08-24--02; isDirectory=true; modification_time=1661306400000; access_time=1661306400396; owner=; group=; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false},
2022-08-24T02:00:08.4150885Z Aug 24 02:00:08     DeprecatedRawLocalFileStatus{path=file:/tmp/junit3893779198554813459/junit1595046776902782406/2022-08-24--01; isDirectory=true; modification_time=1661306400000; access_time=1661306400326; owner=; group=; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}] {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40324&view=logs&j=7e3d33c3-a462-5ea8-98b8-27e1aafe4ceb&t=ef77f8d1-44c8-5ee2-f175-1c88f61de8c0",,hxb,kurt.ding,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17594,,,,,FLINK-27185,,,,,,,,"05/Sep/22 09:31;kurt.ding;image-2022-09-05-17-31-44-813.png;https://issues.apache.org/jira/secure/attachment/13048958/image-2022-09-05-17-31-44-813.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 12:15:34 UTC 2022,,,,,,,,,,"0|z180nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Aug/22 07:26;hxb;[~slinkydeveloper] Could you help take a look? Thx.;;;","05/Sep/22 02:32;kurt.ding;Let me try , [~hxb] ;;;","05/Sep/22 09:31;kurt.ding;Ha,Ha，I found the cause . The date format string pattern is ""yyyy-MM-dd–HH"" ,so if the first part in  stream source  come in at  2022-08-24-01:xx:xx and the second part of stream source come in at 2022-08-24-02:xx:xx  . So the date time assigner will assign file into two different file . Here is  the proof in log trace .

!image-2022-09-05-17-31-44-813.png!;;;","05/Sep/22 09:47;kurt.ding;Pr is avaliable, [~hxb] 

 ;;;","05/Sep/22 09:51;hxb;[~kurt.ding] Good job. I have assigned it to you.;;;","08/Nov/22 06:36;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42896&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=12322;;;","29/Nov/22 12:15;mapohl;master: ed46cb2fd64f1cb306ae5b7654d2b4d64ab69f22
1.16: 73dfd61858b7762e7d979bb8b09051abc0d82734
1.15: 84ec72f2cade11251c263a1a367634521461d225;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the determinism declaration of the rand function to be consistent with current behavior,FLINK-29091,13478325,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,24/Aug/22 05:21,04/Sep/22 13:36,13/Jul/23 08:13,04/Sep/22 13:36,,,,,,,,1.16.0,,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"RAND and RAND_INTEGER are declared as dynamic function (isDynamicFuntion returns true), as the declaration it should only evaluate once at query-level (not per record) for batch mode, FLINK-21713 did the similar fix for temporal functions.

But current behavior is completely a non-deterministic function which evaluated per record for both batch and streaming mode, it's not a good choice to break current behavior,  and the determinism of RAND function are also different across vendors:

[1] evaluated at query-level though it is treated as non-deterministic function [https://docs.microsoft.com/en-us/sql/relational-databases/user-defined-functions/deterministic-and-nondeterministic-functions?view=sql-server-ver16#built-in-function-determinism|https://docs.microsoft.com/en-us/sql/relational-databases/user-defined-functions/deterministic-and-nondeterministic-functions?view=sql-server-ver16#built-in-function-determinism)]

[2][ evaluated at row level:  [https://dev.mysql.com/doc/refman/5.7/en/mathematical-functions.html#function_rand]|https://dev.mysql.com/doc/refman/5.7/en/mathematical-functions.html#function_rand)]

[3] evaluated at row level if not specifies a seed,  e.g., DBMS_RANDOM.normal, DBMS_RANDOM.value(1,10)  [https://docs.oracle.com/database/timesten-18.1/TTPLP/d_random.htm#TTPLP71231|https://docs.oracle.com/database/timesten-18.1/TTPLP/d_random.htm#TTPLP71231)]

So just fix the determinism declaration of the rand function to be consistent with the current behavior and make it clear in the documentation.",,godfrey,jark,libenchao,lincoln.86xy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Sep 04 13:36:57 UTC 2022,,,,,,,,,,"0|z180io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/22 13:36;godfrey;Fixed in master: 4b15bc900eb60b1830bc406975ce974ad6050f98;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jdbc connector sql ITCase failed when run in idea,FLINK-29087,13478219,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,xuyangzhong,lsy,lsy,23/Aug/22 12:53,31/Aug/22 01:44,13/Jul/23 08:13,31/Aug/22 01:44,1.16.0,,,,,,,1.16.0,,,,,Connectors / JDBC,,,,,,,0,pull-request-available,,,,"java.lang.NoSuchFieldError: CORRELATE

    at org.apache.flink.table.planner.hint.FlinkHintStrategies.createHintStrategyTable(FlinkHintStrategies.java:91)
    at org.apache.flink.table.planner.delegation.PlannerContext.lambda$getSqlToRelConverterConfig$1(PlannerContext.java:288)
    at java.util.Optional.orElseGet(Optional.java:267)
    at org.apache.flink.table.planner.delegation.PlannerContext.getSqlToRelConverterConfig(PlannerContext.java:283)
    at org.apache.flink.table.planner.delegation.PlannerContext.createFrameworkConfig(PlannerContext.java:146)
    at org.apache.flink.table.planner.delegation.PlannerContext.<init>(PlannerContext.java:124)
    at org.apache.flink.table.planner.delegation.PlannerBase.<init>(PlannerBase.scala:121)
    at org.apache.flink.table.planner.delegation.StreamPlanner.<init>(StreamPlanner.scala:65)
    at org.apache.flink.table.planner.delegation.DefaultPlannerFactory.create(DefaultPlannerFactory.java:65)
    at org.apache.flink.table.factories.PlannerFactoryUtil.createPlanner(PlannerFactoryUtil.java:58)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.create(TableEnvironmentImpl.java:308)
    at org.apache.flink.table.api.TableEnvironment.create(TableEnvironment.java:93)
    at org.apache.flink.connector.jdbc.catalog.MySqlCatalogITCase.setup(MySqlCatalogITCase.java:159)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runners.Suite.runChild(Suite.java:128)
    at org.junit.runners.Suite.runChild(Suite.java:27)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
    at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)",,leonard,lsy,xuyangzhong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 31 01:44:56 UTC 2022,,,,,,,,,,"0|z17zvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 12:54;lsy;cc [~xuyangzhong] ;;;","23/Aug/22 12:58;xuyangzhong;I'll try to fix it.;;;","31/Aug/22 01:44;leonard;Fixed in master(1.16): d55be6850dc2a4e0291c0a4853fa5aa7a51a1d10;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Join Hint cannot be identified by lowercase,FLINK-29081,13478168,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,xuyangzhong,xuyangzhong,xuyangzhong,23/Aug/22 08:43,25/Aug/22 09:03,13/Jul/23 08:13,25/Aug/22 09:03,1.16.0,,,,,,,1.16.0,,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"The following sql can reproduce this bug:

select /*+ bRoadCasT(t1) */* from t1 join t1 as t3 on t1.a = t3.a;",,godfrey,xuyangzhong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 25 09:03:18 UTC 2022,,,,,,,,,,"0|z17zk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/22 09:03;godfrey;Fixed in master: fcaa4f77e0b3253ea902fbfad0bc1b2046ff814d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RescaleBucketITCase#testSuspendAndRecoverAfterRescaleOverwrite is not stable,FLINK-29075,13478148,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,qingyue,qingyue,23/Aug/22 07:35,27/Oct/22 07:35,13/Jul/23 08:13,27/Oct/22 03:55,table-store-0.2.0,,,,,,,table-store-0.3.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"[https://github.com/apache/flink-table-store/runs/7964774584?check_suite_focus=true]

!image-2022-08-23-15-35-59-499.png|width=576,height=370!",,lzljs3620320,qingyue,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/22 07:36;qingyue;image-2022-08-23-15-35-59-499.png;https://issues.apache.org/jira/secure/attachment/13048431/image-2022-08-23-15-35-59-499.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 07:35:07 UTC 2022,,,,,,,,,,"0|z17zfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/22 03:55;lzljs3620320;master: f3b1e813684194fa50d57663fea2f06d747f5f96;;;","27/Oct/22 07:35;TsReaper;release-0.2: 8377a51c76509dcdfbaa3bf28e806b50b2459814;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"use 'add jar' in sql client throws ""Could not find any jdbc dialect factories that implement""",FLINK-29074,13478131,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,xuyangzhong,xuyangzhong,23/Aug/22 06:03,07/Sep/22 02:35,13/Jul/23 08:13,07/Sep/22 02:35,1.16.0,,,,,,,1.16.0,1.17.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"The following step can reproduce this bug:

1、 create a source table 't1' in sql-client using jdbc(mysql)

2、add a jar with jdbc connector

3、select * from 't1'

then an exception throws:

java.lang.IllegalStateException: Could not find any jdbc dialect factories that implement 'org.apache.flink.connector.jdbc.dialect.JdbcDialectFactory' in the classpath.",,jark,lsy,xuyangzhong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 07 02:35:18 UTC 2022,,,,,,,,,,"0|z17zbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 06:22;lsy;[~xuyangzhong] Thanks for report, I will take a look;;;","07/Sep/22 02:35;jark;Fixed in 
 - master: 523546101f0180999f11d68269aad53c59134064 to 481ed78bec4211561e78be7586a102bd37a4dfb1
 - release-1.16: 3760a370ed7e4d92ff1cb14035bcc6abb00e02f8 to 38088230bb57486f81bb089a96aa3fa1e3f414f7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Table Store Hive CDH support,FLINK-29071,13478110,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,23/Aug/22 03:08,23/Aug/22 06:06,13/Jul/23 08:13,23/Aug/22 06:06,table-store-0.2.0,table-store-0.3.0,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,Currently Table Store Hive catalog and connectors cannot run against Hive 2.1 CDH 6.3. We should fix this support.,,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 23 06:06:55 UTC 2022,,,,,,,,,,"0|z17z74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 06:06;lzljs3620320;master: fe57dfa704117ff9f14dcf39bdb1dcba6e826972
release-0.2: aa85abe38963796904c77dd5062ee84d569bf9a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileFormatTest has some wrong assertion,FLINK-29063,13478003,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,klistopad,klistopad,klistopad,22/Aug/22 12:55,23/Aug/22 08:52,13/Jul/23 08:13,23/Aug/22 08:52,table-store-0.2.0,,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,org.apache.flink.table.store.file.FileFormatTest#testWriteRead reads the same value from file twice.,,klistopad,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 23 08:52:08 UTC 2022,,,,,,,,,,"0|z17ykg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 08:52;lzljs3620320;master: 3e4445611c97fde9b796f5be46385f945998f1c3
release-0.2: 287fe32f652780f116b7c1084f8b362ed56cfacb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The existing column stats are deleted incorrectly when analyze table for partial columns,FLINK-29059,13477977,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,22/Aug/22 10:27,25/Aug/22 14:03,13/Jul/23 08:13,25/Aug/22 14:02,1.16.0,,,,,,,1.16.0,,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"If there are three columns named `a, b, c` with column stats already exists,  I just analyze column `a` using `Analyze table xxx FOR COLUMNS a`, the existing column stats of `b, c` will be reset back to empty.",,337361684@qq.com,godfrey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28939,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 25 14:02:10 UTC 2022,,,,,,,,,,"0|z17yeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 10:28;337361684@qq.com;This Jira  is related to [FLINK-28939|https://issues.apache.org/jira/browse/FLINK-28939];;;","25/Aug/22 14:02;godfrey;Fixed in master: fe392645421d10923c75cd5438b91d9ed55900d3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw PartitionNotFoundException if the partition file is not readable for hybrid shuffle.,FLINK-29056,13477971,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,22/Aug/22 10:04,26/Aug/22 14:38,13/Jul/23 08:13,26/Aug/22 14:38,1.16.0,,,,,,,1.16.0,,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,"If data file is not readable especially data loss, throw PartitionNotFoundException to mark this result partition failed. Otherwise, the partition data is not regenerated, so failover can not recover the job.",,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 26 14:38:06 UTC 2022,,,,,,,,,,"0|z17ydc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/22 14:38;xtsong;master (1.16): c643a2953ba44b3b316ba52983932329dc0162e4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hybrid shuffle has concurrent modification of buffer when compression is enabled,FLINK-29053,13477898,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,22/Aug/22 06:04,01/Sep/22 03:36,13/Jul/23 08:13,01/Sep/22 03:36,1.16.0,,,,,,,1.16.0,,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,"When the downstream thread obtains the buffer and consuming it, if the data is compressed in the spilling thread and copied to the original buffer in the same time, since the two threads share the same memory data, the consuming thread will consume incorrect data, causing problems such as deserialize the data disorder.
Considering that the downstream consumption is prohibited during compression, or block spilling thread when the downstream consumption is not completed will have a great impact on performance. I think we should move the compression operation to the write thread and store the compressed buffer directly in memory.

 ",,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 01 03:36:00 UTC 2022,,,,,,,,,,"0|z17xx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/22 03:36;xtsong;master (1.16): 8b8245ba46b25c2617d91cff3d3a44b99879d9f2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveTableSourceStatisticsReportTest fails with Hadoop 3,FLINK-29046,13477609,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,337361684@qq.com,chesnay,chesnay,19/Aug/22 14:01,26/Aug/22 08:26,13/Jul/23 08:13,26/Aug/22 08:26,1.16.0,,,,,,,1.16.0,,,,,Connectors / Hive,Tests,,,,,,0,pull-request-available,,,,"
{code:java}
2022-08-19T13:35:56.1882498Z Aug 19 13:35:56 [ERROR] org.apache.flink.connectors.hive.HiveTableSourceStatisticsReportTest.testFlinkOrcFormatHiveTableSourceStatisticsReport  Time elapsed: 9.442 s  <<< FAILURE!
2022-08-19T13:35:56.1883817Z Aug 19 13:35:56 org.opentest4j.AssertionFailedError: 
2022-08-19T13:35:56.1884543Z Aug 19 13:35:56 
2022-08-19T13:35:56.1890435Z Aug 19 13:35:56 expected: TableStats{rowCount=3, colStats={f_boolean=ColumnStats(nullCount=1), f_smallint=ColumnStats(nullCount=0, max=128, min=100), f_decimal5=ColumnStats(nullCount=0, max=223.45, min=123.45), f_array=null, f_binary=null, f_decimal38=ColumnStats(nullCount=1, max=123433343334333433343334333433343334.34, min=123433343334333433343334333433343334.33), f_map=null, f_float=ColumnStats(nullCount=1, max=33.33300018310547, min=33.31100082397461), f_row=null, f_tinyint=ColumnStats(nullCount=0, max=3, min=1), f_decimal14=ColumnStats(nullCount=0, max=123333333355.33, min=123333333333.33), f_date=ColumnStats(nullCount=0, max=1990-10-16, min=1990-10-14), f_bigint=ColumnStats(nullCount=0, max=1238123899121, min=1238123899000), f_timestamp3=ColumnStats(nullCount=0, max=1990-10-16 12:12:43.123, min=1990-10-14 12:12:43.123), f_double=ColumnStats(nullCount=0, max=10.1, min=1.1), f_string=ColumnStats(nullCount=0, max=def, min=abcd), f_int=ColumnStats(nullCount=1, max=45536, min=31000)}}
2022-08-19T13:35:56.1902811Z Aug 19 13:35:56  but was: TableStats{rowCount=3, colStats={f_boolean=ColumnStats(nullCount=1), f_smallint=ColumnStats(nullCount=0, max=128, min=100), f_decimal5=ColumnStats(nullCount=0, max=223.45, min=0), f_array=null, f_binary=null, f_decimal38=ColumnStats(nullCount=1, max=123433343334333433343334333433343334.34, min=123433343334333433343334333433343334.33), f_map=null, f_float=ColumnStats(nullCount=1, max=33.33300018310547, min=33.31100082397461), f_row=null, f_tinyint=ColumnStats(nullCount=0, max=3, min=1), f_decimal14=ColumnStats(nullCount=0, max=123333333355.33, min=0), f_date=ColumnStats(nullCount=0, max=1990-10-16, min=1990-10-14), f_bigint=ColumnStats(nullCount=0, max=1238123899121, min=1238123899000), f_timestamp3=ColumnStats(nullCount=0, max=1990-10-16 12:12:43.123, min=1990-10-14 12:12:43.123), f_double=ColumnStats(nullCount=0, max=10.1, min=1.1), f_string=ColumnStats(nullCount=0, max=def, min=abcd), f_int=ColumnStats(nullCount=1, max=45536, min=31000)}}
2022-08-19T13:35:56.1908634Z Aug 19 13:35:56 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2022-08-19T13:35:56.1910402Z Aug 19 13:35:56 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2022-08-19T13:35:56.1912266Z Aug 19 13:35:56 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2022-08-19T13:35:56.1913257Z Aug 19 13:35:56 	at org.apache.flink.connectors.hive.HiveTableSourceStatisticsReportTest.assertHiveTableOrcFormatTableStatsEquals(HiveTableSourceStatisticsReportTest.java:339)
2022-08-19T13:35:56.1914512Z Aug 19 13:35:56 	at org.apache.flink.connectors.hive.HiveTableSourceStatisticsReportTest.testFlinkOrcFormatHiveTableSourceStatisticsReport(HiveTableSourceStatisticsReportTest.java:118)
2022-08-19T13:35:56.1915444Z Aug 19 13:35:56 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-19T13:35:56.1916130Z Aug 19 13:35:56 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-19T13:35:56.1916856Z Aug 19 13:35:56 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-19T13:35:56.1917571Z Aug 19 13:35:56 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-19T13:35:56.1918278Z Aug 19 13:35:56 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-08-19T13:35:56.1919020Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-08-19T13:35:56.1919923Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-08-19T13:35:56.1920841Z Aug 19 13:35:56 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-08-19T13:35:56.1921877Z Aug 19 13:35:56 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-08-19T13:35:56.1922778Z Aug 19 13:35:56 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2022-08-19T13:35:56.1923726Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-08-19T13:35:56.1924761Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-08-19T13:35:56.1925690Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-08-19T13:35:56.1926590Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-08-19T13:35:56.1927507Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-08-19T13:35:56.1928422Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-08-19T13:35:56.1929216Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-08-19T13:35:56.1930018Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-08-19T13:35:56.1930866Z Aug 19 13:35:56 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-08-19T13:35:56.1931868Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-19T13:35:56.1932794Z Aug 19 13:35:56 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-08-19T13:35:56.1933757Z Aug 19 13:35:56 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-08-19T13:35:56.1934645Z Aug 19 13:35:56 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-08-19T13:35:56.1935581Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-08-19T13:35:56.1936483Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-19T13:35:56.1937381Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-19T13:35:56.1938153Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-19T13:35:56.1938980Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-19T13:35:56.1939899Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-19T13:35:56.1940713Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-19T13:35:56.1941642Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-19T13:35:56.1942726Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-19T13:35:56.1943944Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-08-19T13:35:56.1945074Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-08-19T13:35:56.1946207Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-08-19T13:35:56.1947104Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-19T13:35:56.1947941Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-19T13:35:56.1948776Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-19T13:35:56.1949613Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-19T13:35:56.1950509Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-19T13:35:56.1951326Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-19T13:35:56.1952371Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-19T13:35:56.1953430Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-19T13:35:56.1954545Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-08-19T13:35:56.1955575Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-08-19T13:35:56.1956466Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-19T13:35:56.1957359Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-19T13:35:56.1958137Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-19T13:35:56.1959059Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-19T13:35:56.1959962Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-19T13:35:56.1960783Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-19T13:35:56.1961688Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-19T13:35:56.1962766Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-19T13:35:56.1963674Z Aug 19 13:35:56 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-08-19T13:35:56.1964372Z Aug 19 13:35:56 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-19T13:35:56.1965093Z Aug 19 13:35:56 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-19T13:35:56.1965758Z Aug 19 13:35:56 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-19T13:35:56.1966500Z Aug 19 13:35:56 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40205&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461",,337361684@qq.com,godfreyhe,hxbks2ks,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27988,HIVE-26492,,,,,,,"22/Aug/22 13:06;337361684@qq.com;image-2022-08-22-21-06-29-980.png;https://issues.apache.org/jira/secure/attachment/13048394/image-2022-08-22-21-06-29-980.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 26 08:26:31 UTC 2022,,,,,,,,,,"0|z17w54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 02:01;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40211&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=25486;;;","22/Aug/22 02:01;hxbks2ks;[~337361684@qq.com] Could you help take a look? Thx.;;;","22/Aug/22 13:11;337361684@qq.com;Hi, [~hxbks2ks] and [~chesnay] , I  tried to reproduce the error of these two failed tests. I found that the problem was not produced by HiveTableSourceStatisticsReport itself. It is caused by _orc.apache.orc.impl.writer.StructTreeWriter_ in {_}hive-exec-3.1.1{_}. In this class,  method _writeFileStatistics_ will create a wrong column stats min value when encounter decimal type data like:

!image-2022-08-22-21-06-29-980.png|width=395,height=273!

Also, I found that there is no test cases to cover decimal type data for hive 3.1.1 or upper version in Flink. So, I think the best solution now is to add tests to cover the decimal type stats report of hive 3.x for orc format, and create issue in hive to fix this error.;;;","22/Aug/22 13:16;337361684@qq.com;In this version, I could add a parameter in _HiveTableSourceStatisticsReport_ to distinguish between hive 3. X and hive 2. X. By default, hive 3. X will produce wrong results.;;;","22/Aug/22 13:17;337361684@qq.com;Could you assign this to me! Thanks a lot.

 ;;;","22/Aug/22 14:06;337361684@qq.com;Hive issue: https://issues.apache.org/jira/browse/HIVE-26492;;;","24/Aug/22 01:18;luoyuxia;Seems like a bug of orc 1.5.6 which Hive3 depends. And I also found a similar issue [ORC-516|https://issues.apache.org/jira/browse/ORC-517].  I think we can skip check decimal in Hive3. ;;;","26/Aug/22 08:26;godfreyhe;Fixed in master: d501b88be5599e14a0da578e3083ef53a6392b11;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AsyncWaitOperatorTest.testProcessingTimeRepeatedCompleteOrderedWithRetry failed with AssertionError,FLINK-29038,13477515,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lincoln.86xy,hxbks2ks,hxbks2ks,19/Aug/22 02:15,06/Feb/23 03:43,13/Jul/23 08:13,06/Feb/23 03:43,1.16.0,1.17.0,,,,,,1.16.2,1.17.0,,,,API / DataStream,,,,,,,0,pull-request-available,test-stability,,,"{code:java}
2022-08-18T15:58:28.8029934Z Aug 18 15:58:28 [INFO] Results:
2022-08-18T15:58:28.8030287Z Aug 18 15:58:28 [INFO] 
2022-08-18T15:58:28.8030644Z Aug 18 15:58:28 [ERROR] Failures: 
2022-08-18T15:58:28.8032800Z Aug 18 15:58:28 [ERROR]   AsyncWaitOperatorTest.testProcessingTimeRepeatedCompleteOrderedWithRetry:1203->testProcessingTimeWithRetry:1253 ORDERED Output was not correct.: array lengths differed, expected.length=3 actual.length=2; arrays first differed at element [2]; expected:<Record @ 6 : 12> but was:<end of array>
2022-08-18T15:58:28.8033826Z Aug 18 15:58:28 [INFO] 
2022-08-18T15:58:28.8034265Z Aug 18 15:58:28 [ERROR] Tests run: 2061, Failures: 1, Errors: 0, Skipped: 28
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40164&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8
",,hxb,hxbks2ks,lincoln.86xy,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27878,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 03:43:23 UTC 2023,,,,,,,,,,"0|z17vk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/22 02:24;hxbks2ks;[~lincoln.86xy] Could you help take a look? Thx.;;;","19/Aug/22 02:37;lincoln.86xy;[~hxbks2ks] sure, I'll lookup into this.;;;","22/Aug/22 03:35;lincoln.86xy;The failure record is the last input record of test data, it doesn't hit retry condition and was expected as a normal process and directly outputs '12,6', can't find the reason the record lost after `StreamTaskMailboxTestHarness#processAll` (no close operation before getOutput) so far, I will continue to track this problem and try to find someone that familiar with the harness test to help lookup at this.

 ;;;","29/Aug/22 09:43;lincoln.86xy;This failure most probably cause by slow execution of internal execution service used by the test async function. We will improve the execution deadline from 1s to 10s so as to reduce the failure possiblilty(In most cases, it will only take less than 1s).  Thanks [~gaoyunhaii] for helping investigating this!;;;","30/Aug/22 02:47;hxb;Thanks [~lincoln.86xy] and [~gaoyunhaii] for the fix.;;;","30/Aug/22 02:48;hxb;Merged into master via a1d74c131b0fd10f34436463077fd5c7a7984a2b;;;","02/Feb/23 08:55;mapohl;I'm reopening this issue since it appeared again in a 1.17 build.

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45586&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9203]

[~lincoln.86xy] or does it deserve a new ticket?;;;","02/Feb/23 12:20;lincoln.86xy;[~mapohl] Looks like the timeout is still occurring under the current test machine load after the time was previously turned up. Let's watch for a while, and if the failure of this case continues to occur I'll turn up the timeout again, WDYT?;;;","03/Feb/23 08:11;mapohl;[~lincoln.86xy]: I didn't check the fix earlier. I did a pass over it now: The test class itself doesn't follow Flink's coding guidelines in terms of timeouts which leaves it vulnerable to test instabilities. We agreed on not using timeouts in tests and let the be shutdown by the test watchdog process which will print a thread dump at the end (see [coding guidelines|https://flink.apache.org/contributing/code-style-and-quality-common.html#avoid-timeouts-in-junit-tests]). If we really have an issue with tests running into timeouts, printing the thread dump in the end might help investigating the issue in comparison to letting the test fail early. I feel like using the 10s deadline in [AsyncWaitOperatorTest:1242|https://github.com/apache/flink/blob/a1d74c131b0fd10f34436463077fd5c7a7984a2b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/async/AsyncWaitOperatorTest.java#L1242] is not necessary. The same applies to other tests in this class where we use timeouts.;;;","03/Feb/23 09:15;lincoln.86xy;[~mapohl] thanks for pointing out the testing guideline violation, agree with you not using local timeout and let the test watchdog process the timeout can be better. I've submitted a fix for this.;;;","03/Feb/23 13:12;lincoln.86xy;fixed in master: b138c82a464d825ec51587e57f136f91128d6bba;;;","06/Feb/23 03:43;lincoln.86xy;fixed in 1.16: db524dfb46673bc98d4cf93b9e84ed9a4b76d837;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HYBRID_FULL result partition type is not yet reConsumable,FLINK-29034,13477410,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,18/Aug/22 10:27,19/Aug/22 09:45,13/Jul/23 08:13,19/Aug/22 09:45,1.16.0,,,,,,,1.16.0,,,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"HYBRID_FULL partitions can be consumed repeatedly, but it does not support concurrent consumption. So re-consumable is false, but double calculation can be avoided during failover. If we regard it as re-consumable, there will be problems when the partition is reused. Therefore, we temporarily set this field false, and reset it to true when HsResultPartition supports downstream concurrent consumption of multiple identical subpartitions.",,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 19 09:45:16 UTC 2022,,,,,,,,,,"0|z17uww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/22 09:45;xtsong;master (1.16): f0a6c0cbd8313de8146c9c2610bb3db98bacaea0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the bug of CSV format doesn't work in Thread Mode,FLINK-29029,13477368,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,18/Aug/22 07:33,25/Aug/22 09:13,13/Jul/23 08:13,25/Aug/22 09:13,1.16.0,,,,,,,1.16.0,,,,,API / Python,,,,,,,0,pull-request-available,,,,,,hxb,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 25 09:13:38 UTC 2022,,,,,,,,,,"0|z17unk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/22 09:13;hxb;Merged into master via 0af535a9134aaf8ff86e936c7fd0a09c33e037a0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add the missing cache api in Python DataStream API,FLINK-29028,13477357,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,18/Aug/22 06:15,22/Aug/22 01:53,13/Jul/23 08:13,22/Aug/22 01:52,1.16.0,,,,,,,1.16.0,,,,,API / Python,,,,,,,0,pull-request-available,,,,,,dianfu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 22 01:52:55 UTC 2022,,,,,,,,,,"0|z17ul4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 01:52;dianfu;Merged to master via 97519d1683f3f5f086ce613d59aa5ba46cf38a30;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaITCase.testMultipleSourcesOnePartition failed with TopicExistsException,FLINK-29018,13477333,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,renqs,hxbks2ks,hxbks2ks,18/Aug/22 02:10,13/Sep/22 02:22,13/Jul/23 08:13,13/Sep/22 02:22,1.16.0,,,,,,,1.16.0,,,,,Connectors / Kafka,,,,,,,0,test-stability,,,,"{code:java}
2022-08-17T22:52:03.5376972Z Aug 17 22:52:03 [ERROR] Tests run: 23, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 133.5 s <<< FAILURE! - in org.apache.flink.streaming.connectors.kafka.KafkaITCase
2022-08-17T22:52:03.5378664Z Aug 17 22:52:03 [ERROR] org.apache.flink.streaming.connectors.kafka.KafkaITCase.testMultipleSourcesOnePartition  Time elapsed: 3.967 s  <<< FAILURE!
2022-08-17T22:52:03.5380231Z Aug 17 22:52:03 java.lang.AssertionError: Create test topic : manyToOneTopic failed, org.apache.kafka.common.errors.TopicExistsException: Topic 'manyToOneTopic' already exists.
2022-08-17T22:52:03.5381136Z Aug 17 22:52:03 	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.createTestTopic(KafkaTestEnvironmentImpl.java:207)
2022-08-17T22:52:03.5381983Z Aug 17 22:52:03 	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.createTestTopic(KafkaTestEnvironment.java:97)
2022-08-17T22:52:03.5382935Z Aug 17 22:52:03 	at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.createTestTopic(KafkaTestBase.java:217)
2022-08-17T22:52:03.5384026Z Aug 17 22:52:03 	at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.runMultipleSourcesOnePartitionExactlyOnceTest(KafkaConsumerTestBase.java:1053)
2022-08-17T22:52:03.5385057Z Aug 17 22:52:03 	at org.apache.flink.streaming.connectors.kafka.KafkaITCase.testMultipleSourcesOnePartition(KafkaITCase.java:105)
2022-08-17T22:52:03.5385741Z Aug 17 22:52:03 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-17T22:52:03.5386344Z Aug 17 22:52:03 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-17T22:52:03.5387038Z Aug 17 22:52:03 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-17T22:52:03.5387658Z Aug 17 22:52:03 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-17T22:52:03.5388350Z Aug 17 22:52:03 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-08-17T22:52:03.5389118Z Aug 17 22:52:03 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-08-17T22:52:03.5389875Z Aug 17 22:52:03 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-08-17T22:52:03.5390556Z Aug 17 22:52:03 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-08-17T22:52:03.5391564Z Aug 17 22:52:03 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
2022-08-17T22:52:03.5392325Z Aug 17 22:52:03 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
2022-08-17T22:52:03.5392985Z Aug 17 22:52:03 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-08-17T22:52:03.5393687Z Aug 17 22:52:03 	at java.lang.Thread.run(Thread.java:748)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40124&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37261",,godfrey,hxb,hxbks2ks,renqs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29153,,,,,FLINK-24119,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 13 02:21:50 UTC 2022,,,,,,,,,,"0|z17ufs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/22 02:10;hxbks2ks;[~renqs] Could help take a look? Thx.;;;","19/Aug/22 02:27;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40164&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","24/Aug/22 07:27;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40324&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","24/Aug/22 08:54;godfrey;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40290&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","26/Aug/22 06:11;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40394&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf;;;","29/Aug/22 02:32;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40398&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","30/Aug/22 07:54;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40505&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc;;;","30/Aug/22 07:54;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40505&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf;;;","30/Aug/22 07:55;hxb;[~renqs]  Any progress on this issue?;;;","31/Aug/22 03:46;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40524&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","31/Aug/22 06:08;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40540&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=f7d83ad5-3324-5307-0eb3-819065cdcb38&l=8272;;;","31/Aug/22 07:03;renqs;Thanks for sharing the info [~hxb]! This is caused by a recent version bump of Kafka client. I created FLINK-29153 for fixing this. ;;;","13/Sep/22 02:21;renqs;Should be fixed by FLINK-29153. Please reopen the issue if the unstable case pops up again. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fail to use ""transform using""  when record reader is binary record reader in Hive dialect",FLINK-29013,13477218,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,17/Aug/22 10:35,05/Sep/22 09:34,13/Jul/23 08:13,05/Sep/22 09:34,1.16.0,,,,,,,1.16.0,1.17.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,"It'll cause NPE when using following code:

 
{code:java}
tableEnv.executeSql(
                ""INSERT OVERWRITE TABLE dest1\n""
                        + ""SELECT TRANSFORM(*)\n""
                        + ""  USING 'cat'\n""
                        + ""  AS mydata STRING\n""
                        + ""    ROW FORMAT SERDE\n""
                        + ""      'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n""
                        + ""    WITH SERDEPROPERTIES (\n""
                        + ""      'serialization.last.column.takes.rest'='true'\n""
                        + ""    )\n""
                        + ""    RECORDREADER 'org.apache.hadoop.hive.ql.exec.BinaryRecordReader'\n""
                        + ""FROM src""){code}
 

The NPE is thrown in

 
{code:java}
// HiveScriptTransformOutReadThread

recordReader.next(reusedWritableObject); {code}
 

For BinaryRecordReader, we should first call method  BinaryRecordReader#createRow to do initialization
{code:java}
// BinaryRecordReader
public Writable createRow() throws IOException {
  bytes = new BytesWritable();
  bytes.setCapacity(maxRecordLength);
  return bytes;
} {code}
otherwise it will throw NPE for the field `bytes` is null in the following code:
{code:java}
// BinaryRecordReader

public int next(Writable row) throws IOException {
  int recordLength = in.read(bytes.get(), 0, maxRecordLength);
  if (recordLength >= 0) {
    bytes.setSize(recordLength);
  }
  return recordLength;
} {code}
 

 ",,jark,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 05 09:34:25 UTC 2022,,,,,,,,,,"0|z17tqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 03:33;luoyuxia;To fix it, we should first call RecordReader#createRow just like what Hive does.;;;","05/Sep/22 09:34;jark;Fixed in 
 - master: 8c2a2854eb0612d43c0d47e07d01fc539e227908
 - release-1.16: 3c6e11e08dde0f4d613928cfd14aecb0e7725adb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink function doc is not correct,FLINK-29012,13477202,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,17/Aug/22 09:15,23/Aug/22 08:11,13/Jul/23 08:13,23/Aug/22 08:11,1.16.0,,,,,,,1.16.0,,,,,Documentation,,,,,,,0,pull-request-available,,,,!image-2022-08-17-17-15-39-702.png!,,godfreyhe,jackylau,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/22 09:15;jackylau;image-2022-08-17-17-15-39-702.png;https://issues.apache.org/jira/secure/attachment/13048230/image-2022-08-17-17-15-39-702.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 23 08:11:05 UTC 2022,,,,,,,,,,"0|z17tn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 08:11;godfreyhe;Fixed in master: b7c6116c98dc21f5dcce70d8849aa2fb6a7c6ed4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink planner test is not correct,FLINK-29010,13477189,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jackylau,jackylau,17/Aug/22 08:18,23/Aug/22 04:01,13/Jul/23 08:13,23/Aug/22 04:01,1.16.0,,,,,,,1.16.0,,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"{code:java}
// code placeholder
private def prepareResult(seq: Seq[Row], isSorted: Boolean): Seq[String] = {
  if (!isSorted) seq.map(_.toString).sortBy(s => s) else seq.map(_.toString)
} {code}",,jackylau,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-17 08:18:20.0,,,,,,,,,,"0|z17tk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UsingRemoteJarITCase failed with NPE in hadoop3,FLINK-29007,13477162,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lsy,hxbks2ks,hxbks2ks,17/Aug/22 03:54,13/Feb/23 09:02,13/Jul/23 08:13,18/Aug/22 07:40,1.16.0,,,,,,,1.16.0,,,,,Table SQL / API,,,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-08-17T03:01:45.9844224Z Aug 17 03:01:45 [ERROR] UsingRemoteJarITCase.testUdfInRemoteJar  Time elapsed: 1.319 s  <<< FAILURE!
2022-08-17T03:01:45.9844747Z Aug 17 03:01:45 org.opentest4j.MultipleFailuresError: 
2022-08-17T03:01:45.9845163Z Aug 17 03:01:45 Multiple Failures (2 failures)
2022-08-17T03:01:45.9845669Z Aug 17 03:01:45 	java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration
2022-08-17T03:01:45.9846199Z Aug 17 03:01:45 	java.lang.NullPointerException: <no message>
2022-08-17T03:01:45.9846801Z Aug 17 03:01:45 	at org.junit.vintage.engine.execution.TestRun.getStoredResultOrSuccessful(TestRun.java:196)
2022-08-17T03:01:45.9847582Z Aug 17 03:01:45 	at org.junit.vintage.engine.execution.RunListenerAdapter.fireExecutionFinished(RunListenerAdapter.java:226)
2022-08-17T03:01:45.9848377Z Aug 17 03:01:45 	at org.junit.vintage.engine.execution.RunListenerAdapter.testFinished(RunListenerAdapter.java:192)
2022-08-17T03:01:45.9849199Z Aug 17 03:01:45 	at org.junit.vintage.engine.execution.RunListenerAdapter.testFinished(RunListenerAdapter.java:79)
2022-08-17T03:01:45.9849972Z Aug 17 03:01:45 	at org.junit.runner.notification.SynchronizedRunListener.testFinished(SynchronizedRunListener.java:87)
2022-08-17T03:01:45.9850715Z Aug 17 03:01:45 	at org.junit.runner.notification.RunNotifier$9.notifyListener(RunNotifier.java:225)
2022-08-17T03:01:45.9851413Z Aug 17 03:01:45 	at org.junit.runner.notification.RunNotifier$SafeNotifier.run(RunNotifier.java:72)
2022-08-17T03:01:45.9852105Z Aug 17 03:01:45 	at org.junit.runner.notification.RunNotifier.fireTestFinished(RunNotifier.java:222)
2022-08-17T03:01:45.9852828Z Aug 17 03:01:45 	at org.junit.internal.runners.model.EachTestNotifier.fireTestFinished(EachTestNotifier.java:38)
2022-08-17T03:01:45.9853510Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:372)
2022-08-17T03:01:45.9854156Z Aug 17 03:01:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-08-17T03:01:45.9854864Z Aug 17 03:01:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-08-17T03:01:45.9855512Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-08-17T03:01:45.9856125Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-08-17T03:01:45.9856751Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-08-17T03:01:45.9857372Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-08-17T03:01:45.9857977Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-08-17T03:01:45.9858581Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-08-17T03:01:45.9859177Z Aug 17 03:01:45 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-08-17T03:01:45.9859726Z Aug 17 03:01:45 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-08-17T03:01:45.9860299Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-08-17T03:01:45.9860905Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-08-17T03:01:45.9861508Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-08-17T03:01:45.9862134Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-08-17T03:01:45.9862831Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-08-17T03:01:45.9863477Z Aug 17 03:01:45 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-08-17T03:01:45.9864128Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-17T03:01:45.9864729Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-08-17T03:01:45.9865289Z Aug 17 03:01:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-08-17T03:01:45.9865851Z Aug 17 03:01:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-08-17T03:01:45.9866564Z Aug 17 03:01:45 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-08-17T03:01:45.9867293Z Aug 17 03:01:45 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-08-17T03:01:45.9868004Z Aug 17 03:01:45 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-08-17T03:01:45.9868759Z Aug 17 03:01:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-08-17T03:01:45.9869577Z Aug 17 03:01:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-08-17T03:01:45.9870401Z Aug 17 03:01:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-08-17T03:01:45.9871369Z Aug 17 03:01:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-08-17T03:01:45.9872223Z Aug 17 03:01:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-08-17T03:01:45.9872988Z Aug 17 03:01:45 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-08-17T03:01:45.9873698Z Aug 17 03:01:45 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-08-17T03:01:45.9874486Z Aug 17 03:01:45 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-08-17T03:01:45.9875313Z Aug 17 03:01:45 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-08-17T03:01:45.9876099Z Aug 17 03:01:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-08-17T03:01:45.9876922Z Aug 17 03:01:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-08-17T03:01:45.9877736Z Aug 17 03:01:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:128)
2022-08-17T03:01:45.9878501Z Aug 17 03:01:45 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-08-17T03:01:45.9879214Z Aug 17 03:01:45 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-08-17T03:01:45.9879881Z Aug 17 03:01:45 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-08-17T03:01:45.9880543Z Aug 17 03:01:45 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-08-17T03:01:45.9881179Z Aug 17 03:01:45 	Suppressed: java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration
2022-08-17T03:01:45.9881755Z Aug 17 03:01:45 		at org.junit.Assert.fail(Assert.java:89)
2022-08-17T03:01:45.9882395Z Aug 17 03:01:45 		at org.apache.flink.table.sql.codegen.UsingRemoteJarITCase.createHDFS(UsingRemoteJarITCase.java:88)
2022-08-17T03:01:45.9883166Z Aug 17 03:01:45 		at org.apache.flink.table.sql.codegen.UsingRemoteJarITCase.before(UsingRemoteJarITCase.java:68)
2022-08-17T03:01:45.9883816Z Aug 17 03:01:45 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-17T03:01:45.9884518Z Aug 17 03:01:45 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-17T03:01:45.9885230Z Aug 17 03:01:45 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-17T03:01:45.9885853Z Aug 17 03:01:45 		at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-17T03:01:45.9886488Z Aug 17 03:01:45 		at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-08-17T03:01:45.9887212Z Aug 17 03:01:45 		at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-08-17T03:01:45.9887921Z Aug 17 03:01:45 		at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-08-17T03:01:45.9888632Z Aug 17 03:01:45 		at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
2022-08-17T03:01:45.9889325Z Aug 17 03:01:45 		at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
2022-08-17T03:01:45.9889993Z Aug 17 03:01:45 		at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-08-17T03:01:45.9890671Z Aug 17 03:01:45 		at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
2022-08-17T03:01:45.9891333Z Aug 17 03:01:45 		at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-08-17T03:01:45.9891997Z Aug 17 03:01:45 		at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-08-17T03:01:45.9892716Z Aug 17 03:01:45 		at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-08-17T03:01:45.9893333Z Aug 17 03:01:45 		at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-17T03:01:45.9893995Z Aug 17 03:01:45 		at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-08-17T03:01:45.9894654Z Aug 17 03:01:45 		at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-08-17T03:01:45.9895132Z Aug 17 03:01:45 		... 39 more
2022-08-17T03:01:45.9895540Z Aug 17 03:01:45 	Suppressed: java.lang.NullPointerException
2022-08-17T03:01:45.9896168Z Aug 17 03:01:45 		at org.apache.flink.table.sql.codegen.UsingRemoteJarITCase.destroyHDFS(UsingRemoteJarITCase.java:95)
2022-08-17T03:01:45.9896836Z Aug 17 03:01:45 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-17T03:01:45.9897442Z Aug 17 03:01:45 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-17T03:01:45.9898150Z Aug 17 03:01:45 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-17T03:01:45.9898792Z Aug 17 03:01:45 		at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-17T03:01:45.9899423Z Aug 17 03:01:45 		at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-08-17T03:01:45.9900141Z Aug 17 03:01:45 		at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-08-17T03:01:45.9900850Z Aug 17 03:01:45 		at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-08-17T03:01:45.9901552Z Aug 17 03:01:45 		at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
2022-08-17T03:01:45.9902219Z Aug 17 03:01:45 		at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
2022-08-17T03:01:45.9902895Z Aug 17 03:01:45 		at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
2022-08-17T03:01:45.9903554Z Aug 17 03:01:45 		at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-08-17T03:01:45.9904220Z Aug 17 03:01:45 		at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-08-17T03:01:45.9904859Z Aug 17 03:01:45 		at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-08-17T03:01:45.9905473Z Aug 17 03:01:45 		at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-17T03:01:45.9906283Z Aug 17 03:01:45 		at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-08-17T03:01:45.9907052Z Aug 17 03:01:45 		at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-08-17T03:01:45.9907534Z Aug 17 03:01:45 		... 39 more
2022-08-17T03:01:45.9907852Z Aug 17 03:01:45 
2022-08-17T03:01:46.2525312Z Aug 17 03:01:46 [INFO] 
2022-08-17T03:01:46.2526850Z Aug 17 03:01:46 [INFO] Results:
2022-08-17T03:01:46.2527307Z Aug 17 03:01:46 [INFO] 
2022-08-17T03:01:46.2527708Z Aug 17 03:01:46 [ERROR] Failures: 
2022-08-17T03:01:46.2528297Z Aug 17 03:01:46 [ERROR] UsingRemoteJarITCase.testCreateCatalogFunctionUsingRemoteJar
2022-08-17T03:01:46.2528903Z Aug 17 03:01:46 [ERROR]   Run 1: Multiple Failures (2 failures)
2022-08-17T03:01:46.2529519Z Aug 17 03:01:46 	java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration
2022-08-17T03:01:46.2551750Z Aug 17 03:01:46 	java.lang.NullPointerException: <no message>
2022-08-17T03:01:46.2552413Z Aug 17 03:01:46 [ERROR]   Run 2: Multiple Failures (2 failures)
2022-08-17T03:01:46.2553047Z Aug 17 03:01:46 	java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration
2022-08-17T03:01:46.2553670Z Aug 17 03:01:46 	java.lang.NullPointerException: <no message>
2022-08-17T03:01:46.2554125Z Aug 17 03:01:46 [INFO] 
2022-08-17T03:01:46.2554658Z Aug 17 03:01:46 [ERROR] UsingRemoteJarITCase.testCreateTemporaryCatalogFunctionUsingRemoteJar
2022-08-17T03:01:46.2555270Z Aug 17 03:01:46 [ERROR]   Run 1: Multiple Failures (2 failures)
2022-08-17T03:01:46.2555862Z Aug 17 03:01:46 	java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration
2022-08-17T03:01:46.2556665Z Aug 17 03:01:46 	java.lang.NullPointerException: <no message>
2022-08-17T03:01:46.2557198Z Aug 17 03:01:46 [ERROR]   Run 2: Multiple Failures (2 failures)
2022-08-17T03:01:46.2557800Z Aug 17 03:01:46 	java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration
2022-08-17T03:01:46.2558407Z Aug 17 03:01:46 	java.lang.NullPointerException: <no message>
2022-08-17T03:01:46.2558865Z Aug 17 03:01:46 [INFO] 
2022-08-17T03:01:46.2559376Z Aug 17 03:01:46 [ERROR] UsingRemoteJarITCase.testCreateTemporarySystemFunctionUsingRemoteJar
2022-08-17T03:01:46.2559989Z Aug 17 03:01:46 [ERROR]   Run 1: Multiple Failures (2 failures)
2022-08-17T03:01:46.2560589Z Aug 17 03:01:46 	java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration
2022-08-17T03:01:46.2561194Z Aug 17 03:01:46 	java.lang.NullPointerException: <no message>
2022-08-17T03:01:46.2561726Z Aug 17 03:01:46 [ERROR]   Run 2: Multiple Failures (2 failures)
2022-08-17T03:01:46.2562317Z Aug 17 03:01:46 	java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration
2022-08-17T03:01:46.2562926Z Aug 17 03:01:46 	java.lang.NullPointerException: <no message>
2022-08-17T03:01:46.2563375Z Aug 17 03:01:46 [INFO] 
2022-08-17T03:01:46.2563836Z Aug 17 03:01:46 [ERROR] UsingRemoteJarITCase.testUdfInRemoteJar
2022-08-17T03:01:46.2564373Z Aug 17 03:01:46 [ERROR]   Run 1: Multiple Failures (2 failures)
2022-08-17T03:01:46.2564979Z Aug 17 03:01:46 	java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration
2022-08-17T03:01:46.2565573Z Aug 17 03:01:46 	java.lang.NullPointerException: <no message>
2022-08-17T03:01:46.2566104Z Aug 17 03:01:46 [ERROR]   Run 2: Multiple Failures (2 failures)
2022-08-17T03:01:46.2566702Z Aug 17 03:01:46 	java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration
2022-08-17T03:01:46.2567306Z Aug 17 03:01:46 	java.lang.NullPointerException: <no message>
2022-08-17T03:01:46.2567755Z Aug 17 03:01:46 [INFO] 
2022-08-17T03:01:46.2568104Z Aug 17 03:01:46 [INFO] 
2022-08-17T03:01:46.2568576Z Aug 17 03:01:46 [ERROR] Tests run: 6, Failures: 4, Errors: 0, Skipped: 0
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40084&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678
",,hxbks2ks,lsy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28916,,,FLINK-31033,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 18 07:40:04 UTC 2022,,,,,,,,,,"0|z17te8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/22 03:55;hxbks2ks;[~lsy] Could you help take a look? Thx;;;","17/Aug/22 04:11;lsy;ok, also cc [~Jiangang] ;;;","18/Aug/22 07:40;chesnay;master: 0be27a5e7b8d57c66cb809d563bbb594ce6abbad;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet row type reader should not return null value when some child fields is null ,FLINK-29005,13477149,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yuchuanchen,lsy,lsy,17/Aug/22 02:48,31/Aug/22 04:09,13/Jul/23 08:13,31/Aug/22 04:09,1.16.0,,,,,,,1.16.0,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,pull-request-available,,,,,,jark,lsy,martijnvisser,yuchuanchen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 31 04:09:39 UTC 2022,,,,,,,,,,"0|z17tbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/22 03:55;yuchuanchen;hi [~lsy] Could you assign this to me?;;;","17/Aug/22 04:10;lsy;cc [~jark] ;;;","30/Aug/22 07:45;martijnvisser;[~yuchuanchen] I've assigned it to you;;;","31/Aug/22 04:09;jark;Fixed in master: 00f585234f8db8fe1e2bfec5c6c323ca99d9b775;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OperatorCoordinatorHolder.checkpointCoordinatorInternal cannot mark for checkpoint ,FLINK-28999,13477037,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,hxbks2ks,hxbks2ks,16/Aug/22 10:53,16/Aug/22 11:06,13/Jul/23 08:13,16/Aug/22 11:06,1.16.0,,,,,,,1.16.0,,,,,Runtime / Checkpointing,,,,,,,0,test-stability,,,,"
{code:java}
2022-08-16T09:02:23.8660487Z Aug 16 09:02:23 java.util.concurrent.ExecutionException: org.apache.flink.runtime.checkpoint.CheckpointException: Trigger checkpoint failure.
2022-08-16T09:02:23.8661381Z Aug 16 09:02:23 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-08-16T09:02:23.8662108Z Aug 16 09:02:23 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2022-08-16T09:02:23.8663272Z Aug 16 09:02:23 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.restartFromSavepoint(SourceTestSuiteBase.java:343)
2022-08-16T09:02:23.8667137Z Aug 16 09:02:23 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.testSavepoint(SourceTestSuiteBase.java:236)
2022-08-16T09:02:23.8668444Z Aug 16 09:02:23 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-16T09:02:23.8669103Z Aug 16 09:02:23 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-16T09:02:23.8669997Z Aug 16 09:02:23 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-16T09:02:23.8671097Z Aug 16 09:02:23 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-16T09:02:23.8672030Z Aug 16 09:02:23 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-08-16T09:02:23.8672772Z Aug 16 09:02:23 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-08-16T09:02:23.8673625Z Aug 16 09:02:23 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-08-16T09:02:23.8674477Z Aug 16 09:02:23 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-08-16T09:02:23.8675549Z Aug 16 09:02:23 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-08-16T09:02:23.8676389Z Aug 16 09:02:23 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)
2022-08-16T09:02:23.8677556Z Aug 16 09:02:23 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-08-16T09:02:23.8678483Z Aug 16 09:02:23 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-08-16T09:02:23.8679356Z Aug 16 09:02:23 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-08-16T09:02:23.8680249Z Aug 16 09:02:23 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-08-16T09:02:23.8681482Z Aug 16 09:02:23 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-08-16T09:02:23.8682365Z Aug 16 09:02:23 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-08-16T09:02:23.8683409Z Aug 16 09:02:23 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-08-16T09:02:23.8684177Z Aug 16 09:02:23 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-08-16T09:02:23.8684996Z Aug 16 09:02:23 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-08-16T09:02:23.8685857Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-16T09:02:23.8686699Z Aug 16 09:02:23 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-08-16T09:02:23.8687639Z Aug 16 09:02:23 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-08-16T09:02:23.8688458Z Aug 16 09:02:23 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-08-16T09:02:23.8689301Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-08-16T09:02:23.8690233Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-16T09:02:23.8691703Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-16T09:02:23.8692970Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-16T09:02:23.8694170Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-16T09:02:23.8695490Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-16T09:02:23.8697088Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-16T09:02:23.8698124Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-16T09:02:23.8699258Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-16T09:02:23.8700344Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.submit(ForkJoinPoolHierarchicalTestExecutorService.java:118)
2022-08-16T09:02:23.8701511Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
2022-08-16T09:02:23.8702766Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:204)
2022-08-16T09:02:23.8704174Z Aug 16 09:02:23 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)
2022-08-16T09:02:23.8705582Z Aug 16 09:02:23 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)
2022-08-16T09:02:23.8706870Z Aug 16 09:02:23 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-08-16T09:02:23.8707882Z Aug 16 09:02:23 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-16T09:02:23.8708557Z Aug 16 09:02:23 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
2022-08-16T09:02:23.8709200Z Aug 16 09:02:23 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-16T09:02:23.8709858Z Aug 16 09:02:23 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-08-16T09:02:23.8710514Z Aug 16 09:02:23 	at java.util.LinkedList$LLSpliterator.forEachRemaining(LinkedList.java:1235)
2022-08-16T09:02:23.8711547Z Aug 16 09:02:23 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647)
2022-08-16T09:02:23.8712593Z Aug 16 09:02:23 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2022-08-16T09:02:23.8713777Z Aug 16 09:02:23 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2022-08-16T09:02:23.8714679Z Aug 16 09:02:23 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-08-16T09:02:23.8715352Z Aug 16 09:02:23 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-08-16T09:02:23.8716031Z Aug 16 09:02:23 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-08-16T09:02:23.8716728Z Aug 16 09:02:23 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-08-16T09:02:23.8717746Z Aug 16 09:02:23 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-08-16T09:02:23.8718415Z Aug 16 09:02:23 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-08-16T09:02:23.8719084Z Aug 16 09:02:23 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2022-08-16T09:02:23.8719949Z Aug 16 09:02:23 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2022-08-16T09:02:23.8720750Z Aug 16 09:02:23 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-08-16T09:02:23.8721724Z Aug 16 09:02:23 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-08-16T09:02:23.8722860Z Aug 16 09:02:23 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-08-16T09:02:23.8723832Z Aug 16 09:02:23 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-08-16T09:02:23.8724526Z Aug 16 09:02:23 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-08-16T09:02:23.8725164Z Aug 16 09:02:23 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-08-16T09:02:23.8725923Z Aug 16 09:02:23 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)
2022-08-16T09:02:23.8726970Z Aug 16 09:02:23 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)
2022-08-16T09:02:23.8728221Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-08-16T09:02:23.8729045Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-16T09:02:23.8730141Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-16T09:02:23.8731495Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-16T09:02:23.8732605Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-16T09:02:23.8733444Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-16T09:02:23.8734303Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-16T09:02:23.8735054Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-16T09:02:23.8735974Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-16T09:02:23.8737088Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-08-16T09:02:23.8738540Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-08-16T09:02:23.8739687Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-08-16T09:02:23.8740778Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-16T09:02:23.8741589Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-16T09:02:23.8742476Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-16T09:02:23.8743624Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-16T09:02:23.8744830Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-16T09:02:23.8746096Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-16T09:02:23.8747508Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-16T09:02:23.8749049Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-16T09:02:23.8750886Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-08-16T09:02:23.8752423Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-08-16T09:02:23.8753846Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-16T09:02:23.8755201Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-16T09:02:23.8756198Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-16T09:02:23.8756954Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-16T09:02:23.8757972Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-16T09:02:23.8758769Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-16T09:02:23.8759538Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-16T09:02:23.8760457Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-16T09:02:23.8761708Z Aug 16 09:02:23 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-08-16T09:02:23.8762365Z Aug 16 09:02:23 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-16T09:02:23.8762997Z Aug 16 09:02:23 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-16T09:02:23.8763654Z Aug 16 09:02:23 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-16T09:02:23.8764657Z Aug 16 09:02:23 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-08-16T09:02:23.8765392Z Aug 16 09:02:23 Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Trigger checkpoint failure.
2022-08-16T09:02:23.8766097Z Aug 16 09:02:23 	at org.apache.flink.runtime.checkpoint.PendingCheckpoint.abort(PendingCheckpoint.java:550)
2022-08-16T09:02:23.8767045Z Aug 16 09:02:23 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2096)
2022-08-16T09:02:23.8768208Z Aug 16 09:02:23 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2083)
2022-08-16T09:02:23.8769060Z Aug 16 09:02:23 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.onTriggerFailure(CheckpointCoordinator.java:956)
2022-08-16T09:02:23.8769940Z Aug 16 09:02:23 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.onTriggerFailure(CheckpointCoordinator.java:929)
2022-08-16T09:02:23.8770864Z Aug 16 09:02:23 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.lambda$startTriggeringCheckpoint$7(CheckpointCoordinator.java:657)
2022-08-16T09:02:23.8771863Z Aug 16 09:02:23 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2022-08-16T09:02:23.8773015Z Aug 16 09:02:23 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2022-08-16T09:02:23.8774173Z Aug 16 09:02:23 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2022-08-16T09:02:23.8775251Z Aug 16 09:02:23 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2022-08-16T09:02:23.8776338Z Aug 16 09:02:23 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-08-16T09:02:23.8777649Z Aug 16 09:02:23 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
2022-08-16T09:02:23.8779006Z Aug 16 09:02:23 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
2022-08-16T09:02:23.8780292Z Aug 16 09:02:23 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-08-16T09:02:23.8781377Z Aug 16 09:02:23 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-08-16T09:02:23.8782051Z Aug 16 09:02:23 	at java.lang.Thread.run(Thread.java:748)
2022-08-16T09:02:23.8782658Z Aug 16 09:02:23 Caused by: java.lang.IllegalStateException: Cannot mark for checkpoint 211, already marked for checkpoint 210
2022-08-16T09:02:23.8783647Z Aug 16 09:02:23 	at org.apache.flink.runtime.operators.coordination.SubtaskGatewayImpl.markForCheckpoint(SubtaskGatewayImpl.java:185)
2022-08-16T09:02:23.8784578Z Aug 16 09:02:23 	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.lambda$checkpointCoordinatorInternal$6(OperatorCoordinatorHolder.java:328)
2022-08-16T09:02:23.8785350Z Aug 16 09:02:23 	at java.util.HashMap.forEach(HashMap.java:1289)
2022-08-16T09:02:23.8786101Z Aug 16 09:02:23 	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.checkpointCoordinatorInternal(OperatorCoordinatorHolder.java:327)
2022-08-16T09:02:23.8787056Z Aug 16 09:02:23 	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.lambda$checkpointCoordinator$0(OperatorCoordinatorHolder.java:243)
2022-08-16T09:02:23.8788361Z Aug 16 09:02:23 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:453)
2022-08-16T09:02:23.8789659Z Aug 16 09:02:23 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-08-16T09:02:23.8791066Z Aug 16 09:02:23 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:453)
2022-08-16T09:02:23.8792253Z Aug 16 09:02:23 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:218)
2022-08-16T09:02:23.8793351Z Aug 16 09:02:23 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2022-08-16T09:02:23.8794385Z Aug 16 09:02:23 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2022-08-16T09:02:23.8795385Z Aug 16 09:02:23 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-08-16T09:02:23.8796185Z Aug 16 09:02:23 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-08-16T09:02:23.8797442Z Aug 16 09:02:23 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-08-16T09:02:23.8798174Z Aug 16 09:02:23 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-08-16T09:02:23.8798819Z Aug 16 09:02:23 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-08-16T09:02:23.8799547Z Aug 16 09:02:23 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-08-16T09:02:23.8800690Z Aug 16 09:02:23 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-16T09:02:23.8801895Z Aug 16 09:02:23 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-16T09:02:23.8802879Z Aug 16 09:02:23 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-08-16T09:02:23.8803628Z Aug 16 09:02:23 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-08-16T09:02:23.8804614Z Aug 16 09:02:23 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-08-16T09:02:23.8805553Z Aug 16 09:02:23 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-08-16T09:02:23.8806571Z Aug 16 09:02:23 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-08-16T09:02:23.8807544Z Aug 16 09:02:23 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-08-16T09:02:23.8808366Z Aug 16 09:02:23 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-08-16T09:02:23.8809211Z Aug 16 09:02:23 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-08-16T09:02:23.8810075Z Aug 16 09:02:23 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-16T09:02:23.8810819Z Aug 16 09:02:23 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-16T09:02:23.8811484Z Aug 16 09:02:23 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-16T09:02:23.8812143Z Aug 16 09:02:23 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40044&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203
",,hxbks2ks,yunfengzhou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28941,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 16 11:05:04 UTC 2022,,,,,,,,,,"0|z17smg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 10:55;hxbks2ks;[~yunfengzhou] Could you help take a look? Whether it is related to https://issues.apache.org/jira/browse/FLINK-28606?;;;","16/Aug/22 11:03;yunfengzhou;This problem is revealed and reported by the changes in FLINK-28606, but it requires changing the implementation of Flink's general checkpoint mechanism to solve this problem. I created a similar ticket (FLINK-28941) which includes my previous analysis of the cause of this problem.;;;","16/Aug/22 11:05;hxbks2ks;Thanks a lot for the investigation. [~yunfengzhou];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix NPE problem: createRowData will return null while encounter hive default partition,FLINK-28995,13477013,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pltbkd,337361684@qq.com,337361684@qq.com,16/Aug/22 09:09,23/Aug/22 09:50,13/Jul/23 08:13,23/Aug/22 09:50,1.16.0,,,,,,,1.16.0,,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,"createRowData will return null while encounter hive default partition, which will make {{‘data.contains(partitionRow)’ throw NPE.}}",,337361684@qq.com,godfreyhe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 23 09:50:49 UTC 2022,,,,,,,,,,"0|z17sh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 09:50;godfreyhe;Fixed in master: 2322791284e4e09895d8eb1bc45f4fad8287fcc0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable withCredentials for Flink UI,FLINK-28994,13477012,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,airblader,airblader,airblader,16/Aug/22 08:59,16/Aug/22 13:22,13/Jul/23 08:13,16/Aug/22 13:22,,,,,,,,1.15.2,1.16.0,,,,Runtime / Web Frontend,,,,,,,0,pull-request-available,,,,"Some environments require cookies to authenticate the Flink UI. By enabling the withCredentials flag, Angular will send cookies along with the request.",,airblader,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 16 13:22:49 UTC 2022,,,,,,,,,,"0|z17sgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 13:22;twalthr;Fixed in master: bd274ca8a1b8249aa8a6a25514246ff79a89203b
Fixed in 1.15: 44c6496e95426a186dba7f775d3f65df3a00c979;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix adjusting join cost for dpp query pattern error,FLINK-28993,13477010,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,16/Aug/22 08:56,24/Aug/22 02:20,13/Jul/23 08:13,24/Aug/22 02:20,1.16.0,,,,,,,1.16.0,,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"Now, adjusting join cost for dpp query pattern have two errors:

1. For these table which without table stats, we haven't plus the

dynamicPartitionPruningFactor.

2. For method #DynamicPartitionPruningUtils$visitFactSide, we have not judge whether candiateFields contain partition keys.

 ",,337361684@qq.com,godfreyhe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 24 02:20:30 UTC 2022,,,,,,,,,,"0|z17sgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Aug/22 02:20;godfreyhe;Fixed in master: 1a0f591a59b59f0c6ce71f5af9e0660293c33fc1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change Ndv takes the max value instead of sum of all partitions when getting partition table column stats,FLINK-28992,13477006,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,16/Aug/22 08:42,22/Aug/22 02:24,13/Jul/23 08:13,22/Aug/22 02:24,1.16.0,,,,,,,1.16.0,,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"Now, when we obtain column statistics Ndv of the partition table, we use the method of taking sum of Ndv of all partitions, which will cause the obtained stats to be far from the real stats. So now we take the max value instead of sum of all partitions.",,337361684@qq.com,godfreyhe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 22 02:24:02 UTC 2022,,,,,,,,,,"0|z17sfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 02:24;godfreyhe;Fixed in master: 10e4c82902164f2651d73f018393f930dcef55a8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix BatchPhysicalDynamicFilteringDataCollector with empty output type,FLINK-28990,13476998,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,16/Aug/22 08:15,19/Aug/22 03:36,13/Jul/23 08:13,19/Aug/22 03:36,1.16.0,,,,,,,1.16.0,,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"When dpp fact side have calc node, and partition key index was changed in calc node, 
{code:java}
BatchPhysicalDynamicFilteringDataCollector  {code}
will be set with a empty output type, which will throw exception in 
{code:java}
HiveSourceDynamicFileEnumerator{code}
  while check argument:
{code:java}
Preconditions.checkArgument(rowType.getFieldCount() == dynamicFilterPartitionKeys.size()); {code}
in method 
{code:java}
setDynamicFilteringData {code}
 ",,337361684@qq.com,godfreyhe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 19 03:36:14 UTC 2022,,,,,,,,,,"0|z17sds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/22 03:36;godfreyhe;Fixed in master: 06e8b7fb30fee04ed61cdf99763fc7cd17401c8e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect result for filter after temporal join,FLINK-28988,13476990,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,csq,xuannan,xuannan,16/Aug/22 07:58,10/Jan/23 19:53,13/Jul/23 08:13,05/Dec/22 12:32,1.15.1,,,,,,,1.16.1,1.17.0,,,,Table SQL / API,,,,,,,1,pull-request-available,,,,"The following code can reproduce the case

 
{code:java}
public class TemporalJoinSQLExample1 {

    public static void main(String[] args) throws Exception {

        // set up the Java DataStream API
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // set up the Java Table API
        final StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);

        final DataStreamSource<Tuple3<Integer, String, Instant>> ds =
                env.fromElements(
                        new Tuple3<>(0, ""online"", Instant.ofEpochMilli(0)),
                        new Tuple3<>(0, ""offline"", Instant.ofEpochMilli(10)),
                        new Tuple3<>(0, ""online"", Instant.ofEpochMilli(20)));

        final Table table =
                tableEnv.fromDataStream(
                                ds,
                                Schema.newBuilder()
                                        .column(""f0"", DataTypes.INT())
                                        .column(""f1"", DataTypes.STRING())
                                        .column(""f2"", DataTypes.TIMESTAMP_LTZ(3))
                                        .watermark(""f2"", ""f2 - INTERVAL '2' SECONDS"")
                                        .build())
                        .as(""id"", ""state"", ""ts"");
        tableEnv.createTemporaryView(""source_table"", table);
        final Table dedupeTable =
                tableEnv.sqlQuery(
                        ""SELECT * FROM (""
                                + "" SELECT *, ROW_NUMBER() OVER (PARTITION BY id ORDER BY ts DESC) AS row_num FROM source_table""
                                + "") WHERE row_num = 1"");
        tableEnv.createTemporaryView(""versioned_table"", dedupeTable);

        DataStreamSource<Tuple2<Integer, Instant>> event =
                env.fromElements(
                        new Tuple2<>(0, Instant.ofEpochMilli(0)),
                        new Tuple2<>(0, Instant.ofEpochMilli(5)),
                        new Tuple2<>(0, Instant.ofEpochMilli(10)),
                        new Tuple2<>(0, Instant.ofEpochMilli(15)),
                        new Tuple2<>(0, Instant.ofEpochMilli(20)),
                        new Tuple2<>(0, Instant.ofEpochMilli(25)));

        final Table eventTable =
                tableEnv.fromDataStream(
                                event,
                                Schema.newBuilder()
                                        .column(""f0"", DataTypes.INT())
                                        .column(""f1"", DataTypes.TIMESTAMP_LTZ(3))
                                        .watermark(""f1"", ""f1 - INTERVAL '2' SECONDS"")
                                        .build())
                        .as(""id"", ""ts"");

        tableEnv.createTemporaryView(""event_table"", eventTable);

        final Table result =
                tableEnv.sqlQuery(
                        ""SELECT * FROM event_table""
                                + "" LEFT JOIN versioned_table FOR SYSTEM_TIME AS OF event_table.ts""
                                + "" ON event_table.id = versioned_table.id"");
        result.execute().print();

        result.filter($(""state"").isEqual(""online"")).execute().print();
    }
} {code}
 

The result of temporal join is the following:
|op|         id|                     ts|        id0|                         state|                    ts0|             row_num|
|+I|          0|1970-01-01 08:00:00.000|          0|                        online|1970-01-01 08:00:00.000|                   1|
|+I|          0|1970-01-01 08:00:00.005|          0|                        online|1970-01-01 08:00:00.000|                   1|
|+I|          0|1970-01-01 08:00:00.010|          0|                       offline|1970-01-01 08:00:00.010|                   1|
|+I|          0|1970-01-01 08:00:00.015|          0|                       offline|1970-01-01 08:00:00.010|                   1|
|+I|          0|1970-01-01 08:00:00.020|          0|                        online|1970-01-01 08:00:00.020|                   1|
|+I|          0|1970-01-01 08:00:00.025|          0|                        online|1970-01-01 08:00:00.020|                   1|

 

After filtering with predicate state = 'online', I expect only the two rows with state offline will be filtered out. But I got the following result:

|op|         id|                     ts|        id0|                         state|                    ts0|             row_num|
|+I|          0|1970-01-01 08:00:00.020|          0|                        online|1970-01-01 08:00:00.020|                   1|
|+I|          0|1970-01-01 08:00:00.025|          0|                        online|1970-01-01 08:00:00.020|                   1|

 
 
 

 ",,csq,godfrey,jingge,Mathieu Druart,qingyue,xuannan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,Tue Jan 10 19:53:17 UTC 2023,,,,,,,,,,"0|z17sc0:",9223372036854775807,"After FLINK-28988 applied, the filter will not be pushed down into both inputs of the event time temporal join.
Note this may cause incompatible plan changes compare to 1.16.0, e.g., when left input is an upsert source(use upsert-kafka connector), the query plan will remove the ChangelogNormalize node from which appeared in 1.16.0.",,,,,,,,,,,,,,,,,,,"29/Aug/22 15:27;csq;[~xuannan] Thank you for reporting the issue. I have also reproduced the output with the code snippet you provided in the latest master branch.  I would like to have a deep dive into the issue.;;;","05/Sep/22 03:04;csq;TLDR: The filters in aboveFilter should not be pushed down into right table when it is a temporal join.

when there's no filter after temporal join, the query is explained as below:

{code:xml}
== Abstract Syntax Tree ==
LogicalProject(id=[$0], ts=[$1], id0=[$2], state=[$3], ts0=[$4], row_num=[$5])
+- LogicalCorrelate(correlation=[$cor0], joinType=[left], requiredColumns=[{0, 1}])
   :- LogicalProject(id=[AS($0, _UTF-16LE'id')], ts=[AS($1, _UTF-16LE'ts')])
   :  +- LogicalWatermarkAssigner(rowtime=[f1], watermark=[-($1, 2000:INTERVAL SECOND)])
   :     +- LogicalTableScan(table=[[*anonymous_datastream_source$2*]])
   +- LogicalFilter(condition=[=($cor0.id, $0)])
      +- LogicalSnapshot(period=[$cor0.ts])
         +- LogicalProject(id=[$0], state=[$1], ts=[$2], row_num=[$3])
            +- LogicalFilter(condition=[=($3, 1)])
               +- LogicalProject(id=[AS($0, _UTF-16LE'id')], state=[AS($1, _UTF-16LE'state')], ts=[AS($2, _UTF-16LE'ts')], row_num=[ROW_NUMBER() OVER (PARTITION BY AS($0, _UTF-16LE'id') ORDER BY AS($2, _UTF-16LE'ts') DESC NULLS LAST)])
                  +- LogicalWatermarkAssigner(rowtime=[f2], watermark=[-($2, 2000:INTERVAL SECOND)])
                     +- LogicalTableScan(table=[[*anonymous_datastream_source$1*]])

== Optimized Physical Plan ==
Calc(select=[id, ts, id0, state, CAST(ts0 AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS ts0, row_num])
+- TemporalJoin(joinType=[LeftOuterJoin], where=[AND(=(id, id0), __TEMPORAL_JOIN_CONDITION(ts, ts0, __TEMPORAL_JOIN_CONDITION_PRIMARY_KEY(id0), __TEMPORAL_JOIN_LEFT_KEY(id), __TEMPORAL_JOIN_RIGHT_KEY(id0)))], select=[id, ts, id0, state, ts0, row_num])
   :- Exchange(distribution=[hash[id]])
   :  +- Calc(select=[f0 AS id, f1 AS ts])
   :     +- WatermarkAssigner(rowtime=[f1], watermark=[-(f1, 2000:INTERVAL SECOND)])
   :        +- TableSourceScan(table=[[*anonymous_datastream_source$2*]], fields=[f0, f1])
   +- Exchange(distribution=[hash[id]])
      +- Calc(select=[$0 AS id, $1 AS state, $2 AS ts, 1:BIGINT AS row_num])
         +- Deduplicate(keep=[LastRow], key=[$0], order=[ROWTIME])
            +- Exchange(distribution=[hash[$0]])
               +- Calc(select=[f0 AS $0, f1 AS $1, f2 AS $2])
                  +- WatermarkAssigner(rowtime=[f2], watermark=[-(f2, 2000:INTERVAL SECOND)])
                     +- TableSourceScan(table=[[*anonymous_datastream_source$1*]], fields=[f0, f1, f2])

== Optimized Execution Plan ==
Calc(select=[id, ts, id0, state, CAST(ts0 AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS ts0, row_num])
+- TemporalJoin(joinType=[LeftOuterJoin], where=[((id = id0) AND __TEMPORAL_JOIN_CONDITION(ts, ts0, __TEMPORAL_JOIN_CONDITION_PRIMARY_KEY(id0), __TEMPORAL_JOIN_LEFT_KEY(id), __TEMPORAL_JOIN_RIGHT_KEY(id0)))], select=[id, ts, id0, state, ts0, row_num])
   :- Exchange(distribution=[hash[id]])
   :  +- Calc(select=[f0 AS id, f1 AS ts])
   :     +- WatermarkAssigner(rowtime=[f1], watermark=[(f1 - 2000:INTERVAL SECOND)])
   :        +- TableSourceScan(table=[[*anonymous_datastream_source$2*]], fields=[f0, f1])
   +- Exchange(distribution=[hash[id]])
      +- Calc(select=[$0 AS id, $1 AS state, $2 AS ts, 1 AS row_num])
         +- Deduplicate(keep=[LastRow], key=[$0], order=[ROWTIME])
            +- Exchange(distribution=[hash[$0]])
               +- Calc(select=[f0 AS $0, f1 AS $1, f2 AS $2])
                  +- WatermarkAssigner(rowtime=[f2], watermark=[(f2 - 2000:INTERVAL SECOND)])
                     +- TableSourceScan(table=[[*anonymous_datastream_source$1*]], fields=[f0, f1, f2])
{code}

And after the FlinkChangelogModeInferenceProgram, the UpdateKindTrait of Deduplicate(keep=[LastRow], key=[$0], order=[ROWTIME]) will be come [ONLY_UPDATE_AFTER]. Therefore,  during  execution runtime, the rightSortedState in TemporalRowTimeJoinOperator contains the following rows:
[+I, 0, online, 1970-01-01 08:00:00.000]
[+I, 0, offline, 1970-01-01 08:00:00.010]
[+I, 0, online, 1970-01-01 08:00:00.020]

So we can get the expected temporal join result:
[+I,0,1970-01-01 08:00:00.000,0,online,970-01-01 08:00:00.000 ,1]
[+I,0,1970-01-01 08:00:00.005,0,online,1970-01-01 08:00:00.000,1]
[+I,0,1970-01-01 08:00:00.0010,0,offline,1970-01-01 08:00:00.010,1]
[+I,0,1970-01-01 08:00:00.0015,0,offline,1970-01-01 08:00:00.010,1]
[+I,0,1970-01-01 08:00:00.020,0,online,1970-01-01 08:00:00.020,1]
[+I,0,1970-01-01 08:00:00.025,0,online,1970-01-01 08:00:00.020,1]

However, if the filter was pushed down into the right table, the right sorted state will bocome:
[+I, 0, online, 1970-01-01 08:00:00.000]
[+I, 0, online, 1970-01-01 08:00:00.020]

and the temporal join result will become:
[+I,0,1970-01-01 08:00:00.000,0,online,970-01-01 08:00:00.000 ,1]
[+I,0,1970-01-01 08:00:00.005,0,online,1970-01-01 08:00:00.000,1]
[+I,0,1970-01-01 08:00:00.0010,0,online,1970-01-01 08:00:00.000,1]
[+I,0,1970-01-01 08:00:00.0015,0,online,1970-01-01 08:00:00.000,1]
[+I,0,1970-01-01 08:00:00.020,0,online,1970-01-01 08:00:00.020,1]
[+I,0,1970-01-01 08:00:00.025,0,online,1970-01-01 08:00:00.020,1]

while the expected result is:
[+I,0,1970-01-01 08:00:00.000,0,online,970-01-01 08:00:00.000 ,1]
[+I,0,1970-01-01 08:00:00.005,0,online,1970-01-01 08:00:00.000,1]
[+I,0,1970-01-01 08:00:00.020,0,online,1970-01-01 08:00:00.020,1]
[+I,0,1970-01-01 08:00:00.025,0,online,1970-01-01 08:00:00.020,1].

*We can find that if the record in right table is filtered before join, the temporal join result may be wrong.*

However, when we added a filter after temporal join, the actual output is:
[+I,0,1970-01-01 08:00:00.020,0,online,1970-01-01 08:00:00.020,1]
[+I,0,1970-01-01 08:00:00.025,0,online,1970-01-01 08:00:00.020,1]

this is not equal to 
[+I,0,1970-01-01 08:00:00.000,0,online,970-01-01 08:00:00.000 ,1]
[+I,0,1970-01-01 08:00:00.005,0,online,1970-01-01 08:00:00.000,1]
[+I,0,1970-01-01 08:00:00.0010,0,online,1970-01-01 08:00:00.000,1]
[+I,0,1970-01-01 08:00:00.0015,0,online,1970-01-01 08:00:00.000,1]
[+I,0,1970-01-01 08:00:00.020,0,online,1970-01-01 08:00:00.020,1]
[+I,0,1970-01-01 08:00:00.025,0,online,1970-01-01 08:00:00.020,1]

*why?*
we notice that the query with filter is explained as below:

{code:xml}
== Abstract Syntax Tree ==
LogicalFilter(condition=[=($3, _UTF-16LE'online')])
+- LogicalProject(id=[$0], ts=[$1], id0=[$2], state=[$3], ts0=[$4], row_num=[$5])
   +- LogicalCorrelate(correlation=[$cor0], joinType=[left], requiredColumns=[{0, 1}])
      :- LogicalProject(id=[AS($0, _UTF-16LE'id')], ts=[AS($1, _UTF-16LE'ts')])
      :  +- LogicalWatermarkAssigner(rowtime=[f1], watermark=[-($1, 2000:INTERVAL SECOND)])
      :     +- LogicalTableScan(table=[[*anonymous_datastream_source$2*]])
      +- LogicalFilter(condition=[=($cor0.id, $0)])
         +- LogicalSnapshot(period=[$cor0.ts])
            +- LogicalProject(id=[$0], state=[$1], ts=[$2], row_num=[$3])
               +- LogicalFilter(condition=[=($3, 1)])
                  +- LogicalProject(id=[AS($0, _UTF-16LE'id')], state=[AS($1, _UTF-16LE'state')], ts=[AS($2, _UTF-16LE'ts')], row_num=[ROW_NUMBER() OVER (PARTITION BY AS($0, _UTF-16LE'id') ORDER BY AS($2, _UTF-16LE'ts') DESC NULLS LAST)])
                     +- LogicalWatermarkAssigner(rowtime=[f2], watermark=[-($2, 2000:INTERVAL SECOND)])
                        +- LogicalTableScan(table=[[*anonymous_datastream_source$1*]])

== Optimized Physical Plan ==
Calc(select=[id, ts, id0, CAST(_UTF-16LE'online':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" AS VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AS state, CAST(ts0 AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS ts0, CAST(row_num AS BIGINT) AS row_num])
+- TemporalJoin(joinType=[InnerJoin], where=[AND(=(id, id0), __TEMPORAL_JOIN_CONDITION(ts, ts0, __TEMPORAL_JOIN_CONDITION_PRIMARY_KEY(id0), __TEMPORAL_JOIN_LEFT_KEY(id), __TEMPORAL_JOIN_RIGHT_KEY(id0)))], select=[id, ts, id0, ts0, row_num])
   :- Exchange(distribution=[hash[id]])
   :  +- Calc(select=[f0 AS id, f1 AS ts])
   :     +- WatermarkAssigner(rowtime=[f1], watermark=[-(f1, 2000:INTERVAL SECOND)])
   :        +- TableSourceScan(table=[[*anonymous_datastream_source$2*]], fields=[f0, f1])
   +- Exchange(distribution=[hash[id]])
      +- Calc(select=[$0 AS id, $2 AS ts, 1:BIGINT AS row_num], where=[=($1, _UTF-16LE'online')])
         +- Deduplicate(keep=[LastRow], key=[$0], order=[ROWTIME])
            +- Exchange(distribution=[hash[$0]])
               +- Calc(select=[f0 AS $0, f1 AS $1, f2 AS $2])
                  +- WatermarkAssigner(rowtime=[f2], watermark=[-(f2, 2000:INTERVAL SECOND)])
                     +- TableSourceScan(table=[[*anonymous_datastream_source$1*]], fields=[f0, f1, f2])

== Optimized Execution Plan ==
Calc(select=[id, ts, id0, CAST('online' AS VARCHAR(2147483647)) AS state, CAST(ts0 AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS ts0, CAST(row_num AS BIGINT) AS row_num])
+- TemporalJoin(joinType=[InnerJoin], where=[((id = id0) AND __TEMPORAL_JOIN_CONDITION(ts, ts0, __TEMPORAL_JOIN_CONDITION_PRIMARY_KEY(id0), __TEMPORAL_JOIN_LEFT_KEY(id), __TEMPORAL_JOIN_RIGHT_KEY(id0)))], select=[id, ts, id0, ts0, row_num])
   :- Exchange(distribution=[hash[id]])
   :  +- Calc(select=[f0 AS id, f1 AS ts])
   :     +- WatermarkAssigner(rowtime=[f1], watermark=[(f1 - 2000:INTERVAL SECOND)])
   :        +- TableSourceScan(table=[[*anonymous_datastream_source$2*]], fields=[f0, f1])
   +- Exchange(distribution=[hash[id]])
      +- Calc(select=[$0 AS id, $2 AS ts, 1 AS row_num], where=[($1 = 'online')])
         +- Deduplicate(keep=[LastRow], key=[$0], order=[ROWTIME])
            +- Exchange(distribution=[hash[$0]])
               +- Calc(select=[f0 AS $0, f1 AS $1, f2 AS $2])
                  +- WatermarkAssigner(rowtime=[f2], watermark=[(f2 - 2000:INTERVAL SECOND)])
                     +- TableSourceScan(table=[[*anonymous_datastream_source$1*]], fields=[f0, f1, f2])
{code}

There is a Calc(select=[$0 AS id, $2 AS ts, 1 AS row_num], where=[($1 = 'online')]) between Exchange(distribution=[hash[id]]) and Deduplicate(keep=[LastRow], key=[$0], order=[ROWTIME]). According to [FLINK-9528|https://issues.apache.org/jira/browse/FLINK-9528] and [FLINK-16887|https://issues.apache.org/jira/browse/FLINK-16887], after the FlinkChangelogModeInferenceProgram, the UpdateKindTrait of Deduplicate(keep=[LastRow], key=[$0], order=[ROWTIME]) will be come [BEFORE_AND_AFTER], that the Deduplicate operator will produce message with UPDATE_BEFORE message. As a result, the sortedRightState will become:
[-U, 0, online, 1970-01-01 08:00:00.000]
#[-U, 0, offline, 1970-01-01 08:00:00.010], it is filtered
[+U, 0, online, 1970-01-01 08:00:00.020]

Finally, only the [+U, 0, online, 1970-01-01 08:00:00.020] is joined, that's exactly equal to the actual output.

Overall, the solution would be not to push the filters in above filters into right table when it's a temporal join.


;;;","05/Sep/22 04:59;csq;Hi, [~jark], could you please help evaluate the solution and review the pr? Thanks!;;;","05/Dec/22 12:32;godfrey;Fixed in 1.17.0:

b2203eaef68364306dfcc27fb34ac82baefda3d3

2851fac9c4c052876c80440b6b0b637603de06ea

 

1.16.1:

17b42516ceb73fa342101aedf830df40a84d82bc

c14355243995bff7b03a527ed073a2bbaab70ce8;;;","10/Jan/23 19:53;jingge;[~csq] [~xuannan] Is there a typo in the description? 

After filtering with predicate state = 'online', I expect only the -two- {color:#FF0000}four{color} rows with state offline will be filtered.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect async to sync lookup fallback path of LegacyTableSourceTable,FLINK-28987,13476981,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,16/Aug/22 07:38,19/Aug/22 14:25,13/Jul/23 08:13,19/Aug/22 14:25,1.16.0,,,,,,,1.16.0,,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"FLINK-28779 introduces a new lookup hint, for LegacyTableSourceTable interface, it defines only one lookup capability of either sync or async is supported by 'isAsyncEnabled', but we add a unnecessary fallback path to fetch both async and sync lookup in `LookupJoinUtil`, this should be fixed.
{code:java}
if (tableSource.isAsyncEnabled()) {
asyncLookupFunction = tableSource.getAsyncLookupFunction(lookupFieldNamesInOrder);
}
syncLookupFunction = tableSource.getLookupFunction(lookupFieldNamesInOrder);

{code}
Once this was fixed, it can completely avoid create user lookup function instance to determine if async lookup is enabled for execution, this can make the exec lookup join node immutable and easier to maintain.

Also, since the new hint adds the capability of configuring async params on join level (different joins in same query may has different params), so the description of lookup join transformation should carry async params and retry strategy (which take effect in runtime) for easier debugging",,godfreyhe,lincoln.86xy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 19 14:25:46 UTC 2022,,,,,,,,,,"0|z17sa0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/22 14:25;godfreyhe;Fixed in master:
d402fe255cdca37568ad6ac585ac6fbdf24b5e74
ef97c651f06dc835c36e8213687d66e78d80a0b6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UNNEST function with nested filter fails to generate plan,FLINK-28986,13476959,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,16/Aug/22 06:23,18/Aug/22 10:24,13/Jul/23 08:13,18/Aug/22 10:24,1.16.0,,,,,,,1.16.0,,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"h3. How to reproduce

add the following case to TableEnvironmentITCase
{code:scala}
@Test
def debug(): Unit = {
  tEnv.executeSql(
    s""""""
       |CREATE TEMPORARY TABLE source_kafka_wip_his_all (
       |  GUID varchar,
       |  OPERATION varchar,
       |  PRODUCTID varchar,
       |  LOTNO varchar,
       |  SERIALNO varchar,
       |  QUERYSERIALNO varchar,
       |  SERIALNO1 varchar,
       |  SERIALNO2 varchar,
       |  WIPORDERNO varchar,
       |  WIPORDERTYPE varchar,
       |  VIRTUALLOT varchar,
       |  PREOPERATION varchar,
       |  NORMALPREOPERATION varchar,
       |  PROCESSID varchar,
       |  EQUIPMENT varchar,
       |  INBOUNDDATE varchar,
       |  OUTBOUNDDATE varchar,
       |  REWORK varchar,
       |  REWORKPROCESSID varchar,
       |  CONTAINER varchar,
       |  WIPCONTENTCLASSID varchar,
       |  STATUSCODE varchar,
       |  WIPSTATUS varchar,
       |  TESTPROCESSID varchar,
       |  TESTORDERTYPE varchar,
       |  TESTORDER varchar,
       |  TEST varchar,
       |  SORTINGPROCESSID varchar,
       |  SORTINGORDERTYPE varchar,
       |  SORTINGORDER varchar,
       |  SORTING varchar,
       |  MINO varchar,
       |  GROUPCODE varchar,
       |  HIGHLOWGROUP varchar,
       |  PRODUCTNO varchar,
       |  FACILITY varchar,
       |  WIPLINE varchar,
       |  CHILDEQUCODE varchar,
       |  STATION varchar,
       |  QTY varchar,
       |  PASS_FLAG varchar,
       |  DEFECTCODELIST varchar,
       |  ISFIRST varchar,
       |  PARALIST ARRAY<ROW(GUID string,WIP_HIS_GUID string,QUERYSERIALNO string,OPERATION string,REWORKPROCESSID string,CHARACTERISTIC string,CHARACTERISTICREVISION string,CHARACTERISTICTYPE string,CHARACTERISTICCLASS string,UPPERCONTROLLIMIT string,TARGETVALUE string,LOWERCONTROLLIMIT string,TESTVALUE string,TESTATTRIBUTE string,TESTINGSTARTDATE string,TESTFINISHDATE string,UOMCODE string,DEFECTCODE string,SPECPARAMID string,STATION string,GP_TIME string,REFERENCEID string,LASTUPDATEON string,LASTUPDATEDBY string,CREATEDON string,CREATEDBY string,ACTIVE string,LASTDELETEON string,LASTDELETEDBY string,LASTREACTIVATEON string,LASTREACTIVATEDBY string,ARCHIVEID string,LASTARCHIVEON string,LASTARCHIVEDBY string,LASTRESTOREON string,LASTRESTOREDBY string,ROWVERSIONSTAMP string)>,
       |  REFERENCEID varchar,
       |  LASTUPDATEON varchar,
       |  LASTUPDATEDBY varchar,
       |  CREATEDON varchar,
       |  CREATEDBY varchar,
       |  ACTIVE varchar,
       |  LASTDELETEON varchar,
       |  LASTDELETEDBY varchar,
       |  LASTREACTIVATEON varchar,
       |  LASTREACTIVATEDBY varchar,
       |  ARCHIVEID varchar,
       |  LASTARCHIVEON varchar,
       |  LASTARCHIVEDBY varchar,
       |  LASTRESTOREON varchar,
       |  LASTRESTOREDBY varchar,
       |  ROWVERSIONSTAMP varchar,
       |  proctime as PROCTIME()
       |  ) with (
       |  'connector' = 'datagen'
       |)
       |"""""".stripMargin)

  tEnv.executeSql(
    s""""""
       |create TEMPORARY view transform_main_data as
       |select
       |      r.GUID as wip_his_guid,
       |      r.EQUIPMENT as equipment,
       |      r.WIPLINE as wipline,
       |      r.STATION as station,
       |      cast(r.PROCESSID as decimal) as processid,
       |      r.PRODUCTNO as productno,
       |      t.TESTFINISHDATE as testfinishdate,
       |      t.OPERATION as operation,
       |      t.CHARACTERISTIC as characteristic,
       |      t.LOWERCONTROLLIMIT as lowercontrollimit,
       |      t.UPPERCONTROLLIMIT as uppercontrollimit,
       |      t.TARGETVALUE as targetvalue,
       |      t.DEFECTCODE as defectcode,
       |      t.TESTVALUE as testvalue,
       |      t.CHARACTERISTICTYPE as characteristictype,
       |      proctime
       |  from
       |  (select
       |      GUID,
       |      EQUIPMENT,
       |      WIPLINE,
       |      STATION,
       |      PROCESSID,
       |      PRODUCTNO,
       |      PARALIST,
       |      proctime
       |  FROM source_kafka_wip_his_all) r
       |  cross join
       |  unnest(PARALIST) as t (GUID,WIP_HIS_GUID,QUERYSERIALNO,OPERATION,REWORKPROCESSID,CHARACTERISTIC,CHARACTERISTICREVISION,CHARACTERISTICTYPE,CHARACTERISTICCLASS,UPPERCONTROLLIMIT,TARGETVALUE,LOWERCONTROLLIMIT,TESTVALUE,TESTATTRIBUTE,TESTINGSTARTDATE,TESTFINISHDATE,UOMCODE,DEFECTCODE,SPECPARAMID,STATION,GP_TIME,REFERENCEID,LASTUPDATEON,LASTUPDATEDBY,CREATEDON,CREATEDBY,ACTIVE,LASTDELETEON,LASTDELETEDBY,LASTREACTIVATEON,LASTREACTIVATEDBY,ARCHIVEID,LASTARCHIVEON,LASTARCHIVEDBY,LASTRESTOREON,LASTRESTOREDBY,ROWVERSIONSTAMP)
       |  where t.CHARACTERISTICTYPE = '2'
       |"""""".stripMargin)

  tEnv.executeSql(
    s""""""
       |explain plan for
       |select * from transform_main_data
       |where operation not in ('G1208','G1209','G1211','G1213','G1206','G1207','G1214','G1215','G1282','G1292','G1216')
       |"""""".stripMargin).print()
} {code}
Stacktrace
{code:java}
org.apache.flink.table.api.TableException: Cannot generate a valid execution plan for the given query: LogicalProject(inputs=[0..3], exprs=[[CAST($4):DECIMAL(10, 0), $5, $23, $11, $13, $19, $17, $18, $25, $20, $15, $7]])
+- LogicalCorrelate(correlation=[$cor1], joinType=[inner], requiredColumns=[{6}])
   :- LogicalProject(inputs=[0], exprs=[[$14, $36, $38, $13, $34, $43, PROCTIME()]])
   :  +- LogicalTableScan(table=[[default_catalog, default_database, source_kafka_wip_his_all]])
   +- LogicalFilter(condition=[AND(SEARCH($7, Sarg[_UTF-16LE'2':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""]:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), SEARCH($3, Sarg[(-∞.._UTF-16LE'G1206'), (_UTF-16LE'G1206'.._UTF-16LE'G1207'), (_UTF-16LE'G1207'.._UTF-16LE'G1208'), (_UTF-16LE'G1208'.._UTF-16LE'G1209'), (_UTF-16LE'G1209'.._UTF-16LE'G1211'), (_UTF-16LE'G1211'.._UTF-16LE'G1213'), (_UTF-16LE'G1213'.._UTF-16LE'G1214'), (_UTF-16LE'G1214'.._UTF-16LE'G1215'), (_UTF-16LE'G1215'.._UTF-16LE'G1216'), (_UTF-16LE'G1216'.._UTF-16LE'G1282'), (_UTF-16LE'G1282'.._UTF-16LE'G1292'), (_UTF-16LE'G1292'..+∞)]:CHAR(5) CHARACTER SET ""UTF-16LE""))])
      +- Uncollect
         +- LogicalProject(exprs=[[$cor1.PARALIST]])
            +- LogicalValues(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]])This exception indicates that the query uses an unsupported SQL feature.
Please check the documentation for the set of currently supported SQL features.
org.apache.flink.table.api.TableException: Cannot generate a valid execution plan for the given query: 

LogicalProject(inputs=[0..3], exprs=[[CAST($4):DECIMAL(10, 0), $5, $23, $11, $13, $19, $17, $18, $25, $20, $15, $7]])
+- LogicalCorrelate(correlation=[$cor1], joinType=[inner], requiredColumns=[{6}])
   :- LogicalProject(inputs=[0], exprs=[[$14, $36, $38, $13, $34, $43, PROCTIME()]])
   :  +- LogicalTableScan(table=[[default_catalog, default_database, source_kafka_wip_his_all]])
   +- LogicalFilter(condition=[AND(SEARCH($7, Sarg[_UTF-16LE'2':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""]:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), SEARCH($3, Sarg[(-∞.._UTF-16LE'G1206'), (_UTF-16LE'G1206'.._UTF-16LE'G1207'), (_UTF-16LE'G1207'.._UTF-16LE'G1208'), (_UTF-16LE'G1208'.._UTF-16LE'G1209'), (_UTF-16LE'G1209'.._UTF-16LE'G1211'), (_UTF-16LE'G1211'.._UTF-16LE'G1213'), (_UTF-16LE'G1213'.._UTF-16LE'G1214'), (_UTF-16LE'G1214'.._UTF-16LE'G1215'), (_UTF-16LE'G1215'.._UTF-16LE'G1216'), (_UTF-16LE'G1216'.._UTF-16LE'G1282'), (_UTF-16LE'G1282'.._UTF-16LE'G1292'), (_UTF-16LE'G1292'..+∞)]:CHAR(5) CHARACTER SET ""UTF-16LE""))])
      +- Uncollect
         +- LogicalProject(exprs=[[$cor1.PARALIST]])
            +- LogicalValues(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]])

This exception indicates that the query uses an unsupported SQL feature.
Please check the documentation for the set of currently supported SQL features.

    at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:70)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)
    at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
    at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
    at scala.collection.Iterator.foreach(Iterator.scala:937)
    at scala.collection.Iterator.foreach$(Iterator.scala:937)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
    at scala.collection.IterableLike.foreach(IterableLike.scala:70)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
    at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:176)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:83)
    at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87)
    at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:315)
    at org.apache.flink.table.planner.delegation.PlannerBase.getExplainGraphs(PlannerBase.scala:527)
    at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:96)
    at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:51)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainInternal(TableEnvironmentImpl.java:695)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1356)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:733)
    at org.apache.flink.table.api.TableEnvironmentITCase.debug(TableEnvironmentITCase.scala:695)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runners.Suite.runChild(Suite.java:128)
    at org.junit.runners.Suite.runChild(Suite.java:27)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.RunRules.evaluate(RunRules.java:20)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: org.apache.calcite.plan.RelOptPlanner$CannotPlanException: There are not enough rules to produce a node with desired properties: convention=LOGICAL, FlinkRelDistributionTraitDef=any, MiniBatchIntervalTraitDef=None: 0, ModifyKindSetTraitDef=[NONE], UpdateKindTraitDef=[NONE].
Missing conversion is Uncollect[convention: NONE -> LOGICAL]
There is 1 empty subset: rel#485:RelSubset#4.LOGICAL.any.None: 0.[NONE].[NONE], the relevant part of the original plan is as follows
460:Uncollect
  458:LogicalProject(subset=[rel#459:RelSubset#3.NONE.any.None: 0.[NONE].[NONE]], PARALIST=[$cor1.PARALIST])
    17:LogicalValues(subset=[rel#457:RelSubset#2.NONE.any.None: 0.[NONE].[NONE]], tuples=[[{ 0 }]]){code}",,godfreyhe,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/22 06:36;qingyue;image-2022-08-16-14-36-07-061.png;https://issues.apache.org/jira/secure/attachment/13048154/image-2022-08-16-14-36-07-061.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 18 10:24:56 UTC 2022,,,,,,,,,,"0|z17s54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 06:36;qingyue;The reason is that during the decorrelation rewrite, the top filter rel node was pushed down to form a nested filter pattern. Since the filter merge rule is not defined in the default rewrite rule sets, the nested filter rel nodes rendered the LogicalUnnestRule unmatched. This can be fixed by adding  CoreRules.FILTER_MERGE to FlinkStreamRuleSets. cc [~godfrey] 

!image-2022-08-16-14-36-07-061.png|width=1047,height=177!;;;","16/Aug/22 06:44;godfreyhe;[~qingyue] Thanks for reporting this, assign to you ;;;","18/Aug/22 10:24;godfreyhe;Fixed in master: bdffb6bd4ae3ea32bdcd6ec6bce6c6e0e8b92a11;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kinesis connector doesn't work for new AWS regions,FLINK-28978,13476905,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liangtl,liangtl,liangtl,15/Aug/22 23:25,17/Aug/22 19:11,13/Jul/23 08:13,17/Aug/22 19:11,1.13.6,1.14.5,1.15.1,,,,,1.14.6,1.15.2,1.16.0,,,Connectors / Kinesis,,,,,,,0,pull-request-available,,,,"The current validation in the Kinesis connector checks that the AWS Region string specified is present in the {{Regions}} enum attached in the {{{}AWS SDK{}}}. This is not desirable because every time AWS launches a new region, we will have to update the AWS SDK shaded into the connector. 

We want to change it such that we validate the shape of the string, allowing for future AWS Regions. 

 

Current list of regions:

ap-south-1, eu-south-1, us-gov-east-1, ca-central-1, eu-central-1, us-west-1, us-west-2, af-south-1, eu-north-1, eu-west-3, eu-west-2, eu-west-1, ap-northeast-3, ap-northeast-2, ap-northeast-1, me-south-1, sa-east-1, ap-east-1, cn-north-1, us-gov-west-1, ap-southeast-1, ap-southeast-2, ap-southeast-3, us-iso-east-1, us-east-1, us-east-2, cn-northwest-1, us-isob-east-1, aws-global, aws-cn-global, aws-us-gov-global, aws-iso-global, aws-iso-b-global",,dannycranmer,liangtl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 17 19:11:03 UTC 2022,,,,,,,,,,"0|z17rt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 10:40;dannycranmer;Merged commit [{{c90d0d5}}|https://github.com/apache/flink/commit/c90d0d537e662c2c0db1ff497a87da6a55b03bc4] into apache:release-1.15 ;;;","17/Aug/22 19:11;dannycranmer;* Merged commit [{{05bd634}}|https://github.com/apache/flink/commit/05bd63499358b589f2954b1822e68f43b5158cc1] into apache:release-1.14
 * Merged commit [{{a42e737}}|https://github.com/apache/flink/commit/a42e7373f79017ee4fffd1af5d61e25999b4038b] into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in HybridSourceSplitEnumerator.close,FLINK-28977,13476899,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Benenson,Benenson,Benenson,15/Aug/22 21:03,30/Aug/22 07:50,13/Jul/23 08:13,23/Aug/22 12:29,1.14.4,1.15.1,,,,,,1.16.0,,,,,Connectors / Common,,,,,,,0,pull-request-available,pull-requests-available,,,"HybridSource pipeline has an intermittent error when reading from s3, usually this error is fixed when pipeline restarts after recovering from checkpoint. But intermittently happens:


2022/08/02 22:26:51.435 INFO  o.a.f.runtime.jobmaster.JobMaster - Trying to recover from a global failure.
org.apache.flink.util.FlinkException: Global failure triggered by OperatorCoordinator for 'Source: hybrid-source -> decrypt -> map2Events -> filterOutNulls -> assignTimestampsAndWatermarks -> logRawJson' (operator fd9fbc680ee884c4eafd0b9c2d3d007f).
at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder$LazyInitializedCoordinatorContext.failJob(OperatorCoordinatorHolder.java:545)
at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$DeferrableCoordinator.cleanAndFailJob(RecreateOnResetOperatorCoordinator.java:393)
...
Caused by: java.lang.NullPointerException: null
at org.apache.flink.connector.base.source.hybridspp.HybridSourceSplitEnumerator.close(HybridSourceSplitEnumerator.java:246)
at org.apache.flink.runtime.source.coordinator.SourceCoordinator.close(SourceCoordinator.java:151)
at org.apache.flink.runtime.operators.coordination.ComponentClosingUtils.lambda$closeAsyncWithTimeout$0(ComponentClosingUtils.java:70)
at java.lang.Thread.run(Thread.java:750)
 ",,Benenson,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 30 07:49:57 UTC 2022,,,,,,,,,,"0|z17rrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/22 21:52;Benenson;I have verified the fix, the problem is intermittent, but reproducible in my environment.
[~thw] could you, please, assign this Jira to me & review PR for the fix?
Thank you.;;;","30/Aug/22 07:49;martijnvisser;Fixed in master: ee4d27411b3bf1cfcc4171aab777eb8bf08f1550;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changelog 1st materialization delayed unneccesarily,FLINK-28976,13476881,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,15/Aug/22 17:27,29/Aug/22 20:09,13/Jul/23 08:13,26/Aug/22 11:59,1.15.1,1.16.0,,,,,,1.15.3,1.16.0,,,,Runtime / State Backends,,,,,,,0,pull-request-available,,,,"In PeriodicMaterializationManager.start(), the 1st materialization is scheduled with a delay: materialization_interval + random_offset 

Here, random_offset is added to avoid thundering herd problem.
The next materialization will be scheduled with a delay of only materialization_interval.

That means that the 1st materialization will have to compact up to 2 times more state changes than the subsequent ones. 

Which in turn can cause FLINK--26590 or other problems.",,roman,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25842,,,,,,,,,,,,,,,,,FLINK-26590,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 26 11:59:53 UTC 2022,,,,,,,,,,"0|z17rns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/22 11:59;roman;Merged into master as 91e1291e942afc69779f09ead549352d5d357f22 .. a38b852bbbdb812aa404c226717a2fa3bdd89665,
into release-1.15 as 493a1aa8556038283e256efc5368bd319bd06d17 .. 258c3e35265bb3a966bd317340f2a5fe7cfd7364.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
withIdleness marks all streams from FLIP-27 sources as idle,FLINK-28975,13476849,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,renqs,danderson,danderson,15/Aug/22 14:35,04/Nov/22 06:59,13/Jul/23 08:13,27/Sep/22 11:20,1.15.1,,,,,,,1.15.3,1.16.0,,,,API / DataStream,,,,,,,0,pull-request-available,,,,"Using withIdleness with a FLIP-27 source leads to all of the streams from the source being marked idle, which in turn leads to incorrect results, e.g., from joins that rely on watermarks.

Quoting from the user ML thread:

In org.apache.flink.streaming.api.operators.SourceOperator, there are separate instances of WatermarksWithIdleness created for each split output and the main output. There is multiplexing of watermarks between split outputs but no multiplexing between split output and main output.
 
For a source such as org.apache.flink.connector.kafka.source.KafkaSource, {color:#353833}there is only output from splits and no output from main. Hence the main output will (after an initial timeout) be marked as idle.{color}
{color:#353833} {color}
{color:#353833}The implementation of {color}WatermarksWithIdleness is such that once an output is idle, it will periodically re-mark the output as idle. Since there is no multiplexing between split outputs and main output, the idle marks coming from main output will repeatedly set the output to idle even though there are events from the splits. Result is that the entire source is repeatedly marked as idle.


See this ML thread for more details: [https://lists.apache.org/thread/bbokccohs16tzkdtybqtv1vx76gqkqj4]

This probably affects older versions of Flink as well, but that needs to be verified.",,danderson,godfreyhe,knaufk,leeys.1,martijnvisser,mason6345,pnowojski,qinjunjerry,renqs,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29048,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 13 03:24:28 UTC 2022,,,,,,,,,,"0|z17rgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 07:47;godfreyhe;[~renqs] could you have a look ?;;;","30/Aug/22 12:12;pnowojski;Why are we re-emitting idleness periodically? ;;;","31/Aug/22 09:46;renqs;[~pnowojski] There are two outputs in the source: per-split output and main output, and each of them has a copy of {{{}WatermarkGenerator{}}}:

[https://github.com/apache/flink/blob/c0f080762e7a1c1763942fed2e34420164a04bf3/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/source/ProgressiveTimestampsAndWatermarks.java#L102-L120]

{{WatermarksWithIdleness}} (which is an impl of {{{}WatermarkGenerator{}}}) periodically checks if there's any invocations of {{{}onEvent{}}}, and mark the watermark output as idle if the {{onEvent}} is never called with in the idleness timeout. For the source only uses per-split output instead of main output (like KafkaSource), the {{WatermarksWithIdleness}} on the main output is never touched so it keeps marking the watermark output as idle in {{{}onPeriodicEmit{}}}.

I'm not sure adding the main output to the watermark multiplexer in per-split output is a correct way to fix. From the current logic of {{ProgressiveTimestampsAndWatermarks}} the watermark from the main output should be directly sent to downstream instead of calculating min with watermarks from per-split output.

A possible solution in my mind is like we add another layer on WatermarkOutput of source that only mark idle if both main and per-split output are idle. WDYT?;;;","31/Aug/22 12:05;pnowojski;Thanks for the explanation
{quote}WatermarksWithIdleness (which is an impl of WatermarkGenerator) periodically checks if there's any invocations of onEvent, and mark the watermark output as idle if the onEvent is never called with in the idleness timeout.{quote}

Wouldn't it be more correct to not call {{output.markIdle();}} if previous {{onPeriodicEmit}} call has already called (i.e {{idlenessTimer.checkIfIdle()}} returned true in the previous call)?

{quote}A possible solution in my mind is like we add another layer on WatermarkOutput of source that only mark idle if both main and per-split output are idle. WDYT?{quote}
I think that sounds about right.

;;;","13/Sep/22 03:24;renqs;Fixed on master: d62df6899a1d7bd92ae998d3b610aa8b4de64505

release-1.16: a5be641a9a95de631dc697f3d2906e46cb126ee5

release-1.15: 2a99acde6cd9508da702474b69e3812805d17615;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Hive multi-version support for Table Store,FLINK-28967,13476775,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,15/Aug/22 08:10,16/Aug/22 06:09,13/Jul/23 08:13,16/Aug/22 06:09,table-store-0.2.0,table-store-0.3.0,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,Currently table store Hive catalog and connector can only work for Hive 2.3. Support for Hive 2.1 and 2.2 should be fixed.,,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 16 06:09:05 UTC 2022,,,,,,,,,,"0|z17r0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 06:09;lzljs3620320;master: 6652905c4c0ff7f2533353c2d7d3d6552eb978e3
release-0.2: cb5a24e4fc804df4ea746182ea28d03e1866971c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shouldn't create empty partition when it's for dynamic partition,FLINK-28965,13476759,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tartarus,luoyuxia,luoyuxia,15/Aug/22 06:24,19/Aug/22 04:06,13/Jul/23 08:13,19/Aug/22 04:06,1.16.0,,,,,,,1.16.0,,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,"Can be reproduced by the following code in HiveDialectTest:

 
{code:java}
tableEnv.executeSql(
        ""create table over1k_part_orc(\n""
                + ""           si smallint,\n""
                + ""           i int,\n""
                + ""           b bigint,\n""
                + ""           f float)\n""
                + ""       partitioned by (ds string, t tinyint) stored as orc"");
tableEnv.executeSql(
        ""create table over1k(\n""
                + ""           t tinyint,\n""
                + ""           si smallint,\n""
                + ""           i int,\n""
                + ""           b bigint,\n""
                + ""           f float,\n""
                + ""           d double,\n""
                + ""           bo boolean,\n""
                + ""           s string,\n""
                + ""           ts timestamp,\n""
                + ""           dec decimal(4,2),\n""
                + ""           bin binary)"");
tableEnv.executeSql(
                ""insert overwrite table over1k_part_orc partition(ds=\""foo\"", t)""
                        + "" select si,i,b,f,t from over1k where t is null or t=27 order by si"")
        .await(); {code}
 

Althogh it's for dynamic partition, the current code will try to create a partition for it (ds='foo') when there's no data wrotten to it , so the exception will be thrown since the partition spec (ds='foo') is not full path
{code:java}
Caused by: MetaException(message:Invalid partition key & values; keys [ds, t, ], values [foo, ]) {code}
 ",,jark,luoyuxia,tartarus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 19 04:06:16 UTC 2022,,,,,,,,,,"0|z17qww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/22 06:29;luoyuxia;[~tartarus] Could you please have a look?;;;","15/Aug/22 16:47;tartarus;[~luoyuxia]   thank for you raise this issue, I will fix it soon!;;;","19/Aug/22 04:06;jark;Fixed in master: e967a7b54da969c6d974a4ec6e3afcd1cc17a8be;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar throws java.lang.NoClassDefFoundError: javax/xml/bind/annotation/XmlElement,FLINK-28960,13476746,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,syhily,syhily,syhily,15/Aug/22 03:49,18/Oct/22 13:16,13/Jul/23 08:13,18/Oct/22 13:16,1.14.6,1.15.1,,,,,,1.14.7,1.15.3,1.16.1,1.17.0,,Connectors / Pulsar,,,,,,,0,pull-request-available,,,,"{code:java}
Unknown HK2 failure detected:
MultiException stack 1 of 2
java.lang.NoClassDefFoundError: javax/xml/bind/annotation/XmlElement
	at org.apache.pulsar.shade.com.fasterxml.jackson.module.jaxb.JaxbAnnotationIntrospector.<init>(JaxbAnnotationIntrospector.java:137)
	at org.apache.pulsar.shade.com.fasterxml.jackson.module.jaxb.JaxbAnnotationIntrospector.<init>(JaxbAnnotationIntrospector.java:124)
	at org.apache.pulsar.shade.com.fasterxml.jackson.module.jaxb.JaxbAnnotationIntrospector.<init>(JaxbAnnotationIntrospector.java:116)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at java.base/java.lang.Class.newInstance(Class.java:584)
{code}
",,syhily,tison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 18 13:16:11 UTC 2022,,,,,,,,,,"0|z17qu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/22 13:16;tison;master via 40cfdda7895e9c70c37124bd9e1314d015fc1b78
1.16 via cbda786b5afb54f19c29ba2b4c95f4d80c3b35c3
1.15 via 962b6e041deb1da42090c3b3c22def744675fa8e
1.14 via 32326b6fb3053f5b0b57a33d69dfd19cae05ea8a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
504 gateway timeout when consume large number of topics using TopicPatten,FLINK-28959,13476744,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,syhily,syhily,15/Aug/22 03:46,16/Feb/23 10:15,13/Jul/23 08:13,16/Feb/23 09:46,pulsar-4.0.0,,,,,,,pulsar-4.0.0,,,,,Connectors / Pulsar,,,,,,,0,pull-request-available,,,,"Our situation is as follows:

* In a single namespace, more than 300 topics(partitioned-topic with a single partition) will report this error;
* Error still exists after resource expansion
* A flink client program consumes 30 to 50 topics per program. This error is bound to be reported after five consecutive programs","* flink-connector-pulsar: 1.15.0
* Pulsar 2.9.2",martijnvisser,syhily,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 10:15:26 UTC 2023,,,,,,,,,,"0|z17qtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 09:45;Weijie Guo;main(4.0) via 64bc829c17b8686d35f66b274cb241cca51c7c1c;;;","16/Feb/23 09:49;Weijie Guo;[~martijnvisser] Sorry to bother you, but I want to ask whether we should using the flink-version or the connector-version for the affected version and the fixed version of the externalized connector in jira? I guess we should use the version of connector(i.e. 4.0) .;;;","16/Feb/23 10:05;martijnvisser;[~Weijie Guo] If it's an externalized connector, it should be the version of the connector indeed :);;;","16/Feb/23 10:15;Weijie Guo;Got it, thanks for your explanation~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FIx non-multi insert statement fall into multi insert logic  in Hive dialect,FLINK-28956,13476736,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,luoyuxia,luoyuxia,luoyuxia,15/Aug/22 03:04,19/Aug/22 04:09,13/Jul/23 08:13,19/Aug/22 04:09,1.16.0,,,,,,,1.16.0,,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,"In [FLINK-27387|https://issues.apache.org/jira/browse/FLINK-27387], we support multi insert statment in Hive dialect,  but the check for  multi insert statment  is not strict which will result non-multi insert statement fall into such logic.

For example, 

 
{code:java}
with cte as (select t.a as a,t.a as b,t.a as c from t where t.b is null) select * from cte {code}
 

. The AST for it is [TOK_CTE, TOK_FROM, TOK_INSERT], then it 

will be mistaken as multi-insert statement

 
{code:java}
private boolean isMultiDestQuery(HiveParserASTNode astNode) {
    // Hive's multi dest insert will always be [FROM, INSERT+]
    // so, if it's children count is more than 2, it should be a multi-dest query
    return astNode.getChildCount() > 2;
} {code}
 ",,jark,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 19 04:09:22 UTC 2022,,,,,,,,,,"0|z17qrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/22 03:11;luoyuxia;To fix it, we need a more strict check.;;;","19/Aug/22 04:09;jark;Fixed in master: 682ec6a5d73222d5c88a86e9b4caa29ee47e0bc7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Header in janino generated java files can merge with line numbers,FLINK-28951,13476726,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,14/Aug/22 22:44,23/Aug/22 12:39,13/Jul/23 08:13,18/Aug/22 10:01,,,,,,,,1.15.3,1.16.0,,,,Table SQL / API,,,,,,,0,pull-request-available,,,,"Since Line numbers are generated only for debug output it should not be a big issue.
From the other side currently this behavior leads to not compiled code.
The suggestion is usage of one-line comments for header to prevent this",,Sergey Nuyanzin,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 18 10:01:07 UTC 2022,,,,,,,,,,"0|z17qpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/22 10:01;twalthr;Fixed in master: 22f574fc660e045cfde66be4e4e58bb5781a9ed7

Fixed in 1.15: 29ff78a898c476202c5c4d1d153e3aa3ac0b4855;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock may occurs when releasing readers for SortMergeResultPartition,FLINK-28942,13476490,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,12/Aug/22 04:26,12/Aug/22 09:53,13/Jul/23 08:13,12/Aug/22 09:52,1.16.0,,,,,,,1.16.0,,,,,,,,,,,,0,pull-request-available,,,,"After adding the logic of recycling buffers in CompositeBuffer in https://issues.apache.org/jira/browse/FLINK-28373, when reading data and recycling buffers simultaneously, the deadlock between the lock of SortMergeResultPartition and the lock of SingleInputGate may occur.

In short, the deadlock may occur as follows.

1. SingleInputGate.getNextBufferOrEvent (SingleInputGate lock)

CompositeBuffer.getFullBufferData -> CompositeBuffer.recycleBuffer -> waiting for 

SortMergeResultPartition lock;

2. ResultPartitionManager.releasePartition (SortMergeResultPartition lock) -> 

SortMergeSubpartitionReader.notifyDataAvailable -> 

SingleInputGate.notifyChannelNonEmpty -> waiting for SingleInputGate lock.

The possibility of this deadlock is very small, but we should fix the bug as soon as possible.",,kevin.cyj,tanyuxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28373,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 12 09:52:33 UTC 2022,,,,,,,,,,"0|z17p94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/22 09:52;kevin.cyj;Merged into master via f2fb6b20ec493a3af3f19a6f69f25e26ed226dda;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Savepoint ignores MaxConcurrentCheckpoint limit in aligned checkpoint case,FLINK-28941,13476483,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,12/Aug/22 03:29,07/Sep/22 05:59,13/Jul/23 08:13,06/Sep/22 14:02,1.16.0,,,,,,,1.16.0,,,,,Runtime / Checkpointing,,,,,,,0,pull-request-available,test-stability,,,"When the unaligned checkpoint is disabled, savepoints would be set as forced[1], which means they can ignore the maxConcurrentCheckpoint limit[2] and lead to the situation that there are more than maxConcurrentCheckpoint running simultaneously. 

This behavior is incompatible with OperatorCoordinatorHolder, which requires that there should be at most one pending checkpoint at a time. As a result, exceptions, as follows, might be thrown[3].


{code:java}
java.lang.IllegalStateException: Cannot mark for checkpoint 9, already marked for checkpoint 8
	at org.apache.flink.runtime.operators.coordination.SubtaskGatewayImpl.markForCheckpoint(SubtaskGatewayImpl.java:185) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.lambda$checkpointCoordinatorInternal$6(OperatorCoordinatorHolder.java:328) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at java.util.HashMap.forEach(HashMap.java:1289) ~[?:1.8.0_292]
	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.checkpointCoordinatorInternal(OperatorCoordinatorHolder.java:327) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.lambda$checkpointCoordinator$0(OperatorCoordinatorHolder.java:243) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:453) ~[classes/:?]
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:453) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:218) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[classes/:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [scala-library-2.12.7.jar:?]
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [scala-library-2.12.7.jar:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.12.7.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [scala-library-2.12.7.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [scala-library-2.12.7.jar:?]
	at akka.actor.Actor.aroundReceive(Actor.scala:537) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at akka.actor.ActorCell.invoke(ActorCell.scala:548) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
{code}


[1] https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointRequestDecider.java#L160-L164
[2] https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L444-L449
[3] https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39860&view=logs&j=219f6d90-20a2-5863-7c1b-c80377a1018f&t=20186858-1485-5059-c9c6-446952519524&s=ab6e269b-88b2-5ded-2544-4aa5b1124530",,freeke,gaoyunhaii,godfrey,hxb,hxbks2ks,maguowei,Ming Li,yunfengzhou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28606,,,FLINK-29217,,,,FLINK-28999,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 06 06:33:01 UTC 2022,,,,,,,,,,"0|z17p7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 11:06;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40044&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203
It is the failed case in connector test.;;;","16/Aug/22 11:07;hxbks2ks;Hi [~gaoyunhaii] , could you help take a look? Thx;;;","17/Aug/22 07:42;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40093&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37273;;;","25/Aug/22 03:00;hxb;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40340&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203]

 ;;;","25/Aug/22 03:41;gaoyunhaii;[~hxb] I'll have a look~;;;","30/Aug/22 07:53;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40505&view=logs&j=9c5a5fe6-2f39-545e-1630-feb3d8d0a1ba&t=99b23320-1d05-5741-d63f-9e78473da39e;;;","01/Sep/22 01:50;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40556&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","01/Sep/22 01:51;hxb;Hi [~gaoyunhaii] Is there any progress on this issue? ;;;","01/Sep/22 03:00;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40563&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=17015;;;","01/Sep/22 06:10;gaoyunhaii;Hi [~hxb] Sorry for the delay, I'm still confirming the issue, and I'll try to have it fixed inside this week. ;;;","02/Sep/22 08:46;hxb;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40616&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=5846934b-7a4f-545b-e5b0-eb4d8bda32e1];;;","02/Sep/22 09:05;gaoyunhaii;[~hxb] I have a double check with [~yunfengzhou] offline, that the modified exactly-once mechanism of the operator coordinator now relied on that no concurrent checkpoints, which is different from the previous implementation that only requires that no concurrent checkpoints in the trigger period. However, currently we could not ensure no concurrent checkpoints, since maxConcurrentCheckpoints is an open option to the users, even if we could have different thoughts for the forced checkpoints, it could not solve the issue if users have set maxConcurrentCheckpoints explicitly. Thus I think we may need to try to remove the dependency on the assumption that there is no concurrent checkpoints. ;;;","02/Sep/22 09:12;hxb;Thanks for the investigations [~gaoyunhaii] . +1 for removing the assumption that there is no concurrent checkpoints.;;;","06/Sep/22 06:33;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40713&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix HiveServer2 Endpoint can not set variable correctly,FLINK-28938,13476477,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,luoyuxia,fsk119,fsk119,12/Aug/22 02:58,31/Aug/22 11:26,13/Jul/23 08:13,31/Aug/22 11:26,1.16.0,,,,,,,1.16.0,,,,,Connectors / Hive,Table SQL / Gateway,,,,,,0,pull-request-available,,,,"Hive JDBC URL also supports Hive variable replacement. But the current implementation doesn't finish this.  

 

HiveServer2 Endoint should thorw exception to notify users if users tries to fetch logs.",,fsk119,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 31 11:26:39 UTC 2022,,,,,,,,,,"0|z17p6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/22 11:26;fsk119;Merged into master:

549d4327cf4ae9646f74a1da561dcebecd3d47ff

6630ce7d6fcb118900a19bc0bb9143b3d388bf1a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix RSET endpoint can not serialize CHAR(0),FLINK-28936,13476472,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yzl,fsk119,fsk119,12/Aug/22 02:33,25/Aug/22 03:35,13/Jul/23 08:13,25/Aug/22 03:35,1.16.0,,,,,,,1.16.0,,,,,Table SQL / Gateway,,,,,,,0,pull-request-available,,,,The current implementation doesn't align with the FLIP. We need to introduce a new serializer to fix this.,,fsk119,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 25 03:35:25 UTC 2022,,,,,,,,,,"0|z17p5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/22 03:35;fsk119;Merged into master: 64f11ee95499af94d9ca3c0ce86091ab9d0ba9fa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar Source put all the splits to only one parallelism when using Exclusive subscription,FLINK-28934,13476406,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,syhily,syhily,syhily,11/Aug/22 15:29,16/Sep/22 01:38,13/Jul/23 08:13,16/Sep/22 01:38,1.14.5,1.15.1,1.16.0,,,,,1.14.6,1.15.3,1.16.0,,,Connectors / Pulsar,,,,,,,1,bug,pull-request-available,stale-critical,test-stability," !image-2022-08-11-23-27-04-268.png|width=500px!

The image here shows if we start a Flink application with four parallelism and four splits. All the splits would be sent to the first added reader. This is because we don't assign splits by pre-divide splits according to the size of parallelism. The readers are added to the enumerator one by one in the first bootstrap.",,longtimer,syhily,tison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27388,FLINK-27611,FLINK-27400,FLINK-28084,,,,,,,,,,,,,,,,,,,"11/Aug/22 15:27;syhily;image-2022-08-11-23-27-04-268.png;https://issues.apache.org/jira/secure/attachment/13048023/image-2022-08-11-23-27-04-268.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 16 01:38:45 UTC 2022,,,,,,,,,,"0|z17oqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","16/Sep/22 01:38;tison;https://github.com/apache/flink/pull/20725;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the concurrency problem in hybrid shuffle,FLINK-28925,13476362,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,11/Aug/22 10:07,17/Aug/22 03:08,13/Jul/23 08:13,17/Aug/22 03:08,1.16.0,,,,,,,1.16.0,,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,"Through tpc-ds testing and code analysis, I found some thread unsafe problems in hybrid shuffle:
 # HsSubpartitionMemeoryDataManager#consumeBuffer should return a readOnlySlice buffer to downstream instead of original buffer: If the spilling thread is processing while  downstream task is consuming the same buffer, the amount of data written to the disk will be smaller than the actual value. To solve this, we should let the consuming thread and the spilling thread share the same data but not index.
 # HsSubpartitionMemoryDataManager#releaseSubpartitionBuffers should ignore the release decision if the buffer already removed from bufferIndexToContexts instead of throw an exception. It should be pointed out that although the actual release operation is synchronous, a double release can still happen. The reason is that non-global decisions do not need to be synchronized. In other words, the main task thread and the consumer thread may decide to release a buffer at the same time.",,godfreyhe,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 17 03:08:56 UTC 2022,,,,,,,,,,"0|z17ogw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 06:03;godfreyhe;[~Weijie Guo] would you like to fix it ?;;;","16/Aug/22 06:58;Weijie Guo;[~godfreyhe], Yes I will fix this.;;;","17/Aug/22 03:08;xtsong;master (1.16): 7ed817f2054a13c3e2754c37f7681d8fbdba4b41;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add sql test for adaptive hash join,FLINK-28917,13476308,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lsy,lsy,lsy,11/Aug/22 06:48,19/Aug/22 04:07,13/Jul/23 08:13,19/Aug/22 04:07,1.16.0,,,,,,,1.16.0,,,,,Table SQL / Runtime,,,,,,,0,pull-request-available,,,,,,jark,lsy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 19 04:07:31 UTC 2022,,,,,,,,,,"0|z17o4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/22 04:07;jark;Fixed in master: d9516238cefb03324b0468bd23fd8962166ee565;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add e2e test for create function using jar syntax,FLINK-28916,13476307,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lsy,lsy,lsy,11/Aug/22 06:47,17/Aug/22 03:54,13/Jul/23 08:13,15/Aug/22 07:28,1.16.0,,,,,,,1.16.0,,,,,Table SQL / API,,,,,,,0,pull-request-available,,,,Add e2e test for create function using jar syntax that using remote hdfs jar,,jark,lsy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29007,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 15 07:28:15 UTC 2022,,,,,,,,,,"0|z17o4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/22 07:28;jark;Fixed in master: 7661af066f5830f8dc7fae62b67df31ac7fc2e45;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Could not find any factories that implement,FLINK-28914,13476284,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,CoderDM,CoderDM,11/Aug/22 03:12,25/Oct/22 01:43,13/Jul/23 08:13,25/Oct/22 01:43,1.16.0,,,,,,,1.17.0,,,,,Table SQL / Gateway,,,,,,,0,,,,,"2022-08-11 11:09:53,135 ERROR org.apache.flink.table.gateway.SqlGateway                    [] - Failed to start the endpoints.

org.apache.flink.table.api.ValidationException: Could not find any factories that implement 'org.apache.flink.table.gateway.api.endpoint.SqlGatewayEndpointFactory' in the classpath.

-----------------------------------------------------

I packaged Flink-Master and tried to start sql-gateway, but some problems arise.

I found tow problem with Factory under resources of flink-sql-gateway module.

META-INF.services should not be a folder name, ti should be ... /META-INF/services/... 

The 

`` org.apache.flink.table.gateway.rest.SqlGatewayRestEndpointFactory ``  in the org.apache.flink.table.factories.Factory file should be 

``  org.apache.flink.table.gateway.rest.util.SqlGatewayRestEndpointFactory `` . ",,CoderDM,fsk119,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27773,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 24 07:24:43 UTC 2022,,,,,,,,,,"0|z17nzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 09:04;martijnvisser;[~fsk119] Is this something that's on your radar?;;;","24/Oct/22 02:07;fsk119;Yes. Thanks for the mention. I think it should be fixed during the [FLINK-27773|https://issues.apache.org/jira/browse/FLINK-27773];;;","24/Oct/22 07:24;martijnvisser;[~fsk119] Given that that ticket is already closed, should this ticket also be considered fixed?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix fail to open HiveCatalog when it's for hive3,FLINK-28913,13476277,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,luoyuxia,luoyuxia,luoyuxia,11/Aug/22 02:25,15/Aug/22 08:54,13/Jul/23 08:13,15/Aug/22 08:54,,,,,,,,1.16.0,,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,"When use HiveCatalog for hive3, it will throw such exception:
{code:java}
java.lang.NoClassDefFoundError: org/apache/calcite/plan/RelOptRule
        at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.<init>(HiveMetastoreClientWrapper.java:91)
        at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.<init>(HiveMetastoreClientWrapper.java:79)
        at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientFactory.create(HiveMetastoreClientFactory.java:32)
        at org.apache.flink.table.catalog.hive.HiveCatalog.open(HiveCatalog.java:306)
        at org.apache.flink.table.catalog.CatalogManager.registerCatalog(CatalogManager.java:211)
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.registerCatalog(TableEnvironmentImpl.java:382) {code}
The failure is introduced by [FLINK-26413|https://issues.apache.org/jira/browse/FLINK-26413], which introduces `Hive.get(hiveConf);` in method `HiveMetastoreClientFactory.create` to support Hive's ""load data inpath` syntax.

But the class `Hive` will import class 'org.apache.calcite.plan.RelOptRule', then when try to load the class `Hive`, it'll throw class not found exception since this class is not in class path.",,jark,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 15 08:54:16 UTC 2022,,,,,,,,,,"0|z17ny8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/22 03:00;luoyuxia;The workaround way is to swap `opt/flink-table-planner` and `lib/flink-table-planner-loader` as flink-table-planner contains calcite dependency.

But to fix it, I think we can lazy init the `Hive` class, only when we need to call method `loadTable` / `loadPartition`. 

I think it's fine for only in Hive dialect, do we need `Hive` class, and when user want to use Hive dialect, they need to swap `lib/flink-table-planner-loader` and `opt/flink-table-planner` so that the calcite will exist in class path.

 ;;;","15/Aug/22 08:54;jark;Fixed in master: cb4ead75d594e351dfe81af42e52db88faae356a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Coder for LIST type is incorrectly chosen is PyFlink,FLINK-28908,13476147,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,10/Aug/22 10:16,15/Aug/22 04:44,13/Jul/23 08:13,15/Aug/22 04:44,1.14.5,1.15.1,,,,,,1.14.6,1.15.2,1.16.0,,,API / Python,,,,,,,0,pull-request-available,,,,"Code to reproduce this bug, the result is `[None, None, None]`:
{code:python}
jvm = get_gateway().jvm
env = StreamExecutionEnvironment.get_execution_environment()
j_item = jvm.java.util.ArrayList()
j_item.add(1)
j_item.add(2)
j_item.add(3)
j_list = jvm.java.util.ArrayList()
j_list.add(j_item)
type_info = Types.LIST(Types.INT())
ds = DataStream(env._j_stream_execution_environment.fromCollection(j_list, type_info.get_java_type_info()))
ds.map(lambda e: print(e))
env.execute() {code}",,dianfu,Juntao Hu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 15 04:44:14 UTC 2022,,,,,,,,,,"0|z17n5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/22 04:44;dianfu;Fixed in:
- master via de8ff096a5344d85eb9be497902b99dd4b24e2a9 and 9b50ff584ecdb76256be8c35ee01b2ecd03d3dcb
- release-1.15 via 63c1c1b9f6ee9669305e081030768b43a267c0f5
- release-1.14 via f0545f42607655187b53381506e55d33beec4542;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-table-store-hive-catalog could not shade hive-shims-0.23,FLINK-28903,13476119,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,nicholasjiang,nicholasjiang,10/Aug/22 08:13,11/Aug/22 09:51,13/Jul/23 08:13,11/Aug/22 09:51,table-store-0.3.0,,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"flink-table-store-hive-catalog could not shade hive-shims-0.23 because artifactSet doesn't include hive-shims-0.23 and the hive-shims-0.23 isn't specifically included with filters. The exception is as follows for setting hive.metastore.use.SSL or hive.metastore.sasl.enabled is true:
{code:java}
￼Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.HiveMetaStoreClient
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1708) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:97) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalog.createClient(HiveCatalog.java:380) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalog.<init>(HiveCatalog.java:80) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalogFactory.create(HiveCatalogFactory.java:51) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.file.catalog.CatalogFactory.createCatalog(CatalogFactory.java:93) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:62) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:57) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:31) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.factories.FactoryUtil.createCatalog(FactoryUtil.java:428) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1356) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1111) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeOperation$3(LocalExecutor.java:209) ~[flink-sql-client-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:88) ~[flink-sql-client-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:209) ~[flink-sql-client-1.15.1.jar:1.15.1]
    ... 10 more
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_181]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_181]
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_181]
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_181]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1706) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:97) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalog.createClient(HiveCatalog.java:380) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalog.<init>(HiveCatalog.java:80) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalogFactory.create(HiveCatalogFactory.java:51) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.file.catalog.CatalogFactory.createCatalog(CatalogFactory.java:93) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:62) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:57) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:31) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.factories.FactoryUtil.createCatalog(FactoryUtil.java:428) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1356) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1111) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeOperation$3(LocalExecutor.java:209) ~[flink-sql-client-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:88) ~[flink-sql-client-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:209) ~[flink-sql-client-1.15.1.jar:1.15.1]
    ... 10 more
Caused by: java.lang.RuntimeException: Could not load shims in class org.apache.flink.table.store.shaded.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge23
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.shims.ShimLoader.createShim(ShimLoader.java:132) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.shims.ShimLoader.loadShims(ShimLoader.java:124) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.shims.ShimLoader.getHadoopThriftAuthBridge(ShimLoader.java:108) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:414) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:247) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_181]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_181]
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_181]
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_181]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1706) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:97) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalog.createClient(HiveCatalog.java:380) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalog.<init>(HiveCatalog.java:80) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalogFactory.create(HiveCatalogFactory.java:51) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.file.catalog.CatalogFactory.createCatalog(CatalogFactory.java:93) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:62) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:57) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:31) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.factories.FactoryUtil.createCatalog(FactoryUtil.java:428) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1356) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1111) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeOperation$3(LocalExecutor.java:209) ~[flink-sql-client-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:88) ~[flink-sql-client-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:209) ~[flink-sql-client-1.15.1.jar:1.15.1]
    ... 10 more
Caused by: java.lang.ClassNotFoundException: org.apache.flink.table.store.shaded.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge23
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[?:1.8.0_181]
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_181]
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) ~[?:1.8.0_181]
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_181]
    at java.lang.Class.forName0(Native Method) ~[?:1.8.0_181]
    at java.lang.Class.forName(Class.java:264) ~[?:1.8.0_181]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.shims.ShimLoader.createShim(ShimLoader.java:129) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.shims.ShimLoader.loadShims(ShimLoader.java:124) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.shims.ShimLoader.getHadoopThriftAuthBridge(ShimLoader.java:108) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:414) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:247) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_181]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_181]
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_181]
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_181]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1706) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:97) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalog.createClient(HiveCatalog.java:380) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalog.<init>(HiveCatalog.java:80) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalogFactory.create(HiveCatalogFactory.java:51) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.file.catalog.CatalogFactory.createCatalog(CatalogFactory.java:93) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:62) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:57) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:31) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.factories.FactoryUtil.createCatalog(FactoryUtil.java:428) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1356) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1111) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeOperation$3(LocalExecutor.java:209) ~[flink-sql-client-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:88) ~[flink-sql-client-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:209) ~[flink-sql-client-1.15.1.jar:1.15.1]
    ... 10 more {code}",,lzljs3620320,nicholasjiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 11 09:51:47 UTC 2022,,,,,,,,,,"0|z17mzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/22 04:43;lzljs3620320;We can change this to `support hive 2.1&2.2`;;;","11/Aug/22 07:00;nicholasjiang;[~lzljs3620320] , when the value of hive.metastore.use.SSL or hive.metastore.sasl.enabled is true, there is above exception for Hive 2.x including Hive 2.3.;;;","11/Aug/22 09:51;lzljs3620320;master: 3b6e602cd64223a7861d30c57054d57b9557cf94
release-0.2: 3da1a91444bcf48a0345fbc102e9294eedaa0359;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSystemJobResultStoreTestInternal is a unit test that's not following the unit test naming convention,FLINK-28902,13476105,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,tonyzhu,mapohl,mapohl,10/Aug/22 07:23,16/Sep/22 09:49,13/Jul/23 08:13,16/Sep/22 09:49,1.15.1,1.16.0,,,,,,1.17.0,,,,,Runtime / Coordination,Test Infrastructure,,,,,,0,pull-request-available,starter,,,"AzureCI still picks the test up as part of {{mvn verify}} (see an [example build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39780&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8539]).

Anyway, we should move the {{Internal}} suffix and move it somewhere inside of the test name.",,mapohl,tonyzhu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 16 09:49:45 UTC 2022,,,,,,,,,,"0|z17mwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/22 00:38;tonyzhu;Can I try this ticket?;;;","14/Sep/22 08:25;mapohl;Sure, thanks for offering your help. I assigned the ticket to you.;;;","15/Sep/22 05:37;tonyzhu;I created this PR [https://github.com/apache/flink/pull/20839]

Could you help review please;;;","16/Sep/22 03:07;tonyzhu;Thanks for review. I created another PR [#20845|https://github.com/apache/flink/pull/20845] .;;;","16/Sep/22 09:49;mapohl;master: 9d2ae5572897f3e2d9089414261a250cfc2a2ab8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RecreateOnResetOperatorCoordinatorTest compile failed in jdk11,FLINK-28900,13476093,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yunfengzhou,hxb,hxb,10/Aug/22 06:19,10/Aug/22 16:29,13/Jul/23 08:13,10/Aug/22 16:29,1.16.0,,,,,,,1.16.0,,,,,Runtime / Coordination,,,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-08-10T00:19:25.3221073Z [ERROR] COMPILATION ERROR : 
2022-08-10T00:19:25.3221634Z [INFO] -------------------------------------------------------------
2022-08-10T00:19:25.3222878Z [ERROR] /__w/1/s/flink-runtime/src/test/java/org/apache/flink/runtime/operators/coordination/RecreateOnResetOperatorCoordinatorTest.java:[241,58] method containsExactly in class org.assertj.core.api.AbstractIterableAssert<SELF,ACTUAL,ELEMENT,ELEMENT_ASSERT> cannot be applied to given types;
2022-08-10T00:19:25.3223786Z   required: capture#1 of ? extends java.lang.Integer[]
2022-08-10T00:19:25.3224245Z   found: int
2022-08-10T00:19:25.3224684Z   reason: varargs mismatch; int cannot be converted to capture#1 of ? extends java.lang.Integer
2022-08-10T00:19:25.3225128Z [INFO] 1 error
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39795&view=logs&j=946871de-358d-5815-3994-8175615bc253&t=e0240c62-4570-5d1c-51af-dd63d2093da1",,hxb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28606,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 10 06:21:57 UTC 2022,,,,,,,,,,"0|z17mts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/22 06:20;hxb;[~yunfengzhou] Could you help take a look?;;;","10/Aug/22 06:21;hxb;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39795&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=a7382ec4-87d2-5a9d-7c53-a2f93e317458]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39795&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=dffc2faa-5b48-5b4e-0797-dec1b1f74872]

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogRecoverySwitchStateBackendITCase.testSwitchFromEnablingToDisablingWithRescalingOut failed,FLINK-28898,13476077,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,hxb,hxb,10/Aug/22 03:49,11/Jan/23 10:34,13/Jul/23 08:13,11/Jan/23 10:27,1.16.0,1.17.0,,,,,,1.16.0,,,,,Runtime / Checkpointing,Runtime / State Backends,,,,,,0,pull-request-available,test-stability,,,"{code:java}
2022-08-10T02:48:19.5711924Z Aug 10 02:48:19 [ERROR] ChangelogRecoverySwitchStateBackendITCase.testSwitchFromEnablingToDisablingWithRescalingOut  Time elapsed: 6.064 s  <<< ERROR!
2022-08-10T02:48:19.5712815Z Aug 10 02:48:19 org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=0, backoffTimeMS=0)
2022-08-10T02:48:19.5714530Z Aug 10 02:48:19 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
2022-08-10T02:48:19.5716211Z Aug 10 02:48:19 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
2022-08-10T02:48:19.5717627Z Aug 10 02:48:19 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256)
2022-08-10T02:48:19.5718885Z Aug 10 02:48:19 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247)
2022-08-10T02:48:19.5720430Z Aug 10 02:48:19 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
2022-08-10T02:48:19.5721733Z Aug 10 02:48:19 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:738)
2022-08-10T02:48:19.5722680Z Aug 10 02:48:19 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:715)
2022-08-10T02:48:19.5723612Z Aug 10 02:48:19 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2022-08-10T02:48:19.5724389Z Aug 10 02:48:19 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:477)
2022-08-10T02:48:19.5725046Z Aug 10 02:48:19 	at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
2022-08-10T02:48:19.5725708Z Aug 10 02:48:19 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-10T02:48:19.5726374Z Aug 10 02:48:19 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-10T02:48:19.5727065Z Aug 10 02:48:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
2022-08-10T02:48:19.5727932Z Aug 10 02:48:19 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-08-10T02:48:19.5729087Z Aug 10 02:48:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
2022-08-10T02:48:19.5730134Z Aug 10 02:48:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
2022-08-10T02:48:19.5731536Z Aug 10 02:48:19 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2022-08-10T02:48:19.5732549Z Aug 10 02:48:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2022-08-10T02:48:19.5735018Z Aug 10 02:48:19 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-08-10T02:48:19.5735821Z Aug 10 02:48:19 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-08-10T02:48:19.5736465Z Aug 10 02:48:19 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-08-10T02:48:19.5737234Z Aug 10 02:48:19 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-08-10T02:48:19.5737895Z Aug 10 02:48:19 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-08-10T02:48:19.5738574Z Aug 10 02:48:19 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-08-10T02:48:19.5739276Z Aug 10 02:48:19 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-10T02:48:19.5740315Z Aug 10 02:48:19 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-10T02:48:19.5741211Z Aug 10 02:48:19 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-08-10T02:48:19.5741878Z Aug 10 02:48:19 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-08-10T02:48:19.5742497Z Aug 10 02:48:19 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-08-10T02:48:19.5743242Z Aug 10 02:48:19 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-08-10T02:48:19.5743977Z Aug 10 02:48:19 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-08-10T02:48:19.5744851Z Aug 10 02:48:19 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-08-10T02:48:19.5745438Z Aug 10 02:48:19 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-08-10T02:48:19.5746059Z Aug 10 02:48:19 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-08-10T02:48:19.5746885Z Aug 10 02:48:19 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-10T02:48:19.5747571Z Aug 10 02:48:19 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-10T02:48:19.5748236Z Aug 10 02:48:19 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-10T02:48:19.5749314Z Aug 10 02:48:19 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-08-10T02:48:19.5750188Z Aug 10 02:48:19 Caused by: java.io.IOException: Could not parse external pointer as state base path
2022-08-10T02:48:19.5751178Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase$DeserializationContext.createExclusiveDirPath(MetadataV2V3SerializerBase.java:875)
2022-08-10T02:48:19.5752359Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase$DeserializationContext.getExclusiveDirPath(MetadataV2V3SerializerBase.java:865)
2022-08-10T02:48:19.5753888Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.deserializeStreamStateHandle(MetadataV2V3SerializerBase.java:727)
2022-08-10T02:48:19.5754918Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.deserializeOperatorStateHandle(MetadataV2V3SerializerBase.java:630)
2022-08-10T02:48:19.5755929Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.deserializeSubtaskState(MetadataV2V3SerializerBase.java:279)
2022-08-10T02:48:19.5756887Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.metadata.MetadataV3Serializer.deserializeOperatorState(MetadataV3Serializer.java:184)
2022-08-10T02:48:19.5757815Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.deserializeMetadata(MetadataV2V3SerializerBase.java:182)
2022-08-10T02:48:19.5758890Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.metadata.MetadataV3Serializer.deserialize(MetadataV3Serializer.java:90)
2022-08-10T02:48:19.5759762Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.metadata.MetadataV4Serializer.deserialize(MetadataV4Serializer.java:49)
2022-08-10T02:48:19.5760584Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.Checkpoints.loadCheckpointMetadata(Checkpoints.java:118)
2022-08-10T02:48:19.5761414Z Aug 10 02:48:19 	at org.apache.flink.test.util.TestUtils.loadCheckpointMetadata(TestUtils.java:103)
2022-08-10T02:48:19.5762243Z Aug 10 02:48:19 	at org.apache.flink.test.checkpointing.ChangelogRecoveryITCaseBase.getAllStateHandleId(ChangelogRecoveryITCaseBase.java:233)
2022-08-10T02:48:19.5763361Z Aug 10 02:48:19 	at org.apache.flink.test.checkpointing.ChangelogRecoverySwitchEnvTestBase$2.lambda$beforeElement$cdc83b0a$1(ChangelogRecoverySwitchEnvTestBase.java:120)
2022-08-10T02:48:19.5764755Z Aug 10 02:48:19 	at org.apache.flink.test.checkpointing.ChangelogRecoveryITCaseBase$ControlledSource.waitWhile(ChangelogRecoveryITCaseBase.java:359)
2022-08-10T02:48:19.5765732Z Aug 10 02:48:19 	at org.apache.flink.test.checkpointing.ChangelogRecoverySwitchEnvTestBase$2.beforeElement(ChangelogRecoverySwitchEnvTestBase.java:117)
2022-08-10T02:48:19.5766685Z Aug 10 02:48:19 	at org.apache.flink.test.checkpointing.ChangelogRecoveryITCaseBase$ControlledSource.run(ChangelogRecoveryITCaseBase.java:342)
2022-08-10T02:48:19.5767503Z Aug 10 02:48:19 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
2022-08-10T02:48:19.5768234Z Aug 10 02:48:19 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
2022-08-10T02:48:19.5769041Z Aug 10 02:48:19 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
2022-08-10T02:48:19.5771138Z Aug 10 02:48:19 Caused by: java.io.FileNotFoundException: Cannot find meta data file '_metadata' in directory 'file:/tmp/junit8987764363790519654/junit1534389645319798457/0d8e7372aaeb10c57565dae19d6f6ed6/chk-4'. Please try to load the checkpoint/savepoint directly from the metadata file instead of the directory.
2022-08-10T02:48:19.5772448Z Aug 10 02:48:19 	at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorageAccess.resolveCheckpointPointer(AbstractFsCheckpointStorageAccess.java:290)
2022-08-10T02:48:19.5773761Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase$DeserializationContext.createExclusiveDirPath(MetadataV2V3SerializerBase.java:872)
2022-08-10T02:48:19.5774662Z Aug 10 02:48:19 	... 18 more {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39795&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba",,hxb,hxbks2ks,mapohl,roman,Yanfei Lei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28440,,,,,,,,,FLINK-30561,,FLINK-28766,FLINK-30107,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 11 10:27:26 UTC 2023,,,,,,,,,,"0|z17mq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/22 03:49;hxb;[~Yanfei Lei] Could you help take a look?;;;","10/Aug/22 16:08;roman;Merged as 3268ec6a7ce0e060eb401917f7d169969334d07d.;;;","16/Aug/22 08:01;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40021&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7 new instance, but the stack is a little different.

{code:java}
2022-08-16T02:33:54.7104899Z Aug 16 02:33:54 [ERROR] ChangelogRecoverySwitchStateBackendITCase.testSwitchFromEnablingToDisabling  Time elapsed: 1.777 s  <<< ERROR!
2022-08-16T02:33:54.7106371Z Aug 16 02:33:54 org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=0, backoffTimeMS=0)
2022-08-16T02:33:54.7108148Z Aug 16 02:33:54 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
2022-08-16T02:33:54.7109843Z Aug 16 02:33:54 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
2022-08-16T02:33:54.7111540Z Aug 16 02:33:54 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256)
2022-08-16T02:33:54.7112933Z Aug 16 02:33:54 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247)
2022-08-16T02:33:54.7114297Z Aug 16 02:33:54 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
2022-08-16T02:33:54.7115692Z Aug 16 02:33:54 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:738)
2022-08-16T02:33:54.7117495Z Aug 16 02:33:54 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:715)
2022-08-16T02:33:54.7118891Z Aug 16 02:33:54 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2022-08-16T02:33:54.7120184Z Aug 16 02:33:54 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:477)
2022-08-16T02:33:54.7121454Z Aug 16 02:33:54 	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
2022-08-16T02:33:54.7122591Z Aug 16 02:33:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-16T02:33:54.7123720Z Aug 16 02:33:54 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-16T02:33:54.7124919Z Aug 16 02:33:54 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
2022-08-16T02:33:54.7126454Z Aug 16 02:33:54 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-08-16T02:33:54.7127962Z Aug 16 02:33:54 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
2022-08-16T02:33:54.7129466Z Aug 16 02:33:54 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
2022-08-16T02:33:54.7130872Z Aug 16 02:33:54 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2022-08-16T02:33:54.7132172Z Aug 16 02:33:54 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2022-08-16T02:33:54.7133358Z Aug 16 02:33:54 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-08-16T02:33:54.7134409Z Aug 16 02:33:54 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-08-16T02:33:54.7135475Z Aug 16 02:33:54 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-08-16T02:33:54.7136530Z Aug 16 02:33:54 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-08-16T02:33:54.7137793Z Aug 16 02:33:54 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-08-16T02:33:54.7138882Z Aug 16 02:33:54 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-08-16T02:33:54.7139851Z Aug 16 02:33:54 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-16T02:33:54.7141054Z Aug 16 02:33:54 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-16T02:33:54.7142108Z Aug 16 02:33:54 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-08-16T02:33:54.7143090Z Aug 16 02:33:54 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-08-16T02:33:54.7144138Z Aug 16 02:33:54 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-08-16T02:33:54.7145216Z Aug 16 02:33:54 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-08-16T02:33:54.7146262Z Aug 16 02:33:54 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-08-16T02:33:54.7147460Z Aug 16 02:33:54 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-08-16T02:33:54.7148504Z Aug 16 02:33:54 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-08-16T02:33:54.7149483Z Aug 16 02:33:54 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-08-16T02:33:54.7150525Z Aug 16 02:33:54 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-16T02:33:54.7151788Z Aug 16 02:33:54 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-16T02:33:54.7152997Z Aug 16 02:33:54 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-16T02:33:54.7154220Z Aug 16 02:33:54 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-08-16T02:33:54.7155427Z Aug 16 02:33:54 Caused by: java.lang.Exception: Exception while creating StreamOperatorStateContext.
2022-08-16T02:33:54.7156924Z Aug 16 02:33:54 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:256)
2022-08-16T02:33:54.7158987Z Aug 16 02:33:54 	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268)
2022-08-16T02:33:54.7160411Z Aug 16 02:33:54 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
2022-08-16T02:33:54.7161891Z Aug 16 02:33:54 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:723)
2022-08-16T02:33:54.7163204Z Aug 16 02:33:54 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
2022-08-16T02:33:54.7164520Z Aug 16 02:33:54 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:699)
2022-08-16T02:33:54.7165749Z Aug 16 02:33:54 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:666)
2022-08-16T02:33:54.7166969Z Aug 16 02:33:54 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
2022-08-16T02:33:54.7168473Z Aug 16 02:33:54 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
2022-08-16T02:33:54.7169553Z Aug 16 02:33:54 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
2022-08-16T02:33:54.7170693Z Aug 16 02:33:54 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
2022-08-16T02:33:54.7171631Z Aug 16 02:33:54 	at java.lang.Thread.run(Thread.java:748)
2022-08-16T02:33:54.7172910Z Aug 16 02:33:54 Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for WindowOperator_08a489791a4e7fcd83ae029ef13928c6_(4/4) from any of the 1 provided restore options.
2022-08-16T02:33:54.7174517Z Aug 16 02:33:54 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:160)
2022-08-16T02:33:54.7175999Z Aug 16 02:33:54 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:353)
2022-08-16T02:33:54.7177669Z Aug 16 02:33:54 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:165)
2022-08-16T02:33:54.7178709Z Aug 16 02:33:54 	... 11 more
2022-08-16T02:33:54.7181176Z Aug 16 02:33:54 Caused by: java.lang.RuntimeException: java.io.FileNotFoundException: /tmp/junit5354320522396578132/junit5523751389985007360/8145786fe678df6770655845be5f9dc9/dstl/95c98d3b-a010-44e8-813c-3f5d0e4af9a0 (No such file or directory)
2022-08-16T02:33:54.7182986Z Aug 16 02:33:54 	at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321)
2022-08-16T02:33:54.7184444Z Aug 16 02:33:54 	at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:87)
2022-08-16T02:33:54.7186197Z Aug 16 02:33:54 	at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.hasNext(StateChangelogHandleStreamHandleReader.java:69)
2022-08-16T02:33:54.7188080Z Aug 16 02:33:54 	at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.readBackendHandle(ChangelogBackendRestoreOperation.java:107)
2022-08-16T02:33:54.7189740Z Aug 16 02:33:54 	at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.restore(ChangelogBackendRestoreOperation.java:78)
2022-08-16T02:33:54.7191428Z Aug 16 02:33:54 	at org.apache.flink.state.changelog.DeactivatedChangelogStateBackend.restore(DeactivatedChangelogStateBackend.java:70)
2022-08-16T02:33:54.7193056Z Aug 16 02:33:54 	at org.apache.flink.state.changelog.AbstractChangelogStateBackend.createKeyedStateBackend(AbstractChangelogStateBackend.java:136)
2022-08-16T02:33:54.7194769Z Aug 16 02:33:54 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:336)
2022-08-16T02:33:54.7196452Z Aug 16 02:33:54 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168)
2022-08-16T02:33:54.7198411Z Aug 16 02:33:54 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
2022-08-16T02:33:54.7199429Z Aug 16 02:33:54 	... 13 more
2022-08-16T02:33:54.7201525Z Aug 16 02:33:54 Caused by: java.io.FileNotFoundException: /tmp/junit5354320522396578132/junit5523751389985007360/8145786fe678df6770655845be5f9dc9/dstl/95c98d3b-a010-44e8-813c-3f5d0e4af9a0 (No such file or directory)
2022-08-16T02:33:54.7202933Z Aug 16 02:33:54 	at java.io.FileInputStream.open0(Native Method)
2022-08-16T02:33:54.7203887Z Aug 16 02:33:54 	at java.io.FileInputStream.open(FileInputStream.java:195)
2022-08-16T02:33:54.7204911Z Aug 16 02:33:54 	at java.io.FileInputStream.<init>(FileInputStream.java:138)
2022-08-16T02:33:54.7206087Z Aug 16 02:33:54 	at org.apache.flink.core.fs.local.LocalDataInputStream.<init>(LocalDataInputStream.java:50)
2022-08-16T02:33:54.7207582Z Aug 16 02:33:54 	at org.apache.flink.core.fs.local.LocalFileSystem.open(LocalFileSystem.java:134)
2022-08-16T02:33:54.7209065Z Aug 16 02:33:54 	at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.open(SafetyNetWrapperFileSystem.java:87)
2022-08-16T02:33:54.7210405Z Aug 16 02:33:54 	at org.apache.flink.runtime.state.filesystem.FileStateHandle.openInputStream(FileStateHandle.java:69)
2022-08-16T02:33:54.7212048Z Aug 16 02:33:54 	at org.apache.flink.changelog.fs.ChangelogStreamHandleReaderWithCache.openAndSeek(ChangelogStreamHandleReaderWithCache.java:89)
2022-08-16T02:33:54.7213573Z Aug 16 02:33:54 	at org.apache.flink.changelog.fs.StateChangeIteratorImpl.read(StateChangeIteratorImpl.java:42)
2022-08-16T02:33:54.7215163Z Aug 16 02:33:54 	at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:85)
2022-08-16T02:33:54.7216361Z Aug 16 02:33:54 	... 21 more
{code}

[~masteryhx] Could you help take another look? Thx;;;","13/Sep/22 03:24;hxb;Given that it hasn't appeared for a month, I will downgrade the priority to major.;;;","26/Sep/22 02:47;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41318&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798;;;","25/Nov/22 03:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43448&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10007;;;","25/Nov/22 04:39;mapohl;[~masteryhx] may you have another look?;;;","30/Nov/22 13:43;mapohl;FLINK-28898 and FLINK-28440 both are caused by some temporary file not being found anymore.;;;","30/Nov/22 13:57;mapohl;FLINK-28440 appeared to be a duplicate of this issue based on the stacktrace. Therefore, the cause of this issue also affects the stability of {{EventTimeWindowCheckpointingITCase.testSlidingTimeWindow}};;;","30/Nov/22 15:33;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43615&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8171;;;","09/Jan/23 08:08;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44558&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=7616;;;","11/Jan/23 10:27;mapohl;I took a look at the stacktraces: Only the issue description has a stacktrace pointing to a missing metafile. All the other build failures were caused by an Exception in while creating the {{StreamOperatorStateContext}}, which will be handled in FLINK-28898. Therefore, reopening this issue was a mistake.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect semantic of latestLoadTime in CachingLookupFunction and CachingAsyncLookupFunction,FLINK-28890,13475937,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,renqs,renqs,renqs,09/Aug/22 10:32,23/Sep/22 14:35,13/Jul/23 08:13,23/Sep/22 14:35,1.16.0,,,,,,,1.16.0,,,,,Table SQL / Runtime,,,,,,,0,pull-request-available,,,,"The semantic of latestLoadTime in CachingLookupFunction and CachingAsyncLookupFunction is not correct, which should be the time spent for the latest load operation",,renqs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 23 06:19:22 UTC 2022,,,,,,,,,,"0|z17lvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/22 06:19;renqs;Fixed on master: 24c685a58ef72db4c64c90e37056a07eb562be15

release-1.16: 0830c2ac819fd33628d11315b2485a12c23af5e6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The statistics of HsResultPartition are not updated correctly,FLINK-28888,13475928,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,09/Aug/22 09:44,11/Aug/22 10:19,13/Jul/23 08:13,11/Aug/22 10:19,1.16.0,,,,,,,1.16.0,,,,,Runtime / Metrics,,,,,,,0,pull-request-available,,,,"The statistics of HsResultPartition are not updated correctly, such as numBuffersOut, numBytesProduced and numBytesOut. This will cause bytes sent not to be displayed correctly on the Web UI, and will also affect the subsequent support for adaptive BatchScheduler.",,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 11 10:19:16 UTC 2022,,,,,,,,,,"0|z17ltk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/22 10:19;xtsong;master (1.16): e39fb24ecd77306b57130d01226608aea8ce5d96;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the bug of custom metrics in Thread Mode,FLINK-28887,13475925,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,09/Aug/22 09:31,11/Aug/22 12:39,13/Jul/23 08:13,11/Aug/22 12:39,1.16.0,,,,,,,1.16.0,,,,,API / Python,,,,,,,0,pull-request-available,,,,,,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 11 12:39:52 UTC 2022,,,,,,,,,,"0|z17lsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/22 12:39;hxbks2ks;Merged into master via 4ebb787ff354e5c54ea4c55d712bab6220d9ed55;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Downstream task may never be notified of data available in hybrid shuffle when number of credits is zero.,FLINK-28884,13475894,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,09/Aug/22 07:36,11/Aug/22 09:48,13/Jul/23 08:13,11/Aug/22 09:48,1.16.0,,,,,,,1.16.0,,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,"Downstream task may never be notified of data available in hybrid shuffle when number of credits is zero.

There are two potential problems:
 # HsSubpartitionView should be initialized to a notifiable state, there may be a problem of never consuming otherwise. Imagine the following situation: Downstream task has no initial credit(i.e. exclusive buffers is configured to zero), if there is no data output in the upstream, it will feedback a zero backlog to downstream input channel. All subsequent data available notifications will be intercepted as needNotify is false.
 # HsSubpartitionView should be notifiable when downstream get a zero backlog. Generally speaking, if the backlog is zero, when data become available, even if there is no credit, the backlog information will be notified also. However, in the hybrid shuffle, the notification will be ignored. This behavior is incorrect.",,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 11 09:48:55 UTC 2022,,,,,,,,,,"0|z17lm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/22 09:48;xtsong;master (1.16): 0b5aa420180729d8792c5d6feecfeb14382f72ee;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix HiveTableSink failed to report metrics to hive metastore,FLINK-28883,13475892,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,luoyuxia,Jiangang,Jiangang,09/Aug/22 07:24,26/Aug/22 12:20,13/Jul/23 08:13,26/Aug/22 12:20,1.16.0,,,,,,,1.16.0,,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,"Currently, HiveTableSink is failed to report metrics to metastores, like file number, total line number and total size.",,jark,Jiangang,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 26 12:20:00 UTC 2022,,,,,,,,,,"0|z17llk:",9223372036854775807,"In batch mode, Hive sink now will report statistics for written tables and partitions to Hive metastore by default. This might be time-consuming when there are many written files. You can disable this feature by setting `table.exec.hive.sink.statistic-auto-gather.enable` to `false`.",,,,,,,,,,,,,,,,,,,"11/Aug/22 04:27;luoyuxia;I'll try to fix it. [~jark] Could you please assign it to me.;;;","26/Aug/22 12:20;jark;Fixed in master: 4399b3fc40d11c2083197b6a505c23c4fcfec6df;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix CEP doc with wrong result of strict contiguity of looping patterns,FLINK-28880,13475834,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,Juntao Hu,Juntao Hu,09/Aug/22 03:08,09/Aug/22 03:39,13/Jul/23 08:13,09/Aug/22 03:35,1.15.1,,,,,,,1.14.6,1.15.2,1.16.0,,,Library / CEP,,,,,,,0,pull-request-available,,,,"https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/libs/cep/#contiguity-within-looping-patterns
The result of strict contiguity should be {a b1 c}, {a b2 c}, {a b3 c}, since b is *followed by* c.",,hxbks2ks,Juntao Hu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 09 03:35:30 UTC 2022,,,,,,,,,,"0|z17l8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 03:35;hxbks2ks;Merged into master via a08b050eb9fbea319275771fd9e95bbb025e2737
Merged into release-1.15 via 6eced8aa39c9e16b9918bd8b05fed1a1e17b6fe7
Merged into release-1.14 via c5108f6ab078ffbf1cbc483ad469197988e5553b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PipelinedRegionSchedulingITCase.testRecoverFromPartitionException failed with AssertionError,FLINK-28878,13475827,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhuzh,hxbks2ks,hxbks2ks,09/Aug/22 01:53,15/Aug/22 03:44,13/Jul/23 08:13,15/Aug/22 03:42,1.14.5,1.15.1,1.16.0,,,,,1.15.2,1.16.0,,,,Tests,,,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-08-08T20:38:43.3934646Z Aug 08 20:38:43 [ERROR] org.apache.flink.test.scheduling.PipelinedRegionSchedulingITCase.testRecoverFromPartitionException  Time elapsed: 20.288 s  <<< FAILURE!
2022-08-08T20:38:43.3935309Z Aug 08 20:38:43 java.lang.AssertionError: 
2022-08-08T20:38:43.3937070Z Aug 08 20:38:43 
2022-08-08T20:38:43.3938015Z Aug 08 20:38:43 Expected: is <false>
2022-08-08T20:38:43.3940277Z Aug 08 20:38:43      but: was <true>
2022-08-08T20:38:43.3940927Z Aug 08 20:38:43 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
2022-08-08T20:38:43.3941571Z Aug 08 20:38:43 	at org.junit.Assert.assertThat(Assert.java:964)
2022-08-08T20:38:43.3942120Z Aug 08 20:38:43 	at org.junit.Assert.assertThat(Assert.java:930)
2022-08-08T20:38:43.3943202Z Aug 08 20:38:43 	at org.apache.flink.test.scheduling.PipelinedRegionSchedulingITCase.testRecoverFromPartitionException(PipelinedRegionSchedulingITCase.java:98)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39652&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9994",,hxbks2ks,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 15 03:42:08 UTC 2022,,,,,,,,,,"0|z17l74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 01:55;hxbks2ks;cc [~zhuzh];;;","12/Aug/22 04:19;zhuzh;Thanks for reporting it! [~hxbks2ks]
The test fails due to an unexpected slowness of test running (may be due to an environment slowness). The slowness resulted in a slow request timeout and triggered a failover. This made the job failover number to be 2 instead of the expected 1.
Will increase the slot request timeout to make the tests more stable.;;;","15/Aug/22 03:42;zhuzh;Fixed via 
master: 5f8f387cba774a2c3900ea38e8a3dad017cf1790
release-1.15: ded03b750f46d8636d6744d4e094943d04f787dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Snapshot result of RocksDB native savepoint should have empty shared-state,FLINK-28863,13475688,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lijinzhong,yunta,yunta,08/Aug/22 08:37,26/Dec/22 02:16,13/Jul/23 08:13,23/Dec/22 02:26,,,,,,,,1.15.4,1.16.1,1.17.0,,,Runtime / Checkpointing,Runtime / State Backends,,,,,,0,pull-request-available,,,,"The current snapshot result of RocksDB native savepoint has non-empty shared state, which is obviously not correct as all snapshot artifacts already stay in the exclusive checkpoint scope folder.

This does not bring real harmful result due to we would not register the snapshot results of RocksDB native savepoint.",,klion26,lijinzhong,Yanfei Lei,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 26 02:16:29 UTC 2022,,,,,,,,,,"0|z17kc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/22 03:12;lijinzhong;In my opinion, for Rocksdb native savepoint SnapshotResult, we should put [sstFiles|https://github.com/apache/flink/blob/bb9f2525e6e16d00ef2f0739d9cb96c2e47e35e7/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/snapshot/RocksIncrementalSnapshotStrategy.java#L302] into privateState Map of IncrementalRemoteKeyedStateHandle to fix this issue.

This change has no effect on restore, which [downloads both the priavateStates and shareStates|[https://github.com/apache/flink/blob/35c5f674041bcefea93e1de459cea0d1789f98e0/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBStateDownloader.java#L53].]

 

[~yunta] WDYT?  If my understanding is correct, I can fix the issue.;;;","31/Oct/22 03:58;yunta;[~lijinzhong] I think this idea should be correct, already assigned to you.;;;","02/Nov/22 06:41;lijinzhong;[~yunta] 

1. I found that the changes I metioned above would change native savepoint's behavior in CLAIM mode.

Now, when job restore from native savepoint in CLAIM mode, the first checkpoint will be incremental based on savepoint sstFiles.
This means although native savepoint sstFiles stay in the exclusive scope folder, the checkpoints of the job which restore from the savepoint can still share sstFiles with it.
If we put [sstFiles|https://github.com/apache/flink/blob/bb9f2525e6e16d00ef2f0739d9cb96c2e47e35e7/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/snapshot/RocksIncrementalSnapshotStrategy.java#L302] into privateState Map of IncrementalRemoteKeyedStateHandle for native savepoint, the above behavior will be changed.

2. Back to the issue itself, i think, if the semantic of ""[sharedState|https://github.com/apache/flink/blob/bb9f2525e6e16d00ef2f0739d9cb96c2e47e35e7/flink-runtime/src/main/java/org/apache/flink/runtime/state/IncrementalRemoteKeyedStateHandle.java#L80]"" in IncrementalRemoteKeyedStateHandle is that state files can be shared by other checkpoints (include checkpoints after restore), *current behavior is by design.*   In other words,  the snapshot artifacts in the exclusive scope folder can still share sstFiles with checkpoints after restore in CLAIM mode.

[~yunta]  WDYT? ;;;","24/Nov/22 06:45;yunta;[~lijinzhong] Thanks for the analysis, do you mean if we put the ""sstFiles"" into the privateState map, the next restored checkpoint in CLAIM mode would be a full checkpoint? If so, I think we can keep the logic as it was now to not break the behavior. However, the logic is still not so straightforward, and we could add some comments in related code for developers.;;;","16/Dec/22 04:17;lijinzhong;[~yunta]  Yes, that is. And +1 for keeping the logic as it was now. 
Thanks for the advice. I‘ve added some comments in related code. Could you please help review it?;;;","22/Dec/22 14:08;yunta;merged in master: 287929c407d26541f1af1c7eaed24507d691725d;;;","26/Dec/22 02:16;yunta;merged in release-1.16: bb17c3baab727c23059db978ca48adadbbd0e897
merged in release-1.15: beaa0f698b0551a04cc751292cddeb4ba39c01e2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Non-deterministic UID generation might cause issues during restore,FLINK-28861,13475673,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,twalthr,colinsmetz,colinsmetz,08/Aug/22 07:11,17/Aug/22 07:55,13/Jul/23 08:13,16/Aug/22 07:41,1.15.1,,,,,,,1.15.2,1.16.0,,,,Runtime / Checkpointing,,,,,,,0,pull-request-available,,,,"I want to use the savepoint mechanism to move existing jobs from one version of Flink to another, by:
 # Stopping a job with a savepoint
 # Creating a new job from the savepoint, on the new version.

In Flink 1.15.1, it fails, even when going from 1.15.1 to 1.15.1. I get this error, meaning that it could not map the state from the previous job to the new one because of one operator:
{quote}{{Failed to rollback to checkpoint/savepoint hdfs://hdfs-name:8020/flink-savepoints/savepoint-046708-238e921f5e78. Cannot map checkpoint/savepoint state for operator d14a399e92154660771a806b90515d4c to the new program, because the operator is not available in the new program.}}
{quote}
After investigation, the problematic operator corresponds to a {{ChangelogNormalize}} operator, that I do not explicitly create. It is generated because I use [{{tableEnv.fromChangelogStream(stream, schema, ChangelogMode.upsert())}}|https://nightlies.apache.org/flink/flink-docs-release-1.15/api/java/org/apache/flink/table/api/bridge/java/StreamTableEnvironment.html#fromChangelogStream-org.apache.flink.streaming.api.datastream.DataStream-org.apache.flink.table.api.Schema-org.apache.flink.table.connector.ChangelogMode-] (the upsert mode is important, other modes do not fail). The table created is passed to an SQL query using the SQL API, which generates something like:
{quote}{{ChangelogNormalize[8] -> Calc[9] -> TableToDataSteam -> [my_sql_transformation] -> [my_sink]}}
{quote}
In previous versions of Flink it seems this operator was always given the same uid so the state could match when starting from the savepoint. In Flink 1.15.1, I see that a different uid is generated every time. I could not find a reliable way to set that uid manually. The only way I found was by going backwards from the transformation:
{quote}{{dataStream.getTransformation().getInputs().get(0).getInputs().get(0).getInputs().get(0).setUid(""the_user_defined_id"");}}
{quote}",,aitozi,colinsmetz,godfreyhe,jackylau,Ming Li,rmetzger,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 16 07:33:11 UTC 2022,,,,,,,,,,"0|z17k94:",9223372036854775807,1.15.0 and 1.15.1 generated non-deterministic UIDs for operators that make it difficult/impossible to restore state or upgrade to next patch version. A new table.exec.uid.generation config option (with correct default behavior) disables setting a UID for new pipelines from non-compiled plans. Existing pipelines can set table.exec.uid.generation=ALWAYS if the 1.15.0/1 behavior was acceptable.,,,,,,,,,,,,,,,,,,,"08/Aug/22 08:51;twalthr;This is clearly a bug that needs investigation. We introduced stable UIDs for all operators that the planner ingests. The default before was to set no UIDs at all, which resulted in autogenerated UIDs based on the topology and previous operators. ;;;","08/Aug/22 13:24;twalthr;After looking into this topic, I have a first guess what the problem might be. Could you share the UID of the affected operator with us after multiple tries? There might be a bug in the design. A UID is currently set as {{1_stream-exec-table-source-scan_1_external-datastream}}, however, the first component is generated by an {{AtomicCounter}} that is started per JVM. Which means that multiple runs of the same main() in the same JVM might generated different UIDs. How do you submit your jobs?

In any case, there is a workaround, you can set {{table.exec.legacy-transformation-uids}} to {{true}}. Let me know if this solves your problem for 1.15 upgrades or simple pipelines from previous versions? However, keep in mind that we don't support stateful upgrades between Flink versions for Flink SQL yet. [FLIP-190|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=191336489] aims to fix this.;;;","08/Aug/22 14:07;colinsmetz;{quote}Could you share the UID of the affected operator with us after multiple tries?
{quote}
I only have what seems to be a hash of the uid (obtained via [getGeneratedOperatorID|https://nightlies.apache.org/flink/flink-docs-release-1.12/api/java/org/apache/flink/runtime/OperatorIDPair.html#getGeneratedOperatorID--]), but here's what I get for three successive submissions:
 * 3c217b755850a1fb331af4b7f67946f5
 * 2ea21244917b900c533f89ff25291e8d
 * 48f2f08e9f1663064e1369bd518990a1

Does that help?
{quote}How do you submit your jobs?
{quote}
When we first noticed the problem, we were submitting the job with the REST API (POST /jars/:jarid/run). But we've since reproduced it in our tests using [PackagedProgramUtils.createJobGraph|[https://nightlies.apache.org/flink/flink-docs-release-1.15/api/java/org/apache/flink/client/program/PackagedProgramUtils.html]]
{quote}In any case, there is a workaround, you can set {{table.exec.legacy-transformation-uids}} to {{{}true{}}}. Let me know if this solves your problem for 1.15 upgrades or simple pipelines from previous versions? 
{quote}
It works indeed, thanks! At least from 1.15.1 to 1.15.1, I'll check more complex cases later.
{quote}However, keep in mind that we don't support stateful upgrades between Flink versions for Flink SQL yet. [FLIP-190|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=191336489] aims to fix this.
{quote}
We are aware of that but since it does seem to work at least sometimes, it's still better than nothing. Good to see there is a FLIP to support that.;;;","08/Aug/22 14:39;twalthr;Thanks for the response! The generated UID hashes were Flink 1.14 behavior, we replaced them with explicit and human readable UIDs as shown above. So 1.15.1 to 1.15.1 should not show you a hash. But nevertheless, the bug that will result in different UIDs when executing the same JAR twice needs to be fixed.;;;","09/Aug/22 10:18;twalthr;Let me summarize the issue: The UID generation using a static AtomicInteger as counter makes sense for plan compilation. There, the generated UID will be immediately persisted in JSON and thus remains static.

However, it should not be enabled by default for regular Table API jobs that potentially connect to DataStream API. The current counter approach for UID generation causes issues when the same JVM translates multiple SQL/Table API pipelines. It is not easily possible to ensure uniqueness as one DataStream API job could potentially consist of multiple SQL pipelines. The previous approach in 1.14 (viewing the entire StreamGraph and assigning the UIDs in a last step at the end) makes more sense if the pipeline is not constructed from a compiled plan.

I would suggest to revert the change made in 1.15. Not all pipelines are affected by this bug. It works nicely in containerized environments that start a new JVM per job. I suggest to introduce a new config option:
{{table.exec.uid-generation}} of type enum with values {{PLAN_ONLY}} (default) and {{ALWAYS}} (for 1.15.0, 1.15.1 behavior and expert users that know what they are doing). {{PLAN_ONLY}} means that we use the behavior of {{table.exec.legacy-transformation-uids}} if the translation does not happen for a compiled plan.

CC [~godfrey] [~jark];;;","15/Aug/22 13:27;twalthr;Fixed in master:
commit b142860352721fe65b8fe7e4106cbcd2059714e5
{code}
[FLINK-28861][table] Make UID generation behavior configurable and plan-only by default

Before this commit, due to changes for FLIP-190, every operator generated by the planner
got a UID assigned. However, the UID is based on a static counter that might return different
results depending on the environment. Thus, UIDs are not deterministic and make stateful
restores impossible e.g. when going from 1.15.0 -> 1.15.1. This PR restores the old pre-1.15
behavior for regular Table API. It only adds UIDs if the operator has been created from a
compiled plan. A compiled plan makes the UIDs static and thus deterministic.

table.exec.uid.generation=ALWAYS exists for backwards compatibility and could make stateful
upgrades possible even with invalid UIDs on best effort basis.
{code}

commit 881b2bf046e510b1b6dddddf8c15af45926397f1
{code}
[FLINK-28861][table] Fix bug in UID format for future migrations and make it configurable

Before this commit, the UID format was not future-proof for migrations. The ExecNode version
should not be in the UID, otherwise, operator migration won't be possible once plan migration
is executed. See the FLIP-190 example that drops a version in the plan, once operator migration
has been performed. Given that the plan feature is marked as @Experimental, this change should
still be possible without providing backwards compatibility.

However, the config option table.exec.uid.format allows for restoring the old format and solves
other UID related issues on the way.
{code};;;","16/Aug/22 07:33;twalthr;Fixed in 1.15.2:

commit 105d7c911bd0c5d8634417c22164547651abf07b
{code}
[FLINK-28861][table] Make UID generation behavior configurable and plan-only by default

Before this commit, due to changes for FLIP-190, every operator generated by the planner
got a UID assigned. However, the UID is based on a static counter that might return different
results depending on the environment. Thus, UIDs are not deterministic and make stateful
restores impossible e.g. when going from 1.15.0 -> 1.15.1. This PR restores the old pre-1.15
behavior for regular Table API. It only adds UIDs if the operator has been created from a
compiled plan. A compiled plan makes the UIDs static and thus deterministic.

table.exec.uid.generation=ALWAYS exists for backwards compatibility and could make stateful
upgrades possible even with invalid UIDs on best effort basis.
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CacheITCase.testBatchProduceCacheStreamConsume failed,FLINK-28860,13475670,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,xuannan,hxbks2ks,hxbks2ks,08/Aug/22 06:58,08/Sep/22 03:26,13/Jul/23 08:13,08/Sep/22 03:26,1.16.0,,,,,,,1.16.0,,,,,API / DataStream,,,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-08-08T03:27:22.1988575Z Aug 08 03:27:22 [ERROR] org.apache.flink.test.streaming.runtime.CacheITCase.testBatchProduceCacheStreamConsume(Path)  Time elapsed: 0.593 s  <<< ERROR!
2022-08-08T03:27:22.1989338Z Aug 08 03:27:22 java.lang.RuntimeException: Producing cache IntermediateResult is not supported in streaming mode
2022-08-08T03:27:22.1990401Z Aug 08 03:27:22 	at org.apache.flink.streaming.runtime.translators.CacheTransformationTranslator.translateForStreamingInternal(CacheTransformationTranslator.java:75)
2022-08-08T03:27:22.1991511Z Aug 08 03:27:22 	at org.apache.flink.streaming.runtime.translators.CacheTransformationTranslator.translateForStreamingInternal(CacheTransformationTranslator.java:42)
2022-08-08T03:27:22.1993671Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.graph.SimpleTransformationTranslator.translateForStreaming(SimpleTransformationTranslator.java:62)
2022-08-08T03:27:22.1994900Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.translate(StreamGraphGenerator.java:830)
2022-08-08T03:27:22.1995748Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.transform(StreamGraphGenerator.java:560)
2022-08-08T03:27:22.1996932Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.getParentInputIds(StreamGraphGenerator.java:851)
2022-08-08T03:27:22.1998562Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.translate(StreamGraphGenerator.java:809)
2022-08-08T03:27:22.1999581Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.transform(StreamGraphGenerator.java:560)
2022-08-08T03:27:22.2000376Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.generate(StreamGraphGenerator.java:319)
2022-08-08T03:27:22.2001359Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:2250)
2022-08-08T03:27:22.2002767Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:2241)
2022-08-08T03:27:22.2004121Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:2227)
2022-08-08T03:27:22.2005059Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2178)
2022-08-08T03:27:22.2005939Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.datastream.DataStream.executeAndCollectWithClient(DataStream.java:1469)
2022-08-08T03:27:22.2006735Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.datastream.DataStream.executeAndCollect(DataStream.java:1334)
2022-08-08T03:27:22.2007500Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.datastream.DataStream.executeAndCollect(DataStream.java:1320)
2022-08-08T03:27:22.2008315Z Aug 08 03:27:22 	at org.apache.flink.test.streaming.runtime.CacheITCase.testBatchProduceCacheStreamConsume(CacheITCase.java:190)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39518&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7",,gaoyunhaii,godfreyhe,hxbks2ks,xuannan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 08 03:26:25 UTC 2022,,,,,,,,,,"0|z17k8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 06:59;hxbks2ks;[~xuannan] Could you help take a look?;;;","09/Aug/22 01:59;godfreyhe;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39618&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","09/Aug/22 02:20;xuannan;[~hxbks2ks]I will take a look at it. Could you assign the ticket to me?;;;","09/Aug/22 15:46;gaoyunhaii;Temporarily disabled on master via 208f08b406a7fd48890cda16d317a30ee892a2e7;;;","08/Sep/22 03:26;gaoyunhaii;Merged on master via 208f08b406a7fd48890cda16d317a30ee892a2e7^..b7dd42617a46fcecfffbea3409391e204a40b9b1.

Merged on 1.16 via 38088230bb57486f81bb089a96aa3fa1e3f414f7^..f599a8c444ab44660824a7b3e0a08a635c22d3f4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FAILED pyflink/datastream/connectors/tests/test_file_system.py::FileSinkCsvBulkWriterTests::test_csv_customize_quote_char_write,FLINK-28859,13475649,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,godfreyhe,godfreyhe,08/Aug/22 04:29,08/Aug/22 11:55,13/Jul/23 08:13,08/Aug/22 11:55,1.16.0,,,,,,,1.16.0,,,,,API / Python,,,,,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39522&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901


Aug 08 03:45:08 =================================== FAILURES ===================================
Aug 08 03:45:08 ________ FileSinkCsvBulkWriterTests.test_csv_customize_quote_char_write ________
Aug 08 03:45:08 
Aug 08 03:45:08 self = <pyflink.datastream.connectors.tests.test_file_system.FileSinkCsvBulkWriterTests testMethod=test_csv_customize_quote_char_write>
Aug 08 03:45:08 
Aug 08 03:45:08     def test_csv_customize_quote_char_write(self):
Aug 08 03:45:08         schema, lines = _create_csv_customize_quote_char_schema_lines()
Aug 08 03:45:08         self._build_csv_job(schema, lines)
Aug 08 03:45:08 >       self.env.execute('test_csv_customize_quote_char_write')
Aug 08 03:45:08 
Aug 08 03:45:08 pyflink/datastream/connectors/tests/test_file_system.py:463: 
Aug 08 03:45:08 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ",,godfreyhe,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 08 11:55:17 UTC 2022,,,,,,,,,,"0|z17k4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 04:30;godfreyhe;cc [~hxbks2ks];;;","08/Aug/22 04:49;hxbks2ks;Thanks for reporting this. [~Juntao Hu] is helping looking into this problem.;;;","08/Aug/22 11:55;hxbks2ks;Merged into master via 4cf0b81d4f492b49d12aa317f3dfbc6e50d72ec8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ThriftObjectConversions compile failed,FLINK-28855,13475638,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yzl,hxbks2ks,hxbks2ks,08/Aug/22 02:45,13/Aug/22 02:56,13/Jul/23 08:13,13/Aug/22 02:56,1.16.0,,,,,,,1.16.0,,,,,Connectors / Hive,Table SQL / Gateway,,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-08-08T00:32:45.5104326Z [ERROR] /home/vsts/work/1/s/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/endpoint/hive/util/ThriftObjectConversions.java:[615,31] cannot find symbol
2022-08-08T00:32:45.5105191Z   symbol:   variable INDEX_TABLE
2022-08-08T00:32:45.5107273Z   location: class org.apache.hadoop.hive.metastore.TableType
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39514&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=4632ba9d-f1f2-5ad2-13fc-828d0e28bac4",,fsk119,hxb,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28633,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 13 02:56:09 UTC 2022,,,,,,,,,,"0|z17k28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 02:46;hxbks2ks;cc [~fsk119] [~yzl];;;","08/Aug/22 03:33;fsk119;Thanks for pointing out! [~yzl] will help to fix this soon.;;;","09/Aug/22 02:18;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39675&view=logs&j=b1fcf054-9138-5463-c73c-a49979b9ac2a&t=9291ac46-dd95-5135-b799-3839e65a8691&l=7350;;;","10/Aug/22 03:54;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39795&view=logs&j=b1fcf054-9138-5463-c73c-a49979b9ac2a&t=9291ac46-dd95-5135-b799-3839e65a8691;;;","13/Aug/22 02:56;fsk119;Merged into master: 1232629c80cbb64eb4ca9f6c95d6c5c1a2e8e82d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InitialSavepointPath might be ignored if first deployment fails,FLINK-28845,13475545,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,06/Aug/22 13:54,09/Aug/22 09:49,13/Jul/23 08:13,09/Aug/22 09:49,kubernetes-operator-1.0.0,kubernetes-operator-1.1.0,,,,,,kubernetes-operator-1.1.1,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,If the first deployment completely fails (and the cluster wasn't even created) the initialSavepointPath is subsequently ignored by the operator.,,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 09 09:49:21 UTC 2022,,,,,,,,,,"0|z17jhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/22 13:54;gyfora;cc [~jeesmon] ;;;","09/Aug/22 09:49;gyfora;merged:
main ac21bc8fe148f6dc803988791224f792a66875ce
release-1.1 d0c9f6ba714d5265ec55154213f3b2c383831cdc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to find incremental handle when restoring from changelog checkpoint in claim mode,FLINK-28843,13475516,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,frozen stone,frozen stone,frozen stone,06/Aug/22 03:09,09/Aug/22 03:10,13/Jul/23 08:13,09/Aug/22 03:10,1.15.0,1.15.1,,,,,,1.16.0,,,,,Runtime / State Backends,,,,,,,0,pull-request-available,,,,"# When native checkpoint is enabled and incremental checkpointing is enabled in rocksdb statebackend，if state data is greater than state.storage.fs.memory-threshold，it will be stored in a data file (FileStateHandle，RelativeFileStateHandle, etc) rather than stored with ByteStreamStateHandle in checkpoint metadata, like base-path1/chk-1/file1.
 # Then restore the job from base-path1/chk-1 in claim mode，using changelog statebackend，and the checkpoint path is set to base-path2, then new checkpoint will be saved in base-path2/chk-2, previous checkpoint file (base-path1/chk-1/file1) is needed.
 # Then restore the job from base-path2/chk-2 in changelog statebackend, flink will try to read base-path2/chk-2/file1, rather than the actual file location base-path1/chk-1/file1, which leads to FileNotFoundException and job failed.

 
How to reproduce?
 # Set state.storage.fs.memory-threshold to a small value, like '20b'.
 # {{run org.apache.flink.test.checkpointing.ChangelogPeriodicMaterializationSwitchStateBackendITCase#testSwitchFromDisablingToEnablingInClaimMode}}",,aitozi,frozen stone,gyfora,Ming Li,Yanfei Lei,ym,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25872,FLINK-28699,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 09 03:10:06 UTC 2022,,,,,,,,,,"0|z17jb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/22 06:24;yunta;Thanks for reporting this bug!

The root cause is that the native savepoint could contain the relative file state handles (all files under {{chk-x}} folder would be {{{}RelativeFileStateHandle{}}}), and the snapshot on changelog state-backend might not trigger the materialization part, which leads to the newly created {{chk-y}} folder does not contain previous snapshots. Thus, once restoring from {{{}chk-y{}}}, relocatable {{chk-x/file-1}} would be transferred to {{{}chk-y/file-1{}}}, resulting in the file not found exception.

Since we already give docs that native savepoint is relocatable (refer to [https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/ops/state/checkpoints_vs_savepoints/#capabilities-and-limitations] ), we might have to let changelog state-backend trigger materialization on the 1st checkpoint if restored snapshot containing relative file state handles. cc [~roman]  [~ym] [~Yanfei Lei] ;;;","07/Aug/22 15:21;frozen stone;I think the root cause is that IncrementalKeyedStateHandle  is not handled properly, only KeyGroupsStateHandle will be cast to absolute path during restore, maybe we could fix this by casting  IncrementalRemoteKeyedStateHandle in the same way. ;;;","08/Aug/22 02:22;yunta;[~frozen stone] I noticed that FLINK-25872 has considered such cases via introducing {{{}ChangelogStateBackendHandle#castToAbsolutePath{}}}, unfortunately, it just forget to cover the native savepoint of RocksDB state-backend. ;;;","08/Aug/22 02:36;Yanfei Lei;[~frozen stone] Thanks for reporting this bug! I think your analysis is correct, I had assumed that all  IncrementalKeyedStateHandle were located in /shared, and forgot the private handles of IncrementalKeyedStateHandle are located in /chk-x. ;;;","09/Aug/22 03:10;yunta;merged in master: 7f708d0ba42f727b3f8c3d77cef2108206cad2de;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Savepoint and checkpoint capabilities and limitations table is incorrect,FLINK-28835,13475324,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pnowojski,pnowojski,pnowojski,05/Aug/22 08:21,05/Aug/22 12:12,13/Jul/23 08:13,05/Aug/22 12:12,1.15.1,1.16.0,,,,,,1.15.2,1.16.0,,,,Documentation,,,,,,,0,pull-request-available,,,,"https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/checkpoints_vs_savepoints/

is inconsistent with https://cwiki.apache.org/confluence/display/FLINK/FLIP-203%3A+Incremental+savepoints#FLIP203:Incrementalsavepoints-Proposal. ""Non-arbitrary job upgrade"" for unaligned checkpoints should be officially supported. 

It looks like a typo in the original PR FLINK-26134",,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26134,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 05 12:12:18 UTC 2022,,,,,,,,,,"0|z17i4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 12:12;pnowojski;fixed on release-1.15 as 39a737f31be
merged commit b1c40bd into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in HybridSource when restoring from checkpoint,FLINK-28817,13475253,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhongqishang,Benenson,Benenson,05/Aug/22 00:31,16/Aug/22 20:31,13/Jul/23 08:13,12/Aug/22 23:55,1.14.4,1.15.1,,,,,,1.15.2,1.16.0,,,,Connectors / Common,,,,,,,1,pull-request-available,,,,"Scenario:
 # CheckpointCoordinator - Completed checkpoint 14 for job 00000000000000000000000000000000
 # HybridSource successfully completed processing a few SourceFactories, that reads from s3
 # HybridSourceSplitEnumerator.switchEnumerator failed with com.amazonaws.SdkClientException: Unable to execute HTTP request: Read timed out. This is intermittent error, it is usually fixed, when Flink recover from checkpoint & repeat the operation.
 # Flink starts recovering from checkpoint, 
 # HybridSourceSplitEnumerator receives SourceReaderFinishedEvent\{sourceIndex=-1}
 # Processing this event cause 

2022/08/08 08:39:34.862 ERROR o.a.f.r.s.c.SourceCoordinator - Uncaught exception in the SplitEnumerator for Source Source: hybrid-source while handling operator event SourceEventWrapper[SourceReaderFinishedEvent

{sourceIndex=-1}

] from subtask 6. Triggering job failover.
java.lang.NullPointerException: Source for index=0 is not available from sources: \{788=org.apache.flink.connector.file.src.SppFileSource@5a3803f3}
at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:104)
at org.apache.flink.connector.base.source.hybridspp.SwitchedSources.sourceOf(SwitchedSources.java:36)
at org.apache.flink.connector.base.source.hybridspp.HybridSourceSplitEnumerator.sendSwitchSourceEvent(HybridSourceSplitEnumerator.java:152)
at org.apache.flink.connector.base.source.hybridspp.HybridSourceSplitEnumerator.handleSourceEvent(HybridSourceSplitEnumerator.java:226)
...

I'm running my version of the Hybrid Sources with additional logging, so line numbers & some names could be different from Flink Github.

My Observation: the problem is intermittent, sometimes it works ok, i.e. SourceReaderFinishedEvent comes with correct sourceIndex. As I see from my log, it happens if my SourceFactory.create()  is executed BEFORE HybridSourceSplitEnumerator - handleSourceEvent SourceReaderFinishedEvent\{sourceIndex=-1}.
If  HybridSourceSplitEnumerator - handleSourceEvent is executed before my SourceFactory.create(), then sourceIndex=-1 in SourceReaderFinishedEvent

Preconditions-checkNotNull-error log from JobMgr is attached",,Benenson,dannycranmer,mason6345,nicholasjiang,thw,zhongqishang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/22 22:19;Benenson;Preconditions-checkNotNull-error.zip;https://issues.apache.org/jira/secure/attachment/13047897/Preconditions-checkNotNull-error.zip","05/Aug/22 00:30;Benenson;bf-29-JM-err-analysis.log;https://issues.apache.org/jira/secure/attachment/13047737/bf-29-JM-err-analysis.log",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 16 20:31:20 UTC 2022,,,,,,,,,,"0|z17how:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 09:49;zhongqishang;I encountered the similar problem.

[~thw] Please take a look the following case.

 

org.apache.flink.connector.base.source.hybrid.HybridSourceSplitEnumeratorTest
{code:java}
@Test
public void testRestoreEnumeratorWith2ndSource() throws Exception {
    setupEnumeratorAndTriggerSourceSwitch();
    HybridSourceEnumeratorState enumeratorState = enumerator.snapshotState(0);
    MockSplitEnumerator underlyingEnumerator = getCurrentEnumerator(enumerator);
    assertThat(
            (List<MockSourceSplit>)
                    Whitebox.getInternalState(underlyingEnumerator, ""splits""))
            .hasSize(0);
    enumerator =
            (HybridSourceSplitEnumerator) source.restoreEnumerator(context, enumeratorState);
    enumerator.start();
    enumerator.handleSourceEvent(SUBTASK0, new SourceReaderFinishedEvent(-1));
    underlyingEnumerator = getCurrentEnumerator(enumerator);
    assertThat(
            (List<MockSourceSplit>)
                    Whitebox.getInternalState(underlyingEnumerator, ""splits""))
            .hasSize(0);
}
 {code}
 ;;;","08/Aug/22 22:17;Benenson;Clarification for the problem:

1. --  HybridSourceSplitEnumerator.switchEnumerator failed with 
com.amazonaws.SdkClientException: Unable to execute HTTP request: Read timed out
Caused by: java.net.SocketTimeoutException: Read timed out
This is intermittent error, it is usually fixed, when Flink recover from checkpoint & repeat the operation

2. --  Flink starts recovering from checkpoint:
CheckpointCoordinator - Restoring job 00000000000000000000000000000000 from Checkpoint
SourceCoordinator - Closing SourceCoordinator for source Source: hybrid-source.
SourceCoordinator - Restoring SplitEnumerator of source Source: hybrid-source from checkpoint.
SourceCoordinator - Starting split enumerator for source Source: hybrid-source.
HybridSourceSplitEnumerator - Restoring enumerator for sourceIndex=788
HybridSourceSplitEnumerator - Starting enumerator for sourceIndex=788

3. --  HybridSourceSplitEnumerator receives SourceReaderFinishedEvent\{sourceIndex=-1}
HybridSourceSplitEnumerator - handleSourceEvent SourceReaderFinishedEvent\{sourceIndex=-1} subtask=6

4. --  Processing this event cause 
2022/08/08 08:39:34.862 ERROR o.a.f.r.s.c.SourceCoordinator - Uncaught exception in the SplitEnumerator for Source Source: hybrid-source while handling operator event SourceEventWrapper[SourceReaderFinishedEvent\{sourceIndex=-1}] from subtask 6. Triggering job failover.
java.lang.NullPointerException: Source for index=0 is not available from sources: \{788=org.apache.flink.connector.file.src.SppFileSource@5a3803f3}
at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:104)
at org.apache.flink.connector.base.source.hybridspp.SwitchedSources.sourceOf(SwitchedSources.java:36)
at org.apache.flink.connector.base.source.hybridspp.HybridSourceSplitEnumerator.sendSwitchSourceEvent(HybridSourceSplitEnumerator.java:152)
at org.apache.flink.connector.base.source.hybridspp.HybridSourceSplitEnumerator.handleSourceEvent(HybridSourceSplitEnumerator.java:226)
at org.apache.flink.runtime.source.coordinator.SourceCoordinator.lambda$handleEventFromOperator$1(SourceCoordinator.java:182)
at org.apache.flink.runtime.source.coordinator.SourceCoordinator.lambda$runInEventLoop$8(SourceCoordinator.java:344)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)

I'm running my version of the Hybrid Sources with additional logging, so line numbers & some names could be different from Flink Github.

My Observation: the problem is intermittent, sometimes it works ok, i.e. SourceReaderFinishedEvent comes with correct sourceIndex. As I see from my log, it happens if my SourceFactory.create()  is executed BEFORE HybridSourceSplitEnumerator - handleSourceEvent SourceReaderFinishedEvent\{sourceIndex=-1}.
If  HybridSourceSplitEnumerator - handleSourceEvent is executed before my SourceFactory.create(), then sourceIndex=-1 in SourceReaderFinishedEvent

[~thw] , [~mason6345] could you, please, look at this issue?

Preconditions-checkNotNull-error log from JobMgr is attached;;;","10/Aug/22 07:11;zhongqishang;[~thw] 

I tried to open a PR for this issue.

[~Benenson] has already test for this PR.

 ;;;","11/Aug/22 14:28;thw;[~Benenson] thanks for investigating this issue. I think that has to do with the reader not having any restored splits (most likely because none were previously assigned) and therefore reporting -1 back to the enumerator. Let me check what the correct fix for this is.;;;","12/Aug/22 09:55;nicholasjiang;[~thw], IMO, this pull request could also fixed the bug of FLINK-26938 , right?

cc [~zhongqishang] ;;;","12/Aug/22 23:54;thw;[~nicholasjiang] I believe it does. Can you please verify and close FLINK-26938 if so?;;;","12/Aug/22 23:56;thw;[~Benenson] thank you for the thorough investigation!;;;","16/Aug/22 20:31;dannycranmer;* Merged commit [{{6e80d90}}|https://github.com/apache/flink/commit/6e80d90b1611499375cf74b45a0828db383aac7d] into apache:master
 * Merged commit [{{e5570e3}}|https://github.com/apache/flink/commit/e5570e3e33ac33fd1b31d38c86ac6a291e7bc47e] into apache:release-1.15 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Various components don't respect schema lifecycle,FLINK-28807,13475053,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,04/Aug/22 08:57,13/Sep/22 08:59,13/Jul/23 08:13,06/Aug/22 20:42,1.15.0,,,,,,,1.16.0,elasticsearch-3.0.0,,,,API / Python,Connectors / ElasticSearch,Connectors / Kafka,Connectors / Kinesis,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Tests,,0,pull-request-available,,,,A surprising number of components never call \{{(De)SerializationSchema#open}} making life very difficult for people who want to make use of said method.,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28621,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 06 20:42:37 UTC 2022,,,,,,,,,,"0|z17ggg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/22 20:42;chesnay;master: fb95798b1c301152b912c4b8ec4a737ea16d8641
elasticsearch-main: c9ec08a7bafa343c4a74e2579d13b0a00b6317b5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HsFileDataManager should avoid busy-loop when fileReader has not data to read,FLINK-28800,13475028,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,04/Aug/22 07:26,18/Aug/22 15:10,13/Jul/23 08:13,18/Aug/22 15:10,1.16.0,,,,,,,1.16.0,,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,"When fileReader has no data to read, for example, most of data is consumed from memory. HsFileDataManager will encounter busy-loop problem, which will lead to a meaningless surge in CPU utilization and seriously affect performance.",,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 18 15:10:33 UTC 2022,,,,,,,,,,"0|z17gaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/22 15:10;xtsong;master (1.16): 5d13403429db27d63fdd6932c65c23ed5b90ef96;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hybrid shuffle can't schedule graph contains blocking edge,FLINK-28799,13475025,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,04/Aug/22 07:19,09/Aug/22 02:10,13/Jul/23 08:13,09/Aug/22 02:10,1.16.0,,,,,,,1.16.0,,,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"Based on TPC-DS test, we found that hybrid shuffle can't schedule graph contains blocking edge. The reason is that some batch operators will forcibly set the exchange mode to blocking, which breaks ALL_ EDGE_HYBRID‘s constraint makes the scheduling deadlock.

We should think of a better way to support the scheduling the graph of all kinds of edges, including hybrid edge.",,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 09 02:10:38 UTC 2022,,,,,,,,,,"0|z17ga8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 02:10;xtsong;master (1.16): 4deaf6edc152d06488f8738386b6a8b7544fe5e9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unstable test for HiveDialectQueryITCase#testInsertDirectory,FLINK-28795,13474986,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,luoyuxia,luoyuxia,luoyuxia,04/Aug/22 04:23,15/Aug/22 07:06,13/Jul/23 08:13,15/Aug/22 07:06,,,,,,,,1.16.0,,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,"The HiveDialectQueryITCase#testInsertDirectory is unstable. The failure link [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39233&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461]

we need to wait the insert to finish before select from the table",,fsk119,hxbks2ks,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 15 07:06:58 UTC 2022,,,,,,,,,,"0|z17g1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 01:52;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39278&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=25284;;;","05/Aug/22 02:11;fsk119;Merged into master:

5f620d29787abad2036a28373e80429026e29f21

 ;;;","15/Aug/22 01:53;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39941&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461
Different failed case HiveDialectQueryITCase.testBoolComparison, but the stack is looks like same:

{code:java}
2022-08-13T01:29:51.5076876Z Aug 13 01:29:51 org.apache.flink.connectors.hive.FlinkHiveException: java.io.IOException: Fail to create input splits.
2022-08-13T01:29:51.5084747Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:98)
2022-08-13T01:29:51.5086194Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:174)
2022-08-13T01:29:51.5087528Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:140)
2022-08-13T01:29:51.5088973Z Aug 13 01:29:51 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:141)
2022-08-13T01:29:51.5090646Z Aug 13 01:29:51 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.java:101)
2022-08-13T01:29:51.5092431Z Aug 13 01:29:51 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-08-13T01:29:51.5093827Z Aug 13 01:29:51 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:257)
2022-08-13T01:29:51.5095525Z Aug 13 01:29:51 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
2022-08-13T01:29:51.5096916Z Aug 13 01:29:51 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-08-13T01:29:51.5098430Z Aug 13 01:29:51 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:257)
2022-08-13T01:29:51.5099909Z Aug 13 01:29:51 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:65)
2022-08-13T01:29:51.5101362Z Aug 13 01:29:51 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-08-13T01:29:51.5103082Z Aug 13 01:29:51 	at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:93)
2022-08-13T01:29:51.5104386Z Aug 13 01:29:51 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
2022-08-13T01:29:51.5105516Z Aug 13 01:29:51 	at scala.collection.Iterator.foreach(Iterator.scala:937)
2022-08-13T01:29:51.5106544Z Aug 13 01:29:51 	at scala.collection.Iterator.foreach$(Iterator.scala:937)
2022-08-13T01:29:51.5107651Z Aug 13 01:29:51 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
2022-08-13T01:29:51.5108783Z Aug 13 01:29:51 	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
2022-08-13T01:29:51.5109907Z Aug 13 01:29:51 	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
2022-08-13T01:29:51.5111008Z Aug 13 01:29:51 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2022-08-13T01:29:51.5112618Z Aug 13 01:29:51 	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
2022-08-13T01:29:51.5113726Z Aug 13 01:29:51 	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
2022-08-13T01:29:51.5114888Z Aug 13 01:29:51 	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
2022-08-13T01:29:51.5116119Z Aug 13 01:29:51 	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:92)
2022-08-13T01:29:51.5117429Z Aug 13 01:29:51 	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:197)
2022-08-13T01:29:51.5118842Z Aug 13 01:29:51 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1730)
2022-08-13T01:29:51.5120271Z Aug 13 01:29:51 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:865)
2022-08-13T01:29:51.5122896Z Aug 13 01:29:51 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1375)
2022-08-13T01:29:51.5124442Z Aug 13 01:29:51 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:733)
2022-08-13T01:29:51.5125932Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.HiveDialectQueryITCase.testBoolComparison(HiveDialectQueryITCase.java:737)
2022-08-13T01:29:51.5127189Z Aug 13 01:29:51 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-13T01:29:51.5128360Z Aug 13 01:29:51 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-13T01:29:51.5129674Z Aug 13 01:29:51 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-13T01:29:51.5130773Z Aug 13 01:29:51 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-13T01:29:51.5132093Z Aug 13 01:29:51 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-08-13T01:29:51.5133411Z Aug 13 01:29:51 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-08-13T01:29:51.5134642Z Aug 13 01:29:51 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-08-13T01:29:51.5135847Z Aug 13 01:29:51 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-08-13T01:29:51.5137276Z Aug 13 01:29:51 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-13T01:29:51.5138524Z Aug 13 01:29:51 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-08-13T01:29:51.5139886Z Aug 13 01:29:51 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-08-13T01:29:51.5141112Z Aug 13 01:29:51 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-08-13T01:29:51.5142586Z Aug 13 01:29:51 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-08-13T01:29:51.5143709Z Aug 13 01:29:51 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-08-13T01:29:51.5144859Z Aug 13 01:29:51 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-08-13T01:29:51.5145912Z Aug 13 01:29:51 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-08-13T01:29:51.5147026Z Aug 13 01:29:51 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-08-13T01:29:51.5148157Z Aug 13 01:29:51 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-08-13T01:29:51.5149260Z Aug 13 01:29:51 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-08-13T01:29:51.5150528Z Aug 13 01:29:51 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-13T01:29:51.5151516Z Aug 13 01:29:51 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-08-13T01:29:51.5152655Z Aug 13 01:29:51 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-08-13T01:29:51.5153642Z Aug 13 01:29:51 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-08-13T01:29:51.5154710Z Aug 13 01:29:51 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-08-13T01:29:51.5155983Z Aug 13 01:29:51 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-08-13T01:29:51.5157176Z Aug 13 01:29:51 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-08-13T01:29:51.5158402Z Aug 13 01:29:51 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-08-13T01:29:51.5159780Z Aug 13 01:29:51 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-08-13T01:29:51.5161226Z Aug 13 01:29:51 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-08-13T01:29:51.5162982Z Aug 13 01:29:51 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-08-13T01:29:51.5164495Z Aug 13 01:29:51 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-08-13T01:29:51.5165816Z Aug 13 01:29:51 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-08-13T01:29:51.5167072Z Aug 13 01:29:51 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-08-13T01:29:51.5168449Z Aug 13 01:29:51 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-08-13T01:29:51.5169857Z Aug 13 01:29:51 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-08-13T01:29:51.5171273Z Aug 13 01:29:51 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-08-13T01:29:51.5174304Z Aug 13 01:29:51 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-08-13T01:29:51.5175677Z Aug 13 01:29:51 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-08-13T01:29:51.5176937Z Aug 13 01:29:51 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-08-13T01:29:51.5178382Z Aug 13 01:29:51 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-08-13T01:29:51.5179568Z Aug 13 01:29:51 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-08-13T01:29:51.5180846Z Aug 13 01:29:51 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-08-13T01:29:51.5181990Z Aug 13 01:29:51 Caused by: java.io.IOException: Fail to create input splits.
2022-08-13T01:29:51.5183187Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:84)
2022-08-13T01:29:51.5184564Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:91)
2022-08-13T01:29:51.5185825Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:179)
2022-08-13T01:29:51.5187083Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107)
2022-08-13T01:29:51.5188427Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:95)
2022-08-13T01:29:51.5189395Z Aug 13 01:29:51 	... 71 more
2022-08-13T01:29:51.5192250Z Aug 13 01:29:51 Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error while running command to get file permissions : ExitCodeException exitCode=2: /bin/ls: cannot access '/tmp/junit2768738167464506810/hive_warehouse/tbool/.staging_1660353747721': No such file or directory
2022-08-13T01:29:51.5193692Z Aug 13 01:29:51 
2022-08-13T01:29:51.5194437Z Aug 13 01:29:51 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:585)
2022-08-13T01:29:51.5195378Z Aug 13 01:29:51 	at org.apache.hadoop.util.Shell.run(Shell.java:482)
2022-08-13T01:29:51.5196379Z Aug 13 01:29:51 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:776)
2022-08-13T01:29:51.5197392Z Aug 13 01:29:51 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
2022-08-13T01:29:51.5198326Z Aug 13 01:29:51 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
2022-08-13T01:29:51.5199250Z Aug 13 01:29:51 	at org.apache.hadoop.fs.FileUtil.execCommand(FileUtil.java:1097)
2022-08-13T01:29:51.5200401Z Aug 13 01:29:51 	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:659)
2022-08-13T01:29:51.5201738Z Aug 13 01:29:51 	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:634)
2022-08-13T01:29:51.5203113Z Aug 13 01:29:51 	at org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:49)
2022-08-13T01:29:51.5204108Z Aug 13 01:29:51 	at org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1732)
2022-08-13T01:29:51.5205029Z Aug 13 01:29:51 	at org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1712)
2022-08-13T01:29:51.5206102Z Aug 13 01:29:51 	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:270)
2022-08-13T01:29:51.5207168Z Aug 13 01:29:51 	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
2022-08-13T01:29:51.5208228Z Aug 13 01:29:51 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
2022-08-13T01:29:51.5209334Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-08-13T01:29:51.5210467Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-08-13T01:29:51.5211483Z Aug 13 01:29:51 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-08-13T01:29:51.5212737Z Aug 13 01:29:51 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-08-13T01:29:51.5213768Z Aug 13 01:29:51 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-08-13T01:29:51.5214947Z Aug 13 01:29:51 	at java.lang.Thread.run(Thread.java:748)
2022-08-13T01:29:51.5215549Z Aug 13 01:29:51 
2022-08-13T01:29:51.5216245Z Aug 13 01:29:51 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-08-13T01:29:51.5217136Z Aug 13 01:29:51 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-08-13T01:29:51.5218306Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-08-13T01:29:51.5219143Z Aug 13 01:29:51 	... 75 more
2022-08-13T01:29:51.5221273Z Aug 13 01:29:51 Caused by: java.lang.RuntimeException: Error while running command to get file permissions : ExitCodeException exitCode=2: /bin/ls: cannot access '/tmp/junit2768738167464506810/hive_warehouse/tbool/.staging_1660353747721': No such file or directory
2022-08-13T01:29:51.5222712Z Aug 13 01:29:51 
2022-08-13T01:29:51.5223401Z Aug 13 01:29:51 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:585)
2022-08-13T01:29:51.5224297Z Aug 13 01:29:51 	at org.apache.hadoop.util.Shell.run(Shell.java:482)
2022-08-13T01:29:51.5225223Z Aug 13 01:29:51 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:776)
2022-08-13T01:29:51.5226163Z Aug 13 01:29:51 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
2022-08-13T01:29:51.5227017Z Aug 13 01:29:51 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
2022-08-13T01:29:51.5227854Z Aug 13 01:29:51 	at org.apache.hadoop.fs.FileUtil.execCommand(FileUtil.java:1097)
2022-08-13T01:29:51.5228897Z Aug 13 01:29:51 	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:659)
2022-08-13T01:29:51.5230278Z Aug 13 01:29:51 	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:634)
2022-08-13T01:29:51.5231349Z Aug 13 01:29:51 	at org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:49)
2022-08-13T01:29:51.5232455Z Aug 13 01:29:51 	at org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1732)
2022-08-13T01:29:51.5233314Z Aug 13 01:29:51 	at org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1712)
2022-08-13T01:29:51.5234312Z Aug 13 01:29:51 	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:270)
2022-08-13T01:29:51.5235451Z Aug 13 01:29:51 	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
2022-08-13T01:29:51.5236537Z Aug 13 01:29:51 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
2022-08-13T01:29:51.5237636Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-08-13T01:29:51.5238665Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-08-13T01:29:51.5239613Z Aug 13 01:29:51 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-08-13T01:29:51.5240709Z Aug 13 01:29:51 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-08-13T01:29:51.5242167Z Aug 13 01:29:51 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-08-13T01:29:51.5243205Z Aug 13 01:29:51 	at java.lang.Thread.run(Thread.java:748)
2022-08-13T01:29:51.5243905Z Aug 13 01:29:51 
2022-08-13T01:29:51.5244958Z Aug 13 01:29:51 	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:699)
2022-08-13T01:29:51.5246461Z Aug 13 01:29:51 	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:634)
2022-08-13T01:29:51.5247749Z Aug 13 01:29:51 	at org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:49)
2022-08-13T01:29:51.5248814Z Aug 13 01:29:51 	at org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1732)
2022-08-13T01:29:51.5249773Z Aug 13 01:29:51 	at org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1712)
2022-08-13T01:29:51.5251083Z Aug 13 01:29:51 	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:270)
2022-08-13T01:29:51.5252423Z Aug 13 01:29:51 	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
2022-08-13T01:29:51.5253513Z Aug 13 01:29:51 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
2022-08-13T01:29:51.5254822Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-08-13T01:29:51.5256129Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-08-13T01:29:51.5257367Z Aug 13 01:29:51 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-08-13T01:29:51.5258501Z Aug 13 01:29:51 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-08-13T01:29:51.5259618Z Aug 13 01:29:51 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-08-13T01:29:51.5260578Z Aug 13 01:29:51 	at java.lang.Thread.run(Thread.java:748)
2022-08-13T01:29:51.5261222Z Aug 13 01:29:51 
2022-08-13T01:29:53.8083054Z Aug 13 01:29:53 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2022-08-13T01:29:55.9454104Z Aug 13 01:29:55 [INFO] Running org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReaderITCase
2022-08-13T01:30:39.9002693Z Aug 13 01:30:39 [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 43.859 s - in org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReaderITCase
2022-08-13T01:34:21.8856413Z Aug 13 01:34:21 == Abstract Syntax Tree ==
2022-08-13T01:34:21.8860211Z Aug 13 01:34:21 LogicalSort(sort0=[$0], dir0=[ASC-nulls-first])
2022-08-13T01:34:21.8862493Z Aug 13 01:34:21 +- LogicalProject(a=[$0], b=[$1], c=[$2], p=[$3], x=[$4], y=[$5])
2022-08-13T01:34:21.8864021Z Aug 13 01:34:21    +- LogicalFilter(condition=[AND(=($4, $3), =($6, 1))])
2022-08-13T01:34:21.8865411Z Aug 13 01:34:21       +- LogicalJoin(condition=[true], joinType=[inner])
2022-08-13T01:34:21.8866858Z Aug 13 01:34:21          :- LogicalTableScan(table=[[test-catalog, default, fact]])
2022-08-13T01:34:21.8868511Z Aug 13 01:34:21          +- LogicalTableScan(table=[[test-catalog, default, dim]])
2022-08-13T01:34:21.8869569Z Aug 13 01:34:21 
2022-08-13T01:34:21.8870398Z Aug 13 01:34:21 == Optimized Physical Plan ==
2022-08-13T01:34:21.8871284Z Aug 13 01:34:21 Sort(orderBy=[a ASC])
2022-08-13T01:34:21.8872787Z Aug 13 01:34:21 +- Exchange(distribution=[single])
2022-08-13T01:34:21.8874290Z Aug 13 01:34:21    +- HashJoin(joinType=[InnerJoin], where=[=(x, p)], select=[a, b, c, p, x, y], build=[right])
2022-08-13T01:34:21.8875683Z Aug 13 01:34:21       :- Exchange(distribution=[hash[p]])
2022-08-13T01:34:21.8877169Z Aug 13 01:34:21       :  +- DynamicFilteringTableSourceScan(table=[[test-catalog, default, fact]], fields=[a, b, c, p])
2022-08-13T01:34:21.8878630Z Aug 13 01:34:21       :     +- DynamicFilteringDataCollector(fields=[x])
2022-08-13T01:34:21.8879936Z Aug 13 01:34:21       :        +- Calc(select=[x, y], where=[=(z, 1)])
2022-08-13T01:34:21.8881333Z Aug 13 01:34:21       :           +- TableSourceScan(table=[[test-catalog, default, dim]], fields=[x, y, z])
2022-08-13T01:34:21.8883005Z Aug 13 01:34:21       +- Exchange(distribution=[hash[x]])
2022-08-13T01:34:21.8885064Z Aug 13 01:34:21          +- Calc(select=[x, y], where=[=(z, 1)])
2022-08-13T01:34:21.8886532Z Aug 13 01:34:21             +- TableSourceScan(table=[[test-catalog, default, dim]], fields=[x, y, z])
2022-08-13T01:34:21.8887394Z Aug 13 01:34:21 
2022-08-13T01:34:21.8888901Z Aug 13 01:34:21 == Optimized Execution Plan ==
2022-08-13T01:34:21.8889775Z Aug 13 01:34:21 Sort(orderBy=[a ASC])
2022-08-13T01:34:21.8891058Z Aug 13 01:34:21 +- Exchange(distribution=[single])
2022-08-13T01:34:21.8892592Z Aug 13 01:34:21    +- HashJoin(joinType=[InnerJoin], where=[(x = p)], select=[a, b, c, p, x, y], build=[right])
2022-08-13T01:34:21.8893794Z Aug 13 01:34:21       :- Exchange(distribution=[hash[p]])
2022-08-13T01:34:21.8895346Z Aug 13 01:34:21       :  +- DynamicFilteringTableSourceScan(table=[[test-catalog, default, fact]], fields=[a, b, c, p])
2022-08-13T01:34:21.8896728Z Aug 13 01:34:21       :     +- DynamicFilteringDataCollector(fields=[x])
2022-08-13T01:34:21.8897969Z Aug 13 01:34:21       :        +- Calc(select=[x, y], where=[(z = 1)])(reuse_id=[1])
2022-08-13T01:34:21.8899462Z Aug 13 01:34:21       :           +- TableSourceScan(table=[[test-catalog, default, dim]], fields=[x, y, z])
2022-08-13T01:34:21.8900691Z Aug 13 01:34:21       +- Exchange(distribution=[hash[x]])
2022-08-13T01:34:21.8901761Z Aug 13 01:34:21          +- Reused(reference_id=[1])
{code}
;;;","15/Aug/22 07:06;hxbks2ks;Merged into master via 662f9de97ffadb7c178a94d556ffebfec7d97817;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 TPC-DS tests failed  due to release input gate for task failure,FLINK-28789,13474856,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tanyuxin,leonard,leonard,03/Aug/22 09:40,08/Aug/22 03:16,13/Jul/23 08:13,08/Aug/22 03:16,1.16.0,,,,,,,1.16.0,,,,,Runtime / Network,,,,,,,0,test-stability,,,,"

{code:java}
switched from CANCELING to CANCELED.
2022-08-03 08:03:02,776 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for MultipleInput[2212] -> Calc[2191] -> HashAggregate[2192] (8/8)#1 (cf5f33b100f0efb21b9ff8d27a78cd8e_d806bb3f5ea308ac3f1df304a96163b4_7_1).
2022-08-03 08:03:02,776 ERROR org.apache.flink.runtime.taskmanager.Task                    [] - Failed to release input gate for task MultipleInput[2212] -> Calc[2191] -> HashAggregate[2192] (8/8)#1.
org.apache.flink.shaded.netty4.io.netty.util.IllegalReferenceCountException: refCnt: 0, decrement: 1
	at org.apache.flink.shaded.netty4.io.netty.util.internal.ReferenceCountUpdater.toLiveRealRefCnt(ReferenceCountUpdater.java:74) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.shaded.netty4.io.netty.util.internal.ReferenceCountUpdater.release(ReferenceCountUpdater.java:138) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:100) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:156) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedNetworkBuffer.recycleBuffer(ReadOnlySlicedNetworkBuffer.java:123) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.io.network.buffer.CompositeBuffer.recycleBuffer(CompositeBuffer.java:70) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at java.util.ArrayList.forEach(ArrayList.java:1259) ~[?:1.8.0_332]
	at org.apache.flink.runtime.io.network.partition.SortMergeSubpartitionReader.releaseInternal(SortMergeSubpartitionReader.java:181) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.SortMergeSubpartitionReader.releaseAllResources(SortMergeSubpartitionReader.java:163) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.releaseAllResources(LocalInputChannel.java:341) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.close(SingleInputGate.java:667) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.close(InputGateWithMetrics.java:140) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.closeAllInputGates(Task.java:1010) [flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.releaseResources(Task.java:975) [flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:820) [flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550) [flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_332]
2022-08-03 08:03:02,778 WARN  org.apache.flink.metrics.MetricGroup     
{code}


The failed CI link: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39152&view=results",,aitozi,fsk119,hxb,hxbks2ks,jark,kevin.cyj,leonard,luoyuxia,tanyuxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28373,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 08 03:15:47 UTC 2022,,,,,,,,,,"0|z17f94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 11:30;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39176&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;","03/Aug/22 11:31;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39179&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;","03/Aug/22 11:31;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39185&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;","03/Aug/22 11:50;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39192&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;","04/Aug/22 06:03;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39200&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39202&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39205&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39209&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39213&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=11492
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39220&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=11684
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39229&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39229&view=logs&j=f8e16326-dc75-5ba0-3e95-6178dd55bf6c&t=15c1d318-5ca8-529f-77a2-d113a700ec34
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39229&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39229&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=39a035c3-c65e-573c-fb66-104c66c28912;;;","04/Aug/22 08:27;fsk119;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39233&view=logs&j=ef799394-2d67-5ff4-b2e5-410b80c9c0af&t=860bfb5d-81b0-5968-f128-2a8b5362110d;;;","04/Aug/22 08:36;luoyuxia;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39212&view=logs&s=7d4e458d-e0e0-5f89-c72d-7371ef61b09b&j=fce28b06-a2d8-5021-adb0-154438f48362];;;","04/Aug/22 16:46;kevin.cyj;Though still not know the root cause, by reverting FLINK-28373 and testing multiple times on my own azure account, the issue seems resolved. For CI stability, I am reverting FLINK-28373, let's see if that solves the problem.;;;","06/Aug/22 15:33;jark;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39439&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;","08/Aug/22 02:57;tanyuxin;The commit has been reverted on Aug 05, and the issue has been resolved.;;;","08/Aug/22 03:15;kevin.cyj;Closing this issue, feel free to reopen it if the problem still exists.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot run PyFlink 1.16 on MacOS with M1 chip,FLINK-28786,13474822,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,taoran,taoran,03/Aug/22 06:48,05/May/23 02:26,13/Jul/23 08:13,27/Apr/23 10:04,1.16.0,,,,,,,1.16.2,1.17.1,,,,API / Python,,,,,,,0,pull-request-available,,,,"I have tested it with 2 m1 machines. i will reproduce the bug 100%.

1.m1 machine
macos bigsur 11.5.1 & jdk8 * & jdk11 & python 3.8 & python 3.9
1.m1 machine
macos monterey 12.1 & jdk8 * & jdk11 & python 3.8 & python 3.9

reproduce step:
1.python -m pip install -r flink-python/dev/dev-requirements.txt
2.cd flink-python; python setup.py sdist bdist_wheel; cd apache-flink-libraries; python setup.py sdist; cd ..;
3.python -m pip install apache-flink-libraries/dist/*.tar.gz
4.python -m pip install dist/*.whl

when run [word_count.py|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/table_api_tutorial/] it will cause


{code:java}
<frozen importlib._bootstrap>:219: RuntimeWarning: apache_beam.coders.coder_impl.StreamCoderImpl size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject
Traceback (most recent call last):
  File ""/Users/chucheng/GitLab/pyflink-demo/table/streaming/word_count.py"", line 129, in <module>
    word_count(known_args.input, known_args.output)
  File ""/Users/chucheng/GitLab/pyflink-demo/table/streaming/word_count.py"", line 49, in word_count
    t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 121, in create
    return TableEnvironment(j_tenv)
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 100, in __init__
    self._open()
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 1637, in _open
    startup_loopback_server()
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 1628, in startup_loopback_server
    from pyflink.fn_execution.beam.beam_worker_pool_service import \
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_worker_pool_service.py"", line 44, in <module>
    from pyflink.fn_execution.beam import beam_sdk_worker_main  # noqa # pylint: disable=unused-import
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_sdk_worker_main.py"", line 21, in <module>
    import pyflink.fn_execution.beam.beam_operations # noqa # pylint: disable=unused-import
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_operations.py"", line 27, in <module>
    from pyflink.fn_execution.state_impl import RemoteKeyedStateBackend, RemoteOperatorStateBackend
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/state_impl.py"", line 33, in <module>
    from pyflink.fn_execution.beam.beam_coders import FlinkCoder
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_coders.py"", line 27, in <module>
    from pyflink.fn_execution.beam import beam_coder_impl_fast as beam_coder_impl
  File ""pyflink/fn_execution/beam/beam_coder_impl_fast.pyx"", line 1, in init pyflink.fn_execution.beam.beam_coder_impl_fast
KeyError: '__pyx_vtable__'
{code}


",,ana4,dianfu,grimsby,hxbks2ks,martijnvisser,taoran,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29796,,,,,FLINK-31968,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 27 03:55:10 UTC 2023,,,,,,,,,,"0|z17f1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 07:15;ana4;I can't reproduce this problem in macOS Monterey 12.4 & M1Chip & JDK8 & Python 3.9.;;;","23/Sep/22 08:20;dianfu;[~lemonjing] Hi, could you provide more information? Otherwise, I'm tending to close this issue as ""Cannot Produced"";;;","26/Sep/22 07:07;grimsby;Hi,

I've got this same issue, but thats because I'm using protobuf >3.18 (3.19.5). The reason is this:
{code:java}
// pyflink/fn_execution/flink_fn_execution_pb2.py
...
DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'...')
... {code}
DESCRIPTOR will always be None, since AddSerializedFile in protbuf <3.18 will never return anything:
{code:java}
def AddSerializedFile(self, serialized_file_desc_proto):
    """"""Adds the FileDescriptorProto and its types to this pool.
    Args:
      serialized_file_desc_proto (bytes): A bytes string, serialization of the
        :class:`FileDescriptorProto` to add.
    """"""

    # pylint: disable=g-import-not-at-top
    from google.protobuf import descriptor_pb2
    file_desc_proto = descriptor_pb2.FileDescriptorProto.FromString(
        serialized_file_desc_proto)
    self.Add(file_desc_proto) {code}
Am I missing something, or has pull request 20685 introduced this bug?

{{}};;;","26/Sep/22 09:13;grimsby;This is due to packages from pip is used instead of conda. Adding the dev-requirements.txt from conda instead of pip will resolce this issue.;;;","12/Oct/22 03:39;taoran;[~grimsby] Yes, you are right. when i change to conda, it works well.  thanks.;;;","18/Nov/22 07:58;hxbks2ks;Merged into master via e5762a558f3697294cd73da4247a741fc6f73456
Merged into release-1.16 via 4f0ffa0ddfeffbe25435595d08825cada713ac44;;;","07/Feb/23 09:09;hxbks2ks;If you encounter the following stack trace in m1

{code:java}
<frozen importlib._bootstrap>:219: RuntimeWarning: apache_beam.coders.coder_impl.StreamCoderImpl size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject
Traceback (most recent call last):
  File ""/Users/chucheng/GitLab/pyflink-demo/table/streaming/word_count.py"", line 129, in <module>
    word_count(known_args.input, known_args.output)
  File ""/Users/chucheng/GitLab/pyflink-demo/table/streaming/word_count.py"", line 49, in word_count
    t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 121, in create
    return TableEnvironment(j_tenv)
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 100, in __init__
    self._open()
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 1637, in _open
    startup_loopback_server()
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 1628, in startup_loopback_server
    from pyflink.fn_execution.beam.beam_worker_pool_service import \
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_worker_pool_service.py"", line 44, in <module>
    from pyflink.fn_execution.beam import beam_sdk_worker_main  # noqa # pylint: disable=unused-import
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_sdk_worker_main.py"", line 21, in <module>
    import pyflink.fn_execution.beam.beam_operations # noqa # pylint: disable=unused-import
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_operations.py"", line 27, in <module>
    from pyflink.fn_execution.state_impl import RemoteKeyedStateBackend, RemoteOperatorStateBackend
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/state_impl.py"", line 33, in <module>
    from pyflink.fn_execution.beam.beam_coders import FlinkCoder
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_coders.py"", line 27, in <module>
    from pyflink.fn_execution.beam import beam_coder_impl_fast as beam_coder_impl
  File ""pyflink/fn_execution/beam/beam_coder_impl_fast.pyx"", line 1, in init pyflink.fn_execution.beam.beam_coder_impl_fast
KeyError: '__pyx_vtable__'
{code}

you can execute the following command to solve the problem

{code:java}
pip install cython==0.29.24
brew install gcc
pip uninstall apache-flink
pip uninstall apache-beam
pip install apache-flink
{code}

;;;","14/Feb/23 11:32;hxbks2ks;Merged into master via 1e8d528e293da7ab990f1406fcd209ff4fb177b3
Merged into release-1.17 via 556a3faae9f147edf1c263e57a6661fc88b8f7ab
Merged into release-1.16 via 1e1461945d633486ee41f7137743837d0756e9b6;;;","26/Apr/23 14:40;dianfu;Reopen it as one user reported an issue related to Mac M1 on 1.17.0 (https://apache-flink.slack.com/archives/C03G7LJTS2G/p1679904702297129):

{code}
Traceback (most recent call last):
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 287, in _execute
    response = task()
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 360, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 597, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 634, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 1004, in process_bundle
    element.data)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 226, in process_encoded
    input_stream, True)
  File ""apache_beam/coders/coder_impl.py"", line 1519, in apache_beam.coders.coder_impl.ParamWindowedValueCoderImpl.decode_from_stream
  File ""apache_beam/coders/coder_impl.py"", line 1520, in apache_beam.coders.coder_impl.ParamWindowedValueCoderImpl.decode_from_stream
  File ""apache_beam/coders/coder_impl.py"", line 135, in apache_beam.coders.coder_impl.CoderImpl.decode_from_stream
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 34, in decode_from_stream
    return self._value_coder.decode_from_stream(in_stream, nested)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 58, in decode_from_stream
    return self._value_coder.decode_from_stream(data_input_stream)
TypeError: Argument 'input_stream' has incorrect type (expected pyflink.fn_execution.stream_fast.LengthPrefixInputStream, got BeamInputStream)
{code};;;","27/Apr/23 03:55;dianfu;Fixed in:
 - master via a3368635e3d06f764d144f8c8e2e06e499e79665
 - release-1.17 via 52c9742eed7128284278b07c40785bf1c4e30139
 - release-1.16 via ae998dda4ee60e2268a8c4f8bfdbbd46cc1a0746;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hybrid shuffle consumer thread and upstream thread may have dead lock ,FLINK-28785,13474817,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,03/Aug/22 06:40,09/Aug/22 08:34,13/Jul/23 08:13,09/Aug/22 08:34,1.16.0,,,,,,,1.16.0,,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,"In hybrid shuffle mode, subpartition view lock will be acquired by consumer thread, and further wait the read lock of MemoryDataManager. But MemoryDataManager may acquire write lock to make a global spilling decision, and then wait subpartition view lock to get consuming offset. In this case, deadlock will occurs.

consumer thread : acqurie subpartition lock -> wait read lock.

upstream thread  : acquire write lock -> wait subpartition lock.",,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 09 08:34:09 UTC 2022,,,,,,,,,,"0|z17f0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 08:34;xtsong;master (1.16): b442394c65b1e924c4b5710a9453e3c7eacf05b5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Class ConfigOptions's example code have error.,FLINK-28783,13474786,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Aiden Gong,Aiden Gong,Aiden Gong,03/Aug/22 02:19,11/Aug/22 08:20,13/Jul/23 08:13,11/Aug/22 08:20,1.15.1,,,,,,,1.16.0,,,,,API / Core,,,,,,,0,pull-request-available,,,,!image-2022-08-03-10-18-54-965.png!,,Aiden Gong,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/22 02:18;Aiden Gong;image-2022-08-03-10-18-54-965.png;https://issues.apache.org/jira/secure/attachment/13047566/image-2022-08-03-10-18-54-965.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 11 08:20:18 UTC 2022,,,,,,,,,,"0|z17etk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/22 08:20;leonard;Fixed in master(1.16): 3143526637ef772b43ab413e1cf0a63e566e6dab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unclean shade in flink-table-store-dist,FLINK-28775,13474643,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,02/Aug/22 07:26,02/Aug/22 10:49,13/Jul/23 08:13,02/Aug/22 10:49,,,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"java.lang.NoSuchFieldError: callback
  at org.apache.flink.table.store.kafka.KafkaSinkFunction.open (KafkaSinkFunction.java:71)

Error when using table store with kafka sql-jar.

We should shade more for flink-connector-kafka.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 02 10:49:53 UTC 2022,,,,,,,,,,"0|z17dxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/22 10:49;lzljs3620320;master: dfaa44cbc1b97ef00cb653569e60ae20280fd0ac
release-0.2: 5862320788f323ca1ae46c2fa6ab6cb4e4a433e7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assign speculative execution attempt with correct CREATED timestamp,FLINK-28771,13474618,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhuzh,zhuzh,zhuzh,02/Aug/22 03:48,03/Aug/22 02:43,13/Jul/23 08:13,03/Aug/22 02:43,1.16.0,,,,,,,1.16.0,,,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"Currently, newly created speculative execution attempt is assigned with a wrong CREATED timestamp in SpeculativeScheduler. We need to fix it.",,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 03 02:43:06 UTC 2022,,,,,,,,,,"0|z17ds8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 02:43;zhuzh;Fixed via e1a74df4427e99f4b0f3aaa4e8f4f5ff7cbd044e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlGatewayServiceITCase.testCancelOperation failed with AssertionFailedError,FLINK-28767,13474509,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,hxbks2ks,hxbks2ks,01/Aug/22 11:45,12/Aug/22 02:43,13/Jul/23 08:13,12/Aug/22 02:43,1.16.0,,,,,,,1.16.0,,,,,Table SQL / Gateway,,,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-07-31T03:32:09.3148584Z Jul 31 03:32:09 [ERROR] Tests run: 16, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 7.268 s <<< FAILURE! - in org.apache.flink.table.gateway.service.SqlGatewayServiceITCase
2022-07-31T03:32:09.3205977Z Jul 31 03:32:09 [ERROR] org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.testCancelOperation  Time elapsed: 0.008 s  <<< FAILURE!
2022-07-31T03:32:09.3207151Z Jul 31 03:32:09 org.opentest4j.AssertionFailedError: expected: <org.apache.flink.table.gateway.api.results.OperationInfo@ab4fed9f> but was: <org.apache.flink.table.gateway.api.results.OperationInfo@ea9e78d5>
2022-07-31T03:32:09.3207956Z Jul 31 03:32:09 	at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:55)
2022-07-31T03:32:09.3211582Z Jul 31 03:32:09 	at org.junit.jupiter.api.AssertionUtils.failNotEqual(AssertionUtils.java:62)
2022-07-31T03:32:09.3212267Z Jul 31 03:32:09 	at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:182)
2022-07-31T03:32:09.3212945Z Jul 31 03:32:09 	at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:177)
2022-07-31T03:32:09.3213607Z Jul 31 03:32:09 	at org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:1141)
2022-07-31T03:32:09.3216761Z Jul 31 03:32:09 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.testCancelOperation(SqlGatewayServiceITCase.java:245)
2022-07-31T03:32:09.3217645Z Jul 31 03:32:09 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-31T03:32:09.3218243Z Jul 31 03:32:09 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-31T03:32:09.3218971Z Jul 31 03:32:09 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-31T03:32:09.3219622Z Jul 31 03:32:09 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-31T03:32:09.3227901Z Jul 31 03:32:09 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-07-31T03:32:09.3228714Z Jul 31 03:32:09 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-07-31T03:32:09.3229561Z Jul 31 03:32:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-07-31T03:32:09.3230353Z Jul 31 03:32:09 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-07-31T03:32:09.3231409Z Jul 31 03:32:09 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-07-31T03:32:09.3235059Z Jul 31 03:32:09 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2022-07-31T03:32:09.3236100Z Jul 31 03:32:09 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-07-31T03:32:09.3236978Z Jul 31 03:32:09 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-07-31T03:32:09.3237841Z Jul 31 03:32:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-07-31T03:32:09.3241384Z Jul 31 03:32:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-07-31T03:32:09.3242557Z Jul 31 03:32:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-07-31T03:32:09.3243372Z Jul 31 03:32:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-07-31T03:32:09.3244155Z Jul 31 03:32:09 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-07-31T03:32:09.3244899Z Jul 31 03:32:09 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-07-31T03:32:09.3249715Z Jul 31 03:32:09 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-07-31T03:32:09.3250594Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-31T03:32:09.3251432Z Jul 31 03:32:09 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-07-31T03:32:09.3252291Z Jul 31 03:32:09 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-07-31T03:32:09.3253055Z Jul 31 03:32:09 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-07-31T03:32:09.3256625Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-07-31T03:32:09.3257465Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-31T03:32:09.3258297Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-07-31T03:32:09.3259062Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-07-31T03:32:09.3262851Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-07-31T03:32:09.3263723Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-31T03:32:09.3264528Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-07-31T03:32:09.3265259Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-07-31T03:32:09.3266191Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-07-31T03:32:09.3270216Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-07-31T03:32:09.3271316Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-07-31T03:32:09.3272431Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-07-31T03:32:09.3273258Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-31T03:32:09.3277605Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-07-31T03:32:09.3278454Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-07-31T03:32:09.3279180Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-07-31T03:32:09.3280019Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-31T03:32:09.3280980Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-07-31T03:32:09.3287071Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-07-31T03:32:09.3288036Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-07-31T03:32:09.3289128Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-07-31T03:32:09.3290078Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-07-31T03:32:09.3294254Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-31T03:32:09.3295128Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-07-31T03:32:09.3295912Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-07-31T03:32:09.3296679Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-07-31T03:32:09.3297504Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-31T03:32:09.3305416Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-07-31T03:32:09.3306251Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-07-31T03:32:09.3307191Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-07-31T03:32:09.3308099Z Jul 31 03:32:09 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-07-31T03:32:09.3308702Z Jul 31 03:32:09 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-07-31T03:32:09.3315595Z Jul 31 03:32:09 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-07-31T03:32:09.3316450Z Jul 31 03:32:09 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-07-31T03:32:09.3317133Z Jul 31 03:32:09 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38962&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be
",,fsk119,hxbks2ks,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 12 02:43:15 UTC 2022,,,,,,,,,,"0|z17d40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/22 11:45;hxbks2ks;cc [~fsk119];;;","04/Aug/22 06:13;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39229&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be;;;","08/Aug/22 08:16;hxbks2ks;https://dev.azure.com/hxbks2ks/FLINK-TEST/_build/results?buildId=1963&view=logs&j=43a593e7-535d-554b-08cc-244368da36b4&t=82d122c0-8bbf-56f3-4c0d-8e3d69630d0f;;;","10/Aug/22 05:20;luoyuxia;[https://dev.azure.com/leonardBang/Azure_CI/_build/results?buildId=719&view=logs&j=43a593e7-535d-554b-08cc-244368da36b4&t=82d122c0-8bbf-56f3-4c0d-8e3d69630d0f];;;","10/Aug/22 07:52;fsk119;After investigation, I find the main cause is when the test thread cancel the execution, the SqlGatewayService doesn't wait for the execution thread exits. When the execution thread is interrupted, it will record the error. Here the test cancels the operation and then fetch the OperationStatus. It means it's possible to get the InterruptedException or not.

 

I think we don't need to record the exception if the execution is interrupted by the users. Users knows what happens. ;;;","12/Aug/22 02:43;fsk119;Merged into master: 8813a5ac914835ce76eeeeb39f9e0bf0e1760af3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnalignedCheckpointStressITCase.runStressTest failed with NoSuchFileException,FLINK-28766,13474505,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,hxbks2ks,hxbks2ks,01/Aug/22 11:40,07/Dec/22 12:29,13/Jul/23 08:13,07/Dec/22 12:29,1.16.0,1.17.0,,,,,,1.16.1,1.17.0,,,,Runtime / Checkpointing,,,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-08-01T01:36:16.0563880Z Aug 01 01:36:16 [ERROR] org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase.runStressTest  Time elapsed: 12.579 s  <<< ERROR!
2022-08-01T01:36:16.0565407Z Aug 01 01:36:16 java.io.UncheckedIOException: java.nio.file.NoSuchFileException: /tmp/junit1058240190382532303/f0f99754a53d2c4633fed75011da58dd/chk-7/61092e4a-5b9a-4f56-83f7-d9960c53ed3e
2022-08-01T01:36:16.0566296Z Aug 01 01:36:16 	at java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:88)
2022-08-01T01:36:16.0566972Z Aug 01 01:36:16 	at java.nio.file.FileTreeIterator.hasNext(FileTreeIterator.java:104)
2022-08-01T01:36:16.0567600Z Aug 01 01:36:16 	at java.util.Iterator.forEachRemaining(Iterator.java:115)
2022-08-01T01:36:16.0568290Z Aug 01 01:36:16 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
2022-08-01T01:36:16.0569172Z Aug 01 01:36:16 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-08-01T01:36:16.0569877Z Aug 01 01:36:16 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-08-01T01:36:16.0570554Z Aug 01 01:36:16 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
2022-08-01T01:36:16.0571371Z Aug 01 01:36:16 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-08-01T01:36:16.0572417Z Aug 01 01:36:16 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546)
2022-08-01T01:36:16.0573618Z Aug 01 01:36:16 	at org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase.discoverRetainedCheckpoint(UnalignedCheckpointStressITCase.java:289)
2022-08-01T01:36:16.0575187Z Aug 01 01:36:16 	at org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase.runAndTakeExternalCheckpoint(UnalignedCheckpointStressITCase.java:262)
2022-08-01T01:36:16.0576540Z Aug 01 01:36:16 	at org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase.runStressTest(UnalignedCheckpointStressITCase.java:158)
2022-08-01T01:36:16.0577684Z Aug 01 01:36:16 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-01T01:36:16.0578546Z Aug 01 01:36:16 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-01T01:36:16.0579374Z Aug 01 01:36:16 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-01T01:36:16.0580298Z Aug 01 01:36:16 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-01T01:36:16.0581243Z Aug 01 01:36:16 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-08-01T01:36:16.0582029Z Aug 01 01:36:16 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-08-01T01:36:16.0582766Z Aug 01 01:36:16 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-08-01T01:36:16.0583488Z Aug 01 01:36:16 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-08-01T01:36:16.0584203Z Aug 01 01:36:16 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-08-01T01:36:16.0585087Z Aug 01 01:36:16 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-08-01T01:36:16.0585778Z Aug 01 01:36:16 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-08-01T01:36:16.0586482Z Aug 01 01:36:16 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-08-01T01:36:16.0587155Z Aug 01 01:36:16 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-08-01T01:36:16.0587809Z Aug 01 01:36:16 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-08-01T01:36:16.0588434Z Aug 01 01:36:16 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-01T01:36:16.0589203Z Aug 01 01:36:16 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-08-01T01:36:16.0589867Z Aug 01 01:36:16 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-08-01T01:36:16.0590672Z Aug 01 01:36:16 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-08-01T01:36:16.0591534Z Aug 01 01:36:16 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-08-01T01:36:16.0592209Z Aug 01 01:36:16 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-08-01T01:36:16.0592832Z Aug 01 01:36:16 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-08-01T01:36:16.0593462Z Aug 01 01:36:16 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-08-01T01:36:16.0594097Z Aug 01 01:36:16 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-08-01T01:36:16.0594951Z Aug 01 01:36:16 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-08-01T01:36:16.0595605Z Aug 01 01:36:16 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-08-01T01:36:16.0596215Z Aug 01 01:36:16 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-08-01T01:36:16.0596820Z Aug 01 01:36:16 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-01T01:36:16.0597438Z Aug 01 01:36:16 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-08-01T01:36:16.0598029Z Aug 01 01:36:16 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-08-01T01:36:16.0598579Z Aug 01 01:36:16 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-08-01T01:36:16.0599296Z Aug 01 01:36:16 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-08-01T01:36:16.0600030Z Aug 01 01:36:16 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-08-01T01:36:16.0600750Z Aug 01 01:36:16 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-08-01T01:36:16.0601515Z Aug 01 01:36:16 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-08-01T01:36:16.0602352Z Aug 01 01:36:16 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-08-01T01:36:16.0603185Z Aug 01 01:36:16 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-08-01T01:36:16.0604176Z Aug 01 01:36:16 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-08-01T01:36:16.0605378Z Aug 01 01:36:16 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-08-01T01:36:16.0606160Z Aug 01 01:36:16 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-08-01T01:36:16.0606871Z Aug 01 01:36:16 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-08-01T01:36:16.0607666Z Aug 01 01:36:16 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-08-01T01:36:16.0608513Z Aug 01 01:36:16 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-08-01T01:36:16.0609407Z Aug 01 01:36:16 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-08-01T01:36:16.0610227Z Aug 01 01:36:16 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-08-01T01:36:16.0611053Z Aug 01 01:36:16 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-08-01T01:36:16.0611831Z Aug 01 01:36:16 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-08-01T01:36:16.0612546Z Aug 01 01:36:16 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-08-01T01:36:16.0613229Z Aug 01 01:36:16 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-08-01T01:36:16.0614034Z Aug 01 01:36:16 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-08-01T01:36:16.0615450Z Aug 01 01:36:16 Caused by: java.nio.file.NoSuchFileException: /tmp/junit1058240190382532303/f0f99754a53d2c4633fed75011da58dd/chk-7/61092e4a-5b9a-4f56-83f7-d9960c53ed3e
2022-08-01T01:36:16.0616258Z Aug 01 01:36:16 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
2022-08-01T01:36:16.0616928Z Aug 01 01:36:16 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
2022-08-01T01:36:16.0617591Z Aug 01 01:36:16 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
2022-08-01T01:36:16.0618293Z Aug 01 01:36:16 	at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
2022-08-01T01:36:16.0619099Z Aug 01 01:36:16 	at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
2022-08-01T01:36:16.0619817Z Aug 01 01:36:16 	at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
2022-08-01T01:36:16.0620450Z Aug 01 01:36:16 	at java.nio.file.Files.readAttributes(Files.java:1737)
2022-08-01T01:36:16.0621066Z Aug 01 01:36:16 	at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219)
2022-08-01T01:36:16.0621704Z Aug 01 01:36:16 	at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276)
2022-08-01T01:36:16.0622316Z Aug 01 01:36:16 	at java.nio.file.FileTreeWalker.next(FileTreeWalker.java:372)
2022-08-01T01:36:16.0622968Z Aug 01 01:36:16 	at java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:84)
2022-08-01T01:36:16.0623482Z Aug 01 01:36:16 	... 60 more
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38984&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9
",,akalashnikov,fanrui,hxb,hxbks2ks,kevin.cyj,leonard,mapohl,martijnvisser,pnowojski,samrat007,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28872,,,,,,,,,FLINK-28898,,FLINK-30107,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 07 12:29:18 UTC 2022,,,,,,,,,,"0|z17d34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 01:53;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39120&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","08/Aug/22 02:48;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39369&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=7942;;;","10/Aug/22 03:32;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39779&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","12/Aug/22 09:32;zhuzh;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39908&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","05/Sep/22 02:00;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40645&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9;;;","13/Sep/22 07:15;martijnvisser;[~pnowojski] Any idea who could help out with this test instability? ;;;","19/Sep/22 12:35;akalashnikov;[~martijnvisser], I will take a look;;;","19/Sep/22 12:48;martijnvisser;Thanks [~akalashnikov];;;","21/Sep/22 09:58;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41188&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10741;;;","10/Oct/22 03:47;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41753&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a;;;","21/Oct/22 03:41;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42272&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10622;;;","01/Nov/22 14:22;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42701&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10538;;;","21/Nov/22 09:29;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43254&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=f2c100be-250b-5e85-7bbe-176f68fcddc5;;;","22/Nov/22 07:45;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43367&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a;;;","25/Nov/22 03:52;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43451&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10267;;;","25/Nov/22 03:53;mapohl;[~akalashnikov] any updates on that issue. FYI: We're seeing other tests failing with a similar issue where temporary files are not present anymore (e.g. FLINK-28898);;;","28/Nov/22 10:34;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43521&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=8710;;;","04/Dec/22 13:58;pnowojski;Merged to master as 1ddaa2a460d and 9e1cefe2509;;;","05/Dec/22 13:06;mapohl;I created the missing [1.16 backport PR|https://github.com/apache/flink/pull/21456]. May one of you approve that PR, [~akalashnikov]/[~pnowojski]? Or was there a reason why you didn't proceed with the backport? We observed the test failure in both {{master}} and {{release-1.16}}.;;;","07/Dec/22 09:20;akalashnikov;[~mapohl] , thanks for creating the backport. There are no reasons to have it. I just missed this part. I've approved your PR.;;;","07/Dec/22 12:29;mapohl;Cool, thanks for approving the PR. Here's the summary of the ticket resolution:

master: 
* 1ddaa2a460dd8def96742d0d94b1a86df4ae9c81
* 9e1cefe250972b213ac69322531a4de69ca0e1a6

1.16: 
* 98647fba9a0c04151135a2bbfe611bba9956aea6
* 56273f307c6e5e1d25d712a931ffc352d0c9021e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive table store support can't be fully enabled by ADD JAR statement,FLINK-28760,13474439,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,01/Aug/22 06:21,02/Aug/22 02:28,13/Jul/23 08:13,02/Aug/22 02:28,table-store-0.2.0,table-store-0.3.0,,,,,,table-store-0.2.0,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,"Current table store Hive document states that, to enable support in Hive, we should use ADD JAR statement to add hive connector jar.

However some types of queries, for example join statements in MR engine, may still throw ClassNotFound exception. The correct way to enable table store support is to create an auxlib directory under the root directory of Hive and copy jar into that directory.",,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 02 02:28:57 UTC 2022,,,,,,,,,,"0|z17cog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/22 02:28;lzljs3620320;master: adcf39744c7a2c7b3ec143d1d217eaa02efa10a6
release-0.2: da30ce533b81dc4c8ddd8b37624924e469f7a39a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error when switching from stateless to savepoint upgrade mode,FLINK-28755,13474373,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gyfora,gyfora,31/Jul/22 09:13,21/Sep/22 15:07,13/Jul/23 08:13,21/Sep/22 15:07,kubernetes-operator-1.1.0,kubernetes-operator-1.2.0,,,,,,,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"When using the savepoint upgrade mode the state.savepoints.dir currently comes from the currently deployed spec / config.

This causes a nullpointer exception when switching to savepoint upgrade mode from stateless if state.savepoints.dir was previously undefined: 


{noformat}
org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:59)
org.apache.flink.kubernetes.operator.service.AbstractFlinkService.cancelJob(AbstractFlinkService.java:279)
org.apache.flink.kubernetes.operator.service.NativeFlinkService.cancelJob(NativeFlinkService.java:93)
org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.cancelJob(ApplicationReconciler.java:172)
org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.cancelJob(ApplicationReconciler.java:52)
org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractJobReconciler.reconcileSpecChange(AbstractJobReconciler.java:108)
org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:148)
org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:56)
org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:115){noformat}",,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 21 15:07:53 UTC 2022,,,,,,,,,,"0|z17ca0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/22 15:07;gyfora;merged to main 1eca2c5e6a62bd9f1c9e752191f8a7477903d73c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Table.to_pandas fails with lit(""xxx"")",FLINK-28742,13474147,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,xuannan,xuannan,29/Jul/22 08:11,10/Jan/23 01:43,13/Jul/23 08:13,10/Jan/23 01:42,1.15.0,,,,,,,1.15.4,1.16.1,1.17.0,,,API / Python,,,,,,,0,pull-request-available,,,,"Table.to_pandas method throws the following exception when the table contains lit(""anyString"").

 
{code:none}
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame.
: java.lang.UnsupportedOperationException: Python vectorized UDF doesn't support logical type CHAR(3) NOT NULL currently.
    at org.apache.flink.table.runtime.arrow.ArrowUtils$LogicalTypeToArrowTypeConverter.defaultMethod(ArrowUtils.java:743)
    at org.apache.flink.table.runtime.arrow.ArrowUtils$LogicalTypeToArrowTypeConverter.defaultMethod(ArrowUtils.java:617)
    at org.apache.flink.table.types.logical.utils.LogicalTypeDefaultVisitor.visit(LogicalTypeDefaultVisitor.java:62)
    at org.apache.flink.table.types.logical.CharType.accept(CharType.java:148)
    at org.apache.flink.table.runtime.arrow.ArrowUtils.toArrowField(ArrowUtils.java:189)
    at org.apache.flink.table.runtime.arrow.ArrowUtils.lambda$toArrowSchema$0(ArrowUtils.java:180)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
    at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
    at org.apache.flink.table.runtime.arrow.ArrowUtils.toArrowSchema(ArrowUtils.java:181)
    at org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame(ArrowUtils.java:483)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
    at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
    at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:748)
 {code}
 

The code to reproduce the problem
{code:python}
env = StreamExecutionEnvironment.get_execution_environment()
t_env = StreamTableEnvironment.create(env)

src_table = t_env.from_data_stream(
    env.from_collection([1, 2], type_info=BasicTypeInfo.INT_TYPE_INFO())
)

table = src_table.select(expr.lit(""123""))
# table.execute().print()
print(table.to_pandas()){code}",,dianfu,hxbks2ks,xuannan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 10 01:42:58 UTC 2023,,,,,,,,,,"0|z17aw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/23 01:42;dianfu;Fixed in:
- master via 0f14f13122e2df5e4a2396871936a5b6ff47cd12
- release-1.16 via 2b77c837636e1df2d527f0097726481ce198362b
- release-1.15 via 310b8fb4206fe665e9c70149df37ffacf9e20b91

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SinkSavepointITCase.testRecoverFromSavepoint is unstable,FLINK-28718,13473771,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,27/Jul/22 11:35,19/Mar/23 05:47,13/Jul/23 08:13,19/Mar/23 05:47,,,,,,,,table-store-0.4.0,,,,,Table Store,,,,,,,0,,,,,"https://github.com/apache/flink-table-store/runs/7537817210?check_suite_focus=true


{code:java}
Error:  Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 185.274 s <<< FAILURE! - in org.apache.flink.table.store.connector.sink.SinkSavepointITCase
Error:  testRecoverFromSavepoint  Time elapsed: 180.157 s  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 180000 milliseconds
	at java.lang.Thread.sleep(Native Method)
	at org.apache.flink.table.store.connector.sink.SinkSavepointITCase.testRecoverFromSavepoint(SinkSavepointITCase.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:750)
{code}
",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-07-27 11:35:30.0,,,,,,,,,,"0|z178kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store Hive connector will throw exception if primary key fields are not selected,FLINK-28717,13473770,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,TsReaper,TsReaper,27/Jul/22 11:24,28/Jul/22 02:31,13/Jul/23 08:13,28/Jul/22 02:31,table-store-0.2.0,table-store-0.3.0,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"Table Store Hive connector implement projection pushdown by reading desired fields and setting other unread fields to null. However the nullability of primary key fields are not null, so `RowData.FieldGetter` will not check for null values for these types. This may cause exception when primary key fields are not selected and are set to null.",,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 28 02:31:44 UTC 2022,,,,,,,,,,"0|z178k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jul/22 02:31;lzljs3620320;master: 334c6159e546ba54aa1c38154882122758604d3d
release-0.2: ceab29fda7512b550a2de4d9cff0a90a8cd1d3c8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 uploading multiple files/form datas fail randomly when use rest api,FLINK-28716,13473757,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,hehuiyuan,hehuiyuan,27/Jul/22 10:29,27/Jul/22 12:02,13/Jul/23 08:13,27/Jul/22 11:29,1.14.0,,,,,,,1.15.0,,,,,,,,,,,,0,,,,,"It can happen error randomly when use `jars/upload` rest api.

[https://github.com/eclipse-vertx/vert.x/issues/3949]

Can you help me to look at this question?
{code:java}
java.lang.IndexOutOfBoundsException: index: 1804, length: 1 (expected: range(0, 1804))
    at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.checkRangeBounds(AbstractByteBuf.java:1390)
    at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.checkIndex0(AbstractByteBuf.java:1397)
    at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.checkIndex(AbstractByteBuf.java:1384)
    at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.checkIndex(AbstractByteBuf.java:1379)
    at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.getByte(AbstractByteBuf.java:355)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostBodyUtil.findDelimiter(HttpPostBodyUtil.java:238)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.loadDataMultipartOptimized(HttpPostMultipartRequestDecoder.java:1172)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.getFileUpload(HttpPostMultipartRequestDecoder.java:926)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.decodeMultipart(HttpPostMultipartRequestDecoder.java:572)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.findMultipartDisposition(HttpPostMultipartRequestDecoder.java:797)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.decodeMultipart(HttpPostMultipartRequestDecoder.java:511)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.findMultipartDelimiter(HttpPostMultipartRequestDecoder.java:663)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.decodeMultipart(HttpPostMultipartRequestDecoder.java:498)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.parseBodyMultipart(HttpPostMultipartRequestDecoder.java:463)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.parseBody(HttpPostMultipartRequestDecoder.java:432)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.offer(HttpPostMultipartRequestDecoder.java:347)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.offer(HttpPostMultipartRequestDecoder.java:54)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostRequestDecoder.offer(HttpPostRequestDecoder.java:223)
    at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:176)
    at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:71) {code}",,hehuiyuan,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25016,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 27 11:28:06 UTC 2022,,,,,,,,,,"0|z178hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/22 11:18;chesnay;Which Flink version did you use? According to the issue you linked this was a Netty bug that has been fixed in 4.1.66, and thus should not occur in Flink 1.15.0.;;;","27/Jul/22 11:22;hehuiyuan;Hi [~chesnay] , flink1.14.

Yes, it is fixed in 4.1.66 version

https://github.com/netty/netty/pull/11335;;;","27/Jul/22 11:28;chesnay;Then you will need to upgrade to 1.15.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Describe partitioned spark table lost partition info,FLINK-28703,13473716,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,27/Jul/22 08:14,27/Jul/22 09:01,13/Jul/23 08:13,27/Jul/22 09:00,table-store-0.2.0,,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"SparkTable should override #partitioning

!image-2022-07-27-16-17-14-240.png|width=740,height=364!",,lzljs3620320,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/22 08:17;qingyue;image-2022-07-27-16-17-14-240.png;https://issues.apache.org/jira/secure/attachment/13047278/image-2022-07-27-16-17-14-240.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 27 09:00:46 UTC 2022,,,,,,,,,,"0|z17888:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/22 09:00;lzljs3620320;master: a251033ece720a5fc571f36f5335c83715dc07de
release-0.2: 625dbabbd75737399777760b9e0fa6cdda2b7286;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minimize the explosion range during failover for hybrid shuffle,FLINK-28701,13473679,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,27/Jul/22 06:05,10/Aug/22 01:35,13/Jul/23 08:13,10/Aug/22 01:35,1.16.0,,,,,,,1.16.0,,,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"In hybrid shuffle mode, there are currently two strategies to control spilling. For the full spilling strategy, the data is guaranteed to be persistent to the disk after task finished. When a failover occurs, if the upstream has been finished, the data should be recovered directly from the disk file without re-compute. For selective spilling strategy, the entire topology must be restarted.",,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 10 01:35:21 UTC 2022,,,,,,,,,,"0|z17800:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/22 01:35;xtsong;master (1.16): 56e91700ca56f5535f677314167b94b476eb6c3f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table store sink fails to commit for Flink 1.14 batch job,FLINK-28700,13473678,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,TsReaper,TsReaper,27/Jul/22 05:58,28/Jul/22 04:54,13/Jul/23 08:13,28/Jul/22 04:54,table-store-0.2.0,table-store-0.3.0,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,We can't get configurations from DummyStreamExecutionEnvironment in Flink 1.14 (this is fixed by FLINK-26709 in Flink 1.15) so we have to use Java reflection to check if this execution environment is for batch job or for streaming job.,,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 28 04:54:02 UTC 2022,,,,,,,,,,"0|z177zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jul/22 04:54;lzljs3620320;master: bc451951a0cfbdd5f7e4472df53430b36f3a9df7
release-0.2: 77230ddf70f809c578ab54c8e2881be774da044c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to send partition request to restarted taskmanager,FLINK-28695,13473576,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fanrui,simonas.gelazevicius@vinted.com,simonas.gelazevicius@vinted.com,26/Jul/22 14:00,21/Nov/22 18:52,13/Jul/23 08:13,21/Nov/22 18:51,1.15.0,1.15.1,,,,,,1.15.4,1.16.1,1.17.0,,,Deployment / Kubernetes,Runtime / Network,,,,,,0,pull-request-available,,,,"After upgrade to *1.15.1* we started getting error while running JOB

 
{code:java}
org.apache.flink.runtime.io.network.netty.exception.LocalTransportException: Sending the partition request to '/XXX.XXX.XX.32:6121 (#0)' failed.    at org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClient$1.operationComplete(NettyPartitionRequestClient.java:145)    .... {code}
{code:java}
Caused by: org.apache.flink.shaded.netty4.io.netty.channel.StacklessClosedChannelException atrg.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source){code}
After investigation we managed narrow it down to the exact behavior then this issue happens:
 # Deploying JOB on fresh kubernetes session cluster with multiple TaskManagers: TM1 and TM2 is successful. Job has multiple partitions running on both TM1 and TM2.
 # One TaskManager TM2 (XXX.XXX.XX.32) fails for unrelated issue. For example OOM exception.
 # Kubernetes POD with mentioned TaskManager TM2 is restarted. POD retains same IP address as before.
 # JobManager is able to pickup the restarted TM2 (XXX.XXX.XX.32)
 # JOB is restarted because it was running on the failed TaskManager TM2
 # TM1 data channel to TM2 is closed and we get LocalTransportException: Sending the partition request to '/XXX.XXX.XX.32:6121 (#0)' failed during JOB running stage.  
 # When we explicitly delete pod with TM2 it creates new POD with different IP address and JOB is able to start again.

Important to note that we didn't encountered this issue with previous *1.14.4* version and TaskManager restarts didn't cause such error.

Please note attached kubernetes deployments and reduced logs from JobManager. TaskManager logs did show errors before error, but doesn't show anything significant after restart.

EDIT:
{quote}
Setting {{taskmanager.network.max-num-tcp-connections}} to a very high number workarounds the problem
{quote}",,aitozi,Brian Zhou,fanrui,martijnvisser,nobleyd,pnowojski,simonas.gelazevicius@vinted.com,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15455,,,,,,,,,,,,,"26/Jul/22 13:43;simonas.gelazevicius@vinted.com;deployment.txt;https://issues.apache.org/jira/secure/attachment/13047246/deployment.txt","20/Nov/22 08:16;fanrui;image-2022-11-20-16-16-45-705.png;https://issues.apache.org/jira/secure/attachment/13052899/image-2022-11-20-16-16-45-705.png","21/Nov/22 09:16;fanrui;image-2022-11-21-17-15-58-749.png;https://issues.apache.org/jira/secure/attachment/13052921/image-2022-11-21-17-15-58-749.png","26/Jul/22 13:55;simonas.gelazevicius@vinted.com;job_log.txt;https://issues.apache.org/jira/secure/attachment/13047241/job_log.txt","26/Jul/22 13:49;simonas.gelazevicius@vinted.com;jobmanager_config.txt;https://issues.apache.org/jira/secure/attachment/13047244/jobmanager_config.txt","26/Jul/22 13:54;simonas.gelazevicius@vinted.com;jobmanager_logs.txt;https://issues.apache.org/jira/secure/attachment/13047242/jobmanager_logs.txt","26/Jul/22 13:53;simonas.gelazevicius@vinted.com;pod_restart.txt;https://issues.apache.org/jira/secure/attachment/13047243/pod_restart.txt","26/Jul/22 13:49;simonas.gelazevicius@vinted.com;taskmanager_config.txt;https://issues.apache.org/jira/secure/attachment/13047245/taskmanager_config.txt",,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 21 18:51:18 UTC 2022,,,,,,,,,,"0|z177d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 09:38;Brian Zhou;We have experienced this issue on Flink 1.15, is there anyone that can help on this issue?;;;","07/Nov/22 02:47;Brian Zhou;From investigating the code, looks like it is related to FLINK-22643. While reusing the TCP connections in taskmanagers, it also brings the risk that the reusing the dead connection if the IP address does not change under a Kubernetes session cluster.

Ping the author [~fanrui] for further discussion and help on this issue.;;;","07/Nov/22 02:52;fanrui;Hi [~Brian Zhou] , thanks for your feedback, I will take a look this week. cc [~pnowojski] [~kevin.cyj] ;;;","07/Nov/22 04:04;Weijie Guo;[~Brian Zhou] Just to confirm, if you do not manually delete the POD of TM2, what is the status of job, can this job recover from the exception thrown in 6 or never recover? If so, is the job constantly reporting this errors or is it stuck?;;;","07/Nov/22 05:33;Brian Zhou;Not sure if the original reporter has the same, but from our testing, the job is constantly reporting with this error and keeps restarting.;;;","07/Nov/22 06:17;Weijie Guo;[~Brian Zhou] If you suspect that the problem is caused by connection reuse, you can try to set `taskmanager.network.max-num-tcp-connections` to a large value.What I'm puzzled about is that even if the TCP connection between two TMs is reused, the K8S should theoretically be able to correctly deliver the message to the new pod. And It sounds like you can reproduce this problem, could you provide me with a minimum reproducible approach to do further investigation.;;;","07/Nov/22 07:16;Brian Zhou;Thanks [~Weijie Guo] for your suggestion, we can try that.

For reproduction, in our case, we are experiencing this issue on a Kubernetes node reboot case. In other words, the other steps are the same as the description, except Step 2, we have a node reboot. 

However, unfortunately, we have tested several times during the weekend with the same steps, but cannot reproduce the problem.

 ;;;","17/Nov/22 10:51;nobleyd;We also encounter this problem in production, and task failures will continue to restart because of this problem.;;;","18/Nov/22 16:22;pnowojski;I've assigned the ticket to you [~fanrui]. Please ping me/request a review from me once you have some fix ready.

In the meantime can someone, who can reproduce this issue, try out if setting {{`taskmanager.network.max-num-tcp-connections`}} to a very high number work arounds the problem?;;;","20/Nov/22 08:21;fanrui;Thank you all for your feedback.

After analysis, I think the root cause is as follows:

`CreditBasedPartitionRequestClientHandler#channelInactive` doesn't call the `notifyAllChannelsOfErrorAndClose` when  the `inputChannels.isEmpty()`.

code link: https://github.com/apache/flink/blob/d745f5b3f7a64445854c735668afa9b72edb3fee/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/CreditBasedPartitionRequestClientHandler.java#L126

!image-2022-11-20-16-16-45-705.png!


FLINK-15455 enables TCP connection reuse across multiple jobs. That means `CreditBasedPartitionRequestClientHandler#channelInactive` doesn't call the `notifyAllChannelsOfErrorAndClose` when  after old job stop and before new job start. If the tcp connection is disconnected during this period, the new job should create a new tcp connection. However, the `PartitionRequestClientFactory` doesn't know that the tcp connection is broken, so the new job reuses the failed connection.

Please correct me if there is any mistake. cc [~Weijie Guo]  [~pnowojski]  [~guoyangze];;;","21/Nov/22 02:15;nobleyd;[~pnowojski] Hi, we have the same problem, and adjusting `taskmanager.network.max-num-tcp-connections` words fine.;;;","21/Nov/22 08:54;pnowojski;[~fanrui] What's your proposed solution? If the {{inputChannels}} is empty how would calling {{notifyAllChannelsOfErrorAndClose}} help with anything? Would it help, because it would set {{channelError}} for any future use? ;;;","21/Nov/22 09:20;fanrui;> Would it help, because it would set {{channelError}} for any future use?

[~pnowojski] Yes, you're right. From the notifyAllChannelsOfErrorAndClose code, we can see if inputChannels is empty, nothing to to do. It just set channelError and close ctx.

The channelError will be used within {_}PartitionRequestClientFactory#createPartitionRequestClient{_}. When create the new client, factory will think the old client has error, so create a new client.

If channelError is null, factory will think old client is well, so don't create the new client, and reuse the old client.

 

!image-2022-11-21-17-15-58-749.png|width=1273,height=696!;;;","21/Nov/22 09:37;pnowojski;Thanks for the explanation!;;;","21/Nov/22 18:51;pnowojski;merged commit 6b56505 into apache:release-1.15 
merged commit ecede7a into apache:release-1.16 
merged e7854193816^^^..e7854193816 into apache:master

Thanks [~fanrui] for fixing, and others for reporting/analyzing and confirming the bug and workaround.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check warehouse path in CatalogFactory,FLINK-28692,13473527,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,26/Jul/22 10:48,27/Jul/22 01:35,13/Jul/23 08:13,27/Jul/22 01:35,,,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"* Not exist, automatic creating the directory.
* Exist but it is not directory, throw exception.
* Exist and it is a directory, pass...",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 27 01:35:46 UTC 2022,,,,,,,,,,"0|z17728:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/22 01:35;lzljs3620320;master: d30e7998b136dec0f9aded8376a516a76b467f2e
release-0.2: f149a4f349fa2ced986bec42b3d1f0cd27004343;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UpdateSchema#fromCatalogTable lost column comment,FLINK-28690,13473522,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,qingyue,qingyue,qingyue,26/Jul/22 09:59,19/Mar/23 05:47,13/Jul/23 08:13,19/Mar/23 05:47,table-store-0.2.0,,,,,,,table-store-0.4.0,,,,,Table Store,,,,,,,0,,,,,"The reason is that org.apache.flink.table.api.TableSchema#toPhysicalRowDataType lost column comments, which leads to comparison failure in AbstractTableStoreFactory#buildFileStoreTable.",,lzljs3620320,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28686,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 09 03:09:31 UTC 2023,,,,,,,,,,"0|z17714:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 03:09;lzljs3620320;wait for Flink 1.17;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize Spark documentation to Catalog and Dataset,FLINK-28689,13473511,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,26/Jul/22 09:06,26/Jul/22 10:07,13/Jul/23 08:13,26/Jul/22 10:07,,,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"* Introduce Dataset API.
* Unify table_store and tablestore.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 26 10:07:54 UTC 2022,,,,,,,,,,"0|z176yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 10:07;lzljs3620320;master: c62d721865e08d4f119b118461829261c8f4819b
release-0.2: fbb5807127d7d190c5631f3e006ea6845395897a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BucketSelector is wrong when hashcode is negative,FLINK-28687,13473492,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,26/Jul/22 07:50,26/Jul/22 08:44,13/Jul/23 08:13,26/Jul/22 08:44,,,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,The calculation of bucket should be `Math.abs(hashcode % numBucket)` instead of `hashcode % numBucket`.,,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 26 08:44:18 UTC 2022,,,,,,,,,,"0|z176ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 08:44;lzljs3620320;master: ff697ba12a80f51702a5412c2deb6d4f89a966ff
release-0.2: 0fc2c32db3cf3aab4a0887f24f22eed5b415fb27;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark table with column comment cannot be read/write by Flink,FLINK-28686,13473491,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,qingyue,qingyue,qingyue,26/Jul/22 07:48,26/Jul/22 11:54,13/Jul/23 08:13,26/Jul/22 11:54,table-store-0.2.0,,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"The reason is that org.apache.flink.table.api.TableSchema#toPhysicalRowDataType lost column comments, which leads to comparison failure in AbstractTableStoreFactory#buildFileStoreTable.


!screenshot-1.png|width=662,height=253!

 

The reason why Flink does not encounter this problem is due to the column is lost before persisting the schema",,lzljs3620320,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28690,,,,,,,,,,,,,"26/Jul/22 07:51;qingyue;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13047220/screenshot-1.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 26 11:54:12 UTC 2022,,,,,,,,,,"0|z176u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 11:54;lzljs3620320;master: 0dcf21dc1b6c17057083c144f5bda9ec7dbf0324
release-0.2: 3c76a3916e55271eb08e10d03b4124fb8325d6b8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException at OneHotEncoder.GenerateModelDataOperator.snapshot,FLINK-28684,13473455,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,zhangzp,zhangzp,26/Jul/22 04:13,24/Aug/22 03:17,13/Jul/23 08:13,26/Jul/22 08:06,ml-2.1.0,,,,,,,ml-2.2.0,,,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,"E Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 1 for operator GenerateModelDataOperator -> *anonymous_datastream_source$215*[229] -> ConstraintEnforcer[230] -> TableToDataSteam (1/1)#0. Failure reason: Checkpoint was declined. 
[193|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:194]E at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:269) 
[194|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:195]E at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:173) 
[195|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:196]E at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:348) 
[196|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:197]E at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.checkpointStreamOperator(RegularOperatorChain.java:227) 
[197|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:198]E at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.buildOperatorSnapshotFutures(RegularOperatorChain.java:212) 
[198|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:199]E at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.snapshotState(RegularOperatorChain.java:192) 
[199|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:200]E at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:647) 
[200|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:201]E at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:320) 
[201|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:202]E at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$12(StreamTask.java:1253) 
[202|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:203]E at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) 
[203|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:204]E at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:1241) 
[204|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:205]E at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:1198) 
[205|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:206]E ... 22 more 
[206|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:207]E Caused by: java.lang.NullPointerException 
[207|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:208]E at org.apache.flink.api.common.typeutils.base.GenericArraySerializer.copy(GenericArraySerializer.java:92) 
[208|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:209]E at org.apache.flink.api.common.typeutils.base.GenericArraySerializer.copy(GenericArraySerializer.java:37) 
[209|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:210]E at org.apache.flink.runtime.state.ArrayListSerializer.copy(ArrayListSerializer.java:75) 
[210|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:211]E at org.apache.flink.runtime.state.PartitionableListState.<init>(PartitionableListState.java:65) 
[211|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:212]E at org.apache.flink.runtime.state.PartitionableListState.deepCopy(PartitionableListState.java:79) 
[212|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:213]E at org.apache.flink.runtime.state.DefaultOperatorStateBackendSnapshotStrategy.syncPrepareResources(DefaultOperatorStateBackendSnapshotStrategy.java:77) 
[213|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:214]E at org.apache.flink.runtime.state.DefaultOperatorStateBackendSnapshotStrategy.syncPrepareResources(DefaultOperatorStateBackendSnapshotStrategy.java:36) 
[214|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:215]E at org.apache.flink.runtime.state.SnapshotStrategyRunner.snapshot(SnapshotStrategyRunner.java:77) 
[215|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:216]E at org.apache.flink.runtime.state.DefaultOperatorStateBackend.snapshot(DefaultOperatorStateBackend.java:230) 
[216|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:217]E at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:230) 
[217|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:218]E ... 33 more
 

 

https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true",,yunfengzhou,zhangzp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 26 08:06:18 UTC 2022,,,,,,,,,,"0|z176m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 06:31;yunfengzhou;This problem occurs when a checkpoint barrier reaches a `GenerateModelDataOperator` before the first `StreamRecord` arrives at the operator. In this case, the `maxIndices` variable is not initialized and thus a `null` value is attempted to be stored in the snapshot. The solution is to provide a proper initial value for this variable.;;;","26/Jul/22 06:36;yunfengzhou;It is not found that this problem exists in other variables to be saved in snapshots. They have either adopted the solution above or only update the state when there is a not-null value, as follows.
{code:java}
if (x != null){
    xState.update(x);
}
{code}
Thus it is enough to just add fixes in OneHotEncoder.;;;","26/Jul/22 08:06;zhangzp;fixed on (1) master: 8df38791a0243870d9b11f03ee5ae24cae880d89 (2) release-2.1: ccee3a319f731d29d26049955880ae8fe077573f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer2 Endpoint supports to build with Hive3 ,FLINK-28679,13473430,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,hxbks2ks,hxbks2ks,26/Jul/22 02:08,29/Jul/22 10:30,13/Jul/23 08:13,29/Jul/22 10:30,1.16.0,,,,,,,1.16.0,,,,,Connectors / Hive,Table SQL / Gateway,,,,,,0,pull-request-available,,,,"
{code:java}
2022-07-26T00:28:38.9402708Z [ERROR] COMPILATION ERROR : 
2022-07-26T00:28:38.9403248Z [INFO] -------------------------------------------------------------
2022-07-26T00:28:38.9404706Z [ERROR] /home/vsts/work/1/s/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/endpoint/hive/HiveServer2Endpoint.java:[116,8] org.apache.flink.table.endpoint.hive.HiveServer2Endpoint is not abstract and does not override abstract method SetClientInfo(org.apache.hive.service.rpc.thrift.TSetClientInfoReq) in org.apache.hive.service.rpc.thrift.TCLIService.Iface
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38660&view=logs&j=b1fcf054-9138-5463-c73c-a49979b9ac2a&t=9291ac46-dd95-5135-b799-3839e65a8691
",,hxbks2ks,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28150,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 29 10:30:09 UTC 2022,,,,,,,,,,"0|z176go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 02:08;hxbks2ks;[~fsk119] Could you help take a look?;;;","29/Jul/22 10:30;jark;Fixed in master: aa624025dce378da3c11fc3ab6e8cf91fc94c30e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Primary key should not be null when creating Spark table,FLINK-28677,13473348,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,qingyue,qingyue,qingyue,25/Jul/22 14:54,26/Jul/22 07:35,13/Jul/23 08:13,26/Jul/22 07:34,table-store-0.2.0,,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,Should add a null check on pk fields,,lzljs3620320,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 26 07:34:33 UTC 2022,,,,,,,,,,"0|z175yg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 07:34;lzljs3620320;master: ac950a299c487416b9f558f9067df0b0c3606304
release-0.2: 746f1e28695d74bf29649f93b4cdb5cc127a6586;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation of spark2 is wrong,FLINK-28670,13473261,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,25/Jul/22 10:01,25/Jul/22 10:23,13/Jul/23 08:13,25/Jul/22 10:23,,,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"DDL is not supported. Spark2 just support:
```
spark.read().format(""tablestore"").load(""file:/tmp/warehouse/default.db/myTable"")
```",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 25 10:23:42 UTC 2022,,,,,,,,,,"0|z175f4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/22 10:23;lzljs3620320;master: 
37106b0039d721d492fd802b695d79afde6dd311
b5fdf24b9fca476286055770f1a3c123afefa694
release-0.2: 
ae4fb439bc2a569557a12876d500d87711992134
f6811b8ff0534696d8af56eb3313855646b8c095;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileStoreExpireTest.testExpireWithTime failed,FLINK-28669,13473258,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,lzljs3620320,lzljs3620320,25/Jul/22 09:56,26/Jul/22 08:58,13/Jul/23 08:13,26/Jul/22 08:58,,,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,https://github.com/apache/flink-table-store/runs/7496947558?check_suite_focus=true,,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 26 08:58:35 UTC 2022,,,,,,,,,,"0|z175eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 08:58;lzljs3620320;master: 11053c16b3fd9fc75287f386f2baf19bca112db1
release-0.2: 72c6330470cd994aebefdaedc30f14a40ee6718e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to create Spark table without primary key,FLINK-28668,13473256,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,qingyue,qingyue,qingyue,25/Jul/22 09:51,26/Jul/22 02:38,13/Jul/23 08:13,26/Jul/22 02:00,table-store-0.2.0,,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,The primary key list should be empty when pk is not specified.,,lzljs3620320,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 26 02:00:27 UTC 2022,,,,,,,,,,"0|z175e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 02:00;lzljs3620320;master: e48d9b00ecf29d89aee0c1765a69ca55db86f2f4
release-0.2: a8559f9e159ee821982e93bd7ca7fbd80ff76936;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Alter spark table's primary key should throw exception,FLINK-28666,13473238,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,qingyue,qingyue,qingyue,25/Jul/22 08:35,25/Jul/22 09:22,13/Jul/23 08:13,25/Jul/22 09:22,table-store-0.2.0,,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,,,lzljs3620320,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 25 09:22:30 UTC 2022,,,,,,,,,,"0|z175a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/22 09:22;lzljs3620320;master: 73e20888c528ac31562ce67a5f203e148455e43a
release-0.2: eb1bb9a2f89d0b01a8fdb6809bb95bc521204ad6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction may block job cancelling,FLINK-28662,13473158,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,TsReaper,TsReaper,25/Jul/22 02:51,25/Jul/22 06:56,13/Jul/23 08:13,25/Jul/22 06:55,table-store-0.2.0,table-store-0.3.0,,,,,,table-store-0.2.0,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,"Currently when cancelling a job, we have to wait for current compaction thread to finish. If the compaction takes too long, the task manager may fail due to job cancelling takes more than 180 seconds.",,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 25 06:56:06 UTC 2022,,,,,,,,,,"0|z174s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/22 06:56;lzljs3620320;master: 7c2bf3453c68eabd2935f4a8f0275933779f1dd5
release-0.2: ebe5cfa5887085ab9a66b0a2b1b61fc90d418ff4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JarRunHandler ignores restore mode set in the configuration,FLINK-28651,13472948,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nuafonso,nuafonso,nuafonso,22/Jul/22 16:44,15/Nov/22 09:09,13/Jul/23 08:13,15/Nov/22 09:09,1.15.0,1.15.1,,,,,,1.17.0,,,,,Runtime / REST,,,,,,,0,pull-request-available,stale-assigned,,,"Hello all,

 

I started a Flink cluster with execution.savepoint-restore-mode set to CLAIM. Then, I submitted a job through the REST API without specifying restoreMode in the request. I would expect Flink to use CLAIM, but the job used NO_CLAIM as restore mode.

 

After looking into the source code, I believe the issue comes from the way JarRunHandler picks the default for restoreMode. It directly gets it from the default of execution.savepoint-restore-mode instead of looking into the existing configuration: [https://github.com/apache/flink/blob/release-1.15.1/flink-runtime-web/src/main/java/org/apache/flink/runtime/webmonitor/handlers/JarRunHandler.java#L150]

 

I think the fix can be achieved by getting execution.savepoint-restore-mode from the configuration.

 

Looking forward to hearing your feedback.",,aitozi,dannycranmer,gyfora,JasonLee,nuafonso,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29543,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 28 22:38:02 UTC 2022,,,,,,,,,,"0|z173hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/22 09:37;chesnay;Sounds good, do you want to open a PR [~nuafonso] ?;;;","26/Jul/22 09:25;nuafonso;Hi [~chesnay],

Yes, I'm planning to open a PR soon.;;;","28/Aug/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot create sink for Temporary table in table store catalog,FLINK-28642,13472856,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,22/Jul/22 08:53,23/Jul/22 07:15,13/Jul/23 08:13,23/Jul/22 07:15,,,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"Using Flink 1.14. Temporary table in table store catalog will use the Factory provided by catalog.
We need to do some delegate work like HiveDynamicTableFactory.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jul 23 07:15:07 UTC 2022,,,,,,,,,,"0|z172xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/22 07:15;lzljs3620320;master: f8f0f64c3f6ecaf81374659b2d2d0da895766e86
release-0.2: 8fa384217f974f69356f301ee5801220146b8e95;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
High vulnerability in flink-kubernetes-operator-1.1.0-shaded.jar,FLINK-28637,13472724,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jeesmon,jbusche,jbusche,21/Jul/22 16:18,25/Jul/22 15:45,13/Jul/23 08:13,25/Jul/22 15:45,kubernetes-operator-1.0.1,kubernetes-operator-1.1.0,,,,,,kubernetes-operator-1.1.1,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"I noticed a high vulnerability in the flink-kubernetes-operator-1.1.0-shaded.jar file.

=======

cvss: 7.5

riskFactors: Has fix,High severity

cve: PRISMA-2022-0239    

link: https://github.com/square/okhttp/issues/6738

status: fixed in 4.9.2

packagePath: /flink-kubernetes-operator/flink-kubernetes-operator-1.1.0-shaded.jar

description: com.squareup.okhttp3_okhttp packages prior to version 4.9.2 are vulnerable for sensitive information disclosure. An illegal character in a header value will cause IllegalArgumentException which will include full header value. This applies to Authorization, Cookie, Proxy-Authorization and Set-Cookie headers. 

=======

It looks like we're using version 3.12.12, and there's no plans to provide this fix for the 3.x version.",,gyfora,jbusche,jeesmon,mbalassi,tedhtchang,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 25 15:45:27 UTC 2022,,,,,,,,,,"0|z17240:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 16:23;gyfora;We need to make sure that fabric8 and java-operator-sdk use a version of this library that has this fix and then bump those versions.

Based on the description this should not really affect the operator as the http client is not exposed to the user.;;;","21/Jul/22 16:25;gyfora;I think it would be risky for us to manually upgrade this transitive dependency to a new major version as it is likely to cause some unintended behaviour in the fabric8 client.;;;","21/Jul/22 18:14;jbusche;Thanks Gyula - makes complete since as it could require some extensive testing.;;;","21/Jul/22 19:23;mbalassi;For reference at the moment this is the transitive dependency hierarchy that we are pulling this through:

|  +- io.fabric8:kubernetes-client:jar:5.12.2:provided

|  |  +- com.squareup.okhttp3:okhttp:jar:3.12.12:provided

As [~gyfora] suggested we best keep the fabric8 version in synch with our JOSDK dependency which at the currently used 3.0.3 version also uses 5.12.2 of the fabric8 client.

It seems that the fabric8 community is currently working on their 6.0.0 release:

[https://github.com/fabric8io/kubernetes-client/releases/tag/v6.0.0-RC1]

But this still has the same okhttp version as listed above:

[https://github.com/fabric8io/kubernetes-client/blob/v6.0.0-RC1/pom.xml#L84]

Looking through their open issues and PRs I have not found an issue for bumping the okhttp version, but found this relevant:

[https://github.com/fabric8io/kubernetes-client/issues/2764]

[~jbusche] would you mind opening an issue on the fabric8 client to report this issue and ask their community whether they think this is relevant and if they would bump the version given this or they will rather merge the PR that tries to make the HTTP client agnostic that I linked above.;;;","21/Jul/22 20:57;jbusche;Sure [~mbalassi] I've opened [issue 4290|https://github.com/fabric8io/kubernetes-client/issues/4290], thanks! ;;;","22/Jul/22 01:45;wangyang0918;It seems that we also have this vulnerability in Flink project since we are using 3.14.9 there. So I am in favor of not making this ticket as a blocker.;;;","22/Jul/22 09:39;mbalassi;Thanks, [~jbusche].

fyi [~wangyang0918], [~gyfora]:

[https://github.com/fabric8io/kubernetes-client/issues/4290#issuecomment-1192194532]

I think we are fine for the 1.1 operator release as is, but it might make sense to explore swapping the dependency version later. What do you think?;;;","22/Jul/22 09:55;gyfora;We also need to check with the JOSDK team if they plan on migrating to fabric8 6.0.0 soon, in that case we could get rid of okhttp completely instead of swapping the dependency.;;;","22/Jul/22 12:59;jeesmon;There is a new PR against OSDK to bump up okhttp version: https://github.com/fabric8io/kubernetes-client/issues/4290#issuecomment-1192511844;;;","22/Jul/22 14:49;mbalassi;Fortunately both the Fabric8 and the JOSDK community was very responsive, this gives a path for fixing this. However given the following:
 
1. The HTTP client is internal to the operator, this vulnerability is very unlikely to affect it,
2. We also need to bump the dependency within the Flink native k8s integration,
3. We need extensive testing to make sure the new dependency version behaves properly,
 
My suggestion is to release 1.1.0 with this as a known issue and fix it in 1.1.1. That said we can merge a fix for it to the release-1.1 as soon as possible, so folks who are prohibited to use the 1.1.0 version can roll their own image from source.
 
The JOSDK folks offered to produce a new patch release that we can depend on in 1.1.1.;;;","22/Jul/22 15:48;jeesmon;[~mbalassi] Like JOSDK explicitly set okhttp version, can we use the same approach in 1.1.0 until we can upgrade JOSDK? That way we don't need to ship a new version with vulnerability. I'm using this approach locally in 1.0.1 and happy to create a PR. All e2e tests are passing with okhttp version upgrade.

Just adding the diff here so we can refer in case we decide not to fix it now and someone need it to satisfy their internal security requirements.
{code:java}
diff --git a/flink-kubernetes-operator/pom.xml b/flink-kubernetes-operator/pom.xml
index 6e85b8c..e82a5e9 100644
--- a/flink-kubernetes-operator/pom.xml
+++ b/flink-kubernetes-operator/pom.xml
@@ -143,6 +143,28 @@ under the License.
             <version>${junit.jupiter.version}</version>
             <scope>test</scope>
         </dependency>
+
+        <!--
+            regarding the okhttp explicit version
+            see https://github.com/fabric8io/kubernetes-client/issues/4290
+            and https://issues.apache.org/jira/browse/FLINK-28637
+            -->
+        <dependency>
+            <groupId>com.squareup.okhttp3</groupId>
+            <artifactId>okhttp</artifactId>
+            <version>${okhttp.version}</version>
+        </dependency>
+        <dependency>
+            <groupId>com.squareup.okhttp3</groupId>
+            <artifactId>logging-interceptor</artifactId>
+            <version>${okhttp.version}</version>
+        </dependency>
+        <dependency>
+            <groupId>com.squareup.okhttp3</groupId>
+            <artifactId>mockwebserver</artifactId>
+            <version>${okhttp.version}</version>
+            <scope>test</scope>
+        </dependency>
     </dependencies>

     <build>
diff --git a/pom.xml b/pom.xml
index 279f0b5..9f04d01 100644
--- a/pom.xml
+++ b/pom.xml
@@ -79,6 +79,8 @@ under the License.

         <spotless.version>2.4.2</spotless.version>
         <it.skip>true</it.skip>
+
+        <okhttp.version>4.10.0</okhttp.version>
     </properties>

     <dependencyManagement>
{code};;;","22/Jul/22 15:52;gyfora;Personally I am not extremely confident in simply swapping out a HttpClient implementation and releasing it with only minimal testing. The current JOSDK, fabric8, okhttp clients have been tested in various cloud environments for weeks/months.

It would be a real shame to introduce instability or any other problems for fixing a vulnaribility that cannot reasonable surface for the operator. Especially by doing a last minute change like that.

Please open a PR, we can merge this for the main/release-1.1 branches and release a patch release after 1-2 weeks of testing.;;;","25/Jul/22 15:45;gyfora;Merged

main: f8f8b96273eae68b10bb24eff01c5d44db5b10f0
release-1.1: 651a165f542bb1a491e0e23fbd3ca98eccacde79;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RescaleCheckpointManuallyITCase.testCheckpointRescalingInKeyedState failed with FileNotFoundException,FLINK-28626,13472672,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Yanfei Lei,hxbks2ks,hxbks2ks,21/Jul/22 11:39,12/Aug/22 08:06,13/Jul/23 08:13,12/Aug/22 08:06,1.16.0,,,,,,,1.16.0,,,,,Runtime / Checkpointing,,,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-07-21T04:14:20.8815245Z Jul 21 04:14:20 [ERROR] org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.testCheckpointRescalingInKeyedState  Time elapsed: 17.495 s  <<< ERROR!
2022-07-21T04:14:20.8816943Z Jul 21 04:14:20 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-07-21T04:14:20.8818144Z Jul 21 04:14:20 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-07-21T04:14:20.8819625Z Jul 21 04:14:20 	at org.apache.flink.test.util.TestUtils.submitJobAndWaitForResult(TestUtils.java:93)
2022-07-21T04:14:20.8821068Z Jul 21 04:14:20 	at org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.restoreAndAssert(RescaleCheckpointManuallyITCase.java:229)
2022-07-21T04:14:20.8822767Z Jul 21 04:14:20 	at org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.testCheckpointRescalingKeyedState(RescaleCheckpointManuallyITCase.java:147)
2022-07-21T04:14:20.8824888Z Jul 21 04:14:20 	at org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.testCheckpointRescalingInKeyedState(RescaleCheckpointManuallyITCase.java:115)
2022-07-21T04:14:20.8826253Z Jul 21 04:14:20 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-21T04:14:20.8827387Z Jul 21 04:14:20 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-21T04:14:20.8828651Z Jul 21 04:14:20 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-21T04:14:20.8829804Z Jul 21 04:14:20 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-21T04:14:20.8830909Z Jul 21 04:14:20 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-21T04:14:20.8832209Z Jul 21 04:14:20 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-21T04:14:20.8833685Z Jul 21 04:14:20 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-21T04:14:20.8834929Z Jul 21 04:14:20 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-21T04:14:20.8836083Z Jul 21 04:14:20 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-21T04:14:20.8837322Z Jul 21 04:14:20 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-07-21T04:14:20.8838517Z Jul 21 04:14:20 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-07-21T04:14:20.8839670Z Jul 21 04:14:20 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-07-21T04:14:20.8840778Z Jul 21 04:14:20 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-21T04:14:20.8841995Z Jul 21 04:14:20 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-21T04:14:20.8843136Z Jul 21 04:14:20 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-21T04:14:20.8844590Z Jul 21 04:14:20 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-21T04:14:20.8845750Z Jul 21 04:14:20 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-21T04:14:20.8846777Z Jul 21 04:14:20 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-21T04:14:20.8863771Z Jul 21 04:14:20 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-21T04:14:20.8865160Z Jul 21 04:14:20 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-21T04:14:20.8866042Z Jul 21 04:14:20 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-21T04:14:20.8867111Z Jul 21 04:14:20 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-21T04:14:20.8868244Z Jul 21 04:14:20 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-07-21T04:14:20.8869232Z Jul 21 04:14:20 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-07-21T04:14:20.8870076Z Jul 21 04:14:20 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-21T04:14:20.8870708Z Jul 21 04:14:20 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-21T04:14:20.8871304Z Jul 21 04:14:20 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-21T04:14:20.8872096Z Jul 21 04:14:20 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-21T04:14:20.8872756Z Jul 21 04:14:20 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-21T04:14:20.8874193Z Jul 21 04:14:20 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-21T04:14:20.8875381Z Jul 21 04:14:20 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-21T04:14:20.8876305Z Jul 21 04:14:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-21T04:14:20.8877492Z Jul 21 04:14:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-21T04:14:20.8878537Z Jul 21 04:14:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-21T04:14:20.8880053Z Jul 21 04:14:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-21T04:14:20.8881201Z Jul 21 04:14:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-21T04:14:20.8882004Z Jul 21 04:14:20 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-21T04:14:20.8882763Z Jul 21 04:14:20 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-21T04:14:20.8884157Z Jul 21 04:14:20 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-21T04:14:20.8885538Z Jul 21 04:14:20 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-21T04:14:20.8887119Z Jul 21 04:14:20 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-21T04:14:20.8888580Z Jul 21 04:14:20 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-21T04:14:20.8889928Z Jul 21 04:14:20 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-21T04:14:20.8891021Z Jul 21 04:14:20 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-21T04:14:20.8891768Z Jul 21 04:14:20 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-21T04:14:20.8892448Z Jul 21 04:14:20 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-21T04:14:20.8893141Z Jul 21 04:14:20 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-21T04:14:20.8894148Z Jul 21 04:14:20 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-07-21T04:14:20.8895611Z Jul 21 04:14:20 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
2022-07-21T04:14:20.8897152Z Jul 21 04:14:20 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
2022-07-21T04:14:20.8898332Z Jul 21 04:14:20 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:257)
2022-07-21T04:14:20.8899810Z Jul 21 04:14:20 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:248)
2022-07-21T04:14:20.8901228Z Jul 21 04:14:20 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
2022-07-21T04:14:20.8902622Z Jul 21 04:14:20 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:736)
2022-07-21T04:14:20.8904411Z Jul 21 04:14:20 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:713)
2022-07-21T04:14:20.8906179Z Jul 21 04:14:20 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2022-07-21T04:14:20.8907708Z Jul 21 04:14:20 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:477)
2022-07-21T04:14:20.8908979Z Jul 21 04:14:20 	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
2022-07-21T04:14:20.8910204Z Jul 21 04:14:20 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-21T04:14:20.8911365Z Jul 21 04:14:20 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-21T04:14:20.8912807Z Jul 21 04:14:20 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
2022-07-21T04:14:20.8914590Z Jul 21 04:14:20 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-07-21T04:14:20.8916179Z Jul 21 04:14:20 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
2022-07-21T04:14:20.8917591Z Jul 21 04:14:20 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
2022-07-21T04:14:20.8919097Z Jul 21 04:14:20 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2022-07-21T04:14:20.8920536Z Jul 21 04:14:20 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2022-07-21T04:14:20.8921840Z Jul 21 04:14:20 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-07-21T04:14:20.8923032Z Jul 21 04:14:20 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-07-21T04:14:20.8924524Z Jul 21 04:14:20 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-07-21T04:14:20.8925603Z Jul 21 04:14:20 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-07-21T04:14:20.8926835Z Jul 21 04:14:20 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-07-21T04:14:20.8927999Z Jul 21 04:14:20 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-07-21T04:14:20.8929267Z Jul 21 04:14:20 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-07-21T04:14:20.8930424Z Jul 21 04:14:20 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-07-21T04:14:20.8931618Z Jul 21 04:14:20 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-07-21T04:14:20.8932624Z Jul 21 04:14:20 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-07-21T04:14:20.8933999Z Jul 21 04:14:20 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-07-21T04:14:20.8935237Z Jul 21 04:14:20 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-07-21T04:14:20.8936275Z Jul 21 04:14:20 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-07-21T04:14:20.8937399Z Jul 21 04:14:20 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-07-21T04:14:20.8938443Z Jul 21 04:14:20 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-07-21T04:14:20.8939444Z Jul 21 04:14:20 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-07-21T04:14:20.8940598Z Jul 21 04:14:20 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-07-21T04:14:20.8941795Z Jul 21 04:14:20 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-07-21T04:14:20.8943108Z Jul 21 04:14:20 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-07-21T04:14:20.8944484Z Jul 21 04:14:20 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-07-21T04:14:20.8945754Z Jul 21 04:14:20 Caused by: java.lang.Exception: Exception while creating StreamOperatorStateContext.
2022-07-21T04:14:20.8947259Z Jul 21 04:14:20 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:256)
2022-07-21T04:14:20.8949110Z Jul 21 04:14:20 	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268)
2022-07-21T04:14:20.8950972Z Jul 21 04:14:20 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
2022-07-21T04:14:20.8952682Z Jul 21 04:14:20 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:723)
2022-07-21T04:14:20.8954586Z Jul 21 04:14:20 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
2022-07-21T04:14:20.8955961Z Jul 21 04:14:20 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:699)
2022-07-21T04:14:20.8957056Z Jul 21 04:14:20 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:666)
2022-07-21T04:14:20.8958332Z Jul 21 04:14:20 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
2022-07-21T04:14:20.8959729Z Jul 21 04:14:20 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
2022-07-21T04:14:20.8960812Z Jul 21 04:14:20 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
2022-07-21T04:14:20.8961814Z Jul 21 04:14:20 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
2022-07-21T04:14:20.8962645Z Jul 21 04:14:20 	at java.lang.Thread.run(Thread.java:748)
2022-07-21T04:14:20.8963814Z Jul 21 04:14:20 Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for StreamFlatMap_20ba6b65f97481d5570070de90e4e791_(1/3) from any of the 1 provided restore options.
2022-07-21T04:14:20.8965034Z Jul 21 04:14:20 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:160)
2022-07-21T04:14:20.8965992Z Jul 21 04:14:20 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:353)
2022-07-21T04:14:20.8967579Z Jul 21 04:14:20 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:165)
2022-07-21T04:14:20.8968966Z Jul 21 04:14:20 	... 11 more
2022-07-21T04:14:20.8969810Z Jul 21 04:14:20 Caused by: org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.
2022-07-21T04:14:20.8971107Z Jul 21 04:14:20 	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:405)
2022-07-21T04:14:20.8972946Z Jul 21 04:14:20 	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:503)
2022-07-21T04:14:20.8974883Z Jul 21 04:14:20 	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:98)
2022-07-21T04:14:20.8976443Z Jul 21 04:14:20 	at org.apache.flink.state.changelog.AbstractChangelogStateBackend.lambda$createKeyedStateBackend$1(AbstractChangelogStateBackend.java:145)
2022-07-21T04:14:20.8977963Z Jul 21 04:14:20 	at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.restore(ChangelogBackendRestoreOperation.java:69)
2022-07-21T04:14:20.8979280Z Jul 21 04:14:20 	at org.apache.flink.state.changelog.ChangelogStateBackend.restore(ChangelogStateBackend.java:92)
2022-07-21T04:14:20.8980302Z Jul 21 04:14:20 	at org.apache.flink.state.changelog.AbstractChangelogStateBackend.createKeyedStateBackend(AbstractChangelogStateBackend.java:136)
2022-07-21T04:14:20.8981411Z Jul 21 04:14:20 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:336)
2022-07-21T04:14:20.8982399Z Jul 21 04:14:20 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168)
2022-07-21T04:14:20.8983579Z Jul 21 04:14:20 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
2022-07-21T04:14:20.8984609Z Jul 21 04:14:20 	... 13 more
2022-07-21T04:14:20.8986340Z Jul 21 04:14:20 Caused by: java.io.FileNotFoundException: /tmp/junit2530256108929796473/junit8041656567883419966/87c3128889b96de94a21d4f7e1fdf2a2/taskowned/03c0e5d4-a423-4c9f-b9fb-b5aed08dda7a (No such file or directory)
2022-07-21T04:14:20.8987775Z Jul 21 04:14:20 	at java.io.FileInputStream.open0(Native Method)
2022-07-21T04:14:20.8988345Z Jul 21 04:14:20 	at java.io.FileInputStream.open(FileInputStream.java:195)
2022-07-21T04:14:20.8988969Z Jul 21 04:14:20 	at java.io.FileInputStream.<init>(FileInputStream.java:138)
2022-07-21T04:14:20.8989679Z Jul 21 04:14:20 	at org.apache.flink.core.fs.local.LocalDataInputStream.<init>(LocalDataInputStream.java:50)
2022-07-21T04:14:20.8990401Z Jul 21 04:14:20 	at org.apache.flink.core.fs.local.LocalFileSystem.open(LocalFileSystem.java:134)
2022-07-21T04:14:20.8991271Z Jul 21 04:14:20 	at org.apache.flink.runtime.state.filesystem.FileStateHandle.openInputStream(FileStateHandle.java:69)
2022-07-21T04:14:20.8992295Z Jul 21 04:14:20 	at org.apache.flink.contrib.streaming.state.RocksDBStateDownloader.downloadDataForStateHandle(RocksDBStateDownloader.java:127)
2022-07-21T04:14:20.8993367Z Jul 21 04:14:20 	at org.apache.flink.contrib.streaming.state.RocksDBStateDownloader.lambda$createDownloadRunnables$0(RocksDBStateDownloader.java:110)
2022-07-21T04:14:20.8994408Z Jul 21 04:14:20 	at org.apache.flink.util.function.ThrowingRunnable.lambda$unchecked$0(ThrowingRunnable.java:49)
2022-07-21T04:14:20.8995182Z Jul 21 04:14:20 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
2022-07-21T04:14:20.8995914Z Jul 21 04:14:20 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-21T04:14:20.8996630Z Jul 21 04:14:20 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-21T04:14:20.8997142Z Jul 21 04:14:20 	... 1 more
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38504&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7",,hxb,hxbks2ks,Yanfei Lei,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 12 08:06:08 UTC 2022,,,,,,,,,,"0|z171sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/22 08:37;Yanfei Lei;Hi [~hxbks2ks], could you please assign it to me?;;;","01/Aug/22 08:39;hxbks2ks;Thanks [~Yanfei Lei]. I have assigned it to you.;;;","01/Aug/22 11:30;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38992&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10133;;;","03/Aug/22 11:33;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39185&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","12/Aug/22 08:06;yunta;merged in master: 7cf71585a603866822ed0aee3978eea8d8587eee;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Guard against changing target session deployment for sessionjob,FLINK-28625,13472671,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aitozi,gyfora,gyfora,21/Jul/22 11:39,03/Aug/22 09:53,13/Jul/23 08:13,03/Aug/22 09:53,kubernetes-operator-1.0.1,kubernetes-operator-1.1.0,,,,,,kubernetes-operator-1.1.1,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,We should guard against changing the target session cluster as this can lead to an inconsistent state within the operator and make it difficult to delete the resources.,,aitozi,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 03 09:53:37 UTC 2022,,,,,,,,,,"0|z171s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 11:39;gyfora;cc [~aitozi] ;;;","31/Jul/22 05:19;aitozi;Make sense, I will push a PR for this.;;;","03/Aug/22 07:33;gyfora;Merged to main: 0cba89cdea5e19b92b1000c7e19e325b6d7625a2;;;","03/Aug/22 09:53;gyfora;merged to release-1.1: 67f0770c04714aa17fa86ca53b01bac2467a98c5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot use hive.dialect on master ,FLINK-28618,13472575,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,liubox123,liubox123,21/Jul/22 03:07,12/Oct/22 04:19,13/Jul/23 08:13,12/Oct/22 04:19,,,,,,,,,,,,,Connectors / Hive,,,,,,,0,,,,,"I build the newest master flink and copy \{hive-exec-2.3.9.jar;flink-sql-connector-hive-2.3.9_2.12-1.16-SNAPSHOT.jar;flink-connector-hive_2.12-1.16-SNAPSHOT.jar} to $FLINK_HOME/lib 。

 

then ， i got faild in sql-client  !image-2022-07-21-11-01-12-395.png!

and after copy opt/flink-table-planner_2.12-1.16-SNAPSHOT.jar 

even cannot open the sql-client 

!image-2022-07-21-11-04-12-552.png!

 

so , what's wronge? ","hadoop_class  2.10

openjdk11",liubox123,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/22 03:01;liubox123;image-2022-07-21-11-01-12-395.png;https://issues.apache.org/jira/secure/attachment/13047052/image-2022-07-21-11-01-12-395.png","21/Jul/22 03:04;liubox123;image-2022-07-21-11-04-12-552.png;https://issues.apache.org/jira/secure/attachment/13047051/image-2022-07-21-11-04-12-552.png",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 12 04:19:35 UTC 2022,,,,,,,,,,"0|z1716w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 08:05;chesnay;Did you remove the flink-table-planner-loader jar from lib?;;;","22/Jul/22 06:45;liubox123;already did that  ""Did you remove the flink-table-planner-loader jar from lib?""  

 

and sql-client can open  if i remove link-sql-connector-hive* out from lib  。

it's so strange

 ;;;","19/Aug/22 12:03;luoyuxia;[~liubox123] I think the current master has fixed this problem. Could you please try it again?;;;","12/Oct/22 04:19;luoyuxia;Close it since I haven't reproduced this problem using  master branch. Feel free to open it if the problem still exists.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink-Pulsar connector fails on larger schemas,FLINK-28609,13472248,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,jacek_wislicki,jacek_wislicki,19/Jul/22 11:05,30/Aug/22 02:33,13/Jul/23 08:13,30/Aug/22 02:33,1.14.3,1.14.4,1.14.5,1.15.1,,,,1.14.6,1.15.3,1.16.0,,,Connectors / Pulsar,,,,,,,1,pull-request-available,,,,"When a model results in a larger schema (this seems to be related to its byte array representation), the number of expected bytes to read is different than the number of actually read bytes: [^exception.txt]. The ""read"" is such a case is always 1018 while the expected ""byteLen"" gives a greater value. For smaller schemata, the numbers are equal (less than 1018) and no issue occurs.

The problem reproduction is on [GitHub|https://github.com/JacekWislicki/vp-test2]. There are 2 simple jobs (SimpleJob1 and SimpleJob2) using basic models for the Pulsar source definition (PulsarMessage1 and PulsarMessage2, respectively). Each of the corresponding schemata is properly serialised and deserialised, unless an effective byte array length becomes excessive (marked with ""the problem begins"" in model classes). The fail condition can be achieved by a number of fields (PulsarMessage1) or just longer field names (PulsarMessage2). The problem occurs on either Avro or a JSON schema set in the Pulsar source definition.",,AleksandraSarna,jacek_wislicki,martijnvisser,syhily,tison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/22 14:43;syhily;[FLINK-28609][Connector_Pulsar]_PulsarSchema_didn't_get_properly_serialized_.patch;https://issues.apache.org/jira/secure/attachment/13047081/%5BFLINK-28609%5D%5BConnector_Pulsar%5D_PulsarSchema_didn%27t_get_properly_serialized_.patch","19/Jul/22 10:50;jacek_wislicki;exception.txt;https://issues.apache.org/jira/secure/attachment/13046970/exception.txt",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 30 02:33:59 UTC 2022,,,,,,,,,,"0|z16z68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 13:30;syhily;Thanks for your report.;;;","21/Jul/22 14:43;syhily;This bug is easy to fix. I'll submit a PR tonight.;;;","21/Jul/22 14:54;jacek_wislicki;Great, thank you, Yufan!;;;","21/Jul/22 19:26;syhily;[~jacek_wislicki] The PR is ready. But I don't know when we can merge it to master and backport to 1.14 and 1.15.;;;","21/Jul/22 20:20;jacek_wislicki;[~syhily] - it would be perfect to have it in any of the current releases. Still, if this is not going to be possible, we will need to wait for 1.16. Do you know when we could expect this one? ;;;","22/Jul/22 06:42;syhily;[~jacek_wislicki] You can use streamnative self-maintained connector. https://github.com/streamnative/flink/releases;;;","25/Jul/22 07:31;jacek_wislicki;Thank you, but unfortunately we are constrained to used only official releases that can be pulled from public Maven repos. For development purposes, we will be using the old (deprecated) connector version from StreamNative/Pulsar.

 

Do you know a time plan from Flink 1.16 to be released? I see that there is a lot of work done but still some issues are open and being discussed.  ;;;","25/Jul/22 08:17;martijnvisser;The Flink 1.16 release branch will be cut in two weeks, see https://cwiki.apache.org/confluence/display/FLINK/1.16+Release;;;","25/Jul/22 08:52;jacek_wislicki;It's perfect, thank you, [~martijnvisser]!;;;","10/Aug/22 12:52;AleksandraSarna;Hi [~martijnvisser] ,

Could you please confirm that the fix for this issue is still in scope of 1.16 release?

Regards,
Aleksandra;;;","11/Aug/22 18:57;syhily;[~AleksandraSarna] We will get this merged soon after the FLINK-27399;;;","12/Aug/22 07:04;AleksandraSarna;Thanks for the information [~syhily].;;;","30/Aug/22 02:33;tison;master via d9bcbffc006481c09a8d2e04aa05cc92cd5c80d2
1.15 via cbd6c8d47b43f67c15d3a7d7bfb9b42b3e7d033b
1.14 via 55cf3b37bc6c864cc6ce5b7e9913de2de00b95cb;;;","30/Aug/22 02:33;tison;Thank you for reporting and taking care of this issue!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
job failover and not restore from checkpoint in zookeeper HA mode,FLINK-28604,13472196,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zouyunhe,zouyunhe,19/Jul/22 06:32,25/Jul/22 02:24,13/Jul/23 08:13,19/Jul/22 10:20,1.14.2,,,,,,,,,,,,Runtime / Checkpointing,,,,,,,0,,,,,"Run a job with flink 1.14.2 by configure the zookeeper ha 
{code:java}
high-availability.storageDir: hdfs://testcluster/app/ha
high-availability: zookeeper
high-availability.zookeeper.quorum: *****
high-availability.zookeeper.path.root: /flink{code}
when the zookeeper node restart, I see the JM failover with log ""Close and clean up all data for  ZookeeperHaServices"",  So the ha data was cleaned when the first JM shutdown. 

when the second JM was started,  the log was ""No checkpoint found during restore"", and no checkpoint to restored  .

From debug, I find when job failover, it would goto the `ClusterEntryPoint.java` line 285

!image-2022-07-19-14-30-27-198.png!

and will set the `cleanupHaData` as true.

 ",,martijnvisser,yunta,zouyunhe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/22 06:30;zouyunhe;image-2022-07-19-14-30-27-198.png;https://issues.apache.org/jira/secure/attachment/13046939/image-2022-07-19-14-30-27-198.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 25 02:24:30 UTC 2022,,,,,,,,,,"0|z16yuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/22 07:49;martijnvisser;[~zouyunhe] Can you verify this please with the latest Flink 1.14 release to make sure this is not already fixed? ;;;","19/Jul/22 10:19;zouyunhe;[~martijnvisser] this problem not exist in flink 1.14.5, it should be fixed. I will close this issue.;;;","25/Jul/22 02:24;yunta;BTW, [~zouyunhe] could this problem be easily reproduced in flink-1.14.2, and it will disappear once we bump to flink-1.14.5?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StateChangeFsUploader cannot close stream normally while enabling compression,FLINK-28602,13472188,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,masteryhx,masteryhx,19/Jul/22 05:32,29/Jul/22 07:05,13/Jul/23 08:13,29/Jul/22 07:05,1.15.1,1.16.0,,,,,,1.15.2,1.16.0,,,,Runtime / State Backends,,,,,,,0,pull-request-available,,,,"While enabling compression, Changelog part will wrap output stream using   

StreamCompressionDecorator#decorateWithCompression.

As the comment said, ""IMPORTANT: For streams returned by this method, \{@link OutputStream#close()} is not propagated to the inner stream. The inner stream must be closed separately."".

But StateChangeFsUploader will not close inner stream if wrapped stream has been closed.

So the upload may not complete when enabling compression even if it returns success.",,Feifan Wang,masteryhx,Ming Li,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28581,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 29 07:05:01 UTC 2022,,,,,,,,,,"0|z16ysw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/22 07:05;roman;Merged into master as e77d3a226bab6d06272976a742c225811dc3ca36,
into 1.15 as d5dc354dce663230b38973b285ff31b943da8fff.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Empty checkpoint folders not deleted on job cancellation if their shared state is still in use,FLINK-28597,13472158,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,18/Jul/22 23:41,03/Aug/22 11:00,13/Jul/23 08:13,03/Aug/22 11:00,1.16.0,,,,,,,1.16.0,,,,,Runtime / Checkpointing,,,,,,,0,pull-request-available,,,,"After FLINK-25872, SharedStateRegistry registers all state handles, including private ones.
Once the state isn't use AND the checkpoint is subsumed, it will actually be discarded.

This is done to prevent premature deletion when recovering in CLAIM mode:
1. RocksDB native savepoint folder (shared state is stored in chk-xx folder so it might fail the deletion)
2. Initial non-changelog checkpoint when switching to changelog-based checkpoints (private state of the initial checkpoint might be included into later checkpoints and its deletion would invalidate them)

Additionally, checkpoint folders are not deleted for a longer time which might be confusing.
In case of a crash, more folders will remain.

cc: [~Yanfei Lei], [~ym]",,Ming Li,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25872,,,,,FLINK-28581,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 03 11:00:15 UTC 2022,,,,,,,,,,"0|z16ym8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 11:00;roman;Merged as d8a4304b892412eab4c5c19b5deb84166943d3bb.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LSM tree structure may be incorrect when multiple jobs are committing into the same bucket,FLINK-28582,13471993,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,TsReaper,TsReaper,18/Jul/22 07:20,19/Jul/22 06:48,13/Jul/23 08:13,19/Jul/22 06:48,table-store-0.2.0,,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"Currently `FileStoreCommitImpl` only checks for conflicts by checking the files we're going to delete (due to compaction) are still there.

However, consider two jobs committing into the same LSM tree at the same time. For their first compaction no conflict is detected because they'll only delete their own level 0 files. But they will both produce level 1 files and the key ranges of these files may overlap. This is not correct for our LSM tree structure.",,lzljs3620320,qingyue,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 19 06:48:37 UTC 2022,,,,,,,,,,"0|z16xm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/22 06:48;lzljs3620320;master: f34be4ba159341d52eced8f7d0b0a5afc97ff91f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1.15.1 web ui console report error about checkpoint size,FLINK-28577,13471908,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yu Chen,nobleyd,nobleyd,17/Jul/22 12:01,27/Jul/22 11:49,13/Jul/23 08:13,27/Jul/22 11:46,1.15.1,,,,,,,1.15.2,1.16.0,,,,Runtime / Web Frontend,,,,,,,0,pull-request-available,,,,"1.15.1

1 start-cluster

2 submit job: ./bin/flink run -d ./examples/streaming/TopSpeedWindowing.jar

3 trigger savepoint: ./bin/flink savepoint {{{jobId} ./sp0}}

{{4 open web ui for job and change to checkpoint tab, nothing showed.}}

{{Chrome console log shows some error:}}

{{main.a7e97c2f60a2616e.js:1 ERROR TypeError: Cannot read properties of null (reading 'checkpointed_size')
    at q (253.e9e8f2b56b4981f5.js:1:607974)
    at Sl (main.a7e97c2f60a2616e.js:1:186068)
    at Br (main.a7e97c2f60a2616e.js:1:184696)
    at N8 (main.a7e97c2f60a2616e.js:1:185128)
    at Br (main.a7e97c2f60a2616e.js:1:185153)
    at N8 (main.a7e97c2f60a2616e.js:1:185128)
    at Br (main.a7e97c2f60a2616e.js:1:185153)
    at N8 (main.a7e97c2f60a2616e.js:1:185128)
    at Br (main.a7e97c2f60a2616e.js:1:185153)
    at B8 (main.a7e97c2f60a2616e.js:1:191872)}}

 

 

 ",,nobleyd,Yu Chen,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25557,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 27 11:46:34 UTC 2022,,,,,,,,,,"0|z16x34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/22 03:14;Yu Chen;Well, I think I've found the root cause of this issue: it is mainly introduced by the mistake modification of the flink-runtime-web by [JIRA-25557|https://issues.apache.org/jira/browse/FLINK-25557].

And I can propose a PR to resolve the problem.

cc: [~yunta] ;;;","27/Jul/22 03:28;yunta;[~Yu Chen] Thanks for figuring out this problem, and it seems we cannot have tests to cover the web UI changes:(, and maybe typos could lead such problems.

 

Already assigned this ticket to you, please go ahead.;;;","27/Jul/22 11:46;yunta;merged in master: be4e0fe050b6f71e8522311e598e4d9997134d49

release-1.15: 64038da8b2e974db13052a7b5dbab4cc8fc4c43c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump the fabric8 kubernetes-client to 6.0.0,FLINK-28574,13471862,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,ConradJam,ConradJam,16/Jul/22 06:19,20/Sep/22 08:50,13/Jul/23 08:13,09/Aug/22 12:47,kubernetes-operator-1.1.0,,,,,,,,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"fabric8 kubernetes-client now is release to [6.0.0|https://github.com/fabric8io/kubernetes-client/releases/tag/v6.0.0] , Later we can upgrade this version and remove the deprecated API usage

 ",,ConradJam,gyfora,nicholasjiang,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 09 12:47:57 UTC 2022,,,,,,,,,,"0|z16wsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jul/22 06:24;ConradJam;Hi [~gyfora] , What do you think ？Do you think we should upgrade now or put this work on hold until fabric8 Client has several more versions available from 6.x, such as 6.0.x or 6.1.x;;;","16/Jul/22 07:04;gyfora;Let's try to keep the fabric8 version in sync with the Java Operator SDK instead. There is no reason to upgrade independently.;;;","18/Jul/22 09:17;yunta;I think you could refer to FLINK-28481 first before considering bump fabric8 k8s-client version.;;;","24/Jul/22 07:16;ConradJam;[~gyfora] agree you mind, when Java Operator SDK update we can start this work;;;","09/Aug/22 12:01;nicholasjiang;[~ConradJam], the Java Operator SDK 3.1.1 has been released and bumped the fabric8 kubernetes-client to 5.12.3. I have already bumped the kubernetes-client. This ticket could be closed after above PR is merged.

cc [~gyfora] ;;;","09/Aug/22 12:47;gyfora;merged to main cd45721c53caaa7d16764c6faec68c985f8ebc39;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nested type will lose nullability when converting from TableSchema,FLINK-28573,13471802,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,15/Jul/22 14:59,18/Jul/22 01:52,13/Jul/23 08:13,18/Jul/22 01:52,table-store-0.2.0,,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"E.g. ArrayDataType, MultisetDataType etc",,lzljs3620320,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 18 01:52:34 UTC 2022,,,,,,,,,,"0|z16wfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/22 01:52;lzljs3620320;master: b1b9827b08d1182b30cdd34464d154560e7e2c62;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SinkUpsertMaterializer should be aware of the input upsertKey if it is not empty otherwise wrong result maybe produced,FLINK-28569,13471746,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,15/Jul/22 09:45,11/Oct/22 13:17,13/Jul/23 08:13,15/Sep/22 10:50,1.14.5,1.15.2,,,,,,1.16.0,1.17.0,,,,Table SQL / Runtime,,,,,,,0,pull-request-available,,,,"Currently SinkUpsertMaterializer only update row by comparing the complete row in anycase, but this may cause wrong result if input has upsertKey and also non-deterministic column values, see such a case:
{code:java}
@Test
public void testCdcWithNonDeterministicFuncSinkWithDifferentPk() {
tEnv.createTemporaryFunction(
""ndFunc"", new JavaUserDefinedScalarFunctions.NonDeterministicUdf());

String cdcDdl =
""CREATE TABLE users (\n""
+ "" user_id STRING,\n""
+ "" user_name STRING,\n""
+ "" email STRING,\n""
+ "" balance DECIMAL(18,2),\n""
+ "" primary key (user_id) not enforced\n""
+ "") WITH (\n""
+ "" 'connector' = 'values',\n""
+ "" 'changelog-mode' = 'I,UA,UB,D'\n""
+ "")"";

String sinkTableDdl =
""CREATE TABLE sink (\n""
+ "" user_id STRING,\n""
+ "" user_name STRING,\n""
+ "" email STRING,\n""
+ "" balance DECIMAL(18,2),\n""
+ "" PRIMARY KEY(email) NOT ENFORCED\n""
+ "") WITH(\n""
+ "" 'connector' = 'values',\n""
+ "" 'sink-insert-only' = 'false'\n""
+ "")"";
tEnv.executeSql(cdcDdl);
tEnv.executeSql(sinkTableDdl);

util.verifyJsonPlan(
""insert into sink select user_id, ndFunc(user_name), email, balance from users"");
}
{code}

for original cdc source records:
{code}
+I[user1, Tom, tom@gmail.com, 10.02],
-D[user1, Tom, tom@gmail.com, 10.02],
{code}

the above query cannot correctly delete the former insertion row because of the non-deterministic column value 'ndFunc(user_name)'

this canbe solved by letting the SinkUpsertMaterializer be aware of input upsertKey and update by it


",,godfreyhe,leonard,libenchao,lincoln.86xy,qingyue,qinjunjerry,Terry1897,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 11 13:16:47 UTC 2022,,,,,,,,,,"0|z16w34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/22 10:50;godfreyhe;Fixed in master: dbcd2d7b86fcb7fa7a26e181f1719ea4c6dad828
in 1.16.0: 390612320fa9be297d9eed1f4b75f8ba2ec83c40;;;","02/Oct/22 19:59;qinjunjerry;Can we backport this fix into 1.14 and 1.15? ;;;","08/Oct/22 12:57;lincoln.86xy;[~godfreyhe] I think it's valueable to port this fix to older versions(those users who encountered such case will get wrong result), this patch does not break the compatibility in some way (jobs can restore from previous state though new behavior changes), what do you think?;;;","11/Oct/22 10:45;Terry1897;+1 to backport this fix into 1.14 and 1.15;;;","11/Oct/22 11:26;Terry1897;But for the query which don't have upsert key and with non-deterministic values, the potential wrong result seems can not be avoid?
Should we disable such plan or give a warning during plan generation?;;;","11/Oct/22 13:16;lincoln.86xy;[~Terry1897] in 1.16, we introduced a [new mechanism|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/concepts/determinism/#33-how-to-eliminate-the-impact-of-non-deterministic-update-in-streaming] to do such validation, the case you mentioned which has no upsertKey but with changes will get an error message if set ’table.optimizer.non-deterministic-update.strategy’ to `TRY_RESOLVE`.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointCoordinatorTriggeringTest.discardingTriggeringCheckpointWillExecuteNextCheckpointRequest Process produced no output for 900 seconds,FLINK-28557,13471672,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,hxbks2ks,hxbks2ks,15/Jul/22 03:14,15/Jul/22 03:16,13/Jul/23 08:13,15/Jul/22 03:16,1.16.0,,,,,,,,,,,,Runtime / Checkpointing,Tests,,,,,,0,test-stability,,,,"
{code:java}
.1740649Z Jul 14 04:51:02 ""main"" #1 prio=5 os_prio=0 tid=0x00007f329000b800 nid=0x658 in Object.wait() [0x00007f3299b34000]
2022-07-14T04:51:03.1741235Z Jul 14 04:51:02    java.lang.Thread.State: WAITING (on object monitor)
2022-07-14T04:51:03.1741715Z Jul 14 04:51:02 	at java.lang.Object.wait(Native Method)
2022-07-14T04:51:03.1742197Z Jul 14 04:51:02 	at java.lang.Object.wait(Object.java:502)
2022-07-14T04:51:03.1742867Z Jul 14 04:51:02 	at org.apache.flink.core.testutils.OneShotLatch.await(OneShotLatch.java:61)
2022-07-14T04:51:03.1743707Z Jul 14 04:51:02 	- locked <0x00000000f6778e80> (a java.lang.Object)
2022-07-14T04:51:03.1744626Z Jul 14 04:51:02 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTriggeringTest.discardingTriggeringCheckpointWillExecuteNextCheckpointRequest(CheckpointCoordinatorTriggeringTest.java:731)
2022-07-14T04:51:03.1745509Z Jul 14 04:51:02 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-14T04:51:03.1746147Z Jul 14 04:51:02 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-14T04:51:03.1746877Z Jul 14 04:51:02 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-14T04:51:03.1747530Z Jul 14 04:51:02 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-14T04:51:03.1748167Z Jul 14 04:51:02 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-14T04:51:03.1748901Z Jul 14 04:51:02 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-14T04:51:03.1749593Z Jul 14 04:51:02 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-14T04:51:03.1750303Z Jul 14 04:51:02 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-14T04:51:03.1750993Z Jul 14 04:51:02 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-14T04:51:03.1751661Z Jul 14 04:51:02 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-07-14T04:51:03.1752326Z Jul 14 04:51:02 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-07-14T04:51:03.1752968Z Jul 14 04:51:02 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-07-14T04:51:03.1753635Z Jul 14 04:51:02 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-14T04:51:03.1754384Z Jul 14 04:51:02 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-14T04:51:03.1755055Z Jul 14 04:51:02 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-14T04:51:03.1755715Z Jul 14 04:51:02 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-14T04:51:03.1756430Z Jul 14 04:51:02 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-14T04:51:03.1757080Z Jul 14 04:51:02 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-14T04:51:03.1757788Z Jul 14 04:51:02 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-14T04:51:03.1758394Z Jul 14 04:51:02 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-14T04:51:03.1759016Z Jul 14 04:51:02 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-14T04:51:03.1759632Z Jul 14 04:51:02 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-14T04:51:03.1760264Z Jul 14 04:51:02 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-07-14T04:51:03.1760865Z Jul 14 04:51:02 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-07-14T04:51:03.1761460Z Jul 14 04:51:02 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-14T04:51:03.1762046Z Jul 14 04:51:02 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-14T04:51:03.1762624Z Jul 14 04:51:02 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-14T04:51:03.1763187Z Jul 14 04:51:02 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-14T04:51:03.1763891Z Jul 14 04:51:02 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-14T04:51:03.1764674Z Jul 14 04:51:02 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-14T04:51:03.1765391Z Jul 14 04:51:02 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-14T04:51:03.1766264Z Jul 14 04:51:02 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-14T04:51:03.1767083Z Jul 14 04:51:02 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-14T04:51:03.1767917Z Jul 14 04:51:02 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-14T04:51:03.1768722Z Jul 14 04:51:02 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator$$Lambda$154/1773638882.accept(Unknown Source)
2022-07-14T04:51:03.1769523Z Jul 14 04:51:02 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-14T04:51:03.1770381Z Jul 14 04:51:02 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-14T04:51:03.1771153Z Jul 14 04:51:02 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-14T04:51:03.1771858Z Jul 14 04:51:02 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-14T04:51:03.1772627Z Jul 14 04:51:02 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-14T04:51:03.1773538Z Jul 14 04:51:02 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-14T04:51:03.1774431Z Jul 14 04:51:02 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
2022-07-14T04:51:03.1775222Z Jul 14 04:51:02 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider$$Lambda$30/1188392295.accept(Unknown Source)
2022-07-14T04:51:03.1775879Z Jul 14 04:51:02 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2022-07-14T04:51:03.1776569Z Jul 14 04:51:02 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
2022-07-14T04:51:03.1777399Z Jul 14 04:51:02 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-14T04:51:03.1778195Z Jul 14 04:51:02 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2022-07-14T04:51:03.1778959Z Jul 14 04:51:02 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-14T04:51:03.1779771Z Jul 14 04:51:02 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-14T04:51:03.1780444Z Jul 14 04:51:02 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-14T04:51:03.1781106Z Jul 14 04:51:02 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-14T04:51:03.1781589Z Jul 14 04:51:02 
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38187&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7",,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28398,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-07-15 03:14:48.0,,,,,,,,,,"0|z16vmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The commit partition base path is not created when no data is sent which may cause FileNotFoundException,FLINK-28548,13471514,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Jiangang,Jiangang,Jiangang,14/Jul/22 04:02,16/Aug/22 04:11,13/Jul/23 08:13,16/Aug/22 04:11,1.14.5,1.15.1,1.16.0,,,,,1.16.0,,,,,Connectors / FileSystem,,,,,,,0,pull-request-available,,,,"The commit partition base path is not created when no data is sent which may cause FileNotFoundException.  The exception is as following:
{code:java}
Caused by: java.io.FileNotFoundException: File /home/ljg/test_sql.db/flink_batch_test/.staging_1657697612169 does not exist.
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:771) ~[hadoop-hdfs-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:120) ~[hadoop-hdfs-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem$20.doCall(DistributedFileSystem.java:828) ~[hadoop-hdfs-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem$20.doCall(DistributedFileSystem.java:824) ~[hadoop-hdfs-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[hadoop-common-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.hdfs.perflog.FileSystemLinkResolverWithStatistics$1.doCall(FileSystemLinkResolverWithStatistics.java:37) ~[hadoop-hdfs-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.hdfs.perflog.PerfProxy.call(PerfProxy.java:49) ~[hadoop-hdfs-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.hdfs.perflog.FileSystemLinkResolverWithStatistics.resolve(FileSystemLinkResolverWithStatistics.java:39) ~[hadoop-hdfs-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:835) ~[hadoop-hdfs-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.fs.FilterFileSystem.listStatus(FilterFileSystem.java:238) ~[hadoop-common-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.fs.FilterFileSystem.listStatus(FilterFileSystem.java:238) ~[hadoop-common-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.fs.viewfs.ChRootedFileSystem.listStatus(ChRootedFileSystem.java:241) ~[hadoop-common-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.fs.viewfs.ViewFileSystem.listStatus(ViewFileSystem.java:376) ~[hadoop-common-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.flink.hive.shaded.fs.hdfs.HadoopFileSystem.listStatus(HadoopFileSystem.java:170) ~[flink-sql-connector-hive-2.3.6_2.12-1.15.0.jar:1.15.0]
	at org.apache.flink.connector.file.table.PartitionTempFileManager.listTaskTemporaryPaths(PartitionTempFileManager.java:87) ~[flink-connector-files-1.15.0.jar:1.15.0]
	at org.apache.flink.connector.file.table.FileSystemCommitter.commitPartitions(FileSystemCommitter.java:78) ~[flink-connector-files-1.15.0.jar:1.15.0]
	at org.apache.flink.connector.file.table.FileSystemOutputFormat.finalizeGlobal(FileSystemOutputFormat.java:89) ~[flink-connector-files-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.jobgraph.InputOutputFormatVertex.finalizeOnMaster(InputOutputFormatVertex.java:153) ~[flink-dist-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.jobFinished(DefaultExecutionGraph.java:1190) ~[flink-dist-1.15.0.jar:1.15.0]
	... 43 more {code}
We should check whether the base path exists before listStatus for the path.",,jark,Jiangang,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 16 04:11:31 UTC 2022,,,,,,,,,,"0|z16uns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/22 06:34;Jiangang;[~zhangjing2000] [~lzljs3620320]  Could you have a look?  Thanks.;;;","20/Jul/22 06:56;lzljs3620320;Hi [~Jiangang], sorry I dont have time to look to this.
CC [~jark];;;","05/Aug/22 05:14;Jiangang;[~jark] Do you have time to review it? Thanks.;;;","16/Aug/22 04:11;jark;Fixed in master: f2abb51ac91c8b0e9bdd261de791d3aa1ba033dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Elasticsearch6SinkE2ECase failed with no space left on device,FLINK-28544,13471494,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,14/Jul/22 01:56,20/Jul/22 06:19,13/Jul/23 08:13,20/Jul/22 06:19,1.16.0,,,,,,,,,,,,Connectors / ElasticSearch,Tests,,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-07-13T02:49:13.5455800Z Jul 13 02:49:13 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 49.38 s <<< FAILURE! - in org.apache.flink.streaming.tests.Elasticsearch6SinkE2ECase
2022-07-13T02:49:13.5465965Z Jul 13 02:49:13 [ERROR] org.apache.flink.streaming.tests.Elasticsearch6SinkE2ECase  Time elapsed: 49.38 s  <<< ERROR!
2022-07-13T02:49:13.5466765Z Jul 13 02:49:13 java.lang.RuntimeException: Failed to build JobManager image
2022-07-13T02:49:13.5467621Z Jul 13 02:49:13 	at org.apache.flink.connector.testframe.container.FlinkTestcontainersConfigurator.configureJobManagerContainer(FlinkTestcontainersConfigurator.java:67)
2022-07-13T02:49:13.5468645Z Jul 13 02:49:13 	at org.apache.flink.connector.testframe.container.FlinkTestcontainersConfigurator.configure(FlinkTestcontainersConfigurator.java:147)
2022-07-13T02:49:13.5469564Z Jul 13 02:49:13 	at org.apache.flink.connector.testframe.container.FlinkContainers$Builder.build(FlinkContainers.java:197)
2022-07-13T02:49:13.5470467Z Jul 13 02:49:13 	at org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment.<init>(FlinkContainerTestEnvironment.java:88)
2022-07-13T02:49:13.5471424Z Jul 13 02:49:13 	at org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment.<init>(FlinkContainerTestEnvironment.java:51)
2022-07-13T02:49:13.5472504Z Jul 13 02:49:13 	at org.apache.flink.streaming.tests.ElasticsearchSinkE2ECaseBase.<init>(ElasticsearchSinkE2ECaseBase.java:58)
2022-07-13T02:49:13.5473388Z Jul 13 02:49:13 	at org.apache.flink.streaming.tests.Elasticsearch6SinkE2ECase.<init>(Elasticsearch6SinkE2ECase.java:36)
2022-07-13T02:49:13.5474161Z Jul 13 02:49:13 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2022-07-13T02:49:13.5474905Z Jul 13 02:49:13 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2022-07-13T02:49:13.5475756Z Jul 13 02:49:13 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2022-07-13T02:49:13.5476734Z Jul 13 02:49:13 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2022-07-13T02:49:13.5477495Z Jul 13 02:49:13 	at org.junit.platform.commons.util.ReflectionUtils.newInstance(ReflectionUtils.java:550)
2022-07-13T02:49:13.5478313Z Jul 13 02:49:13 	at org.junit.jupiter.engine.execution.ConstructorInvocation.proceed(ConstructorInvocation.java:56)
2022-07-13T02:49:13.5479220Z Jul 13 02:49:13 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-07-13T02:49:13.5480165Z Jul 13 02:49:13 	at org.junit.jupiter.api.extension.InvocationInterceptor.interceptTestClassConstructor(InvocationInterceptor.java:73)
2022-07-13T02:49:13.5481038Z Jul 13 02:49:13 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-07-13T02:49:13.5481944Z Jul 13 02:49:13 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-07-13T02:49:13.5482875Z Jul 13 02:49:13 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-07-13T02:49:13.5483764Z Jul 13 02:49:13 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-07-13T02:49:13.5484642Z Jul 13 02:49:13 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-07-13T02:49:13.5486123Z Jul 13 02:49:13 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-07-13T02:49:13.5488185Z Jul 13 02:49:13 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:77)
2022-07-13T02:49:13.5488883Z Jul 13 02:49:13 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeTestClassConstructor(ClassBasedTestDescriptor.java:355)
2022-07-13T02:49:13.5490237Z Jul 13 02:49:13 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.instantiateTestClass(ClassBasedTestDescriptor.java:302)
2022-07-13T02:49:13.5491099Z Jul 13 02:49:13 	at org.junit.jupiter.engine.descriptor.ClassTestDescriptor.instantiateTestClass(ClassTestDescriptor.java:79)
2022-07-13T02:49:13.5491840Z Jul 13 02:49:13 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.instantiateAndPostProcessTestInstance(ClassBasedTestDescriptor.java:280)
2022-07-13T02:49:13.5492618Z Jul 13 02:49:13 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$4(ClassBasedTestDescriptor.java:272)
2022-07-13T02:49:13.5493228Z Jul 13 02:49:13 	at java.util.Optional.orElseGet(Optional.java:267)
2022-07-13T02:49:13.5493835Z Jul 13 02:49:13 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$5(ClassBasedTestDescriptor.java:271)
2022-07-13T02:49:13.5494551Z Jul 13 02:49:13 	at org.junit.jupiter.engine.execution.TestInstancesProvider.getTestInstances(TestInstancesProvider.java:31)
2022-07-13T02:49:13.5495253Z Jul 13 02:49:13 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$before$2(ClassBasedTestDescriptor.java:197)
2022-07-13T02:49:13.5495940Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-13T02:49:13.5496616Z Jul 13 02:49:13 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:196)
2022-07-13T02:49:13.5497286Z Jul 13 02:49:13 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:80)
2022-07-13T02:49:13.5497973Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:148)
2022-07-13T02:49:13.5498653Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-13T02:49:13.5499323Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-07-13T02:49:13.5500024Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-07-13T02:49:13.5500655Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-07-13T02:49:13.5501345Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-13T02:49:13.5535107Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-07-13T02:49:13.5535791Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-07-13T02:49:13.5538031Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-07-13T02:49:13.5538994Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-07-13T02:49:13.5539797Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-07-13T02:49:13.5540481Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-13T02:49:13.5541154Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-07-13T02:49:13.5541784Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-07-13T02:49:13.5542410Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-07-13T02:49:13.5543090Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-13T02:49:13.5543930Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-07-13T02:49:13.5544575Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-07-13T02:49:13.5545351Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-07-13T02:49:13.5546083Z Jul 13 02:49:13 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-07-13T02:49:13.5546615Z Jul 13 02:49:13 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-07-13T02:49:13.5547162Z Jul 13 02:49:13 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-07-13T02:49:13.5547713Z Jul 13 02:49:13 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-07-13T02:49:13.5548444Z Jul 13 02:49:13 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-07-13T02:49:13.5549641Z Jul 13 02:49:13 Caused by: org.apache.flink.connector.testframe.container.ImageBuildException: Failed to build image ""flink-configured-jobmanager""
2022-07-13T02:49:13.5550318Z Jul 13 02:49:13 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.build(FlinkImageBuilder.java:234)
2022-07-13T02:49:13.5551080Z Jul 13 02:49:13 	at org.apache.flink.connector.testframe.container.FlinkTestcontainersConfigurator.configureJobManagerContainer(FlinkTestcontainersConfigurator.java:65)
2022-07-13T02:49:13.5551656Z Jul 13 02:49:13 	... 57 more
2022-07-13T02:49:13.5552744Z Jul 13 02:49:13 Caused by: java.lang.RuntimeException: com.github.dockerjava.api.exception.DockerClientException: Could not build image: ApplyLayer exit status 1 stdout:  stderr: write /opt/flink/opt/flink-s3-fs-presto-1.16-SNAPSHOT.jar: no space left on device
2022-07-13T02:49:13.5553633Z Jul 13 02:49:13 	at org.rnorth.ducttape.timeouts.Timeouts.callFuture(Timeouts.java:68)
2022-07-13T02:49:13.5554224Z Jul 13 02:49:13 	at org.rnorth.ducttape.timeouts.Timeouts.getWithTimeout(Timeouts.java:43)
2022-07-13T02:49:13.5554761Z Jul 13 02:49:13 	at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:45)
2022-07-13T02:49:13.5555373Z Jul 13 02:49:13 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.buildBaseImage(FlinkImageBuilder.java:252)
2022-07-13T02:49:13.5706429Z Jul 13 02:49:13 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.build(FlinkImageBuilder.java:206)
2022-07-13T02:49:13.5707070Z Jul 13 02:49:13 	... 58 more
2022-07-13T02:49:13.5712449Z Jul 13 02:49:13 Caused by: com.github.dockerjava.api.exception.DockerClientException: Could not build image: ApplyLayer exit status 1 stdout:  stderr: write /opt/flink/opt/flink-s3-fs-presto-1.16-SNAPSHOT.jar: no space left on device
2022-07-13T02:49:13.5765291Z Jul 13 02:49:13 	at com.github.dockerjava.api.command.BuildImageResultCallback.getImageId(BuildImageResultCallback.java:78)
2022-07-13T02:49:13.5766129Z Jul 13 02:49:13 	at com.github.dockerjava.api.command.BuildImageResultCallback.awaitImageId(BuildImageResultCallback.java:50)
2022-07-13T02:49:13.5766802Z Jul 13 02:49:13 	at org.testcontainers.images.builder.ImageFromDockerfile.resolve(ImageFromDockerfile.java:147)
2022-07-13T02:49:13.5767436Z Jul 13 02:49:13 	at org.testcontainers.images.builder.ImageFromDockerfile.resolve(ImageFromDockerfile.java:40)
2022-07-13T02:49:13.5768029Z Jul 13 02:49:13 	at org.testcontainers.utility.LazyFuture.getResolvedValue(LazyFuture.java:17)
2022-07-13T02:49:13.5768573Z Jul 13 02:49:13 	at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:39)
2022-07-13T02:49:13.5769083Z Jul 13 02:49:13 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-07-13T02:49:13.5769625Z Jul 13 02:49:13 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-13T02:49:13.5770223Z Jul 13 02:49:13 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-13T02:49:13.5771000Z Jul 13 02:49:13 	at java.lang.Thread.run(Thread.java:750)
2022-07-13T02:49:13.5771336Z Jul 13 02:49:13 
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38110&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678",,hxbks2ks,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 20 06:19:14 UTC 2022,,,,,,,,,,"0|z16ujc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/22 07:22;martijnvisser;[~hxbks2ks] It looks like the Python Bash e2e tests take up the majority of disk space again:

{code:java}
2022-07-13T02:28:48.9826953Z Jul 13 02:28:48 ##[group]Top 15 biggest directories in terms of used disk space
2022-07-13T02:28:50.8283982Z Jul 13 02:28:50 5778912	.
2022-07-13T02:28:50.8285319Z Jul 13 02:28:50 1615376	./flink-python
2022-07-13T02:28:50.8285977Z Jul 13 02:28:50 1550496	./flink-python/dev
2022-07-13T02:28:50.8286638Z Jul 13 02:28:50 1516288	./flink-end-to-end-tests
2022-07-13T02:28:50.8287306Z Jul 13 02:28:50 1476920	./flink-python/dev/.conda
2022-07-13T02:28:50.8287972Z Jul 13 02:28:50 642240	./flink-python/dev/.conda/pkgs
2022-07-13T02:28:50.8288651Z Jul 13 02:28:50 624860	./flink-dist
2022-07-13T02:28:50.8289280Z Jul 13 02:28:50 624472	./flink-dist/target
2022-07-13T02:28:50.8289931Z Jul 13 02:28:50 565828	./flink-python/dev/.conda/envs
2022-07-13T02:28:50.8290615Z Jul 13 02:28:50 500260	./flink-dist/target/flink-1.16-SNAPSHOT-bin
2022-07-13T02:28:50.8291384Z Jul 13 02:28:50 500256	./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT
2022-07-13T02:28:50.8292092Z Jul 13 02:28:50 461348	./flink-connectors
2022-07-13T02:28:50.8292552Z Jul 13 02:28:50 373572	./.git
2022-07-13T02:28:50.8292993Z Jul 13 02:28:50 369928	./.git/objects
2022-07-13T02:28:50.8293452Z Jul 13 02:28:50 369920	./.git/objects/pack
{code};;;","18/Jul/22 03:06;hxbks2ks;Thanks [~martijnvisser]. I will take a look.;;;","18/Jul/22 03:12;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38281&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678;;;","20/Jul/22 06:19;hxbks2ks;I have merged the commit 6b3ac775fe72514192484f7c923da37ebea5f524 in master to clean up the python environment.

I will continue to pay attention to the CI to see whether this problem have been solved
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spec change event is triggered twice per upgrade,FLINK-28534,13471350,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,morhidi,gyfora,gyfora,13/Jul/22 08:11,24/Nov/22 01:02,13/Jul/23 08:13,14/Jul/22 16:13,kubernetes-operator-1.1.0,,,,,,,kubernetes-operator-1.1.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"The event `Detected spec change, starting reconciliation.` is triggered twice during every application/sessionjob upgrade as we first suspend the job then start it in 2 reconcile steps.

We should only trigger this once",,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 14 16:13:22 UTC 2022,,,,,,,,,,"0|z16tnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/22 16:13;gyfora;merged to main c011dc2210e90dfbde2e6ed3960f40d94b222c0e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogPeriodicMaterializationSwitchStateBackendITCase.testSwitchFromDisablingToEnablingInClaimMode failed with CheckpointException: Checkpoint expired before completing,FLINK-28529,13471308,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Yanfei Lei,hxbks2ks,hxbks2ks,13/Jul/22 02:21,09/Aug/22 21:14,13/Jul/23 08:13,09/Aug/22 21:14,1.16.0,,,,,,,1.16.0,,,,,Runtime / Checkpointing,Runtime / State Backends,,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-07-12T04:30:49.9912088Z Jul 12 04:30:49 [ERROR] ChangelogPeriodicMaterializationSwitchStateBackendITCase.testSwitchFromDisablingToEnablingInClaimMode  Time elapsed: 617.048 s  <<< ERROR!
2022-07-12T04:30:49.9913108Z Jul 12 04:30:49 java.util.concurrent.ExecutionException: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint expired before completing.
2022-07-12T04:30:49.9913880Z Jul 12 04:30:49 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-07-12T04:30:49.9914606Z Jul 12 04:30:49 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2022-07-12T04:30:49.9915572Z Jul 12 04:30:49 	at org.apache.flink.test.checkpointing.ChangelogPeriodicMaterializationSwitchStateBackendITCase.testSwitchFromDisablingToEnablingInClaimMode(ChangelogPeriodicMaterializationSwitchStateBackendITCase.java:125)
2022-07-12T04:30:49.9916483Z Jul 12 04:30:49 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-12T04:30:49.9917377Z Jul 12 04:30:49 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-12T04:30:49.9918121Z Jul 12 04:30:49 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-12T04:30:49.9918788Z Jul 12 04:30:49 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-12T04:30:49.9919456Z Jul 12 04:30:49 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-12T04:30:49.9920193Z Jul 12 04:30:49 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-12T04:30:49.9920923Z Jul 12 04:30:49 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-12T04:30:49.9921630Z Jul 12 04:30:49 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-12T04:30:49.9922326Z Jul 12 04:30:49 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-12T04:30:49.9923023Z Jul 12 04:30:49 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-07-12T04:30:49.9923708Z Jul 12 04:30:49 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-07-12T04:30:49.9924449Z Jul 12 04:30:49 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-07-12T04:30:49.9925124Z Jul 12 04:30:49 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-07-12T04:30:49.9925912Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:30:49.9926742Z Jul 12 04:30:49 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-12T04:30:49.9928142Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-12T04:30:49.9928715Z Jul 12 04:30:49 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-12T04:30:49.9929311Z Jul 12 04:30:49 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-12T04:30:49.9929863Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T04:30:49.9930376Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T04:30:49.9930911Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T04:30:49.9931441Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T04:30:49.9931975Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T04:30:49.9932493Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T04:30:49.9932966Z Jul 12 04:30:49 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-07-12T04:30:49.9933427Z Jul 12 04:30:49 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-07-12T04:30:49.9933911Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T04:30:49.9934424Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T04:30:49.9934951Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T04:30:49.9935471Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T04:30:49.9936271Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T04:30:49.9936826Z Jul 12 04:30:49 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-07-12T04:30:49.9937342Z Jul 12 04:30:49 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-07-12T04:30:49.9937849Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:30:49.9938359Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T04:30:49.9938847Z Jul 12 04:30:49 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-12T04:30:49.9939417Z Jul 12 04:30:49 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-12T04:30:49.9939950Z Jul 12 04:30:49 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-12T04:30:49.9940572Z Jul 12 04:30:49 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-12T04:30:49.9941177Z Jul 12 04:30:49 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-12T04:30:49.9941827Z Jul 12 04:30:49 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-12T04:30:49.9942532Z Jul 12 04:30:49 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-12T04:30:49.9943244Z Jul 12 04:30:49 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-12T04:30:49.9943976Z Jul 12 04:30:49 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-12T04:30:49.9944697Z Jul 12 04:30:49 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-12T04:30:49.9945354Z Jul 12 04:30:49 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-12T04:30:49.9945960Z Jul 12 04:30:49 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-12T04:30:49.9946690Z Jul 12 04:30:49 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-12T04:30:49.9947389Z Jul 12 04:30:49 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-12T04:30:49.9948062Z Jul 12 04:30:49 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-12T04:30:49.9948750Z Jul 12 04:30:49 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-12T04:30:49.9949432Z Jul 12 04:30:49 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-12T04:30:49.9950079Z Jul 12 04:30:49 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-12T04:30:49.9950685Z Jul 12 04:30:49 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-12T04:30:49.9951268Z Jul 12 04:30:49 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-12T04:30:49.9951835Z Jul 12 04:30:49 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-12T04:30:49.9952411Z Jul 12 04:30:49 Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint expired before completing.
2022-07-12T04:30:49.9953009Z Jul 12 04:30:49 	at org.apache.flink.runtime.checkpoint.PendingCheckpoint.abort(PendingCheckpoint.java:549)
2022-07-12T04:30:49.9953687Z Jul 12 04:30:49 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2089)
2022-07-12T04:30:49.9954419Z Jul 12 04:30:49 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2076)
2022-07-12T04:30:49.9955114Z Jul 12 04:30:49 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.access$600(CheckpointCoordinator.java:97)
2022-07-12T04:30:49.9955812Z Jul 12 04:30:49 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator$CheckpointCanceller.run(CheckpointCoordinator.java:2158)
2022-07-12T04:30:49.9956463Z Jul 12 04:30:49 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2022-07-12T04:30:49.9957003Z Jul 12 04:30:49 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-07-12T04:30:49.9957622Z Jul 12 04:30:49 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
2022-07-12T04:30:49.9958393Z Jul 12 04:30:49 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
2022-07-12T04:30:49.9959035Z Jul 12 04:30:49 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-12T04:30:49.9959627Z Jul 12 04:30:49 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-12T04:30:49.9960134Z Jul 12 04:30:49 	at java.lang.Thread.run(Thread.java:748)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38057&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798",,hxbks2ks,leonard,roman,Yanfei Lei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 09 21:14:10 UTC 2022,,,,,,,,,,"0|z16te8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/22 02:23;hxbks2ks;Hi [~Yanfei Lei], could you help take a look?;;;","13/Jul/22 03:00;Yanfei Lei;Sure, look like it's caused by checkpoint expired, I will check it.;;;","14/Jul/22 02:33;hxbks2ks;Anther failed instance https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38128&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba.
But the stack is different:

{code:java}
2022-07-13T09:36:20.6676645Z Jul 13 09:36:20 [ERROR] ChangelogPeriodicMaterializationSwitchStateBackendITCase.testSwitchFromDisablingToEnablingInClaimMode  Time elapsed: 2.207 s  <<< ERROR!
2022-07-13T09:36:20.6677831Z Jul 13 09:36:20 java.util.NoSuchElementException: No value present
2022-07-13T09:36:20.6678727Z Jul 13 09:36:20 	at java.util.Optional.get(Optional.java:135)
2022-07-13T09:36:20.6680252Z Jul 13 09:36:20 	at org.apache.flink.test.checkpointing.ChangelogPeriodicMaterializationSwitchStateBackendITCase.testSwitchFromDisablingToEnablingInClaimMode(ChangelogPeriodicMaterializationSwitchStateBackendITCase.java:115)
2022-07-13T09:36:20.6682394Z Jul 13 09:36:20 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-13T09:36:20.6683581Z Jul 13 09:36:20 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-13T09:36:20.6684792Z Jul 13 09:36:20 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-13T09:36:20.6689681Z Jul 13 09:36:20 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-13T09:36:20.6690994Z Jul 13 09:36:20 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-13T09:36:20.6692214Z Jul 13 09:36:20 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-13T09:36:20.6693463Z Jul 13 09:36:20 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-13T09:36:20.6694894Z Jul 13 09:36:20 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-13T09:36:20.6696003Z Jul 13 09:36:20 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-13T09:36:20.6697015Z Jul 13 09:36:20 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-07-13T09:36:20.6698018Z Jul 13 09:36:20 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-07-13T09:36:20.6699071Z Jul 13 09:36:20 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-07-13T09:36:20.6700062Z Jul 13 09:36:20 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-07-13T09:36:20.6701007Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-13T09:36:20.6702219Z Jul 13 09:36:20 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-13T09:36:20.6703433Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-13T09:36:20.6704186Z Jul 13 09:36:20 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-13T09:36:20.6704928Z Jul 13 09:36:20 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-13T09:36:20.6705598Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-13T09:36:20.6706238Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-13T09:36:20.6706879Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-13T09:36:20.6707523Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-13T09:36:20.6708142Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-13T09:36:20.6708756Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-13T09:36:20.6709351Z Jul 13 09:36:20 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-07-13T09:36:20.6726193Z Jul 13 09:36:20 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-07-13T09:36:20.6727363Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-13T09:36:20.6728407Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-13T09:36:20.6729463Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-13T09:36:20.6730706Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-13T09:36:20.6731855Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-13T09:36:20.6733048Z Jul 13 09:36:20 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-07-13T09:36:20.6734076Z Jul 13 09:36:20 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-07-13T09:36:20.6735256Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-13T09:36:20.6736312Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-13T09:36:20.6737647Z Jul 13 09:36:20 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-13T09:36:20.6738575Z Jul 13 09:36:20 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-13T09:36:20.6739609Z Jul 13 09:36:20 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-13T09:36:20.6740819Z Jul 13 09:36:20 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-13T09:36:20.6742147Z Jul 13 09:36:20 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-13T09:36:20.6743586Z Jul 13 09:36:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-13T09:36:20.6745108Z Jul 13 09:36:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-13T09:36:20.6746788Z Jul 13 09:36:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-13T09:36:20.6748256Z Jul 13 09:36:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-13T09:36:20.6749675Z Jul 13 09:36:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-13T09:36:20.6750944Z Jul 13 09:36:20 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-13T09:36:20.6752243Z Jul 13 09:36:20 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-13T09:36:20.6753854Z Jul 13 09:36:20 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-13T09:36:20.6755257Z Jul 13 09:36:20 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-13T09:36:20.6756612Z Jul 13 09:36:20 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-13T09:36:20.6758018Z Jul 13 09:36:20 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-13T09:36:20.6759384Z Jul 13 09:36:20 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-13T09:36:20.6760670Z Jul 13 09:36:20 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-13T09:36:20.6761950Z Jul 13 09:36:20 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-13T09:36:20.6763255Z Jul 13 09:36:20 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-13T09:36:20.6764355Z Jul 13 09:36:20 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-13T09:36:20.6765151Z Jul 13 09:36:20 
{code}
;;;","18/Jul/22 03:00;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38295&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","18/Jul/22 03:01;hxbks2ks;Hi [~Yanfei Lei] Any updates on the progress?;;;","18/Jul/22 03:30;Yanfei Lei;This is related to the instability of triggering checkpoint, I would open a PR after [https://github.com/apache/flink/pull/19864] megered.;;;","19/Jul/22 02:53;hxbks2ks;Hi [~Yanfei Lei], I have merged the PR https://github.com/apache/flink/pull/19864 . You can go on your work.;;;","21/Jul/22 11:49;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38510&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","29/Jul/22 09:40;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38893&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","29/Jul/22 09:40;hxbks2ks;Hi  [~Yanfei Lei] Any updates on the progress?;;;","01/Aug/22 08:24;Yanfei Lei;Sorry for the late reply, I refactored the test, and the PR is waiting to be reviewed.;;;","01/Aug/22 11:48;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38962&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b;;;","04/Aug/22 06:11;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39237&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","09/Aug/22 02:18;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39675&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a;;;","09/Aug/22 21:14;roman;Merged as c5a8b0f3c294b9e13be4aae0f0f77c64d5405418..6b725d13153491e95a6ffdaa673347d86ef2cd54.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to lateral join with UDTF from Table with timstamp column,FLINK-28526,13471304,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,xuannan,xuannan,13/Jul/22 02:15,17/Jan/23 15:06,13/Jul/23 08:13,17/Jan/23 15:06,1.15.1,,,,,,,1.15.4,1.16.1,1.17.0,,,API / Python,,,,,,,0,pull-request-available,,,,"The bug can be reproduced with the following test


{code:python}
    def test_flink(self):
        env = StreamExecutionEnvironment.get_execution_environment()
        t_env = StreamTableEnvironment.create(env)
        table = t_env.from_descriptor(
            TableDescriptor.for_connector(""filesystem"")
            .schema(
                Schema.new_builder()
                .column(""name"", DataTypes.STRING())
                .column(""cost"", DataTypes.INT())
                .column(""distance"", DataTypes.INT())
                .column(""time"", DataTypes.TIMESTAMP(3))
                .watermark(""time"", ""`time` - INTERVAL '60' SECOND"")
                .build()
            )
            .format(""csv"")
            .option(""path"", ""./input.csv"")
            .build()
        )

        @udtf(result_types=DataTypes.INT())
        def table_func(row: Row):
            return row.cost + row.distance

        table = table.join_lateral(table_func.alias(""cost_times_distance""))

        table.execute().print()
{code}

It causes the following exception


{code:none}
E       pyflink.util.exceptions.TableException: org.apache.flink.table.api.TableException: Unsupported Python SqlFunction CAST.
E       	at org.apache.flink.table.planner.plan.nodes.exec.utils.CommonPythonUtil.createPythonFunctionInfo(CommonPythonUtil.java:146)
E       	at org.apache.flink.table.planner.plan.nodes.exec.utils.CommonPythonUtil.createPythonFunctionInfo(CommonPythonUtil.java:429)
E       	at org.apache.flink.table.planner.plan.nodes.exec.utils.CommonPythonUtil.createPythonFunctionInfo(CommonPythonUtil.java:135)
E       	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCorrelate.extractPythonTableFunctionInfo(CommonExecPythonCorrelate.java:133)
E       	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCorrelate.createPythonOneInputTransformation(CommonExecPythonCorrelate.java:106)
E       	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCorrelate.translateToPlanInternal(CommonExecPythonCorrelate.java:95)
E       	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
E       	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
E       	at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.java:136)
E       	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
E       	at org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:79)
E       	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
E       	at scala.collection.Iterator.foreach(Iterator.scala:937)
E       	at scala.collection.Iterator.foreach$(Iterator.scala:937)
E       	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
E       	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
E       	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
E       	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
E       	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
E       	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
E       	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
E       	at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:78)
E       	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:181)
E       	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1656)
E       	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:828)
E       	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1317)
E       	at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:605)
E       	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E       	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E       	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E       	at java.lang.reflect.Method.invoke(Method.java:498)
E       	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E       	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
E       	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
E       	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E       	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
E       	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
E       	at java.lang.Thread.run(Thread.java:748)
{code}

",,dianfu,hxb,hxbks2ks,xuannan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28527,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 15:06:00 UTC 2023,,,,,,,,,,"0|z16tdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/22 03:46;dianfu;cc [~hxbks2ks] Could you help to take a look at this issue?;;;","17/Jan/23 15:06;dianfu;Fixed in:
- master via d8417565d6fb7f907f54eeabb8a53ebf790ffad8
- release-1.16 via 4150d709906956044928df1c56001eaa0d81188b
- release-1.15 via 4c7001c0dcff400ff2824bf3e75df184c686f2cd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveDialectITCase.testTableWithSubDirsInPartitionDir failed with AssertJMultipleFailuresError,FLINK-28525,13471303,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,luoyuxia,hxbks2ks,hxbks2ks,13/Jul/22 02:14,14/Jul/22 02:51,13/Jul/23 08:13,14/Jul/22 02:16,1.16.0,,,,,,,1.16.0,,,,,Connectors / Hive,,,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-07-12T04:20:27.9597185Z Jul 12 04:20:27 [ERROR] org.apache.flink.connectors.hive.HiveDialectITCase.testTableWithSubDirsInPartitionDir  Time elapsed: 16.31 s  <<< FAILURE!
2022-07-12T04:20:27.9598238Z Jul 12 04:20:27 org.assertj.core.error.AssertJMultipleFailuresError: 
2022-07-12T04:20:27.9598978Z Jul 12 04:20:27 
2022-07-12T04:20:27.9599419Z Jul 12 04:20:27 Multiple Failures (1 failure)
2022-07-12T04:20:27.9600269Z Jul 12 04:20:27 -- failure 1 --
2022-07-12T04:20:27.9601093Z Jul 12 04:20:27 [Any cause contains message 'Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=2'] 
2022-07-12T04:20:27.9602679Z Jul 12 04:20:27 Expecting any element of:
2022-07-12T04:20:27.9603145Z Jul 12 04:20:27   [org.apache.flink.connectors.hive.FlinkHiveException: java.io.IOException: Fail to create input splits.
2022-07-12T04:20:27.9603766Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:98)
2022-07-12T04:20:27.9604403Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:153)
2022-07-12T04:20:27.9605039Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:117)
2022-07-12T04:20:27.9605902Z Jul 12 04:20:27 	...(75 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
2022-07-12T04:20:27.9606408Z Jul 12 04:20:27     java.io.IOException: Fail to create input splits.
2022-07-12T04:20:27.9606952Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:84)
2022-07-12T04:20:27.9607777Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69)
2022-07-12T04:20:27.9608438Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:158)
2022-07-12T04:20:27.9609261Z Jul 12 04:20:27 	...(79 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
2022-07-12T04:20:27.9609899Z Jul 12 04:20:27     java.util.concurrent.ExecutionException: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:27.9610503Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-07-12T04:20:27.9610998Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-07-12T04:20:27.9611574Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-07-12T04:20:27.9612390Z Jul 12 04:20:27 	...(81 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
2022-07-12T04:20:27.9612953Z Jul 12 04:20:27     java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:27.9613522Z Jul 12 04:20:27 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:20:27.9614119Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:20:27.9614731Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:20:27.9615535Z Jul 12 04:20:27 	...(4 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)]
2022-07-12T04:20:27.9616042Z Jul 12 04:20:27 to satisfy the given assertions requirements but none did:
2022-07-12T04:20:27.9616381Z Jul 12 04:20:27 
2022-07-12T04:20:27.9616795Z Jul 12 04:20:27 org.apache.flink.connectors.hive.FlinkHiveException: java.io.IOException: Fail to create input splits.
2022-07-12T04:20:27.9617407Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:98)
2022-07-12T04:20:27.9618034Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:153)
2022-07-12T04:20:27.9618650Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:117)
2022-07-12T04:20:27.9619455Z Jul 12 04:20:27 	...(75 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
2022-07-12T04:20:27.9619961Z Jul 12 04:20:27 error: 
2022-07-12T04:20:27.9620268Z Jul 12 04:20:27 Expecting throwable message:
2022-07-12T04:20:27.9620642Z Jul 12 04:20:27   ""java.io.IOException: Fail to create input splits.""
2022-07-12T04:20:27.9620989Z Jul 12 04:20:27 to contain:
2022-07-12T04:20:27.9621387Z Jul 12 04:20:27   ""Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=2""
2022-07-12T04:20:27.9621777Z Jul 12 04:20:27 but did not.
2022-07-12T04:20:27.9622042Z Jul 12 04:20:27 
2022-07-12T04:20:27.9622345Z Jul 12 04:20:27 Throwable that failed the check:
2022-07-12T04:20:27.9622644Z Jul 12 04:20:27 
2022-07-12T04:20:27.9623052Z Jul 12 04:20:27 org.apache.flink.connectors.hive.FlinkHiveException: java.io.IOException: Fail to create input splits.
2022-07-12T04:20:27.9623647Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:98)
2022-07-12T04:20:27.9624275Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:153)
2022-07-12T04:20:27.9624882Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:117)
2022-07-12T04:20:27.9625599Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:139)
2022-07-12T04:20:27.9626485Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.java:63)
2022-07-12T04:20:27.9627210Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:20:27.9627963Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:20:27.9628640Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
2022-07-12T04:20:27.9629328Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:20:27.9629957Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:20:27.9630612Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:65)
2022-07-12T04:20:27.9631296Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:20:27.9631965Z Jul 12 04:20:27 	at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:88)
2022-07-12T04:20:27.9632584Z Jul 12 04:20:27 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
2022-07-12T04:20:27.9633096Z Jul 12 04:20:27 	at scala.collection.Iterator.foreach(Iterator.scala:937)
2022-07-12T04:20:27.9633587Z Jul 12 04:20:27 	at scala.collection.Iterator.foreach$(Iterator.scala:937)
2022-07-12T04:20:27.9634077Z Jul 12 04:20:27 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
2022-07-12T04:20:27.9634589Z Jul 12 04:20:27 	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
2022-07-12T04:20:27.9635099Z Jul 12 04:20:27 	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
2022-07-12T04:20:27.9635608Z Jul 12 04:20:27 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2022-07-12T04:20:27.9636121Z Jul 12 04:20:27 	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
2022-07-12T04:20:27.9636639Z Jul 12 04:20:27 	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
2022-07-12T04:20:27.9637466Z Jul 12 04:20:27 	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
2022-07-12T04:20:27.9638064Z Jul 12 04:20:27 	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:87)
2022-07-12T04:20:27.9638770Z Jul 12 04:20:27 	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:182)
2022-07-12T04:20:27.9639397Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1651)
2022-07-12T04:20:27.9640061Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:823)
2022-07-12T04:20:27.9640741Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1312)
2022-07-12T04:20:27.9641395Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:696)
2022-07-12T04:20:27.9642086Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveDialectITCase.lambda$testTableWithSubDirsInPartitionDir$0(HiveDialectITCase.java:595)
2022-07-12T04:20:27.9642733Z Jul 12 04:20:27 	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
2022-07-12T04:20:27.9643331Z Jul 12 04:20:27 	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
2022-07-12T04:20:27.9643915Z Jul 12 04:20:27 	at org.assertj.core.api.Assertions.catchThrowable(Assertions.java:1366)
2022-07-12T04:20:27.9644449Z Jul 12 04:20:27 	at org.assertj.core.api.Assertions.assertThatThrownBy(Assertions.java:1210)
2022-07-12T04:20:27.9645140Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveDialectITCase.testTableWithSubDirsInPartitionDir(HiveDialectITCase.java:592)
2022-07-12T04:20:27.9645726Z Jul 12 04:20:27 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-12T04:20:27.9646233Z Jul 12 04:20:27 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-12T04:20:27.9646817Z Jul 12 04:20:27 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-12T04:20:27.9647352Z Jul 12 04:20:27 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-12T04:20:27.9647880Z Jul 12 04:20:27 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-12T04:20:27.9648472Z Jul 12 04:20:27 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-12T04:20:27.9649053Z Jul 12 04:20:27 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-12T04:20:27.9649639Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-12T04:20:27.9650200Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-12T04:20:27.9650750Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-07-12T04:20:27.9651289Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:20:27.9651847Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-12T04:20:27.9652401Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-12T04:20:27.9652948Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-12T04:20:27.9653527Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-12T04:20:27.9654071Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T04:20:27.9654580Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T04:20:27.9655098Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T04:20:27.9655614Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T04:20:27.9656131Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T04:20:27.9656712Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:20:27.9657210Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T04:20:27.9722967Z Jul 12 04:20:27 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-12T04:20:27.9723517Z Jul 12 04:20:27 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-12T04:20:27.9724476Z Jul 12 04:20:27 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-12T04:20:27.9725099Z Jul 12 04:20:27 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-12T04:20:27.9725699Z Jul 12 04:20:27 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-12T04:20:27.9726331Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-12T04:20:27.9727009Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-12T04:20:27.9727704Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-12T04:20:27.9728566Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-12T04:20:27.9729451Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-12T04:20:27.9730088Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-12T04:20:27.9730690Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-12T04:20:27.9731347Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-12T04:20:27.9732038Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-12T04:20:27.9732705Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-12T04:20:27.9733390Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-12T04:20:27.9734061Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-12T04:20:27.9734703Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-12T04:20:27.9735293Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-12T04:20:27.9735860Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-12T04:20:27.9736414Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-12T04:20:27.9736910Z Jul 12 04:20:27 Caused by: java.io.IOException: Fail to create input splits.
2022-07-12T04:20:27.9737458Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:84)
2022-07-12T04:20:27.9738173Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69)
2022-07-12T04:20:27.9738855Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:158)
2022-07-12T04:20:27.9739527Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107)
2022-07-12T04:20:27.9740200Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:95)
2022-07-12T04:20:27.9740672Z Jul 12 04:20:27 	... 77 more
2022-07-12T04:20:27.9741253Z Jul 12 04:20:27 Caused by: java.util.concurrent.ExecutionException: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:27.9741862Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-07-12T04:20:27.9742357Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-07-12T04:20:27.9742928Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-07-12T04:20:27.9743400Z Jul 12 04:20:27 	... 81 more
2022-07-12T04:20:27.9743836Z Jul 12 04:20:27 Caused by: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:27.9744404Z Jul 12 04:20:27 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:20:27.9744989Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:20:27.9745591Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:20:27.9746145Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-07-12T04:20:27.9746675Z Jul 12 04:20:27 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-12T04:20:27.9747316Z Jul 12 04:20:27 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-12T04:20:27.9747803Z Jul 12 04:20:27 	at java.lang.Thread.run(Thread.java:748)
2022-07-12T04:20:27.9748139Z Jul 12 04:20:27 
2022-07-12T04:20:27.9748386Z Jul 12 04:20:27 
2022-07-12T04:20:27.9748715Z Jul 12 04:20:27 java.io.IOException: Fail to create input splits.
2022-07-12T04:20:27.9749249Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:84)
2022-07-12T04:20:27.9749929Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69)
2022-07-12T04:20:27.9750603Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:158)
2022-07-12T04:20:27.9751758Z Jul 12 04:20:27 	...(79 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
2022-07-12T04:20:27.9752175Z Jul 12 04:20:27 error: 
2022-07-12T04:20:27.9752495Z Jul 12 04:20:27 Expecting throwable message:
2022-07-12T04:20:27.9752841Z Jul 12 04:20:27   ""Fail to create input splits.""
2022-07-12T04:20:27.9753166Z Jul 12 04:20:27 to contain:
2022-07-12T04:20:27.9753564Z Jul 12 04:20:27   ""Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=2""
2022-07-12T04:20:27.9753955Z Jul 12 04:20:27 but did not.
2022-07-12T04:20:27.9754216Z Jul 12 04:20:27 
2022-07-12T04:20:27.9754524Z Jul 12 04:20:27 Throwable that failed the check:
2022-07-12T04:20:27.9754828Z Jul 12 04:20:27 
2022-07-12T04:20:27.9755164Z Jul 12 04:20:27 java.io.IOException: Fail to create input splits.
2022-07-12T04:20:27.9755709Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:84)
2022-07-12T04:20:27.9756384Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69)
2022-07-12T04:20:27.9757050Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:158)
2022-07-12T04:20:27.9757908Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107)
2022-07-12T04:20:27.9758572Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:95)
2022-07-12T04:20:27.9759203Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:153)
2022-07-12T04:20:27.9759822Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:117)
2022-07-12T04:20:27.9760640Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:139)
2022-07-12T04:20:27.9761443Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.java:63)
2022-07-12T04:20:27.9762173Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:20:27.9762815Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:20:27.9763491Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
2022-07-12T04:20:27.9764166Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:20:27.9764806Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:20:27.9765472Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:65)
2022-07-12T04:20:27.9766148Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:20:27.9766881Z Jul 12 04:20:27 	at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:88)
2022-07-12T04:20:27.9767512Z Jul 12 04:20:27 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
2022-07-12T04:20:27.9768037Z Jul 12 04:20:27 	at scala.collection.Iterator.foreach(Iterator.scala:937)
2022-07-12T04:20:27.9768523Z Jul 12 04:20:27 	at scala.collection.Iterator.foreach$(Iterator.scala:937)
2022-07-12T04:20:27.9769024Z Jul 12 04:20:27 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
2022-07-12T04:20:27.9769542Z Jul 12 04:20:27 	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
2022-07-12T04:20:27.9770044Z Jul 12 04:20:27 	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
2022-07-12T04:20:27.9770553Z Jul 12 04:20:27 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2022-07-12T04:20:27.9771063Z Jul 12 04:20:27 	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
2022-07-12T04:20:27.9771580Z Jul 12 04:20:27 	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
2022-07-12T04:20:27.9772093Z Jul 12 04:20:27 	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
2022-07-12T04:20:27.9772667Z Jul 12 04:20:27 	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:87)
2022-07-12T04:20:27.9773281Z Jul 12 04:20:27 	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:182)
2022-07-12T04:20:27.9773908Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1651)
2022-07-12T04:20:27.9774579Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:823)
2022-07-12T04:20:27.9775254Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1312)
2022-07-12T04:20:27.9775912Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:696)
2022-07-12T04:20:27.9776617Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveDialectITCase.lambda$testTableWithSubDirsInPartitionDir$0(HiveDialectITCase.java:595)
2022-07-12T04:20:27.9777275Z Jul 12 04:20:27 	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
2022-07-12T04:20:27.9777862Z Jul 12 04:20:27 	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
2022-07-12T04:20:27.9778525Z Jul 12 04:20:27 	at org.assertj.core.api.Assertions.catchThrowable(Assertions.java:1366)
2022-07-12T04:20:27.9779072Z Jul 12 04:20:27 	at org.assertj.core.api.Assertions.assertThatThrownBy(Assertions.java:1210)
2022-07-12T04:20:27.9779706Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveDialectITCase.testTableWithSubDirsInPartitionDir(HiveDialectITCase.java:592)
2022-07-12T04:20:27.9780296Z Jul 12 04:20:27 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-12T04:20:27.9780807Z Jul 12 04:20:27 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-12T04:20:27.9781391Z Jul 12 04:20:27 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-12T04:20:27.9781907Z Jul 12 04:20:27 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-12T04:20:27.9782524Z Jul 12 04:20:27 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-12T04:20:27.9783120Z Jul 12 04:20:27 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-12T04:20:27.9783704Z Jul 12 04:20:27 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-12T04:20:27.9784279Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-12T04:20:27.9784944Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-12T04:20:27.9785490Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-07-12T04:20:27.9786026Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:20:27.9786585Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-12T04:20:27.9787132Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-12T04:20:27.9787690Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-12T04:20:27.9788275Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-12T04:20:27.9788816Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T04:20:27.9789316Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T04:20:27.9789838Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T04:20:27.9790363Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T04:20:27.9790877Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T04:20:27.9791399Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:20:27.9791901Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T04:20:27.9792377Z Jul 12 04:20:27 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-12T04:20:27.9792839Z Jul 12 04:20:27 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-12T04:20:27.9793360Z Jul 12 04:20:27 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-12T04:20:27.9793956Z Jul 12 04:20:27 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-12T04:20:27.9794543Z Jul 12 04:20:27 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-12T04:20:27.9795168Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-12T04:20:27.9795836Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-12T04:20:27.9796519Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-12T04:20:27.9797397Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-12T04:20:27.9798108Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-12T04:20:27.9798739Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-12T04:20:27.9799324Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-12T04:20:27.9799964Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-12T04:20:27.9800645Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-12T04:20:27.9801294Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-12T04:20:27.9801978Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-12T04:20:27.9802649Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-12T04:20:27.9803369Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-12T04:20:27.9803953Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-12T04:20:27.9804509Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-12T04:20:27.9805056Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-12T04:20:27.9805680Z Jul 12 04:20:27 Caused by: java.util.concurrent.ExecutionException: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:27.9806293Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-07-12T04:20:27.9806789Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-07-12T04:20:27.9807358Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-07-12T04:20:27.9807830Z Jul 12 04:20:27 	... 81 more
2022-07-12T04:20:27.9808264Z Jul 12 04:20:27 Caused by: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:27.9808826Z Jul 12 04:20:27 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:20:27.9809412Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:20:27.9810020Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:20:27.9810574Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-07-12T04:20:27.9811101Z Jul 12 04:20:27 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-12T04:20:27.9811668Z Jul 12 04:20:27 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-12T04:20:27.9812156Z Jul 12 04:20:27 	at java.lang.Thread.run(Thread.java:748)
2022-07-12T04:20:27.9812487Z Jul 12 04:20:27 
2022-07-12T04:20:27.9812743Z Jul 12 04:20:27 
2022-07-12T04:20:27.9813208Z Jul 12 04:20:27 java.util.concurrent.ExecutionException: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:27.9813791Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-07-12T04:20:27.9814285Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-07-12T04:20:27.9814846Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-07-12T04:20:27.9815836Z Jul 12 04:20:27 	...(81 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
2022-07-12T04:20:27.9816265Z Jul 12 04:20:27 error: 
2022-07-12T04:20:27.9816580Z Jul 12 04:20:27 Expecting throwable message:
2022-07-12T04:20:27.9817029Z Jul 12 04:20:27   ""java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1""
2022-07-12T04:20:27.9817457Z Jul 12 04:20:27 to contain:
2022-07-12T04:20:27.9817840Z Jul 12 04:20:27   ""Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=2""
2022-07-12T04:20:27.9818229Z Jul 12 04:20:27 but did not.
2022-07-12T04:20:27.9818500Z Jul 12 04:20:27 
2022-07-12T04:20:27.9818802Z Jul 12 04:20:27 Throwable that failed the check:
2022-07-12T04:20:27.9819103Z Jul 12 04:20:27 
2022-07-12T04:20:27.9819563Z Jul 12 04:20:27 java.util.concurrent.ExecutionException: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:27.9820148Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-07-12T04:20:27.9820648Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-07-12T04:20:27.9821221Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-07-12T04:20:27.9821978Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69)
2022-07-12T04:20:27.9822648Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:158)
2022-07-12T04:20:27.9823329Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107)
2022-07-12T04:20:27.9823988Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:95)
2022-07-12T04:20:27.9824614Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:153)
2022-07-12T04:20:27.9825236Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:117)
2022-07-12T04:20:27.9825950Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:139)
2022-07-12T04:20:27.9826762Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.java:63)
2022-07-12T04:20:27.9827488Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:20:27.9828118Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:20:27.9828783Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
2022-07-12T04:20:27.9829448Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:20:27.9830080Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:20:27.9830742Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:65)
2022-07-12T04:20:27.9831416Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:20:27.9832074Z Jul 12 04:20:27 	at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:88)
2022-07-12T04:20:27.9832689Z Jul 12 04:20:27 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
2022-07-12T04:20:27.9833281Z Jul 12 04:20:27 	at scala.collection.Iterator.foreach(Iterator.scala:937)
2022-07-12T04:20:27.9833749Z Jul 12 04:20:27 	at scala.collection.Iterator.foreach$(Iterator.scala:937)
2022-07-12T04:20:27.9834242Z Jul 12 04:20:27 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
2022-07-12T04:20:27.9834755Z Jul 12 04:20:27 	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
2022-07-12T04:20:27.9835265Z Jul 12 04:20:27 	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
2022-07-12T04:20:27.9835774Z Jul 12 04:20:27 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2022-07-12T04:20:27.9836281Z Jul 12 04:20:27 	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
2022-07-12T04:20:27.9836791Z Jul 12 04:20:27 	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
2022-07-12T04:20:27.9837308Z Jul 12 04:20:27 	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
2022-07-12T04:20:27.9878683Z Jul 12 04:20:27 	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:87)
2022-07-12T04:20:27.9879361Z Jul 12 04:20:27 	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:182)
2022-07-12T04:20:27.9880007Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1651)
2022-07-12T04:20:27.9880992Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:823)
2022-07-12T04:20:27.9881689Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1312)
2022-07-12T04:20:27.9882343Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:696)
2022-07-12T04:20:27.9883042Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveDialectITCase.lambda$testTableWithSubDirsInPartitionDir$0(HiveDialectITCase.java:595)
2022-07-12T04:20:27.9883708Z Jul 12 04:20:27 	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
2022-07-12T04:20:27.9884312Z Jul 12 04:20:27 	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
2022-07-12T04:20:27.9884898Z Jul 12 04:20:27 	at org.assertj.core.api.Assertions.catchThrowable(Assertions.java:1366)
2022-07-12T04:20:27.9885452Z Jul 12 04:20:27 	at org.assertj.core.api.Assertions.assertThatThrownBy(Assertions.java:1210)
2022-07-12T04:20:27.9886078Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveDialectITCase.testTableWithSubDirsInPartitionDir(HiveDialectITCase.java:592)
2022-07-12T04:20:27.9886667Z Jul 12 04:20:27 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-12T04:20:27.9887185Z Jul 12 04:20:27 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-12T04:20:27.9887783Z Jul 12 04:20:27 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-12T04:20:27.9888328Z Jul 12 04:20:27 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-12T04:20:27.9888861Z Jul 12 04:20:27 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-12T04:20:27.9889461Z Jul 12 04:20:27 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-12T04:20:27.9890038Z Jul 12 04:20:27 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-12T04:20:27.9890627Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-12T04:20:27.9891205Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-12T04:20:27.9891767Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-07-12T04:20:27.9892302Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:20:27.9892954Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-12T04:20:27.9893499Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-12T04:20:27.9894038Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-12T04:20:27.9894619Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-12T04:20:27.9895166Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T04:20:27.9895671Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T04:20:27.9896190Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T04:20:27.9896702Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T04:20:27.9897205Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T04:20:27.9897720Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:20:27.9898220Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T04:20:27.9898697Z Jul 12 04:20:27 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-12T04:20:27.9899162Z Jul 12 04:20:27 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-12T04:20:27.9899737Z Jul 12 04:20:27 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-12T04:20:27.9900321Z Jul 12 04:20:27 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-12T04:20:27.9900916Z Jul 12 04:20:27 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-12T04:20:27.9901535Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-12T04:20:27.9902218Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-12T04:20:27.9902915Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-12T04:20:27.9903631Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-12T04:20:27.9904328Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-12T04:20:27.9904956Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-12T04:20:27.9905537Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-12T04:20:27.9906176Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-12T04:20:27.9906872Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-12T04:20:27.9907535Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-12T04:20:27.9908206Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-12T04:20:27.9908884Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-12T04:20:27.9909506Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-12T04:20:27.9934332Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-12T04:20:27.9935034Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-12T04:20:27.9935850Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-12T04:20:27.9936432Z Jul 12 04:20:27 Caused by: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:27.9937008Z Jul 12 04:20:27 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:20:28.7230243Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:20:28.7230961Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:20:28.7231529Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-07-12T04:20:28.7232062Z Jul 12 04:20:27 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-12T04:20:28.7232641Z Jul 12 04:20:27 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-12T04:20:28.7233171Z Jul 12 04:20:27 	at java.lang.Thread.run(Thread.java:748)
2022-07-12T04:20:28.7233510Z Jul 12 04:20:27 
2022-07-12T04:20:28.7233772Z Jul 12 04:20:27 
2022-07-12T04:20:28.7234182Z Jul 12 04:20:27 java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:28.7234741Z Jul 12 04:20:27 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:20:28.7235657Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:20:28.7236274Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:20:28.7237640Z Jul 12 04:20:27 	...(4 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
2022-07-12T04:20:28.7238085Z Jul 12 04:20:27 error: 
2022-07-12T04:20:28.7238400Z Jul 12 04:20:27 Expecting throwable message:
2022-07-12T04:20:28.7238827Z Jul 12 04:20:27   ""Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1""
2022-07-12T04:20:28.7239227Z Jul 12 04:20:27 to contain:
2022-07-12T04:20:28.7239617Z Jul 12 04:20:27   ""Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=2""
2022-07-12T04:20:28.7240010Z Jul 12 04:20:27 but did not.
2022-07-12T04:20:28.7240288Z Jul 12 04:20:27 
2022-07-12T04:20:28.7240585Z Jul 12 04:20:27 Throwable that failed the check:
2022-07-12T04:20:28.7240889Z Jul 12 04:20:27 
2022-07-12T04:20:28.7241300Z Jul 12 04:20:27 java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:28.7241859Z Jul 12 04:20:27 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:20:28.7242447Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:20:28.7243063Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:20:28.7243611Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-07-12T04:20:28.7244140Z Jul 12 04:20:27 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-12T04:20:28.7244726Z Jul 12 04:20:27 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-12T04:20:28.7245219Z Jul 12 04:20:27 	at java.lang.Thread.run(Thread.java:748)
2022-07-12T04:20:28.7245692Z Jul 12 04:20:27 at FlinkAssertions.lambda$anyCauseMatches$5(FlinkAssertions.java:106)
2022-07-12T04:20:28.7246209Z Jul 12 04:20:27 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2022-07-12T04:20:28.7246769Z Jul 12 04:20:27 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2022-07-12T04:20:28.7247411Z Jul 12 04:20:27 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2022-07-12T04:20:28.7248230Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveDialectITCase.testTableWithSubDirsInPartitionDir(HiveDialectITCase.java:597)
2022-07-12T04:20:28.7248805Z Jul 12 04:20:27 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-12T04:20:28.7249320Z Jul 12 04:20:27 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-12T04:20:28.7249915Z Jul 12 04:20:27 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-12T04:20:28.7250452Z Jul 12 04:20:27 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-12T04:20:28.7250974Z Jul 12 04:20:27 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-12T04:20:28.7251564Z Jul 12 04:20:27 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-12T04:20:28.7252146Z Jul 12 04:20:27 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-12T04:20:28.7252732Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-12T04:20:28.7253299Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-12T04:20:28.7253855Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-07-12T04:20:28.7254446Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:20:28.7255001Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-12T04:20:28.7255548Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-12T04:20:28.7256098Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-12T04:20:28.7256673Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-12T04:20:28.7257223Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T04:20:28.7257726Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T04:20:28.7258233Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T04:20:28.7258749Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T04:20:28.7259266Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T04:20:28.7259785Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:20:28.7260282Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T04:20:28.7260767Z Jul 12 04:20:27 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-12T04:20:28.7261220Z Jul 12 04:20:27 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-12T04:20:28.7261749Z Jul 12 04:20:27 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-12T04:20:28.7262348Z Jul 12 04:20:27 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-12T04:20:28.7262934Z Jul 12 04:20:27 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-12T04:20:28.7263557Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-12T04:20:28.7264243Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-12T04:20:28.7264934Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-12T04:20:28.7265650Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-12T04:20:28.7266415Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-12T04:20:28.7267051Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-12T04:20:28.7267638Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-12T04:20:28.7268289Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-12T04:20:28.7268973Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-12T04:20:28.7269643Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-12T04:20:28.7270312Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-12T04:20:28.7270981Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-12T04:20:28.7271612Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-12T04:20:28.7272191Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-12T04:20:28.7272809Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-12T04:20:28.7273356Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-12T04:20:28.7273758Z Jul 12 04:20:27 
2022-07-12T04:20:30.2231809Z Jul 12 04:20:29 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2022-07-12T04:20:31.4694903Z Jul 12 04:20:31 [INFO] Running org.apache.flink.table.catalog.hive.HiveCatalogITCase
2022-07-12T04:21:11.2730557Z Jul 12 04:21:11 [WARNING] Tests run: 17, Failures: 0, Errors: 0, Skipped: 3, Time elapsed: 168.97 s - in org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase
2022-07-12T04:21:12.4779976Z Jul 12 04:21:12 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2022-07-12T04:21:14.4325471Z Jul 12 04:21:14 [INFO] Running org.apache.flink.table.catalog.hive.HiveCatalogUdfITCase
2022-07-12T04:21:37.8028634Z Jul 12 04:21:37 [INFO] Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 66.271 s - in org.apache.flink.table.catalog.hive.HiveCatalogITCase
2022-07-12T04:21:44.7479437Z Jul 12 04:21:44 [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 30.298 s - in org.apache.flink.table.catalog.hive.HiveCatalogUdfITCase
2022-07-12T04:21:45.1103445Z Jul 12 04:21:45 [INFO] 
2022-07-12T04:21:45.1104396Z Jul 12 04:21:45 [INFO] Results:
2022-07-12T04:21:45.1104941Z Jul 12 04:21:45 [INFO] 
2022-07-12T04:21:45.1109327Z Jul 12 04:21:45 [ERROR] Failures: 
2022-07-12T04:21:45.1114586Z Jul 12 04:21:45 [ERROR]   HiveDialectITCase.testTableWithSubDirsInPartitionDir:597 
2022-07-12T04:21:45.1115440Z Jul 12 04:21:45 Multiple Failures (1 failure)
2022-07-12T04:21:45.1116520Z Jul 12 04:21:45 -- failure 1 --
2022-07-12T04:21:45.1117948Z Jul 12 04:21:45 [Any cause contains message 'Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=2'] 
2022-07-12T04:21:45.1164459Z Jul 12 04:21:45 Expecting any element of:
2022-07-12T04:21:45.1165322Z Jul 12 04:21:45   [org.apache.flink.connectors.hive.FlinkHiveException: java.io.IOException: Fail to create input splits.
2022-07-12T04:21:45.1169910Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:98)
2022-07-12T04:21:45.1171238Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:153)
2022-07-12T04:21:45.1172885Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:117)
2022-07-12T04:21:45.1174617Z Jul 12 04:21:45 	...(75 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
2022-07-12T04:21:45.1175568Z Jul 12 04:21:45     java.io.IOException: Fail to create input splits.
2022-07-12T04:21:45.1176551Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:84)
2022-07-12T04:21:45.1177691Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69)
2022-07-12T04:21:45.1179025Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:158)
2022-07-12T04:21:45.1180609Z Jul 12 04:21:45 	...(79 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
2022-07-12T04:21:45.1181829Z Jul 12 04:21:45     java.util.concurrent.ExecutionException: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1183614Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-07-12T04:21:45.1184567Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-07-12T04:21:45.1185713Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-07-12T04:21:45.1187373Z Jul 12 04:21:45 	...(81 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
2022-07-12T04:21:45.1188746Z Jul 12 04:21:45     java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1189885Z Jul 12 04:21:45 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:21:45.1191052Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:21:45.1192243Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:21:45.1193984Z Jul 12 04:21:45 	...(4 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)]
2022-07-12T04:21:45.1194951Z Jul 12 04:21:45 to satisfy the given assertions requirements but none did:
2022-07-12T04:21:45.1195546Z Jul 12 04:21:45 
2022-07-12T04:21:45.1196295Z Jul 12 04:21:45 org.apache.flink.connectors.hive.FlinkHiveException: java.io.IOException: Fail to create input splits.
2022-07-12T04:21:45.1197109Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:98)
2022-07-12T04:21:45.1206332Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:153)
2022-07-12T04:21:45.1207287Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:117)
2022-07-12T04:21:45.1208609Z Jul 12 04:21:45 	...(75 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
2022-07-12T04:21:45.1209119Z Jul 12 04:21:45 error: 
2022-07-12T04:21:45.1209475Z Jul 12 04:21:45 Expecting throwable message:
2022-07-12T04:21:45.1209894Z Jul 12 04:21:45   ""java.io.IOException: Fail to create input splits.""
2022-07-12T04:21:45.1210290Z Jul 12 04:21:45 to contain:
2022-07-12T04:21:45.1210719Z Jul 12 04:21:45   ""Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=2""
2022-07-12T04:21:45.1211164Z Jul 12 04:21:45 but did not.
2022-07-12T04:21:45.1211465Z Jul 12 04:21:45 
2022-07-12T04:21:45.1211803Z Jul 12 04:21:45 Throwable that failed the check:
2022-07-12T04:21:45.1214141Z Jul 12 04:21:45 
2022-07-12T04:21:45.1214598Z Jul 12 04:21:45 org.apache.flink.connectors.hive.FlinkHiveException: java.io.IOException: Fail to create input splits.
2022-07-12T04:21:45.1215300Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:98)
2022-07-12T04:21:45.1216027Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:153)
2022-07-12T04:21:45.1216931Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:117)
2022-07-12T04:21:45.1217774Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:139)
2022-07-12T04:21:45.1218721Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.java:63)
2022-07-12T04:21:45.1220057Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:21:45.1220779Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:21:45.1221550Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
2022-07-12T04:21:45.1222353Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:21:45.1223080Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:21:45.1223845Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:65)
2022-07-12T04:21:45.1224716Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:21:45.1225484Z Jul 12 04:21:45 	at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:88)
2022-07-12T04:21:45.1226184Z Jul 12 04:21:45 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
2022-07-12T04:21:45.1226770Z Jul 12 04:21:45 	at scala.collection.Iterator.foreach(Iterator.scala:937)
2022-07-12T04:21:45.1227321Z Jul 12 04:21:45 	at scala.collection.Iterator.foreach$(Iterator.scala:937)
2022-07-12T04:21:45.1227885Z Jul 12 04:21:45 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
2022-07-12T04:21:45.1228466Z Jul 12 04:21:45 	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
2022-07-12T04:21:45.1229041Z Jul 12 04:21:45 	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
2022-07-12T04:21:45.1229612Z Jul 12 04:21:45 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2022-07-12T04:21:45.1230191Z Jul 12 04:21:45 	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
2022-07-12T04:21:45.1230779Z Jul 12 04:21:45 	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
2022-07-12T04:21:45.1231364Z Jul 12 04:21:45 	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
2022-07-12T04:21:45.1232013Z Jul 12 04:21:45 	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:87)
2022-07-12T04:21:45.1232725Z Jul 12 04:21:45 	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:182)
2022-07-12T04:21:45.1233441Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1651)
2022-07-12T04:21:45.1234205Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:823)
2022-07-12T04:21:45.1234997Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1312)
2022-07-12T04:21:45.1235754Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:696)
2022-07-12T04:21:45.1236549Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveDialectITCase.lambda$testTableWithSubDirsInPartitionDir$0(HiveDialectITCase.java:595)
2022-07-12T04:21:45.1237303Z Jul 12 04:21:45 	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
2022-07-12T04:21:45.1238162Z Jul 12 04:21:45 	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
2022-07-12T04:21:45.1238831Z Jul 12 04:21:45 	at org.assertj.core.api.Assertions.catchThrowable(Assertions.java:1366)
2022-07-12T04:21:45.1239441Z Jul 12 04:21:45 	at org.assertj.core.api.Assertions.assertThatThrownBy(Assertions.java:1210)
2022-07-12T04:21:45.1240167Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveDialectITCase.testTableWithSubDirsInPartitionDir(HiveDialectITCase.java:592)
2022-07-12T04:21:45.1240837Z Jul 12 04:21:45 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-12T04:21:45.1241419Z Jul 12 04:21:45 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-12T04:21:45.1242098Z Jul 12 04:21:45 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-12T04:21:45.1242717Z Jul 12 04:21:45 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-12T04:21:45.1243315Z Jul 12 04:21:45 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-12T04:21:45.1243995Z Jul 12 04:21:45 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-12T04:21:45.1244672Z Jul 12 04:21:45 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-12T04:21:45.1245398Z Jul 12 04:21:45 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-12T04:21:45.1246054Z Jul 12 04:21:45 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-12T04:21:45.1246696Z Jul 12 04:21:45 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-07-12T04:21:45.1247309Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:21:45.1247934Z Jul 12 04:21:45 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-12T04:21:45.1248573Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-12T04:21:45.1249199Z Jul 12 04:21:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-12T04:21:45.1249872Z Jul 12 04:21:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-12T04:21:45.1250496Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T04:21:45.1251073Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T04:21:45.1251651Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T04:21:45.1252239Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T04:21:45.1252824Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T04:21:45.1253411Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:21:45.1253983Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T04:21:45.1254523Z Jul 12 04:21:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-12T04:21:45.1255038Z Jul 12 04:21:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-12T04:21:45.1255633Z Jul 12 04:21:45 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-12T04:21:45.1256329Z Jul 12 04:21:45 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-12T04:21:45.1257001Z Jul 12 04:21:45 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-12T04:21:45.1257716Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-12T04:21:45.1258497Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-12T04:21:45.1259355Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-12T04:21:45.1260182Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-12T04:21:45.1261005Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-12T04:21:45.1261740Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-12T04:21:45.1262408Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-12T04:21:45.1263151Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-12T04:21:45.1263944Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-12T04:21:45.1264708Z Jul 12 04:21:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-12T04:21:45.1265478Z Jul 12 04:21:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-12T04:21:45.1266304Z Jul 12 04:21:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-12T04:21:45.1267029Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-12T04:21:45.1267701Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-12T04:21:45.1268333Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-12T04:21:45.1268960Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-12T04:21:45.1269520Z Jul 12 04:21:45 Caused by: java.io.IOException: Fail to create input splits.
2022-07-12T04:21:45.1270134Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:84)
2022-07-12T04:21:45.1270913Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69)
2022-07-12T04:21:45.1271683Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:158)
2022-07-12T04:21:45.1272453Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107)
2022-07-12T04:21:45.1273222Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:95)
2022-07-12T04:21:45.1273756Z Jul 12 04:21:45 	... 77 more
2022-07-12T04:21:45.1274313Z Jul 12 04:21:45 Caused by: java.util.concurrent.ExecutionException: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1274998Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-07-12T04:21:45.1275556Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-07-12T04:21:45.1276205Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-07-12T04:21:45.1276748Z Jul 12 04:21:45 	... 81 more
2022-07-12T04:21:45.1277237Z Jul 12 04:21:45 Caused by: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1280605Z Jul 12 04:21:45 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:21:45.1281285Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:21:45.1281989Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:21:45.1282727Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-07-12T04:21:45.1283330Z Jul 12 04:21:45 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-12T04:21:45.1283990Z Jul 12 04:21:45 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-12T04:21:45.1284555Z Jul 12 04:21:45 	at java.lang.Thread.run(Thread.java:748)
2022-07-12T04:21:45.1284930Z Jul 12 04:21:45 
2022-07-12T04:21:45.1285205Z Jul 12 04:21:45 
2022-07-12T04:21:45.1285569Z Jul 12 04:21:45 java.io.IOException: Fail to create input splits.
2022-07-12T04:21:45.1286176Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:84)
2022-07-12T04:21:45.1286954Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69)
2022-07-12T04:21:45.1287727Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:158)
2022-07-12T04:21:45.1288795Z Jul 12 04:21:45 	...(79 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
2022-07-12T04:21:45.1289279Z Jul 12 04:21:45 error: 
2022-07-12T04:21:45.1289625Z Jul 12 04:21:45 Expecting throwable message:
2022-07-12T04:21:45.1290077Z Jul 12 04:21:45   ""Fail to create input splits.""
2022-07-12T04:21:45.1290430Z Jul 12 04:21:45 to contain:
2022-07-12T04:21:45.1297758Z Jul 12 04:21:45   ""Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=2""
2022-07-12T04:21:45.1298208Z Jul 12 04:21:45 but did not.
2022-07-12T04:21:45.1298510Z Jul 12 04:21:45 
2022-07-12T04:21:45.1298845Z Jul 12 04:21:45 Throwable that failed the check:
2022-07-12T04:21:45.1299180Z Jul 12 04:21:45 
2022-07-12T04:21:45.1299551Z Jul 12 04:21:45 java.io.IOException: Fail to create input splits.
2022-07-12T04:21:45.1300164Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:84)
2022-07-12T04:21:45.1300955Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69)
2022-07-12T04:21:45.1301855Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:158)
2022-07-12T04:21:45.1302639Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107)
2022-07-12T04:21:45.1303495Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:95)
2022-07-12T04:21:45.1304232Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:153)
2022-07-12T04:21:45.1304952Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:117)
2022-07-12T04:21:45.1305863Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:139)
2022-07-12T04:21:45.1306812Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.java:63)
2022-07-12T04:21:45.1307747Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:21:45.1308485Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:21:45.1309343Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
2022-07-12T04:21:45.1310135Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:21:45.1310872Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:21:45.1311875Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:65)
2022-07-12T04:21:45.1312643Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:21:45.1313493Z Jul 12 04:21:45 	at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:88)
2022-07-12T04:21:45.1314287Z Jul 12 04:21:45 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
2022-07-12T04:21:45.1314882Z Jul 12 04:21:45 	at scala.collection.Iterator.foreach(Iterator.scala:937)
2022-07-12T04:21:45.1315530Z Jul 12 04:21:45 	at scala.collection.Iterator.foreach$(Iterator.scala:937)
2022-07-12T04:21:45.1316104Z Jul 12 04:21:45 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
2022-07-12T04:21:45.1316719Z Jul 12 04:21:45 	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
2022-07-12T04:21:45.1317298Z Jul 12 04:21:45 	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
2022-07-12T04:21:45.1318086Z Jul 12 04:21:45 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2022-07-12T04:21:45.1318676Z Jul 12 04:21:45 	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
2022-07-12T04:21:45.1319352Z Jul 12 04:21:45 	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
2022-07-12T04:21:45.1320028Z Jul 12 04:21:45 	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
2022-07-12T04:21:45.1320675Z Jul 12 04:21:45 	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:87)
2022-07-12T04:21:45.1321393Z Jul 12 04:21:45 	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:182)
2022-07-12T04:21:45.1322201Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1651)
2022-07-12T04:21:45.1322986Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:823)
2022-07-12T04:21:45.1323783Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1312)
2022-07-12T04:21:45.1324543Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:696)
2022-07-12T04:21:45.1326914Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveDialectITCase.lambda$testTableWithSubDirsInPartitionDir$0(HiveDialectITCase.java:595)
2022-07-12T04:21:45.1333950Z Jul 12 04:21:45 	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
2022-07-12T04:21:45.1334661Z Jul 12 04:21:45 	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
2022-07-12T04:21:45.1335343Z Jul 12 04:21:45 	at org.assertj.core.api.Assertions.catchThrowable(Assertions.java:1366)
2022-07-12T04:21:45.1335981Z Jul 12 04:21:45 	at org.assertj.core.api.Assertions.assertThatThrownBy(Assertions.java:1210)
2022-07-12T04:21:45.1336712Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveDialectITCase.testTableWithSubDirsInPartitionDir(HiveDialectITCase.java:592)
2022-07-12T04:21:45.1337873Z Jul 12 04:21:45 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-12T04:21:45.1338473Z Jul 12 04:21:45 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-12T04:21:45.1339141Z Jul 12 04:21:45 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-12T04:21:45.1339753Z Jul 12 04:21:45 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-12T04:21:45.1340457Z Jul 12 04:21:45 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-12T04:21:45.1341151Z Jul 12 04:21:45 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-12T04:21:45.1341936Z Jul 12 04:21:45 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-12T04:21:45.1342615Z Jul 12 04:21:45 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-12T04:21:45.1343275Z Jul 12 04:21:45 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-12T04:21:45.1343911Z Jul 12 04:21:45 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-07-12T04:21:45.1344655Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:21:45.1345791Z Jul 12 04:21:45 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-12T04:21:45.1348562Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-12T04:21:45.1349893Z Jul 12 04:21:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-12T04:21:45.1350958Z Jul 12 04:21:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-12T04:21:45.1351785Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T04:21:45.1352945Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T04:21:45.1353939Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T04:21:45.1354928Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T04:21:45.1355531Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T04:21:45.1356127Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:21:45.1356688Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T04:21:45.1357231Z Jul 12 04:21:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-12T04:21:45.1357933Z Jul 12 04:21:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-12T04:21:45.1358645Z Jul 12 04:21:45 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-12T04:21:45.1359775Z Jul 12 04:21:45 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-12T04:21:45.1361251Z Jul 12 04:21:45 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-12T04:21:45.1362006Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-12T04:21:45.1363351Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-12T04:21:45.1364799Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-12T04:21:45.1366301Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-12T04:21:45.1367158Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-12T04:21:45.1368527Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-12T04:21:45.1369798Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-12T04:21:45.1370757Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-12T04:21:45.1371989Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-12T04:21:45.1373338Z Jul 12 04:21:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-12T04:21:45.1374888Z Jul 12 04:21:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-12T04:21:45.1375890Z Jul 12 04:21:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-12T04:21:45.1377089Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-12T04:21:45.1378344Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-12T04:21:45.1379375Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-12T04:21:45.1380320Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-12T04:21:45.1381577Z Jul 12 04:21:45 Caused by: java.util.concurrent.ExecutionException: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1382937Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-07-12T04:21:45.1383883Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-07-12T04:21:45.1384769Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-07-12T04:21:45.1385323Z Jul 12 04:21:45 	... 81 more
2022-07-12T04:21:45.1385899Z Jul 12 04:21:45 Caused by: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1386561Z Jul 12 04:21:45 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:21:45.1387241Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:21:45.1387947Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:21:45.1388578Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-07-12T04:21:45.1389191Z Jul 12 04:21:45 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-12T04:21:45.1389852Z Jul 12 04:21:45 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-12T04:21:45.1390975Z Jul 12 04:21:45 	at java.lang.Thread.run(Thread.java:748)
2022-07-12T04:21:45.1391385Z Jul 12 04:21:45 
2022-07-12T04:21:45.1391680Z Jul 12 04:21:45 
2022-07-12T04:21:45.1392211Z Jul 12 04:21:45 java.util.concurrent.ExecutionException: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1395911Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-07-12T04:21:45.1396951Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-07-12T04:21:45.1398285Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-07-12T04:21:45.1400980Z Jul 12 04:21:45 	...(81 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
2022-07-12T04:21:45.1402240Z Jul 12 04:21:45 error: 
2022-07-12T04:21:45.1403021Z Jul 12 04:21:45 Expecting throwable message:
2022-07-12T04:21:45.1404120Z Jul 12 04:21:45   ""java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1""
2022-07-12T04:21:45.1404837Z Jul 12 04:21:45 to contain:
2022-07-12T04:21:45.1405642Z Jul 12 04:21:45   ""Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=2""
2022-07-12T04:21:45.1410624Z Jul 12 04:21:45 but did not.
2022-07-12T04:21:45.1411207Z Jul 12 04:21:45 
2022-07-12T04:21:45.1411752Z Jul 12 04:21:45 Throwable that failed the check:
2022-07-12T04:21:45.1412578Z Jul 12 04:21:45 
2022-07-12T04:21:45.1413115Z Jul 12 04:21:45 java.util.concurrent.ExecutionException: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1414559Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-07-12T04:21:45.1415593Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-07-12T04:21:45.1416710Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-07-12T04:21:45.1417988Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69)
2022-07-12T04:21:45.1419359Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:158)
2022-07-12T04:21:45.1420383Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107)
2022-07-12T04:21:45.1421618Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:95)
2022-07-12T04:21:45.1422780Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:153)
2022-07-12T04:21:45.1423885Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:117)
2022-07-12T04:21:45.1425187Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:139)
2022-07-12T04:21:45.1426702Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.java:63)
2022-07-12T04:21:45.1427775Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:21:45.1429100Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:21:45.1430140Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
2022-07-12T04:21:45.1431275Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:21:45.1432271Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:21:45.1433302Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:65)
2022-07-12T04:21:45.1434569Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:21:45.1435867Z Jul 12 04:21:45 	at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:88)
2022-07-12T04:21:45.1436847Z Jul 12 04:21:45 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
2022-07-12T04:21:45.1450452Z Jul 12 04:21:45 	at scala.collection.Iterator.foreach(Iterator.scala:937)
2022-07-12T04:21:45.1456329Z Jul 12 04:21:45 	at scala.collection.Iterator.foreach$(Iterator.scala:937)
2022-07-12T04:21:45.1457063Z Jul 12 04:21:45 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
2022-07-12T04:21:45.1457657Z Jul 12 04:21:45 	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
2022-07-12T04:21:45.1458245Z Jul 12 04:21:45 	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
2022-07-12T04:21:45.1458840Z Jul 12 04:21:45 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2022-07-12T04:21:45.1459409Z Jul 12 04:21:45 	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
2022-07-12T04:21:45.1460006Z Jul 12 04:21:45 	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
2022-07-12T04:21:45.1460602Z Jul 12 04:21:45 	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
2022-07-12T04:21:45.1462100Z Jul 12 04:21:45 	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:87)
2022-07-12T04:21:45.1466655Z Jul 12 04:21:45 	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:182)
2022-07-12T04:21:45.1467395Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1651)
2022-07-12T04:21:45.1468177Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:823)
2022-07-12T04:21:45.1469061Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1312)
2022-07-12T04:21:45.1469831Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:696)
2022-07-12T04:21:45.1472878Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveDialectITCase.lambda$testTableWithSubDirsInPartitionDir$0(HiveDialectITCase.java:595)
2022-07-12T04:21:45.1477862Z Jul 12 04:21:45 	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
2022-07-12T04:21:45.1478585Z Jul 12 04:21:45 	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
2022-07-12T04:21:45.1479268Z Jul 12 04:21:45 	at org.assertj.core.api.Assertions.catchThrowable(Assertions.java:1366)
2022-07-12T04:21:45.1479897Z Jul 12 04:21:45 	at org.assertj.core.api.Assertions.assertThatThrownBy(Assertions.java:1210)
2022-07-12T04:21:45.1480748Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveDialectITCase.testTableWithSubDirsInPartitionDir(HiveDialectITCase.java:592)
2022-07-12T04:21:45.1481429Z Jul 12 04:21:45 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-12T04:21:45.1482018Z Jul 12 04:21:45 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-12T04:21:45.1482702Z Jul 12 04:21:45 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-12T04:21:45.1483318Z Jul 12 04:21:45 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-12T04:21:45.1483924Z Jul 12 04:21:45 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-12T04:21:45.1484607Z Jul 12 04:21:45 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-12T04:21:45.1485272Z Jul 12 04:21:45 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-12T04:21:45.1485948Z Jul 12 04:21:45 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-12T04:21:45.1486608Z Jul 12 04:21:45 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-12T04:21:45.1487262Z Jul 12 04:21:45 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-07-12T04:21:45.1487879Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:21:45.1488520Z Jul 12 04:21:45 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-12T04:21:45.1489142Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-12T04:21:45.1489773Z Jul 12 04:21:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-12T04:21:45.1490449Z Jul 12 04:21:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-12T04:21:45.1491075Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T04:21:45.1491654Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T04:21:45.1492245Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T04:21:45.1492826Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T04:21:45.1495285Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T04:21:45.1497986Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:21:45.1500206Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T04:21:45.1500764Z Jul 12 04:21:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-12T04:21:45.1501293Z Jul 12 04:21:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-12T04:21:45.1508220Z Jul 12 04:21:45 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-12T04:21:45.1509438Z Jul 12 04:21:45 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-12T04:21:45.1511339Z Jul 12 04:21:45 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-12T04:21:45.1558574Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-12T04:21:45.1559403Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-12T04:21:45.1560236Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-12T04:21:45.1561086Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-12T04:21:45.1562106Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-12T04:21:45.1562844Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-12T04:21:45.1563528Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-12T04:21:45.1564281Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-12T04:21:45.1565084Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-12T04:21:45.1565859Z Jul 12 04:21:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-12T04:21:45.1566830Z Jul 12 04:21:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-12T04:21:45.1567622Z Jul 12 04:21:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-12T04:21:45.1568344Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-12T04:21:45.1569023Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-12T04:21:45.1569670Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-12T04:21:45.1573398Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-12T04:21:45.1574129Z Jul 12 04:21:45 Caused by: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1574716Z Jul 12 04:21:45 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:21:45.1575306Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:21:45.1575932Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:21:45.1576491Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-07-12T04:21:45.1577041Z Jul 12 04:21:45 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-12T04:21:45.1577624Z Jul 12 04:21:45 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-12T04:21:45.1578123Z Jul 12 04:21:45 	at java.lang.Thread.run(Thread.java:748)
2022-07-12T04:21:45.1578660Z Jul 12 04:21:45 
2022-07-12T04:21:45.1578911Z Jul 12 04:21:45 
2022-07-12T04:21:45.1579329Z Jul 12 04:21:45 java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1579909Z Jul 12 04:21:45 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:21:45.1580511Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:21:45.1581135Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:21:45.1582315Z Jul 12 04:21:45 	...(4 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
2022-07-12T04:21:45.1582895Z Jul 12 04:21:45 error: 
2022-07-12T04:21:45.1583204Z Jul 12 04:21:45 Expecting throwable message:
2022-07-12T04:21:45.1583637Z Jul 12 04:21:45   ""Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1""
2022-07-12T04:21:45.1584056Z Jul 12 04:21:45 to contain:
2022-07-12T04:21:45.1584453Z Jul 12 04:21:45   ""Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=2""
2022-07-12T04:21:45.1584858Z Jul 12 04:21:45 but did not.
2022-07-12T04:21:45.1585141Z Jul 12 04:21:45 
2022-07-12T04:21:45.1585434Z Jul 12 04:21:45 Throwable that failed the check:
2022-07-12T04:21:45.1585840Z Jul 12 04:21:45 
2022-07-12T04:21:45.1586246Z Jul 12 04:21:45 java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1586810Z Jul 12 04:21:45 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:21:45.1587413Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:21:45.1588028Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:21:45.1588573Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-07-12T04:21:45.1589125Z Jul 12 04:21:45 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-12T04:21:45.1589710Z Jul 12 04:21:45 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-12T04:21:45.1590211Z Jul 12 04:21:45 	at java.lang.Thread.run(Thread.java:748)
2022-07-12T04:21:45.1590705Z Jul 12 04:21:45 at FlinkAssertions.lambda$anyCauseMatches$5(FlinkAssertions.java:106)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38057&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199",,hxbks2ks,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 14 02:16:51 UTC 2022,,,,,,,,,,"0|z16td4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/22 02:14;hxbks2ks;Hi [~luoyuxia] , could you help take a look?;;;","13/Jul/22 02:36;luoyuxia;Thanks for reporting it. I'll fix it as soon as possible.

The reason is it will read any path first, but the test assumes it'll always read the specific path first, thus cause the unstable test.;;;","13/Jul/22 02:37;hxbks2ks;Thanks for the quick fix.;;;","14/Jul/22 02:16;hxbks2ks;Merged into master via f23ae51fa4f28d7ca4d8c194ac6fc1a05fb5515c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaMetricWrapper does incorrect cast,FLINK-28488,13470978,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,11/Jul/22 10:45,17/Aug/22 21:59,13/Jul/23 08:13,17/Aug/22 21:59,1.15.1,1.16.0,,,,,,1.15.3,1.16.0,,,,Connectors / Kafka,,,,,,,0,pull-request-available,,,,"Same as FLINK-27487, which unfortunately missed 1 of 2 wrapper classes ({{{}KafkaMetricWrapper{}}}).

This only affects the deprecated KafkaConsumer/-Producer.",,cyc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27487,,,,,,,,"29/Jul/22 03:32;cyc;image-2022-07-29-11-32-05-733.png;https://issues.apache.org/jira/secure/attachment/13047351/image-2022-07-29-11-32-05-733.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 17 21:58:32 UTC 2022,,,,,,,,,,"0|z16rd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/22 03:32;cyc;I kown how to fix this. and  I want to try , how can I commit my code 

!image-2022-07-29-11-32-05-733.png!;;;","17/Aug/22 21:58;chesnay;master: 3c2fa303074453d4f4c95c2290b9b773684dac61
1.15: 51b3752ac320a45a20738fea28c8fdd923061f1c ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
num-sorted-run.stop-trigger introduced a unstable merging,FLINK-28482,13470917,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,lzljs3620320,lzljs3620320,11/Jul/22 06:29,20/Jul/22 10:22,13/Jul/23 08:13,20/Jul/22 10:19,,,,,,,,table-store-0.2.0,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,"We have introduced the num-sorted-run.stop-trigger. The default value is 10.
This means that the maximum number of runs generated is 10, and 10 runs may be merged at the same time during compaction or read.
Reading 10 ORC files at the same time may lead to OOM.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 20 10:19:53 UTC 2022,,,,,,,,,,"0|z16qzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/22 06:42;lzljs3620320;Maybe we should introduce a compaction.max-input-runs;;;","20/Jul/22 10:19;lzljs3620320;master: 4cb870b74662fc22ea25fbc29cacf0b14f78a4bf
release-0.2: 95bb7a54d62e6b02492d33381cfb9d73ee2665fe;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump the fabric8 kubernetes-client to 5.12.3,FLINK-28481,13470895,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,bzhaoop,wangyang0918,wangyang0918,11/Jul/22 02:41,10/Aug/22 02:59,13/Jul/23 08:13,10/Aug/22 02:59,,,,,,,,1.16.0,,,,,Deployment / Kubernetes,,,,,,,0,pull-request-available,,,,"The current fabric8 kubernetes-client(5.5.0) will swallow the {{KubernetesClientException}} and then the next renewing could not work properly until reach the deadline. This will be a serious problem because one time failure of renewing leader annotation will cause leadership lost.

 

Refer to the following ticket for more information.

https://github.com/fabric8io/kubernetes-client/issues/4246",,bgeng777,bzhaoop,godfreyhe,wangyang0918,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28832,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 10 02:59:21 UTC 2022,,,,,,,,,,"0|z16quo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/22 10:16;yunta;From the implementation of [LeaderElector#renew|https://github.com/fabric8io/kubernetes-client/blob/v4.9.2/kubernetes-client/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L118-L125] at 4.9.2 version. I think this bug also exists for flink-1.13, right?;;;","20/Jul/22 12:25;wangyang0918;Yes. If you believe it is necessary, we also need to push the upstream project to release a hotfix version for v4.9.;;;","28/Jul/22 02:39;bzhaoop;Let's waiting for the latest version(v5.12.3) mvn pkg from fabric8 upstream. Looks the v5.12.3 was just released 13 hours. May I contribute a PR as a volunteer?

 

Thanks;;;","28/Jul/22 02:42;wangyang0918;Thanks for volunteering. I have assigned this ticket to you. Happy coding.

Please remember to update the dependencies in the NOTICE file.;;;","28/Jul/22 02:44;bzhaoop;Cool, thank you very much.;;;","10/Aug/22 02:59;wangyang0918;Fixed via:

master: f2ec01241c5e5bcb860180dae5c0a660bc816926;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Session Cluster will lost if it failed between status recorded and deploy,FLINK-28478,13470825,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,aitozi,aitozi,10/Jul/22 04:01,15/Jul/22 06:19,13/Jul/23 08:13,15/Jul/22 06:19,kubernetes-operator-1.1.0,,,,,,,kubernetes-operator-1.1.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"When I test case with https://issues.apache.org/jira/browse/FLINK-28187 
I hit that the session cluster deploy can not be deployed if it fails between status recorded and deploy. Because, in the next reconcile loop, the spec is not detected changed by {{checkNewSpecAlreadyDeployed}}, so it will not try to start the session cluster again. 

The application mode have no problem, because the deployed spec SUSPEND state of the job is not equal to the desired state, so it will try to reconcile the spec change.",,aitozi,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 15 06:19:51 UTC 2022,,,,,,,,,,"0|z16qf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/22 04:03;aitozi;cc [~gyfora];;;","15/Jul/22 06:19;gyfora;Merged to main a6430378553d7405a037f948576bea5ef5be55ce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChannelStateWriteResult may not fail after checkpoint abort,FLINK-28474,13470809,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,09/Jul/22 14:23,07/Oct/22 08:39,13/Jul/23 08:13,07/Oct/22 07:02,1.14.5,1.15.1,,,,,,1.17.0,,,,,Runtime / Checkpointing,,,,,,,0,pull-request-available,,,,"After Checkpoint abort, ChannelStateWriteResult should fail.

But if _channelStateWriter.start(id, checkpointOptions);_ is executed after Checkpoint abort, ChannelStateWriteResult will not fail.

 
h2. Cause Analysis:

When abort checkpoint, channelStateWriter.start(id, checkpointOptions); may not be executed yet. These checkpointIds will be stored in the abortedCheckpointIds of SubtaskCheckpointCoordinatorImpl, and when checkpointState is called, it will check if the checkpointId should be aborted.

_ChannelStateWriter.abort(checkpointId, exception, true) should also be executed here._

The unit test can reproduce this bug.

!image-2022-07-09-22-21-24-417.png|width=803,height=307!

 

Note: channelStateWriter.abort is only called in notifyCheckpointAborted, it doesn't account for channelStateWriter.start after notifyCheckpointAborted.

JIRA: FLINK-17869

commit: https://github.com/apache/flink/pull/12478/commits/22c99845ef4f863f1753d17b109fd2faecc8201e

 

The bug will affect the new feature FLINK-26803, because the channel state file can be closed only after the Checkpoints of all tasks of the shared file are complete or abort. So when the checkpoint of some tasks fails, if abort is not called, the file cannot be closed and all tasks sharing the file cannot execute inputChannelStateHandles.completeExceptionally(e); and resultSubpartitionStateHandles.completeExceptionally(e); , AsyncCheckpointRunnable will wait forever.",,fanrui,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26803,,,,,,,,"09/Jul/22 14:21;fanrui;image-2022-07-09-22-21-24-417.png;https://issues.apache.org/jira/secure/attachment/13046501/image-2022-07-09-22-21-24-417.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 07 08:39:01 UTC 2022,,,,,,,,,,"0|z16qbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/22 07:01;fanrui;Hi [~pnowojski] , I have submitted this PR, could you help take a look in your free time ? This bug sometimes cause the CI of FLINK-26803 failed.;;;","07/Oct/22 07:02;pnowojski;Merged to master as a6119121164^..a6119121164

Thanks for the fix [~fanrui].

I think there is no need to back port it to the previous releases, as this bug doesn't cause any symptoms without FLINK-26803;;;","07/Oct/22 08:39;fanrui;[~pnowojski]  thanks for your help and review~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the wrong timestamp example of KafkaSource,FLINK-28454,13470617,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,08/Jul/22 05:05,13/Jul/22 08:39,13/Jul/23 08:13,13/Jul/22 07:43,1.13.6,1.14.5,1.15.1,,,,,1.14.6,1.15.2,1.16.0,,,Connectors / Kafka,Documentation,,,,,,0,pull-request-available,,,,"[https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/datastream/kafka/]

 

The timestamp unit of startingOffset is second in kafkaSource doc, but it should be milliseconds. It will mislead flink users.

 

!image-2022-07-08-13-04-59-993.png!

 

 

It should be milliseconds, because the timestamp is used in  OffsetSpec.fromTimestamp() , and the comment is : *@param timestamp in milliseconds*

 

By the way, the timestamp is milliseconds in old source doc. link: [https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/connectors/datastream/kafka/#kafka-consumers-start-position-configuration]

 

!image-2022-07-08-13-46-05-540.png!

 

!image-2022-07-08-13-42-49-902.png|width=1103,height=970!

 

!image-2022-07-08-13-43-14-278.png|width=697,height=692!

 

 ",,fanrui,renqs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22722,,,,,,,,,,,,,"08/Jul/22 05:05;fanrui;image-2022-07-08-13-04-59-993.png;https://issues.apache.org/jira/secure/attachment/13046429/image-2022-07-08-13-04-59-993.png","08/Jul/22 05:42;fanrui;image-2022-07-08-13-42-49-902.png;https://issues.apache.org/jira/secure/attachment/13046430/image-2022-07-08-13-42-49-902.png","08/Jul/22 05:43;fanrui;image-2022-07-08-13-43-14-278.png;https://issues.apache.org/jira/secure/attachment/13046431/image-2022-07-08-13-43-14-278.png","08/Jul/22 05:46;fanrui;image-2022-07-08-13-46-05-540.png;https://issues.apache.org/jira/secure/attachment/13046432/image-2022-07-08-13-46-05-540.png",,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 11 01:58:18 UTC 2022,,,,,,,,,,"0|z16p54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/22 01:58;renqs;Merged to master: ce56e01aeaa6ebd3b350fe089a952f238742823c
release-1.15: 9ffb7963e90b7bdcfbedaf8ef9ede4e0d7089fe8
release-1.14: 04bb6baa539d59052c1e038c92c339a9b5478726;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BoundedDataTestBase has potential problem when compression is enable,FLINK-28448,13470533,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,07/Jul/22 11:59,28/Jul/22 02:42,13/Jul/23 08:13,28/Jul/22 02:42,,,,,,,,1.16.0,,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,"BoundedDataTestBase has potential problem when compression is enable,  I'm sure there is a bug in this part of the code. But unfortunately, the test can pass now, because the generated data used in the test cannot be compressed by LZ4 (i.e. the compressed size is larger than the original size), which causes the branch of enable compression actually the same as disable compression, So the bug didn't surface.

I think there are two steps of work that need to be done:
1. Fix incorrect code
2. Generate data that can be properly compressed",,kevin.cyj,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 28 02:42:17 UTC 2022,,,,,,,,,,"0|z16omg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/22 15:38;Weijie Guo;cc [~kevin.cyj] ;;;","28/Jul/22 02:42;kevin.cyj;Merged into master via 1b012cb2c402a91be58e8b201ffd1b6d38b58ef9 and a4ad7814009f3e799f198d54f24a055894b1ebce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E2E tests run out of disk space on Azure,FLINK-28425,13470280,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,martijnvisser,martijnvisser,06/Jul/22 08:09,30/Aug/22 02:37,13/Jul/23 08:13,30/Aug/22 02:37,1.14.6,,,,,,,,,,,,Tests,,,,,,,0,stale-assigned,test-stability,,,All builds on the {{release-1.14}} branch fail on the E2E step due to running out of disk space. Running out of disk space causes FLINK-28305,,hxb,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28305,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 30 02:37:45 UTC 2022,,,,,,,,,,"0|z16n2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","30/Aug/22 02:37;hxb;Fixed in FLINK-28680;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Annotation @InjectClusterClient does not work correctly with RestClusterClient,FLINK-28404,13470121,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,Leo Zhou,Leo Zhou,05/Jul/22 11:28,06/Jul/22 08:56,13/Jul/23 08:13,06/Jul/22 08:54,,,,,,,,1.15.2,1.16.0,,,,Tests,,,,,,,0,pull-request-available,,,,"*test code:*
{code:java}
public class Test {

    @RegisterExtension
    private static final MiniClusterExtension MINI_CLUSTER_RESOURCE =
            new MiniClusterExtension(
                    new MiniClusterResourceConfiguration.Builder()
                            .setNumberTaskManagers(1)
                            .setNumberSlotsPerTaskManager(4)
                            .build());

    @org.junit.jupiter.api.Test
    void test(@InjectClusterClient RestClusterClient<?> restClusterClient) throws Exception {
        Object clusterId = restClusterClient.getClusterId();
    }
} {code}
*error info:*
{code:java}
org.junit.jupiter.api.extension.ParameterResolutionException: No ParameterResolver registered for parameter [org.apache.flink.client.program.rest.RestClusterClient<?> arg0] in method... {code}
this problem occurs because [MiniClusterExtension#supportsParameter|https://github.com/apache/flink/blob/master/flink-test-utils-parent/flink-test-utils/src/main/java/org/apache/flink/test/junit5/MiniClusterExtension.java#L168] does not support *_RestClusterClient_* parameterType. ",,Leo Zhou,rmetzger,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 06 08:54:58 UTC 2022,,,,,,,,,,"0|z16m68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 11:33;Leo Zhou;Hi [~rmetzger] Can you confirm this problem ？;;;","05/Jul/22 12:03;rmetzger;What's the benefit of injecting {{RestClusterClient}} instead of just {{ClusterClient}}?;;;","05/Jul/22 12:25;Leo Zhou;For example, ReactiveModeITCase use {{RestClusterClient}} to get job details, See [ReactiveModeITCase.java#L202|https://github.com/apache/flink/blob/a89152713aa58647841c46ed2335b45d24c553f9/flink-tests/src/test/java/org/apache/flink/test/scheduling/ReactiveModeITCase.java#L202]. when we port ReactiveModeITCase to junit5, we would need inject {{RestClusterClient.}}

And [MiniClusterExtension.java#L107 |https://github.com/apache/flink/blob/master/flink-test-utils-parent/flink-test-utils/src/main/java/org/apache/flink/test/junit5/MiniClusterExtension.java#L107]provides an example about how to inject RestClusterClient, but it does not work well.;;;","05/Jul/22 12:41;chesnay;This indeed doesn't work. We need to invert the {{isAssignableFrom}} check in \{{MiniClusterExtension#supportsParameter}}.;;;","05/Jul/22 12:50;chesnay;[~rmetzger] The plain cluster client is quite limited, while the rest client can be used to query the entire rest api.;;;","05/Jul/22 14:16;rmetzger;Thanks!;;;","06/Jul/22 08:54;chesnay;master: 85ba36fc0123852a9dbc8663ea786f96682f1128

1.15: ef84a060b4e4b82c209080ac0963cacb0be9f05f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefaultBlocklistHandlerTest.testRemoveTimeoutNodes fails,FLINK-28391,13470067,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wanglijie,Paul Lin,Paul Lin,05/Jul/22 07:44,06/Jul/22 12:31,13/Jul/23 08:13,06/Jul/22 12:31,1.16.0,,,,,,,1.16.0,,,,,Tests,,,,,,,0,pull-request-available,test-stability,,,"Test org.apache.flink.runtime.blocklist.DefaultBlocklistHandlerTest.testRemoveTimeoutNodes unstable.

see https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/37624/logs/111

{code:java}
Jul 05 01:23:40 [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.572 s - in org.apache.flink.runtime.shuffle.ShuffleMasterTest
Jul 05 01:23:41 [INFO] 
Jul 05 01:23:41 [INFO] Results:
Jul 05 01:23:41 [INFO] 
Jul 05 01:23:41 [ERROR] Failures: 
Jul 05 01:23:41 [ERROR]   DefaultBlocklistHandlerTest.testRemoveTimeoutNodes:93 
Jul 05 01:23:41 Expecting actual:
Jul 05 01:23:41   [BlockedNode{id:node1,cause:cause,endTimestamp:1656984203692}]
Jul 05 01:23:41 to contain exactly (and in same order):
Jul 05 01:23:41   [BlockedNode{id:node1,cause:cause,endTimestamp:1656984203692}]
Jul 05 01:23:41 but could not find the following elements:
Jul 05 01:23:41   [BlockedNode{id:node1,cause:cause,endTimestamp:1656984203692}]
Jul 05 01:23:41 
Jul 05 01:23:41 [INFO] 
Jul 05 01:23:41 [ERROR] Tests run: 6453, Failures: 1, Errors: 0, Skipped: 26
{code}",,hxbks2ks,martijnvisser,Paul Lin,wanglijie,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 06 12:31:34 UTC 2022,,,,,,,,,,"0|z16lu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 08:08;wanglijie;I will take a look.;;;","05/Jul/22 08:17;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37602&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=8344;;;","06/Jul/22 08:25;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37722&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8 ;;;","06/Jul/22 12:31;zhuzh;Fixed via 2bd8c209df72679684aeb493c6df03a167fbd2f6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Watermark issue when recovering Finished sources,FLINK-28357,13469687,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,Sandys-Lumsdaine,Sandys-Lumsdaine,01/Jul/22 15:21,09/Jul/22 15:34,13/Jul/23 08:13,07/Jul/22 12:31,1.15.0,,,,,,,1.14.6,1.15.2,1.16.0,,,Runtime / Checkpointing,,,,,,,0,pull-request-available,,,,"Copied mostly from email trail on the flink user mailing list:

I done a lot of experimentation and I’m convinced there is a problem with Flink handling Finished sources and recovery. 

The program consists of:
 * Two sources:
 ** One “Long Running Source” – stays alive and emits a watermark of DateTime.now() every 10 seconds.
 *** Prints the console a message saying the watermark has been emitted.
 *** *Throws an exception every 5 or 10 iterations to force a recovery.*
 ** One “Short Lived Source” – emits a Long.MAX_VALUE watermark, prints a message to the console and returns.
 * The “Short Live Source” feeds into a map() and then it joins with the “Long Running Source” with a KeyedCoProcessFunction. Moves to “FINISHED” state by Flink.

The problem here is that the “Join” receives no Long.MAX_VALUE watermark from the map() in some situations after a recovery. The dashboard goes from showing this:

!https://attachment.outlook.live.net/owa/MSA%3Ajas_sl%40hotmail.com/service.svc/s/GetAttachmentThumbnail?id=AQMkADAwATEyMTk3LTZiMDQtODBkMi0wMAItMDAKAEYAAAOeUdiydD9QS6CQDK1Dg0olBwACkmHn2W1HRKQHhbPYmGe%2BAASF%2B488AAAAApJh59ltR0SkB4Wz2JhnvgAFXJ9puQAAAAESABAAyemY6ar4b0GAFLHn3hpyCw%3D%3D&thumbnailType=2&isc=1&token=eyJhbGciOiJSUzI1NiIsImtpZCI6IkZBRDY1NDI2MkM2QUYyOTYxQUExRThDQUI3OEZGMUIyNzBFNzA3RTkiLCJ0eXAiOiJKV1QiLCJ4NXQiOiItdFpVSml4cThwWWFvZWpLdDRfeHNuRG5CLWsifQ.eyJvcmlnaW4iOiJodHRwczovL291dGxvb2subGl2ZS5jb20iLCJ1YyI6IjMwMDU4MTIzODAzMzRlMmZhNzE5ZGUxOTNjNjA4NjQ3IiwidmVyIjoiRXhjaGFuZ2UuQ2FsbGJhY2suVjEiLCJhcHBjdHhzZW5kZXIiOiJPd2FEb3dubG9hZEA4NGRmOWU3Zi1lOWY2LTQwYWYtYjQzNS1hYWFhYWFhYWFhYWEiLCJpc3NyaW5nIjoiV1ciLCJhcHBjdHgiOiJ7XCJtc2V4Y2hwcm90XCI6XCJvd2FcIixcInB1aWRcIjpcIjMxODQwOTE5NTk0NjE5NFwiLFwic2NvcGVcIjpcIk93YURvd25sb2FkXCIsXCJvaWRcIjpcIjAwMDEyMTk3LTZiMDQtODBkMi0wMDAwLTAwMDAwMDAwMDAwMFwiLFwicHJpbWFyeXNpZFwiOlwiUy0xLTI4MjctNzQxMzUtMTc5NTQ1NzIzNFwifSIsIm5iZiI6MTY1NjY4ODI3OCwiZXhwIjoxNjU2Njg4ODc4LCJpc3MiOiIwMDAwMDAwMi0wMDAwLTBmZjEtY2UwMC0wMDAwMDAwMDAwMDBAODRkZjllN2YtZTlmNi00MGFmLWI0MzUtYWFhYWFhYWFhYWFhIiwiYXVkIjoiMDAwMDAwMDItMDAwMC0wZmYxLWNlMDAtMDAwMDAwMDAwMDAwL2F0dGFjaG1lbnQub3V0bG9vay5saXZlLm5ldEA4NGRmOWU3Zi1lOWY2LTQwYWYtYjQzNS1hYWFhYWFhYWFhYWEiLCJoYXBwIjoib3dhIn0.KI4I55ycdP1duIwxyYZstLCtnNOwEkyTxfEwK_5a35-ZLMrKd8zHCB5Elw-9-A9UHIxFGSYOlwnHXRvDT0xa6FqFIlO8cnebBRLKv9DhxHwfZqdKWIeF2EcUqwH0ejeA3RvD3-dR95iHPTf52-tuKi27nclPUUEJgbfRWQY3wHMDAFLLaLvKM6AV5S1IhGjBmy3MF_1oulTXbqRZx0ar3L8YQiHEGnfKGjFO2zSxQcTZXAp_rch4HIrVv9GSEcQnD7nBhWPBuuzuvXOvJiUzg0u_e9CUuf1-OcQwhUV3cf7cvme8JadfliY6ywkOne1OZsclQeDFc8EnGZke3l2V_Q&X-OWA-CANARY=Es9QgEoDXEyksG3kZxXeMGC1LvlzW9oYJ-lyWNl-xblWQjmqz5FH_a2-eHuR6Zr51XNjigQpQDs.&owa=outlook.live.com&scriptVer=20220617005.11&animation=true!

To the below after a recovery (with the currentInput1/2Watermark metrics showing input 2 having not received a watermark from the map, saying –Long.MAX_VALUE):

!image-2022-07-01-16-18-14-768.png!

The program is currently set to checkpoint every 5 seconds. By experimenting with 70 seconds, it seems that if only one checkpoint has been taken with the “Short Lived Source” in a FINISHED state since the last recovery then everything works fine and the restarted “Short Lived Source” emits its watermark and I see the “ShortedLivedEmptySource emitting Long.MAX_VALUE watermark” message on the console meaning the run() definitely executed. However, I found that if 2 or more checkpoints are taken since the last recovery with the source in a FINISHED state then the console message does not appear and the watermark is not emitted.

To repeat – the Join does not get a Long.MAX_VALUE watermark from my source or Flink if I see two or more checkpoints logged in between recoveries. If zero or checkpoints are made, everything is fine – the join gets the watermark and I see my console message. You can play with the checkpointing frequency as per the code comments:

        // Useful checkpoint interval options:

        //    5 - see the problem after the first recovery

        //   70 - useful to see bad behaviour kick in after a recovery or two

        //  120 - won't see the problem as we don't have 2 checkpoints within a single recovery session

If I merge the Triggering/Completed checkpoint messages in the log with my console output I see something like this clearly showing the “Short Lived Source” run() method is not executed after 2 checkpoints with the operators marked as FINISHED:

 

2022-06-29T11:52:31.268Z: *ShortLivedEmptySource* emitting Long.MAX_VALUE watermark.

2022-06-29T11:52:31.293Z: LongRunningSource emitting initial watermark=1656503551268

2022-06-29T11:52:41.302Z: LongRunningSource emitting loop watermark=1656503561302

2022-06-29T11:52:51.302Z: LongRunningSource emitting loop watermark=1656503571302

2022-06-29T11:53:01.303Z: LongRunningSource emitting loop watermark=1656503581303

2022-06-29 11:53:02.772 INFO  [Checkpoint Timer] o.a.f.r.c.CheckpointCoordinator           Triggering checkpoint 1 (type=CheckpointType\{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD})

2022-06-29 11:53:02.870 INFO  [jobmanager-io-thread-10] o.a.f.r.c.CheckpointCoordinator    Completed checkpoint 1 for job 877656d7752bc1304c2cb92790e6aefb

2022-06-29T11:53:11.303Z: LongRunningSource emitting loop watermark=1656503591303

2022-06-29T11:53:21.304Z: LongRunningSource emitting loop watermark=1656503601304

2022-06-29T11:53:21.304Z: ------------------ Recovery ------------------

2022-06-29T11:53:22.405Z: LongRunningSource emitting initial watermark=1656503602405

2022-06-29T11:53:22.408Z: *ShortLivedEmptySource* emitting Long.MAX_VALUE watermark.

2022-06-29T11:53:32.406Z: LongRunningSource emitting loop watermark=1656503612406

2022-06-29T11:53:42.406Z: LongRunningSource emitting loop watermark=1656503622406

2022-06-29 11:53:51.048 INFO  [Checkpoint Timer] o.a.f.r.c.CheckpointCoordinator           Triggering checkpoint 2 (type=CheckpointType\{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD})

2022-06-29 11:53:51.067 INFO  [jobmanager-io-thread-4] o.a.f.r.c.CheckpointCoordinator     Completed checkpoint 2 for job 877656d7752bc1304c2cb92790e6aefb

2022-06-29T11:53:52.407Z: LongRunningSource emitting loop watermark=1656503632407

2022-06-29T11:54:02.407Z: LongRunningSource emitting loop watermark=1656503642407

2022-06-29T11:54:12.408Z: LongRunningSource emitting loop watermark=1656503652408

2022-06-29T11:54:22.408Z: LongRunningSource emitting loop watermark=1656503662408

2022-06-29T11:54:32.409Z: LongRunningSource emitting loop watermark=1656503672409

2022-06-29T11:54:42.409Z: LongRunningSource emitting loop watermark=1656503682409

2022-06-29T11:54:52.410Z: LongRunningSource emitting loop watermark=1656503692410

2022-06-29 11:55:01.048 INFO  [Checkpoint Timer] o.a.f.r.c.CheckpointCoordinator           Triggering checkpoint 3 (type=CheckpointType\{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD})

2022-06-29 11:55:01.057 INFO  [jobmanager-io-thread-10] o.a.f.r.c.CheckpointCoordinator    Completed checkpoint 3 for job 877656d7752bc1304c2cb92790e6aefb

2022-06-29T11:55:02.410Z: LongRunningSource emitting loop watermark=1656503702410

2022-06-29T11:55:02.411Z: ------------------ Recovery ------------------

2022-06-29T11:55:03.445Z: LongRunningSource emitting initial watermark=1656503703444       <<<<< NO “ShortLivedEmptySource” message after recovery

2022-06-29T11:55:13.446Z: LongRunningSource emitting loop watermark=1656503713445

2022-06-29T11:55:23.446Z: LongRunningSource emitting loop watermark=1656503723446

2022-06-29T11:55:33.446Z: LongRunningSource emitting loop watermark=1656503733446

 

I have also attached a longer example with shows everything working fine after 5 recoveries, and then breaking after the 6{^}th{^}.

I am guessing here it has something to do with the checkpointing and recovery of a FINISHED source.

Finally, here are some ways that allows the code to work:
 * Change the code so the “Short Lived Source” doesn’t return from run() and stays RUNNING (uncomment the Thread.sleep)
 * As I mentioned before, if I remove the map() operator the problem in the join also goes away. (I don’t see the console output but the join is happy)
 * Use a long enough checkpoint interval (e.g. 120 seconds) so we don’t have two checkpoints with FINISHED state per recovery.

The fact these changes prevent the issue means I really think there’s some bug or inconsistency here – if somebody could explain I would really appreciate it.

 ",This can be reproduced in an IDE with the attached sample program.,martijnvisser,pnowojski,Sandys-Lumsdaine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/22 15:21;Sandys-Lumsdaine;WatermarkDemoMain.java;https://issues.apache.org/jira/secure/attachment/13046159/WatermarkDemoMain.java","01/Jul/22 15:18;Sandys-Lumsdaine;image-2022-07-01-16-18-14-768.png;https://issues.apache.org/jira/secure/attachment/13046161/image-2022-07-01-16-18-14-768.png","08/Jul/22 16:06;Sandys-Lumsdaine;image-2022-07-08-17-06-01-256.png;https://issues.apache.org/jira/secure/attachment/13046462/image-2022-07-08-17-06-01-256.png","01/Jul/22 15:21;Sandys-Lumsdaine;longExample.txt;https://issues.apache.org/jira/secure/attachment/13046160/longExample.txt",,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jul 09 15:34:45 UTC 2022,,,,,,,,,,"0|z16jhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/22 16:38;pnowojski;Thanks for the bug report. Indeed there was a bug where max watermark was being swallowed by this non chained map.

The problem was that {{FinishedOnRestoreInput#FinishedOnRestoreInput}} was being constructed with wrong number of inputs, because of some accidental {{null}} passed from the {{StreamGraphGenerator}}.

Just note that even with the bug fix (please see my PR), you will still not see the printed message from the short lived source:
{noformat}
System.out.println(String.format(""%s: ShortLivedEmptySource emitting Long.MAX_VALUE watermark."", DateTime.now()));
{noformat}
as after recovery this source is never started, so the code never reaches the run method. And that's fine, MAX_WATERMARK is emitted by the framework.

;;;","05/Jul/22 07:09;Sandys-Lumsdaine;Great - thanks for the feedback. Understood about the source not starting again after being marked as Finished and the framework emitting a MAX_WATERMARK - this is the behaviour I was expecting.;;;","05/Jul/22 07:11;Sandys-Lumsdaine;Also - you say ""non-chained map"" but {+}I believe this problem also occured with operator chaining{+}. I disabled operator chaining in my test program to make it clearer what was going on.;;;","05/Jul/22 08:44;pnowojski;{quote}
Also - you say ""non-chained map"" but I believe this problem also occurred with operator chaining. I disabled operator chaining in my test program to make it clearer what was going on.
{quote}
Are you sure? I can not reproduce this problem with or without my fix when chaining is enabled. There might be another issue lurking somewhere around, but I can not reproduce it at the moment. If you could confirm that there was no problem with enabled chaining, or if you could provide some reproduction steps (ideally modify [the ITCase that I'm adding in my PR|https://github.com/apache/flink/pull/20158/commits/2db367614bb16c6af714cb8c0cfefbb4ace272f3#diff-24e86c59d8547cbd9a95f798d3e0c300daa99dae9fba5be017ed4b2143bd5787]. This ITCase would livelock/never finish if the watermarks stagnate after recovery), that would be great. Just note what I wrote before, that verifying this issue based on messages printed from the sources won't work. ;;;","07/Jul/22 12:31;pnowojski;I've merged the non-chained tasks fix. [~Sandys-Lumsdaine] if you think there is still another bug present, please feel free to re-open the ticket.

merged commit 574ffa4 into apache:master
merged commit fa4b327 into apache:release-1.14
merged commit 5c1d412 into apache:release-1.15;;;","08/Jul/22 16:08;Sandys-Lumsdaine;Hello - sorry been busy and just got round to rechecking my test program. 

If I comment out the line ""env.disableOperatorChaining();"" to allow operator chaining the issue also occurs. The screenshot of the dashboard after a recovery and the watermark not working properly is shown below:

!image-2022-07-08-17-06-01-256.png!

As you can see input2 for the join is saying Long.MIN_VALUE meaning the framework didn't trasnmit the watermark on behalf of my finished source (I didn't see the console message as you describe because it is finished).

If you're having trouble reproducing on 1.15.0 before your fix, try changing the checkpointing frequency to 5 * 1000 instead of 70 as per my comments. For me that cause the issue immediately after the first recovery.

 

BTW, I can't re-open this issue as you describe - not sure if I have perms etc.;;;","09/Jul/22 14:26;Sandys-Lumsdaine;Maybe my example above is actually the same scenario as before.... if you're happy the chaining above isn't relevant for this bug I'm happy too.;;;","09/Jul/22 15:34;pnowojski;Hi [~Sandys-Lumsdaine]
{quote}
Maybe my example above is actually the same scenario as before....
{quote}
Yes, it looks like that. On the screen shot above for whatever a reason the {{Empty Stream Map}} is not chained with the {{Short Running Source}}. I'm not sure why, maybe you have slot sharing disabled, or maybe there is a keyed exchange before the {{Empty Stream Map}} (note that in my ITCase I've removed keyed exchanges). You can see that for example {{Join}} and {{Sink: Join Sink}} operators are chained together in one single task. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Python Bash e2e tests don't clean-up after they've ran, causing disk space issues",FLINK-28355,13469672,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,martijnvisser,martijnvisser,01/Jul/22 14:20,06/Jul/22 06:31,13/Jul/23 08:13,06/Jul/22 02:02,1.14.6,1.15.2,1.16.0,,,,,1.14.6,1.15.2,1.16.0,,,API / Python,Test Infrastructure,,,,,,0,pull-request-available,,,,"The Bash based E2E tests that are used in Python aren't cleaned-up after they've ran. These cause disk space issues further downstream.

See the CI run from https://github.com/apache/flink/pull/20114 for results, for example:

-- When starting with the Bash e2e tests
{code:java}
08:47:10 ##[group]Top 15 biggest directories in terms of used disk space
Jul 01 08:47:12 3983560	.
Jul 01 08:47:12 1266692	./flink-end-to-end-tests
Jul 01 08:47:12 624568	./flink-dist
Jul 01 08:47:12 624180	./flink-dist/target
Jul 01 08:47:12 500076	./flink-dist/target/flink-1.16-SNAPSHOT-bin
Jul 01 08:47:12 500072	./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT
Jul 01 08:47:12 460812	./flink-connectors
Jul 01 08:47:12 392588	./.git
Jul 01 08:47:12 366396	./.git/objects
Jul 01 08:47:12 366388	./.git/objects/pack
Jul 01 08:47:12 349272	./flink-table
Jul 01 08:47:12 335592	./.git/objects/pack/pack-38d46915823ebec2bc660fd160e5cfca5bc3e567.pack
Jul 01 08:47:12 293044	./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT/opt
Jul 01 08:47:12 251272	./flink-filesystems
Jul 01 08:47:12 246596	./flink-end-to-end-tests/flink-streaming-kinesis-test
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37425&view=logs&j=ef799394-2d67-5ff4-b2e5-410b80c9c0af&t=860bfb5d-81b0-5968-f128-2a8b5362110d&l=664

-- After completing all Bash bashed e2e tests:
{code:java}
2022-07-01T10:20:17.3594718Z Jul 01 10:20:17 ##[group]Top 15 biggest directories in terms of used disk space
2022-07-01T10:20:18.7520631Z Jul 01 10:20:18 5425892	.
2022-07-01T10:20:18.7521823Z Jul 01 10:20:18 1521472	./flink-end-to-end-tests
2022-07-01T10:20:18.7522566Z Jul 01 10:20:18 1242528	./flink-python
2022-07-01T10:20:18.7523244Z Jul 01 10:20:18 952336	./flink-python/dev
2022-07-01T10:20:18.7524159Z Jul 01 10:20:18 878764	./flink-python/dev/.conda
2022-07-01T10:20:18.7524870Z Jul 01 10:20:18 834200	./flink-python/dev/.conda/lib
2022-07-01T10:20:18.7525619Z Jul 01 10:20:18 726528	./flink-python/dev/.conda/lib/python3.7
2022-07-01T10:20:18.7526397Z Jul 01 10:20:18 683256	./flink-python/dev/.conda/lib/python3.7/site-packages
2022-07-01T10:20:18.7527101Z Jul 01 10:20:18 624568	./flink-dist
2022-07-01T10:20:18.7527768Z Jul 01 10:20:18 624180	./flink-dist/target
2022-07-01T10:20:18.7528494Z Jul 01 10:20:18 500076	./flink-dist/target/flink-1.16-SNAPSHOT-bin
2022-07-01T10:20:18.7529298Z Jul 01 10:20:18 500072	./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT
2022-07-01T10:20:18.7530046Z Jul 01 10:20:18 460812	./flink-connectors
2022-07-01T10:20:18.7530546Z Jul 01 10:20:18 392588	./.git
2022-07-01T10:20:18.7531014Z Jul 01 10:20:18 366396	./.git/objects
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37425&view=logs&j=ef799394-2d67-5ff4-b2e5-410b80c9c0af&t=860bfb5d-81b0-5968-f128-2a8b5362110d&l=9631",,dianfu,hxbks2ks,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28305,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 06 02:02:16 UTC 2022,,,,,,,,,,"0|z16jeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 14:20;martijnvisser;[~hxbks2ks] [~dianfu] Can either of you help with this?;;;","05/Jul/22 17:34;martijnvisser;Fixed in

master: b0f0144b287e95a1189098d164cc02b9a6fa42aa;;;","06/Jul/22 02:02;hxbks2ks;Merged into release-1.15 via 230706488987d9ae96154ddf95247fc293e2bc6c
Merged into release-1.14 via 1e1935e12f641cdbb4fccbbd19d75c49a05f8600;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Umbrella] Make Pulsar connector stable,FLINK-28352,13469647,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,martijnvisser,martijnvisser,01/Jul/22 11:16,13/Apr/23 14:32,13/Jul/23 08:13,13/Apr/23 08:32,1.14.6,1.15.3,1.16.0,,,,,pulsar-3.0.1,pulsar-4.0.0,,,,Connectors / Pulsar,,,,,,,1,pull-request-available,test-stability,,,This ticket is an umbrella ticket to keep track of all currently known Pulsar connector test instabilities. These need to be resolved as soon as possible and before other new Pulsar features can be added. ,,longtimer,mapohl,martijnvisser,syhily,tanyuxin,,,,,,,,,,,,,,,,,,,,,,FLINK-25686,FLINK-26202,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 13 14:32:27 UTC 2023,,,,,,,,,,"0|z16j8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/22 18:39;syhily;After detailed debugging and logging. We can confirm this is a race condition on pulsar-client-all. It's reported as a bug on https://github.com/apache/pulsar/pull/16171. We will have a quick workaround for fixing this issue. Sorry for anyone who was bothered by Pulsar's test failure. 🧎;;;","22/Dec/22 08:27;mapohl;I disabled {{PulsarSourceUnorderedE2ECase}}, {{PulsarUnorderedPartitionSplitReaderTest}} and {{PulsarUnorderedSourceITCase}} in {{release-1.15}} and {{release-1.16}} due to the instabilities caused by these tests. The tests are not disabled in the externalized connector repository for Pulsar. See FLINK-30351 subtasks for details.

1.16: 9215ef5bbd76e11dd5b373cce4acdbbc9f1207fa
1.15: aa2c0395e8a368d7dd19605b94a0af4d77c519db;;;","04/Jan/23 06:03;syhily;The only issue we have now is the Direct buffer memory leak with Java 11. We will try to get it resolved in Pulsar 2.11.0 release.;;;","13/Apr/23 08:32;martijnvisser;I've converted FLINK-24302 to its own issue, so that we can close this umbrella ticket since all other subtickets are closed;;;","13/Apr/23 14:32;syhily;Thanks. The FLINK-24302 will be closed in Pulsar itself 3.0.0 release. I think.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Delete topic after tests,FLINK-28335,13469539,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,lzljs3620320,lzljs3620320,01/Jul/22 00:48,04/Jul/22 04:04,13/Jul/23 08:13,04/Jul/22 04:04,,,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"Currently our test does not remove the topic, kafka local cluster may reuse the data inside the topic, resulting in a test error.",,lzljs3620320,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 04 04:04:28 UTC 2022,,,,,,,,,,"0|z16ikw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/22 04:04;lzljs3620320;master: c24d3d244b4ae00e599924fa8814498f98f74aab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PushProjectIntoTableSourceScanRule should cover the case when table source SupportsReadingMetadata and not SupportsProjectionPushDown,FLINK-28334,13469490,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,30/Jun/22 15:12,07/Jul/22 06:55,13/Jul/23 08:13,07/Jul/22 06:28,1.15.0,,,,,,,1.16.0,,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"""SELECT id, ts FROM src"" query on such a table:

{code}

CREATE TABLE src (
  id int,
  name varchar,
  tags varchar METADATA VIRTUAL,
  ts timestamp(3) METADATA VIRTUAL
) WITH (
  'connector' = 'values',
  'readable-metadata' = 'tags:varchar,ts:timestamp(3)',
  'enable-projection-push-down' = 'false'
)

{code}

 

error occurs

{code}

java.lang.AssertionError: Sql optimization: Assertion error: RexInputRef index 3 out of range 0..2

    at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:84)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:62)
    at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
    at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
    at scala.collection.Iterator.foreach(Iterator.scala:937)
    at scala.collection.Iterator.foreach$(Iterator.scala:937)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
    at scala.collection.IterableLike.foreach(IterableLike.scala:70)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
    at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:58)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizerV2.optimizeTree(StreamCommonSubGraphBasedOptimizerV2.scala:209)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizerV2.optimizeBlock(StreamCommonSubGraphBasedOptimizerV2.scala:156)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizerV2.$anonfun$doOptimize$1(StreamCommonSubGraphBasedOptimizerV2.scala:79)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizerV2.$anonfun$doOptimize$1$adapted(StreamCommonSubGraphBasedOptimizerV2.scala:78)
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizerV2.doOptimize(StreamCommonSubGraphBasedOptimizerV2.scala:78)
    at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:94)
    at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:389)
    at org.apache.flink.table.planner.utils.TableTestUtilBase.assertPlanEquals(TableTestBase.scala:1199)
    at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan2(TableTestBase.scala:1109)
    at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:1066)
    at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyExecPlan(TableTestBase.scala:687)
    at org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.testReuseSourceWithoutProjectionPushDown(TableSourceJsonPlanTest.java:308)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
    at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: java.lang.AssertionError: RexInputRef index 3 out of range 0..2
    at org.apache.calcite.util.Litmus$1.fail(Litmus.java:31)
    at org.apache.calcite.rex.RexChecker.visitInputRef(RexChecker.java:121)
    at org.apache.calcite.rex.RexChecker.visitInputRef(RexChecker.java:57)
    at org.apache.calcite.rex.RexInputRef.accept(RexInputRef.java:112)
    at org.apache.calcite.rel.core.Project.isValid(Project.java:214)
    at org.apache.calcite.rel.core.Project.<init>(Project.java:93)
    at org.apache.calcite.rel.logical.LogicalProject.<init>(LogicalProject.java:67)
    at org.apache.calcite.rel.logical.LogicalProject.copy(LogicalProject.java:128)
    at org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRule.onMatch(PushProjectIntoTableSourceScanRule.java:167)
    at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:229)
    at org.apache.calcite.plan.volcano.IterativeRuleDriver.drive(IterativeRuleDriver.java:58)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:510)
    at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:312)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:69)
    ... 60 more

{code}

 ",,godfreyhe,lincoln.86xy,,,,,,,,,,,,,,,,,,,,,,,FLINK-28346,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 07 06:28:45 UTC 2022,,,,,,,,,,"0|z16ia0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/22 06:28;godfreyhe;Fix in
master: fb9843af5ffeb6d7561876704d463dea1fcdc153
1.15.2: 1dc4c5ba71e2c920ee94bf4274a850db6bc870d9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResultPartitionTest.testIdleAndBackPressuredTime failed with AssertError,FLINK-28326,13469444,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,hxbks2ks,hxbks2ks,30/Jun/22 11:26,08/Feb/23 04:13,13/Jul/23 08:13,08/Feb/23 04:13,1.16.0,1.17.0,,,,,,1.16.2,1.17.0,,,,Runtime / Network,,,,,,,0,pull-request-available,stale-assigned,test-stability,,"
{code:java}
2022-06-30T09:23:24.0469768Z Jun 30 09:23:24 [INFO] 
2022-06-30T09:23:24.0470382Z Jun 30 09:23:24 [ERROR] Failures: 
2022-06-30T09:23:24.0471581Z Jun 30 09:23:24 [ERROR]   ResultPartitionTest.testIdleAndBackPressuredTime:414 
2022-06-30T09:23:24.0472898Z Jun 30 09:23:24 Expected: a value greater than <0L>
2022-06-30T09:23:24.0474090Z Jun 30 09:23:24      but: <0L> was equal to <0L>
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37406&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8",,hxbks2ks,kevin.cyj,mapohl,martijnvisser,renqs,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 08 04:11:31 UTC 2023,,,,,,,,,,"0|z16hzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 07:49;Weijie Guo;Thanks for reporting the issue, it's an unstable test, I'll fix it.

[~kevin.cyj] [~martijnvisser] can you help assign this ticket to me.;;;","04/Jul/22 06:59;martijnvisser;[~Weijie Guo] Thanks, I've assigned it to you;;;","07/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","30/Sep/22 08:45;chesnay;master: f543b8ac690b1dee58bc3cb345a1c8ad0db0941e;;;","21/Nov/22 04:15;renqs;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43136&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8114]

Looks like the issue happens again. [~Weijie Guo] any thought?;;;","22/Nov/22 13:27;Weijie Guo;[~renqs] Sorry for the reply. I checked the code path and reproduced it locally. Through the previous fix, we can ensure that the request thread must be blocked before the main thread recycle the buffer. However, there is still an extreme case where the request thread recovers from blocking very quickly, resulting in a calculated back-pressure time is equal to 0. I propose that we can add a very short waiting time before `buffer.recycleBuffer` and I tested it thousands of times locally, it seems that the problem did not recur.

If there are no other suggestions, I will open a pull request based on this approach.;;;","18/Jan/23 07:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44987&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=9464;;;","03/Feb/23 04:26;kevin.cyj;master: 7980d4b7956bbb5952933d6f4c49e27f59c96834;;;","03/Feb/23 05:54;Weijie Guo;Closed it, feel free to reopen this if it appears again.;;;","03/Feb/23 10:18;mapohl;Reopening: Shouldn't we provide a 1.16 backport for this issue as well, [~Weijie Guo]?;;;","03/Feb/23 10:48;Weijie Guo;[~mapohl]Thank you for your reminder :), and I found that the original fix commit was also not picked up to 1.16 branch. I have opened the back-port pull request to handle this.;;;","08/Feb/23 04:11;kevin.cyj;1.16: 096c4a5e29cdffc8b4ed72f7bc6fc7b42dab2e9b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataStreamScanProvider's new method is not compatible,FLINK-28322,13469402,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,30/Jun/22 08:27,01/Sep/22 11:11,13/Jul/23 08:13,06/Jul/22 02:25,1.15.0,,,,,,,1.15.2,,,,,Table SQL / API,,,,,,,0,pull-request-available,,,,"In FLINK-25990 , 
Add a method ""DataStream<RowData> produceDataStream(ProviderContext providerContext, StreamExecutionEnvironment execEnv)"" in DataStreamScanProvider.
But this method has no default implementation, this is not compatible when users upgrade their DataStreamScanProvider implementation from 1.14 to 1.15. (The old method will not be called)
This method should be:
{code:java}
    default DataStream<RowData> produceDataStream(
            ProviderContext providerContext, StreamExecutionEnvironment execEnv) {
        return produceDataStream(execEnv);
    }
{code}

DataStreamSinkProvider is the same.

Impact on DataStreamSinkProvider:
DataStreamSinkProvider is no longer a functional interface and requires the use of an anonymous internal class.",,danderson,leonard,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 06 02:25:52 UTC 2022,,,,,,,,,,"0|z16hqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/22 02:25;lzljs3620320;master: 03a1a42d210bf30751a8a5fed295cd56b0ff34df
release-1.15: 57c48a95a7f3cf952e1531fb763431c4284001d0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Client end-to-end test failed with ElasticsearchException,FLINK-28305,13469331,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,30/Jun/22 02:01,26/Jul/22 02:30,13/Jul/23 08:13,20/Jul/22 06:21,1.14.5,,,,,,,1.14.6,,,,,Table SQL / Client,,,,,,,0,test-stability,,,,"
{code:java}
2022-06-29T19:14:39.0384925Z Jun 29 19:14:38 java.lang.RuntimeException: An error occurred in ElasticsearchSink.
2022-06-29T19:14:39.0385547Z Jun 29 19:14:38 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkErrorAndRethrow(ElasticsearchSinkBase.java:427) ~[?:?]
2022-06-29T19:14:39.0386327Z Jun 29 19:14:38 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkAsyncErrorsAndRequests(ElasticsearchSinkBase.java:432) ~[?:?]
2022-06-29T19:14:39.0387078Z Jun 29 19:14:38 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.invoke(ElasticsearchSinkBase.java:329) ~[?:?]
2022-06-29T19:14:39.0388071Z Jun 29 19:14:38 	at org.apache.flink.table.runtime.operators.sink.SinkOperator.processElement(SinkOperator.java:65) ~[flink-table_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0389189Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0390354Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0391515Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0392712Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0393822Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:495) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0394861Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0395885Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:806) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0396858Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:758) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0397824Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0398882Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0400103Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0401000Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0401575Z Jun 29 19:14:38 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_332]
2022-06-29T19:14:39.0402051Z Jun 29 19:14:38 	Suppressed: java.lang.RuntimeException: An error occurred in ElasticsearchSink.
2022-06-29T19:14:39.0402682Z Jun 29 19:14:38 		at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkErrorAndRethrow(ElasticsearchSinkBase.java:427) ~[?:?]
2022-06-29T19:14:39.0403421Z Jun 29 19:14:38 		at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.close(ElasticsearchSinkBase.java:366) ~[?:?]
2022-06-29T19:14:39.0404395Z Jun 29 19:14:38 		at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0405470Z Jun 29 19:14:38 		at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:114) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0406549Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:141) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0407648Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:127) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0408717Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:1032) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0409755Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1018) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0410752Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:925) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0411732Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:940) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0412711Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0413665Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:940) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0414726Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0415607Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0416179Z Jun 29 19:14:38 		at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_332]
2022-06-29T19:14:39.0417155Z Jun 29 19:14:38 	Caused by: org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException: Elasticsearch exception [type=cluster_block_exception, reason=index [my_users] blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];]
2022-06-29T19:14:39.0418020Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException.innerFromXContent(ElasticsearchException.java:496) ~[?:?]
2022-06-29T19:14:39.0418792Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException.fromXContent(ElasticsearchException.java:407) ~[?:?]
2022-06-29T19:14:39.0419560Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.action.bulk.BulkItemResponse.fromXContent(BulkItemResponse.java:138) ~[?:?]
2022-06-29T19:14:39.0420302Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.action.bulk.BulkResponse.fromXContent(BulkResponse.java:196) ~[?:?]
2022-06-29T19:14:39.0421041Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:1793) ~[?:?]
2022-06-29T19:14:39.0421853Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient.lambda$performRequestAsyncAndParseEntity$10(RestHighLevelClient.java:1581) ~[?:?]
2022-06-29T19:14:39.0422676Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient$1.onSuccess(RestHighLevelClient.java:1663) ~[?:?]
2022-06-29T19:14:39.0423465Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$FailureTrackingResponseListener.onSuccess(RestClient.java:590) ~[?:?]
2022-06-29T19:14:39.0424215Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$1.completed(RestClient.java:333) ~[?:?]
2022-06-29T19:14:39.0424907Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$1.completed(RestClient.java:327) ~[?:?]
2022-06-29T19:14:39.0425605Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:122) ~[?:?]
2022-06-29T19:14:39.0426411Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:181) ~[?:?]
2022-06-29T19:14:39.0427309Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse(HttpAsyncRequestExecutor.java:448) ~[?:?]
2022-06-29T19:14:39.0428140Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.nio.protocol.HttpAsyncRequestExecutor.inputReady(HttpAsyncRequestExecutor.java:338) ~[?:?]
2022-06-29T19:14:39.0428965Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput(DefaultNHttpClientConnection.java:265) ~[?:?]
2022-06-29T19:14:39.0429771Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:81) ~[?:?]
2022-06-29T19:14:39.0430553Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:39) ~[?:?]
2022-06-29T19:14:39.0431331Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:114) ~[?:?]
2022-06-29T19:14:39.0432205Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:162) ~[?:?]
2022-06-29T19:14:39.0432961Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:337) ~[?:?]
2022-06-29T19:14:39.0433744Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:315) ~[?:?]
2022-06-29T19:14:39.0434792Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:276) ~[?:?]
2022-06-29T19:14:39.0435534Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104) ~[?:?]
2022-06-29T19:14:39.0459721Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:591) ~[?:?]
2022-06-29T19:14:39.0460527Z Jun 29 19:14:38 		... 1 more
2022-06-29T19:14:39.0461628Z Jun 29 19:14:38 Caused by: org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException: Elasticsearch exception [type=cluster_block_exception, reason=index [my_users] blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];]
2022-06-29T19:14:39.0462516Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException.innerFromXContent(ElasticsearchException.java:496) ~[?:?]
2022-06-29T19:14:39.0463295Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException.fromXContent(ElasticsearchException.java:407) ~[?:?]
2022-06-29T19:14:39.0464052Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.action.bulk.BulkItemResponse.fromXContent(BulkItemResponse.java:138) ~[?:?]
2022-06-29T19:14:39.0464810Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.action.bulk.BulkResponse.fromXContent(BulkResponse.java:196) ~[?:?]
2022-06-29T19:14:39.0465563Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:1793) ~[?:?]
2022-06-29T19:14:39.0466397Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient.lambda$performRequestAsyncAndParseEntity$10(RestHighLevelClient.java:1581) ~[?:?]
2022-06-29T19:14:39.0467225Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient$1.onSuccess(RestHighLevelClient.java:1663) ~[?:?]
2022-06-29T19:14:39.0468016Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$FailureTrackingResponseListener.onSuccess(RestClient.java:590) ~[?:?]
2022-06-29T19:14:39.0468773Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$1.completed(RestClient.java:333) ~[?:?]
2022-06-29T19:14:39.0469464Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$1.completed(RestClient.java:327) ~[?:?]
2022-06-29T19:14:39.0470155Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:122) ~[?:?]
2022-06-29T19:14:39.0470967Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:181) ~[?:?]
2022-06-29T19:14:39.0471854Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse(HttpAsyncRequestExecutor.java:448) ~[?:?]
2022-06-29T19:14:39.0472688Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.nio.protocol.HttpAsyncRequestExecutor.inputReady(HttpAsyncRequestExecutor.java:338) ~[?:?]
2022-06-29T19:14:39.0473770Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput(DefaultNHttpClientConnection.java:265) ~[?:?]
2022-06-29T19:14:39.0474587Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:81) ~[?:?]
2022-06-29T19:14:39.0475633Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:39) ~[?:?]
2022-06-29T19:14:39.0476411Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:114) ~[?:?]
2022-06-29T19:14:39.0477277Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:162) ~[?:?]
2022-06-29T19:14:39.0478054Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:337) ~[?:?]
2022-06-29T19:14:39.0478967Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:315) ~[?:?]
2022-06-29T19:14:39.0479884Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:276) ~[?:?]
2022-06-29T19:14:39.0480637Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104) ~[?:?]
2022-06-29T19:14:39.0481424Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:591) ~[?:?]
2022-06-29T19:14:39.0481994Z Jun 29 19:14:38 	... 1 more
2022-06-29T19:14:39.0482845Z Jun 29 19:14:38 2022-06-29 19:14:37,877 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job 5ef97af7fdd0c4b551549dfceac18104
2022-06-29T19:14:39.0484012Z Jun 29 19:14:38 2022-06-29 19:14:37,879 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - Calculating tasks to restart to recover the failed task df8d2cd231aa2f7a73136c030b1a8da4_0.
2022-06-29T19:14:39.0485681Z Jun 29 19:14:38 2022-06-29 19:14:37,880 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - 2 tasks should be restarted to recover the failed task df8d2cd231aa2f7a73136c030b1a8da4_0. 
2022-06-29T19:14:39.0486975Z Jun 29 19:14:38 2022-06-29 19:14:37,881 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job insert-into_default_catalog.default_database.ElasticsearchAppendSinkTable (5ef97af7fdd0c4b551549dfceac18104) switched from state RUNNING to FAILING.
2022-06-29T19:14:39.0487770Z Jun 29 19:14:38 org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-06-29T19:14:39.0488762Z Jun 29 19:14:38 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0489968Z Jun 29 19:14:38 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0491102Z Jun 29 19:14:38 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0492170Z Jun 29 19:14:38 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0493277Z Jun 29 19:14:38 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0494715Z Jun 29 19:14:38 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0495748Z Jun 29 19:14:38 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0496761Z Jun 29 19:14:38 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0497414Z Jun 29 19:14:38 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_332]
2022-06-29T19:14:39.0497973Z Jun 29 19:14:38 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_332]
2022-06-29T19:14:39.0498613Z Jun 29 19:14:38 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_332]
2022-06-29T19:14:39.0499182Z Jun 29 19:14:38 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_332]
2022-06-29T19:14:39.0500434Z Jun 29 19:14:38 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0501693Z Jun 29 19:14:38 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0502871Z Jun 29 19:14:38 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0504007Z Jun 29 19:14:38 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0505167Z Jun 29 19:14:38 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0506301Z Jun 29 19:14:38 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0507347Z Jun 29 19:14:38 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0508336Z Jun 29 19:14:38 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0509323Z Jun 29 19:14:38 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0510309Z Jun 29 19:14:38 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0511318Z Jun 29 19:14:38 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0512335Z Jun 29 19:14:38 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0513356Z Jun 29 19:14:38 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0514378Z Jun 29 19:14:38 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0515347Z Jun 29 19:14:38 	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0516410Z Jun 29 19:14:38 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0517381Z Jun 29 19:14:38 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0518558Z Jun 29 19:14:38 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0519542Z Jun 29 19:14:38 	at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0520481Z Jun 29 19:14:38 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0521421Z Jun 29 19:14:38 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0522345Z Jun 29 19:14:38 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0522979Z Jun 29 19:14:38 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_332]
2022-06-29T19:14:39.0523555Z Jun 29 19:14:38 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_332]
2022-06-29T19:14:39.0524137Z Jun 29 19:14:38 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_332]
2022-06-29T19:14:39.0524708Z Jun 29 19:14:38 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_332]
2022-06-29T19:14:39.0525253Z Jun 29 19:14:38 Caused by: java.lang.RuntimeException: An error occurred in ElasticsearchSink.
2022-06-29T19:14:39.0525874Z Jun 29 19:14:38 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkErrorAndRethrow(ElasticsearchSinkBase.java:427) ~[?:?]
2022-06-29T19:14:39.0526657Z Jun 29 19:14:38 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkAsyncErrorsAndRequests(ElasticsearchSinkBase.java:432) ~[?:?]
2022-06-29T19:14:39.0527410Z Jun 29 19:14:38 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.invoke(ElasticsearchSinkBase.java:329) ~[?:?]
2022-06-29T19:14:39.0528401Z Jun 29 19:14:38 	at org.apache.flink.table.runtime.operators.sink.SinkOperator.processElement(SinkOperator.java:65) ~[flink-table_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0529506Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0530680Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0531847Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0532961Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0534005Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:495) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0535053Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0536089Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:806) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0537211Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:758) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0538184Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0539134Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0540046Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0540924Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0541485Z Jun 29 19:14:38 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_332]
2022-06-29T19:14:39.0541967Z Jun 29 19:14:38 	Suppressed: java.lang.RuntimeException: An error occurred in ElasticsearchSink.
2022-06-29T19:14:39.0542603Z Jun 29 19:14:38 		at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkErrorAndRethrow(ElasticsearchSinkBase.java:427) ~[?:?]
2022-06-29T19:14:39.0543337Z Jun 29 19:14:38 		at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.close(ElasticsearchSinkBase.java:366) ~[?:?]
2022-06-29T19:14:39.0544308Z Jun 29 19:14:38 		at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0545383Z Jun 29 19:14:38 		at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:114) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0546464Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:141) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0547559Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:127) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0548606Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:1032) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0549639Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1018) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0550635Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:925) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0551615Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:940) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0552611Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0553575Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:940) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0554892Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0555837Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0556411Z Jun 29 19:14:38 		at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_332]
2022-06-29T19:14:39.0557388Z Jun 29 19:14:38 	Caused by: org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException: Elasticsearch exception [type=cluster_block_exception, reason=index [my_users] blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];]
2022-06-29T19:14:39.0558455Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException.innerFromXContent(ElasticsearchException.java:496) ~[?:?]
2022-06-29T19:14:39.0559232Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException.fromXContent(ElasticsearchException.java:407) ~[?:?]
2022-06-29T19:14:39.0559999Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.action.bulk.BulkItemResponse.fromXContent(BulkItemResponse.java:138) ~[?:?]
2022-06-29T19:14:39.0560747Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.action.bulk.BulkResponse.fromXContent(BulkResponse.java:196) ~[?:?]
2022-06-29T19:14:39.0561492Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:1793) ~[?:?]
2022-06-29T19:14:39.0562322Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient.lambda$performRequestAsyncAndParseEntity$10(RestHighLevelClient.java:1581) ~[?:?]
2022-06-29T19:14:39.0563270Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient$1.onSuccess(RestHighLevelClient.java:1663) ~[?:?]
2022-06-29T19:14:39.0576211Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$FailureTrackingResponseListener.onSuccess(RestClient.java:590) ~[?:?]
2022-06-29T19:14:39.0577089Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$1.completed(RestClient.java:333) ~[?:?]
2022-06-29T19:14:39.0577791Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$1.completed(RestClient.java:327) ~[?:?]
2022-06-29T19:14:39.0578496Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:122) ~[?:?]
2022-06-29T19:14:39.0579331Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:181) ~[?:?]
2022-06-29T19:14:39.0580224Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse(HttpAsyncRequestExecutor.java:448) ~[?:?]
2022-06-29T19:14:39.0581044Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.nio.protocol.HttpAsyncRequestExecutor.inputReady(HttpAsyncRequestExecutor.java:338) ~[?:?]
2022-06-29T19:14:39.0581870Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput(DefaultNHttpClientConnection.java:265) ~[?:?]
2022-06-29T19:14:39.0582689Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:81) ~[?:?]
2022-06-29T19:14:39.0583493Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:39) ~[?:?]
2022-06-29T19:14:39.0584262Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:114) ~[?:?]
2022-06-29T19:14:39.0585017Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:162) ~[?:?]
2022-06-29T19:14:39.0585771Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:337) ~[?:?]
2022-06-29T19:14:39.0586546Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:315) ~[?:?]
2022-06-29T19:14:39.0587322Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:276) ~[?:?]
2022-06-29T19:14:39.0588299Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104) ~[?:?]
2022-06-29T19:14:39.0589098Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:591) ~[?:?]
2022-06-29T19:14:39.0589669Z Jun 29 19:14:38 		... 1 more
2022-06-29T19:14:39.0590752Z Jun 29 19:14:38 Caused by: org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException: Elasticsearch exception [type=cluster_block_exception, reason=index [my_users] blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];]
2022-06-29T19:14:39.0591642Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException.innerFromXContent(ElasticsearchException.java:496) ~[?:?]
2022-06-29T19:14:39.0592434Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException.fromXContent(ElasticsearchException.java:407) ~[?:?]
2022-06-29T19:14:39.0593196Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.action.bulk.BulkItemResponse.fromXContent(BulkItemResponse.java:138) ~[?:?]
2022-06-29T19:14:39.0593935Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.action.bulk.BulkResponse.fromXContent(BulkResponse.java:196) ~[?:?]
2022-06-29T19:14:39.0594676Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:1793) ~[?:?]
2022-06-29T19:14:39.0595509Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient.lambda$performRequestAsyncAndParseEntity$10(RestHighLevelClient.java:1581) ~[?:?]
2022-06-29T19:14:39.0596352Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient$1.onSuccess(RestHighLevelClient.java:1663) ~[?:?]
2022-06-29T19:14:39.0597144Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$FailureTrackingResponseListener.onSuccess(RestClient.java:590) ~[?:?]
2022-06-29T19:14:39.0597889Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$1.completed(RestClient.java:333) ~[?:?]
2022-06-29T19:14:39.0598697Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$1.completed(RestClient.java:327) ~[?:?]
2022-06-29T19:14:39.0599402Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:122) ~[?:?]
2022-06-29T19:14:39.0600208Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:181) ~[?:?]
2022-06-29T19:14:39.0601451Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse(HttpAsyncRequestExecutor.java:448) ~[?:?]
2022-06-29T19:14:39.0602310Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.nio.protocol.HttpAsyncRequestExecutor.inputReady(HttpAsyncRequestExecutor.java:338) ~[?:?]
2022-06-29T19:14:39.0603127Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput(DefaultNHttpClientConnection.java:265) ~[?:?]
2022-06-29T19:14:39.0603927Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:81) ~[?:?]
2022-06-29T19:14:39.0604709Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:39) ~[?:?]
2022-06-29T19:14:39.0605477Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:114) ~[?:?]
2022-06-29T19:14:39.0606364Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:162) ~[?:?]
2022-06-29T19:14:39.0607123Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:337) ~[?:?]
2022-06-29T19:14:39.0607908Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:315) ~[?:?]
2022-06-29T19:14:39.0608673Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:276) ~[?:?]
2022-06-29T19:14:39.0609399Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104) ~[?:?]
2022-06-29T19:14:39.0610193Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:591) ~[?:?]
2022-06-29T19:14:39.0610854Z Jun 29 19:14:38 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_332]
2022-06-29T19:14:39.0611795Z Jun 29 19:14:38 2022-06-29 19:14:37,890 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 505af083caeb63496e86df5e472f21d0.
2022-06-29T19:14:39.0612982Z Jun 29 19:14:38 2022-06-29 19:14:37,895 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job insert-into_default_catalog.default_database.ElasticsearchAppendSinkTable (5ef97af7fdd0c4b551549dfceac18104) switched from state FAILING to FAILED.
2022-06-29T19:14:39.0613767Z Jun 29 19:14:38 org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-06-29T19:14:39.0614755Z Jun 29 19:14:38 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0615982Z Jun 29 19:14:38 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0617122Z Jun 29 19:14:38 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0618187Z Jun 29 19:14:38 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0619282Z Jun 29 19:14:38 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0620362Z Jun 29 19:14:38 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0621419Z Jun 29 19:14:38 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0622423Z Jun 29 19:14:38 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0623077Z Jun 29 19:14:38 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_332]
2022-06-29T19:14:39.0623635Z Jun 29 19:14:38 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_332]
2022-06-29T19:14:39.0624269Z Jun 29 19:14:38 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_332]
2022-06-29T19:14:39.0624899Z Jun 29 19:14:38 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_332]
2022-06-29T19:14:39.0625955Z Jun 29 19:14:38 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0627169Z Jun 29 19:14:38 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0628340Z Jun 29 19:14:38 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0629460Z Jun 29 19:14:38 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0630623Z Jun 29 19:14:38 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0631770Z Jun 29 19:14:38 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0632819Z Jun 29 19:14:38 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0633812Z Jun 29 19:14:38 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0634812Z Jun 29 19:14:38 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0635805Z Jun 29 19:14:38 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0636829Z Jun 29 19:14:38 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0637846Z Jun 29 19:14:38 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0638969Z Jun 29 19:14:38 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0639983Z Jun 29 19:14:38 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0640959Z Jun 29 19:14:38 	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0641899Z Jun 29 19:14:38 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0642882Z Jun 29 19:14:38 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0643869Z Jun 29 19:14:38 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0644826Z Jun 29 19:14:38 	at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0645787Z Jun 29 19:14:38 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0646718Z Jun 29 19:14:38 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0647709Z Jun 29 19:14:38 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0648410Z Jun 29 19:14:38 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_332]
2022-06-29T19:14:39.0648988Z Jun 29 19:14:38 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_332]
2022-06-29T19:14:39.0649573Z Jun 29 19:14:38 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_332]
2022-06-29T19:14:39.0650154Z Jun 29 19:14:38 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_332]
2022-06-29T19:14:39.0650700Z Jun 29 19:14:38 Caused by: java.lang.RuntimeException: An error occurred in ElasticsearchSink.
2022-06-29T19:14:39.0651335Z Jun 29 19:14:38 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkErrorAndRethrow(ElasticsearchSinkBase.java:427) ~[?:?]
2022-06-29T19:14:39.0652104Z Jun 29 19:14:38 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkAsyncErrorsAndRequests(ElasticsearchSinkBase.java:432) ~[?:?]
2022-06-29T19:14:39.0652859Z Jun 29 19:14:38 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.invoke(ElasticsearchSinkBase.java:329) ~[?:?]
2022-06-29T19:14:39.0653852Z Jun 29 19:14:38 	at org.apache.flink.table.runtime.operators.sink.SinkOperator.processElement(SinkOperator.java:65) ~[flink-table_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0654973Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0656147Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0657309Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0658450Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0659486Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:495) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0660536Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0661583Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:806) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0662547Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:758) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0663531Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0664492Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0665410Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0666300Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0666878Z Jun 29 19:14:38 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_332]
2022-06-29T19:14:39.0667356Z Jun 29 19:14:38 	Suppressed: java.lang.RuntimeException: An error occurred in ElasticsearchSink.
2022-06-29T19:14:39.0668101Z Jun 29 19:14:38 		at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkErrorAndRethrow(ElasticsearchSinkBase.java:427) ~[?:?]
2022-06-29T19:14:39.0668829Z Jun 29 19:14:38 		at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.close(ElasticsearchSinkBase.java:366) ~[?:?]
2022-06-29T19:14:39.0669807Z Jun 29 19:14:38 		at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0670882Z Jun 29 19:14:38 		at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:114) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0671966Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:141) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0673057Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:127) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0674126Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:1032) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0675169Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1018) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0676176Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:925) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0677190Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:940) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0678175Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0679223Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:940) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0680132Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0681028Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0681602Z Jun 29 19:14:38 		at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_332]
2022-06-29T19:14:39.0682580Z Jun 29 19:14:38 	Caused by: org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException: Elasticsearch exception [type=cluster_block_exception, reason=index [my_users] blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];]
2022-06-29T19:14:39.0683468Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException.innerFromXContent(ElasticsearchException.java:496) ~[?:?]
2022-06-29T19:14:39.0684252Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException.fromXContent(ElasticsearchException.java:407) ~[?:?]
2022-06-29T19:14:39.0685017Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.action.bulk.BulkItemResponse.fromXContent(BulkItemResponse.java:138) ~[?:?]
2022-06-29T19:14:39.0685770Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.action.bulk.BulkResponse.fromXContent(BulkResponse.java:196) ~[?:?]
2022-06-29T19:14:39.0686513Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:1793) ~[?:?]
2022-06-29T19:14:39.0687329Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient.lambda$performRequestAsyncAndParseEntity$10(RestHighLevelClient.java:1581) ~[?:?]
2022-06-29T19:14:39.0688293Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient$1.onSuccess(RestHighLevelClient.java:1663) ~[?:?]
2022-06-29T19:14:39.0689078Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$FailureTrackingResponseListener.onSuccess(RestClient.java:590) ~[?:?]
2022-06-29T19:14:39.0689831Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$1.completed(RestClient.java:333) ~[?:?]
2022-06-29T19:14:39.0690515Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$1.completed(RestClient.java:327) ~[?:?]
2022-06-29T19:14:39.0691211Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:122) ~[?:?]
2022-06-29T19:14:39.0692018Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:181) ~[?:?]
2022-06-29T19:14:39.0693088Z Jun 29 19:14:38 2022-06-29 19:14:09,618 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - --------------------------------------------------------------------------------
2022-06-29T19:14:39.0693967Z Jun 29 19:14:38 2022-06-29 19:14:09,620 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Preconfiguration: 
2022-06-29T19:14:39.0694727Z Jun 29 19:14:38 2022-06-29 19:14:09,620 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - 
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37377&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c
",,godfreyhe,hxbks2ks,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28355,FLINK-28425,FLINK-28680,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 20 06:21:41 UTC 2022,,,,,,,,,,"0|z16hao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/22 07:24;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37383&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=2a8cc459-df7a-5e6f-12bf-96efcc369aa9&l=20541;;;","30/Jun/22 07:26;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37383&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=89ed5489-a970-5ff2-67f7-d7391de0165f&l=21631

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37383&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=cc5499f8-bdde-5157-0d76-b6528ecd808e&l=21262

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37383&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=75f4c82e-ad02-5844-81c9-d16399e3372d&l=21540;;;","04/Jul/22 11:10;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37536&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=20973

This is most likely caused because there's no more diskspace available, which causes Elasticsearch to go into read-only mode. Most likely caused by something like FLINK-28355;;;","05/Jul/22 08:30;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37603&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=21293;;;","06/Jul/22 08:03;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37690&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=20822;;;","06/Jul/22 08:26;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37722&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","13/Jul/22 02:29;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38058&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c;;;","20/Jul/22 06:21;hxbks2ks;I have merged the commit 6ab358e87525d41526648fd6e802179e9ceed2f3 in release-1.14 to clean up the python environment.

I will continue to pay attention to the CI to see whether this problem have been solved
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Helm/Helmfile lint fails with ""invalid Yaml document separator""",FLINK-28300,13469106,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,maksimaniskov,maksimaniskov,maksimaniskov,29/Jun/22 13:03,07/Jul/22 14:48,13/Jul/23 08:13,07/Jul/22 14:48,kubernetes-operator-1.0.0,kubernetes-operator-1.0.1,,,,,,kubernetes-operator-1.1.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"helmfile lint output:
{quote}{{STDERR:}}
{{    Error: 1 chart(s) linted, 1 chart(s) failed}}
{{  }}
{{  COMBINED OUTPUT:}}
{{    Error: 1 chart(s) linted, 1 chart(s) failed}}
{{    ==> Linting /var/folders/yb/r7jlgg613z5dxhpf83zxbxlh0000gn/T/helmfile1348989835/flink-operator-jobs/namespaces/apache-flink-kubernetes-operator/flink-kubernetes-operator/latest/flink-kubernetes-operator}}
{{    [INFO] Chart.yaml: icon is recommended}}
{{    [ERROR] templates/serviceaccount.yaml: unable to parse YAML: invalid Yaml document separator: apiVersion: v1}}
{quote}
 

Problematic is trimming whitespaces, including the line feeds, in [helm/flink-kubernetes-operator/templates/serviceaccount.yaml|https://github.com/apache/flink-kubernetes-operator/blob/97883eb8f586ccd703952efc59a43f019ef83fa7/helm/flink-kubernetes-operator/templates/serviceaccount.yaml#L20]
{quote}{{---}}
{{ {{{}- if .Values.operatorServiceAccount.create -{}}}}}
{{apiVersion: v1}}{quote}",,gyfora,maksimaniskov,mbalassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 07 14:48:45 UTC 2022,,,,,,,,,,"0|z16gsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/22 14:02;maksimaniskov;Here is fix for it [https://github.com/MaksimAniskov/flink-kubernetes-operator/commit/565050a9b050e6fb72545e77feab48bf267a6701|https://github.com/MaksimAniskov/flink-kubernetes-operator/commit/565050a9b050e6fb72545e77feab48bf267a6701];;;","29/Jun/22 15:14;gyfora;Hi [~maksimaniskov] , thank you for reporting this issue. Could you please open a PR to the apache repo?;;;","30/Jun/22 06:42;maksimaniskov;https://github.com/apache/flink-kubernetes-operator/pull/286;;;","07/Jul/22 14:48;mbalassi;[4c8875a|https://github.com/apache/flink-kubernetes-operator/commit/4c8875aba1104ab376120829099aa1f3f092a7dd] in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SessionManagerTest.testSessionNumberLimit failed with AssertionFailedError,FLINK-28290,13469003,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,hxbks2ks,hxbks2ks,29/Jun/22 02:42,30/Jun/22 12:10,13/Jul/23 08:13,30/Jun/22 12:06,1.16.0,,,,,,,,,,,,Table SQL / Gateway,,,,,,,0,pull-request-available,test-stability,,,"{code:java}
2022-06-28T11:40:17.0099766Z Jun 28 11:40:17 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 6.192 s <<< FAILURE! - in org.apache.flink.table.gateway.service.session.SessionManagerTest
2022-06-28T11:40:17.0101180Z Jun 28 11:40:17 [ERROR] org.apache.flink.table.gateway.service.session.SessionManagerTest.testSessionNumberLimit  Time elapsed: 0.115 s  <<< FAILURE!
2022-06-28T11:40:17.0103132Z Jun 28 11:40:17 org.opentest4j.AssertionFailedError: Failed to create session, the count of active sessions exceeds the max count: 3 ==> Expected org.apache.flink.table.gateway.api.utils.SqlGatewayException to be thrown, but nothing was thrown.
2022-06-28T11:40:17.0104111Z Jun 28 11:40:17 	at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:71)
2022-06-28T11:40:17.0105112Z Jun 28 11:40:17 	at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:41)
2022-06-28T11:40:17.0105830Z Jun 28 11:40:17 	at org.junit.jupiter.api.Assertions.assertThrows(Assertions.java:3098)
2022-06-28T11:40:17.0106714Z Jun 28 11:40:17 	at org.apache.flink.table.gateway.service.session.SessionManagerTest.testSessionNumberLimit(SessionManagerTest.java:103)
2022-06-28T11:40:17.0108109Z Jun 28 11:40:17 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-28T11:40:17.0108750Z Jun 28 11:40:17 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-28T11:40:17.0109463Z Jun 28 11:40:17 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-28T11:40:17.0110099Z Jun 28 11:40:17 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-06-28T11:40:17.0110738Z Jun 28 11:40:17 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-06-28T11:40:17.0111459Z Jun 28 11:40:17 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-06-28T11:40:17.0112413Z Jun 28 11:40:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-06-28T11:40:17.0113461Z Jun 28 11:40:17 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-06-28T11:40:17.0114613Z Jun 28 11:40:17 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-06-28T11:40:17.0115411Z Jun 28 11:40:17 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2022-06-28T11:40:17.0116262Z Jun 28 11:40:17 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-06-28T11:40:17.0117111Z Jun 28 11:40:17 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-06-28T11:40:17.0117957Z Jun 28 11:40:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-06-28T11:40:17.0118829Z Jun 28 11:40:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-06-28T11:40:17.0119660Z Jun 28 11:40:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-06-28T11:40:17.0120492Z Jun 28 11:40:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-06-28T11:40:17.0121260Z Jun 28 11:40:17 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-06-28T11:40:17.0122126Z Jun 28 11:40:17 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-06-28T11:40:17.0122934Z Jun 28 11:40:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-06-28T11:40:17.0123774Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T11:40:17.0124659Z Jun 28 11:40:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-06-28T11:40:17.0125490Z Jun 28 11:40:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-06-28T11:40:17.0126280Z Jun 28 11:40:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-06-28T11:40:17.0127081Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-06-28T11:40:17.0127896Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T11:40:17.0128712Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-28T11:40:17.0129463Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-28T11:40:17.0130210Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-28T11:40:17.0131123Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T11:40:17.0132179Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-28T11:40:17.0132934Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-28T11:40:17.0133847Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-28T11:40:17.0134965Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-06-28T11:40:17.0136044Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-06-28T11:40:17.0137068Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-06-28T11:40:17.0137881Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T11:40:17.0138696Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-28T11:40:17.0139446Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-28T11:40:17.0140191Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-28T11:40:17.0140994Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T11:40:17.0141922Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-28T11:40:17.0142691Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-28T11:40:17.0143607Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-28T11:40:17.0144669Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-06-28T11:40:17.0145606Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-06-28T11:40:17.0146415Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T11:40:17.0147235Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-28T11:40:17.0147978Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-28T11:40:17.0148707Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-28T11:40:17.0149506Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T11:40:17.0150300Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-28T11:40:17.0151059Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-28T11:40:17.0152134Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-28T11:40:17.0153104Z Jun 28 11:40:17 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-06-28T11:40:17.0153730Z Jun 28 11:40:17 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-06-28T11:40:17.0154361Z Jun 28 11:40:17 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-06-28T11:40:17.0155016Z Jun 28 11:40:17 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-06-28T11:40:17.0155670Z Jun 28 11:40:17 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-06-28T11:40:17.0156155Z Jun 28 11:40:17 
2022-06-28T11:40:17.8466649Z Jun 28 11:40:17 [INFO] 
2022-06-28T11:40:17.8467245Z Jun 28 11:40:17 [INFO] Results:
2022-06-28T11:40:17.8467619Z Jun 28 11:40:17 [INFO] 
2022-06-28T11:40:17.8467995Z Jun 28 11:40:17 [ERROR] Failures: 
2022-06-28T11:40:17.8468867Z Jun 28 11:40:17 [ERROR]   SessionManagerTest.testSessionNumberLimit:103 Failed to create session, the count of active sessions exceeds the max count: 3 ==> Expected org.apache.flink.table.gateway.api.utils.SqlGatewayException to be thrown, but nothing was thrown.
2022-06-28T11:40:17.8470031Z Jun 28 11:40:17 [INFO] 
2022-06-28T11:40:17.8470490Z Jun 28 11:40:17 [ERROR] Tests run: 10, Failures: 1, Errors: 0, Skipped: 0
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37302&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4",,fsk119,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28270,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 30 12:06:34 UTC 2022,,,,,,,,,,"0|z16g5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/22 02:42;hxbks2ks;cc [~fsk119];;;","30/Jun/22 12:06;fsk119;Fixed in master ac94fa1e9f32ef25c665bc167146bede73b46afd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.flink.connectors.hive.FlinkHiveException: Unable to instantiate the hadoop input format,FLINK-28276,13468830,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,migowei,migowei,28/Jun/22 09:03,01/Jul/22 03:08,13/Jul/23 08:13,01/Jul/22 03:08,1.14.2,,,,,,,,,,,,Connectors / Hive,,,,,,,0,,,,,"When I read Iceberg tables using Flink HiveCatalog, based on S3A,  I got this error:

 
{code:java}
//代码占位符
Exception in thread ""main"" org.apache.flink.connectors.hive.FlinkHiveException: Unable to instantiate the hadoop input format
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createMRSplits(HiveSourceFileEnumerator.java:100)
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:71)
    at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:149)
    at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107)
    at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:95)
    at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:144)
    at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:114)
    at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:106)
    at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.java:49)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:250)
    at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:58)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
    at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:82)
    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
    at scala.collection.Iterator.foreach(Iterator.scala:937)
    at scala.collection.Iterator.foreach$(Iterator.scala:937)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
    at scala.collection.IterableLike.foreach(IterableLike.scala:70)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableLike.map(TraversableLike.scala:233)
    at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
    at scala.collection.AbstractTraversable.map(Traversable.scala:104)
    at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:81)
    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:185)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1665)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:805)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1274)
    at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:601)
    at loshu.flink.hive.FlinkSQLHiveWriter.main(FlinkSQLHiveWriter.java:69)
Caused by: java.lang.InstantiationException
    at sun.reflect.InstantiationExceptionConstructorAccessorImpl.newInstance(InstantiationExceptionConstructorAccessorImpl.java:48)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at java.lang.Class.newInstance(Class.java:442)
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createMRSplits(HiveSourceFileEnumerator.java:98)
    ... 30 more
16:33:36,767 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - Stopping s3a-file-system metrics system...
16:33:36,767 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - s3a-file-system metrics system stopped.
16:33:36,768 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - s3a-file-system metrics system shutdown complete.Process finished with exit code 1
 {code}
My code is:
{code:java}
//代码占位符
public class FlinkSQLHiveWriter {
    private static org.apache.log4j.Logger log = Logger.getLogger(FlinkSQLHiveWriter.class);
    public static void main(String[] args) throws Exception {
        System.setProperty(""HADOOP_USER_NAME"", ""root"");
        System.setProperty(""hadoop.home.dir"", ""/opt/hadoop-3.2.1/"");
        EnvironmentSettings settings = EnvironmentSettings.newInstance()
                .inBatchMode()
                .build();
        TableEnvironment tableEnv = TableEnvironment.create(settings);

        String catalogName = ""s3IcebergCatalog"";
        String defaultDatabase = ""s3a_flink"";
        String hiveConfDir = ""flink-cloud/src/main/resources""; 
        HiveCatalog hive = new HiveCatalog(catalogName, defaultDatabase, hiveConfDir);

        tableEnv.registerCatalog(catalogName, hive);
        tableEnv.useCatalog(catalogName);
        tableEnv.useDatabase(defaultDatabase);

        System.out.println(hive.listDatabases());
        System.out.println(hive.listTables(defaultDatabase));
        String tableName = ""icebergTBCloudTracking"";
        // set sql dialect as default, means using flink sql.
        tableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);

        String sql = ""select vin from "" + tableName;
//        String sql = ""DESC "" + tableName;
        System.out.println(sql);
        Table table = tableEnv.sqlQuery(sql);
        table.execute();
    }
} {code}
I can ""show tables"" or ""describe tables"", but when using ""select * from table"" the error occurs.

 ","Flink 1.14.2

Hive 3.1.2

Scala 2.12

Iceberg 0.12.1

Hadoop 3.2.1",luoyuxia,migowei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jun/22 02:05;luoyuxia;BA9CEEA0-BF38-4568-A7AD-66C68B19CF14.png;https://issues.apache.org/jira/secure/attachment/13046065/BA9CEEA0-BF38-4568-A7AD-66C68B19CF14.png","30/Jun/22 01:37;migowei;image-2022-06-30-09-37-44-705.png;https://issues.apache.org/jira/secure/attachment/13045972/image-2022-06-30-09-37-44-705.png","30/Jun/22 02:36;migowei;image-2022-06-30-10-36-17-075.png;https://issues.apache.org/jira/secure/attachment/13046068/image-2022-06-30-10-36-17-075.png","01/Jul/22 03:05;migowei;image-2022-07-01-11-05-01-238.png;https://issues.apache.org/jira/secure/attachment/13046116/image-2022-07-01-11-05-01-238.png",,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,java,Fri Jul 01 03:07:31 UTC 2022,,,,,,,,,,"0|z16f34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/22 08:35;luoyuxia;[~migowei] What's the table's input format? You can use `describe extended xxx` to check it. And please make sure the class of table's inputFormat exists in your classpath.

 ;;;","30/Jun/22 01:46;migowei;The table input format displayed by `describe extended xxx` is as follows. 

!image-2022-06-30-09-37-44-705.png!

 

Debugging in debug mode shows this input format is used:

org.apache.hadoop.mapred.FileInputFormat

org.apache.hadoop.mapred.FileOutputFormat

 

And part of my pom.xml is:
{code:java}
<!-- mapreduce -->
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-mapreduce-client-core</artifactId>
    <version>${hadoop.version}</version>
    </exclusions>
</dependency>

<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-mapreduce-client-common</artifactId>
    <version>${hadoop.version}</version>
</dependency>

<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-mapreduce-client-jobclient</artifactId>
    <version>${hadoop.version}</version>
</dependency>

<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-hdfs</artifactId>
</dependency>

<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-common</artifactId>
    <version>${hadoop.version}</version>
</dependency> 

<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-mapred</artifactId>
    <version>0.22.0</version>
</dependency>{code}
This issue still exists.  T T;;;","30/Jun/22 02:05;luoyuxia;FileInputFormat is just an abstract class. I mean the what is the real inputformat? E.g. OrcInputFormat, MapredParquetInputFormat, TextInputFormat.

!BA9CEEA0-BF38-4568-A7AD-66C68B19CF14.png|width=500,height=204!;;;","30/Jun/22 02:37;migowei;I didn't specify the input format when creating the table, so the result is as follows.  Which input format should I use when using iceberg table? Thanks a lot!!!

!image-2022-06-30-10-36-17-075.png!;;;","30/Jun/22 03:12;luoyuxia;The picture isn't completed so that I can't see the full inputFormt. It'll be better if I can know the full class name of inputFormt.

But I try to google it for the exception. Seems the inputFormt class is an abtract class so that it can'be Instantiated. [https://stackoverflow.com/questions/7896773/instantiationexception-on-simple-reflective-call-to-newinstance-on-a-class]

Any input format you can use in iceberg table. But the column based format, like OrcInputFormat,ParquetFormat is more common for data warehouse.;;;","30/Jun/22 03:46;migowei;Thanks! I think this is the cause of the problem. I create table by flink sdk as follow:
{code:java}
Map<String, String> properties = new HashMap<>();
        properties.put(""type"", ""iceberg"");
        properties.put(""clients"", ""5"");
        properties.put(""property-version"", ""1"");
        properties.put(""warehouse"", ""s3a://warehouse/"");
        properties.put(""catalog-type"", ""hive"");
        properties.put(""uri"", ""thrift://hiveserver:9083"");


        Configuration conf = new Configuration();
        conf.set(""fs.s3a.connection.ssl.enabled"", ""false"");
        conf.set(""fs.s3a.endpoint"", ""http://mys3"");
        conf.set(""fs.s3a.region"", ""us-east-1"");
        conf.set(""fs.s3a.path.style.access"", ""true"");
        conf.set(""fs.s3a.impl"", ""org.apache.hadoop.fs.s3a.S3AFileSystem"");
        conf.set(""fs.s3a.fast.upload"", ""true"");
        conf.set(""execution.checkpointing.checkpoints-after-tasks-finish.enabled"", ""true"");

        String HIVE_CATALOG = ""s3IcebergCatalog"";
        CatalogLoader catalogLoader = CatalogLoader.hive(HIVE_CATALOG, conf, properties);
        HiveCatalog catalog = (HiveCatalog) catalogLoader.loadCatalog();

        Namespace namespace = Namespace.of(""s3a_flink"");
        if (!catalog.namespaceExists(namespace))
            catalog.createNamespace(namespace);

        TableIdentifier name =
                TableIdentifier.of(namespace, ""icebergTBCloudTracking"");

        Schema schema = new Schema(0,
                Types.NestedField.required(1, ""vin"", Types.StringType.get()),
                Types.NestedField.required(2, ""name"", Types.StringType.get()),
                Types.NestedField.optional(3, ""uuid"", Types.StringType.get()),
                Types.NestedField.required(4, ""channel"", Types.StringType.get()),
                Types.NestedField.required(5, ""run_scene"", Types.StringType.get()),
                Types.NestedField.required(6, ""timestamp"", Types.TimestampType.withoutZone()),
                Types.NestedField.required(7, ""rcv_timestamp"", Types.TimestampType.withoutZone()),
                Types.NestedField.required(8, ""raw"", Types.StringType.get())
                );

         PartitionSpec spec = PartitionSpec.unpartitioned();

        Map<String, String> props =
                ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, FileFormat.ORC.name());

        Table table = null;
 
        if (!catalog.tableExists(name))
            table = catalog.createTable(name, schema, spec, props);
        else
            table = catalog.loadTable(name); {code}
May I ask how to specify inputformat when creating table by sdk?  Thanks!;;;","30/Jun/22 04:23;luoyuxia;It's about iceberg sdk. I'm not familar with it. And I'm not sure wheher we can specific the inputfomat using sdk.

If not, I think you may need to use Hive SQL in HiveCli or switch to [hive dialect  |https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/table/hive/hive_dialect]in Flink.;;;","01/Jul/22 03:07;migowei;I add  `table.updateProperties().set(""engine.hive.enabled"", ""true"").commit();` in code and it works.

!image-2022-07-01-11-05-01-238.png!

 

But there are other errors. I will open another question. Thanks~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ContinuousFileMonitoringFunction doesn't work with reactive mode,FLINK-28274,13468793,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Leo Zhou,rmetzger,rmetzger,28/Jun/22 06:34,06/Jul/22 06:12,13/Jul/23 08:13,06/Jul/22 06:12,1.16.0,,,,,,,1.16.0,,,,,API / DataStream,Runtime / Coordination,,,,,,0,pull-request-available,,,,"This issue was first reported in the Flink Slack: https://apache-flink.slack.com/archives/C03G7LJTS2G/p1656257678477659

It seems that reactive mode is changing the parallelism of the `ContinuousFileMonitoringFunction`, which is supposed to always run with a parallelism of 1.

This is the error
{code}
INITIALIZING to FAILED with failure cause: java.lang.IllegalArgumentException: ContinuousFileMonitoringFunction retrieved invalid state.
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)
{code}

You can see from the logs that the parallelism is changing on a rescale event:
{code}
2022-06-27 13:38:54,979 | INFO  | .executiongraph.ExecutionGraph  | Source: Custom File Source (1/1) (cbaad20beee908b95c9fe5c34ba76bfa) switched from RUNNING to CANCELING.
2022-06-27 13:38:55,254 | INFO  | .executiongraph.ExecutionGraph  | Source: Custom File Source (1/1) (cbaad20beee908b95c9fe5c34ba76bfa) switched from CANCELING to CANCELED.
2022-06-27 13:38:55,657 | INFO  | .executiongraph.ExecutionGraph  | Source: Custom File Source (2/2) (6ceaacbe8d9aa507b0a56c850082da8c) switched from DEPLOYING to INITIALIZING.
2022-06-27 13:38:55,722 | INFO  | .executiongraph.ExecutionGraph  | Source: Custom File Source (2/2) (6ceaacbe8d9aa507b0a56c850082da8c) switched from INITIALIZING to RUNNING.
2022-06-27 13:44:54,058 | INFO  | .executiongraph.ExecutionGraph  | Source: Custom File Source (1/2) (665b12194741744d6bba4408a252fa45) switched from RUNNING to CANCELING.
2022-06-27 13:45:00,825 | INFO  | .executiongraph.ExecutionGraph  | Source: Custom File Source (1/2) (3cc408fd0eb9ddfa97b22f4dfc09d8dc) switched from DEPLOYING to INITIALIZING.
2022-06-27 13:45:00,826 | INFO  | .executiongraph.ExecutionGraph  | Source: Custom File Source (1/2) (3cc408fd0eb9ddfa97b22f4dfc09d8dc) switched from INITIALIZING to RUNNING.
2022-06-27 13:45:01,434 | INFO  | .executiongraph.ExecutionGraph  | Source: Custom File Source (2/2) (79338042d84b6458c34760bc85145512) switched from DEPLOYING to INITIALIZING.
2022-06-27 13:45:02,427 | INFO  | .executiongraph.ExecutionGraph  | Source: Custom File Source (2/2) (79338042d84b6458c34760bc85145512) switched from INITIALIZING to RUNNING.
{code}",,Leo Zhou,martijnvisser,rmetzger,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 06 06:12:33 UTC 2022,,,,,,,,,,"0|z16euw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 06:40;rmetzger;I currently don't have the capacity to look into fixing this, but if somebody is interested, this is what I would do:
1. Add a new test using the file monitoring source to the ReactiveModeITCase and verify that the test really fails. Maybe this is not the best way to test the fix, but its a starting point for locally reproducing the issue.
2. I guess that the “computeVertexParallelismStoreForExecution” method is where you’ll find the logic where reactive mode sets the parallelism
3. The fix is either ensuring that maxParallelism for the monitoring function is set correctly, or somehow telling the adaptive scheduler to not change the parallelism of that function (in a generic way);;;","30/Jun/22 13:06;Leo Zhou;---The fix is either ensuring that maxParallelism for the monitoring function is set correctly, or somehow telling the adaptive scheduler to not change the parallelism of that function (in a generic way)

I prefer to set the max parallelism for the monitoring function correctly. Since ContinuousFileMonitoringFunction is a legacy source function, we'd better not change the adaptive scheduler for  this issue.

[AdaptiveScheduler.java#L344|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptive/AdaptiveScheduler.java#L344] will calculate the maxParallelism for vertex only if no max parallelism was configured, so if we can ensure that maxParallelism for the monitoring function is 1, then reactive mode would not change the parallelism of the `ContinuousFileMonitoringFunction`.

Since `ContinuousFileMonitoringFunction` is only used in [StreamExecutionEnvironment.java#L1855|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java#L1855] , It's safe to explicitly set the max parallelism here.;;;","30/Jun/22 13:15;rmetzger;Thanks for your research. I agree that we shouldn't include source-specific logic into the adaptive scheduler for this.

I like the idea of setting maxParallelism in the SEE (I currently can not imagine any issues with that, but we'll see if all the tests are passing properly once you've done the change ;) );;;","30/Jun/22 13:59;Leo Zhou;HI [~rmetzger] , could you assign this ticket to me before I open a PR ?;;;","04/Jul/22 14:44;martijnvisser;Does this problem only occur for {ContinuousFileMonitoringFunction} or also for the new {FileSource}? ;;;","05/Jul/22 02:37;wanglijie;I think it only occur for {{ContinuousFileMonitoringFunction (legacy file source)}}. From what I understand, the {{ContinuousFileMonitoringFunction}} plays the role of SplitEnumerator in legacy file source (parallelism must be 1). The new FileSource does not need it, so new FileSource shouldn't have this problem.;;;","05/Jul/22 02:42;wanglijie;I have one more question, does it also occur with other operators whose parallelism must be 1 (For example, {{GlobalAgg}} in Table/SQL)?;;;","05/Jul/22 06:52;martijnvisser;Honest question: why fix it if it's only occurring in a legacy component and there is a solution for it, using the target approach? Especially since {ContinuousFileMonitoringFunction} is internal as well. ;;;","05/Jul/22 07:35;Leo Zhou;-- Does this problem only occur for \{ContinuousFileMonitoringFunction} or also for the new \{FileSource}

As [~wanglijie95] said, with FLIP-27, FileSource does not need {{ContinuousFileMonitoringFunction}}  to plays the role of SplitEnumerator, so this problem won't occur for new FileSource.

 

-- does it also occur with other operators whose parallelism must be 1 ?

From what I understand, if the max parallelism is not set to 1, it's possible that this problem may occur for these operators whose parallelism must be 1.

 

-- why fix it if it's only occurring in a legacy component ?

The legacy source are still widely used, for example, When reading files with datastream api, the first thought is to use readFile/readTextFile() methods, especially for starters. Since the fix work won't take much effort, may be it's worthy.;;;","05/Jul/22 07:41;martijnvisser;-- The legacy source are still widely used, for example, When reading files with datastream api, the first thought is to use readFile/readTextFile() methods, especially for starters. Since the fix work won't take much effort, may be it's worthy.

I don't think that the usage is a good reason, because that's always the case when something new is introduced. Without an incentive for users to migrate, they will never do so. It probably makes sense to fix it in this case, but I still think that we are doing something wrong as a Flink community since people shouldn't use {Internal} interfaces at all. ;;;","05/Jul/22 07:48;Leo Zhou;`Without an incentive for users to migrate, they will never do so. `  cannot agree more :);;;","06/Jul/22 06:10;rmetzger;Merged to master in https://github.com/apache/flink/commit/c71762686b1ef9c8fbc1cdb58bc0c9c2dbaa2da9;;;","06/Jul/22 06:12;rmetzger;Thanks [~Leo Zhou] for the fix!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle TLS Certificate Renewal in Webhook,FLINK-28272,13468784,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,morhidi,,28/Jun/22 05:22,24/Nov/22 01:03,13/Jul/23 08:13,12/Sep/22 09:42,kubernetes-operator-1.0.0,,,,,,,kubernetes-operator-1.2.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"We found that flink-kubernetes-operator v1.0.0 does not reload new certificate when updated by cert-manager, and it causes the following error when updating FlinkDeployment


{{Failed sync attempt to 597d35a7434bede526f526852c33a65262765219: one or more objects failed to apply, reason: Internal error occurred: failed calling webhook ""flinkoperator.flink.apache.org"": Post ""}}
{{[https://flink-operator-webhook-service.flink-operator.svc:443/validate?timeout=10s|https://flink-operator-webhook-service.flink-operator.svc/validate?timeout=10s]}}
{{"": x509: certificate signed by unknown authority (possibly because of ""x509: invalid signature: parent certificate cannot sign this kind of certificate"" while trying to verify candidate authority certificate ""FlinkDeployment Validator"") (retried 3 times).}}",,mbalassi,wu3396,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 12 09:42:17 UTC 2022,,,,,,,,,,"0|z16esw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 05:25;morhidi;We can do something similar:

https://goldius.medium.com/netty-reloading-ssl-tls-certificate-2078cdf6bfc1;;;","17/Aug/22 06:37;wu3396;Hello,
I've just hit the same issue , my env is:
kubernetes-operator:1.1.0
 
flink-webhook logs
{code:java}
{ ""@timestamp"": ""2022-08-17T05:58:52.578Z"", 
   ""ecs.version"": ""1.2.0"", 
   ""log.level"": ""WARN"", 
   ""message"": ""An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception."", 
   ""process.thread.name"": ""nioEventLoopGroup-3-2"",
   ""log.logger"": ""org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline"", 
   ""error.type"": ""org.apache.flink.shaded.netty4.io.netty.handler.codec.DecoderException"", 
   ""error.message"": ""javax.net.ssl.SSLHandshakeException: Received fatal alert: bad_certificate"",
   ""error.stack_trace"": ""org.apache.flink.shaded.netty4.io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: Received fatal alert: bad_certificate 
at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:477) at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Unknown Source) Caused by: javax.net.ssl.SSLHandshakeException: Received fatal alert: bad_certificate at java.base/sun.security.ssl.Alert.createSSLException(Unknown Source) at java.base/sun.security.ssl.Alert.createSSLException(Unknown Source) at java.base/sun.security.ssl.TransportContext.fatal(Unknown Source) at java.base/sun.security.ssl.Alert$AlertConsumer.consume(Unknown Source) at java.base/sun.security.ssl.TransportContext.dispatch(Unknown Source) at java.base/sun.security.ssl.SSLTransport.decode(Unknown Source) at java.base/sun.security.ssl.SSLEngineImpl.decode(Unknown Source) at java.base/sun.security.ssl.SSLEngineImpl.readRecord(Unknown Source) at java.base/sun.security.ssl.SSLEngineImpl.unwrap(Unknown Source) at java.base/sun.security.ssl.SSLEngineImpl.unwrap(Unknown Source) at java.base/javax.net.ssl.SSLEngine.unwrap(Unknown Source) at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler$SslEngineType$3.unwrap(SslHandler.java:296) at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1342) at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1235) at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1284) at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:507) at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:446) ... 17 more"" 
}{code}
 ;;;","12/Sep/22 09:42;mbalassi;[e5a325c|https://github.com/apache/flink-kubernetes-operator/commit/e5a325c48965a50d61d0aa29e61ba79e97f27082] in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SessionManagerTest.testIdleSessionCleanup failed with Session doesn't exist,FLINK-28270,13468776,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,hxbks2ks,hxbks2ks,28/Jun/22 03:24,30/Jun/22 12:10,13/Jul/23 08:13,30/Jun/22 12:10,1.16.0,,,,,,,,,,,,Table SQL / Gateway,,,,,,,0,test-stability,,,,"
{code:java}
2022-06-28T02:21:32.7045686Z Jun 28 02:21:32 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.367 s <<< FAILURE! - in org.apache.flink.table.gateway.service.session.SessionManagerTest
2022-06-28T02:21:32.7046900Z Jun 28 02:21:32 [ERROR] org.apache.flink.table.gateway.service.session.SessionManagerTest.testIdleSessionCleanup  Time elapsed: 2.328 s  <<< ERROR!
2022-06-28T02:21:32.7048127Z Jun 28 02:21:32 org.apache.flink.table.gateway.api.utils.SqlGatewayException: Session 'b15bff8b-33b8-4784-a53e-d3c900f64137' does not exist.
2022-06-28T02:21:32.7048951Z Jun 28 02:21:32 	at org.apache.flink.table.gateway.service.session.SessionManager.getSession(SessionManager.java:128)
2022-06-28T02:21:32.7049799Z Jun 28 02:21:32 	at org.apache.flink.table.gateway.service.session.SessionManagerTest.testIdleSessionCleanup(SessionManagerTest.java:80)
2022-06-28T02:21:32.7050524Z Jun 28 02:21:32 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-28T02:21:32.7051281Z Jun 28 02:21:32 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-28T02:21:32.7052008Z Jun 28 02:21:32 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-28T02:21:32.7052830Z Jun 28 02:21:32 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-06-28T02:21:32.7053710Z Jun 28 02:21:32 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-06-28T02:21:32.7054472Z Jun 28 02:21:32 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-06-28T02:21:32.7055316Z Jun 28 02:21:32 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-06-28T02:21:32.7056162Z Jun 28 02:21:32 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-06-28T02:21:32.7056948Z Jun 28 02:21:32 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-06-28T02:21:32.7057926Z Jun 28 02:21:32 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2022-06-28T02:21:32.7059622Z Jun 28 02:21:32 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-06-28T02:21:32.7060852Z Jun 28 02:21:32 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-06-28T02:21:32.7061721Z Jun 28 02:21:32 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-06-28T02:21:32.7062621Z Jun 28 02:21:32 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-06-28T02:21:32.7063480Z Jun 28 02:21:32 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-06-28T02:21:32.7064327Z Jun 28 02:21:32 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-06-28T02:21:32.7065312Z Jun 28 02:21:32 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-06-28T02:21:32.7066107Z Jun 28 02:21:32 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-06-28T02:21:32.7066925Z Jun 28 02:21:32 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-06-28T02:21:32.7067849Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T02:21:32.7068646Z Jun 28 02:21:32 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-06-28T02:21:32.7069491Z Jun 28 02:21:32 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-06-28T02:21:32.7070317Z Jun 28 02:21:32 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-06-28T02:21:32.7071342Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-06-28T02:21:32.7072450Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T02:21:32.7073289Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-28T02:21:32.7074056Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-28T02:21:32.7074809Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-28T02:21:32.7075646Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T02:21:32.7076663Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-28T02:21:32.7077614Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-28T02:21:32.7078568Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-28T02:21:32.7079717Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-06-28T02:21:32.7080884Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-06-28T02:21:32.7081851Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-06-28T02:21:32.7082666Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T02:21:32.7083639Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-28T02:21:32.7084768Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-28T02:21:32.7085594Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-28T02:21:32.7086429Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T02:21:32.7087311Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-28T02:21:32.7088100Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-28T02:21:32.7089056Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-28T02:21:32.7090142Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-06-28T02:21:32.7091175Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-06-28T02:21:32.7092010Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T02:21:32.7092848Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-28T02:21:32.7093615Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-28T02:21:32.7094391Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-28T02:21:32.7095218Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T02:21:32.7096026Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-28T02:21:32.7096788Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-28T02:21:32.7098317Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-28T02:21:32.7099619Z Jun 28 02:21:32 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-06-28T02:21:32.7100266Z Jun 28 02:21:32 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-06-28T02:21:32.7101011Z Jun 28 02:21:32 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-06-28T02:21:32.7101729Z Jun 28 02:21:32 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-06-28T02:21:32.7102408Z Jun 28 02:21:32 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-06-28T02:21:32.7102891Z Jun 28 02:21:32 
2022-06-28T02:21:33.1062704Z Jun 28 02:21:33 [INFO] 
2022-06-28T02:21:33.1063557Z Jun 28 02:21:33 [INFO] Results:
2022-06-28T02:21:33.1064328Z Jun 28 02:21:33 [INFO] 
2022-06-28T02:21:33.1064943Z Jun 28 02:21:33 [ERROR] Errors: 
2022-06-28T02:21:33.1066905Z Jun 28 02:21:33 [ERROR]   SessionManagerTest.testIdleSessionCleanup:80 » SqlGateway Session 'b15bff8b-33...
2022-06-28T02:21:33.1068294Z Jun 28 02:21:33 [INFO] 
2022-06-28T02:21:33.1069001Z Jun 28 02:21:33 [ERROR] Tests run: 10, Failures: 0, Errors: 1, Skipped: 0
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37264&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4

",,fsk119,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28290,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 30 12:10:04 UTC 2022,,,,,,,,,,"0|z16er4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 03:25;hxbks2ks;cc [~fsk119];;;","30/Jun/22 12:10;fsk119;This has a similar reason as FLINK-28290. The check thread may touch the session before the subsequent request, which causes the thread is expired.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes test failed with permission denied,FLINK-28269,13468774,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,hxbks2ks,hxbks2ks,28/Jun/22 03:20,17/Oct/22 15:53,13/Jul/23 08:13,29/Jun/22 16:55,1.15.0,1.16.0,,,,,,1.14.6,1.15.2,1.16.0,,,Deployment / Kubernetes,Tests,,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-06-28T00:57:41.2724102Z Jun 28 00:57:41 Preparing to unpack .../conntrack_1%3a1.4.5-2_amd64.deb ...
2022-06-28T00:57:41.2747620Z Jun 28 00:57:41 Unpacking conntrack (1:1.4.5-2) ...
2022-06-28T00:57:41.3287046Z Jun 28 00:57:41 Setting up conntrack (1:1.4.5-2) ...
2022-06-28T00:57:41.3316320Z Jun 28 00:57:41 Processing triggers for man-db (2.9.1-1) ...
2022-06-28T00:57:43.0246060Z Jun 28 00:57:43 fs.protected_regular = 0
2022-06-28T00:57:46.5039126Z Jun 28 00:57:46 * Profile ""minikube"" not found. Run ""minikube profile list"" to view all profiles.
2022-06-28T00:57:46.5047104Z Jun 28 00:57:46   To start a cluster, run: ""minikube start""
2022-06-28T00:57:46.5088285Z Jun 28 00:57:46 Starting minikube ...
2022-06-28T00:57:46.6801624Z Jun 28 00:57:46 * minikube v1.26.0 on Ubuntu 20.04
2022-06-28T00:57:46.7004754Z Jun 28 00:57:46 * Using the none driver based on user configuration
2022-06-28T00:57:46.7128484Z Jun 28 00:57:46 * Starting control plane node minikube in cluster minikube
2022-06-28T00:57:46.7178545Z Jun 28 00:57:46 * Running on localhost (CPUs=2, Memory=6946MB, Disk=85173MB) ...
2022-06-28T00:57:47.0620600Z Jun 28 00:57:47 * OS release is Ubuntu 20.04.4 LTS
2022-06-28T00:57:49.3027006Z Jun 28 00:57:49 
2022-06-28T00:57:49.3040058Z X Exiting due to RUNTIME_ENABLE: sudo systemctl enable cri-docker.socket: exit status 1
2022-06-28T00:57:49.3040497Z stdout:
2022-06-28T00:57:49.3040596Z 
2022-06-28T00:57:49.3040809Z stderr:
2022-06-28T00:57:49.3041270Z Failed to enable unit: Unit file cri-docker.socket does not exist.
2022-06-28T00:57:49.3041461Z 
2022-06-28T00:57:49.3041663Z * 
2022-06-28T00:57:49.3063195Z ╭─────────────────────────────────────────────────────────────────────────────────────────────╮
2022-06-28T00:57:49.3063744Z │                                                                                             │
2022-06-28T00:57:49.3064375Z │    * If the above advice does not help, please let us know:                                 │
2022-06-28T00:57:49.3064975Z │      https://github.com/kubernetes/minikube/issues/new/choose                               │
2022-06-28T00:57:49.3065484Z │                                                                                             │
2022-06-28T00:57:49.3066019Z │    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    │
2022-06-28T00:57:49.3066543Z │                                                                                             │
2022-06-28T00:57:49.3067048Z ╰─────────────────────────────────────────────────────────────────────────────────────────────╯
2022-06-28T00:57:49.3067360Z Jun 28 00:57:49 
2022-06-28T00:57:49.3922883Z E0628 00:57:49.392012  194614 root.go:88] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
2022-06-28T00:57:49.3948489Z Jun 28 00:57:49 
2022-06-28T00:57:49.3957501Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:57:49.3960093Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:57:49.3960924Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:57:49.3966499Z Jun 28 00:57:49 
2022-06-28T00:57:49.4693197Z Jun 28 00:57:49 
2022-06-28T00:57:49.4700763Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:57:49.4702406Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:57:49.4703168Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:57:49.4731042Z Jun 28 00:57:49 
2022-06-28T00:57:49.4733005Z Jun 28 00:57:49 Command: start_kubernetes_if_not_running failed. Retrying...
2022-06-28T00:57:54.5441686Z Jun 28 00:57:54 
2022-06-28T00:57:54.5443916Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:57:54.5445961Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:57:54.5446526Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:57:54.5450376Z Jun 28 00:57:54 
2022-06-28T00:57:54.5469377Z Jun 28 00:57:54 Starting minikube ...
2022-06-28T00:57:54.6254675Z Jun 28 00:57:54 * minikube v1.26.0 on Ubuntu 20.04
2022-06-28T00:57:54.6299271Z Jun 28 00:57:54 * Using the none driver based on existing profile
2022-06-28T00:57:54.6324036Z Jun 28 00:57:54 * Starting control plane node minikube in cluster minikube
2022-06-28T00:57:54.6867587Z Jun 28 00:57:54 * Restarting existing none bare metal machine for ""minikube"" ...
2022-06-28T00:57:54.6954113Z Jun 28 00:57:54 * OS release is Ubuntu 20.04.4 LTS
2022-06-28T00:57:55.7715259Z Jun 28 00:57:55 
2022-06-28T00:57:55.7730526Z X Exiting due to RUNTIME_ENABLE: sudo systemctl enable cri-docker.socket: exit status 1
2022-06-28T00:57:55.7731022Z stdout:
2022-06-28T00:57:55.7731134Z 
2022-06-28T00:57:55.7731336Z stderr:
2022-06-28T00:57:55.7731822Z Failed to enable unit: Unit file cri-docker.socket does not exist.
2022-06-28T00:57:55.7732019Z 
2022-06-28T00:57:55.7732211Z * 
2022-06-28T00:57:55.7732693Z ╭─────────────────────────────────────────────────────────────────────────────────────────────╮
2022-06-28T00:57:55.7733157Z │                                                                                             │
2022-06-28T00:57:55.7733672Z │    * If the above advice does not help, please let us know:                                 │
2022-06-28T00:57:55.7734360Z │      https://github.com/kubernetes/minikube/issues/new/choose                               │
2022-06-28T00:57:55.7734861Z │                                                                                             │
2022-06-28T00:57:55.7735418Z │    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    │
2022-06-28T00:57:55.7735959Z │                                                                                             │
2022-06-28T00:57:55.7736589Z ╰─────────────────────────────────────────────────────────────────────────────────────────────╯
2022-06-28T00:57:55.7740218Z Jun 28 00:57:55 
2022-06-28T00:57:55.8458528Z E0628 00:57:55.845551  194918 root.go:88] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
2022-06-28T00:57:55.8471612Z Jun 28 00:57:55 
2022-06-28T00:57:55.8477089Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:57:55.8479071Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:57:55.8480223Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:57:55.8482785Z Jun 28 00:57:55 
2022-06-28T00:57:55.9175143Z Jun 28 00:57:55 
2022-06-28T00:57:55.9189731Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:57:55.9192377Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:57:55.9194955Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:57:55.9207163Z Jun 28 00:57:55 
2022-06-28T00:57:55.9233298Z Jun 28 00:57:55 Command: start_kubernetes_if_not_running failed. Retrying...
2022-06-28T00:58:00.9957446Z Jun 28 00:58:00 
2022-06-28T00:58:00.9963674Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:58:00.9964966Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:58:00.9965541Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:58:00.9973430Z Jun 28 00:58:00 
2022-06-28T00:58:01.0002581Z Jun 28 00:58:00 Starting minikube ...
2022-06-28T00:58:01.0729479Z Jun 28 00:58:01 * minikube v1.26.0 on Ubuntu 20.04
2022-06-28T00:58:01.0790218Z Jun 28 00:58:01 * Using the none driver based on existing profile
2022-06-28T00:58:01.0808505Z Jun 28 00:58:01 * Starting control plane node minikube in cluster minikube
2022-06-28T00:58:01.1067402Z Jun 28 00:58:01 * Restarting existing none bare metal machine for ""minikube"" ...
2022-06-28T00:58:01.1177185Z Jun 28 00:58:01 * OS release is Ubuntu 20.04.4 LTS
2022-06-28T00:58:02.2118290Z Jun 28 00:58:02 
2022-06-28T00:58:02.2134209Z X Exiting due to RUNTIME_ENABLE: sudo systemctl enable cri-docker.socket: exit status 1
2022-06-28T00:58:02.2136970Z stdout:
2022-06-28T00:58:02.2137355Z 
2022-06-28T00:58:02.2137725Z stderr:
2022-06-28T00:58:02.2138686Z Failed to enable unit: Unit file cri-docker.socket does not exist.
2022-06-28T00:58:02.2139016Z 
2022-06-28T00:58:02.2139342Z * 
2022-06-28T00:58:02.2139953Z ╭─────────────────────────────────────────────────────────────────────────────────────────────╮
2022-06-28T00:58:02.2140560Z │                                                                                             │
2022-06-28T00:58:02.2141218Z │    * If the above advice does not help, please let us know:                                 │
2022-06-28T00:58:02.2141916Z │      https://github.com/kubernetes/minikube/issues/new/choose                               │
2022-06-28T00:58:02.2142571Z │                                                                                             │
2022-06-28T00:58:02.2143272Z │    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    │
2022-06-28T00:58:02.2143943Z │                                                                                             │
2022-06-28T00:58:02.2144601Z ╰─────────────────────────────────────────────────────────────────────────────────────────────╯
2022-06-28T00:58:02.2145064Z Jun 28 00:58:02 
2022-06-28T00:58:02.2880448Z E0628 00:58:02.287766  195222 root.go:88] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
2022-06-28T00:58:02.2892504Z Jun 28 00:58:02 
2022-06-28T00:58:02.2896830Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:58:02.2899275Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:58:02.2900544Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:58:02.2901362Z Jun 28 00:58:02 
2022-06-28T00:58:02.3582770Z Jun 28 00:58:02 
2022-06-28T00:58:02.3587835Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:58:02.3589055Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:58:02.3589630Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:58:02.3599452Z Jun 28 00:58:02 
2022-06-28T00:58:02.3621002Z Jun 28 00:58:02 Command: start_kubernetes_if_not_running failed. Retrying...
2022-06-28T00:58:07.3638880Z Jun 28 00:58:07 Command: start_kubernetes_if_not_running failed 3 times.
2022-06-28T00:58:07.3639385Z Jun 28 00:58:07 Could not start minikube. Aborting...
2022-06-28T00:58:07.3639759Z Jun 28 00:58:07 Debugging failed Kubernetes test:
2022-06-28T00:58:07.3640148Z Jun 28 00:58:07 Currently existing Kubernetes resources
2022-06-28T00:58:07.5490786Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2022-06-28T00:58:07.5922448Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2022-06-28T00:58:07.5945237Z Jun 28 00:58:07 Flink logs:
2022-06-28T00:58:07.6330601Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2022-06-28T00:58:07.6707241Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2022-06-28T00:58:07.7097992Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2022-06-28T00:58:07.7506609Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2022-06-28T00:58:07.7886315Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2022-06-28T00:58:07.7896794Z Jun 28 00:58:07 Stopping minikube ...
2022-06-28T00:58:07.8560167Z E0628 00:58:07.855722  195270 root.go:88] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
2022-06-28T00:58:07.8564021Z E0628 00:58:07.856252  195270 cloud_events.go:60] unable to write to /home/vsts/.minikube/profiles/minikube/events.json: open /home/vsts/.minikube/profiles/minikube/events.json: permission denied
2022-06-28T00:58:07.8583780Z Jun 28 00:58:07 
2022-06-28T00:58:07.8595235Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:58:07.8596811Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:58:07.8597431Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:58:07.8605881Z Jun 28 00:58:07 
2022-06-28T00:58:07.8632037Z Jun 28 00:58:07 Command: minikube stop failed. Retrying...
2022-06-28T00:58:12.9320992Z E0628 00:58:12.931814  195281 root.go:88] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
2022-06-28T00:58:12.9323097Z E0628 00:58:12.932141  195281 cloud_events.go:60] unable to write to /home/vsts/.minikube/profiles/minikube/events.json: open /home/vsts/.minikube/profiles/minikube/events.json: permission denied
2022-06-28T00:58:12.9346464Z Jun 28 00:58:12 
2022-06-28T00:58:12.9356598Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:58:12.9359608Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:58:12.9361856Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:58:12.9370436Z Jun 28 00:58:12 
2022-06-28T00:58:12.9401892Z Jun 28 00:58:12 Command: minikube stop failed. Retrying...
2022-06-28T00:58:18.0093241Z E0628 00:58:18.009042  195290 root.go:88] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
2022-06-28T00:58:18.0094620Z E0628 00:58:18.009278  195290 cloud_events.go:60] unable to write to /home/vsts/.minikube/profiles/minikube/events.json: open /home/vsts/.minikube/profiles/minikube/events.json: permission denied
2022-06-28T00:58:18.0105801Z Jun 28 00:58:18 
2022-06-28T00:58:18.0109189Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:58:18.0112493Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:58:18.0114979Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:58:18.0119419Z Jun 28 00:58:18 
2022-06-28T00:58:18.0139916Z Jun 28 00:58:18 Command: minikube stop failed. Retrying...
2022-06-28T00:58:23.0155447Z Jun 28 00:58:23 Command: minikube stop failed 3 times.
2022-06-28T00:58:23.0156414Z Jun 28 00:58:23 Could not stop minikube. Aborting...
2022-06-28T00:58:23.0159072Z Jun 28 00:58:23 [FAIL] Test script contains errors.
2022-06-28T00:58:23.0166657Z Jun 28 00:58:23 Checking for errors...
2022-06-28T00:58:23.0402412Z Jun 28 00:58:23 No errors in log files.
2022-06-28T00:58:23.0404271Z Jun 28 00:58:23 Checking for exceptions...
2022-06-28T00:58:23.0675070Z Jun 28 00:58:23 No exceptions in log files.
2022-06-28T00:58:23.0677372Z Jun 28 00:58:23 Checking for non-empty .out files...
2022-06-28T00:58:23.0700681Z grep: /home/vsts/work/_temp/debug_files/flink-logs/*.out: No such file or directory
2022-06-28T00:58:23.0706900Z Jun 28 00:58:23 No non-empty .out files.
2022-06-28T00:58:23.0707516Z Jun 28 00:58:23 
2022-06-28T00:58:23.0708309Z Jun 28 00:58:23 [FAIL] 'Run Kubernetes test' failed after 0 minutes and 44 seconds! Test exited with exit code 1
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37264&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4975",,hxbks2ks,martijnvisser,rmetzger,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29671,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 29 16:55:40 UTC 2022,,,,,,,,,,"0|z16eqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 03:26;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37264&view=logs&j=af885ea8-6b05-5dc2-4a37-eab9c0d1ab09&t=f779a55a-0ffe-5bbc-8824-3a79333d4559;;;","28/Jun/22 09:03;martijnvisser;A quick Google search turned me to https://github.com/kubernetes/minikube/issues/14403 - My assumption is that Azure has had an update which is now causing this error;;;","28/Jun/22 17:38;martijnvisser;[~yangwang166] [~rmetzger] I've been doing some investigation on this but I do have one question: Do you know if it was it a deliberate decision to use the none driver over the other existing ones?;;;","29/Jun/22 02:39;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37302&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14;;;","29/Jun/22 02:43;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37306&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14;;;","29/Jun/22 02:44;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37313&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14;;;","29/Jun/22 02:46;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37327&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c;;;","29/Jun/22 02:47;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37328&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14;;;","29/Jun/22 02:47;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37329&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14;;;","29/Jun/22 06:08;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37332&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14;;;","29/Jun/22 06:26;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37334&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14;;;","29/Jun/22 07:31;rmetzger;[~martijnvisser] afaik [~yangwang166] introduced this code originally.
I assume the ""none"" driver was used for performance / efficiency reasons. Most of the disadvantages listed https://minikube.sigs.k8s.io/docs/drivers/none/ don't apply to CI environments.;;;","29/Jun/22 09:13;martijnvisser;Thanks for the info [~rmetzger];;;","29/Jun/22 16:55;martijnvisser;Fixed in
master: aa26f46bf3908d7db58e3a822246b54f2bd9eece
release-1.15: 82953560041636f397bc30d47d2312a00349b2b5
release-1.14: d101a8a755db611213eff77db9ea385cae636387;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistency in Kubernetes HA service: broken state handle,FLINK-28265,13468587,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangyang0918,rmetzger,rmetzger,27/Jun/22 12:29,27/Aug/22 06:58,13/Jul/23 08:13,27/Aug/22 06:56,1.14.4,,,,,,,1.15.3,1.16.0,,,,Deployment / Kubernetes,Runtime / Coordination,,,,,,0,pull-request-available,,,,"I have a JobManager, which at some point failed to acknowledge a checkpoint:

{code}
Error while processing AcknowledgeCheckpoint message
org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete the pending checkpoint 193393. Failure reason: Failure to finalize checkpoint.
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1255)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:1100)
	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$acknowledgeCheckpoint$1(ExecutionGraphHandler.java:89)
	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.apache.flink.runtime.persistence.StateHandleStore$AlreadyExistException: checkpointID-0000000000000193393 already exists in ConfigMap cm-00000000000000000000000000000000-jobmanager-leader
	at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStore.getKeyAlreadyExistException(KubernetesStateHandleStore.java:534)
	at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStore.lambda$addAndLock$0(KubernetesStateHandleStore.java:155)
	at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$attemptCheckAndUpdateConfigMap$11(Fabric8FlinkKubeClient.java:316)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source)
	... 3 common frames omitted
{code}

the JobManager creates subsequent checkpoints successfully.
Upon failure, it tries to recover this checkpoint (0000000000000193393), but fails to do so because of:
{code}
Caused by: org.apache.flink.util.FlinkException: Could not retrieve checkpoint 193393 from state handle under checkpointID-0000000000000193393. This indicates that the retrieved state handle is broken. Try cleaning the state handle store ... Caused by: java.io.FileNotFoundException: No such file or directory: s3://xxx/flink-ha/xxx/completedCheckpoint72e30229420c
{code}

I'm running Flink 1.14.4.

Note: This issue has been first discussed here: https://github.com/apache/flink/pull/15832#pullrequestreview-1005973050 ",,aitozi,cymau,EnriqueL8,gyfora,mapohl,martijnvisser,qinjunjerry,rmetzger,tanyuxin,thw,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25098,FLINK-29105,,,,,"30/Jun/22 11:03;cymau;flink_checkpoint_issue.txt;https://issues.apache.org/jira/secure/attachment/13046087/flink_checkpoint_issue.txt",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 27 06:56:34 UTC 2022,,,,,,,,,,"0|z16dzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/22 12:31;rmetzger;[~mapohl] I'm trying to get the full JobManager logs ... but they might have been gone by now, due to the retention of our log management system.

UPDATE: the log is indeed gone. I will properly collect the logs next time this happens.;;;","27/Jun/22 12:32;rmetzger;We don't have a reliable way of reproducing this issue yet.
But another user reported something similar in: https://github.com/apache/flink/pull/15832#issuecomment-1167167537;;;","29/Jun/22 07:26;wangyang0918;Thanks [~rmetzger] for sharing this issue.

 

I am curious why Flink still recovers from the checkpoint (0000000000000193393) if JobManager creates subsequent checkpoints successfully. It should pick the latest the successful checkpoint.;;;","29/Jun/22 07:36;rmetzger;Yes, I was wondering exactly the same. I've setup some internal monitoring to catch if this situation occurs again. I'll then share the logs.;;;","30/Jun/22 11:03;cymau;[^flink_checkpoint_issue.txt];;;","30/Jun/22 11:14;martijnvisser;[~cymau] That log doesn't relates to this issue, since this issue is strictly about the Kubernetes tests that we are running. Your log are Flink logs;;;","01/Jul/22 02:48;cymau;[~martijnvisser] We had another incident where we hit this issue and uploaded the full JobManager log: 

Caused by: java.util.concurrent.CompletionException: java.lang.RuntimeException: org.apache.flink.util.FlinkException: Could not retrieve checkpoint 9702 from state handle under checkpointID-0000000000000009702. This indicates that the retrieved state handle is broken. Try cleaning the state handle store.

Please ignore if not related to this issue.;;;","01/Jul/22 07:17;martijnvisser;[~cymau] Like I said, this can't be related to this ticket. This ticket is about a test instability on the Flink source code. This test isn't included in the actual released Flink version. If you have a problem, I recommend to post this to the User mailing list, Stackoverflow or Slack per https://flink.apache.org/gettinghelp.html;;;","11/Jul/22 10:18;EnriqueL8;Hi [~martijnvisser] , 

I understand that the original ticket was created due to a test instability but the test is highlighting the fact that there is a problem in the flow that results in a broken state. What [~cymau] is saying and that I original flagged is that we are seeing the same type of behaviour in the field. I'm happy to raise a separate ticket to track the same problem occurring in the field but it's the exact same behaviour. All the test is doing which is great is highlighting that there is a problem.

Thanks,
Enrique;;;","11/Jul/22 11:12;martijnvisser;[~EnriqueL8] It feels like my comment is misplaced on this Jira ticket, because I believe I made that comment when I was talking about FLINK-28269. I've made a mistake or some comments were removed. 

Regarding the provided logs, are there also logs available of the previous runs? Are there also taskmanager logs available? CC [~cymau]

[~wangyang0918] [~mapohl] What are your thoughts on this? ;;;","13/Jul/22 02:44;wangyang0918;Since we always add the new checkpoint first and then subsume the oldest one, I am curious how it could happen we only have one checkpoint which is invalid. If adding the new checkpoint failed, we should have the old successful checkpoint. On the contrary, if subsuming the oldest one failed, we should still have the newly added checkpoint.

 

Could you please verify the checkpoint 9701 or 9703 exists on the S3?

 

I believe the the logs of previous run(e.g. kubectl logs <pod-name> --previous) and the Kubernetes APIServer audit log will help a lot to debug the root cause.;;;","13/Jul/22 10:27;mapohl;I looked into the {{EOFException}} generation for [~cymau] case. Based on the stacktrace, we can assume that the file was empty:
{code}
Caused by: java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(Unknown Source) ~[?:?]
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(Unknown Source) ~[?:?]
	at java.io.ObjectInputStream.readStreamHeader(Unknown Source) ~[?:?]
	at java.io.ObjectInputStream.<init>(Unknown Source) ~[?:?]
	at org.apache.flink.util.InstantiationUtil$ClassLoaderObjectInputStream.<init>(InstantiationUtil.java:66) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:613) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:593) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
[...]
{code}
I'm wondering whether there is some external process that ran that cleaned up files? Because usually, we only delete files but don't change the content. Another issue I could think of is that some network issue (since we're accessing some (maybe distributed) FileSystem) resulted in the InputStream for the deserialization not to contain any data. But I would have thought that this error case would result in a different {{IOException}}.

In contrast, the other appearances were caused by the RetrievableStateHandle file being entirely deleted (i.e. discarded) which is a behavior I would rather expect if we have a bug in Flink.;;;","25/Jul/22 02:24;wangyang0918;I want to share some progress about this ticket. The root cause might be we should not discard the state when coming across {{AlreadyExistException}} in {{{}KubernetesStateHandleStore#addAndLock{}}}.

If something is temporarily wrong with the JobManager network, {{Fabric8FlinkKubeClient#checkAndUpdateConfigMap}} failed with {{KubernetesException}} in the first run and retried again. However, the http request is actually sent successfully and handled by the K8s APIServer, which means the entry was added to the ConfigMap. This will cause the second retry fails with {{AlreadyExistException}} and then discard the state. If the JobManager crashed exactly, it will throw the {{FileNotFoundException: No such file or directory: s3://xxx/flink-ha/xxx/completedCheckpoint72e30229420c}} in the following attempts since added entry is not cleaned up.

 

 ;;;","16/Aug/22 08:32;wangyang0918;I will work on this ticket.;;;","27/Aug/22 06:56;wangyang0918;Fixed via:

master(1.16): aae96d0c9d1768c396bdf2ee6510677fbb8f317a

release-1.15: fcff4903c8d625edb8f4e33b03bfded52c3deba8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
exactly-once sink kafka cause out of memory,FLINK-28250,13468405,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,charles-tan,jinshuangxian,jinshuangxian,25/Jun/22 14:10,01/Oct/22 06:16,13/Jul/23 08:13,18/Jul/22 08:44,1.15.0,,,,,,,1.15.2,1.16.0,,,,Connectors / Kafka,,,,,,,0,pull-request-available,,,,"*my sql code:*

CREATE TABLE sourceTable (

data bytes

)WITH(

'connector'='kafka',

'topic'='topic1',

'properties.bootstrap.servers'='host1',

'properties.group.id'='gorup1',

'scan.startup.mode'='latest-offset',

'format'='raw'

);

 

CREATE TABLE sinkTable (

data bytes

)

WITH (

'connector'='kafka',

'topic'='topic2',

'properties.bootstrap.servers'='host2',

'properties.transaction.timeout.ms'='30000',

'sink.semantic'='exactly-once',

'sink.transactional-id-prefix'='xx-kafka-sink-a',

'format'='raw'

);

insert into sinkTable

select data

from sourceTable;

 

*problem:*

After the program runs online for about half an hour, full gc frequently appears

 

{*}Troubleshoot{*}：

I use command 'jmap -dump:live,format=b,file=/tmp/dump2.hprof' dump the problem tm memory. It is found that there are 115 FlinkKafkaInternalProducers, which is not normal.

!image-2022-06-25-22-07-54-649.png!!image-2022-06-25-22-07-35-686.png!

After reading the code of KafkaCommitter, it is found that after the commit is successful, the producer is not recycled, only abnormal situations are recycled.

!image-2022-06-25-22-08-04-891.png!

I added a few lines of code. After the online test, the program works normally, and the problem of oom memory is solved.

!image-2022-06-25-22-08-15-024.png!","*flink version: flink-1.15.0*

*tm: 8* parallelism, 1 slot, 2g

centos7",charles-tan,JasonLee,jinshuangxian,martijnvisser,mason6345,renqs,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28962,,,,,,,,,,,,,,"25/Jun/22 14:07;jinshuangxian;image-2022-06-25-22-07-35-686.png;https://issues.apache.org/jira/secure/attachment/13045612/image-2022-06-25-22-07-35-686.png","25/Jun/22 14:08;jinshuangxian;image-2022-06-25-22-07-54-649.png;https://issues.apache.org/jira/secure/attachment/13045611/image-2022-06-25-22-07-54-649.png","25/Jun/22 14:08;jinshuangxian;image-2022-06-25-22-08-04-891.png;https://issues.apache.org/jira/secure/attachment/13045610/image-2022-06-25-22-08-04-891.png","25/Jun/22 14:08;jinshuangxian;image-2022-06-25-22-08-15-024.png;https://issues.apache.org/jira/secure/attachment/13045609/image-2022-06-25-22-08-15-024.png",,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 01 06:16:44 UTC 2022,,,,,,,,,,"0|z16cv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/22 10:10;martijnvisser;[~renqs] Any thoughts?;;;","05/Jul/22 03:12;renqs;Thanks for the detailed investigation [~jinshuangxian] ! I think this explains the OOM issue. Would you like to create a patch? ;;;","08/Jul/22 16:37;charles-tan;Hi all, I've gone ahead and opened a PR with [~jinshuangxian]'s suggested patch, as I'm running into the same issues with my application ([mailing list thread|https://lists.apache.org/thread/c86cd8qyqb6qxy639hkzbozkwv2qxk84]). Would appreciate feedback on the [PR|https://github.com/apache/flink/pull/20205], thanks!;;;","11/Jul/22 02:01;renqs;Thanks for the contribution [~charles-tan]! I've assigned the ticket to you. ;;;","18/Jul/22 08:45;renqs;Merged on master: 74f90d722f7be5db5298b84626935a585391f0df
release-1.15: adbf09fb941c8f793df6d322ed95df87bc4254f3;;;","01/Oct/22 06:16;rmetzger;Thanks a lot for the fix.
I've opened a related ticket here: https://issues.apache.org/jira/browse/FLINK-29492;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Massive regression on 20.06.2021,FLINK-28241,13466637,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,akalashnikov,akalashnikov,24/Jun/22 08:14,27/Jun/22 06:17,13/Jul/23 08:13,27/Jun/22 06:17,,,,,,,,,,,,,Benchmarks,,,,,,,0,pull-request-available,,,,"In FLINK-27921 added property `REQUIREMENTS_CHECK_DELAY` that optimizes the number of checks of resources but it also leads to a little delay during the init phase which is totally not critical for the normal scenario. But unfortunately, our benchmarks can not exclude the time of the initial phase(or shutdown phase) from the total time of benchmarks. As result, all benchmarks, that have a relatively short execution time(compare to the initial phase time), have regression(up to 50%). As a quick solution, it makes sense just to set `REQUIREMENTS_CHECK_DELAY` to zero for all benchmarks.
Not full list of affected benchmarks:
 * asyncWait
 * globalWindow
 * mapSink
 * readFileSplit
 * remoteRebalance
 * serializer*
 * sorted*
 * twoInputOneIdleMapSink",,akalashnikov,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27921,,,,,FLINK-28243,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 27 06:17:17 UTC 2022,,,,,,,,,,"0|z161y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/22 06:17;roman;Fix merged as 5d2f5a96b4bcb6f2e96db0a16a7ccea6280c91e2
(disable the new feature in benchmarks).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NettyShuffleMetricFactory#RequestedMemoryUsageMetric#getValue may throw ArithmeticException when the total segments of NetworkBufferPool is 0,FLINK-28240,13466581,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pltbkd,pltbkd,pltbkd,24/Jun/22 07:59,05/Jul/22 07:12,13/Jul/23 08:13,05/Jul/22 07:12,1.15.0,,,,,,,1.15.2,1.16.0,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,"In a single vertex job, the network memory can be set to 0 since the job doesn't need it, and in this case the totalNumberOfMemorySegments of the NetworkBufferPool will also be 0.

While the NettyShuffleMetricFactory#RequestedMemoryUsageMetric#getValue uses the totalNumberOfMemorySegments of NetworkBufferPool as divisor without validating, so an ArithmeticException will be thrown when the totalNumberOfMemorySegments is 0.

Since 0 network memory is in fact valid for a single vertex job, I suppose the RequestedMemoryUsageMetric#getValue should check if the devisor is 0, and return 0 as the usage directly in such cases.",,gaoyunhaii,pltbkd,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 05 07:12:13 UTC 2022,,,,,,,,,,"0|z161ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 07:12;gaoyunhaii;Merged on master via c6b3a8aa25607fb62d51b92f11f72778e5d618b2

Merged on 1.15 via 95f4b6fc7fd4a713d8d6348d32a121498ffce1e2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table-Planner-Loader lacks access to commons-math3,FLINK-28239,13466580,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,chesnay,chesnay,24/Jun/22 07:54,24/Jun/22 13:48,13/Jul/23 08:13,24/Jun/22 13:47,1.15.0,,,,,,,1.15.2,1.16.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"{{FlinkRelOptUtil}} requires {{commons-math3}}, but this dependency is neither bundled by {{table-planner}} nor in the owner classpath of the {{PlannerModule}}.

{code}
{""errors"":[""Internal server error."",""<Exception on server side:\norg.apache.flink.client.program.ProgramInvocationException: The program caused an error: \n\nClasspath: [file:/tmp/flink-web-8bc46ccd-f545-474c-8605-d084950afed1/flink-web-upload/38118da6-41f1-4e0b-9bb1-c69ee9662f3a_data-streamverse-core-1.0-SNAPSHOT-jar-with-dependencies.jar]\n\nSystem.out: (none)\n\nSystem.err: (none)
at org.apache.flink.client.program.PackagedProgramUtils.generateException(PackagedProgramUtils.java:264)
at org.apache.flink.client.program.PackagedProgramUtils.getPipelineFromProgram(PackagedProgramUtils.java:172)
at org.apache.flink.client.program.PackagedProgramUtils.createJobGraph(PackagedProgramUtils.java:82)
at org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils$JarHandlerContext.toJobGraph(JarHandlerUtils.java:159)
at org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler.lambda$handleRequest$1(JarPlanHandler.java:107)
at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
at java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NoClassDefFoundError: org/apache/commons/math3/util/ArithmeticUtils
at org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil$.mergeMiniBatchInterval(FlinkRelOptUtil.scala:439)
at org.apache.flink.table.planner.plan.rules.physical.stream.MiniBatchIntervalInferRule.onMatch(MiniBatchIntervalInferRule.scala:81)
at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)
at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.$anonfun$optimize$2(FlinkGroupProgram.scala:63)
at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
at scala.collection.Iterator.foreach(Iterator.scala:937)
at scala.collection.Iterator.foreach$(Iterator.scala:937)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 24 13:47:11 UTC 2022,,,,,,,,,,"0|z161lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/22 13:47;chesnay;master: ec1adccac3ff63a9b13dab8fad8e23b3746b9dca
1.15: 5b60be4791ac7442ceab011256609d4544eaf240 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix unstable testCancelOperationAndFetchResultInParallel,FLINK-28238,13465112,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,24/Jun/22 04:32,04/Jul/22 02:14,13/Jul/23 08:13,04/Jul/22 02:14,1.16.0,,,,,,,1.16.0,,,,,Table SQL / Gateway,,,,,,,0,pull-request-available,,,,"The failed test in https://dev.azure.com/martijn0323/Flink/_build/results?buildId=2711&view=logs&j=43a[…]cc-244368da36b4&t=82d122c0-8bbf-56f3-4c0d-8e3d69630d0f&l=11611

It's possible the fetcher fetches the results from the closed operation.
",,fsk119,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28293,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 04 02:14:49 UTC 2022,,,,,,,,,,"0|z15sjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/22 12:00;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37410&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","04/Jul/22 02:14;fsk119;Fix in master: 7974e81ec51a2071b9658f768f651ffc371b15b0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Successful observe doesn't clear errors due to patching,FLINK-28233,13463009,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,23/Jun/22 15:35,23/Jun/22 17:57,13/Jul/23 08:13,23/Jun/22 17:57,kubernetes-operator-1.0.0,kubernetes-operator-1.1.0,,,,,,kubernetes-operator-1.1.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,We set the error to `null` when trying to clear it but due to the patching mechanism this does nothing. It should be set to an empty string instead.,,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 23 17:57:44 UTC 2022,,,,,,,,,,"0|z15fk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/22 17:57;gyfora;merged to main 5e03a618b22ac8d77fe3e7886fd20be8ab518f75;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Target generation logic might skip over specs when observing already upgraded clusters,FLINK-28228,13462966,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,gyfora,gyfora,23/Jun/22 11:42,01/Jul/22 09:20,13/Jul/23 08:13,01/Jul/22 09:20,kubernetes-operator-1.1.0,,,,,,,kubernetes-operator-1.1.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"There are currently 2 problems with detecting already upgraded clusters in the AbstractDeploymentObserver:

1. Detecting the initial deployments is not reliable, because the user sends in an upgrade before the observer runs the logic will fail because we have a new higher generation.

2. When an upgrade was detected we should not simply use ReconciliationUtils.updateForSpecReconciliationSuccess because this will mark the current spec as reconciled which is not necessarily true",,aitozi,gyfora,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 01 09:20:19 UTC 2022,,,,,,,,,,"0|z15fag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 09:20;gyfora;merged to main 92979998a9342b225b24e04aeb7ed1daebcf730e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'Run kubernetes pyflink application test' fails while pulling image,FLINK-28226,13462753,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,martijnvisser,martijnvisser,23/Jun/22 10:53,05/Jul/22 14:08,13/Jul/23 08:13,27/Jun/22 02:37,1.14.6,,,,,,,1.14.6,1.15.2,1.16.0,,,API / Python,Deployment / Kubernetes,,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37103&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=6592

{code:java}
Jun 23 10:40:35 Flink logs:
Error from server (BadRequest): container ""flink-main-container"" in pod ""flink-native-k8s-pyflink-application-1-5d87889db9-rm8mm"" is waiting to start: image can't be pulled
Jun 23 10:40:35 deployment.apps ""flink-native-k8s-pyflink-application-1"" deleted
Jun 23 10:40:35 clusterrolebinding.rbac.authorization.k8s.io ""flink-role-binding-default"" deleted
Jun 23 10:40:36 pod/flink-native-k8s-pyflink-application-1-5d87889db9-rm8mm condition met
Jun 23 10:40:36 Stopping minikube ...
Jun 23 10:40:36 * Stopping node ""minikube""  ...
Jun 23 10:40:46 * 1 node stopped.
Jun 23 10:40:46 [FAIL] Test script contains errors.
Jun 23 10:40:46 Checking for errors...
Jun 23 10:40:46 No errors in log files.
Jun 23 10:40:46 Checking for exceptions...
Jun 23 10:40:46 No exceptions in log files.
Jun 23 10:40:46 Checking for non-empty .out files...
grep: /home/vsts/work/_temp/debug_files/flink-logs/*.out: No such file or directory
Jun 23 10:40:46 No non-empty .out files.
Jun 23 10:40:46 

{code}",,dianfu,hxbks2ks,leonard,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 27 02:37:37 UTC 2022,,,,,,,,,,"0|z15dz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/22 10:54;martijnvisser;CC [~hxbks2ks]  [~dianfu] ;;;","27/Jun/22 02:37;hxbks2ks;Merged into master via e52484a6b55147d8d4aa32bb51dc46aadb3acdf0
Merged into release-1.15 via af0a415024881851fd4919cb2efd87edaab7c121
Merged into release-1.14 via a473be67375578ebbd330c98dbdd330dbd34dd66;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Savepoint may corrupt file metas by repeat commit,FLINK-28221,13462031,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,lzljs3620320,lzljs3620320,23/Jun/22 09:38,06/Jul/22 09:13,13/Jul/23 08:13,06/Jul/22 09:13,,,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"[https://github.com/apache/flink-table-store/runs/7020439369?check_suite_focus=true]
Error:  Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 46.953 s <<< FAILURE! - in org.apache.flink.table.store.connector.RescaleBucketITCase 
[32285|https://github.com/apache/flink-table-store/runs/7020439369?check_suite_focus=true#step:4:32286]Error:  testSuspendAndRecoverAfterRescaleOverwrite Time elapsed: 25.545 s <<< ERROR!
{code:java}
Caused by: java.lang.IllegalStateException: Trying to add file {org.apache.flink.table.data.binary.BinaryRowData@9c67b85d, 0, 0, data-4756dfaf-e14e-440e-b211-df2b25f2537a-0.orc} which is already added. Manifest might be corrupted.
32416	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:215)
32417	at org.apache.flink.table.store.file.operation.AbstractFileStoreScan.plan(AbstractFileStoreScan.java:189)
32418	at org.apache.flink.table.store.table.source.TableScan.plan(TableScan.java:99)
32419	at org.apache.flink.table.store.connector.source.FileStoreSource.restoreEnumerator(FileStoreSource.java:117)
32420	at org.apache.flink.table.store.connector.source.FileStoreSource.createEnumerator(FileStoreSource.java:93)
32421	at org.apache.flink.runtime.source.coordinator.SourceCoordinator.start(SourceCoordinator.java:197)
32422	... 33 more {code}",,lzljs3620320,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 06 09:13:36 UTC 2022,,,,,,,,,,"0|z159io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/22 09:40;lzljs3620320;We should store FileStoreCommitImpl's commitUser into state.;;;","06/Jul/22 09:13;lzljs3620320;master: dc38c6c8279fa1582ab38c34e34d37aaa09edea2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RescaleBucketITCase is not stable,FLINK-28192,13456714,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,qingyue,qingyue,qingyue,22/Jun/22 07:06,22/Jun/22 08:59,13/Jul/23 08:13,22/Jun/22 08:59,table-store-0.2.0,,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"[https://github.com/apache/flink-table-store/runs/6996213019?check_suite_focus=true]

The job's status is not stable

!image-2022-06-22-15-06-14-271.png|width=760,height=88!",,lzljs3620320,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/22 07:06;qingyue;image-2022-06-22-15-06-14-271.png;https://issues.apache.org/jira/secure/attachment/13045431/image-2022-06-22-15-06-14-271.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 22 08:59:54 UTC 2022,,,,,,,,,,"0|z14cpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/22 07:19;qingyue;Maybe we don't need this assertion, let me remove it. cc [~lzljs3620320] ;;;","22/Jun/22 08:59;lzljs3620320;master: d8e415ae88677698903d0445cbca19dd12270a75;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate job submission for FlinkSessionJob,FLINK-28187,13454597,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,aitozi,jeesmon,jeesmon,21/Jun/22 15:16,11/Jul/22 07:30,13/Jul/23 08:13,11/Jul/22 07:30,kubernetes-operator-1.0.0,kubernetes-operator-1.1.0,,,,,,kubernetes-operator-1.1.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"During a session job submission if a deployment error (ex: concurrent.TimeoutException) is hit, operator will submit the job again. But first submission could have succeeded in jobManager side and second submission could result in duplicate job. Operator log attached.

Per [~gyfora]:

The problem is that in case a deployment error was hit, the SessionJobObserver will not be able to tell whether it has submitted the job or not. So it will simply try to submit it again. We have to find a mechanism to correlate Jobs on the cluster with the SessionJob CR itself. Maybe we could override the job name itself for this purpose or something like that.",,aitozi,gyfora,jeesmon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/22 15:15;jeesmon;flink-operator-log.txt;https://issues.apache.org/jira/secure/attachment/13045379/flink-operator-log.txt",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 11 07:30:16 UTC 2022,,,,,,,,,,"0|z13zn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 15:18;gyfora;cc [~aitozi] ;;;","21/Jun/22 15:30;gyfora;The proper mechanism for this seems to be implemented already in https://issues.apache.org/jira/browse/FLINK-11544

We can set a custom fixed jobId for the deployment itself. We should use a combination of resource name, and generation similar to [https://github.com/apache/flink-kubernetes-operator/commit/ab59d6eb980512775590d0d01e697fe0c28d1b3b]

This way the observer can robustly detect already submitted jobs.;;;","22/Jun/22 11:13;aitozi;Currently, it generates the JobId in advance to help duplicate the job submission [link|https://github.com/apache/flink-kubernetes-operator/blob/91753ec5cef1aef85ff3884197e75fa25f7f6625/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/FlinkService.java#L215].
If we run the same jobID job, it will throw DuplicateJobSubmissionException 
I think the problem is caused by that if the job submitted failed, it will not store the reconcile spec, so the jobId is not stored. And it will regenerate a new one to submit;;;","22/Jun/22 11:21;gyfora;This is not what I am suggesting please check my other PR and other ticket to illustrate the concept;;;","22/Jun/22 11:23;gyfora;We need to deterministically generate the jobid from resource name + meta/generation, and add a logic to the observer to detect already submitted jobs;;;","22/Jun/22 11:29;aitozi;thanks, I will check it now ;;;","22/Jun/22 12:07;aitozi;I get your meaning now, but I think the case is a bit different from the flink deployment. In the FlinkDeployment, we could get the deployment first then compare the generation. 
In the session job mode, the jobID is the unique key, if the spec changed will generate a different jobID and it will make the old job orphaned. The spec change can happen during the job submission failure and the job observed I think (although it is small probability). 

Can we generate the JobID by the uid of the resource. In this way, one CR will have the same JobID through its lifetime. By this, we can always get the job by the same JobID ;;;","22/Jun/22 13:02;gyfora;I think the problem you highlight here could be solved.

Let me give you a hypothetical solution:
Generate the job id with the following function:
""namespace/name@GenerationId""

This way we can easily identify old jobs deployed for the current resource even if the generation changed in the meantime.;;;","22/Jun/22 13:03;gyfora;I know we can't use arbitrary strings as jobid so it is a bit more complicated but as long as we can get the resource name + generation from the jobid itself we can solve this properly I believe;;;","22/Jun/22 15:15;aitozi;I'm afraid of not clearly expressing my meaning. I will try to give an example about what I think:

1. Submit the job with {{Generation1}} , and JobID is generated {{ns/name@Generation1}}
2. The submission timeout but actually succeed and the last reconcile spec not updated
3. User change the spec and the generation become {{Generation2}} (Before the observer have sync the job status and update the last reconcile spec)
4. The observer observe the job with JobID {{ns/name@Generation2}} not match the first job 
5. The reconciler reconcile to submit the job with {{Generation2}}. 

In this sequence, the job {{ns/name@Generation1}} will be orphaned.;;;","22/Jun/22 15:21;aitozi;IMO, there is one  and only one job for one FlinkSessionJob, so I think the JobID associated with the resource UID will be enough here;;;","22/Jun/22 15:26;gyfora;We do upgrades in 2 steps, in the UPGRADING state the expected upgrade target generation is already in the status. 
We have to put the generation in the jobid otherwise we don't know if a job in upgrading state was already upgraded or not.

Please look at this commit: [https://github.com/apache/flink-kubernetes-operator/commit/ab59d6eb980512775590d0d01e697fe0c28d1b3b]

This is not so different how applications work also. You always have a single application cluster but still you need to attach the generation info otherwise you cannot deal with errors happening during or directly after submission.;;;","23/Jun/22 02:54;aitozi;[~gyfora] I add one comment here: [https://github.com/apache/flink-kubernetes-operator/commit/ab59d6eb980512775590d0d01e697fe0c28d1b3b#r76767242];;;","23/Jun/22 05:23;gyfora;Thanks for the comment. Identifying failed first deployments is slightly tricky I agree but this doesn't really affect the general requirement:

 1.  Have a way to detect in the FlinkService if a job for this resource is already running (throw an error) -> never allow double submission
 2.  Have a way to detect in the Observer if an upgrade already happened and update the lastReconciledSpec accordingly

For Deployments 1) is provided by Flink itself, 2) is basically covered in the commit I sent. For sessionjobs we need to cover both using the jobid magic somehow :) ;;;","23/Jun/22 05:44;aitozi;> For sessionjobs we need to cover both using the jobid magic somehow 

Can this done by generating JobID with the resource UID ?

1. Dispatcher will throw DuplicateJobSubmissionException if the same JobID submitted twice.

2. Upgrade happens with the following steps:

 

1) suspend the old job, reconcile status to upgrading

2) submit the job with new spec, same jobId

3) If job submitted succeed, but somehow throws timeout, then observer can detect the JobID has running , then update the reconcile status to deployed and update the lastReconciledSpec

Do you think this is a valid solution? [~gyfora] 

 ;;;","23/Jun/22 05:59;aitozi;I think it over again the generation will make check whether the upgrade already happen much easy and accurate. It will be better flink can have some meta for a session job. By this we could use uniq JobID + generation meta to get this done much beautiful;;;","23/Jun/22 06:01;gyfora;I think the resource UID would work for session jobs. We cannot use that easily for FlinkDeployments as we have a Deployment object after cancel (keep the cluster running) so there the generation makes for a simpler, more accurate check.

I think we can start with your proposal to use the resourceUID as jobId :) ;;;","23/Jun/22 06:08;aitozi;OK, I will work on it in this way;;;","27/Jun/22 13:37;aitozi;FYI, I'm working on this now, please help assign the ticket [~gyfora];;;","08/Jul/22 12:16;gyfora;[~aitozi] we have similar logic now in Flink for Applications: [https://github.com/apache/flink/commit/e70fe68dea764606180ca3728184c00fc63ea0ff];;;","10/Jul/22 02:26;aitozi;[~gyfora] Thanks for your hint, I take a look on it;;;","11/Jul/22 07:30;gyfora;merged to main 16c9f45061d0b6c2ca31f4f0ed98378e70a9f33b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Invalid negative offset"" when using OffsetsInitializer.timestamp(.)",FLINK-28185,13454580,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mason6345,peter.schrott,peter.schrott,21/Jun/22 13:46,07/Jul/23 03:14,13/Jul/23 08:13,05/Jun/23 14:56,1.15.0,,,,,,,1.16.3,1.17.0,,,,Connectors / Kafka,,,,,,,2,pull-request-available,,,,"When using the {{OffsetsInitializer.timestamp(.)}} on a topic with empty partitions – little traffice + low retention – an {{IllegalArgumentException: Invalid negative offset}} occures. See stracktrace below.

The problem here is, that the admin client returns -1 as timestamps and offset for empty partitions in {{{}KafkaAdminClient.listOffsets(.){}}}. [1] Please compare the attached screenshot. When creating {{OffsetAndTimestamp}} object from the admin client response the exception is thrown.
{code:java}
org.apache.flink.util.FlinkRuntimeException: Failed to initialize partition splits due to 
    at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.handlePartitionSplitChanges(KafkaSourceEnumerator.java:299)
    at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$null$1(ExecutorNotifier.java:83)
    at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:40)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266)
    at java.util.concurrent.FutureTask.run(FutureTask.java)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.IllegalArgumentException: Invalid negative offset
    at org.apache.flink.kafka.shaded.org.apache.kafka.clients.consumer.OffsetAndTimestamp.<init>(OffsetAndTimestamp.java:36)
    at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator$PartitionOffsetsRetrieverImpl.lambda$offsetsForTimes$8(KafkaSourceEnumerator.java:622)
    at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321)
    at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)
    at java.util.HashMap$EntrySpliterator.forEachRemaining(HashMap.java:1723)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
    at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
    at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator$PartitionOffsetsRetrieverImpl.offsetsForTimes(KafkaSourceEnumerator.java:615)
    at org.apache.flink.connector.kafka.source.enumerator.initializer.TimestampOffsetsInitializer.getPartitionOffsets(TimestampOffsetsInitializer.java:57)
    at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.initializePartitionSplits(KafkaSourceEnumerator.java:272)
    at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.lambda$checkPartitionChanges$0(KafkaSourceEnumerator.java:242)
    at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$notifyReadyAsync$2(ExecutorNotifier.java:80)
    ... 8 common frames omitted
15:25:58.025 INFO  [flink-akka.actor.default-dispatcher-11] o.a.f.runtime.jobmaster.JobMaster - Trying to recover from a global failure.
org.apache.flink.util.FlinkException: Global failure triggered by OperatorCoordinator for 'Source: XXX -> YYY -> Sink: ZZZ' (operator 351e440289835f2ff3e6fee31bf6e13c).
    at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder$LazyInitializedCoordinatorContext.failJob(OperatorCoordinatorHolder.java:556)
    at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$QuiesceableContext.failJob(RecreateOnResetOperatorCoordinator.java:231)
    at org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.failJob(SourceCoordinatorContext.java:316)
    at org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.handleUncaughtExceptionFromAsyncCall(SourceCoordinatorContext.java:329)
    at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:42)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266)
    at java.util.concurrent.FutureTask.run(FutureTask.java)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.flink.util.FlinkRuntimeException: Failed to initialize partition splits due to 
    at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.handlePartitionSplitChanges(KafkaSourceEnumerator.java:299)
    at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$null$1(ExecutorNotifier.java:83)
    at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:40)
    ... 8 common frames omitted
Caused by: java.lang.IllegalArgumentException: Invalid negative offset
    at org.apache.flink.kafka.shaded.org.apache.kafka.clients.consumer.OffsetAndTimestamp.<init>(OffsetAndTimestamp.java:36)
    at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator$PartitionOffsetsRetrieverImpl.lambda$offsetsForTimes$8(KafkaSourceEnumerator.java:622)
    at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321)
    at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)
    at java.util.HashMap$EntrySpliterator.forEachRemaining(HashMap.java:1723)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
    at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
    at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator$PartitionOffsetsRetrieverImpl.offsetsForTimes(KafkaSourceEnumerator.java:615)
    at org.apache.flink.connector.kafka.source.enumerator.initializer.TimestampOffsetsInitializer.getPartitionOffsets(TimestampOffsetsInitializer.java:57)
    at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.initializePartitionSplits(KafkaSourceEnumerator.java:272)
    at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.lambda$checkPartitionChanges$0(KafkaSourceEnumerator.java:242)
    at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$notifyReadyAsync$2(ExecutorNotifier.java:80)
    ... 8 common frames omitted {code}
*Expected Result:*
Consumer is initialized and records of partitions that contain data (> given timestamp) are consumed. Newly incomming data on ""empty"" partitions are also consumed.

*Actual Result:*
Consumer is not initizalied. No data are consumed.

 

[1] [https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/enumerator/KafkaSourceEnumerator.java#L604]

 ","Flink 1.15.0
Kafka 2.8.1",danderson,Erbureth,martijnvisser,mason6345,mszpatusko,peter.schrott,renqs,tashoyan,,,,,,,,,,,,,,,,,FLINK-28537,,,,,,,,,,,,FLINK-28266,,,FLINK-29032,,,,,,,,,,,,,,"21/Jun/22 13:35;peter.schrott;Bildschirmfoto 2022-06-21 um 15.24.58-1.png;https://issues.apache.org/jira/secure/attachment/13045373/Bildschirmfoto+2022-06-21+um+15.24.58-1.png","28/Jun/22 08:23;tashoyan;NegativeOffsetSpec.scala;https://issues.apache.org/jira/secure/attachment/13045784/NegativeOffsetSpec.scala",,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,"// Start from the first record whose timestamp is greater than or equals a timestamp
.setStartingOffsets(OffsetsInitializer.timestamp(1592323200L))
",false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 05 14:56:20 UTC 2023,,,,,,,,,,"0|z13zjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 13:50;martijnvisser;[~renqs] Any thoughts on this? ;;;","22/Jun/22 08:45;renqs;I prefer to throw an exception with clearer message under this case. If we silently set the starting offset of empty partitions to earliest, there could be situation that the timestamp of newly incomming data are lower the specified starting timestamp, which is against the semantic of OffsetsInitializer.timestamp(). I'd like to leave the decision to users because they can always implement a custom {{{}OffsetsInitializer{}}}. ;;;","22/Jun/22 09:38;martijnvisser;Thanks for the explanation, that makes sense to me too;;;","28/Jun/22 07:49;mason6345;However, the proposal is a minor breaking change in behavior. From the source code: ""the timestamp does not exist in the partition yet, we will just consume from the latest"". So, the implementation is intended to return the latest offsets, if the timestamp cannot be found.

+1 for the proposal, the semantics are clearer to me. Additionally, it is a bit confusing in this case, because the auto offset strategy doesn't really play a role here.;;;","28/Jun/22 08:23;tashoyan;[~renqs] are you proposing to let user select a custom OffsetResetStrategy?
For example:

{code:java}
OffsetsInitializer.timestamp(timestamp, OffsetResetStrategy.LATEST)
{code}

This would be convenient. The default OffsetResetStrategy should be NONE - throw an Exception ;;;","28/Jun/22 08:34;mason6345;Yeah, it seems the implementation needs to account for the configured `OffsetResetStrategy`. Like `SpecifiedOffsetsInitializer` does;;;","14/Jul/22 22:09;mason6345;[~renqs] Mind if I take this one on? 

 

I know the code comment says it will reset to latest timestamp, but wouldn't it be clearer if the logic applied the `OffsetResetStrategy`? The default strategy for this can be latest, for backward compatibility purposes.;;;","15/Jul/22 02:57;renqs;Thanks for the feedback [~tashoyan] [~mason6345] and sorry for my late response. Yeah I think it makes sense to let users to specify the fallback strategy if the given timestamp could not be found. 

[~mason6345] Sure I'lll assign the ticket to you and thanks for the contribution!;;;","16/Aug/22 16:26;mason6345;[~renqs] gentle ping, do you have bandwidth to take a look at the PR?;;;","28/Sep/22 16:04;mason6345;cc: [~renqs] [~martijnvisser] would anyone have bandwidth to look at the PR?;;;","25/Nov/22 07:48;martijnvisser;Fixed in master: e44ecf0536589bf25c6e787d3c6307b30d19de67;;;","14/Dec/22 13:22;Erbureth;Hi,

could you please backport the fix to 1.15 & 1.16 series? As the 1.16.0 was released recently, 1.17 is a long way ahead, and the bug is a blocker for us.;;;","14/Dec/22 13:30;martijnvisser;[~Erbureth] Let's see. Especially given that Flink 1.15 and 1.16 are on different versions of the Kafka Client/Admin so I could imagine that this could be tricky. 

[~mason6345] [~renqs] I believe in the end you kept the original {{OffsetsInitializer}} so it could be backported to 1.16 and 1.15, right? Can it easily be backported?;;;","14/Dec/22 19:17;Erbureth;[~martijnvisser] For what it's worth, I have checked the code, and the relevant portions are the same across 1.15 - master. There was to the different API version happened in 1.15, which is where the bug was introduced.;;;","15/Dec/22 05:47;mason6345;Yup this is easily backported since we decided to defer the public interface changes. Let me finish off 1.16 externalization of the Kafka connector and I'll open some backport PRs;;;","20/Dec/22 15:04;martijnvisser;Re-opening so a backport to 1.16 can be added;;;","01/May/23 22:25;mason6345;[~martijnvisser] I forgot about your comment here. Backport: https://github.com/apache/flink/pull/22505;;;","05/Jun/23 14:56;martijnvisser;Fixed in release-1.16: 219f22e53d848b8055e22da3a8243e21c1dd4e31;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogStoreE2eTest is not stable,FLINK-28184,13454574,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,qingyue,qingyue,21/Jun/22 13:22,20/Jul/22 03:13,13/Jul/23 08:13,20/Jul/22 03:13,table-store-0.2.0,,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,,,,,"[https://github.com/apache/flink-table-store/runs/6984608843?check_suite_focus=true]

!image-2022-06-21-21-23-16-325.png|width=1021,height=124!

 ",,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/22 13:23;qingyue;image-2022-06-21-21-23-16-325.png;https://issues.apache.org/jira/secure/attachment/13045367/image-2022-06-21-21-23-16-325.png",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-06-21 13:22:47.0,,,,,,,,,,"0|z13zi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_es_sink_dynamic failed in jdk11,FLINK-28176,13454551,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,21/Jun/22 11:27,13/Jul/22 13:05,13/Jul/23 08:13,13/Jul/22 11:02,1.16.0,,,,,,,1.16.0,,,,,API / Python,,,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-06-21T03:01:35.4707985Z Jun 21 03:01:35 _________________ FlinkElasticsearch7Test.test_es_sink_dynamic _________________
2022-06-21T03:01:35.4709206Z Jun 21 03:01:35 
2022-06-21T03:01:35.4710708Z Jun 21 03:01:35 self = <pyflink.datastream.tests.test_connectors.FlinkElasticsearch7Test testMethod=test_es_sink_dynamic>
2022-06-21T03:01:35.4711754Z Jun 21 03:01:35 
2022-06-21T03:01:35.4712481Z Jun 21 03:01:35     def test_es_sink_dynamic(self):
2022-06-21T03:01:35.4715653Z Jun 21 03:01:35         ds = self.env.from_collection(
2022-06-21T03:01:35.4718082Z Jun 21 03:01:35             [{'name': 'ada', 'id': '1'}, {'name': 'luna', 'id': '2'}],
2022-06-21T03:01:35.4719972Z Jun 21 03:01:35             type_info=Types.MAP(Types.STRING(), Types.STRING()))
2022-06-21T03:01:35.4721209Z Jun 21 03:01:35     
2022-06-21T03:01:35.4722120Z Jun 21 03:01:35 >       es_dynamic_index_sink = Elasticsearch7SinkBuilder() \
2022-06-21T03:01:35.4723876Z Jun 21 03:01:35             .set_emitter(ElasticsearchEmitter.dynamic_index('name', 'id')) \
2022-06-21T03:01:35.4725448Z Jun 21 03:01:35             .set_hosts(['localhost:9200']) \
2022-06-21T03:01:35.4726419Z Jun 21 03:01:35             .build()
2022-06-21T03:01:35.4727430Z Jun 21 03:01:35 
2022-06-21T03:01:35.4877335Z Jun 21 03:01:35 pyflink/datastream/tests/test_connectors.py:132: 
2022-06-21T03:01:35.4882723Z Jun 21 03:01:35 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-06-21T03:01:35.4884972Z Jun 21 03:01:35 pyflink/datastream/connectors/elasticsearch.py:130: in set_hosts
2022-06-21T03:01:35.4886124Z Jun 21 03:01:35     j_http_hosts_array = to_jarray(JHttpHost, j_http_hosts_list)
2022-06-21T03:01:35.4887527Z Jun 21 03:01:35 pyflink/util/java_utils.py:37: in to_jarray
2022-06-21T03:01:35.4888600Z Jun 21 03:01:35     j_arr[i] = arr[i]
2022-06-21T03:01:35.4890812Z Jun 21 03:01:35 .tox/py39-cython/lib/python3.9/site-packages/py4j/java_collections.py:238: in __setitem__
2022-06-21T03:01:35.4892201Z Jun 21 03:01:35     return self.__set_item(key, value)
2022-06-21T03:01:35.4893842Z Jun 21 03:01:35 .tox/py39-cython/lib/python3.9/site-packages/py4j/java_collections.py:221: in __set_item
2022-06-21T03:01:35.4895153Z Jun 21 03:01:35     return get_return_value(answer, self._gateway_client)
2022-06-21T03:01:35.4896282Z Jun 21 03:01:35 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-06-21T03:01:35.4897191Z Jun 21 03:01:35 
2022-06-21T03:01:35.4900656Z Jun 21 03:01:35 answer = 'zsorg.apache.flink.api.python.shaded.py4j.Py4JException: Cannot convert org.apache.flink.elasticsearch7.shaded.org.ap...haded.py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\tat java.base/java.lang.Thread.run(Thread.java:829)\\n'
2022-06-21T03:01:35.4903369Z Jun 21 03:01:35 gateway_client = <py4j.java_gateway.GatewayClient object at 0x7f7dd5b8b580>
2022-06-21T03:01:35.4904543Z Jun 21 03:01:35 target_id = None, name = None
2022-06-21T03:01:35.4905404Z Jun 21 03:01:35 
2022-06-21T03:01:35.4906381Z Jun 21 03:01:35     def get_return_value(answer, gateway_client, target_id=None, name=None):
2022-06-21T03:01:35.4908583Z Jun 21 03:01:35         """"""Converts an answer received from the Java gateway into a Python object.
2022-06-21T03:01:35.4909687Z Jun 21 03:01:35     
2022-06-21T03:01:35.4910838Z Jun 21 03:01:35         For example, string representation of integers are converted to Python
2022-06-21T03:01:35.4912061Z Jun 21 03:01:35         integer, string representation of objects are converted to JavaObject
2022-06-21T03:01:35.4913137Z Jun 21 03:01:35         instances, etc.
2022-06-21T03:01:35.4913921Z Jun 21 03:01:35     
2022-06-21T03:01:35.4914859Z Jun 21 03:01:35         :param answer: the string returned by the Java gateway
2022-06-21T03:01:35.4916648Z Jun 21 03:01:35         :param gateway_client: the gateway client used to communicate with the Java
2022-06-21T03:01:35.4918294Z Jun 21 03:01:35             Gateway. Only necessary if the answer is a reference (e.g., object,
2022-06-21T03:01:35.4919591Z Jun 21 03:01:35             list, map)
2022-06-21T03:01:35.4920758Z Jun 21 03:01:35         :param target_id: the name of the object from which the answer comes from
2022-06-21T03:01:35.4921963Z Jun 21 03:01:35             (e.g., *object1* in `object1.hello()`). Optional.
2022-06-21T03:01:35.4923122Z Jun 21 03:01:35         :param name: the name of the member from which the answer comes from
2022-06-21T03:01:35.4924246Z Jun 21 03:01:35             (e.g., *hello* in `object1.hello()`). Optional.
2022-06-21T03:01:35.4925140Z Jun 21 03:01:35         """"""
2022-06-21T03:01:35.4925981Z Jun 21 03:01:35         if is_error(answer)[0]:
2022-06-21T03:01:35.4926846Z Jun 21 03:01:35             if len(answer) > 1:
2022-06-21T03:01:35.4927828Z Jun 21 03:01:35                 type = answer[1]
2022-06-21T03:01:35.4928784Z Jun 21 03:01:35                 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
2022-06-21T03:01:35.4929792Z Jun 21 03:01:35                 if answer[1] == REFERENCE_TYPE:
2022-06-21T03:01:35.4930858Z Jun 21 03:01:35                     raise Py4JJavaError(
2022-06-21T03:01:35.4931806Z Jun 21 03:01:35                         ""An error occurred while calling {0}{1}{2}.\n"".
2022-06-21T03:01:35.4932792Z Jun 21 03:01:35                         format(target_id, ""."", name), value)
2022-06-21T03:01:35.4933346Z Jun 21 03:01:35                 else:
2022-06-21T03:01:35.4933841Z Jun 21 03:01:35 >                   raise Py4JError(
2022-06-21T03:01:35.4934829Z Jun 21 03:01:35                         ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".
2022-06-21T03:01:35.4935466Z Jun 21 03:01:35                         format(target_id, ""."", name, value))
2022-06-21T03:01:35.4936110Z Jun 21 03:01:35 E                   py4j.protocol.Py4JError: An error occurred while calling None.None. Trace:
2022-06-21T03:01:35.4937114Z Jun 21 03:01:35 E                   org.apache.flink.api.python.shaded.py4j.Py4JException: Cannot convert org.apache.flink.elasticsearch7.shaded.org.apache.http.HttpHost to org.apache.flink.elasticsearch7.shaded.org.apache.http.HttpHost
2022-06-21T03:01:35.4938983Z Jun 21 03:01:35 E                   	at org.apache.flink.api.python.shaded.py4j.commands.ArrayCommand.convertArgument(ArrayCommand.java:166)
2022-06-21T03:01:35.4940139Z Jun 21 03:01:35 E                   	at org.apache.flink.api.python.shaded.py4j.commands.ArrayCommand.setArray(ArrayCommand.java:144)
2022-06-21T03:01:35.4941251Z Jun 21 03:01:35 E                   	at org.apache.flink.api.python.shaded.py4j.commands.ArrayCommand.execute(ArrayCommand.java:97)
2022-06-21T03:01:35.4942313Z Jun 21 03:01:35 E                   	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
2022-06-21T03:01:35.4943334Z Jun 21 03:01:35 E                   	at java.base/java.lang.Thread.run(Thread.java:829)
2022-06-21T03:01:35.4943905Z Jun 21 03:01:35 
2022-06-21T03:01:35.4945225Z Jun 21 03:01:35 .tox/py39-cython/lib/python3.9/site-packages/py4j/protocol.py:330: Py4JError
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36979&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2

",,dianfu,hxbks2ks,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 13 13:05:25 UTC 2022,,,,,,,,,,"0|z13zcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/22 07:11;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37382&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=27400;;;","01/Jul/22 09:29;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37433&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=27386;;;","04/Jul/22 10:59;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37535&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=27068;;;","05/Jul/22 08:19;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37602&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=26897;;;","06/Jul/22 07:10;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37685&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=27609;;;","07/Jul/22 06:17;hxbks2ks;Merged into master via fa67c3b7072fb8d80d05e10b1703cff5700fcb39;;;","12/Jul/22 06:38;martijnvisser;Re-opened, a new failed run:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37772&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=27201;;;","13/Jul/22 10:07;hxbks2ks;Hi，[~martijnvisser] the commit in the failed case is e5c4e3f519f364b5951e7cac331eb8af48f0ed84 which is before the fixed commit fa67c3b7072fb8d80d05e10b1703cff5700fcb39.;;;","13/Jul/22 13:05;martijnvisser;Thanks for clarifying [~hxbks2ks]
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple Parquet format tests are failing with NoSuchMethodError,FLINK-28173,13454533,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sonice_lj,martijnvisser,martijnvisser,21/Jun/22 10:17,28/Jun/22 12:03,13/Jul/23 08:13,28/Jun/22 12:03,1.16.0,,,,,,,1.16.0,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,pull-request-available,test-stability,,,"{code:java}
Jun 21 02:44:38 java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V
Jun 21 02:44:38 	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)
Jun 21 02:44:38 	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)
Jun 21 02:44:38 	at org.apache.hadoop.conf.Configuration.readFields(Configuration.java:3798)
Jun 21 02:44:38 	at org.apache.flink.formats.parquet.utils.SerializableConfiguration.readObject(SerializableConfiguration.java:50)
Jun 21 02:44:38 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jun 21 02:44:38 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
{code}

{code:java}
Jun 21 02:44:42 [ERROR]   Run 1: com.google.common.base.Preconditions.checkState(ZLjava/lang/String;I)V
Jun 21 02:44:42 [ERROR]   Run 2: com.google.common.base.Preconditions.checkState(ZLjava/lang/String;I)V
Jun 21 02:44:42 [INFO] 
Jun 21 02:44:42 [ERROR] ParquetColumnarRowSplitReaderTest.testProject
Jun 21 02:44:42 [ERROR]   Run 1: com.google.common.base.Preconditions.checkState(ZLjava/lang/String;I)V
Jun 21 02:44:42 [ERROR]   Run 2: com.google.common.base.Preconditions.checkState(ZLjava/lang/String;I)V
Jun 21 02:44:42 [INFO] 
Jun 21 02:44:42 [ERROR] ParquetColumnarRowSplitReaderTest.testReachEnd
Jun 21 02:44:42 [ERROR]   Run 1: com.google.common.base.Preconditions.checkState(ZLjava/lang/String;I)V
Jun 21 02:44:42 [ERROR]   Run 2: com.google.common.base.Preconditions.checkState(ZLjava/lang/String;I)V
Jun 21 02:44:42 [INFO] 
Jun 21 02:44:42 [ERROR]   AvroParquetRecordFormatTest.testCreateGenericReader:161->createReader:269 » NoSuchMethod
Jun 21 02:44:42 [ERROR]   AvroParquetRecordFormatTest.testCreateReflectReader:133->createReader:269 » NoSuchMethod
Jun 21 02:44:42 [ERROR]   AvroParquetRecordFormatTest.testCreateSpecificReader:118->createReader:269 » NoSuchMethod
Jun 21 02:44:42 [ERROR]   AvroParquetRecordFormatTest.testReadWithRestoreGenericReader:203->restoreReader:293 » NoSuchMethod
Jun 21 02:44:42 [ERROR]   AvroParquetRecordFormatTest.testReflectReadFromGenericRecords:147->createReader:269 » NoSuchMethod
Jun 21 02:44:42 [ERROR]   ParquetRowDataWriterTest.testCompression:126 » NoSuchMethod com.google.common....
Jun 21 02:44:42 [ERROR]   ParquetRowDataWriterTest.testTypes:117->innerTest:168 » NoSuchMethod com.googl...
Jun 21 02:44:42 [ERROR]   SerializableConfigurationTest.testResource:45 » NoSuchMethod com.google.common...
Jun 21 02:44:42 [INFO] 
Jun 21 02:44:42 [ERROR] Tests run: 31, Failures: 0, Errors: 24, Skipped: 0
Jun 21 02:44:42 [INFO] 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36979&view=logs&j=7e3d33c3-a462-5ea8-98b8-27e1aafe4ceb&t=ef77f8d1-44c8-5ee2-f175-1c88f61de8c0&l=16375",,hxbks2ks,martijnvisser,sonice_lj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22920,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 28 12:03:01 UTC 2022,,,,,,,,,,"0|z13z8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 10:50;chesnay;Maybe related to https://issues.apache.org/jira/browse/FLINK-22920?;;;","21/Jun/22 11:30;martijnvisser;Indeed since it's started failing since then. Thnx. ;;;","21/Jun/22 14:29;sonice_lj;[~martijnvisser] I'm wondering why my pr can pass all CI testcases in azure and then cause error above.;;;","21/Jun/22 14:48;sonice_lj;[~martijnvisser] If this commit is blocking other PRs, please revert it. I'll setup a brand new local environment to test my commit at tomorrow.;;;","21/Jun/22 14:51;chesnay;[~sonice_lj] This only fails on the cron builds where we use Hadoop 3.;;;","21/Jun/22 15:05;sonice_lj;[~chesnay] Thanks for your information. ;;;","22/Jun/22 02:06;sonice_lj;[~martijnvisser] Please assign this ticket to me.
I will add `hadoop3-tests` profile to cover default dependencies with guava exclusion.;;;","22/Jun/22 10:02;martijnvisser;[~sonice_lj] I've assigned it to you. ;;;","23/Jun/22 02:06;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37079&view=logs&j=7e3d33c3-a462-5ea8-98b8-27e1aafe4ceb&t=ef77f8d1-44c8-5ee2-f175-1c88f61de8c0&l=16746;;;","28/Jun/22 03:27;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37264&view=logs&j=7e3d33c3-a462-5ea8-98b8-27e1aafe4ceb&t=ef77f8d1-44c8-5ee2-f175-1c88f61de8c0;;;","28/Jun/22 12:03;martijnvisser;Fixed in master: ce66d1998d21c89077248f5b41550db316382249;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Python tests are taking more than 240 minutes to complete, causing timeouts",FLINK-28170,13454529,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,martijnvisser,martijnvisser,21/Jun/22 10:12,28/Jun/22 06:08,13/Jul/23 08:13,28/Jun/22 06:08,1.16.0,,,,,,,,,,,,API / Python,,,,,,,0,test-stability,,,,"{code:java}
Jun 21 05:16:50 pyflink/table/tests/test_join.py ......                                  [ 38%]
Jun 21 05:17:56 pyflink/table/tests/test_pandas_conversion.py ............               [ 42%]
Jun 21 05:18:57 pyflink/table/tests/test_pandas_udaf.py .......s........                 [ 46%]
Jun 21 05:19:39 pyflink/table/tests/test_pandas_udf.py .........                         [ 48%]
==========================================================================================
=== WARNING: This task took already 95% of the available time budget of 237 minutes ===
==========================================================================================
==============================================================================
The following Java processes are running (JPS)
==============================================================================
24464 PythonGatewayServer
11860 Jps
==============================================================================
Printing stack trace of Java process 24464
==============================================================================
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36979&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36979&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3",,dianfu,hxbks2ks,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 28 06:08:07 UTC 2022,,,,,,,,,,"0|z13z80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 10:12;martijnvisser;CC [~dianfu] [~hxbks2ks];;;","28/Jun/22 06:08;hxbks2ks;Currently running 4 Python versions of the tests at the same time, the total time is likely to exceed 240min. Since Python 3.6 was declared as deprecate in release 1.16 https://issues.apache.org/jira/browse/FLINK-28195, I stopped the test of python 3.6 first.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GlueSchemaRegistryJsonKinesisITCase fails on JDK11 due to NoSuchMethodError,FLINK-28169,13454525,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,CrynetLogistics,martijnvisser,martijnvisser,21/Jun/22 10:01,01/Jul/22 12:05,13/Jul/23 08:13,01/Jul/22 10:30,1.16.0,,,,,,,1.16.0,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,pull-request-available,test-stability,,,"{code:java}
Jun 21 03:06:27 Caused by: org.testcontainers.containers.ContainerLaunchException: Could not create/start container
Jun 21 03:06:27 	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:537)
Jun 21 03:06:27 	at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:340)
Jun 21 03:06:27 	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)
Jun 21 03:06:27 	... 8 more
Jun 21 03:06:27 Caused by: java.lang.RuntimeException: java.lang.NoSuchMethodError: 'org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.SdkHttpClient org.apache.flink.connector.aws.testutils.AWSServicesTestUtils.createHttpClient()'
Jun 21 03:06:27 	at org.rnorth.ducttape.timeouts.Timeouts.callFuture(Timeouts.java:68)
Jun 21 03:06:27 	at org.rnorth.ducttape.timeouts.Timeouts.getWithTimeout(Timeouts.java:43)
Jun 21 03:06:27 	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:40)
Jun 21 03:06:27 	at org.apache.flink.connectors.kinesis.testutils.KinesaliteContainer$ListStreamsWaitStrategy.retryUntilSuccessRunner(KinesaliteContainer.java:150)
Jun 21 03:06:27 	at org.apache.flink.connectors.kinesis.testutils.KinesaliteContainer$ListStreamsWaitStrategy.waitUntilReady(KinesaliteContainer.java:146)
Jun 21 03:06:27 	at org.testcontainers.containers.wait.strategy.AbstractWaitStrategy.waitUntilReady(AbstractWaitStrategy.java:51)
Jun 21 03:06:27 	at org.testcontainers.containers.GenericContainer.waitUntilContainerStarted(GenericContainer.java:926)
Jun 21 03:06:27 	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:480)
Jun 21 03:06:27 	... 10 more
Jun 21 03:06:27 Caused by: java.lang.NoSuchMethodError: 'org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.SdkHttpClient org.apache.flink.connector.aws.testutils.AWSServicesTestUtils.createHttpClient()'
Jun 21 03:06:27 	at org.apache.flink.connectors.kinesis.testutils.KinesaliteContainer$ListStreamsWaitStrategy.list(KinesaliteContainer.java:157)
Jun 21 03:06:27 	at org.rnorth.ducttape.ratelimits.RateLimiter.getWhenReady(RateLimiter.java:51)
Jun 21 03:06:27 	at org.apache.flink.connectors.kinesis.testutils.KinesaliteContainer$ListStreamsWaitStrategy.lambda$retryUntilSuccessRunner$0(KinesaliteContainer.java:153)
Jun 21 03:06:27 	at org.rnorth.ducttape.unreliables.Unreliables.lambda$retryUntilSuccess$0(Unreliables.java:43)
Jun 21 03:06:27 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
Jun 21 03:06:27 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
Jun 21 03:06:27 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
Jun 21 03:06:27 	... 1 more
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36979&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=5846934b-7a4f-545b-e5b0-eb4d8bda32e1&l=16659",,chalixar,CrynetLogistics,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 01 12:05:51 UTC 2022,,,,,,,,,,"0|z13z74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 10:01;martijnvisser;CC [~dannycranmer];;;","21/Jun/22 10:20;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36926&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=5846934b-7a4f-545b-e5b0-eb4d8bda32e1&l=15615;;;","23/Jun/22 19:18;chalixar;Hello [~martijnvisser]
Can you assign the issue to me.
Additionally a FYI, I will raise a PR to disable the test to unblock the build pipeline and proceed with a fix.
Since this is a packaging/shading issue and non of the dependencies has an ongoing work we shouldn't worry about the ignored test while being fixed.;;;","23/Jun/22 20:18;martijnvisser;Thanks [~chalixar] I've assigned it to you;;;","24/Jun/22 08:44;chalixar;[~martijnvisser] 
Disabling PR available to unblock CI.
I will follow with a fix PR;;;","24/Jun/22 17:45;martijnvisser;Disabled test via 67ff04698e6ec2340a32f1bd5c86255abc428831
Keeping the ticket open for the permanent fix;;;","30/Jun/22 13:10;CrynetLogistics;I have been coordinating with [~chalixar]  in the background and I have found a fix and can push a PR, please reassign to me ([~chalixar]  please confirm you are happy with this);;;","30/Jun/22 13:19;martijnvisser;[~CrynetLogistics] Done :);;;","30/Jun/22 16:38;CrynetLogistics;Thanks for assigning it to me [~martijnvisser] , I have pushed the PR and passed CI. Thanks [~chalixar] for approving and creating the related follow ups  FLINK-28332 and FLINK-28333. ;;;","01/Jul/22 10:30;martijnvisser;Fixed in master: 335c4049ed6ab47877b3ec64db220e5d5adbae04;;;","01/Jul/22 12:05;CrynetLogistics;Thanks [~martijnvisser] much appreciated.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the document for Contribute Documentation,FLINK-28119,13450816,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,martijnvisser,csq,csq,19/Jun/22 14:20,19/Jun/23 12:27,13/Jul/23 08:13,19/Jun/23 12:27,1.16.0,,,,,,,,,,,,Documentation,,,,,,,0,,,,,"Following the [Contribute Documentation|https://flink.apache.org/contributing/contribute-documentation.html], it requires me to install Hugo when executing 
{code:sh}
cd docs
./build_docs.sh -p
{code}
.
And the web server is actually accessed by http://localhost:1313 

Maybe we should:
1. List the prerequisites before building the doc.
2. Correct the web server address.
",,csq,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 20 06:52:36 UTC 2022,,,,,,,,,,"0|z13d34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/22 06:52;martijnvisser;[~csq] Thanks! I'll update this. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The path of the Python client interpreter could not point to an archive file in distributed file system,FLINK-28114,13450633,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,17/Jun/22 14:57,20/Jun/22 01:08,13/Jul/23 08:13,20/Jun/22 01:08,,,,,,,,1.15.1,1.16.0,,,,API / Python,,,,,,,0,,,,,"See https://github.com/apache/flink/blob/master/flink-python/src/main/java/org/apache/flink/client/python/PythonEnvUtils.java#L178 for more details about this limitation.

Users could execute PyFlink jobs in YARN application mode as following:
{code}
./bin/flink run-application -t yarn-application \
      -Djobmanager.memory.process.size=1024m \
      -Dtaskmanager.memory.process.size=1024m \
      -Dyarn.application.name=<ApplicationName> \
      -Dyarn.ship-files=/path/to/shipfiles \
      -pyarch shipfiles/venv.zip \
      -pyclientexec venv.zip/venv/bin/python3 \
      -pyexec venv.zip/venv/bin/python3 \
      -py shipfiles/word_count.py
{code}

In the above case, venv.zip will be distributed to the TMs via Flink blob server. However, blob server doesn't support files with size exceeding of 2GB. See https://github.com/apache/flink/blob/ea52732dc48a4f1c5be0925890cd8aa1ea2a11ed/flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobServerConnection.java#L223 for more details. This is very serious problem as Python users usually tend to install a lot Python libraries inside the venv.zip and some Python libraries are very large.",,dianfu,Paul Lin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 20 01:08:52 UTC 2022,,,,,,,,,,"0|z13byw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/22 01:08;dianfu;Fixed in:
- master via 6b04a50ae2182d4cdd8e44ea9a16171d1d2394ce
- release-1.15 via da05d0f3f6950dcf5e839bae0c396dbdf8a69e9e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support id of document is null,FLINK-28107,13450569,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ana4,ana4,ana4,17/Jun/22 08:47,04/Jul/22 02:19,13/Jul/23 08:13,20/Jun/22 03:17,1.16.0,,,,,,,1.16.0,,,,,API / Python,Connectors / ElasticSearch,,,,,,0,pull-request-available,,,," 
{code:java}
es7_sink = Elasticsearch7SinkBuilder() \
.set_emitter(ElasticsearchEmitter.static_index('foo')) \
.set_hosts(['localhost:9200'])  {code}
Caused by: java.lang.NullPointerException

at org.apache.flink.connector.elasticsearch.sink.SimpleElasticsearchEmitter$StaticIndexRequestGenerator.apply(SimpleElasticsearchEmitter.java:68)

at org.apache.flink.connector.elasticsearch.sink.SimpleElasticsearchEmitter$StaticIndexRequestGenerator.apply(SimpleElasticsearchEmitter.java:55)

at org.apache.flink.connector.elasticsearch.sink.SimpleElasticsearchEmitter.emit(SimpleElasticsearchEmitter.java:52)

at org.apache.flink.connector.elasticsearch.sink.SimpleElasticsearchEmitter.emit(SimpleElasticsearchEmitter.java:30)

at org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriter.write(ElasticsearchWriter.java:123)

at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.processElement(SinkWriterOperator.java:158)",,ana4,dianfu,jingge,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 20 03:17:24 UTC 2022,,,,,,,,,,"0|z13bko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 08:56;martijnvisser;[~afedulov] [~alexanderpreuss] Should this PR also be directed towards the externalized Elasticsearch repo or will that be synced later? ;;;","17/Jun/22 09:08;ana4;[~martijnvisser] When this PR merge the main repo, I will create a new PR to Elasticsearch repo and add Python ES docs.;;;","18/Jun/22 12:41;jingge;[~ana4] please find the external ES connector repo: [https://github.com/apache/flink-connector-elasticsearch.|https://github.com/apache/flink-connector-elasticsearch]

It is recommended to work on the external ES connector repo for further development and bug fixs in the future. Thanks.;;;","19/Jun/22 08:58;ana4;[~jingge] Thanks, currently I will create the same two PR for main and external ES repos.;;;","20/Jun/22 03:17;dianfu;Merged to master via 11910d52cd1b948b21d0ea04263be689d2bd721e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
We should test the copied object in GlobFilePathFilterTest#testGlobFilterSerializable,FLINK-28105,13450555,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Leo Zhou,Leo Zhou,Leo Zhou,17/Jun/22 08:01,20/Jun/22 08:07,13/Jul/23 08:13,20/Jun/22 08:07,1.14.4,1.15.0,1.16.0,,,,,1.16.0,,,,,Tests,,,,,,,0,pull-request-available,,,,"Variable [matcherCopy|https://github.com/apache/flink/blob/master/flink-core/src/test/java/org/apache/flink/api/common/io/GlobFilePathFilterTest.java#L170] is created without testing.",,Leo Zhou,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 20 08:07:51 UTC 2022,,,,,,,,,,"0|z13bhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 08:08;Leo Zhou;Hi [~zhuzh] ，can you take a look ?;;;","17/Jun/22 08:37;zhuzh;Thanks for reporting this problem! [~Leo Zhou] 

The ticket is assigned to you.;;;","20/Jun/22 08:07;zhuzh;Fixed via 44b941557e131ea03486ba5324232fe7b421a6c4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink AkkaRpcSystemLoader fails when temporary directory is a symlink,FLINK-28102,13450529,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,prabhujoseph,prabhujoseph,17/Jun/22 04:37,22/Nov/22 09:44,13/Jul/23 08:13,31/Oct/22 01:34,1.15.2,1.16.0,,,,,,1.16.1,1.17.0,,,,Runtime / RPC,,,,,,,0,pull-request-available,,,,"Flink AkkaRpcSystemLoader fails when temporary directory is a symlink

*Error Message:*
{code}
Caused by: java.nio.file.FileAlreadyExistsException: /tmp
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:88) ~[?:1.8.0_332]
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) ~[?:1.8.0_332]
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) ~[?:1.8.0_332]
        at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384) ~[?:1.8.0_332]
        at java.nio.file.Files.createDirectory(Files.java:674) ~[?:1.8.0_332]
        at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781) ~[?:1.8.0_332]
        at java.nio.file.Files.createDirectories(Files.java:727) ~[?:1.8.0_332]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcSystemLoader.loadRpcSystem(AkkaRpcSystemLoader.java:58) ~[flink-dist-1.15.0.jar:1.15.0]
        at org.apache.flink.runtime.rpc.RpcSystem.load(RpcSystem.java:101) ~[flink-dist-1.15.0.jar:1.15.0]
        at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.startTaskManagerRunnerServices(TaskManagerRunner.java:186) ~[flink-dist-1.15.0.jar:1.15.0]
        at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.start(TaskManagerRunner.java:288) ~[flink-dist-1.15.0.jar:1.15.0]
        at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManager(TaskManagerRunner.java:481) ~[flink-dist-1.15.0.jar:1.15.0]
{code}


*Repro:*

{code}
1. /tmp is a symlink points to actual directory /mnt/tmp

[root@prabhuHost log]# ls -lrt /tmp
lrwxrwxrwx 1 root root 8 Jun 15 07:51 /tmp -> /mnt/tmp

2. Start Cluster
./bin/start-cluster.sh

{code}
",,prabhujoseph,samrat007,TsReaper,vineethNaroju,wanglijie,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29698,,,,,,,,,FLINK-29728,FLINK-30139,FLINK-30143,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 31 01:34:47 UTC 2022,,,,,,,,,,"0|z13bbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 07:19;Weijie Guo;You can set io.tmp.dirs yourself using -D

createDirectories will throw FileAlreadyExistsException if dir exists but is not a directory, such as symlink.;;;","17/Jun/22 08:07;prabhujoseph;Yes setting io.tmp.dirs to the actual directory pointed by symlink worked. Shall we improve the logic to handle the symlink which points to Actual Directory case as well.;;;","17/Jun/22 08:58;Weijie Guo; We can handle symlinks correctly before FLINK-23500, but now it's broken, from my personal point of view, we should allow symlinks as before, what do you think [~chesnay] [~prabhujoseph] ,I can try to fix this if you guys think so too.;;;","20/Oct/22 07:21;TsReaper;Mistakenly closed.;;;","20/Oct/22 07:48;Weijie Guo;_[~xtsong]_ I will fix this, could you help assign this ticket to me? Thanks~;;;","31/Oct/22 01:34;xtsong;- master (1.17): 2859196f9ab1d86a3d90e47a89cbd13be74741b9
- release-1.16: 3ae578e2233abd42f770d1bf395792c85698fd89;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Close all the pending Pulsar transactions when flink shutdown the pipeline.,FLINK-28085,13450311,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,syhily,syhily,15/Jun/22 21:59,18/Oct/22 12:23,13/Jul/23 08:13,18/Oct/22 12:23,1.14.4,1.15.0,,,,,,1.17.0,,,,,Connectors / Pulsar,,,,,,,1,pull-request-available,stale-assigned,,,Currently transactionId is not persisted. After a job restart we lose handle to the transaction which is still not aborted in Pulsar broker. Pulsar broker will abort these hanging transactions after a timeout but this is not desirable. We need to close all the pending transactionId.,,syhily,tison,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 18 12:23:37 UTC 2022,,,,,,,,,,"0|z139zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","18/Oct/22 12:23;tison;master via 713b0b170bf3d8d13b1663e73c1e9d6f100da731;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar unordered reader should disable retry and delete reconsume logic.,FLINK-28084,13450307,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,syhily,syhily,15/Jun/22 21:38,16/Sep/22 01:37,13/Jul/23 08:13,16/Sep/22 01:37,1.14.4,1.15.0,,,,,,1.14.6,1.15.3,1.16.0,,,Connectors / Pulsar,,,,,,,0,pull-request-available,,,,"UnroderdPulsarSourceReader currently calls reconsume, but this feature relys on retry topic. But if retry topic is enabled the initial search will only support earliest and lates (because it will be a multiconsumer impl). We plan to delete the reconsume logic to get rid of dependency on retry topic and should disable retry.",,syhily,tison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28934,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 16 01:37:24 UTC 2022,,,,,,,,,,"0|z139yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/22 01:37;tison;https://github.com/apache/flink/pull/20725;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PulsarSource cannot work with object-reusing DeserializationSchema.,FLINK-28083,13450295,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,syhily,syhily,15/Jun/22 18:54,08/Nov/22 06:53,13/Jul/23 08:13,08/Nov/22 06:53,1.14.4,1.15.0,,,,,,1.17.0,,,,,Connectors / Pulsar,,,,,,,0,pull-request-available,stale-assigned,,,This issue is the same as Kafka's https://issues.apache.org/jira/browse/FLINK-25132,,syhily,tison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25132,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 06:53:36 UTC 2022,,,,,,,,,,"0|z139w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","08/Nov/22 06:53;tison;master via 27d42b2e599d4fafc45698711167810407ea0fa2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperMultipleComponentLeaderElectionDriverTest.testLeaderElectionWithMultipleDrivers runs into timeout,FLINK-28078,13450185,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,15/Jun/22 09:16,09/Mar/23 12:08,13/Jul/23 08:13,09/Mar/23 12:08,1.15.2,1.16.0,,,,,,1.15.3,1.16.0,,,,Runtime / Coordination,,,,,,,0,pull-request-available,stale-assigned,test-stability,,"[Build #36189|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36189&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=10455] got stuck in {{ZooKeeperMultipleComponentLeaderElectionDriverTest.testLeaderElectionWithMultipleDrivers}}
{code}
""ForkJoinPool-45-worker-25"" #525 daemon prio=5 os_prio=0 tid=0x00007fc74d9e3800 nid=0x62c8 waiting on condition [0x00007fc6ff2f2000]
May 30 16:36:10    java.lang.Thread.State: WAITING (parking)
May 30 16:36:10 	at sun.misc.Unsafe.park(Native Method)
May 30 16:36:10 	- parking to wait for  <0x00000000c2571b80> (a java.util.concurrent.CompletableFuture$Signaller)
May 30 16:36:10 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
May 30 16:36:10 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
May 30 16:36:10 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)
May 30 16:36:10 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
May 30 16:36:10 	at java.util.concurrent.CompletableFuture.join(CompletableFuture.java:1947)
May 30 16:36:10 	at org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriverTest.testLeaderElectionWithMultipleDrivers(ZooKeeperMultipleComponentLeaderElectionDriverTest.java:256)
May 30 16:36:10 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
May 30 16:36:10 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
May 30 16:36:10 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
May 30 16:36:10 	at java.lang.reflect.Method.invoke(Method.java:498)
[...]
{code}

 ",,dannycranmer,hxb,hxbks2ks,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27880,FLINK-29173,,,,,,,,,,,,,,,,,FLINK-30484,FLINK-31379,,,,CURATOR-645,"31/Aug/22 08:32;mapohl;FLINK-28078-build-40525-20220830.14.tar.gz;https://issues.apache.org/jira/secure/attachment/13048798/FLINK-28078-build-40525-20220830.14.tar.gz",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 09 12:08:12 UTC 2023,,,,,,,,,,"0|z1397k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/22 08:42;chesnay;We seem to be entering a strange loop. The ZK log show the following over and over again; the final number inf the getData calls is incremented on each loop.

{code}
FinalRequestProcessor - Processing request:: sessionid:0x100cf6d9cf60000 type:getData cxid:0x20 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_ddb7327c-1775-4084-9fbd-40e263758697-latch-0000000000
FinalRequestProcessor - sessionid:0x100cf6d9cf60000 type:getData cxid:0x20 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_ddb7327c-1775-4084-9fbd-40e263758697-latch-0000000000
FinalRequestProcessor - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x21 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
FinalRequestProcessor - sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x21 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
FinalRequestProcessor - Processing request:: sessionid:0x100cf6d9cf60000 type:delete cxid:0x22 zxid:0xc txntype:2 reqpath:n/a
FinalRequestProcessor - sessionid:0x100cf6d9cf60000 type:delete cxid:0x22 zxid:0xc txntype:2 reqpath:n/a
FinalRequestProcessor - Processing request:: sessionid:0x100cf6d9cf60000 type:create2 cxid:0x23 zxid:0xd txntype:15 reqpath:n/a
FinalRequestProcessor - sessionid:0x100cf6d9cf60000 type:create2 cxid:0x23 zxid:0xd txntype:15 reqpath:n/a
{code};;;","21/Jun/22 12:05;mapohl;{code}
16:17:07,802 [ForkJoinPool-45-worker-25] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl [] - Starting
16:17:07,804 [ForkJoinPool-45-worker-25] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl [] - Default schema
16:17:07,814 [ForkJoinPool-45-worker-25-EventThread] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager [] - State change: CONNECTED
16:17:07,817 [ForkJoinPool-45-worker-25-EventThread] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker [] - New config event received: {}
16:17:07,824 [Curator-ConnectionStateManager-0] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Connected to ZooKeeper quorum. Leader election can start.
16:17:07,824 [Curator-ConnectionStateManager-0] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Connected to ZooKeeper quorum. Leader election can start.
16:17:07,826 [ForkJoinPool-45-worker-25-EventThread] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker [] - New config event received: {}
16:17:07,848 [ForkJoinPool-45-worker-25-EventThread] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - ZooKeeperMultipleComponentLeaderElectionDriver obtained the leadership.
16:17:07,860 [ForkJoinPool-45-worker-25] INFO  org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Closing ZooKeeperMultipleComponentLeaderElectionDriver.
{code}

The test itself usually creates three {{ElectionDriver}} instances and removes them one by one through a for loop. The logs of the failed test reveal that only two out of the three have the quorum connection established (i.e. the log message {{Connected to ZooKeeper quorum. Leader election can start.}} is printed). The first iteration picks the first instance, checks its leadership and closes it. 

The {{anyOf}} call in the next iteration should actually still succeed because there's one {{ElectionDriver}} that has an established connection. But the resulting {{anyOf}} composite future doesn't complete, i.e. non of the left Leadership futures completes resulting in the test getting stuck in the subsequent {{join}} call.;;;","21/Jun/22 15:02;mapohl;It's not clear to me, yet, why we're not seeing the second ElectionDriver taking over the leadership. I'd expect to get through the second iteration as well because the leadership should be obtainable for the second ElectionDriver as well. ;;;","24/Jun/22 06:37;mapohl;I did a comparison of a successful run and the failed run. I'm getting the feeling that we're running into some race condition on the {{LeaderLatch}} implementation side.

Here are the logs of a successful run starting from the log message where the latch zNode is checked for existence after being created:
{code:java}
1376 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:exists cxid:0x1b zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1376 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:exists cxid:0x1b zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1377 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:createContainer cxid:0x1c zxid:0xa txntype:19 reqpath:n/a
1378 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:createContainer cxid:0x1c zxid:0xa txntype:19 reqpath:n/a
1380 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x1d zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1380 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x1d zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1380 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x1e zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1380 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x1e zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1380 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x1f zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1380 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x1f zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1381 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren cxid:0x20 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1381 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren cxid:0x20 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1385 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:create2 cxid:0x21 zxid:0xb txntype:15 reqpath:n/a
1385 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:create2 cxid:0x21 zxid:0xb txntype:15 reqpath:n/a
1385 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:exists cxid:0x22 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink
1385 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:exists cxid:0x22 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink
1386 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:exists cxid:0x23 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1386 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:exists cxid:0x23 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1387 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x24 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1387 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x24 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1387 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:exists cxid:0x25 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1387 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:exists cxid:0x25 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1388 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren cxid:0x26 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1388 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren cxid:0x26 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1390 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:create2 cxid:0x27 zxid:0xc txntype:15 reqpath:n/a
1391 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:create2 cxid:0x27 zxid:0xc txntype:15 reqpath:n/a
1391 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:exists cxid:0x28 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink
1391 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:exists cxid:0x28 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink
1391 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:exists cxid:0x29 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1391 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:exists cxid:0x29 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1392 [ForkJoinPool-1-worker-9-EventThread] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - ZooKeeperMultipleComponentLeaderElectionDriver obtained the leadership.
1392 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x2a zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1392 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x2a zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1393 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:exists cxid:0x2b zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1393 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:exists cxid:0x2b zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1393 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren cxid:0x2c zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1393 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren cxid:0x2c zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1394 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:create2 cxid:0x2d zxid:0xd txntype:15 reqpath:n/a
1395 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:create2 cxid:0x2d zxid:0xd txntype:15 reqpath:n/a
1421 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getData cxid:0x2e zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_2b57761c-31e0-458d-9466-b409010d3d14-latch-0000000000
1421 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getData cxid:0x2e zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_2b57761c-31e0-458d-9466-b409010d3d14-latch-0000000000
1421 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x2f zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1421 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x2f zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1422 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getData cxid:0x30 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_1e836e28-a9d1-4bea-97f3-222034044247-latch-0000000001
1422 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getData cxid:0x30 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_1e836e28-a9d1-4bea-97f3-222034044247-latch-0000000001
1497 [ForkJoinPool-1-worker-9] INFO  org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Closing ZooKeeperMultipleComponentLeaderElectionDriver.
1501 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:checkWatches cxid:0x31 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1501 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:checkWatches cxid:0x31 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1505 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:delete cxid:0x32 zxid:0xe txntype:2 reqpath:n/a
1506 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:delete cxid:0x32 zxid:0xe txntype:2 reqpath:n/a
1506 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x33 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1506 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x33 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1507 [ForkJoinPool-1-worker-9-EventThread] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - ZooKeeperMultipleComponentLeaderElectionDriver obtained the leadership.
1507 [ForkJoinPool-1-worker-9] INFO  org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Closing ZooKeeperMultipleComponentLeaderElectionDriver.
1508 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:checkWatches cxid:0x34 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1508 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:checkWatches cxid:0x34 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1508 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:delete cxid:0x35 zxid:0xf txntype:2 reqpath:n/a
1508 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:delete cxid:0x35 zxid:0xf txntype:2 reqpath:n/a
1509 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x36 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1509 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x36 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1509 [ForkJoinPool-1-worker-9-EventThread] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - ZooKeeperMultipleComponentLeaderElectionDriver obtained the leadership.
1510 [ForkJoinPool-1-worker-9] INFO  org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Closing ZooKeeperMultipleComponentLeaderElectionDriver.
1510 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:checkWatches cxid:0x37 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1510 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:checkWatches cxid:0x37 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1510 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:delete cxid:0x38 zxid:0x10 txntype:2 reqpath:n/a
1511 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:delete cxid:0x38 zxid:0x10 txntype:2 reqpath:n/a
1511 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:checkWatches cxid:0x39 zxid:0xfffffffffffffffe txntype:unknown reqpath:/zookeeper/config
1511 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:checkWatches cxid:0x39 zxid:0xfffffffffffffffe txntype:unknown reqpath:/zookeeper/config
1513 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:closeSession cxid:0x3a zxid:0x11 txntype:-11 reqpath:n/a
1513 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:closeSession cxid:0x3a zxid:0x11 txntype:-11 reqpath:n/a
{code}
and here are the merged (i.e. I integrated the test-sides logs into the zookeeper-server logs based on the time) logs:
{code:java}
16:17:07,824 [Curator-ConnectionStateManager-0] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Connected to ZooKeeper quorum. Leader election can start.
16:17:07,824 [Curator-ConnectionStateManager-0] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Connected to ZooKeeper quorum. Leader election can start.
16:17:07,829 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:exists cxid:0x10 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,829 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:exists cxid:0x10 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,831 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getData cxid:0x11 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,832 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getData cxid:0x11 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,837 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x12 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,837 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x12 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,838 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:createContainer cxid:0x13 zxid:0x7 txntype:19 reqpath:n/a
16:17:07,838 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:createContainer cxid:0x13 zxid:0x7 txntype:19 reqpath:n/a
16:17:07,839 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:create2 cxid:0x14 zxid:0x8 txntype:15 reqpath:n/a
16:17:07,839 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:create2 cxid:0x14 zxid:0x8 txntype:15 reqpath:n/a
16:17:07,839 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getData cxid:0x15 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,839 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getData cxid:0x15 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,839 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x16 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,839 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x16 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,846 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:create2 cxid:0x17 zxid:0x9 txntype:15 reqpath:n/a
16:17:07,846 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:create2 cxid:0x17 zxid:0x9 txntype:15 reqpath:n/a
16:17:07,846 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren cxid:0x18 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,846 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getChildren cxid:0x18 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,846 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x19 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,846 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x19 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,846 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x1a zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,846 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x1a zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,846 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x1b zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,846 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x1b zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,848 [ForkJoinPool-45-worker-25-EventThread] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - ZooKeeperMultipleComponentLeaderElectionDriver obtained the leadership.
16:17:07,854 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x1c zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,854 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x1c zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,848 [ForkJoinPool-45-worker-25-EventThread] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - ZooKeeperMultipleComponentLeaderElectionDriver obtained the leadership.
16:17:07,860 [ForkJoinPool-45-worker-25] INFO  org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Closing ZooKeeperMultipleComponentLeaderElectionDriver.
16:17:07,860 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:create2 cxid:0x1d zxid:0xa txntype:15 reqpath:n/a
16:17:07,860 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:create2 cxid:0x1d zxid:0xa txntype:15 reqpath:n/a
16:17:07,862 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:checkWatches cxid:0x1e zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,862 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:checkWatches cxid:0x1e zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,863 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:delete cxid:0x1f zxid:0xb txntype:2 reqpath:n/a
16:17:07,863 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:delete cxid:0x1f zxid:0xb txntype:2 reqpath:n/a
16:17:07,863 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getData cxid:0x20 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_ddb7327c-1775-4084-9fbd-40e263758697-latch-0000000000
16:17:07,863 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getData cxid:0x20 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_ddb7327c-1775-4084-9fbd-40e263758697-latch-0000000000
16:17:07,864 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x21 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,864 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x21 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,866 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:delete cxid:0x22 zxid:0xc txntype:2 reqpath:n/a
16:17:07,866 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:delete cxid:0x22 zxid:0xc txntype:2 reqpath:n/a
16:17:07,869 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:create2 cxid:0x23 zxid:0xd txntype:15 reqpath:n/a
16:17:07,869 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:create2 cxid:0x23 zxid:0xd txntype:15 reqpath:n/a
16:17:07,869 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getData cxid:0x24 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_6eb174e9-bb77-4a73-9604-531242c11c0e-latch-0000000001
16:17:07,869 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getData cxid:0x24 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_6eb174e9-bb77-4a73-9604-531242c11c0e-latch-0000000001
16:17:07,871 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x25 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,871 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x25 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,875 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:delete cxid:0x26 zxid:0xe txntype:2 reqpath:n/a
16:17:07,875 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:delete cxid:0x26 zxid:0xe txntype:2 reqpath:n/a
16:17:07,876 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:create2 cxid:0x27 zxid:0xf txntype:15 reqpath:n/a
16:17:07,876 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:create2 cxid:0x27 zxid:0xf txntype:15 reqpath:n/a
16:17:07,876 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getData cxid:0x28 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_8008ceb9-e0b9-459b-8fa5-44428fa31e29-latch-0000000002
16:17:07,876 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getData cxid:0x28 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_8008ceb9-e0b9-459b-8fa5-44428fa31e29-latch-0000000002
[...]
{code}
The LeaderLatch implementation works like that (based on the code):
 # {{LeaderLatch}} is started pointing to a given zNode path ({{{}/flink/default/latch/{}}} in our case).
 # The instance will get {{reset()}} initially setting the leadership for this latch to {{false}} and triggering a child creation
 # Child creation is done ""withProtection"" (i.e. a random String will be prefixed to the child) in mode {{EPHEMERAL_SEQUENTIAL}} (i.e. the child will be deleted on connection-loss and a monotonically increasing number is added as a suffix) with a callback that triggers {{getChildren}} in case of success and selecting the leader based on the suffix of the child nodes names (lower ID wins). Hence, an order between the children is established based on the ID.
 # {{getChildren}} callback triggers leadership detection in the local {{LeaderLatch}} instance.
 ## For the child with the lowest ID, the leadership is triggered in the callback
 ## If the current {{LeaderLatch}} doesn't correspond to the child with the lowest ID, a watcher is set up watching the direct predecessor (based on the ID-based ordering I mentioned before) for deletion. If the deletion happens, the watcher is triggered (only once, i.e. not repeatedly) and reinitiates the leadership detection through {{getChildren}} (4.) if the {{LeaderLatch}} isn't closed, yet. This is what we observe in the successful test run with the watcher being triggered. 

There is another code path if the predecessor node doesn't exist in {{getChildren}} (4.). In that case, an entire {{reset()}} (2.) is triggered resulting in the deletion of the current's {{LeaderLatch}}'s zNode and the recreation of the child zNode. It appears that we're ending up in a this code path for the unsuccessful test run.;;;","24/Jun/22 07:11;mapohl;The loop consists of the following logs:
{code}
16:17:07,864 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x21 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,864 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x21 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,866 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:delete cxid:0x22 zxid:0xc txntype:2 reqpath:n/a
16:17:07,866 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:delete cxid:0x22 zxid:0xc txntype:2 reqpath:n/a
16:17:07,869 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:create2 cxid:0x23 zxid:0xd txntype:15 reqpath:n/a
16:17:07,869 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:create2 cxid:0x23 zxid:0xd txntype:15 reqpath:n/a
16:17:07,869 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getData cxid:0x24 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_6eb174e9-bb77-4a73-9604-531242c11c0e-latch-0000000001
{code}
# The {{reset()}} triggers [getChildren|https://github.com/apache/curator/blob/d1a9234ecae47e3704037c839e6041931c24d1f4/curator-recipes/src/main/java/org/apache/curator/framework/recipes/leader/LeaderLatch.java#L629] through the [LeaderLatch#getChildren|https://github.com/apache/curator/blob/d1a9234ecae47e3704037c839e6041931c24d1f4/curator-recipes/src/main/java/org/apache/curator/framework/recipes/leader/LeaderLatch.java#L525] after a new child is created (I would assume {{create2}} entry in the logs before {{getChildren}} entry which is not the case; so, I might be wrong in my observation)
# The callback of {{getChildren}} triggers [checkLeadership|https://github.com/apache/curator/blob/d1a9234ecae47e3704037c839e6041931c24d1f4/curator-recipes/src/main/java/org/apache/curator/framework/recipes/leader/LeaderLatch.java#L625].
# In the meantime, the predecessor gets deleted (I'd assume because of the deterministic ordering of the events in ZK). This causes the [callback in checkLeadership|https://github.com/apache/curator/blob/d1a9234ecae47e3704037c839e6041931c24d1f4/curator-recipes/src/main/java/org/apache/curator/framework/recipes/leader/LeaderLatch.java#L607] to fail with a {{NONODE}} event and triggering the reset of the current {{LeaderLatch}} instance which again triggers the deletion of the current's {{LeaderLatch}}'s child zNode and which is executed on the server later on.;;;","24/Jun/22 07:12;mapohl;I found [CURATOR-3|https://issues.apache.org/jira/browse/CURATOR-3] that might be of relevance here. In the unsuccessful test run, there is a {{checkWatch}} event triggered after the first {{LeaderLatch}} is closed. But the ticket refers to a single {{LeaderLatch}} not getting the leadership anymore. So, our observed behavior with the loop is different.;;;","01/Jul/22 06:43;mapohl;I created CURATOR-645 to cover the issue on the curator side. I'll leave this issue open because it might mean that curator needs to be updated to cover a fix.;;;","31/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","16/Aug/22 08:27;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40027&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=10214;;;","17/Aug/22 07:50;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40084&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","29/Aug/22 02:35;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40433&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53;;;","29/Aug/22 16:21;mapohl;CURATOR-645 is probably caused by some issue where we revoke the leadership in the test ""too fast"" which makes the curator code end up in a different code path that contains a bug. There's already a fix for that in [a PR in the CURATOR project|https://github.com/apache/curator/pull/430]. But I'm not sure how fast we're going to get this merged.

I started experimenting with some temporary (dirty) workaround for our test to make it less likely to fail. I'm suspecting that we only need to ""add some workload on the leader's side"" to throttle the leadership revocation. Still, this issue can happen in production as well. There's a race condition between the leader losing its leadership and candidates going through an re-evaluation of the leadership on their end ({{LeaderLatch#getChildren}} is called before the leader's znode is deleted but {{LeaderLatch#checkLeadership}} is called after the leader's znode is deleted). We can only overcome this by fixing CURATOR-645 and upgrading to the corresponding Apache Curator version.

I would still leave the Jira issue as {{Major}} because of nobody having it reported by now. The test case made the issue only visible because the race condition because more likely in the test with no actual workload being processed by the leader process.;;;","31/Aug/22 08:31;mapohl;Ok, I tried to reproduce the issue in [build 20220830.14|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40525&view=results] (I let each of the test jobs run the unit test repeatedly except for the `core` stage due to some bug in the if statement :facepalm:). I attached the results of the run for reproducability reasons to this issue (see  [^FLINK-28078-build-40525-20220830.14.tar.gz]).

In total there were 6178 test execution over all participating modules (there were failed test runs due to the jobs being cancelled after ~4hours).
{code}
for f in $(ls *zip); do job_name=""${f%"".zip""}""; unzip -p $f ${job_name}/mvn-1.log | grep -c ""Test org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriverTest.testLeaderElectionWithMultipleDrivers successfully run.""; done  | paste -sd+ | bc
{code}

I couldn't find any evidence that any of the test runs ran into the issue by checking the size of the {{zookeeper-server-1.log}} for each of the jobs (we should have observed a peak in file size due to the infinite loop on the ZK side). All logs have similar sizes:
{code}
for f in $(ls *zip); do job_name=""${f%"".zip""}""; echo $job_name; unzip -p $f ${job_name}/zookeeper-server-1.log | wc --bytes | numfmt --to iec --format ""%8.4f""; done
logs-ci-test_ci_connect_1-1661858590
41,0784M
logs-ci-test_ci_connect_2-1661858534
43,2716M
logs-ci-test_ci_core-1661858500
caution: filename not matched:  logs-ci-test_ci_core-1661858500/zookeeper-server-1.log
  0,0000
logs-ci-test_ci_finegrained_resource_management-1661858511
42,9698M
logs-ci-test_ci_misc-1661858514
39,8520M
logs-ci-test_ci_python-1661858538
41,2648M
logs-ci-test_ci_table-1661858510
43,8197M
logs-ci-test_ci_tests-1661858630
40,8045M
{code}

I will proceed with implementing the (dirty temporary) workaround in the PR. Let's see whether that reduces the likelihood for this test failing again.;;;","01/Sep/22 12:41;mapohl;master: 655184cdb086ac2adec3e743701868f1a55b6129
1.15: ed8700d03cccc47bfa39da5e1e6611eb9be7d5a1;;;","01/Sep/22 14:25;mapohl;Created FLINK-29173 as a follow-up to revert the changes from this issue and upgrading curator.;;;","06/Mar/23 14:44;mapohl;Looks like this issue can still appear:
1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46843&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","06/Mar/23 14:46;mapohl;Another workaround to cover the issue which is closer to reality is that we use separate client for each LeaderElectionService. This will avoid using the same event queue and breaks the strict orderness of events between different LeaderElectionService instances.;;;","09/Mar/23 11:13;dannycranmer;[~mapohl] since this is already merged into a fix version I suggest we create a follow up Jira and link them. Having a single Jira with fixVersion 1.5.3 and 1.15.5 does not make sense.;;;","09/Mar/23 12:08;mapohl;Fair point. Thanks for bringing that up, [~dannycranmer]. I created a follow-up FLINK-31379 to cover the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tasks get stuck during cancellation in ChannelStateWriteRequestExecutorImpl,FLINK-28077,13450184,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fanrui,mapohl,mapohl,15/Jun/22 09:14,23/Jun/22 12:40,13/Jul/23 08:13,21/Jun/22 12:24,1.16.0,,,,,,,1.16.0,,,,,Runtime / Checkpointing,Tests,,,,,,0,pull-request-available,test-stability,,,"[Build #36209|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36209&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9370] got stuck in {{KeyedStateCheckpointingITCase.testWithMemoryBackendSync}}:
{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007f849c00b800 nid=0x19c3 waiting on condition [0x00007f84a45b7000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000080074870> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1989)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1951)
	at org.apache.flink.test.checkpointing.KeyedStateCheckpointingITCase.testProgramWithBackend(KeyedStateCheckpointingITCase.java:175)
	at org.apache.flink.test.checkpointing.KeyedStateCheckpointingITCase.testWithMemoryBackendSync(KeyedStateCheckpointingITCase.java:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[...]
{code}",,clouding,fanrui,mapohl,martijnvisser,pnowojski,roman,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27880,,,,,,,FLINK-28076,FLINK-28024,FLINK-27832,,FLINK-27251,,,,,,,FLINK-27792,,FLINK-27792,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 21 12:24:10 UTC 2022,,,,,,,,,,"0|z1397c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/22 12:18;chesnay;The TM is crashing because a task gets stuck during cancellation:
{code}
 java.lang.Object.wait(Native Method)
java.lang.Thread.join(Thread.java:1252)
java.lang.Thread.join(Thread.java:1326)
org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.close(ChannelStateWriteRequestExecutorImpl.java:166)
org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.close(ChannelStateWriterImpl.java:234)
org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.cancel(SubtaskCheckpointCoordinatorImpl.java:560)
org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.close(SubtaskCheckpointCoordinatorImpl.java:547)
org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$1220/1213892815.close(Unknown Source)
org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72)
org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127)
org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:938)
org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$1(Task.java:923)
org.apache.flink.runtime.taskmanager.Task$$Lambda$1886/761627220.run(Unknown Source)
org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:923)
org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
java.lang.Thread.run(Thread.java:748)
{code};;;","15/Jun/22 12:21;chesnay;Interestingly enough after the TM is shutdown we can see the ChannelStateWriter for the blocked task making progress again and continues the shutdown.;;;","15/Jun/22 12:22;chesnay;This is also not specific to this test; also saw it on another one (but I lost the build).;;;","15/Jun/22 14:19;chesnay;I could reproduce this by adding a 200ms sleep before the dispatch in {{org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl#loop}}. The thread gets stuck during the cancellation in the discard action ChannelStateWriteRequest, waiting on the dataFuture:

{code}
CloseableIterator.fromList(dataFuture.get(), Buffer::recycleBuffer).close();
{code};;;","15/Jun/22 14:20;chesnay;May be caused by FLINK-27251.;;;","15/Jun/22 15:06;chesnay;/cc [~fanrui] [~pnowojski];;;","16/Jun/22 02:10;fanrui;Hi [~mapohl] [~chesnay] , thanks for this information, I will take a look this week.:);;;","21/Jun/22 12:24;chesnay;master: 0912765acc25f179169f8e371683acaf3e9133a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PredicateBuilder.in should accept null parameters,FLINK-28064,13450118,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,lzljs3620320,lzljs3620320,15/Jun/22 02:32,01/Jul/22 08:17,13/Jul/23 08:13,01/Jul/22 08:17,,,,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,pull-request-available,,,,"PredicateBuilder.in(int idx, List<Literal> literals).

Literals must be not null, but this is not enough to meet the needs of SQL `in`.

It is allowed to have null parameters in `in`.",,lzljs3620320,qingyue,,,,,,,,,,,,,,,,,,,,,,,FLINK-28179,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 01 08:17:31 UTC 2022,,,,,,,,,,"0|z138so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 08:17;lzljs3620320;master: 0688ce66bbf5f2736c82a4d4c6e4279e33e24f66;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Commit on checkpointing fails repeatedly after a broker restart,FLINK-28060,13450052,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,Christian.Lorenz77,Christian.Lorenz77,14/Jun/22 15:08,25/Apr/23 07:46,13/Jul/23 08:13,10/Aug/22 10:14,1.15.0,,,,,,,1.16.0,,,,,API / DataStream,Connectors / Kafka,,,,,,3,pull-request-available,,,,"When Kafka Offset committing is enabled and done on Flinks checkpointing, an error might occur if one Kafka broker is shutdown which might be the leader of that partition in Kafkas internal __consumer_offsets topic.

This is an expected behaviour. But once the broker is started up again, the next checkpoint issued by flink should commit the meanwhile processed offsets back to kafka. Somehow this does not seem to happen always in Flink 1.15.0 anymore and the offset committing is broken. An warning like the following will be logged on each checkpoint:

{code}
[info] 14:33:13.684 WARN  [Source Data Fetcher for Source: input-kafka-source -> Sink: output-stdout-sink (1/1)#1] o.a.f.c.k.s.reader.KafkaSourceReader - Failed to commit consumer offsets for checkpoint 35
[info] org.apache.kafka.clients.consumer.RetriableCommitFailedException: Offset commit failed with a retriable exception. You should retry committing the latest consumed offsets.
[info] Caused by: org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.
{code}

To reproduce this I've attached a small flink job program.  To execute this java8, scala sbt and docker / docker-compose is required.  Also see readme.md for more details.
The job can be run with `sbt run`, kafka cluster is started by `docker-compose up`. If then the kafka brokers are restarted gracefully by e.g. `docker-compose stop kafka1` and `docker-compose start kafka1` with kafka2 and kafka3 afterwards, this warning will occur and no offsets will be committed into kafka.

This is not reproducible in flink 1.14.4.

","Reproduced on MacOS and Linux.

Using java 8, Flink 1.15.0, Kafka 2.8.1.",Christian.Lorenz77,danderson,david.artiga,elanv,godfreyhe,igaevd,kyle.stehbens,laughingman7743,leesenlen,leonard,martijnvisser,mason6345,peter.schrott,renqs,showuon,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27962,,,,,,FLINK-27962,,,,,,,,"14/Jun/22 15:01;Christian.Lorenz77;flink-kafka-testjob.zip;https://issues.apache.org/jira/secure/attachment/13045068/flink-kafka-testjob.zip",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 10 10:14:05 UTC 2022,,,,,,,,,,"0|z138e0:",9223372036854775807,The Kafka Client version has been updated to 3.2.1. ,,,,,,,,,,,,,,,,,,,"14/Jun/22 17:42;martijnvisser;[~renqs] This seems related to FLINK-27962 - Should we raise priority to at least Critical, potential Blocker? Seems like this is a regression. I linked the two tickets together, because they both have value. Probably we should close one of them in the end;;;","15/Jun/22 08:21;renqs;I did some investigation and strongly suspect that this is caused by KAFKA-13563. We upgraded the Kafka client from 2.4.1 to 2.8.1 in Flink 1.15, and the bug was introduced in 2.6.2 by KAFKA-10793 :(

The patch of KAFKA-13563 is applied only on Kafka 3.1 and 3.2, so we have to bump the version as well, or urge Kafka guys to cherry-pick the patch back on 2.x. 

WDYT? [~martijnvisser] ;;;","15/Jun/22 10:13;danderson;I'd like to include a fix for this in 1.15.1, but I'm not sure about bumping up the Kafka client to 3.x. What are you thinking? [~martijnvisser] [~renqs] ;;;","15/Jun/22 11:34;martijnvisser;I think it depends on the impact of bumping the Kafka Clients dependency. If it's ""just"" a dependency update without modifying the actual source code, I would probably be OK with it. If it requires refactoring of the interface, then I would say we can only fix this in Flink 1.16.0. I'll setup a test CI run to see what happens. 

Looking at the release notes, I do hope that the main reason for going to a new main version was the deprecation of Java 8 and the deprecation of Scala 2.12. If that's it, I do hope impact will be low. ;;;","15/Jun/22 13:24;chesnay;Could someone clarify what this issue means in practice to users? Could we just _not_ commit the offsets?

I'm quite wary of doing a major version upgrade, independent of whether we have to adjust our code or not.
Alternatives include reverting FLINK-24765, or going back to 2.6.1 (i.e., some version that doesn't have the Kafka bug).;;;","15/Jun/22 13:44;peter.schrott;From a users perspective I see 2 issues here:

1) Monitoring / Alerting: We are using consumer offsets / consumer lag to monitor potential issues with the Flink job, also in terms of performance, i.e. the lag gets too large.

2) StartingOffsets: Flink Kafka connector offers the feature of resuming from committed offsets [1]. This feature would be obsolet if offsets are not committed to Kafka (sure, one can always use savepoints when restarting a job)

 

[1] https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/datastream/kafka/#starting-offset;;;","16/Jun/22 07:07;danderson;Along the lines of what Chesnay mentioned, what would be the impact of either reverting FLINK-24765, or rolling back the kafka client dependency to something like 2.6.1 (i.e., something newer than 2.4.1 but before the Kafka bug that was introduced by KAFKA-10793 in 2.6.2 and 2.7.1)?

 ;;;","16/Jun/22 07:35;martijnvisser;I have sincere doubts about reverting FLINK-24765, because more changes has happened since then (like FLINK-25573, FLINK-27487, FLINK-27480 and I'm sure I'm missing some more).

So that probably leaves us with a downgrade of the Kafka Clients dependency or an upgrade of the Kafka Clients dependency? ;;;","16/Jun/22 07:40;Christian.Lorenz77;Does someone know if the Kafka Team is aware that this issue exists in 2.8.1? I think Flink might be one of the most prominent usescases of using topic#assign where this is occurring, but for sure this will harm other users as well. The most natural fix for this would be to upgrade to kafka-client 2.8.2 once available. ;;;","16/Jun/22 07:54;chesnay;Indeed going back to 2.4.1 doesn't seem feasible; going back to 2.6.1 at least doesn't produce compile errors; waiting on CI.;;;","16/Jun/22 08:03;renqs;Personally I prefer to upgrade the Kafka client. Downgrading the Kafka client could introduce new bugs (like the one fixed in KAFKA-10793). I think the backward compatibility of Kafka is trustable according to our previous experience. On consumer side we don't use any hacks in Kafka source, purely depend on APIs so I think it should be fine, but for the producer we use [reflection|https://github.com/apache/flink/blob/c9a706b8388b324a37da43298e37074d0a452a34/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducer.java#L331-L343] which is concerning. I can have a try to bump the version and see if it works. 

Any ideas from [~becket_qin] ?;;;","16/Jun/22 08:10;martijnvisser;I'm waiting for CI to complete a run upgrading to 3.1.1, that's looking good as well without needing to change any interface/implementation details. ;;;","16/Jun/22 12:29;mason6345;+1 on [~peter.schrott] 's assessment–we need also need this for metrics and in case Flink state is lost or we need to do a migration where it is operationally easier to throwaway Flink state.

In addition, Flink hasn't removed support for FlinkKafkaConsumer, right? The group offsets are essential for the migration process to the FLIP 27 Kafka Source since users will have operational issues moving without committed offsets in Kafka.

[~Christian.Lorenz77] I can look at the reproduction code next week. Does the commit eventually succeed? e.g. after the 5th checkpoint, nth checkpoint, etc?;;;","16/Jun/22 12:33;martijnvisser;{{FlinkKafkaConsumer}} has not been removed, but it has been deprecated since 1.14.0 via FLINK-24055. ;;;","16/Jun/22 12:37;Christian.Lorenz77;[~mason6345] I did not observe that the commit error vanished. To fix the error we currently restart our taskmanagers gracefully. I guess this will implicitly reset the broken kafka consumer state.;;;","16/Jun/22 19:44;martijnvisser;In my private CI run the upgrade to 3.1.1 was successful (the failing tests are other test stabilities), see https://dev.azure.com/martijn0323/Flink/_build/results?buildId=2650&view=results

No interface changes were required. The only thing that would be good to verify is if this the newer version has any change with regards to a minimum version of Kafka protocol or broker.  I looked for it but I think this is not the case based on what I could find;;;","16/Jun/22 21:24;martijnvisser;I've opened a PR against {{master}} to bump Kafka Clients there. 

I've also opened a draft PR against {{release-1.15}} - I'm not sure we're yet OK with bumping Kafka Clients in a patch release, but I wanted to verify if CI works for this one too. Given the limited change and the arguments brought forward by [~renqs] on possible other bugs that we might re-introduce when we downgrade, I would probably +1 to bump Kafka Clients for Flink 1.15.1;;;","17/Jun/22 10:15;danderson;It doesn't seem wise to upgrade the Kafka client from 2.8.1 to 3.1.1 for Flink 1.15.1. That's a big change to make this close to our release, and we shouldn't delay this release any further. So here's a proposal:
 * We bump the Kafka Clients to 3.1.1 in master now.
 * We don't try to fix FLINK-28060 for 1.15.1.

 * We create the Flink 1.15.1 release straight away, noting that there's a known issue with Kafka (FLINK-28060).
 * We reach out to the Kafka community to see if they're willing to create a 2.8.2 release with this patch.
 * In parallel, we merge the Kafka Clients bump to 3.1.1 after release 1.15.1 is done, to see how it behaves on the CI for the next few weeks and plan a quick Flink 1.15.2 release (most likely something like a month later).;;;","17/Jun/22 11:38;martijnvisser;+1 for this approach;;;","18/Jun/22 11:03;martijnvisser;Fixed in master: 189f88485d75821fe285e61bbf6623e88aec24d3;;;","20/Jun/22 10:52;Christian.Lorenz77;I think the according issue in Kafka is https://issues.apache.org/jira/browse/KAFKA-13840.;;;","21/Jun/22 16:33;mason6345;[~renqs] [~martijnvisser] were we able to reproduce this issue in Flink CI/unit test?

+1, I think we need to closely monitor KAFKA-13840, not sure if bumping to 3.1.1 really fixed the issue. A user states that they still see the issue in the 3.1.1 upgrade.;;;","22/Jun/22 07:13;martijnvisser;I'm not sure that we have a test for this behaviour. It would actually be nice if we could get one in. ;;;","26/Jul/22 21:30;danderson;[~martijnvisser] [~renqs] [~mason6345] Are we any closer to understanding this issue? Do we know how to fix it, either for a 1.15.2 release, or for 1.16.0?;;;","28/Jul/22 02:31;renqs;[~danderson] The ticket KAFKA-13840 is still pending for validation. Few things we could do on our side but I'll try to find some time to validate the patch in Kafka and push it forward.;;;","02/Aug/22 16:45;kyle.stehbens;Hi, we we're experiencing this issues and downgraded our kafka client to 2.5.1. There we're changes to how the group coordinator reconnect logic works in 2.6.0 and later that seems to have broken the recovery of the group co-coordinator and all kafka client version post 2.6.0 (inclusive)

Downgrading to v2.5.1 of the kafka library is confirmed working for us;;;","04/Aug/22 10:11;showuon;Could we try to test with Kafka v3.2.1? As mentioned in this comment: https://issues.apache.org/jira/browse/KAFKA-13840?focusedCommentId=17575186&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17575186 , I believe this issue fix be fixed in Kafka v3.2.1. Thanks.;;;","09/Aug/22 11:45;chesnay;I ran CI with Kafka 3.2.1 and it passed without requiring any other changes.
https://dev.azure.com/chesnay/flink/_build/results?buildId=2944&view=results

We could think about including it in 1.16.0, although I'm not too fond of bumping dependencies so close before the feature freeze. (To be fair though, we would have a few weeks to observe it).;;;","10/Aug/22 00:05;mason6345;[~showuon] Earlier, I had attempted to write an integration test with testcontainers in an attempt to reproduce the issue by following the steps in the ticket details. I compared the versions kafka-clients 2.8.1 and 3.1.1.

My test is as follows:
 # Start Kafka container and create topic
 # Create reader and assign the topics to underlying consumer
 # Fetch and invoke poll()
 # Call commitAsync()
 # Restart Kafka container 
 # Call commitAsync()
 # Call commitAsync()

For 2.8.1, there are 2 failures in the commit async call in step 6 and 7. For 3.1.1, there is only 1 failure at step 6.

3.1.1 seems to be the desired behavior. Some concerns:
 # However, unusually, step 4 will fail if the test doesn't invoke poll regardless of the kafka clients version. I think it is possible for Flink to have a race condition where commitAsync is executed before poll (short checkpoint interval causing commitAsync before poll if topic partitions take long to assign). Is this behavior intended?
 # Otherwise, do you have any recommendations for reproducing the issue in a CI environment where we do not assume poll() is invoked?;;;","10/Aug/22 00:16;showuon;[~mason6345] , yes, Kafka v3.1.1 assumes consumer poll ran before commitAsync call. But in Kafka v3.2.1, this assumption is removed. So, in v3.2.1, even if commitAsync calls earlier than poll, it'll work well.

So, for your question:

1. However, unusually, step 4 will fail if the test doesn't invoke poll regardless of the kafka clients version. I think it is possible for Flink to have a race condition where commitAsync is executed before poll (short checkpoint interval causing commitAsync before poll if topic partitions take long to assign). Is this behavior intended?

--> Yes, Kafka should not assume poll calls first or commitAsync calls first. That's a bug fixed in Kafka v3.2.1

2. Otherwise, do you have any recommendations for reproducing the issue in a CI environment where we do not assume poll() is invoked?

--> I think it's good. I also use the reproducer in this ticket with Kafka v3.2.1, and also investigate Kafka client logs, I confirmed it works well even if commitAsync calls earlier than poll.

 

Thanks.;;;","10/Aug/22 10:14;chesnay;master: bc9b401ed1f2e7257c7b44c9838e34ede9c52ed5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LD_PRELOAD is hardcoded to x64 on flink-docker,FLINK-28057,13450040,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,nferrario,nferrario,nferrario,14/Jun/22 13:39,30/Aug/22 09:16,13/Jul/23 08:13,26/Jul/22 13:24,1.15.0,,,,,,,1.14.6,1.15.2,1.16.0,,,flink-docker,,,,,,,0,pull-request-available,,,,"ARM images are not using jemalloc because LD_PRELOAD is hardcoded to use an x64 path, causing this error:
{noformat}
ERROR: ld.so: object '/usr/lib/x86_64-linux-gnu/libjemalloc.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
{noformat}


Right now docker-entrypoint is using this:

{code:sh}
maybe_enable_jemalloc() {
    if [ ""${DISABLE_JEMALLOC:-false}"" == ""false"" ]; then
        export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libjemalloc.so
    fi
}
{code}

I propose we use this instead:
{code:sh}
maybe_enable_jemalloc() {
    if [ ""${DISABLE_JEMALLOC:-false}"" == ""false"" ]; then
        # Maybe use export LD_PRELOAD=$LD_PRELOAD:/usr/lib/$(uname -i)-linux-gnu/libjemalloc.so
        if [[ `uname -i` == 'aarch64' ]]; then
            export LD_PRELOAD=$LD_PRELOAD:/usr/lib/aarch64-linux-gnu/libjemalloc.so
        else
            export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libjemalloc.so
        fi
    fi
}
{code}

https://github.com/apache/flink-docker/pull/117",,nferrario,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 30 09:16:06 UTC 2022,,,,,,,,,,"0|z138bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/22 13:46;yunta;Thanks for creating this issue. Flink seems do not have official arm-based docker image, do we need some changes to create arm-based docker image?;;;","14/Jun/22 13:51;nferrario;Hi [~yunta]. Flink is building official ARM images since 1.15 as far as I know, right? ([https://hub.docker.com/_/flink])

Everything works perfectly except for jemalloc, which doesn't prevent Flink from starting anyway.

 

A current workaround is to use these configs:
{code:yaml}
containerized.jobmanager.env.DISABLE_JEMALLOC: true
containerized.jobmanager.env.LD_PRELOAD: /usr/lib/aarch64-linux-gnu/libjemalloc.so
containerized.taskmanager.env.DISABLE_JEMALLOC: true
containerized.taskmanager.env.LD_PRELOAD: /usr/lib/aarch64-linux-gnu/libjemalloc.so 
{code};;;","14/Jun/22 13:52;chesnay;The images we release via official-images are actually also built for arm since FLINK-25679 (1.15).;;;","15/Jun/22 05:43;yunta;Thanks for the information [~chesnay].

[~nferrario] I have already assigned this ticket to you and we can discuss in the PR.;;;","26/Jul/22 13:24;yunta;merged in master: ed004f8cdac8e5c96b81ec069863a4f836d170d5;;;","26/Jul/22 13:25;yunta;Since Flink supports arm image from flink-1.14, [~nferrario]  could you also create PR targets for {{dev-1.14}} and {{dev-1.15}} branch? Remember to modify your commit message with the ticket ID {{{}[FLINK-xxx]{}}}.;;;","28/Jul/22 19:46;nferrario;Hi [~yunta]

[https://github.com/apache/flink-docker/pull/126] (1.14)

[https://github.com/apache/flink-docker/pull/125] (1.15);;;","29/Jul/22 02:25;yunta;merged

dev-1.15: ff2d07865f2f0d4a0adfda3525a42a5d758c64bb

dev-1.14: 04931762b8e9b6f0d29b902b8b195e562c8c4c0b

 ;;;","30/Aug/22 08:55;chesnay;[~yunta] Why do we see this warning on CI of every modified image:

??WARNING: attempted to load jemalloc from /usr/lib/x86_64-linux-gnu/libjemalloc.so but the library couldn't be found. glibc will be used instead.??;;;","30/Aug/22 09:06;chesnay;Ehhh...before that change we also got another error on CI...

??ERROR: ld.so: object '/usr/lib/x86_64-linux-gnu/libjemalloc.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.??;;;","30/Aug/22 09:13;nferrario;[~chesnay] this happens because the CI host doesn’t have jemalloc installed, and it’s actually running as “bare metal”. We’d need to change the test suite to execute the Docker image instead to make that warning go away.;;;","30/Aug/22 09:16;chesnay;[~nferrario] Got it, thank you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Invalid lambda deserialization"" in AvroParquetReaders",FLINK-28043,13449974,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,14/Jun/22 09:42,01/Jul/22 03:22,13/Jul/23 08:13,29/Jun/22 12:31,1.15.0,,,,,,,1.16.0,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,pull-request-available,,,,"I packed a bundle jar including flink-parquet and flink-avro with ""org.apache.avro"" relocated, to support PyFlink reading avro records from parquet file, and ""Invalid lambda deserialization"" error occurs at runtime. I guess this is similar to FLINK-18006 and points to MSHADE-260

 ",,dianfu,jingge,Juntao Hu,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,FLINK-28336,,,,,,,,,,,,,,MSHADE-260,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 29 12:31:54 UTC 2022,,,,,,,,,,"0|z137wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/22 12:00;Juntao Hu;[~jingge] could you take a look at this PR?;;;","15/Jun/22 16:56;jingge;Just want to let you know that I will find time to do it this week. ;;;","29/Jun/22 12:31;dianfu;Merged to master via 5c7716b4c0b8e3a9c34a236333bf6f7745bcb0f6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
table store cannot  distinguish  filesystem Scheme when system have 'hadoop classpath',FLINK-28041,13449941,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,chaojipaopao,chaojipaopao,14/Jun/22 07:43,19/Jul/22 09:51,13/Jul/23 08:13,19/Jul/22 09:51,table-store-0.1.0,table-store-0.1.1,table-store-0.2.0,,,,,table-store-0.2.0,,,,,Table Store,,,,,,,0,easyfix,,,,"when using flink-table-store Quick Start 
{code:java}
//step5: 
SET 'table-store.path' = '/tmp/table_store'; {code}
then write data submit the  insert sql  to the cluster
{code:java}
//代码占位符
INSERT INTO word_count SELECT word, COUNT(*) FROM word_table GROUP BY word; {code}
wecan see the job failed ;

the log like this:


java.io.IOException: Could not perform checkpoint 1 for operator Writer -> Local Committer (1/1)#0.
at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:1210)
at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:147)
at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.triggerCheckpoint(SingleCheckpointBarrierHandler.java:287)
at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.access$100(SingleCheckpointBarrierHandler.java:64)
at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler$ControllerImpl.triggerGlobalCheckpoint(SingleCheckpointBarrierHandler.java:493)
at org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlignedBarrierHandlerState.triggerGlobalCheckpoint(AbstractAlignedBarrierHandlerState.java:74)
at org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlignedBarrierHandlerState.barrierReceived(AbstractAlignedBarrierHandlerState.java:66)
at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.lambda$processBarrier$2(SingleCheckpointBarrierHandler.java:234)
at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.markCheckpointAlignedAndTransformState(SingleCheckpointBarrierHandler.java:262)
at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.processBarrier(SingleCheckpointBarrierHandler.java:231)
at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:181)
at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:159)
at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: java.lang.RuntimeException: java.io.FileNotFoundException: File does not exist: /tmp/table_store/default_catalog.catalog/default_database.db/ word_count/bucket-0/sst-795c7ecf-40d9-433a-8a49-81336940be7a-0
at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:66)
at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:2157)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2127)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2040)
at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:583)
at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getBlockLocations(AuthorizationProviderProxyClientProtocol.java:94)
at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:377)
at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2278)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2274)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2272)
at org.apache.flink.table.store.connector.sink.StoreSinkWriter.prepareCommit(StoreSinkWriter.java:172)
at org.apache.flink.table.store.connector.sink.StoreSinkWriter.prepareCommit(StoreSinkWriter.java:51)
at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.emitCommittables(SinkWriterOperator.java:196)
at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.prepareSnapshotPreBarrier(SinkWriterOperator.java:166)
at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.prepareSnapshotPreBarrier(RegularOperatorChain.java:89)
at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:300)
at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$12(StreamTask.java:1253)
at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:1241)
at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:1198)
... 22 more

 

*the program can distinguish filesystem*

 ","flink 1.15.0

flink-table-store 0.1.0

hadoop 2.6.5",chaojipaopao,lzljs3620320,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28072,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Java,Tue Jul 19 09:51:13 UTC 2022,,,,,,,,,,"0|z137pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/22 09:20;lzljs3620320;Thanks [~chaojipaopao] for your reporting.

I think we should enrich the path to a URI for making sure a consistent default scheme.;;;","19/Jul/22 09:51;lzljs3620320;Fixed in  FLINK-28072;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Initialise Async Sink maximum number of in flight messages to low number for rate limiting strategy,FLINK-28027,13449836,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,liangtl,CrynetLogistics,CrynetLogistics,13/Jun/22 16:28,11/Aug/22 10:11,13/Jul/23 08:13,11/Aug/22 10:11,1.15.0,1.15.1,,,,,,1.15.2,,,,,Connectors / Common,Connectors / Kinesis,,,,,,0,pull-request-available,,,,"*Background*

In the AsyncSinkWriter, we implement a rate limiting strategy.

The initial value for the maximum number of in flight messages is set extremely high ({{{}maxBatchSize * maxInFlightRequests{}}}).

However, in accordance with the AIMD strategy, the TCP implementation for congestion control has found a small value to start with [is better]([https://en.wikipedia.org/wiki/TCP_congestion_control#Slow_start]).

*Suggestion*

A better default might be:
 * maxBatchSize
 * maxBatchSize / parallelism",,CrynetLogistics,dannycranmer,Zsigner,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28487,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 11 10:11:23 UTC 2022,,,,,,,,,,"0|z13720:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 08:20;Zsigner;Hi [~CrynetLogistics] ，When I use jdbc sink, I found that there are several parameters. I don't know if it can help you. I will provide you with reference.
{code:java}
JdbcExecutionOptions
.builder() 
.withBatchSize(4000) 
.withBatchIntervalMs(200) 
.withMaxRetries(3) 
.build()
{code}
 ;;;","10/Aug/22 14:32;dannycranmer;This is fixed by FLINK-28487 in 1.16. ;;;","11/Aug/22 10:11;dannycranmer;Merged commit [{{d22c52f}}|https://github.com/apache/flink/commit/d22c52f6dda6d5aec37f50b2657293157ca40d96] into apache:release-1.15 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error in RetractableTopNFunction when retracting a stale record with state ttl enabled,FLINK-28019,13449748,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,13/Jun/22 08:32,22/Jun/22 09:57,13/Jul/23 08:13,16/Jun/22 06:53,1.14.4,1.15.0,,,,,,1.14.6,1.15.1,1.16.0,,,,,,,,,,0,pull-request-available,,,,"We found an error occurred when retract a staled record when enable state ttl in RetractableTopNFunction, a reproduce case:
{code}
    @Test
    public void testRetractAnStaledRecordWithRowNumber() throws Exception {
        StateTtlConfig ttlConfig = StateConfigUtil.createTtlConfig(1_000);
        AbstractTopNFunction func =
                new RetractableTopNFunction(
                        ttlConfig,
                        InternalTypeInfo.ofFields(
                                VarCharType.STRING_TYPE, new BigIntType(), new IntType()),
                        comparableRecordComparator,
                        sortKeySelector,
                        RankType.ROW_NUMBER,
                        new ConstantRankRange(1, 2),
                        generatedEqualiser,
                        true,
                        true);

        OneInputStreamOperatorTestHarness<RowData, RowData> testHarness = createTestHarness(func);
        testHarness.open();
        testHarness.setStateTtlProcessingTime(0);
        testHarness.processElement(insertRecord(""a"", 1L, 10));
        testHarness.setStateTtlProcessingTime(1001);
        testHarness.processElement(insertRecord(""a"", 2L, 11));
        testHarness.processElement(deleteRecord(""a"", 1L, 10));
        testHarness.close();

        List<Object> expectedOutput = new ArrayList<>();
        expectedOutput.add(insertRecord(""a"", 1L, 10, 1L));
        expectedOutput.add(insertRecord(""a"", 2L, 11, 1L));
        // the following delete record should not be sent because the left row is null which is
        // illegal.
        // -D{row1=null, row2=+I(1)};

        assertorWithRowNumber.assertOutputEquals(
                ""output wrong."", expectedOutput, testHarness.getOutput());
    }
{code}

the reason is the uncomplete path when deal with staled records.",,lincoln.86xy,lzljs3620320,straw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 16 06:53:34 UTC 2022,,,,,,,,,,"0|z136ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/22 06:53;lzljs3620320;master: c4d4bb5c28d5319fe567b31464683e3f5f22ba67

release-1.14: 5dbb51c09e0d810eabbdc2f4c0f4045dee5be519

release-1.15: 921b608158288bc807493e1c425f6d7ec6f47b18;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the start index to create empty splits in BinaryInputFormat#createInputSplits is inappropriate,FLINK-28018,13449747,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Leo Zhou,Leo Zhou,Leo Zhou,13/Jun/22 08:32,15/Jun/22 03:05,13/Jul/23 08:13,15/Jun/22 03:05,1.14.4,1.15.0,1.16.0,,,,,1.15.1,1.16.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,pull-request-available,,,,"when the number of created split is smaller than the minimum desired number of file splits, [BinaryInputFormat.java#L150|https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/common/io/BinaryInputFormat.java#L150] use `{_}*files.size()*{_}` as the start index to create empty splits. That is inappropriate, the start index should be `{_}*inputSplits.size()*{_}`.  ",,Leo Zhou,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 15 03:05:43 UTC 2022,,,,,,,,,,"0|z136i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/22 08:34;Leo Zhou;Hi [~zhuzh] , can you take a look ?;;;","13/Jun/22 09:22;zhuzh;Yes it is a bug. Thanks for reporting this! [~Leo Zhou] 

Do you want to fix it?;;;","13/Jun/22 09:27;Leo Zhou;Thanks for confirming this [~zhuzh] , I'd like to fix it.;;;","13/Jun/22 09:31;zhuzh;I have assigned you the ticket. Feel free to open a fix for it.;;;","15/Jun/22 03:05;zhuzh;Fixed via:

master: 0cf8208ede55097987abb243874d670ed5f504ae

1.15: c9a706b8388b324a37da43298e37074d0a452a34;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not get secondary resource from context after operator restart,FLINK-28008,13449577,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,morhidi,aitozi,aitozi,11/Jun/22 01:52,24/Nov/22 01:03,13/Jul/23 08:13,27/Jun/22 09:01,kubernetes-operator-1.1.0,,,,,,,kubernetes-operator-1.1.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"how to reproduce:
 * create session and submit session job
 * delete session job
 * restart operator
 * submit session job again

Then it will print log 
{noformat}
2022-06-11 01:51:15,645 o.a.f.k.o.r.s.FlinkSessionJobReconciler [WARN ][default/basic-session-job-example] Session cluster deployment is not found
2022-06-11 01:51:15,645 o.a.f.k.o.r.s.FlinkSessionJobReconciler [WARN ][default/basic-session-job-example2] Session cluster deployment is not found{noformat}
But the session cluster is still there
{noformat}
basic-session-cluster-547655d95c-888mm    1/1     Running   0          6m34s{noformat}",,aitozi,gyfora,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 27 09:01:26 UTC 2022,,,,,,,,,,"0|z135go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jun/22 03:01;aitozi;I test with the last release version with 
{code:java}
helm install flink-kubernetes-operator flink-kubernetes-operator-1.0.0/flink-kubernetes-operator {code}
It do not reproduce. I will take a closer look how it happen;;;","11/Jun/22 03:28;aitozi;I don't know if there any trick with the new sdk version. cc [~morhidi], maybe you have some more insights for this;;;","13/Jun/22 02:12;wangyang0918;Does it mean the shared informer does not work properly?;;;","13/Jun/22 07:05;aitozi;I think so, but I have not make sure;;;","22/Jun/22 13:00;morhidi;I'll have a look at it.;;;","23/Jun/22 07:05;morhidi;Reported this under https://github.com/java-operator-sdk/java-operator-sdk/issues/1299 and driving it till resolution.;;;","24/Jun/22 08:53;morhidi;quick update: we have a PR open in JOSDK with the [fix|[https://github.com/java-operator-sdk/java-operator-sdk/pull/1300]] we need to address this issue. ETA for JOSDK v3.0.3 is today or Monday. I'll open a PR once the patch release is out.;;;","27/Jun/22 09:01;gyfora;merged to main 6a4e6a5edb386675fede2790b81e31cdbcb2952c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive sql is wrongly modified by SqlCompleter in SQL client when using -f {file},FLINK-28003,13449447,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Jiangang,jingzhang,jingzhang,10/Jun/22 12:40,16/Aug/22 04:10,13/Jul/23 08:13,16/Aug/22 04:10,1.15.0,,,,,,,1.16.0,,,,,Table SQL / Client,,,,,,,0,pull-request-available,,,,"When I run the following sql in SqlClient using 'sql-client.sh -f zj_test.sql'
{code:java}
create table if not exists db.zj_test(
pos                   int,
rank_cmd              string
)
partitioned by (
`p_date` string,
`p_hourmin` string);

INSERT OVERWRITE TABLE db.zj_test PARTITION (p_date='20220605', p_hourmin = '0100')
SELECT
pos ,
rank_cmd
FROM db.sourceT
where p_date = '20220605' and p_hourmin = '0100'; {code}
An error would be thrown out because the 'pos' field is changed to 'POSITION'. I guess `SqlCompleter` in sqlClient module might do something wrong.

The error could be reproduced using the attached file.

 

 ",,jark,Jiangang,jingzhang,martijnvisser,zoucao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/22 12:38;jingzhang;zj_test.sql;https://issues.apache.org/jira/secure/attachment/13044938/zj_test.sql",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 16 04:10:41 UTC 2022,,,,,,,,,,"0|z134ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/22 10:10;martijnvisser;If you want to use a reserved keyword in your SQL syntax, you'll need to use backticks for these columns. ;;;","14/Jun/22 02:51;jingzhang;[~martijnvisser] Thanks for response, however you might misunderstand here.

I don't use reserved keyword, I use 'pos', but SqlClient replaces it to

'position' which leads to an error.

This replacement causes a lot of problems.;;;","14/Jun/22 03:14;jingzhang;As discussed with [~fsk119] offline, we would add a configure to disable sql complete in SqlClient. Besides, for '-f sqlFile', it makes sense to disable sql complete by default.;;;","14/Jun/22 07:29;martijnvisser;[~jingzhang] I was under the impression that pos was a reserved keyword, but I misread that. Apologies. ;;;","16/Jun/22 10:32;jingzhang;[~martijnvisser] Never mind, thanks for your attention.;;;","06/Jul/22 10:06;Jiangang;I have discussed with [~jingzhang] and solve it in our inner flink version. Could you review the code, [~fsk119] . Thanks.;;;","05/Aug/22 05:14;Jiangang;[~shengkai] Do you have time to review it? Thanks.;;;","16/Aug/22 04:10;jark;Fixed in master: 226f1602c047092cc1997f6e861aa37858df21f7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"test_connectors.py failed with ""object is not an instance of declaring class"" in JDK11 ",FLINK-28002,13449435,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,10/Jun/22 11:51,22/Jun/22 05:49,13/Jul/23 08:13,20/Jun/22 12:24,1.16.0,,,,,,,1.16.0,,,,,API / Python,,,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-06-10T02:43:20.7206790Z Jun 10 02:43:20 E                   : java.lang.IllegalArgumentException: object is not an instance of declaring class
2022-06-10T02:43:20.7207481Z Jun 10 02:43:20 E                   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-10T02:43:20.7208200Z Jun 10 02:43:20 E                   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-10T02:43:20.7209003Z Jun 10 02:43:20 E                   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-10T02:43:20.7209720Z Jun 10 02:43:20 E                   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2022-06-10T02:43:20.7210572Z Jun 10 02:43:20 E                   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-10T02:43:20.7211291Z Jun 10 02:43:20 E                   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-10T02:43:20.7212101Z Jun 10 02:43:20 E                   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-10T02:43:20.7212796Z Jun 10 02:43:20 E                   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2022-06-10T02:43:20.7213500Z Jun 10 02:43:20 E                   	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
2022-06-10T02:43:20.7214327Z Jun 10 02:43:20 E                   	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
2022-06-10T02:43:20.7215097Z Jun 10 02:43:20 E                   	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
2022-06-10T02:43:20.7215885Z Jun 10 02:43:20 E                   	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
2022-06-10T02:43:20.7216700Z Jun 10 02:43:20 E                   	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
2022-06-10T02:43:20.7217558Z Jun 10 02:43:20 E                   	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
2022-06-10T02:43:20.7218225Z Jun 10 02:43:20 E                   	at java.base/java.lang.Thread.run(Thread.java:829)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36508&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2
",,dianfu,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28148,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 20 12:24:03 UTC 2022,,,,,,,,,,"0|z134l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/22 12:24;dianfu;Merged to master via 314e276f6c6bff990e82515d6ce90fd6a7c9561d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoSuchMethodError when using Hive 3 dialect,FLINK-27999,13449413,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,10/Jun/22 10:08,20/Jun/22 12:34,13/Jul/23 08:13,14/Jun/22 08:32,1.15.0,,,,,,,1.15.1,1.16.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,"VirtualColumn.VIRTUAL_COLUMN_NAMES uses different types in Hive2 / Hive3, causing NoSuchMethodErrors when the Hive dialect is used.

When use 'mvn test -PHive3.1.1'  Hive connector, it will throw the following exception ""Caused by: MetaException(message:Required table missing : ""DBS"" in Catalog """" Schema """". DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable ""datanucleus.schema.autoCreateTables"")"".

From the error message, we can know the reason is the configuration ""datanucleus.schema.autoCreateTables"" is not true. But when create HiveCatalog, we do really set the configuration ""datanucleus.schema.autoCreateTables"" to true in the hive-site.xml.

After some debuging, I found the reason is that some test modify a static variable, and then boil the other test.

When running HiveCatalogFactoryTest, it will call  HiveCatalog#createHiveConf(@Nullable String hiveConfDir, @Nullable String hadoopConfDir) to create a HiveConf for HiveCatalog. In this method, it will set  the static variable ""hiveSiteURL"" to null.

Then, if we run ""HiveCatalogHiveMetadataTest"", it will call HiveTestUtils#createHiveConf() to create HiveConf. The following code will create a empty HiveConf;

 
{code:java}
HiveConf hiveConf = new HiveConf(); {code}
But in the initialize function of HiveConf, it first will apply all default variables, so ""datanucleus.schema.autoCreateTables"" will be set to false. And the ""hiveSiteURL"" is null, so skip add the resource. Then, it will check """"hive.metastore.schema.verification"" is true or false. If it's true, it will set ""datanucleus.schema.autoCreateTables"" to false. This have a higher priority and thus overwrite the value we configure in hive-site.xml.

To fix it, we only need to reset the static variable ""hiveSiteURL"" with our hive-site.xml.

 

 

 

 ",,luoyuxia,martijnvisser,tartarus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28042,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 14 08:32:46 UTC 2022,,,,,,,,,,"0|z134g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/22 10:15;martijnvisser;[~luoyuxia] Can you make the title more descriptive / add a description? This ticket is now rather vague;;;","10/Jun/22 10:35;martijnvisser;Thank you so much [~luoyuxia] :);;;","10/Jun/22 10:35;luoyuxia;[~martijnvisser] Thanks for your reminder. I will update it later.;;;","14/Jun/22 08:32;chesnay;master: 69390b71be731557397c4523e818c3fae1af278a
1.15: 90b8e82ee8a868864e65e63076fd723bdc9796d9 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition between task/savepoint notification failure,FLINK-27972,13449200,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,09/Jun/22 10:35,22/Jun/22 06:55,13/Jul/23 08:13,22/Jun/22 06:55,1.15.0,,,,,,,1.16.0,,,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"When a task throws an exception in notifyCheckpointComplete we send 2 messages to the JobManager:
1) we inform the CheckpointCoordinator about the failed savepoint
2) we inform the scheduler about the failed task.

Depending on how these arrive the adaptive scheduler exhibits different behaviors. If 1) arrives first it properly informs the user about the created savepoint which might contain uncommitted transactions; if 2) arrives first it just restarts the job.

I'm not sure how big of an issue the latter case is, but it does invalidate FLINK-26923.

In any case we might want to consider having the StopWithSavepoint state wait until the savepoint future has failed before doing anything else.",,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27869,,,,,,,,,FLINK-26923,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 22 06:55:37 UTC 2022,,,,,,,,,,"0|z13368:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/22 06:55;chesnay;master: d13cb056912d9011df96671c3bd60299a59a1117;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
end-to-end-tests-sql CI test failed,FLINK-27968,13449173,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lsy,ana4,ana4,09/Jun/22 08:07,10/Jun/22 13:09,13/Jul/23 08:13,10/Jun/22 13:09,1.16.0,,,,,,,1.15.1,1.16.0,,,,Build System / CI,Table SQL / Client,Table SQL / Planner,Tests,,,,0,pull-request-available,,,," 
{code:java}
Jun 09 03:15:01 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
Jun 09 03:15:01 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
Jun 09 03:15:01 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
Jun 09 03:15:01 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Jun 09 03:15:01 Caused by: java.io.IOException: error=13, Permission denied
Jun 09 03:15:01 	at java.lang.UNIXProcess.forkAndExec(Native Method)
Jun 09 03:15:01 	at java.lang.UNIXProcess.<init>(UNIXProcess.java:247)
Jun 09 03:15:01 	at java.lang.ProcessImpl.start(ProcessImpl.java:134)
Jun 09 03:15:01 	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
Jun 09 03:15:01 	... 65 more
Jun 09 03:15:01 
Jun 09 03:15:02 [INFO] 
Jun 09 03:15:02 [INFO] Results:
Jun 09 03:15:02 [INFO] 
Jun 09 03:15:02 [ERROR] Errors: 
Jun 09 03:15:02 [ERROR] PlannerScalaFreeITCase.testImperativeUdaf
Jun 09 03:15:02 [ERROR]   Run 1: Cannot run program ""/tmp/junit915579470100095315/junit4815507674620015662/bin/sql-client.sh"": error=13, Permission denied
Jun 09 03:15:02 [ERROR]   Run 2: Cannot run program ""/tmp/junit5631176215080579455/junit3588658300175738616/bin/sql-client.sh"": error=13, Permission denied
Jun 09 03:15:02 [INFO] 
Jun 09 03:15:02 [INFO] 
Jun 09 03:15:02 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0
 {code}
 

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36468&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461]",,ana4,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28001,,,,FLINK-27606,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 10 13:09:24 UTC 2022,,,,,,,,,,"0|z13308:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/22 08:37;jark;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36472&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461;;;","09/Jun/22 08:39;chesnay;yeah let's just disable that thing because it also fails for me.;;;","09/Jun/22 08:40;chesnay;Disabled in b77582f0bb234196a0d6e17da753d4567a0642c3.;;;","09/Jun/22 08:41;chesnay;It's weird that it runs in the misc profile anyway; it should run in e2e_2.;;;","10/Jun/22 13:09;chesnay;master: ca47e88a91ac28e962c9ac586bdcf983a36027d1
1.15.1: 52b3569e01607e2eda598511eeb0951e7f9238dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EXPLAIN PLAN produces wrong query schema for insert clause with static partition,FLINK-27965,13449115,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,09/Jun/22 03:22,15/Jun/22 06:50,13/Jul/23 08:13,15/Jun/22 06:50,1.15.0,,,,,,,1.16.0,,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"h3. How to reproduce

Add the following test under TableEnvironmentITCase
{code:java}
@Test
def testExplainInsertOverwrite(): Unit = {
  val sinkPath = tempFolder.newFolder().toString
  tEnv.executeSql(
    s""""""
       |create table MySink (
       |  first string,
       |  part string
       |) partitioned by (part)
       |with (
       |  'connector' = 'filesystem',
       |  'path' = '$sinkPath',
       |  'format' = 'testcsv'
       |)
     """""".stripMargin)

    tEnv.executeSql(
      ""explain plan for "" +
        ""insert overwrite MySink partition (part = '123') "" +
        ""select first from MySink where part = '123'"")
}
{code}
h3. Stacktrace
{code:java}
org.apache.flink.table.api.ValidationException: Column types of query result and sink for 'default_catalog.default_database.MySink' do not match.
Cause: Different number of columns.Query schema: [first: STRING, EXPR$1: STRING NOT NULL, EXPR$2: STRING NOT NULL]
Sink schema:  [first: STRING, part: STRING]    at org.apache.flink.table.planner.connectors.DynamicSinkUtils.createSchemaMismatchException(DynamicSinkUtils.java:453)
    at org.apache.flink.table.planner.connectors.DynamicSinkUtils.validateSchemaAndApplyImplicitCast(DynamicSinkUtils.java:256)
    at org.apache.flink.table.planner.connectors.DynamicSinkUtils.convertSinkToRel(DynamicSinkUtils.java:208)
    at org.apache.flink.table.planner.connectors.DynamicSinkUtils.convertSinkToRel(DynamicSinkUtils.java:170)
    at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translateToRel$1(PlannerBase.scala:258)
    at scala.Option.map(Option.scala:146)
    at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:228)
    at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$getExplainGraphs$2(PlannerBase.scala:508)
    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
    at scala.collection.Iterator.foreach(Iterator.scala:937)
    at scala.collection.Iterator.foreach$(Iterator.scala:937)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
    at scala.collection.IterableLike.foreach(IterableLike.scala:70)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableLike.map(TraversableLike.scala:233)
    at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
    at scala.collection.AbstractTraversable.map(Traversable.scala:104)
    at org.apache.flink.table.planner.delegation.PlannerBase.getExplainGraphs(PlannerBase.scala:487)
    at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:93)
    at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:50)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainInternal(TableEnvironmentImpl.java:664)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1298)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:701)
    at org.apache.flink.table.api.TableEnvironmentITCase.testExplainInsertOverwrite(TableEnvironmentITCase.scala:179)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45) {code}",,lzljs3620320,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22155,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 15 06:50:34 UTC 2022,,,,,,,,,,"0|z132nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jun/22 15:54;qingyue;Another case

Add ""explain plan for"" under TableSinkJsonPlanTest
{code:java}
@Test
public void testPartitioning() {
    String sinkTableDdl =
            ""CREATE TABLE MySink (\n""
                    + ""  a bigint,\n""
                    + ""  b int,\n""
                    + ""  c varchar\n""
                    + "") partitioned by (c) with (\n""
                    + ""  'connector' = 'filesystem',\n""
                    + ""  'format' = 'testcsv',\n""
                    + ""  'path' = '/tmp')"";
    tEnv.executeSql(sinkTableDdl);
    util.verifyJsonPlan(""insert into MySink partition (c='A') select a, b from MyTable""); // ok
    tEnv.explainSql(""insert into MySink partition (c='A') select a, b from MyTable""); // ok
    tEnv.executeSql(
                    ""explain plan for ""
                            + ""insert into MySink partition (c='A') select a, b from MyTable"")
            .print(); // throw exception
} {code};;;","15/Jun/22 06:50;lzljs3620320;master: 10ee79ac8c3cd8698d1696d4689d31292f42d8aa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The EventUtils generate event name should take the resource's uid into account,FLINK-27961,13449032,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,aitozi,aitozi,08/Jun/22 14:49,09/Jun/22 06:08,13/Jul/23 08:13,09/Jun/22 06:08,,,,,,,,kubernetes-operator-1.0.1,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Currently the event name do not include the uid of the target resource. If a resource is recreated, it will be associated with the former object's events. It's not expected and will be confusing with the empty events when describe the resource.",,aitozi,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 09 06:08:12 UTC 2022,,,,,,,,,,"0|z13254:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/22 06:08;gyfora;main: 9f46163e7c4490b006a2f25824158eb45eeee7ee
release-1.0: d8e0928a5a272604e9135015455910b6b2fcb66c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes Operator Deployment strategy type should be Recreate,FLINK-27956,13448991,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mbalassi,gyfora,gyfora,08/Jun/22 11:38,09/Jun/22 06:07,13/Jul/23 08:13,09/Jun/22 06:07,kubernetes-operator-1.0.0,,,,,,,kubernetes-operator-1.0.1,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"We should change the Deployment strategy.type from the default (RollingUpdate) to Recreate to avoid potential problems when a new operator pod is deployed during upgrade.

[https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy]

 

Only one operator pod is supposed to run at any given time to avoid any errors/inconsistencies, and without HA/leader election, this setting is necessary.",,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 09 06:07:18 UTC 2022,,,,,,,,,,"0|z131w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/22 06:07;gyfora;main: 1b470f77bde76861e80b57f0b261b5131d0e723d

release-1.0: cc8207cf8d283b112ef0bd1a82415addab32036a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink installation failure on Windows OS,FLINK-27955,13448986,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,08/Jun/22 11:22,17/Jun/22 13:10,13/Jul/23 08:13,08/Jun/22 12:26,1.15.0,,,,,,,1.15.1,1.16.0,,,,API / Python,,,,,,,0,pull-request-available,,,,"Because pemja doesn't support windows os, it makes installation failed in windows os in release-1.15. We need to fix it asap.",,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 08 12:26:54 UTC 2022,,,,,,,,,,"0|z131uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/22 11:27;chesnay;Why should we fix it? We don't support Windows.;;;","08/Jun/22 11:32;hxbks2ks;Many pyflink users are used to developing pyflink jobs locally in windows os
;;;","08/Jun/22 12:26;hxbks2ks;Merged into master via 9184bdf7ae8cc269a0104b2216b8e1c24bd5ef14
Merged into release-1.15 via 499ca049840141c12a929a3d29da31e1a164292d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IO metrics collision happens if a task has union inputs,FLINK-27944,13448847,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,zhuzh,zhuzh,07/Jun/22 17:03,01/Dec/22 16:01,13/Jul/23 08:13,01/Dec/22 16:01,1.15.0,,,,,,,1.15.4,1.16.1,1.17.0,,,Runtime / Metrics,,,,,,,0,pull-request-available,stale-assigned,,,"When a task has union inputs, some IO metrics(numBytesIn* and numBuffersIn*) of the different inputs may collide and failed to be registered.

 

The problem can be reproduced with a simple job like:
{code:java}
DataStream<String> source1 = env.fromElements(""abc"");
DataStream<String> source2 = env.fromElements(""123"");
source1.union(source2).print();{code}
 

Logs of collisions:
{code:java}
2022-06-08 00:59:01,629 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBytesInLocal'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]
2022-06-08 00:59:01,629 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBytesInLocalPerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]
2022-06-08 00:59:01,629 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBytesInLocal'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]
2022-06-08 00:59:01,629 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBytesInLocalPerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBytesInRemote'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBytesInRemotePerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBytesInRemote'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBytesInRemotePerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBuffersInLocal'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBuffersInLocalPerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBuffersInLocal'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBuffersInLocalPerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBuffersInRemote'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBuffersInRemotePerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBuffersInRemote'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBuffersInRemotePerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]
{code}",,fanrui,pnowojski,Weijie Guo,zhoujira86,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 01 16:01:44 UTC 2022,,,,,,,,,,"0|z13100:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 06:33;zhoujira86;this issue is introduced by register the metrics twice, I have created a PR to avoid the warning. ;;;","21/Jun/22 07:15;zhoujira86;[~chesnay] hi Chesnay, would you please help review the PR;;;","04/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","30/Nov/22 12:26;Weijie Guo;[~zhoujira86] Is there any new progress on this PR? We have encountered the same problem, if you don't have time, I would like to add some tests and continue to complete the work.;;;","30/Nov/22 12:58;pnowojski;I think this PR has been abandoned (I asked to rebase it over a month ago without any response), so feel free to take the code over and create a new one! ;;;","30/Nov/22 13:42;Weijie Guo;[~pnowojski] Thanks for re-assigning this to me,  I will take over and go head~;;;","01/Dec/22 13:20;pnowojski;merged to master as 20808fd^ and 20808fd 
merged to release 1.16 as c589cd5f7c8^ and c589cd5f7c8

[~Weijie Guo], could you prepare a backport to release-1.15 branch? I've backported your changes to release-1.16 already, but while doing that for release-1.15 I stumbled across some conflicts.;;;","01/Dec/22 14:06;Weijie Guo;[~pnowojski] Thanks for the merge and backport, I have created the pull request to backport this to release-1.15(https://github.com/apache/flink/pull/21439), PTAL~;;;","01/Dec/22 16:01;pnowojski;Thanks! 

Merged to release 1.15 as dd3c3473a85^ and dd3c3473a85;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Savepoint status cannot be queried from standby jobmanager,FLINK-27933,13448742,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,07/Jun/22 08:49,23/Jun/22 19:23,13/Jul/23 08:13,23/Jun/22 19:22,1.15.0,,,,,,,1.15.1,1.16.0,,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"The savepoint status handler currently doesn't work on standby dispatchers because the OperationResult isn't serializable.

This wasn't caught by the recently added serialization safeguards, as those only covered the caller side (i.e., arguments passed to callee), but not the return value. ",,danderson,elischiff,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27954,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 10 14:16:52 UTC 2022,,,,,,,,,,"0|z130co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/22 14:16;chesnay;OperationResult made serializable:
master: 72cc925a88ad054449f0ade9a476ead7a287cb6f
1.15: 0cd291ddae6581784e8d57e34fe3346607cb4f26

Additional safeguards:
master: f2ef60c4fd5470995c4acd6e6f96c6cd6bde84c3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo fix for release-1.0.0 quick-start.md,FLINK-27923,13448623,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,jbusche,jbusche,jbusche,06/Jun/22 17:01,06/Jun/22 19:41,13/Jul/23 08:13,06/Jun/22 19:41,kubernetes-operator-1.0.0,,,,,,,kubernetes-operator-1.0.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Noticed a typo while deploying the example.

Currently:
kubectl create -f https://raw.githubusercontent.com/apache/flink-kubernetes-operator/release-0.1/examples/basic.yaml


Where it should be:

kubectl create -f https://raw.githubusercontent.com/apache/flink-kubernetes-operator/release-1.0.0/examples/basic.yaml",,jbusche,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-06-06 17:01:22.0,,,,,,,,,,"0|z12zm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSink not enforcing rolling policy if started from scratch,FLINK-27910,13448551,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pltbkd,gaoyunhaii,gaoyunhaii,06/Jun/22 09:53,17/Jun/22 13:08,13/Jul/23 08:13,08/Jun/22 07:53,1.15.0,1.16.0,,,,,,1.15.1,1.16.0,,,,Connectors / FileSystem,,,,,,,1,pull-request-available,,,,"The current FileWriter only register the timer in initializeState, which is now only called on restoring. Thus if the job is started from scratch, the timer would fail to be registered and cause the rolling policy not work. ",,danderson,gaoyunhaii,martijnvisser,pltbkd,sap1ens,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 08 07:53:13 UTC 2022,,,,,,,,,,"0|z12z68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/22 09:54;gaoyunhaii;Also cc [~pltbkd] ;;;","06/Jun/22 17:00;danderson;This is super confusing, and there's no reasonable workaround – so I've bumped up the priority to Critical.;;;","07/Jun/22 06:27;pltbkd;This is a mistake while migrating the FileSink to the new sink API. In the new sink API createWriter and restoreWriter have been separated into two methods, while originally creating a writer is by calling restoreWriter with an empty state collection. We mistook the the meaning of createWriter and only created a writer in it. 
A PR has been provided to fix this bug, which changes the createWriter as the original behavior.;;;","08/Jun/22 07:53;gaoyunhaii;Fix on master via d036c23c0e5c079eaafef250a5a14b7f3eead8f1

Fix on release-1.15 via a9905e2a14b54a10c429af8b75f414c8ac3b7638;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More than 1 secondary resource related to primary,FLINK-27892,13448304,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,morhidi,morhidi,,03/Jun/22 09:32,24/Nov/22 01:03,13/Jul/23 08:13,14/Jun/22 14:54,kubernetes-operator-1.1.0,,,,,,,kubernetes-operator-1.1.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"When submitting the `the basic-session-job.yaml' in multiple namespaces:

{{flink-kubernetes-operator java.lang.IllegalStateException: More than 1 secondary resource related to primary flink-kubernetes-operator at io.javaoperatorsdk.operator.processing.event.source.ResourceEventSource.getSecondaryResource(ResourceEventSource.java:19) flink-kubernetes-operator at io.javaoperatorsdk.operator.api.reconciler.DefaultContext.getSecondaryResource(DefaultContext.java:47) flink-kubernetes-operator at io.javaoperatorsdk.operator.api.reconciler.Context.getSecondaryResource(Context.java:15) flink-kubernetes-operator at org.apache.flink.kubernetes.operator.controller.FlinkSessionJobController.validateSessionJob(FlinkSessionJobController.java:135) flink-kubernetes-operator at org.apache.flink.kubernetes.operator.controller.FlinkSessionJobController.reconcile(FlinkSessionJobController.java:91) flink-kubernetes-operator at org.apache.flink.kubernetes.operator.controller.FlinkSessionJobController.reconcile(FlinkSessionJobController.java:51) flink-kubernetes-operator at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:201) flink-kubernetes-operator at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:153) flink-kubernetes-operator at io.javaoperatorsdk.operator.api.monitoring.Metrics.timeControllerExecution(Metrics.java:34) flink-kubernetes-operator at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:152) flink-kubernetes-operator at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:135) flink-kubernetes-operator at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:115) flink-kubernetes-operator at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:86) flink-kubernetes-operator at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:59) flink-kubernetes-operator at io.javaoperatorsdk.operator.processing.event.EventProcessor$ControllerExecution.run(EventProcessor.java:390) flink-kubernetes-operator at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) flink-kubernetes-operator at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) flink-kubernetes-operator at java.base/java.lang.Thread.run(Unknown Source)}}",,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 14 14:54:11 UTC 2022,,,,,,,,,,"0|z12xnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/22 10:46;morhidi;Can be reproduced by submitting the same session jobs into multiplenamespaces:

 

{{apiVersion: flink.apache.org/v1beta1}}
{{kind: FlinkDeployment}}
{{metadata:}}
{{  name: basic-session-cluster}}
{{  namespace: default}}
{{spec:}}
{{  image: flink:1.15}}
{{  flinkVersion: v1_15}}
{{  jobManager:}}
{{    resource:}}
{{      memory: ""2048m""}}
{{      cpu: 1}}
{{  taskManager:}}
{{    resource:}}
{{      memory: ""2048m""}}
{{      cpu: 1}}
{{  serviceAccount: flink}}
{{—}}
{{apiVersion: flink.apache.org/v1beta1}}
{{kind: FlinkSessionJob}}
{{metadata:}}
{{  name: basic-session-job-example}}
{{  namespace: default}}
{{spec:}}
{{  deploymentName: basic-session-cluster}}
{{  job:}}
{{    jarURI: [https://repo1.maven.org/maven2/org/apache/flink/flink-examples-streaming_2.12/1.15.0/flink-examples-streaming_2.12-1.15.0-TopSpeedWindowing.jar]}}
{{    parallelism: 4}}
{{    upgradeMode: stateless}}
{{—}}
{{apiVersion: flink.apache.org/v1beta1}}
{{kind: FlinkDeployment}}
{{metadata:}}
{{  name: basic-session-cluster}}
{{  namespace: flink}}
{{spec:}}
{{  image: flink:1.15}}
{{  flinkVersion: v1_15}}
{{  jobManager:}}
{{    resource:}}
{{      memory: ""2048m""}}
{{      cpu: 1}}
{{  taskManager:}}
{{    resource:}}
{{      memory: ""2048m""}}
{{      cpu: 1}}
{{  serviceAccount: flink}}
{{—}}
{{apiVersion: flink.apache.org/v1beta1}}
{{kind: FlinkSessionJob}}
{{metadata:}}
{{  name: basic-session-job-example}}
{{  namespace: flink}}
{{spec:}}
{{  deploymentName: basic-session-cluster}}
{{  job:}}
{{    jarURI: [https://repo1.maven.org/maven2/org/apache/flink/flink-examples-streaming_2.12/1.15.0/flink-examples-streaming_2.12-1.15.0-TopSpeedWindowing.jar]}}
{{    parallelism: 4}}
{{    upgradeMode: stateless}};;;","14/Jun/22 14:54;gyfora;merged to main 33ca85ea4b7ec45a35c554cab0fba562160672b2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SideOutputExample.java fails,FLINK-27890,13448272,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,coderap,wxwmd,wxwmd,03/Jun/22 07:16,17/Jun/22 13:08,13/Jul/23 08:13,13/Jun/22 08:25,1.15.0,,,,,,,1.15.1,1.16.0,,,,Examples,,,,,,,0,pull-request-available,,,,"The bug appears on line 87 of flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/sideoutput/SideOutputExample.java：
{code:java}
text.assignTimestampsAndWatermarks(IngestionTimeWatermarkStrategy.create()); {code}
This line forgets to set the return value, so the timestamp is not assigned to the element in {_}text{_}. As a result, running this code throws an error.","# flink version 1.15
 # jdk 1.8
 # scala 2.12",coderap,danderson,JasonLee,martijnvisser,wxwmd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19317,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 13 08:25:26 UTC 2022,,,,,,,,,,"0|z12xg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/22 03:55;coderap;Bug introduced by FLINK-19317 [~aljoscha]

It assigns the TimestampsAndWatermarks but not uses the return result for the next step

Please assign this ticket to me cc [~aljoscha] 

 

 

 ;;;","07/Jun/22 08:45;martijnvisser;CC [~danderson];;;","09/Jun/22 10:09;danderson;[~coderap] Good catch! You're right, this doesn't work correctly. I'm assigning this to you.;;;","13/Jun/22 08:25;danderson;Merged in master with [{{a70e704}}|https://github.com/apache/flink/commit/a70e7045a3eabacb50f54a204bcde4fe554d8e8b]
Merged in release-1.15 with [{{6fcec2c}}|https://github.com/apache/flink/commit/6fcec2cf464f0467c3bb5ca3d249ac3ac754820a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error when the LastReconciledSpec is null,FLINK-27889,13448258,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Miuler,Miuler,Miuler,03/Jun/22 05:35,04/Jun/22 17:32,13/Jul/23 08:13,04/Jun/22 17:25,,,,,,,,kubernetes-operator-1.0.1,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"My FlinkDeployment was left with erros, when he can not start correctly, the following message:

 

 
{code:java}
2022-06-01 04:36:10,070 o.a.f.k.o.r.ReconciliationUtils [WARN ][flink-02/cosmosdb] Attempt count: 5, last attempt: true
2022-06-01 04:36:10,072 i.j.o.p.e.ReconciliationDispatcher [ERROR][flink-02/cosmosdb] Error during event processing ExecutionScope{ resource id: CustomResourceID
{name='cosmosdb', namespace='flink-02'}, version: null} failed.
org.apache.flink.kubernetes.operator.exception.ReconciliationException: java.lang.IllegalArgumentException: Only ""local"" is supported as schema for application mode. This assumes that the jar is located in the image, not the Flink client. An example of such path is: local:///opt/flink/examples/streaming/WindowJoin.jar
        at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:130)
        at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:59)
        at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:101)
        at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:76)
        at io.javaoperatorsdk.operator.api.monitoring.Metrics.timeControllerExecution(Metrics.java:34)
        at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:75)
        at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:143)
        at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:109)
        at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:74)
        at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:50)
        at io.javaoperatorsdk.operator.processing.event.EventProcessor$ControllerExecution.run(EventProcessor.java:349)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base/java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: Only ""local"" is supported as schema for application mode. This assumes that the jar is located in the image, not the Flink client. An example of such path is: local:///opt/flink/examples/streaming/WindowJoin.jar
        at org.apache.flink.kubernetes.utils.KubernetesUtils.lambda$checkJarFileForApplicationMode$2(KubernetesUtils.java:407)
        at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedFunction$2(FunctionUtils.java:73)
        at java.base/java.util.stream.ReferencePipeline$3$1.accept(Unknown Source)
        at java.base/java.util.Collections$2.tryAdvance(Unknown Source)
        at java.base/java.util.Collections$2.forEachRemaining(Unknown Source)
        at java.base/java.util.stream.AbstractPipeline.copyInto(Unknown Source)
        at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source)
        at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(Unknown Source)
        at java.base/java.util.stream.AbstractPipeline.evaluate(Unknown Source)
        at java.base/java.util.stream.ReferencePipeline.collect(Unknown Source)
        at org.apache.flink.kubernetes.utils.KubernetesUtils.checkJarFileForApplicationMode(KubernetesUtils.java:412)
        at org.apache.flink.kubernetes.KubernetesClusterDescriptor.deployApplicationCluster(KubernetesClusterDescriptor.java:206)
        at org.apache.flink.client.deployment.application.cli.ApplicationClusterDeployer.run(ApplicationClusterDeployer.java:67)
        at org.apache.flink.kubernetes.operator.service.FlinkService.submitApplicationCluster(FlinkService.java:163)
        at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.deployFlinkJob(ApplicationReconciler.java:283)
        at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.reconcile(ApplicationReconciler.java:83)
        at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.reconcile(ApplicationReconciler.java:58)
        at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:126)
        ... 13 more
2022-06-01 04:36:10,073 i.j.o.p.e.EventProcessor       [ERROR][flink-02/cosmosdb] Exhausted retries for ExecutionScope{ resource id: CustomResourceID{name='cosmosdb', namespace='flink-02'}
, version: null}
2022-06-01 04:37:27,344 o.a.f.k.o.c.FlinkDeploymentController [INFO ][flink-02/cosmosdb] Deleting FlinkDeployment
2022-06-01 04:37:27,345 i.j.o.p.e.ReconciliationDispatcher [ERROR][flink-02/cosmosdb] Error during event processing ExecutionScope{ resource id: CustomResourceID
{name='cosmosdb', namespace='flink-02'}, version: 5206202} failed.
java.lang.RuntimeException: Cannot create observe config before first deployment, this indicates a bug.
        at org.apache.flink.kubernetes.operator.config.FlinkConfigManager.getObserveConfig(FlinkConfigManager.java:137)
        at org.apache.flink.kubernetes.operator.service.FlinkService.cancelJob(FlinkService.java:357)
        at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.shutdown(ApplicationReconciler.java:327)
        at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractDeploymentReconciler.cleanup(AbstractDeploymentReconciler.java:56)
        at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractDeploymentReconciler.cleanup(AbstractDeploymentReconciler.java:37)
        at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.cleanup(FlinkDeploymentController.java:107)
        at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.cleanup(FlinkDeploymentController.java:59)
        at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:68)
        at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:50)
        at io.javaoperatorsdk.operator.api.monitoring.Metrics.timeControllerExecution(Metrics.java:34)
        at io.javaoperatorsdk.operator.processing.Controller.cleanup(Controller.java:49)
        at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleCleanup(ReconciliationDispatcher.java:252)
        at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:72)
        at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:50)
        at io.javaoperatorsdk.operator.processing.event.EventProcessor$ControllerExecution.run(EventProcessor.java:349)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base/java.lang.Thread.run(Unknown Source)
{{2022-06-01 04:37:27,345 i.j.o.p.e.EventProcessor       [ERROR][flink-02/cosmosdb] Exhausted retries for ExecutionScope{ resource id: CustomResourceID{name='cosmosdb', namespace='flink-02'}
, version: 5206202}}}
2022-06-01 04:40:44,540 o.a.f.k.o.c.FlinkConfigManager [INFO ] Default configuration did not change, nothing to do...
2022-06-01 04:40:44,848 o.a.f.m.s.Slf4jReporter        [INFO ]
{code}
 

 

The FlinkDeployment of the example is the fallowing: [^scratch_7.json]

 

 

 ",,gyfora,Miuler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/22 05:42;Miuler;scratch_7.json;https://issues.apache.org/jira/secure/attachment/13044599/scratch_7.json",,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jun 04 17:31:18 UTC 2022,,,,,,,,,,"0|z12xd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jun/22 17:25;gyfora;merged to main cd5107d7ace9d2896efde91ac0ca3c7a9226f3bc;;;","04/Jun/22 17:31;gyfora;release-1.0 : 6ce1c6b51241ca2b866816232acb1612e2b18c40;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The key(String) in PulsarMessageBuilder returns null,FLINK-27881,13448098,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,csq,csq,02/Jun/22 10:56,20/Jun/22 12:30,13/Jul/23 08:13,16/Jun/22 14:58,1.15.0,,,,,,,1.15.1,1.16.0,,,,Connectors / Pulsar,,,,,,,0,pull-request-available,,,,"The PulsarMessageBuild.key(String) always return null, which might cause NPE in later call.",,csq,martijnvisser,pemide,syhily,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 16 14:58:01 UTC 2022,,,,,,,,,,"0|z12wdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/22 13:24;martijnvisser;[~syhily] Any thoughts on this? ;;;","15/Jun/22 22:24;syhily;I'll submit a PR today. This is a known bug.;;;","16/Jun/22 14:58;chesnay;master: 6b2a3fe3afce52b0f8bf8969f95f1f5a4d94ccb1
1.15: b6d5e3dd11f3071f6c99ee86bcb86ac184d654ca ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configuration change is undedected on config removal,FLINK-27871,13447964,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,morhidi,,01/Jun/22 15:38,24/Nov/22 01:02,13/Jul/23 08:13,22/Jun/22 13:07,kubernetes-operator-1.0.0,,,,,,,kubernetes-operator-1.1.0,,,,,Kubernetes Operator,,,,,,,0,,,,,"The Operator does not detect when a configuration entry is removed from the configmap. The equals check in *FlinkConfigManager.updateDefaultConfig* returns *true* incorrectly:

 

{{if (newConf.equals(defaultConfig)) {}}
{{LOG.info(""Default configuration did not change, nothing to do..."");}}
{{return;}}
{{}}}",,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28141,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 21 08:23:02 UTC 2022,,,,,,,,,,"0|z12vjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 08:23;gyfora;This is fixed already right?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
List the CSS/docs dependencies in the NOTICE,FLINK-27860,13447847,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wangyang0918,wangyang0918,wangyang0918,01/Jun/22 05:57,01/Jun/22 07:07,13/Jul/23 08:13,01/Jun/22 07:07,,,,,,,,kubernetes-operator-1.0.0,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,We should list the CSS/docs dependencies in the NOTICE file.,,wangyang0918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 01 07:07:05 UTC 2022,,,,,,,,,,"0|z12uts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/22 07:07;wangyang0918;Fixed via:

main: a393a1c2d232a61d7e45c734d6d3643c70465423

release-1.0: d5d7b664a09ae5d717742a879f6bc2a5d6f0f0a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
